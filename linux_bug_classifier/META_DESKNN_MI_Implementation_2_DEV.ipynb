{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e2e1f0-68a5-43aa-9d1c-ac4588e96abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68392542-2e4c-4ee7-af82-348eb630eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf4f04b-57dd-4c32-a612-755ce8a92334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    # Load the datasets\n",
    "    df_net = pd.read_csv('data/linux_driver_net.csv')\n",
    "    df_scsi = pd.read_csv('data/linux_driver_scsi.csv')\n",
    "    df_ext3 = pd.read_csv('data/linux_ext3.csv')\n",
    "    df_ipv4 = pd.read_csv('data/linux_ipv4.csv')\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([df_net, df_scsi, df_ext3, df_ipv4], ignore_index=True)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"Missing values before preprocessing: {combined_df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Fill missing values - use mean for numeric columns\n",
    "    numeric_columns = combined_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    combined_df[numeric_columns] = combined_df[numeric_columns].fillna(combined_df[numeric_columns].mean())\n",
    "    \n",
    "    # Drop any rows that still have missing values\n",
    "    combined_df = combined_df.dropna()\n",
    "    \n",
    "    # Check for imbalance in the target variable\n",
    "    print(\"Class distribution:\")\n",
    "    print(combined_df['AgingRelatedBugs'].value_counts())\n",
    "    print(f\"Imbalance ratio: {combined_df['AgingRelatedBugs'].value_counts()[0] / combined_df['AgingRelatedBugs'].value_counts()[1]}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = combined_df.drop(['Filename', 'AgingRelatedBugs'], axis=1)\n",
    "    y = combined_df['AgingRelatedBugs']\n",
    "    \n",
    "    # Feature scaling \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3173befb-7809-484e-aeb0-9eebf90bce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_splits(X, y):\n",
    "    # Split data into training, dynamic selection, and testing sets (33%, 33%, 33%)\n",
    "    X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "    X_train, X_dsel, y_train, y_dsel = train_test_split(X_train_temp, y_train_temp, test_size=0.5, random_state=42, stratify=y_train_temp)\n",
    "    \n",
    "    # Count minority samples\n",
    "    minority_count = Counter(y_train)[1]\n",
    "    \n",
    "    # Choose appropriate k for SMOTE based on minority class size\n",
    "    k_neighbors = min(5, minority_count - 1)  # Must be less than minority count\n",
    "    \n",
    "    if k_neighbors > 0:\n",
    "        # Apply SMOTE with adjusted k_neighbors\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If too few samples, duplicate existing minority samples\n",
    "        print(\"Too few minority samples for SMOTE. Using simple duplication.\")\n",
    "        X_train_balanced, y_train_balanced = simple_duplicate_balance(X_train, y_train)\n",
    "    \n",
    "    print(\"Original training set class distribution:\", Counter(y_train))\n",
    "    print(\"Balanced training set class distribution:\", Counter(y_train_balanced))\n",
    "    \n",
    "    return X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0d5777-19a0-4ee6-a943-f78c9af0864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_duplicate_balance(X, y):\n",
    "    \"\"\"Balance dataset by duplicating minority class samples\"\"\"\n",
    "    majority_count = Counter(y).most_common(1)[0][1]\n",
    "    minority_class = Counter(y).most_common()[-1][0]\n",
    "    \n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "    \n",
    "    # How many times to duplicate each minority sample\n",
    "    n_duplicates = int(np.ceil(majority_count / len(minority_indices)))\n",
    "    \n",
    "    # Create duplicated minority samples\n",
    "    X_minority = X[minority_indices]\n",
    "    y_minority = y[minority_indices]\n",
    "    \n",
    "    X_minority_dup = np.repeat(X_minority, n_duplicates, axis=0)[:majority_count]\n",
    "    y_minority_dup = np.repeat(y_minority, n_duplicates)[:majority_count]\n",
    "    \n",
    "    # Combine with majority class\n",
    "    majority_indices = np.where(y != minority_class)[0]\n",
    "    X_balanced = np.vstack([X[majority_indices], X_minority_dup])\n",
    "    y_balanced = np.concatenate([y[majority_indices], y_minority_dup])\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1cbdcb3-09d8-4bb5-b44e-071febe3dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_pool(X_train_balanced, y_train_balanced, M=100):\n",
    "    \"\"\"\n",
    "    Creates a pool of M diverse classifiers using RandomForestClassifier with different parameters\n",
    "    \"\"\"\n",
    "    classifier_pool = []\n",
    "    \n",
    "    # Create base classifiers with different parameters for diversity\n",
    "    for i in range(M):\n",
    "        # Vary parameters for diversity\n",
    "        max_depth = np.random.randint(3, 20) if i % 3 == 0 else None\n",
    "        min_samples_split = np.random.randint(2, 20) if i % 5 == 0 else 2\n",
    "        min_samples_leaf = np.random.randint(1, 10) if i % 7 == 0 else 1\n",
    "        \n",
    "        # Create a random forest classifier with varied parameters\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=20 if DEV_MODE else 100,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            bootstrap=bool(i % 2),\n",
    "            random_state=i\n",
    "        )\n",
    "        \n",
    "        # Train the classifier on the balanced training set\n",
    "        clf.fit(X_train_balanced, y_train_balanced)\n",
    "        classifier_pool.append(clf)\n",
    "    \n",
    "    return classifier_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7a0eeb-00ee-4522-996b-0471489647ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the find_k_nearest_neighbors function\n",
    "def find_k_nearest_neighbors(x, dataset, K):\n",
    "    \"\"\"Find K nearest neighbors using scikit-learn's optimized implementation\"\"\"\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        dataset_array = dataset.values\n",
    "    else:\n",
    "        dataset_array = dataset\n",
    "        \n",
    "    # Reshape x if needed\n",
    "    x_reshaped = x.reshape(1, -1) if len(x.shape) == 1 else x\n",
    "    \n",
    "    # Use scikit-learn's optimized KNN implementation\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(K, len(dataset_array)), algorithm='ball_tree').fit(dataset_array)\n",
    "    _, indices = nbrs.kneighbors(x_reshaped)\n",
    "    \n",
    "    return indices[0]  # Return as a simple array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43faf52-8f07-444b-a5ef-a6c2dee6a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate output profile\n",
    "def compute_output_profile(x, classifier_pool):\n",
    "    \"\"\"Compute the output profile of an instance based on the classifier pool predictions\"\"\"\n",
    "    return np.array([clf.predict([x])[0] for clf in classifier_pool])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399ff71b-a42d-4d89-978b-53f9610380fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar output profiles\n",
    "def find_similar_output_profiles(x_profile, output_profiles, K_s):\n",
    "    \"\"\"Find indices of K_s most similar output profiles using Hamming distance\"\"\"\n",
    "    hamming_distances = np.sum(output_profiles != x_profile, axis=1)\n",
    "    return np.argsort(hamming_distances)[:K_s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc1eafb9-32d3-4453-9fb0-5ceb0057ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_instance_weights(region_indices, y, alpha=0.9):\n",
    "    \"\"\"Calculate weights for instances in competence regions\"\"\"\n",
    "    weights = []\n",
    "    \n",
    "    # Convert y to numpy array if it's a pandas Series\n",
    "    if isinstance(y, pd.Series):\n",
    "        y_array = y.values\n",
    "    else:\n",
    "        y_array = y\n",
    "    \n",
    "    # Ensure region_indices are within bounds\n",
    "    valid_indices = [idx for idx in region_indices if idx < len(y_array)]\n",
    "    \n",
    "    for idx in valid_indices:\n",
    "        current_class = y_array[idx]\n",
    "        # Count instances with the same class\n",
    "        num_same_class = sum(1 for i in valid_indices if y_array[i] == current_class)\n",
    "        # Calculate weight\n",
    "        weight = 1 / (1.0 + np.exp(alpha * num_same_class))\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Handle empty weights list\n",
    "    if not weights:\n",
    "        return np.array([1.0])\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    \n",
    "    return normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2010fac9-3723-4d44-97c8-5a8ec0a6b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta_features(theta_j, phi_j, theta_weights, phi_weights, y, classifier, X):\n",
    "    \"\"\"\n",
    "    Extract meta-features for a classifier based on competence regions\n",
    "    \n",
    "    Returns:\n",
    "    - Feature vector containing 7 sets of meta-features described in the META-DESKNN-MI algorithm\n",
    "    \"\"\"\n",
    "    meta_features = []\n",
    "    \n",
    "    # Convert to numpy arrays for consistent indexing\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_array = X.values\n",
    "    else:\n",
    "        X_array = X\n",
    "        \n",
    "    if isinstance(y, pd.Series):\n",
    "        y_array = y.values\n",
    "    else:\n",
    "        y_array = y\n",
    "    \n",
    "    # Ensure indices are within bounds\n",
    "    max_idx = len(X_array) - 1\n",
    "    valid_theta_j = [idx for idx in theta_j if idx <= max_idx]\n",
    "    valid_phi_j = [idx for idx in phi_j if idx <= max_idx]\n",
    "    \n",
    "    # Handle empty regions\n",
    "    if not valid_theta_j:\n",
    "        valid_theta_j = [0]  # Use first sample as fallback\n",
    "    \n",
    "    if not valid_phi_j:\n",
    "        valid_phi_j = [0]  # Use first sample as fallback\n",
    "    \n",
    "    # f1: Neighbors' hard classification (based on feature space)\n",
    "    f1 = []\n",
    "    for idx in valid_theta_j:\n",
    "        # If classifier correctly predicts the instance\n",
    "        if classifier.predict([X_array[idx]])[0] == y_array[idx]:\n",
    "            f1.append(1)\n",
    "        else:\n",
    "            f1.append(0)\n",
    "    \n",
    "    # f2: Posterior probability\n",
    "    f2 = []\n",
    "    for idx in valid_theta_j:\n",
    "        # Get probability of the true class\n",
    "        probs = classifier.predict_proba([X_array[idx]])[0]\n",
    "        true_class_idx = int(y_array[idx])\n",
    "        if true_class_idx < len(probs):  # Ensure index is valid\n",
    "            f2.append(probs[true_class_idx])\n",
    "        else:\n",
    "            f2.append(0.0)\n",
    "    \n",
    "    # f3: Overall local accuracy (feature space)\n",
    "    f3 = [np.mean(f1)]\n",
    "    \n",
    "    # f4: Output profiles classification (decision space)\n",
    "    f4 = []\n",
    "    for idx in valid_phi_j:\n",
    "        # If classifier correctly predicts the instance\n",
    "        if classifier.predict([X_array[idx]])[0] == y_array[idx]:\n",
    "            f4.append(1)\n",
    "        else:\n",
    "            f4.append(0)\n",
    "    \n",
    "    # f5: Classifier's confidence (implementation simplified)\n",
    "    # Using mean probability as a confidence measure\n",
    "    instance_idx = valid_theta_j[0]  # Using the first neighbor to compute confidence\n",
    "    probs = classifier.predict_proba([X_array[instance_idx]])[0]\n",
    "    f5 = [np.max(probs)]\n",
    "    \n",
    "    # Adjust weights if necessary to match valid indices\n",
    "    if len(theta_weights) > len(valid_theta_j):\n",
    "        theta_weights = theta_weights[:len(valid_theta_j)]\n",
    "    elif len(theta_weights) < len(valid_theta_j):\n",
    "        # Extend weights with zeros\n",
    "        theta_weights = np.pad(theta_weights, (0, len(valid_theta_j) - len(theta_weights)))\n",
    "    \n",
    "    if len(phi_weights) > len(valid_phi_j):\n",
    "        phi_weights = phi_weights[:len(valid_phi_j)]\n",
    "    elif len(phi_weights) < len(valid_phi_j):\n",
    "        # Extend weights with zeros\n",
    "        phi_weights = np.pad(phi_weights, (0, len(valid_phi_j) - len(phi_weights)))\n",
    "    \n",
    "    # f6: Overall weighted accuracy based on feature space\n",
    "    f6 = [np.sum(np.array(f1) * theta_weights)]\n",
    "    \n",
    "    # f7: Overall weighted accuracy based on decision space\n",
    "    f7 = [np.sum(np.array(f4) * phi_weights)]\n",
    "    \n",
    "    # Combine all meta-features\n",
    "    meta_features = f1 + f2 + f3 + f4 + f5 + f6 + f7\n",
    "    \n",
    "    return np.array(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfac142-fce4-411d-9c81-b7997787254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_training_phase(X_train, y_train, classifier_pool, K=7, K_s=5, alpha=0.9, h_c=0.8):\n",
    "    \"\"\"\n",
    "    Implement the meta-training phase to train the meta-classifier\n",
    "    \n",
    "    Returns:\n",
    "    - Trained meta-classifier lambda\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for consistent indexing\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_array = X_train.values\n",
    "    else:\n",
    "        X_array = X_train.copy()\n",
    "        \n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_array = y_train.values\n",
    "    else:\n",
    "        y_array = y_train.copy()\n",
    "    \n",
    "    # Step 1: Sample selection process\n",
    "    selected_indices = []\n",
    "    y_pred_per_classifier = np.zeros((len(X_array), len(classifier_pool)))\n",
    "    \n",
    "    # Calculate predictions for all samples and classifiers\n",
    "    for i, clf in enumerate(classifier_pool):\n",
    "        y_pred_per_classifier[:, i] = clf.predict(X_array)\n",
    "    \n",
    "    # Check consensus of the pool for each instance\n",
    "    for j in range(len(X_array)):\n",
    "        # Count votes for each class\n",
    "        votes = Counter(y_pred_per_classifier[j, :])\n",
    "        \n",
    "        # If there are at least two classes with votes\n",
    "        if len(votes) >= 2:\n",
    "            sorted_votes = votes.most_common()\n",
    "            # Calculate difference between votes of winning class and second class\n",
    "            consensus = sorted_votes[0][1] - sorted_votes[1][1]\n",
    "            consensus_ratio = consensus / len(classifier_pool)\n",
    "            \n",
    "            # If consensus is low, select the instance\n",
    "            if consensus_ratio < h_c:\n",
    "                selected_indices.append(j)\n",
    "    \n",
    "    # If no instances selected or too few, use a minimum number\n",
    "    min_required = max(K, K_s + 1)\n",
    "    if len(selected_indices) < min_required:\n",
    "        print(f\"Too few instances selected ({len(selected_indices)}). Using top {min_required} instances.\")\n",
    "        # Ensure we don't select more indices than available\n",
    "        max_possible = min(min_required, len(X_array))\n",
    "        selected_indices = list(range(max_possible))\n",
    "    \n",
    "    # Also ensure that any index used later is valid\n",
    "    selected_indices = [idx for idx in selected_indices if idx < len(X_array)]\n",
    "    \n",
    "    # Step 2: Meta-feature extraction\n",
    "    X_meta = []  # Meta-features\n",
    "    y_meta = []  # Meta-labels\n",
    "    \n",
    "    # Compute output profiles for all instances\n",
    "    output_profiles = np.array([\n",
    "        [clf.predict([X_array[i]])[0] for clf in classifier_pool]\n",
    "        for i in range(len(X_array))\n",
    "    ])\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        # Find the competence region (K nearest neighbors)\n",
    "        theta_j = find_k_nearest_neighbors(X_array[idx], X_array, K)\n",
    "        \n",
    "        # Compute the output profile\n",
    "        output_profile = output_profiles[idx]\n",
    "        \n",
    "        # Find similar output profiles\n",
    "        phi_j = find_similar_output_profiles(output_profile, output_profiles, K_s)\n",
    "        \n",
    "        # Ensure all indices are valid\n",
    "        theta_j = [i for i in theta_j if i < len(X_array)]\n",
    "        phi_j = [i for i in phi_j if i < len(X_array)]\n",
    "        \n",
    "        # If we lost all indices, use some fallback\n",
    "        if not theta_j:\n",
    "            theta_j = [0]  # Use first sample as fallback\n",
    "        \n",
    "        if not phi_j:\n",
    "            phi_j = [0]  # Use first sample as fallback\n",
    "        \n",
    "        # Calculate instance weights\n",
    "        theta_weights = calculate_instance_weights(theta_j, y_array, alpha)\n",
    "        phi_weights = calculate_instance_weights(phi_j, y_array, alpha)\n",
    "        \n",
    "        # For each classifier, extract meta-features\n",
    "        for i, clf in enumerate(classifier_pool):\n",
    "            # Extract meta-features\n",
    "            meta_features = extract_meta_features(theta_j, phi_j, theta_weights, phi_weights, y_array, clf, X_array)\n",
    "            \n",
    "            # Determine if the classifier correctly classifies the instance\n",
    "            is_correct = int(clf.predict([X_array[idx]])[0] == y_array[idx])\n",
    "            \n",
    "            # Add meta-features and meta-label\n",
    "            X_meta.append(meta_features)\n",
    "            y_meta.append(is_correct)\n",
    "    \n",
    "    # Step 3: Train meta-classifier\n",
    "    # Ensure we have non-negative values for MultinomialNB\n",
    "    X_meta = np.abs(X_meta)\n",
    "    \n",
    "    # Handle case where all meta-labels are the same\n",
    "    unique_labels = np.unique(y_meta)\n",
    "    if len(unique_labels) < 2:\n",
    "        print(\"Warning: All meta-labels are the same. Adding artificial diversity.\")\n",
    "        # Add some artificial diversity\n",
    "        if 1 in unique_labels:\n",
    "            X_meta = np.vstack([X_meta, X_meta[0]])\n",
    "            y_meta = np.append(y_meta, 0)\n",
    "        else:\n",
    "            X_meta = np.vstack([X_meta, X_meta[0]])\n",
    "            y_meta = np.append(y_meta, 1)\n",
    "    \n",
    "    meta_classifier = MultinomialNB()\n",
    "    meta_classifier.fit(X_meta, y_meta)\n",
    "    \n",
    "    return meta_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25d7ee2e-39af-4fb8-b84d-9f895f542063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Ensemble selection phase\n",
    "def ensemble_selection_phase(x_test, X_dsel, y_dsel, classifier_pool, meta_classifier, K=7, K_s=5, alpha=0.9, J=30):\n",
    "    \"\"\"\n",
    "    Implement the ensemble selection phase to select the best classifiers for test instance\n",
    "    \n",
    "    Returns:\n",
    "    - Selected classifier ensemble for the test instance\n",
    "    \"\"\"\n",
    "    # Convert y_dsel to numpy array if it's a pandas Series\n",
    "    if isinstance(y_dsel, pd.Series):\n",
    "        y_dsel_array = y_dsel.values\n",
    "    else:\n",
    "        y_dsel_array = y_dsel\n",
    "        \n",
    "    # Convert X_dsel to numpy array if it's a pandas DataFrame\n",
    "    if isinstance(X_dsel, pd.DataFrame):\n",
    "        X_dsel_array = X_dsel.values\n",
    "    else:\n",
    "        X_dsel_array = X_dsel\n",
    "    \n",
    "    # Step 1: Select competent classifiers\n",
    "    competent_classifiers = []\n",
    "    \n",
    "    # Find the competence region of the test instance\n",
    "    theta_t = find_k_nearest_neighbors(x_test, X_dsel_array, K)\n",
    "    \n",
    "    # Compute output profiles for all dynamic selection instances\n",
    "    output_profiles = np.array([\n",
    "        [clf.predict([X_dsel_array[i]])[0] for clf in classifier_pool]\n",
    "        for i in range(len(X_dsel_array))\n",
    "    ])\n",
    "    \n",
    "    # Compute output profile for test instance\n",
    "    test_output_profile = np.array([clf.predict([x_test])[0] for clf in classifier_pool])\n",
    "    \n",
    "    # Find similar output profiles\n",
    "    phi_t = find_similar_output_profiles(test_output_profile, output_profiles, K_s)\n",
    "    \n",
    "    # Calculate instance weights\n",
    "    theta_weights = calculate_instance_weights(theta_t, y_dsel_array, alpha)\n",
    "    phi_weights = calculate_instance_weights(phi_t, y_dsel_array, alpha)\n",
    "    \n",
    "    # Calculate competence of each classifier\n",
    "    competent_classifiers = []\n",
    "    classifier_competence = []\n",
    "    \n",
    "    for i, clf in enumerate(classifier_pool):\n",
    "        # Extract meta-features\n",
    "        meta_features = extract_meta_features(theta_t, phi_t, theta_weights, phi_weights, y_dsel_array, clf, X_dsel_array)\n",
    "        \n",
    "        # Get competence from meta-classifier\n",
    "        competence = meta_classifier.predict_proba(np.abs([meta_features]))[0][1]  # Probability of being competent\n",
    "        \n",
    "        # If competence is above 0.5, add to competent classifiers\n",
    "        if competence > 0.5:\n",
    "            competent_classifiers.append(i)\n",
    "            classifier_competence.append(competence)\n",
    "    \n",
    "    # If no competent classifiers found, use the top J most competent from the pool\n",
    "    if not competent_classifiers:\n",
    "        print(\"No competent classifiers found. Using top J most competent classifiers.\")\n",
    "        all_competences = []\n",
    "        for i, clf in enumerate(classifier_pool):\n",
    "            meta_features = extract_meta_features(theta_t, phi_t, theta_weights, phi_weights, y_dsel_array, clf, X_dsel_array)\n",
    "            competence = meta_classifier.predict_proba(np.abs([meta_features]))[0][1]\n",
    "            all_competences.append((i, competence))\n",
    "        \n",
    "        # Sort by competence\n",
    "        all_competences.sort(key=lambda x: x[1], reverse=True)\n",
    "        competent_classifiers = [idx for idx, _ in all_competences[:J]]\n",
    "    \n",
    "    # Step 2: Select the J most diverse classifiers\n",
    "    if len(competent_classifiers) <= J:\n",
    "        selected_classifiers = competent_classifiers\n",
    "    else:\n",
    "        # Compute pairwise diversity (DF measure) between competent classifiers\n",
    "        n_competent = len(competent_classifiers)\n",
    "        diversity_matrix = np.zeros((n_competent, n_competent))\n",
    "        \n",
    "        # For all pairs of competent classifiers\n",
    "        for i in range(n_competent):\n",
    "            for j in range(i+1, n_competent):\n",
    "                clf_i_idx = competent_classifiers[i]\n",
    "                clf_j_idx = competent_classifiers[j]\n",
    "                \n",
    "                clf_i = classifier_pool[clf_i_idx]\n",
    "                clf_j = classifier_pool[clf_j_idx]\n",
    "                \n",
    "                # Compute N00, N11, N01, N10 over dynamic selection set\n",
    "                y_pred_i = clf_i.predict(X_dsel_array)\n",
    "                y_pred_j = clf_j.predict(X_dsel_array)\n",
    "                \n",
    "                N00 = sum(1 for k in range(len(y_dsel_array)) if y_pred_i[k] != y_dsel_array[k] and y_pred_j[k] != y_dsel_array[k])\n",
    "                N11 = sum(1 for k in range(len(y_dsel_array)) if y_pred_i[k] == y_dsel_array[k] and y_pred_j[k] == y_dsel_array[k])\n",
    "                N01 = sum(1 for k in range(len(y_dsel_array)) if y_pred_i[k] != y_dsel_array[k] and y_pred_j[k] == y_dsel_array[k])\n",
    "                N10 = sum(1 for k in range(len(y_dsel_array)) if y_pred_i[k] == y_dsel_array[k] and y_pred_j[k] != y_dsel_array[k])\n",
    "                \n",
    "                # Calculate DF diversity\n",
    "                if (N11 + N01 + N10 + N00) > 0:  # Avoid division by zero\n",
    "                    df_ij = N00 / (N11 + N01 + N10 + N00)\n",
    "                else:\n",
    "                    df_ij = 0\n",
    "                \n",
    "                diversity_matrix[i, j] = df_ij\n",
    "                diversity_matrix[j, i] = df_ij\n",
    "        \n",
    "        # Calculate overall diversity of each classifier\n",
    "        overall_diversity = np.sum(diversity_matrix, axis=1)\n",
    "        \n",
    "        # Select the J classifiers with highest diversity\n",
    "        selected_indices = np.argsort(overall_diversity)[-J:]\n",
    "        selected_classifiers = [competent_classifiers[i] for i in selected_indices]\n",
    "    \n",
    "    # Return the selected ensemble\n",
    "    return [classifier_pool[i] for i in selected_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c25dd40-4cb0-4272-bba6-cdcbea4a79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Combination phase\n",
    "def predict_with_ensemble(selected_ensemble, x_test):\n",
    "    \"\"\"\n",
    "    Combine predictions of selected classifiers using majority voting\n",
    "    \n",
    "    Returns:\n",
    "    - Final prediction\n",
    "    \"\"\"\n",
    "    # Ensure x_test is a numpy array\n",
    "    if isinstance(x_test, pd.Series):\n",
    "        x_test_array = x_test.values\n",
    "    else:\n",
    "        x_test_array = x_test\n",
    "        \n",
    "    # Reshape x_test if needed\n",
    "    x_reshaped = x_test_array.reshape(1, -1) if len(x_test_array.shape) == 1 else x_test_array\n",
    "    \n",
    "    # Get predictions from each classifier\n",
    "    predictions = [clf.predict(x_reshaped)[0] for clf in selected_ensemble]\n",
    "    \n",
    "    # Apply majority voting\n",
    "    prediction_counts = Counter(predictions)\n",
    "    final_prediction = prediction_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad90127a-217b-42c1-bd23-9595635b965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_desknn_mi(X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test):\n",
    "    # Convert data to numpy arrays for processing\n",
    "    if isinstance(X_test, pd.DataFrame):\n",
    "        X_test_array = X_test.values\n",
    "    else:\n",
    "        X_test_array = X_test\n",
    "        \n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_test_array = y_test.values\n",
    "    else:\n",
    "        y_test_array = y_test\n",
    "\n",
    "    # Phase 1: Generation phase - Create classifier pool\n",
    "    print(\"Phase 1: Generation phase - Creating classifier pool...\")\n",
    "    classifier_pool = create_classifier_pool(X_train_balanced, y_train_balanced, M=20 if DEV_MODE else 100)\n",
    "    print(f\"Created pool of {len(classifier_pool)} classifiers\")\n",
    "    \n",
    "    # Phase 2: Meta-training phase - Train meta-classifier\n",
    "    print(\"Phase 2: Meta-training phase - Training meta-classifier...\")\n",
    "    meta_classifier = meta_training_phase(X_train, y_train, classifier_pool)\n",
    "    print(\"Meta-classifier trained\")\n",
    "    \n",
    "    # Phase 3 & 4: For each test instance, select ensemble and predict\n",
    "    print(\"Phase 3 & 4: Ensemble selection and combination for test set...\")\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    \n",
    "    # Use smaller test set in dev mode\n",
    "    test_instances = X_test_array[:100] if DEV_MODE and len(X_test_array) > 100 else X_test_array\n",
    "    \n",
    "    for i, x_test in enumerate(test_instances):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing test instance {i}/{len(test_instances)}\")\n",
    "        \n",
    "        # Phase 3: Ensemble selection phase\n",
    "        selected_ensemble = ensemble_selection_phase(\n",
    "            x_test, X_dsel, y_dsel, classifier_pool, meta_classifier\n",
    "        )\n",
    "        \n",
    "        # Phase 4: Combination phase - predict using majority voting\n",
    "        prediction = predict_with_ensemble(selected_ensemble, x_test)\n",
    "        y_pred.append(prediction)\n",
    "        \n",
    "        # Calculate probabilities for ROC AUC\n",
    "        probs = [clf.predict_proba([x_test])[0][1] for clf in selected_ensemble]\n",
    "        avg_prob = np.mean(probs)\n",
    "        y_prob.append(avg_prob)\n",
    "    \n",
    "    # Evaluate results\n",
    "    if len(y_pred) < len(y_test_array):\n",
    "        # We only processed a subset in dev mode, so use the same subset for evaluation\n",
    "        test_y = y_test_array[:len(y_pred)]\n",
    "    else:\n",
    "        test_y = y_test_array\n",
    "        \n",
    "    print(\"\\nResults:\")\n",
    "    print(classification_report(test_y, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(test_y, y_pred))\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(test_y, y_prob)\n",
    "    print(f\"\\nAUC: {auc:.4f}\")\n",
    "    \n",
    "    return y_pred, y_prob, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f88aa0fe-f44e-47ea-b496-7c23dc77a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Missing values before preprocessing: 0\n",
      "Class distribution:\n",
      "AgingRelatedBugs\n",
      "0    3380\n",
      "1      20\n",
      "Name: count, dtype: int64\n",
      "Imbalance ratio: 169.0\n",
      "\n",
      "Creating data splits...\n",
      "Original training set class distribution: Counter({0: 1132, 1: 7})\n",
      "Balanced training set class distribution: Counter({1: 1132, 0: 1132})\n",
      "\n",
      "Running META-DESKNN-MI algorithm...\n",
      "Phase 1: Generation phase - Creating classifier pool...\n",
      "Created pool of 20 classifiers\n",
      "Phase 2: Meta-training phase - Training meta-classifier...\n",
      "Meta-classifier trained\n",
      "Phase 3 & 4: Ensemble selection and combination for test set...\n",
      "Processing test instance 0/100\n",
      "\n",
      "Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        98\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.74      0.74      0.74       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97  1]\n",
      " [ 1  1]]\n",
      "\n",
      "AUC: 0.9745\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X, y, feature_names = load_and_preprocess_data()\n",
    "    \n",
    "    print(\"\\nCreating data splits...\")\n",
    "    X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test = create_data_splits(X, y)\n",
    "    \n",
    "    print(\"\\nRunning META-DESKNN-MI algorithm...\")\n",
    "    y_pred, y_prob, auc = meta_desknn_mi(X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test)\n",
    "    \n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d094c61-1d7c-490a-9bf4-3903a9a461d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b8c35-0c71-4cc8-8eb2-f8a27fcea54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba7c62-8168-46a6-82a2-90ca6eda63bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f8f67-e3b9-4b85-8595-d4ee0a37b0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
