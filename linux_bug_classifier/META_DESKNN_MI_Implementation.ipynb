{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "627b905a-d15b-4834-bc0a-bf0423d64806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8eb5bed0-9db7-4d14-94f9-a7991f45ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24d4d0d9-f076-4233-af69-9b82c6773b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before preprocessing: 0\n",
      "Class distribution:\n",
      "AgingRelatedBugs\n",
      "0    3380\n",
      "1      20\n",
      "Name: count, dtype: int64\n",
      "Imbalance ratio: 169.0\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data():\n",
    "    # Load the datasets\n",
    "    df_net = pd.read_csv('data/linux_driver_net.csv')\n",
    "    df_scsi = pd.read_csv('data/linux_driver_scsi.csv')\n",
    "    df_ext3 = pd.read_csv('data/linux_ext3.csv')\n",
    "    df_ipv4 = pd.read_csv('data/linux_ipv4.csv')\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([df_net, df_scsi, df_ext3, df_ipv4], ignore_index=True)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"Missing values before preprocessing: {combined_df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Fill missing values - use mean for numeric columns\n",
    "    numeric_columns = combined_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    combined_df[numeric_columns] = combined_df[numeric_columns].fillna(combined_df[numeric_columns].mean())\n",
    "    \n",
    "    # Drop any rows that still have missing values\n",
    "    combined_df = combined_df.dropna()\n",
    "    \n",
    "    # Check for imbalance in the target variable\n",
    "    print(\"Class distribution:\")\n",
    "    print(combined_df['AgingRelatedBugs'].value_counts())\n",
    "    print(f\"Imbalance ratio: {combined_df['AgingRelatedBugs'].value_counts()[0] / combined_df['AgingRelatedBugs'].value_counts()[1]}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = combined_df.drop(['Filename', 'AgingRelatedBugs'], axis=1)\n",
    "    y = combined_df['AgingRelatedBugs']\n",
    "    \n",
    "    # Feature scaling \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y, X.columns\n",
    "\n",
    "# Load the data\n",
    "X, y, feature_names = load_and_preprocess_data()\n",
    "\n",
    "if DEV_MODE:\n",
    "    def sample_dataset(X, y, sample_size=0.3):\n",
    "        \"\"\"Sample dataset while ensuring minimum number of minority samples\"\"\"\n",
    "        # Get indices of minority and majority classes\n",
    "        minority_indices = np.where(y == 1)[0]\n",
    "        majority_indices = np.where(y == 0)[0]\n",
    "        \n",
    "        # Keep all minority samples to ensure SMOTE works\n",
    "        selected_minority = minority_indices\n",
    "        \n",
    "        # Sample from majority class to reach desired ratio\n",
    "        majority_sample_size = int(sample_size * len(X)) - len(minority_indices)\n",
    "        majority_sample_size = max(0, majority_sample_size)  # Ensure non-negative\n",
    "        \n",
    "        selected_majority = np.random.choice(\n",
    "            majority_indices, \n",
    "            size=min(majority_sample_size, len(majority_indices)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # Combine indices\n",
    "        selected_indices = np.concatenate([selected_minority, selected_majority])\n",
    "        \n",
    "        if isinstance(X, np.ndarray):\n",
    "            return X[selected_indices], y[selected_indices]\n",
    "        else:\n",
    "            return X.iloc[selected_indices], y.iloc[selected_indices]\n",
    "    \n",
    "    # Use the function\n",
    "    X, y = sample_dataset(X, y, sample_size=0.3)\n",
    "    print(f\"Using sampled dataset: {len(X)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e858e44-ee40-426c-bc42-2f5548f87625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_splits(X, y):\n",
    "    # Split data into training, dynamic selection, and testing sets (33%, 33%, 33%)\n",
    "    X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "    X_train, X_dsel, y_train, y_dsel = train_test_split(X_train_temp, y_train_temp, test_size=0.5, random_state=42, stratify=y_train_temp)\n",
    "    \n",
    "    # Count minority samples\n",
    "    minority_count = Counter(y_train)[1]\n",
    "    \n",
    "    # Choose appropriate k for SMOTE based on minority class size\n",
    "    k_neighbors = min(5, minority_count - 1)  # Must be less than minority count\n",
    "    \n",
    "    if k_neighbors > 0:\n",
    "        # Apply SMOTE with adjusted k_neighbors\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        # If too few samples, duplicate existing minority samples\n",
    "        print(\"Too few minority samples for SMOTE. Using simple duplication.\")\n",
    "        X_train_balanced, y_train_balanced = simple_duplicate_balance(X_train, y_train)\n",
    "    \n",
    "    print(\"Original training set class distribution:\", Counter(y_train))\n",
    "    print(\"Balanced training set class distribution:\", Counter(y_train_balanced))\n",
    "    \n",
    "    return X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test\n",
    "\n",
    "def simple_duplicate_balance(X, y):\n",
    "    \"\"\"Balance dataset by duplicating minority class samples\"\"\"\n",
    "    majority_count = Counter(y).most_common(1)[0][1]\n",
    "    minority_class = Counter(y).most_common()[-1][0]\n",
    "    \n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "    \n",
    "    # How many times to duplicate each minority sample\n",
    "    n_duplicates = int(np.ceil(majority_count / len(minority_indices)))\n",
    "    \n",
    "    # Create duplicated minority samples\n",
    "    X_minority = X[minority_indices]\n",
    "    y_minority = y[minority_indices]\n",
    "    \n",
    "    X_minority_dup = np.repeat(X_minority, n_duplicates, axis=0)[:majority_count]\n",
    "    y_minority_dup = np.repeat(y_minority, n_duplicates)[:majority_count]\n",
    "    \n",
    "    # Combine with majority class\n",
    "    majority_indices = np.where(y != minority_class)[0]\n",
    "    X_balanced = np.vstack([X[majority_indices], X_minority_dup])\n",
    "    y_balanced = np.concatenate([y[majority_indices], y_minority_dup])\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f565e26-8098-4076-a9ff-ee6c4e4f0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a candidate classifier pool (Phase 1: generation phase)\n",
    "def create_classifier_pool(X_train_balanced, y_train_balanced, M=100):\n",
    "    \"\"\"\n",
    "    Creates a pool of M diverse classifiers using RandomForestClassifier with different parameters\n",
    "    \"\"\"\n",
    "    classifier_pool = []\n",
    "    \n",
    "    # Create base classifiers with different parameters for diversity\n",
    "    for i in range(M):\n",
    "        # Vary parameters for diversity\n",
    "        max_depth = np.random.randint(3, 20) if i % 3 == 0 else None\n",
    "        min_samples_split = np.random.randint(2, 20) if i % 5 == 0 else 2\n",
    "        min_samples_leaf = np.random.randint(1, 10) if i % 7 == 0 else 1\n",
    "        \n",
    "        # Create a random forest classifier with varied parameters\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=20 if DEV_MODE else 100,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            bootstrap=bool(i % 2),\n",
    "            random_state=i\n",
    "        )\n",
    "        \n",
    "        # Train the classifier on the balanced training set\n",
    "        clf.fit(X_train_balanced, y_train_balanced)\n",
    "        classifier_pool.append(clf)\n",
    "    \n",
    "    return classifier_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "869cfaac-9f60-4117-abd8-8d6c96fe3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the find_k_nearest_neighbors function\n",
    "def find_k_nearest_neighbors(x, dataset, K):\n",
    "    \"\"\"Find K nearest neighbors using scikit-learn's optimized implementation\"\"\"\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        dataset_array = dataset.values\n",
    "    else:\n",
    "        dataset_array = dataset\n",
    "        \n",
    "    # Reshape x if needed\n",
    "    x_reshaped = x.reshape(1, -1) if len(x.shape) == 1 else x\n",
    "    \n",
    "    # Use scikit-learn's optimized KNN implementation\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(K, len(dataset_array)), algorithm='ball_tree').fit(dataset_array)\n",
    "    _, indices = nbrs.kneighbors(x_reshaped)\n",
    "    \n",
    "    return indices[0]  # Return as a simple array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "311da2a5-90ae-49cd-9b1b-ff628fe77ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate output profile\n",
    "def compute_output_profile(x, classifier_pool):\n",
    "    \"\"\"Compute the output profile of an instance based on the classifier pool predictions\"\"\"\n",
    "    return np.array([clf.predict([x])[0] for clf in classifier_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f9510a2-0f61-413e-82be-f44bf5402368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar output profiles\n",
    "def find_similar_output_profiles(x_profile, output_profiles, K_s):\n",
    "    \"\"\"Find indices of K_s most similar output profiles using Hamming distance\"\"\"\n",
    "    hamming_distances = np.sum(output_profiles != x_profile, axis=1)\n",
    "    return np.argsort(hamming_distances)[:K_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57c99b5a-db1e-409e-8d32-950811bef483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_instance_weights(region_indices, y, alpha=0.9):\n",
    "    \"\"\"Calculate weights for instances in competence regions\"\"\"\n",
    "    weights = []\n",
    "    \n",
    "    # Convert y to numpy array if it's a pandas Series\n",
    "    if isinstance(y, pd.Series):\n",
    "        y_array = y.values\n",
    "    else:\n",
    "        y_array = y\n",
    "    \n",
    "    # Ensure region_indices are within bounds\n",
    "    valid_indices = [idx for idx in region_indices if idx < len(y_array)]\n",
    "    \n",
    "    for idx in valid_indices:\n",
    "        current_class = y_array[idx]\n",
    "        # Count instances with the same class\n",
    "        num_same_class = sum(1 for i in valid_indices if y_array[i] == current_class)\n",
    "        # Calculate weight\n",
    "        weight = 1 / (1.0 + np.exp(alpha * num_same_class))\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Handle empty weights list\n",
    "    if not weights:\n",
    "        return np.array([1.0])\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    \n",
    "    return normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac0074db-93f6-451b-93a2-d086fef6b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta_features(theta_j, phi_j, theta_weights, phi_weights, y, classifier, X):\n",
    "    \"\"\"\n",
    "    Extract meta-features for a classifier based on competence regions\n",
    "    \n",
    "    Returns:\n",
    "    - Feature vector containing 7 sets of meta-features described in the META-DESKNN-MI algorithm\n",
    "    \"\"\"\n",
    "    meta_features = []\n",
    "    \n",
    "    # Convert to numpy arrays for consistent indexing\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_array = X.values\n",
    "    else:\n",
    "        X_array = X\n",
    "        \n",
    "    if isinstance(y, pd.Series):\n",
    "        y_array = y.values\n",
    "    else:\n",
    "        y_array = y\n",
    "    \n",
    "    # Ensure indices are within bounds\n",
    "    max_idx = len(X_array) - 1\n",
    "    valid_theta_j = [idx for idx in theta_j if idx <= max_idx]\n",
    "    valid_phi_j = [idx for idx in phi_j if idx <= max_idx]\n",
    "    \n",
    "    # Handle empty regions\n",
    "    if not valid_theta_j:\n",
    "        valid_theta_j = [0]  # Use first sample as fallback\n",
    "    \n",
    "    if not valid_phi_j:\n",
    "        valid_phi_j = [0]  # Use first sample as fallback\n",
    "    \n",
    "    # f1: Neighbors' hard classification (based on feature space)\n",
    "    f1 = []\n",
    "    for idx in valid_theta_j:\n",
    "        # If classifier correctly predicts the instance\n",
    "        if classifier.predict([X_array[idx]])[0] == y_array[idx]:\n",
    "            f1.append(1)\n",
    "        else:\n",
    "            f1.append(0)\n",
    "    \n",
    "    # f2: Posterior probability\n",
    "    f2 = []\n",
    "    for idx in valid_theta_j:\n",
    "        # Get probability of the true class\n",
    "        probs = classifier.predict_proba([X_array[idx]])[0]\n",
    "        true_class_idx = int(y_array[idx])\n",
    "        if true_class_idx < len(probs):  # Ensure index is valid\n",
    "            f2.append(probs[true_class_idx])\n",
    "        else:\n",
    "            f2.append(0.0)\n",
    "    \n",
    "    # f3: Overall local accuracy (feature space)\n",
    "    f3 = [np.mean(f1)]\n",
    "    \n",
    "    # f4: Output profiles classification (decision space)\n",
    "    f4 = []\n",
    "    for idx in valid_phi_j:\n",
    "        # If classifier correctly predicts the instance\n",
    "        if classifier.predict([X_array[idx]])[0] == y_array[idx]:\n",
    "            f4.append(1)\n",
    "        else:\n",
    "            f4.append(0)\n",
    "    \n",
    "    # f5: Classifier's confidence (implementation simplified)\n",
    "    # Using mean probability as a confidence measure\n",
    "    instance_idx = valid_theta_j[0]  # Using the first neighbor to compute confidence\n",
    "    probs = classifier.predict_proba([X_array[instance_idx]])[0]\n",
    "    f5 = [np.max(probs)]\n",
    "    \n",
    "    # Adjust weights if necessary to match valid indices\n",
    "    if len(theta_weights) > len(valid_theta_j):\n",
    "        theta_weights = theta_weights[:len(valid_theta_j)]\n",
    "    elif len(theta_weights) < len(valid_theta_j):\n",
    "        # Extend weights with zeros\n",
    "        theta_weights = np.pad(theta_weights, (0, len(valid_theta_j) - len(theta_weights)))\n",
    "    \n",
    "    if len(phi_weights) > len(valid_phi_j):\n",
    "        phi_weights = phi_weights[:len(valid_phi_j)]\n",
    "    elif len(phi_weights) < len(valid_phi_j):\n",
    "        # Extend weights with zeros\n",
    "        phi_weights = np.pad(phi_weights, (0, len(valid_phi_j) - len(phi_weights)))\n",
    "    \n",
    "    # f6: Overall weighted accuracy based on feature space\n",
    "    f6 = [np.sum(np.array(f1) * theta_weights)]\n",
    "    \n",
    "    # f7: Overall weighted accuracy based on decision space\n",
    "    f7 = [np.sum(np.array(f4) * phi_weights)]\n",
    "    \n",
    "    # Combine all meta-features\n",
    "    meta_features = f1 + f2 + f3 + f4 + f5 + f6 + f7\n",
    "    \n",
    "    return np.array(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c614b7b-72f4-43da-8bb7-dc0543249029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_training_phase(X_train, y_train, classifier_pool, K=7 if not DEV_MODE else 5, K_s=5 if not DEV_MODE else 3, alpha=0.9, h_c=0.8):\n",
    "    \"\"\"\n",
    "    Implement the meta-training phase to train the meta-classifier\n",
    "    \n",
    "    Returns:\n",
    "    - Trained meta-classifier lambda\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for consistent indexing\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_array = X_train.values\n",
    "    else:\n",
    "        X_array = X_train.copy()\n",
    "        \n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_array = y_train.values\n",
    "    else:\n",
    "        y_array = y_train.copy()\n",
    "    \n",
    "    # Step 1: Sample selection process\n",
    "    selected_indices = []\n",
    "    y_pred_per_classifier = np.zeros((len(X_array), len(classifier_pool)))\n",
    "    \n",
    "    # Calculate predictions for all samples and classifiers\n",
    "    for i, clf in enumerate(classifier_pool):\n",
    "        y_pred_per_classifier[:, i] = clf.predict(X_array)\n",
    "    \n",
    "    # Check consensus of the pool for each instance\n",
    "    for j in range(len(X_array)):\n",
    "        # Count votes for each class\n",
    "        votes = Counter(y_pred_per_classifier[j, :])\n",
    "        \n",
    "        # If there are at least two classes with votes\n",
    "        if len(votes) >= 2:\n",
    "            sorted_votes = votes.most_common()\n",
    "            # Calculate difference between votes of winning class and second class\n",
    "            consensus = sorted_votes[0][1] - sorted_votes[1][1]\n",
    "            consensus_ratio = consensus / len(classifier_pool)\n",
    "            \n",
    "            # If consensus is low, select the instance\n",
    "            if consensus_ratio < h_c:\n",
    "                selected_indices.append(j)\n",
    "    \n",
    "    # If no instances selected or too few, use a minimum number\n",
    "    min_required = max(K, K_s + 1)\n",
    "    if len(selected_indices) < min_required:\n",
    "        print(f\"Too few instances selected ({len(selected_indices)}). Using top {min_required} instances.\")\n",
    "        # Ensure we don't select more indices than available\n",
    "        max_possible = min(min_required, len(X_array))\n",
    "        selected_indices = list(range(max_possible))\n",
    "    \n",
    "    # Also ensure that any index used later is valid\n",
    "    selected_indices = [idx for idx in selected_indices if idx < len(X_array)]\n",
    "    \n",
    "    # Step 2: Meta-feature extraction\n",
    "    X_meta = []  # Meta-features\n",
    "    y_meta = []  # Meta-labels\n",
    "    \n",
    "    # Compute output profiles for all instances\n",
    "    output_profiles = np.array([\n",
    "        [clf.predict([X_array[i]])[0] for clf in classifier_pool]\n",
    "        for i in range(len(X_array))\n",
    "    ])\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        # Find the competence region (K nearest neighbors)\n",
    "        theta_j = find_k_nearest_neighbors(X_array[idx], X_array, K)\n",
    "        \n",
    "        # Compute the output profile\n",
    "        output_profile = output_profiles[idx]\n",
    "        \n",
    "        # Find similar output profiles\n",
    "        phi_j = find_similar_output_profiles(output_profile, output_profiles, K_s)\n",
    "        \n",
    "        # Ensure all indices are valid\n",
    "        theta_j = [i for i in theta_j if i < len(X_array)]\n",
    "        phi_j = [i for i in phi_j if i < len(X_array)]\n",
    "        \n",
    "        # If we lost all indices, use some fallback\n",
    "        if not theta_j:\n",
    "            theta_j = [0]  # Use first sample as fallback\n",
    "        \n",
    "        if not phi_j:\n",
    "            phi_j = [0]  # Use first sample as fallback\n",
    "        \n",
    "        # Calculate instance weights\n",
    "        theta_weights = calculate_instance_weights(theta_j, y_array, alpha)\n",
    "        phi_weights = calculate_instance_weights(phi_j, y_array, alpha)\n",
    "        \n",
    "        # For each classifier, extract meta-features\n",
    "        for i, clf in enumerate(classifier_pool):\n",
    "            # Extract meta-features\n",
    "            meta_features = extract_meta_features(theta_j, phi_j, theta_weights, phi_weights, y_array, clf, X_array)\n",
    "            \n",
    "            # Determine if the classifier correctly classifies the instance\n",
    "            is_correct = int(clf.predict([X_array[idx]])[0] == y_array[idx])\n",
    "            \n",
    "            # Add meta-features and meta-label\n",
    "            X_meta.append(meta_features)\n",
    "            y_meta.append(is_correct)\n",
    "    \n",
    "    # Step 3: Train meta-classifier\n",
    "    # Ensure we have non-negative values for MultinomialNB\n",
    "    X_meta = np.abs(X_meta)\n",
    "    \n",
    "    # Handle case where all meta-labels are the same\n",
    "    unique_labels = np.unique(y_meta)\n",
    "    if len(unique_labels) < 2:\n",
    "        print(\"Warning: All meta-labels are the same. Adding artificial diversity.\")\n",
    "        # Add some artificial diversity\n",
    "        if 1 in unique_labels:\n",
    "            X_meta = np.vstack([X_meta, X_meta[0]])\n",
    "            y_meta = np.append(y_meta, 0)\n",
    "        else:\n",
    "            X_meta = np.vstack([X_meta, X_meta[0]])\n",
    "            y_meta = np.append(y_meta, 1)\n",
    "    \n",
    "    meta_classifier = MultinomialNB()\n",
    "    meta_classifier.fit(X_meta, y_meta)\n",
    "    \n",
    "    return meta_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec4d7aa3-de9e-47dc-ac5c-2149b604db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Ensemble selection phase\n",
    "def ensemble_selection_phase(x_test, X_dsel, y_dsel, classifier_pool, meta_classifier, K=7 if not DEV_MODE else 5, K_s=5 if not DEV_MODE else 3, alpha=0.9, J=30 if not DEV_MODE else 20):\n",
    "    \"\"\"\n",
    "    Implement the ensemble selection phase to select the best classifiers for test instance\n",
    "    \n",
    "    Returns:\n",
    "    - Selected classifier ensemble for the test instance\n",
    "    \"\"\"\n",
    "    if DEV_MODE:\n",
    "        # Just select the first J classifiers\n",
    "        J_actual = min(J, len(classifier_pool))\n",
    "        return classifier_pool[:J_actual]\n",
    "    \n",
    "    # Step 1: Select competent classifiers\n",
    "    competent_classifiers = []\n",
    "    \n",
    "    # Find the competence region of the test instance\n",
    "    theta_t = find_k_nearest_neighbors(x_test, X_dsel, K)\n",
    "    \n",
    "    # Compute output profiles for all dynamic selection instances\n",
    "    output_profiles = np.array([\n",
    "        [clf.predict([X_dsel[i]])[0] for clf in classifier_pool]\n",
    "        for i in range(len(X_dsel))\n",
    "    ])\n",
    "    \n",
    "    # Compute output profile for test instance\n",
    "    test_output_profile = np.array([clf.predict([x_test])[0] for clf in classifier_pool])\n",
    "    \n",
    "    # Find similar output profiles\n",
    "    phi_t = find_similar_output_profiles(test_output_profile, output_profiles, K_s)\n",
    "    \n",
    "    # Calculate instance weights\n",
    "    theta_weights = calculate_instance_weights(theta_t, y_dsel, alpha)\n",
    "    phi_weights = calculate_instance_weights(phi_t, y_dsel, alpha)\n",
    "    \n",
    "    # Calculate competence of each classifier\n",
    "    competent_classifiers = []\n",
    "    classifier_competence = []\n",
    "    \n",
    "    for i, clf in enumerate(classifier_pool):\n",
    "        # Extract meta-features\n",
    "        meta_features = extract_meta_features(theta_t, phi_t, theta_weights, phi_weights, y_dsel, clf, X_dsel)\n",
    "        \n",
    "        # Get competence from meta-classifier\n",
    "        competence = meta_classifier.predict_proba(np.abs([meta_features]))[0][1]  # Probability of being competent\n",
    "        \n",
    "        # If competence is above 0.5, add to competent classifiers\n",
    "        if competence > 0.5:\n",
    "            competent_classifiers.append(i)\n",
    "            classifier_competence.append(competence)\n",
    "    \n",
    "    # If no competent classifiers found, use the top J most competent from the pool\n",
    "    if not competent_classifiers:\n",
    "        print(\"No competent classifiers found. Using top J most competent classifiers.\")\n",
    "        all_competences = []\n",
    "        for i, clf in enumerate(classifier_pool):\n",
    "            meta_features = extract_meta_features(theta_t, phi_t, theta_weights, phi_weights, y_dsel, clf, X_dsel)\n",
    "            competence = meta_classifier.predict_proba(np.abs([meta_features]))[0][1]\n",
    "            all_competences.append((i, competence))\n",
    "        \n",
    "        # Sort by competence\n",
    "        all_competences.sort(key=lambda x: x[1], reverse=True)\n",
    "        competent_classifiers = [idx for idx, _ in all_competences[:J]]\n",
    "        \n",
    "    # Step 2: Select the J most diverse classifiers\n",
    "    if len(competent_classifiers) <= J:\n",
    "        selected_classifiers = competent_classifiers\n",
    "    else:\n",
    "        # Compute pairwise diversity (DF measure) between competent classifiers\n",
    "        n_competent = len(competent_classifiers)\n",
    "        diversity_matrix = np.zeros((n_competent, n_competent))\n",
    "        \n",
    "        # For all pairs of competent classifiers\n",
    "        for i in range(n_competent):\n",
    "            for j in range(i+1, n_competent):\n",
    "                clf_i_idx = competent_classifiers[i]\n",
    "                clf_j_idx = competent_classifiers[j]\n",
    "                \n",
    "                clf_i = classifier_pool[clf_i_idx]\n",
    "                clf_j = classifier_pool[clf_j_idx]\n",
    "                \n",
    "                # Compute N00, N11, N01, N10 over dynamic selection set\n",
    "                y_pred_i = clf_i.predict(X_dsel)\n",
    "                y_pred_j = clf_j.predict(X_dsel)\n",
    "                \n",
    "                N00 = sum(1 for k in range(len(y_dsel)) if y_pred_i[k] != y_dsel[k] and y_pred_j[k] != y_dsel[k])\n",
    "                N11 = sum(1 for k in range(len(y_dsel)) if y_pred_i[k] == y_dsel[k] and y_pred_j[k] == y_dsel[k])\n",
    "                N01 = sum(1 for k in range(len(y_dsel)) if y_pred_i[k] != y_dsel[k] and y_pred_j[k] == y_dsel[k])\n",
    "                N10 = sum(1 for k in range(len(y_dsel)) if y_pred_i[k] == y_dsel[k] and y_pred_j[k] != y_dsel[k])\n",
    "                \n",
    "                # Calculate DF diversity\n",
    "                if (N11 + N01 + N10 + N00) > 0:  # Avoid division by zero\n",
    "                    df_ij = N00 / (N11 + N01 + N10 + N00)\n",
    "                else:\n",
    "                    df_ij = 0\n",
    "                \n",
    "                diversity_matrix[i, j] = df_ij\n",
    "                diversity_matrix[j, i] = df_ij\n",
    "        \n",
    "        # Calculate overall diversity of each classifier\n",
    "        overall_diversity = np.sum(diversity_matrix, axis=1)\n",
    "        \n",
    "        # Select the J classifiers with highest diversity\n",
    "        selected_indices = np.argsort(overall_diversity)[-J:]\n",
    "        selected_classifiers = [competent_classifiers[i] for i in selected_indices]\n",
    "    \n",
    "    # Return the selected ensemble\n",
    "    return [classifier_pool[i] for i in selected_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bd90457-3628-4d4a-a666-af3e02a94ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Combination phase\n",
    "def predict_with_ensemble(selected_ensemble, x_test):\n",
    "    \"\"\"\n",
    "    Combine predictions of selected classifiers using majority voting\n",
    "    \n",
    "    Returns:\n",
    "    - Final prediction\n",
    "    \"\"\"\n",
    "    predictions = [clf.predict([x_test])[0] for clf in selected_ensemble]\n",
    "    \n",
    "    # Apply majority voting\n",
    "    prediction_counts = Counter(predictions)\n",
    "    final_prediction = prediction_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f67002c6-e364-4cd5-a282-9e067a65a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_desknn_mi(X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test):\n",
    "    # Phase 1: Generation phase - Create classifier pool\n",
    "    print(\"Phase 1: Generation phase - Creating classifier pool...\")\n",
    "    classifier_pool = create_classifier_pool(X_train_balanced, y_train_balanced, M=20 if DEV_MODE else 100)\n",
    "    print(f\"Created pool of {len(classifier_pool)} classifiers\")\n",
    "    \n",
    "    # Phase 2: Meta-training phase - Train meta-classifier\n",
    "    print(\"Phase 2: Meta-training phase - Training meta-classifier...\")\n",
    "    meta_classifier = meta_training_phase(X_train, y_train, classifier_pool)\n",
    "    print(\"Meta-classifier trained\")\n",
    "    \n",
    "    # Phase 3 & 4: For each test instance, select ensemble and predict\n",
    "    print(\"Phase 3 & 4: Ensemble selection and combination for test set...\")\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    \n",
    "    # Use smaller test set in dev mode\n",
    "    test_instances = X_test[:100] if DEV_MODE and len(X_test) > 100 else X_test\n",
    "    \n",
    "    for i, x_test in enumerate(test_instances):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing test instance {i}/{len(test_instances)}\")\n",
    "        \n",
    "        # Phase 3: Ensemble selection phase\n",
    "        selected_ensemble = ensemble_selection_phase(\n",
    "            x_test, X_dsel, y_dsel, classifier_pool, meta_classifier\n",
    "        )\n",
    "        \n",
    "        # Phase 4: Combination phase - predict using majority voting\n",
    "        prediction = predict_with_ensemble(selected_ensemble, x_test)\n",
    "        y_pred.append(prediction)\n",
    "        \n",
    "        # Calculate probabilities for ROC AUC\n",
    "        probs = [clf.predict_proba([x_test])[0][1] for clf in selected_ensemble]\n",
    "        avg_prob = np.mean(probs)\n",
    "        y_prob.append(avg_prob)\n",
    "    \n",
    "    # Evaluate results\n",
    "    if len(y_pred) < len(y_test):\n",
    "        # We only processed a subset in dev mode, so use the same subset for evaluation\n",
    "        test_y = y_test[:len(y_pred)]\n",
    "    else:\n",
    "        test_y = y_test\n",
    "        \n",
    "    print(\"\\nResults:\")\n",
    "    print(classification_report(test_y, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(test_y, y_pred))\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(test_y, y_prob)\n",
    "    print(f\"\\nAUC: {auc:.4f}\")\n",
    "    \n",
    "    return y_pred, y_prob, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "56b4bf9b-ba61-40c7-9ccc-0d217f638281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Missing values before preprocessing: 0\n",
      "Class distribution:\n",
      "AgingRelatedBugs\n",
      "0    3380\n",
      "1      20\n",
      "Name: count, dtype: int64\n",
      "Imbalance ratio: 169.0\n",
      "\n",
      "Creating data splits...\n",
      "Original training set class distribution: Counter({0: 1132, 1: 7})\n",
      "Balanced training set class distribution: Counter({1: 1132, 0: 1132})\n",
      "\n",
      "Running META-DESKNN-MI algorithm...\n",
      "Phase 1: Generation phase - Creating classifier pool...\n",
      "Created pool of 100 classifiers\n",
      "Phase 2: Meta-training phase - Training meta-classifier...\n",
      "Meta-classifier trained\n",
      "Phase 3 & 4: Ensemble selection and combination for test set...\n",
      "Processing test instance 0/1122\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/HiveBugExtractionARBs/linux_bug_classifier/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test \u001b[38;5;241m=\u001b[39m create_data_splits(X, y)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning META-DESKNN-MI algorithm...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m y_pred, y_prob, auc \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_desknn_mi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_dsel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dsel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[80], line 25\u001b[0m, in \u001b[0;36mmeta_desknn_mi\u001b[0;34m(X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing test instance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_instances)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Phase 3: Ensemble selection phase\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m selected_ensemble \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_selection_phase\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_dsel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dsel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_classifier\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Phase 4: Combination phase - predict using majority voting\u001b[39;00m\n\u001b[1;32m     30\u001b[0m prediction \u001b[38;5;241m=\u001b[39m predict_with_ensemble(selected_ensemble, x_test)\n",
      "Cell \u001b[0;32mIn[78], line 86\u001b[0m, in \u001b[0;36mensemble_selection_phase\u001b[0;34m(x_test, X_dsel, y_dsel, classifier_pool, meta_classifier, K, K_s, alpha, J)\u001b[0m\n\u001b[1;32m     83\u001b[0m y_pred_i \u001b[38;5;241m=\u001b[39m clf_i\u001b[38;5;241m.\u001b[39mpredict(X_dsel)\n\u001b[1;32m     84\u001b[0m y_pred_j \u001b[38;5;241m=\u001b[39m clf_j\u001b[38;5;241m.\u001b[39mpredict(X_dsel)\n\u001b[0;32m---> 86\u001b[0m N00 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_dsel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_pred_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_dsel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_pred_j\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_dsel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m N11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_dsel)) \u001b[38;5;28;01mif\u001b[39;00m y_pred_i[k] \u001b[38;5;241m==\u001b[39m y_dsel[k] \u001b[38;5;129;01mand\u001b[39;00m y_pred_j[k] \u001b[38;5;241m==\u001b[39m y_dsel[k])\n\u001b[1;32m     88\u001b[0m N01 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_dsel)) \u001b[38;5;28;01mif\u001b[39;00m y_pred_i[k] \u001b[38;5;241m!=\u001b[39m y_dsel[k] \u001b[38;5;129;01mand\u001b[39;00m y_pred_j[k] \u001b[38;5;241m==\u001b[39m y_dsel[k])\n",
      "Cell \u001b[0;32mIn[78], line 86\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m y_pred_i \u001b[38;5;241m=\u001b[39m clf_i\u001b[38;5;241m.\u001b[39mpredict(X_dsel)\n\u001b[1;32m     84\u001b[0m y_pred_j \u001b[38;5;241m=\u001b[39m clf_j\u001b[38;5;241m.\u001b[39mpredict(X_dsel)\n\u001b[0;32m---> 86\u001b[0m N00 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_dsel)) \u001b[38;5;28;01mif\u001b[39;00m y_pred_i[k] \u001b[38;5;241m!=\u001b[39m \u001b[43my_dsel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m y_pred_j[k] \u001b[38;5;241m!=\u001b[39m y_dsel[k])\n\u001b[1;32m     87\u001b[0m N11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_dsel)) \u001b[38;5;28;01mif\u001b[39;00m y_pred_i[k] \u001b[38;5;241m==\u001b[39m y_dsel[k] \u001b[38;5;129;01mand\u001b[39;00m y_pred_j[k] \u001b[38;5;241m==\u001b[39m y_dsel[k])\n\u001b[1;32m     88\u001b[0m N01 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_dsel)) \u001b[38;5;28;01mif\u001b[39;00m y_pred_i[k] \u001b[38;5;241m!=\u001b[39m y_dsel[k] \u001b[38;5;129;01mand\u001b[39;00m y_pred_j[k] \u001b[38;5;241m==\u001b[39m y_dsel[k])\n",
      "File \u001b[0;32m~/repos/HiveBugExtractionARBs/linux_bug_classifier/venv/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/repos/HiveBugExtractionARBs/linux_bug_classifier/venv/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/repos/HiveBugExtractionARBs/linux_bug_classifier/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X, y, feature_names = load_and_preprocess_data()\n",
    "    \n",
    "    print(\"\\nCreating data splits...\")\n",
    "    X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test = create_data_splits(X, y)\n",
    "    \n",
    "    print(\"\\nRunning META-DESKNN-MI algorithm...\")\n",
    "    y_pred, y_prob, auc = meta_desknn_mi(X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test)\n",
    "    \n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa6fc0-757c-416c-a133-2e2d824e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the META-DESKNN-MI model\n",
    "print(\"Running META-DESKNN-MI algorithm...\")\n",
    "y_pred, y_prob, auc = meta_desknn_mi(X_train, y_train, X_train_balanced, y_train_balanced, X_dsel, y_dsel, X_test, y_test)\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# In dev mode, we need to use only the subset of test data that was processed\n",
    "if DEV_MODE and len(y_pred) < len(y_test):\n",
    "    # Use the same subset for visualization that was used for prediction\n",
    "    test_y = y_test[:len(y_pred)]\n",
    "else:\n",
    "    test_y = y_test\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(2, 2, 1)\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(test_y, y_prob)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(2, 2, 2)\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal AUC: {auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff088170-f185-4a84-ad51-c6918ee3c943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b96099-0dad-498a-8af8-e9f54af52ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7922d2-c29d-4081-8dff-fb9b1d57de2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
