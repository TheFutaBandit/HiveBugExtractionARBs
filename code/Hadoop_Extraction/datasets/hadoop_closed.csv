Bug_ID,Bug_Summary,Bug_Description
HADOOP-15896,Refine Kerberos based AuthenticationHandler to check proxyuser ACL,"JWTRedirectAuthenticationHandler is based on KerberosAuthenticationHandler, and authentication method in KerberosAuthenticationHandler basically do this:

 {code}
String clientPrincipal = gssContext.getSrcName().toString();
        KerberosName kerberosName = new KerberosName(clientPrincipal);
        String userName = kerberosName.getShortName();
        token = new AuthenticationToken(userName, clientPrincipal, getType());
        response.setStatus(HttpServletResponse.SC_OK);
        LOG.trace(""SPNEGO completed for client principal [{}]"",
            clientPrincipal);
{code}

It obtains the short name of the client principal and respond OK.  This is fine for verifying end user.  However, in proxy user case (knox), this authentication is insufficient because knox principal name is: knox/host1.example.com@EXAMPLE.COM . KerberosAuthenticationHandler will gladly confirm that knox is knox.  Even if the knox/host1.example.com@EXAMPLE.COM is used from botnet.rogueresearchlab.tld host.  KerberosAuthenticationHandler may not need to change, if it does not have plan to support proxy, and ignores instance name of kerberos principal.  For JWTRedirectAuthenticationHandler which is designed for proxy use case.  It should check remote host matches the clientPrincipal instance name, without this check, it makes Kerberos vulnerable."
HADOOP-15688,ABFS: InputStream wrapped in FSDataInputStream twice,"I can't read Parquet files from ABFS. It has 2 different implementations to read seekable streams, and it'll use the one that uses ByteBuffer reads if it can. It currently decides to use the ByteBuffer read implementation because the FSDataInputStream it gets back wraps another FSDataInputStream, which implements ByteBufferReadable.

That's not the most robust way to check that ByteBufferReads are supported by the ultimately underlying InputStream, but it's unnecessary and probably a mistake to double-wrap the InputStream, so let's not."
HADOOP-15661,ABFS: Add support for ACL,- Add support for ACL
HADOOP-15660,ABFS: Add support for OAuth,- Add support for OAuth
HADOOP-14597,Native compilation broken with OpenSSL-1.1.0 because EVP_CIPHER_CTX has been made opaque,"Trying to build Hadoop trunk on Fedora 26 which has openssl-devel-1.1.0 fails with this error
{code}[WARNING] /home/raviprak/Code/hadoop/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/OpensslCipher.c: In function ‘check_update_max_output_len’:
[WARNING] /home/raviprak/Code/hadoop/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/OpensslCipher.c:256:14: error: dereferencing pointer to incomplete type ‘EVP_CIPHER_CTX {aka struct evp_cipher_ctx_st}’
[WARNING]    if (context->flags & EVP_CIPH_NO_PADDING) {
[WARNING]               ^~
{code}

https://github.com/openssl/openssl/issues/962 mattcaswell says
{quote}
One of the primary differences between master (OpenSSL 1.1.0) and the 1.0.2 version is that many types have been made opaque, i.e. applications are no longer allowed to look inside the internals of the structures
{quote}"
HADOOP-14583,wasb throws an exception if you try to create a file and there's no parent directory,"It's a known defect of the Hadoop FS API (and one we don't explicitly test for enough), but you can create a file on a path which doesn't exist. In that situation, the create() logic is expectd to create the entries.

Wasb appears to raise an exception if you try to call {{create(filepath)}} without calling {{mkdirs(filepath.getParent()}} first. That's the semantics expected of {{createNonRecursive()}}
"
HADOOP-14516,Update WASB driver to use the latest version (5.2.0) of SDK for Microsoft Azure Storage Clients,"Update WASB driver to use the latest version (5.2.0) of SDK for Microsoft Azure Storage Clients. We are currently using version 4.2.0 of the SDK.

Azure Storage Clients changes between 4.2 and 5.2:

 * Fixed Exists() calls on Shares and Directories to now populate metadata. This was already being done for Files.
 * Changed blob constants to support up to 256 MB on put blob for block blobs. The default value for put blob threshold has also been updated to half of the maximum, or 128 MB currently.
 * Fixed a bug that prevented setting content MD5 to true when creating a new file.
 * Fixed a bug where access conditions, options, and operation context were not being passed when calling openWriteExisting() on a page blob or a file.
 * Fixed a bug where an exception was being thrown on a range get of a blob or file when the options disableContentMD5Validation is set to false and useTransactionalContentMD5 is set to true and there is no overall MD5.
 * Fixed a bug where retries were happening immediately if a socket exception was thrown.
 * In CloudFileShareProperties, setShareQuota() no longer asserts in bounds. This check has been moved to create() and uploadProperties() in CloudFileShare.
 * Prefix support for listing files and directories.
 * Added support for setting public access when creating a blob container
 * The public access setting on a blob container is now a container property returned from downloadProperties.
 * Add Message now modifies the PopReceipt, Id, NextVisibleTime, InsertionTime, and ExpirationTime properties of its CloudQueueMessage parameter.
 * Populate content MD5 for range gets on Blobs and Files.
 * Added support in Page Blob for incremental copy.
 * Added large BlockBlob upload support. Blocks can now support sizes up to 100 MB.
 * Added a new, memory-optimized upload strategy for the upload* APIs. This algorithm only applies for blocks greater than 4MB and when storeBlobContentMD5 and Client-Side Encryption are disabled.
 * getQualifiedUri() has been deprecated for Blobs. Please use getSnapshotQualifiedUri() instead. This new function will return the blob including the snapshot (if present) and no SAS token.
 * getQualifiedStorageUri() has been deprecated for Blobs. Please use getSnapshotQualifiedStorageUri() instead. This new function will return the blob including the snapshot (if present) and no SAS token.
 * Fixed a bug where copying from a blob that included a SAS token and a snapshot ommitted the SAS token.
 * Fixed a bug in client-side encryption for tables that was preventing the Java client from decrypting entities encrypted with the .NET client, and vice versa.
 * Added support for server-side encryption.
 * Added support for getBlobReferenceFromServer methods on CloudBlobContainer to support retrieving a blob without knowing its type.
 * Fixed a bug in the retry policies where 300 status codes were being retried when they shouldn't be.
"
HADOOP-14513,A little performance improvement of HarFileSystem,"In the Java source of HarFileSystem.java:
{code:title=HarFileSystem.java|borderStyle=solid}
...................
...................
private Path archivePath(Path p) {
    Path retPath = null;
    Path tmp = p;
    
    // I think p.depth() need not be loop many times, depth() is a complex calculation
    for (int i=0; i< p.depth(); i++) {
      if (tmp.toString().endsWith("".har"")) {
        retPath = tmp;
        break;
      }
      tmp = tmp.getParent();
    }
    return retPath;
  }
...................
...................
{code}
 
I think the fellow is more suitable:
{code:title=HarFileSystem.java|borderStyle=solid}
...................
...................
private Path archivePath(Path p) {
    Path retPath = null;
    Path tmp = p;
    
    // just loop once
    for (int i=0,depth=p.depth(); i< depth; i++) {
      if (tmp.toString().endsWith("".har"")) {
        retPath = tmp;
        break;
      }
      tmp = tmp.getParent();
    }
    return retPath;
  }
...................
...................
{code}"
HADOOP-14473,Optimize NativeAzureFileSystem::seek for forward seeks,"{{NativeAzureFileSystem::seek()}} closes and re-opens the inputstream irrespective of forward/backward seek. It would be beneficial to re-open the stream on backward seek.

https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java#L889"
HADOOP-14338,Fix warnings from Spotbugs in hadoop-yarn,Fix warnings from Spotbugs in hadoop-yarn since switched from findbugs to spotbugs.
HADOOP-14319,Under replicated blocks are not getting re-replicated,"Under replicated blocks are not getting re-replicated

In production Hadoop cluster of 5 Manangement + 5 Data Nodes, under replicated blocks are not re-replicated even after 2 days. 

Here is quick view of required configurations;

 Default replication factor:	3
 Average block replication:	3.0
 Corrupt blocks:		0
 Missing replicas:		0 (0.0 %)
 Number of data-nodes:		5
 Number of racks:		1

After bringing one of the DataNodes down, the replication factor for the blocks allocated on the Data Node became 2. It is observed that, even after 2 days the replication factor remains as 2. Under replicated blocks are not getting re-replicated to another DataNodes in the cluster. 

If a Data Node goes down, HDFS will try to replicate the blocks from Dead DN to other nodes and the priority. Are there any configuration changes to speed up the re-replication process for the under replicated blocks? 

When tested for blocks with replication factor 1, the re-replication happened to 2 overnight in around 10 hours of time. But blocks with 2 replication factor are not being re-replicated to default replication factor 3. "
HADOOP-14229,hadoop.security.auth_to_local example is incorrect in the documentation,"Let's see jhs as example:
{code}RULE:[2:$1@$0](jhs/.*@.*REALM.TLD)s/.*/mapred/{code}
That means principal has 2 components (jhs/myhost@REALM).
The second column converts this to jhs@REALM. So the regex will not match on this since regex expects / in the principal.

My suggestion is
{code}RULE:[2:$1](jhs)s/.*/mapred/{code}

https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html"
HADOOP-14179,Namenode - changing config without restart,"Hi there,

Is there a way to change the address of the secondary namenode in the configuration files of the primary namenode without having to restart the latter for the changes to have an effect? If one can avoid any downtime for the namenode for such a change, that would be great. 

Thanks"
HADOOP-14165,"Add S3Guard.dirListingUnion in S3AFileSystem#listFiles, listLocatedStatus",{{S3Guard::dirListingUnion}} merges information from backing store and DDB to create consistent view. This needs to be added in {{S3AFileSystem::listFiles}} and {{S3AFileSystem::listLocatedStatus}}
HADOOP-13959,S3guard: replace dynamo.describe() call in init with more efficient query,"HADOOP-13908 adds initialization when a table isn't created, using the {{describe()}} call.

AWS document this as inefficient, and throttle it. We should be able to get away with a simple table lookup as the probe"
HADOOP-13925,"S3Guard: NPE when table is already populated in dynamodb and user specifies ""fs.s3a.s3guard.ddb.table.create=false""","When table is present dynamodb store and already populated, it is possible that users can specify {{fs.s3a.s3guard.ddb.table.create=false}}.  In such cases, {{DynamoDBMetadataStore.get}} would end up throwing NPE as {{table}} object may not be initialized. "
HADOOP-13579,Fix source-level compatibility after HADOOP-11252,"Reported by [~chiwanpark]
bq. Since 2.7.3 release, Client.get/setPingInterval is changed from public to package-private.
bq. Giraph is one of broken examples for this changes. (https://github.com/apache/giraph/blob/release-1.0/giraph-core/src/main/java/org/apache/giraph/job/GiraphJob.java#L202)"
HADOOP-13494,ReconfigurableBase can log sensitive information,"ReconfigurableBase will log old and new configuration values, which may cause sensitive parameters (most notably cloud storage keys, though there may be other instances) to get included in the logs. 

Given the currently small list of reconfigurable properties, an argument could be made for simply not logging the property values at all, but this is not the only instance where potentially sensitive configuration gets written somewhere else in plaintext. I think a generic mechanism for redacting sensitive information for textual display will be useful to some of the web UIs too."
HADOOP-13451,S3Guard: Implement access policy using metadata store as source of truth.,"Implement an S3A access policy that provides strong consistency and improved performance by using the metadata store as the source of truth for metadata operations.  In many cases, this will allow S3A to short-circuit calls to S3.  Assuming shorter latency for calls to the metadata store compared to S3, we expect this will improve overall performance.  With this policy, a client may not be capable of reading data loaded into an S3 bucket by external tools that don't integrate with the metadata store.  Users need to be made aware of this limitation."
HADOOP-13450,S3Guard: Implement access policy providing strong consistency with S3 as source of truth.,"Implement an S3A access policy that provides strong consistency by cross-checking with the consistent metadata store, but still using S3 as the the source of truth.  This access policy will be well suited to users who want an improved consistency guarantee but also want the freedom to load data into the bucket using external tools that don't integrate with the metadata store."
HADOOP-13434,Add quoting to Shell class,"The Shell class makes assumptions that the parameters won't have spaces or other special characters, even when it invokes bash."
HADOOP-13424,namenode connect time out in cluster with 65 machiones,"Befor out cluster has 50 nodes ,it runs ok. Recently we add 15 node ,it always reports errors with connectint  timeout.Who can help me ,thanks."
HADOOP-13379,Hadoop: Failed to set permissions of path: \tmp\hadoop-User\mapred\staging\ while running testdriver from eclipse,"Hadoop: Failed to set permissions of path: \tmp\hadoop-User\mapred\staging\

I am trying to work through this tutorial about hadoop and eclipse: http://v-lad.org/Tutorials/Hadoop/. It went fine until the last step ""Running Hadoop Project"". Here, when I run the project in Eclipse, I get the error:

14/11/26 16:25:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/11/26 16:25:24 ERROR security.UserGroupInformation: PriviledgedActionException as:User cause:java.io.IOException: Failed to set permissions of path: \tmp\hadoop-User\mapred\staging\User660196934\.staging to 0700
java.io.IOException: Failed to set permissions of path: \tmp\hadoop-User\mapred\staging\User660196934\.staging to 0700
    at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:691)
    at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:664)
    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:514)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:349)
    at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:193)
    at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:126)
    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:942)
    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Unknown Source)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)
    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)
    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)
    at TestDriver.main(TestDriver.java:41)
I had earlier this permission issue (taskTracker could not start because ""Failed to set permissions"" ), but I resolved it using this patch: https://github.com/congainc/patch-hadoop_7682-1.0.x-win. I don't understand why this doesn't work when running the project form Eclipse. How can I solve this issue?

Hadoop-version is 1.2.1


Core-Site.xml

<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:50000</value>
</property>
 <property>
        <name>fs.file.impl</name>
        <value>com.conga.services.hadoop.patch.HADOOP_7682.WinLocalFileSystem</value>
        <description>Enables patch for issue HADOOP-7682 on Windows</description>
    </property>
<property>
<name>hadoop.tmp.dir</name>
 <value>/tmp/hadoop-${USER}/</value>
 <description>A base for other temporary directories.</description>
</property>
</configuration>
"
HADOOP-13370,"After creating HA, getting java.net.UnknownHostException","I have made my standalone cluster to a high available using quorum journal method. After that i am not able to access HDFS. Getting below error :

[hadoop@namenode hadoop]$ hdfs dfs -ls /
16/07/12 22:46:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-ls: java.net.UnknownHostException: myhahdpcluster
Usage: hadoop fs [generic options] -ls [-d] [-h] [-R] [<path> ...]
[hadoop@namenode hadoop]$


All the other HA components are working fine. Can you please let me know if i am missing anything."
HADOOP-13362,DefaultMetricsSystem leaks the source name when a source unregisters,"Ran across a nodemanager that was spending most of its time in GC.  Upon examination of the heap most of the memory was going to the map of names in org.apache.hadoop.metrics2.lib.UniqueNames.  In this case the map had almost 2 million entries.  Looking at a few of the map showed entries like ""ContainerResource_container_e01_1459548490386_8560138_01_002020"", ""ContainerResource_container_e01_1459548490386_2378745_01_000410"", etc.

Looks like the ContainerMetrics for each container will cause a unique name to be registered with UniqueNames and the name will never be unregistered."
HADOOP-13350,Additional fix to LICENSE and NOTICE,Fix up LICENSE and NOTICE after HADOOP-12893.
HADOOP-13312,Update CHANGES.txt to reflect all the changes in branch-2.7,"When committing to branch-2.7, we need to edit CHANGES.txt. However, there are some commits to branch-2.7 without editing CHANGES.txt. We need to update the change log."
HADOOP-13298,Fix the leftover L&N files in hadoop-build-tools/src/main/resources/META-INF/,"After HADOOP-12893, an extra copy of LICENSE.txt and NOTICE.txt exists in {{hadoop-build-tools/src/main/resources/META-INF/}} after build. We should remove it and do it the maven way.

Details in https://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201606.mbox/%3CCAFS=Wjwx8nMqj6FZXUzZBWRAEoGgfr+_YWL_mkFp4LNuxpggMA@mail.gmail.com%3E

Thanks [~stevel@apache.org] for raising the issue and [~busbey] for offering the help!"
HADOOP-13297,Add missing dependency in setting maven-remote-resource-plugin to fix builds,"After HADOOP-12893, we are seeing {{mvn install -DskipTests}} failing in branch-2.7, branch-2.7.3, and branch-2.6. This failure is caused by the followings
* hadoop-project module depends on hadoop-build-tools module, but hadoop-project module does not declare hadoop-build-tools as its submodule. Therefore, hadoop-build-tools is not built before building hadoop-project.
* hadoop-build-tools pom and jar are not uploaded to the snapshot repository (https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-build-tools/)

The build failure occurs if the *both* of the above conditions are satisfied."
HADOOP-13290,Appropriate use of generics in FairCallQueue,"# {{BlockingQueue}} is intermittently used with and without generic parameters in {{FairCallQueue}} class. Should be parameterized.
# Same for {{FairCallQueue}}. Should be parameterized. Could be a bit more tricky for that one."
HADOOP-13270,"BZip2CompressionInputStream finds the same compression marker twice in corner case, causing duplicate data blocks","Unit test TestTextInputFormat.testSplitableCodecs() failed when the seed is  1313094493.

Stacktrace
java.lang.AssertionError: Key in multiple partitions.
at org.junit.Assert.fail(Assert.java:88)
at org.junit.Assert.assertTrue(Assert.java:41)
at org.junit.Assert.assertFalse(Assert.java:64)
at org.apache.hadoop.mapred.TestTextInputFormat.testSplitableCodecs(TestTextInputFormat.java:223)
"
HADOOP-13255,KMSClientProvider should check and renew tgt when doing delegation token operations.,
HADOOP-13192,org.apache.hadoop.util.LineReader cannot handle multibyte delimiters correctly,"org.apache.hadoop.util.LineReader.readCustomLine()  has a bug,
when line is   aaaabccc, recordDelimiter is aaab, the result should be a,ccc,
show the code on line 310:
      for (; bufferPosn < bufferLength; ++bufferPosn) {
        if (buffer[bufferPosn] == recordDelimiterBytes[delPosn]) {
          delPosn++;
          if (delPosn >= recordDelimiterBytes.length) {
            bufferPosn++;
            break;
          }
        } else if (delPosn != 0) {
          bufferPosn--;
          delPosn = 0;
        }
      }

shoud be :
      for (; bufferPosn < bufferLength; ++bufferPosn) {
        if (buffer[bufferPosn] == recordDelimiterBytes[delPosn]) {
          delPosn++;
          if (delPosn >= recordDelimiterBytes.length) {
            bufferPosn++;
            break;
          }
        } else if (delPosn != 0) {
         // ------------- change here ------------- start ----

          bufferPosn -= delPosn;
         // ------------- change here ------------- end ----
  
          delPosn = 0;
        }
      }
"
HADOOP-13189,FairCallQueue makes callQueue larger than the configured capacity.,"{{FairCallQueue}} divides {{callQueue}} into multiple (4 by default) sub-queues, with each sub-queue corresponding to a different level of priority. The constructor for {{FairCallQueue}} takes the same parameter {{capacity}} as the default CallQueue implementation, and allocates all its sub-queues of size {{capacity}}. With 4 levels of priority (sub-queues) by default it results in the total callQueue size 4 times larger than it should be based on the configuration.
{{capacity}} should be divided by the number of sub-queues at some place."
HADOOP-13154,S3AFileSystem printAmazonServiceException/printAmazonClientException appear copy & paste of AWS examples,"The logging code in {{S3AFileSystem.printAmazonServiceException()}} and {{printAmazonClientException}} appear to be paste + edits of the example code in  the amazon SDK, such as [http://docs.aws.amazon.com/AmazonS3/latest/dev/ListingObjectKeysUsingJava.html]]

Either we review the license to validate it, and add credits to the code if compatible, or we rework. HADOOP-13130 would be the place to do that, as it is changing exception handling anyway.

tagging as blocker as it is license related"
HADOOP-13103,Group resolution from LDAP may fail on javax.naming.ServiceUnavailableException,"According to the [javadoc|https://docs.oracle.com/javase/7/docs/api/javax/naming/ServiceUnavailableException.html], ServiceUnavailableException is thrown when attempting to communicate with a directory or naming service and that service is not available. It might be unavailable for different reasons. For example, the server might be too busy to service the request, or the server might not be registered to service any requests, etc.

We should retry on it."
HADOOP-13084,Fix ASF License warnings in branch-2.7,"Please have a look following PreCommit build on branch-2.7.

https://builds.apache.org/job/PreCommit-HDFS-Build/15036/artifact/patchprocess/patch-asflicense-problems.txt"
HADOOP-13052,ChecksumFileSystem mishandles crc file permissions,"CheckFileSystem does not override permission related calls to apply those operations to the hidden crc files.  Clients may be unable to read the crcs if the file is created with strict permissions and then relaxed.

The checksum fs is designed to work with or w/o crcs present, so it silently ignores FNF exceptions.  The java file stream apis unfortunately may only throw FNF, so permission denied becomes FNF resulting in this bug going silently unnoticed.

(Problem discovered via public localizer.  Files are downloaded as user-readonly and then relaxed to all-read.  The crc remains user-readonly)"
HADOOP-13043,Add LICENSE.txt entries for bundled javascript dependencies,None of our bundled javascript dependencies are mentioned in LICENSE.txt. Let's fix that.
HADOOP-13042,Restore lost leveldbjni LICENSE and NOTICE changes,"As noted on HADOOP-12893, we lost the leveldbjni related NOTICE and LICENSE updates done in YARN-1704 when HADOOP-10956 was committed. Let's restore them."
HADOOP-13039,Add documentation for configuration property ipc.maximum.data.length for controlling maximum RPC message size.,"The RPC server enforces a maximum length on incoming messages.  Messages larger than the maximum are rejected immediately.  The maximum length can be tuned by setting configuration property {{ipc.maximum.data.length}}, but this is not documented in core-site.xml."
HADOOP-12989,Some tests in org.apache.hadoop.fs.shell.find occasionally time out,"An example:
{noformat}
java.lang.Exception: test timed out after 1000 milliseconds
	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1965)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1872)
	at java.lang.Runtime.loadLibrary0(Runtime.java:849)
	at java.lang.System.loadLibrary(System.java:1088)
	at sun.security.action.LoadLibraryAction.run(LoadLibraryAction.java:67)
	at sun.security.action.LoadLibraryAction.run(LoadLibraryAction.java:47)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.NetworkInterface.<clinit>(NetworkInterface.java:56)
	at org.apache.htrace.core.TracerId.getBestIpString(TracerId.java:179)
	at org.apache.htrace.core.TracerId.processShellVar(TracerId.java:145)
	at org.apache.htrace.core.TracerId.<init>(TracerId.java:116)
	at org.apache.htrace.core.Tracer$Builder.build(Tracer.java:159)
	at org.apache.hadoop.fs.FsTracer.get(FsTracer.java:42)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2794)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2837)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2819)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:381)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:180)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:365)
	at org.apache.hadoop.fs.shell.PathData.<init>(PathData.java:81)
	at org.apache.hadoop.fs.shell.find.TestName.applyGlob(TestName.java:74)
{noformat}"
HADOOP-12958,PhantomReference for filesystem statistics can trigger OOM,I saw an OOM that appears to have been caused by the phantom references introduced for file system statistics management.  I'll post details in a followup comment.
HADOOP-12902,JavaDocs for SignerSecretProvider are out-of-date in AuthenticationFilter,"The Javadocs in {{AuthenticationFilter}} say:
{noformat}
 * Out of the box it provides 3 signer secret provider implementations:
 * ""string"", ""random"", and ""zookeeper""
{noformat}
However, the ""string"" implementation is no longer available because HADOOP-11748 moved it to be a test-only artifact.  This also doesn't mention anything about the file-backed secret provider ({{FileSignerSecretProvider}})."
HADOOP-12893,Verify LICENSE.txt and NOTICE.txt,We have many bundled dependencies in both the source and the binary artifacts that are not in LICENSE.txt and NOTICE.txt.
HADOOP-12872,Fix formatting in ServiceLevelAuth.md,"{noformat} `security.client.protocol.hosts>> will be <<<security.client.protocol.hosts.blocked`
{noformat} should be
{noformat} `security.client.protocol.hosts` will be `security.client.protocol.hosts.blocked`.
{noformat}"
HADOOP-12871,Fix dead link to NativeLibraries.html in CommandsManual.md,"{noformat:title=CommandsManual.md}
This command checks the availability of the Hadoop native code. See [\#NativeLibraries.html](#NativeLibraries.html) for more information. By default, this command only checks the availability of libhadoop.
{noformat}
The link should be fixed to {{\[Native Libaries\](./NativeLibraries.html)}}."
HADOOP-12870,Fix typo admininistration in CommandsManual.md,"{noformat:title=CommandsManual.md}
All of these commands are executed from the `hadoop` shell command. They have been broken up into [User Commands](#User_Commands) and [Admininistration Commands](#Admininistration_Commands).
{noformat}
""Admininistration"" should be ""Administration""."
HADOOP-12810,FileSystem#listLocatedStatus causes unnecessary RPC calls,"{{FileSystem#listLocatedStatus}} lists the files in a directory and then calls {{getFileBlockLocations(stat.getPath(), ...)}} for each instead of {{getFileBlockLocations(stat, ...)}}. That function with the path arg just calls {{getFileStatus}} to get another file status from the path and calls the file status version, so this ends up calling {{getFileStatus}} unnecessarily.

This is particularly bad for S3, where {{getFileStatus}} is expensive. Avoiding the extra call improved input split calculation time for a data set in S3 by ~20x: from 10 minutes to 25 seconds."
HADOOP-12805,Annotate CanUnbuffer with @InterfaceAudience.Public,"See comments toward the tail of HBASE-9393.

The change in HBASE-9393 adds dependency on CanUnbuffer interface which is currently marked @InterfaceAudience.Private

To facilitate downstream projects such as HBase in using this interface, CanUnbuffer interface should be annotated @LimitedPrivate(\{""HBase"", ""HDFS""\})."
HADOOP-12800,Copy docker directory from 2.8 to 2.7/2.6 repos to enable pre-commit Jenkins runs,
HADOOP-12794,Support additional compression levels for GzipCodec,"gzip supports compression levels 1-9. Compression level 4 seems to give best compression per CPU time in some of our tests. Right now ZlibCompressor that is used by GzipCodec only supports levels 1,9 and six (default). 

Adding all the compression levels that are supported by native ZlibCompressor
can provide more options to tweak compression levels. "
HADOOP-12792,TestUserGroupInformation#testGetServerSideGroups fails in chroot,Bug fixed by [HADOOP-7811] broken by [HADOOP-8562]. Need to re-introduce the fix. 
HADOOP-12789,log classpath of ApplicationClassLoader at INFO level,Currently {{ApplicationClassLoader}} does not log the classpath at the INFO level although the system classes are logged at that level. Knowing exactly what classpath {{ApplicationClassLoader}} has is a critical piece of information for troubleshooting. We should log it at the INFO level.
HADOOP-12786,"""hadoop key"" command usage is not documented","I found ""hadoop key"" command usage is not documented when reviewing HDFS-9784.
In addition, we should document that uppercase is not allowed for key name."
HADOOP-12773,HBase classes fail to load with client/job classloader enabled,"Currently if a user uses HBase and enables the client/job classloader, the job fails to load HBase classes. For example,

{noformat}
java.lang.NoClassDefFoundError: Lorg/apache/hadoop/hbase/client/HBaseAdmin;
	at java.lang.Class.getDeclaredFields0(Native Method)
	at java.lang.Class.privateGetDeclaredFields(Class.java:2509)
	at java.lang.Class.getDeclaredField(Class.java:1959)
	at java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1703)
	at java.io.ObjectStreamClass.access$700(ObjectStreamClass.java:72)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:484)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:472)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:472)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:369)
{noformat}

It is because the HBase classes (org.apache.hadoop.hbase.\*) meet the system classes criteria which are supposed to be loaded strictly from the base classloader. But hadoop does not provide HBase as a dependency.

We should exclude the HBase classes from the system classes until/unless HBase is provided by a future version of hadoop."
HADOOP-12772,NetworkTopologyWithNodeGroup.getNodeGroup() can loop infinitely for invalid 'loc' values,"Although the DatanodInfo/DataNodeDescriptor object tends to have a non-null, non-empty value for its network location, the getNodeGroup method does not handle the case if an empty or a null value is passed and can go on recursively causing StackOverflowError. This is an improvement to check for such a state and error out if the value is null and set the location to ROOT or empty string and if the original location is empty. "
HADOOP-12761,incremental maven build is not really incremental,"For any version that uses v.3.1 of the maven-compiler-plugin, the incremental maven build is basically broken. For most of the modules, an incremental build ({{mvn install -DskipTests}} on an already built directory for example) rebuilds the whole module again:

{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop Common 3.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (create-testdirs) @ hadoop-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) @ hadoop-common ---
[INFO] No changes detected in protoc files, skipping generation.
[INFO] 
[INFO] --- hadoop-maven-plugins:3.0.0-SNAPSHOT:version-info (version-info) @ hadoop-common ---
[WARNING] [svn, info] failed with error code 1
[INFO] SCM: GIT
[INFO] Computed MD5: c8e92ce138fcd723204649e4d7c6ddd
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hadoop-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hadoop-common ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 871 source files to /Users/foo/hadoop/hadoop-common-project/hadoop-common/target/classes
{noformat}

It turns out that the incremental build feature of the maven compiler plugin is basically broken at v3.1 (see http://stackoverflow.com/questions/17944108/maven-compiler-plugin-always-detecting-a-set-of-sources-as-stale and MCOMPILER-209). Ironically, this can be fixed by turning off the ""incremental build"" configuration of the plugin."
HADOOP-12736,TestTimedOutTestsListener#testThreadDumpAndDeadlocks sometimes times out,"Saw this test failure today, the {{@Test(timeout=500)}} seems too aggressive to me.
{noformat}
testThreadDumpAndDeadlocks(org.apache.hadoop.test.TestTimedOutTestsListener)  Time elapsed: 0.521 sec  <<< ERROR!
java.lang.Exception: test timed out after 500 milliseconds
	at jdk.internal.org.objectweb.asm.ByteVector.putShort(ByteVector.java:147)
	at jdk.internal.org.objectweb.asm.ClassWriter.toByteArray(ClassWriter.java:942)
	at java.lang.invoke.InvokerBytecodeGenerator.generateCustomizedCodeBytes(InvokerBytecodeGenerator.java:727)
	at java.lang.invoke.InvokerBytecodeGenerator.generateCustomizedCode(InvokerBytecodeGenerator.java:618)
	at java.lang.invoke.LambdaForm.compileToBytecode(LambdaForm.java:654)
	at java.lang.invoke.LambdaForm.prepare(LambdaForm.java:635)
	at java.lang.invoke.MethodHandle.<init>(MethodHandle.java:461)
	at java.lang.invoke.BoundMethodHandle.<init>(BoundMethodHandle.java:56)
	at java.lang.invoke.SimpleMethodHandle.<init>(SimpleMethodHandle.java:37)
	at java.lang.invoke.SimpleMethodHandle.make(SimpleMethodHandle.java:41)
	at java.lang.invoke.LambdaForm.createIdentityForms(LambdaForm.java:1778)
	at java.lang.invoke.LambdaForm.<clinit>(LambdaForm.java:1833)
	at java.lang.invoke.DirectMethodHandle.makePreparedLambdaForm(DirectMethodHandle.java:222)
	at java.lang.invoke.DirectMethodHandle.preparedLambdaForm(DirectMethodHandle.java:187)
	at java.lang.invoke.DirectMethodHandle.preparedLambdaForm(DirectMethodHandle.java:176)
	at java.lang.invoke.DirectMethodHandle.make(DirectMethodHandle.java:83)
	at java.lang.invoke.MethodHandles$Lookup.getDirectMethodCommon(MethodHandles.java:1656)
	at java.lang.invoke.MethodHandles$Lookup.getDirectMethodNoSecurityManager(MethodHandles.java:1613)
	at java.lang.invoke.MethodHandles$Lookup.getDirectMethodForConstant(MethodHandles.java:1798)
	at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1747)
	at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:477)
	at java.lang.UNIXProcess$Platform.get(UNIXProcess.java:155)
	at java.lang.UNIXProcess.<clinit>(UNIXProcess.java:168)
	at java.lang.ProcessImpl.start(ProcessImpl.java:130)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:868)
	at org.apache.hadoop.util.Shell.run(Shell.java:838)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)
	at org.apache.hadoop.util.Shell.checkIsBashSupported(Shell.java:716)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:705)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:98)
	at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73)
	at org.apache.hadoop.test.TimedOutTestsListener.testFailure(TimedOutTestsListener.java:62)
	at org.apache.hadoop.test.TestTimedOutTestsListener.testThreadDumpAndDeadlocks(TestTimedOutTestsListener.java:163)
{noformat}"
HADOOP-12715,TestValueQueue#testgetAtMostPolicyALL fails intermittently,"The test fails intermittently with the following error.
Error Message
{noformat}
expected:<19> but was:<10>
{noformat}
Stacktrace
{noformat}
java.lang.AssertionError: expected:<19> but was:<10>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.crypto.key.TestValueQueue.testgetAtMostPolicyALL(TestValueQueue.java:149)
{noformat}"
HADOOP-12706,TestLocalFsFCStatistics#testStatisticsThreadLocalDataCleanUp times out occasionally,"TestLocalFsFCStatistics has been failing sometimes, and when it fails it appears to be from FCStatisticsBaseTest.testStatisticsThreadLocalDataCleanUp.  The test is timing out when it fails."
HADOOP-12688,Fix deadlinks in Compatibility.md,"There are 2 dead links in Compability.md. The links to MRAppMaster/JobHistoryServer REST API are wrong.
https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/Compatibility.html#REST_APIs"
HADOOP-12682,Fix TestKMS#testKMSRestart* failure,"https://builds.apache.org/job/Hadoop-Common-trunk/2157/testReport/org.apache.hadoop.crypto.key.kms.server/TestKMS/testKMSRestartSimpleAuth/
{noformat}
Error Message

loginUserFromKeyTab must be done first

Stacktrace

java.io.IOException: loginUserFromKeyTab must be done first
	at org.apache.hadoop.security.UserGroupInformation.reloginFromKeytab(UserGroupInformation.java:1029)
	at org.apache.hadoop.security.UserGroupInformation.checkTGTAndReloginFromKeytab(UserGroupInformation.java:994)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:478)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createKeyInternal(KMSClientProvider.java:679)
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createKey(KMSClientProvider.java:697)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$10.call(LoadBalancingKMSClientProvider.java:259)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$10.call(LoadBalancingKMSClientProvider.java:256)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:94)
	at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.createKey(LoadBalancingKMSClientProvider.java:256)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$6$1.run(TestKMS.java:1003)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$6$1.run(TestKMS.java:1000)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1669)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:266)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:75)

{noformat}
Seems to be introduced by HADOOP-12559"
HADOOP-12636,Prevent ServiceLoader failure init for unused FileSystems,"loadFileSystems() loads all the Filesystems in the path. However, some Filesystems cannot be initialized. There is no point on failing the startup because a Filesystem that won't be used."
HADOOP-12613,TestFind.processArguments occasionally fails,"This failure seems to exist after November 3rd. I am still tracing where this can come from.
https://builds.apache.org/job/Hadoop-Common-trunk/2066/testReport/org.apache.hadoop.fs.shell.find/TestFind/processArguments/
Error Message

test timed out after 1000 milliseconds

Stacktrace
{noformat}
java.lang.Exception: test timed out after 1000 milliseconds
	at java.util.AbstractList$Itr.next(AbstractList.java:357)
	at java.util.SubList$1.next(AbstractList.java:707)
	at java.util.AbstractCollection.toArray(AbstractCollection.java:141)
	at java.util.ArrayList.addAll(ArrayList.java:559)
	at org.mockito.internal.exceptions.base.StackTraceFilter.filter(StackTraceFilter.java:54)
	at org.mockito.internal.debugging.Location.<init>(Location.java:22)
	at org.mockito.internal.debugging.Location.<init>(Location.java:17)
	at org.mockito.internal.invocation.Invocation.<init>(Invocation.java:60)
	at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:46)
	at org.apache.hadoop.fs.FileStatus$$EnhancerByMockitoWithCGLIB$$a131b1e2.isSymlink(<generated>)
	at org.apache.hadoop.fs.shell.find.Find.recursePath(Find.java:355)
	at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:323)
	at org.apache.hadoop.fs.shell.Command.recursePath(Command.java:377)
	at org.apache.hadoop.fs.shell.find.Find.recursePath(Find.java:369)
	at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:323)
	at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:293)
	at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:275)
	at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:259)
	at org.apache.hadoop.fs.shell.find.Find.processArguments(Find.java:427)
	at org.apache.hadoop.fs.shell.find.TestFind.processArguments(TestFind.java:253)
{noformat}"
HADOOP-12602,TestMetricsSystemImpl#testQSize occasionally fail,"I have seen this test failed a few times in the past.
Error Message
{noformat}
metricsSink.putMetrics(<Capturing argument>);
Wanted 2 times:
-> at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testQSize(TestMetricsSystemImpl.java:472)
But was 1 time:
-> at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:183)
{noformat}
Stacktrace
{noformat}
org.mockito.exceptions.verification.TooLittleActualInvocations: 
metricsSink.putMetrics(<Capturing argument>);
Wanted 2 times:
-> at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testQSize(TestMetricsSystemImpl.java:472)
But was 1 time:
-> at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:183)

	at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testQSize(TestMetricsSystemImpl.java:472)
{noformat}
Standard Output
{noformat}
2015-11-25 19:07:49,867 INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(115)) - loaded properties from hadoop-metrics2-test.properties
2015-11-25 19:07:49,932 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled snapshot period at 10 second(s).
2015-11-25 19:07:49,932 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(192)) - Test metrics system started
2015-11-25 19:07:50,134 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(203)) - Sink slowSink started
2015-11-25 19:07:50,135 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink slowSink
2015-11-25 19:07:50,135 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(203)) - Sink dataSink started
2015-11-25 19:07:50,136 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink dataSink
2015-11-25 19:07:50,746 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(211)) - Stopping Test metrics system...
2015-11-25 19:07:50,747 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(140)) - slowSink thread interrupted.
2015-11-25 19:07:50,748 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(140)) - dataSink thread interrupted.
2015-11-25 19:07:50,748 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(217)) - Test metrics system stopped.
{noformat}"
HADOOP-12589,Fix intermittent test failure of TestCopyPreserveFlag ,"Found this issue on HADOOP-11149.

{quote}
Tests run: 8, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 0.949 sec <<< FAILURE! - in org.apache.hadoop.fs.shell.TestCopyPreserveFlag
testDirectoryCpWithP(org.apache.hadoop.fs.shell.TestCopyPreserveFlag)  Time elapsed: 0.616 sec  <<< ERROR!
java.io.IOException: Mkdirs failed to create d0 (exists=false, cwd=/testptch/hadoop/hadoop-common-project/hadoop-common/target/test/data/2/testStat)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:449)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:435)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:894)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:856)
	at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:1150)
	at org.apache.hadoop.fs.shell.TestCopyPreserveFlag.initialize(TestCopyPreserveFlag.java:72)
{quote}"
HADOOP-12588,Fix intermittent test failure of TestGangliaMetrics,"Jenkins found this test failure on HADOOP-11149.

{quote}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.773 sec <<< FAILURE! - in org.apache.hadoop.metrics2.impl.TestGangliaMetrics
testGangliaMetrics2(org.apache.hadoop.metrics2.impl.TestGangliaMetrics)  Time elapsed: 0.39 sec  <<< FAILURE!
java.lang.AssertionError: Missing metrics: test.s1rec.Xxx
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.hadoop.metrics2.impl.TestGangliaMetrics.checkMetrics(TestGangliaMetrics.java:159)
	at org.apache.hadoop.metrics2.impl.TestGangliaMetrics.testGangliaMetrics2(TestGangliaMetrics.java:137)
{quote}"
HADOOP-12577,Bump up commons-collections version to 3.2.2 to address a security flaw,"Update commons-collections from 3.2.1 to 3.2.2 because of a major security vulnerability. There are many other open source projects use commons-collections and are also affected.

Please see http://foxglovesecurity.com/2015/11/06/what-do-weblogic-websphere-jboss-jenkins-opennms-and-your-application-have-in-common-this-vulnerability/ for the discovery of the vulnerability.

https://issues.apache.org/jira/browse/COLLECTIONS-580 has the discussion thread of the fix.

https://blogs.apache.org/foundation/entry/apache_commons_statement_to_widespread The ASF response to the security vulnerability.

"
HADOOP-12570,HDFS Secure Mode Documentation updates,Some Kerberos configuration parameters are not documented well enough. 
HADOOP-12565,Replace DSA with RSA for SSH key type in SingleCluster.md,"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html

suggests to create a DSA key to ssh to localhost. However DSA is going to be deprecated[1], and ssh to localhost with DSA actually does not work on all systems. I suggest to replace DSA with RSA, ECDSA, or ED25519.

Ref.:
[1] http://comments.gmane.org/gmane.linux.gentoo.devel/96896"
HADOOP-12559,KMS connection failures should trigger TGT renewal,
HADOOP-12545,"Hadoop javadoc has broken links for AccessControlList, ImpersonationProvider, DefaultImpersonationProvider, and DistCp","1) open hadoop-2.7.1\share\doc\hadoop\api\index.html
2) Click on ""All Classes""
3) Click on ""AccessControlList"", The page shows ""This page can’t be displayed""
Same error for DistCp, ImpersonationProvider and DefaultImpersonationProvider also.

Javadoc generated from Trunk has the same problem
"
HADOOP-12526,[Branch-2] there are duplicate dependency definitions in pom's,"There are several places where dependencies are defined multiple times within pom's, and are causing maven build warnings. They should be fixed. This is specific to branch-2.6."
HADOOP-12482,Race condition in JMX cache update,"updateJmxCache() was updated in HADOOP-11301. However the patch introduced a race condition. In updateJmxCache() function in MetricsSourceAdapter.java:

{code:java}
  private void updateJmxCache() {
    boolean getAllMetrics = false;
    synchronized (this) {
      if (Time.now() - jmxCacheTS >= jmxCacheTTL) {
        // temporarilly advance the expiry while updating the cache
        jmxCacheTS = Time.now() + jmxCacheTTL;
        if (lastRecs == null) {
          getAllMetrics = true;
        }
      } else {
        return;
      }

      if (getAllMetrics) {
        MetricsCollectorImpl builder = new MetricsCollectorImpl();
        getMetrics(builder, true);
      }

      updateAttrCache();
      if (getAllMetrics) {
        updateInfoCache();
      }
      jmxCacheTS = Time.now();
      lastRecs = null; // in case regular interval update is not running
    }
  }
{code}

Notice that getAllMetrics is set to true when:
# jmxCacheTTL has passed
# lastRecs == null

lastRecs is set to null in the same function, but gets reassigned by getMetrics().

However getMetrics() can be called from a different thread:
# MetricsSystemImpl.onTimerEvent()
# MetricsSystemImpl.publishMetricsNow()

Consider the following sequence:
# updateJmxCache() is called by getMBeanInfo() from a thread getting cached info. 
** lastRecs is set to null.
# metrics sources is updated with new value/field.
# getMetrics() is called by publishMetricsNow() or onTimerEvent() from a different thread getting the latest metrics. 
** lastRecs is updated (!= null).
# jmxCacheTTL passed.
# updateJmxCache() is called again via getMBeanInfo().
** However because lastRecs is already updated (!= null), getAllMetrics will not be set to true. So updateInfoCache() is not called and getMBeanInfo() returns the old cached info.

We ran into this issue on a cluster where a new metric did not get published until much later.

The case can be made worse by a periodic call to getMetrics() (driven by an external program or script). In such case getMBeanInfo() may never be able to retrieve the new record.

The desired behavior should be that updateJmxCache() will guarantee to call updateInfoCache() once after jmxCacheTTL, if lastRecs has been set to null by updateJmxCache() itself."
HADOOP-12465,Incorrect javadoc in WritableUtils.java,"Documentation for writeVLong in WritableUtils states that ""For
 -112 <= i <= 127, only one byte is used""

Documentation for writeVInt states that ""For -120 <= i <= 127, only one byte is used""

writeVInt calls internally writeVLong, so the ranges are the same for both functions. 
After examining the code, I see that the documentation that is at writeVLong is correct.
Documentation for writeVInt is therefore incorrect and should be fixed.
"
HADOOP-12464,Interrupted client may try to fail-over and retry,"When an IPC client is interrupted, it sometimes try to fail-over to a different namenode and retry.  We've seen this causing hang during shutdown. "
HADOOP-12451,[Branch-2] Setting HADOOP_HOME explicitly should be allowed,"HADOOP-11464 reinstates cygwin support. In the process, it sets HADOOP_HOME explicitly in hadoop-config.sh without checking if it has already been set. "
HADOOP-12446,Undeprecate createNonRecursive(),"FileSystem#createNonRecursive() is deprecated.
However, there is no DistributedFileSystem#create() implementation which throws exception if parent directory doesn't exist.
This limits clients' migration away from the deprecated method.

For HBase, IO fencing relies on the behavior of FileSystem#createNonRecursive().
Variant of create() method should be added which throws exception if parent directory doesn't exist."
HADOOP-12415,hdfs and nfs builds broken on -missing compile-time dependency on netty,As discovered in BIGTOP-2049 {{hadoop-nfs}} module compilation is broken. Looks like that HADOOP-11489 is the root-cause of it.
HADOOP-12413,AccessControlList should avoid calling getGroupNames in isUserInList with empty groups.,"{{AccessControlList}} should avoid calling {{getGroupNames}} in {{isUserInList}} with empty {{groups}}. Currently {{AccessControlList}} will call {{ugi.getGroupNames()}} in {{isUserInList}} even if {{groups}} is empty. {{ugi.getGroupNames()}} is an expensive operation which call shell script {{id -gn <USER> && id -Gn <user>}} to get the list of groups. For example,
{{ServiceAuthorizationManager#authorize}} will call blocked ACL {{acls[1].isUserAllowed(user)}} to check the user permission. The default value for blocked ACL  is empty
{code}
    String defaultBlockedAcl = conf.get(   CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_AUTHORIZATION_DEFAULT_BLOCKED_ACL, """");
{code}
So every time {{authorize}} is called, {{getGroupNames}} may be called.
It also caused the following warning message:
{code}
2015-09-08 14:55:34,236 WARN [Socket Reader #1 for port 52715] org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user job_1441722221553_0005: id: job_1441722221553_0005: No such user
2015-09-08 14:55:34,236 WARN [Socket Reader #1 for port 52715] org.apache.hadoop.security.UserGroupInformation: No groups available for user job_1441722221553_0005
2015-09-08 14:55:34,236 INFO [Socket Reader #1 for port 52715] SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for job_1441722221553_0005 (auth:TOKEN) for protocol=interface org.apache.hadoop.mapred.TaskUmbilicalProtocol
{code}
"
HADOOP-12406,AbstractMapWritable.readFields throws ClassNotFoundException with custom writables,"Note: I am not an expert at JAVA, Class loaders, or Hadoop. I am just a hacker. My solution might be entirely wrong.

AbstractMapWritable.readFields throws a ClassNotFoundException when reading custom writables. Debugging the job using remote debugging in IntelliJ revealed that the class loader being used in Class.forName() is different than that used by the Thread's current context (Thread.currentThread().getContextClassLoader()). The class path for the system class loader does not include the libraries of the job jar. However, the class path for the context class loader does. The proposed patch changes the class loading mechanism in readFields to use the Thread's context class loader instead of the system's default class loader."
HADOOP-12374,Description of hdfs expunge command is confusing,"Usage: hadoop fs -expunge

Empty the Trash. Refer to the HDFS Architecture Guide for more information on the Trash feature.

this description is confusing. It gives user the impression that this command will empty trash, but actually it only removes old checkpoints. If user sets a pretty long value for fs.trash.interval, this command will not remove anything until checkpoints exist longer than this value."
HADOOP-12359,hadoop fs -getmerge doc is wrong,"The docs at:

    http://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/FileSystemShell.html#getmerge

say that addnl is a valid parameter, but as of HADOOP-7348, it's been replaced with -nl.  The docs should be updated."
HADOOP-12348,MetricsSystemImpl creates MetricsSourceAdapter with wrong time unit parameter.,"MetricsSystemImpl creates MetricsSourceAdapter with wrong time unit parameter. MetricsSourceAdapter expects time unit millisecond  for jmxCacheTTL but MetricsSystemImpl  passes time unit second to MetricsSourceAdapter constructor.
{code}
        jmxCacheTS = Time.now() + jmxCacheTTL;
  /**
   * Current system time.  Do not use this to calculate a duration or interval
   * to sleep, because it will be broken by settimeofday.  Instead, use
   * monotonicNow.
   * @return current time in msec.
   */
  public static long now() {
    return System.currentTimeMillis();
  }
{code}
"
HADOOP-12304,Applications using FileContext fail with the default file system configured to be wasb/s3/etc.,"HADOOP-11618 fixed a bug with {{DelegateToFileSystem}} using the wrong default port.  As a side effect of this patch, file path URLs that previously had no port now insert :0 for the port, as per the default implementation of {{FileSystem#getDefaultPort}}.  At runtime, this can cause an application to erroneously try contacting port 0 for a remote blob store service.  The connection fails.  Ultimately, this renders wasb, s3, and probably custom file system implementations outside the Hadoop source tree completely unusable as the default file system."
HADOOP-12296,"when setnetgrent returns 0 in linux, exception should be thrown","In linux, setnetgrent returns 0 in linux when something wrong is happen, such as out of memory, unknown group, unavailable service, etc. So errorMessage should be set and exception should be thrown"
HADOOP-12280,Skip unit tests based on maven profile rather than NativeCodeLoader.isNativeCodeLoaded,Some tests are skipped if native code is not loaded (i.e. NativeCodeLoader.isNativeCodeLoaded() returns false). Skipping the test or not should be judged from Maven profile rather than isNativeCodeLoaded() because tests should fail if native libraries are misplaced by invalid configuration in native profile.
HADOOP-12232,Upgrade Tomcat dependency to 6.0.44.,"The Hadoop distro currently bundles Tomcat version 6.0.41 by default.  The current Tomcat 6 version is 6.0.44, which includes a few incremental bug fixes.  Let's update our default version so that our users get the latest bug fixes."
HADOOP-12230,"hadoop-project declares duplicate, conflicting curator dependencies","HADOOP-11492 bumped up the curator version in trunk & branch-2, but it looks like there were a couple of extra curator dependencies in branch-2 that didn't get updated, and are still down as 2.6.0

This isn't that serious, maven will have picked the latest one, but its still messy."
HADOOP-12213,Interrupted exception can occur when Client#stop is called,"Its more of a nuisance then a bug, but nevertheless 
{code}
16:16:48,709 ERROR pool-1-thread-1 ipc.Client:195 - Interrupted while waiting for clientExecutorto stop
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2072)
	at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1468)
	at org.apache.hadoop.ipc.Client$ClientExecutorServiceFactory.unrefAndCleanup(Client.java:191)
	at org.apache.hadoop.ipc.Client.stop(Client.java:1235)
	at org.apache.hadoop.ipc.ClientCache.stopClient(ClientCache.java:100)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.close(ProtobufRpcEngine.java:251)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:626)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.close(ApplicationClientProtocolPBClientImpl.java:112)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:621)
	at org.apache.hadoop.io.retry.DefaultFailoverProxyProvider.close(DefaultFailoverProxyProvider.java:57)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.close(RetryInvocationHandler.java:206)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:626)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStop(YarnClientImpl.java:124)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
. . .
{code}
It happens sporadically when stopping YarnClient. 
Looking at the code in Client's 'unrefAndCleanup' its not immediately obvious why and who throws the interrupt but in any event it should not be logged as ERROR. Probably a WARN with no stack trace.
Also, for consistency and correctness you may want to Interrupt current thread as well."
HADOOP-12212,"Hi, I am trying to start the namenode but it keeps showing: Failed to start namenode. java.net.BindException: Address already in use","Hi, I am trying to start the namenode but it keeps showing: Failed to start namenode. java.net.BindException: Address already in use;. netstat -a | grep 9000 returns 
tcp        0      0 *:9000                  *:*                     LISTEN     
tcp6       0      0 [::]:9000               [::]:*                  LISTEN 

Is this normal or do I need to kill one of the processes?

The hdfs-site.xml is given below: <configuration> <property>    <name>dfs.replication</name>    <value>1</value>  </property>  <property>    <name>dfs.namenode.name.dir</name>    <value>file:///usr/local/hdfs/namenode</value>  </property>  <property>    <name>dfs.datanode.data.dir</name>    <value>file:///usr/local/hdfs/datanode</value>  </property> </configuration>


namenode logs are given below:
------------------------------------------
2015-07-10 00:27:02,513 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-07-10 00:27:02,538 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-07-10 00:27:07,549 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-07-10 00:27:09,284 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-07-10 00:27:09,285 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-07-10 00:27:09,339 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-07-10 00:27:09,340 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-07-10 00:27:12,475 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-07-10 00:27:16,632 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-07-10 00:27:17,491 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-07-10 00:27:17,702 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-07-10 00:27:17,876 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-07-10 00:27:17,941 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-07-10 00:27:17,977 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-07-10 00:27:17,977 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-07-10 00:27:17,977 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-07-10 00:27:18,441 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-07-10 00:27:18,525 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-07-10 00:27:18,747 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-07-10 00:27:18,760 INFO org.mortbay.log: jetty-6.1.26
2015-07-10 00:27:20,832 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-07-10 00:27:23,404 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-07-10 00:27:23,416 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-07-10 00:27:24,034 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-07-10 00:27:24,036 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-07-10 00:27:24,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-07-10 00:27:24,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-07-10 00:27:24,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-07-10 00:27:24,854 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Jul 10 00:27:24
2015-07-10 00:27:24,867 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-07-10 00:27:24,883 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-07-10 00:27:24,900 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-07-10 00:27:24,901 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2015-07-10 00:27:25,563 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-07-10 00:27:25,638 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = joe (auth:SIMPLE)
2015-07-10 00:27:25,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-07-10 00:27:25,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-07-10 00:27:25,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-07-10 00:27:25,658 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-07-10 00:27:26,354 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-07-10 00:27:26,354 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-07-10 00:27:26,355 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-07-10 00:27:26,355 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-07-10 00:27:26,993 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2015-07-10 00:27:26,994 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2015-07-10 00:27:26,994 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2015-07-10 00:27:26,994 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-07-10 00:27:27,064 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-07-10 00:27:27,069 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-07-10 00:27:27,070 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-07-10 00:27:27,070 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-07-10 00:27:27,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-07-10 00:27:27,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-07-10 00:27:27,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-07-10 00:27:27,105 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2015-07-10 00:27:27,105 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2015-07-10 00:27:27,105 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2015-07-10 00:27:27,113 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-07-10 00:27:27,113 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: capacity      = 2^16 = 65536 entries
2015-07-10 00:27:27,403 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hdfs/namenode/in_use.lock acquired by nodename 11822@joe-virtual-machine
2015-07-10 00:27:27,882 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /usr/local/hdfs/namenode/current
2015-07-10 00:27:28,446 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-07-10 00:27:28,758 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-07-10 00:27:28,784 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /usr/local/hdfs/namenode/current/fsimage_0000000000000000000
2015-07-10 00:27:28,826 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@fd6cd8 expecting start txid #1
2015-07-10 00:27:28,840 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /usr/local/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002
2015-07-10 00:27:28,912 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/usr/local/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002' to transaction ID 1
2015-07-10 00:27:29,079 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /usr/local/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2015-07-10 00:27:29,164 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-07-10 00:27:29,174 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2015-07-10 00:27:29,854 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-07-10 00:27:29,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2611 msecs
2015-07-10 00:27:33,403 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-07-10 00:27:33,490 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-07-10 00:27:33,625 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state
2015-07-10 00:27:33,628 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2015-07-10 00:27:33,639 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 57 
2015-07-10 00:27:33,642 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /usr/local/hdfs/namenode/current/edits_inprogress_0000000000000000003 -> /usr/local/hdfs/namenode/current/edits_0000000000000000003-0000000000000000004
2015-07-10 00:27:33,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state
2015-07-10 00:27:33,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2015-07-10 00:27:33,885 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-07-10 00:27:33,905 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-07-10 00:27:33,907 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-07-10 00:27:33,907 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-07-10 00:27:33,970 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.net.BindException: Problem binding to [localhost:9000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:721)
	at org.apache.hadoop.ipc.Server.bind(Server.java:425)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:574)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2215)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:938)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:534)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:509)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:783)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:343)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:672)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:645)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:810)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:794)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1487)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1553)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.hadoop.ipc.Server.bind(Server.java:408)
	... 13 more
2015-07-10 00:27:34,004 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-07-10 00:27:34,007 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at joe-virtual-machine/192.168.197.146
************************************************************/"
HADOOP-12191,Bzip2Factory is not thread safe,Bzip2Factory.isNativeBzip2Loaded is not protected from multiple threads calling it simultaneously.  A thread can return false from this method despite logging the fact that was going to return true due to manipulations of the static boolean from another thread calling the same method.
HADOOP-12186,ActiveStandbyElector shouldn't call monitorLockNodeAsync multiple times,"ActiveStandbyElector shouldn't call {{monitorLockNodeAsync}} before StatCallback for previous {{zkClient.exists}} is received.
We saw RM shutdown because ActiveStandbyElector retrying monitorLockNodeAsync exceeded limit. The following is the logs.
Based on the log, it looks like multiple {{monitorLockNodeAsync}} are called at the same time due to back-to-back SyncConnected event received.
The current code doesn't prevent {{zkClient.exists}} from being called before AsyncCallback.StatCallback for previous {{zkClient.exists}} is received.
So the retry for {{monitorLockNodeAsync}} doesn't work correctly sometimes.
{code}
2015-07-01 19:24:12,806 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6674ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:12,919 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:14,704 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:14,704 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43487, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:14,707 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:14,712 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:21,374 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6667ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:21,477 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:22,640 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:22,640 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43526, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:22,641 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:22,642 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:29,310 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6669ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:29,413 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:30,738 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:30,739 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43574, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:30,739 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:30,740 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:37,409 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6670ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:37,512 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:38,979 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:38,979 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43598, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:38,980 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:38,981 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:45,649 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6669ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:45,752 FATAL org.apache.hadoop.ha.ActiveStandbyElector: Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
2015-07-01 19:24:45,855 INFO org.apache.zookeeper.ZooKeeper: Session: 0x14e47693cc20007 closed
2015-07-01 19:25:07,932 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,932 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type EMBEDDED_ELECTOR_FAILED. Cause:
Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
2015-07-01 19:25:07,932 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,932 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,933 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,933 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-07-01 19:25:08,036 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
{code}"
HADOOP-12179,"Test Jira, please ignore",
HADOOP-12107,long running apps may have a huge number of StatisticsData instances under FileSystem,"We observed with some of our apps (non-mapreduce apps that use filesystems) that they end up accumulating a huge memory footprint coming from {{FileSystem$Statistics$StatisticsData}} (in the {{allData}} list of {{Statistics}}).

Although the thread reference from {{StatisticsData}} is a weak reference, and thus can get cleared once a thread goes away, the actual {{StatisticsData}} instances in the list won't get cleared until any of these following methods is called on {{Statistics}}:
- {{getBytesRead()}}
- {{getBytesWritten()}}
- {{getReadOps()}}
- {{getLargeReadOps()}}
- {{getWriteOps()}}
- {{toString()}}

It is quite possible to have an application that interacts with a filesystem but does not call any of these methods on the {{Statistics}}. If such an application runs for a long time and has a large amount of thread churn, the memory footprint will grow significantly.

The current workaround is either to limit the thread churn or to invoke these operations occasionally to pare down the memory. However, this is still a deficiency with {{FileSystem$Statistics}} itself in that the memory is controlled only as a side effect of those operations."
HADOOP-12103,Small refactoring of DelegationTokenAuthenticationFilter to allow code sharing,"This is the hadoop-common portion change for HDFS-8337 patch rev 003.
"
HADOOP-12100,ImmutableFsPermission should not override applyUmask since that method doesn't modify the FsPermission,"ImmutableFsPermission should not override applyUmask since that method doesn't modify the FsPermission.

This bug is currently causing the {{TestRollingLevelDBTimelineStore}} test to fail.  HADOOP-11347 changed some code to use {{applyUmask}}, which exposed this bug.

{noformat}
Running org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore
Tests run: 16, Failures: 0, Errors: 16, Skipped: 0, Time elapsed: 2.65 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore
testGetDomains(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 1.533 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testRelatingToNonExistingEntity(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.085 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testValidateConfig(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.07 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetEntitiesWithPrimaryFilters(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.061 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testRelatingToOldEntityWithoutDomainId(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.055 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testRootDirPermission(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.056 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetSingleEntity(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.05 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testCacheSizes(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.051 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetEntities(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.049 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testRelatingToEntityInSamePut(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.056 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetDomain(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.049 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetEvents(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.049 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetEntitiesWithSecondaryFilters(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.05 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testCheckVersion(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.05 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetEntitiesWithFromId(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.055 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)

testGetEntitiesWithFromTs(org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore)  Time elapsed: 0.049 sec  <<< ERROR!
java.lang.UnsupportedOperationException: null
	at org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission.applyUMask(FsPermission.java:380)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:496)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:551)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:314)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.initFileSystem(RollingLevelDB.java:207)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDB.init(RollingLevelDB.java:200)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceInit(RollingLevelDBTimelineStore.java:321)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.setup(TestRollingLevelDBTimelineStore.java:65)


Results :

Tests in error: 
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation
  TestRollingLevelDBTimelineStore.setup:65 » UnsupportedOperation

Tests run: 16, Failures: 0, Errors: 16, Skipped: 0
{noformat}"
HADOOP-12078,The default retry policy does not handle RetriableException correctly ,"The default policy in RetryUtils does not retry RetriableExceptions even when {{defaultRetryPolicyEnabled}} is true.

This was discovered via an HDFS client failing to retry {{getFileBlockLocations}} after {{checkNNStartup}} failed.
"
HADOOP-12058,Fix dead links to DistCp and Hadoop Archives pages.,The links to DistCp or Hadoop Archives are broken in the commands reference site: [http://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/CommandsManual.html]. The destinations were moved. 
HADOOP-12006,Remove unimplemented option for `hadoop fs -ls` from document in branch-2.7,"{{-t}}, {{-s}}, {{-R}}, and {{-u}} option for {{hadoop fs -ls}} are introduced in HADOOP-8934, which targeted 2.8.0.
It means that the feature is unimplemented in 2.7.0 but documented in http://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/FileSystemShell.html#ls
We should fix the document."
HADOOP-11973,Ensure ZkDelegationTokenSecretManager namespace znodes get created with ACLs,"I recently added an ACL Provider to the curator framework instance I pass to the ZkDelegationTokenSecretManager, and notice some strangeness around ACLs.

I set: ""zk-dt-secret-manager.znodeWorkingPath"" to:
""solr/zkdtsm""

and notice that
/solr/zkdtsm/
/solr/zkdtsm/ZKDTSMRoot
do not have ACLs

but all the znodes under /solr/zkdtsm/ZKDTSMRoot have ACLs.  From adding some logging, it looks like the ACLProvider is never called for /solr/zkdtsm and /solr/zkdtsm/ZKDTSMRoot.  I don't know if that's a Curator or ZkDelegationTokenSecretManager issue."
HADOOP-11966,Variable cygwin is undefined in hadoop-config.sh when executed through hadoop-daemon.sh.,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin.  The logic involves setting a {{cygwin}} flag variable to indicate if the script is executing through Cygwin.  The flag is set in all of the interactive scripts: {{hadoop}}, {{hdfs}}, {{yarn}} and {{mapred}}.  The flag is not set through hadoop-daemon.sh though.  This can cause an erroneous overwrite of {{HADOOP_HOME}} and {{JAVA_LIBRARY_PATH}} inside hadoop-config.sh."
HADOOP-11934,Use of JavaKeyStoreProvider in LdapGroupsMapping causes infinite loop,"I was attempting to use the LdapGroupsMapping code and the JavaKeyStoreProvider at the same time, and hit a really interesting, yet fatal, issue.  The code goes into what ought to have been an infinite loop, were it not for it overflowing the stack and Java ending the loop.  Here is a snippet of the stack; my annotations are at the bottom.

{noformat}
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)
	at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)
	at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)
	at org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)
	at org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:70)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)
	at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)
	at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)
	at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)
	at org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)
	at org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:70)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296){noformat}

Here's my annotation, going from bottom to top.
* Somehow we enter Path.getFileSystem()
* This goes to FileSystem cache stuff, and then it wants the current user
* So we get to UserGroupInformation.getCurrentUser(), which as you can imagine gets to
* getUserToGroupsMappingService and thence to LdapGroupsMapping.setConf().
* That code gets the needed passwords, and we're using the CredentialProvider, so unsurprisingly we get to
* getPasswordFromCredentialProviders() - which chooses the JavaKeyStoreProvider like I told it to.
* The JavaKeyStoreProvider, in its constructor, does ""fs = path.getFileSystem(conf);""
* And guess what, we're back in Path.getFileSystem, where we started at the beginning.

Please let me know if I've somehow configured something incorrectly, but if I have I can't figure out what it is..."
HADOOP-11932, MetricsSinkAdapter hangs when being stopped,"We've seen a situation that one RM hangs on stopping the MetricsSinkAdapter

{code}
""main-EventThread"" daemon prio=10 tid=0x00007f9b24031000 nid=0x2d18 in Object.wait() [0x00007f9afe7eb000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00000000c058dcf8> (a org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1)
        at java.lang.Thread.join(Thread.java:1281)
        - locked <0x00000000c058dcf8> (a org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1)
        at java.lang.Thread.join(Thread.java:1355)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.stop(MetricsSinkAdapter.java:202)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.stopSinks(MetricsSystemImpl.java:472)
        - locked <0x00000000c04cc1a0> (a org.apache.hadoop.metrics2.impl.MetricsSystemImpl)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.stop(MetricsSystemImpl.java:213)
        - locked <0x00000000c04cc1a0> (a org.apache.hadoop.metrics2.impl.MetricsSystemImpl)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.shutdown(MetricsSystemImpl.java:592)
        - locked <0x00000000c04cc1a0> (a org.apache.hadoop.metrics2.impl.MetricsSystemImpl)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.shutdownInstance(DefaultMetricsSystem.java:72)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.shutdown(DefaultMetricsSystem.java:68)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStop(ResourceManager.java:605)
        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
        - locked <0x00000000c0503568> (a java.lang.Object)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.stopActiveServices(ResourceManager.java:1024)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToStandby(ResourceManager.java:1076)
        - locked <0x00000000c03fe3b8> (a org.apache.hadoop.yarn.server.resourcemanager.ResourceManager)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToStandby(AdminService.java:322)
        - locked <0x00000000c0502b10> (a org.apache.hadoop.yarn.server.resourcemanager.AdminService)
        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeStandby(EmbeddedElectorService.java:135)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeStandby(ActiveStandbyElector.java:911)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:428)
        - locked <0x00000000c0718940> (a org.apache.hadoop.ha.ActiveStandbyElector)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
{code}
{code}
""timeline"" daemon prio=10 tid=0x00007f9b34d55000 nid=0x1d93 runnable [0x00007f9b0cbbf000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:152)
        at java.net.SocketInputStream.read(SocketInputStream.java:122)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
        - locked <0x00000000c0f522c8> (a java.io.BufferedInputStream)
        at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78)
        at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106)
        at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116)
        at org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973)
        at org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735)
        at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1098)
        at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398)
        at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
        at org.apache.hadoop.metrics2.sink.timeline.AbstractTimelineMetricsSink.emitMetrics(AbstractTimelineMetricsSink.java:66)
        at org.apache.hadoop.metrics2.sink.timeline.HadoopTimelineMetricsSink.putMetrics(HadoopTimelineMetricsSink.java:203)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:175)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)
        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)
{code}
 looks like the {{sinkThread.interrupt();}} in MetricsSinkAdapter#stop doesn't really interrupt the thread, which cause it to hang at join.

This appears only once."
HADOOP-11902,Prune old javadoc versions from the website.,We have a lot of old versions of javadoc on the website. We should prune the old versions and redirect the old urls to the current versions.
HADOOP-11896,Redesign the releases page on the Hadoop site,"Redesign the Hadoop site to:
* Move the recent releases to the top of the page by reducing the huge table of contents.
* Provide a direct link (via the mirror page) to each release tarball for the last few releases.
* Provide the sha256 for each of the recent release tarballs.
* Provide a direct link the GPG signature for the recent release tarballs.
* Provide directions on how to verify the GPG signature.
* Provide a link to the signatures in https://people.apache.org/keys/group/hadoop.asc"
HADOOP-11891,OsSecureRandom should lazily fill its reservoir,The {{OsSecureRandom}} class that is initialized by on the the {{CryptoCodec}} implementations eagerly fills its reservoir of random bits. This will result in too many file descriptors being opened simply and is not really required if the client already has a random IV and key needed for decryption.
HADOOP-11872,"""hadoop dfs"" command prints message about using ""yarn jar"" on Windows(branch-2 only)","Using the ""hadoop dfs"" command on a branch-2 build prints a message about using yarn jar.

{noformat}
C:\hadoop\hadoop-common-project\hadoop-common\src\main\bin> hadoop.cmd dfs -ls
                       note: please use ""yarn jar"" to launch
                             YARN applications, not this command.
{noformat}"
HADOOP-11868,Invalid user logins trigger large backtraces in server log,"{code}
WARN sso.CookieValidatorHelpers: Cookie has expired by 25364187 msec
WARN server.AuthenticationFilter: Authentication exception: Invalid Cookie
166 org.apache.hadoop.security.authentication.client.AuthenticationException: Invalid Bouncer Cookie
167     at KerberosAuthenticationHandler.bouncerAuthenticate(KerberosAuthenticationHandler.java:94)
168     at AuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:82)
169     at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)
170     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
171     at org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter.doFilter(CrossOriginFilter.java:95)
172     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
173     at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:78)
174     at GzipFilter.doFilter(GzipFilter.java:188)
175     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
176     at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1224)
177     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
178     at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
179     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
180     at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
181     at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
182     at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
183     at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
184     at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
185     at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
186     at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
187     at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
188     at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
189     at org.mortbay.jetty.Server.handle(Server.java:326)
190     at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
191     at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
192     at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
193     at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
194     at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
195     at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
196     at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
 WARN sso.CookieValidatorHelpers: Cookie has expired by 25373197 msec
{code}"
HADOOP-11851,s3n to swallow IOEs on inner stream close,"We've seen a situation where some work was failing from (recurrent) connection reset exceptions.

Irrespective of the root cause, these were surfacing not in the read operations, but when the input stream was being closed -including during a seek()

These exceptions could be caught & logged & warn, rather than trigger immediate failures. It shouldn't matter to the next GET whether the last stream closed prematurely, as long as the new one works"
HADOOP-11837,AuthenticationFilter should destroy SignerSecretProvider in Tomcat deployments,AuthenticationFilter creates SignerSecretProvider if the filter is initialized through Tomcat. The SignerSecretProvider needs to be properly destroyed.
HADOOP-11815,HttpServer2 should destroy SignerSecretProvider when it stops,It is observed that MRAppMaster JVM hungs after unregistered with ResourceManager.
HADOOP-11814,"Reformat hadoop-annotations, o.a.h.classification.tools",RootDocProcessor has some indentation problems. It mixes tabs and spaces for indentation. We may want to fix this. 
HADOOP-11812,Implement listLocatedStatus for ViewFileSystem to speed up split calculation,ViewFileSystem is currently not taking advantage of MAPREDUCE-1981. This causes several x of RPC overhead and added latency.
HADOOP-11802,DomainSocketWatcher thread terminates sometimes after there is an I/O error during requestShortCircuitShm,"In {{DataXceiver#requestShortCircuitShm}}, we attempt to recover from some errors by closing the {{DomainSocket}}.  However, this violates the invariant that the domain socket should never be closed when it is being managed by the {{DomainSocketWatcher}}.  Instead, we should call {{shutdown}} on the {{DomainSocket}}.  When this bug hits, it terminates the {{DomainSocketWatcher}} thread."
HADOOP-11801,Update BUILDING.txt for Ubuntu,ProtocolBuffer is packaged in Ubuntu
HADOOP-11796,Skip TestShellBasedIdMapping.testStaticMapUpdate on Windows,"The test should be skipped on Windows.

{code}
Stacktrace

java.util.NoSuchElementException: null
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:809)
	at java.util.HashMap$EntryIterator.next(HashMap.java:847)
	at java.util.HashMap$EntryIterator.next(HashMap.java:845)
	at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:314)
	at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:306)
	at org.apache.hadoop.security.TestShellBasedIdMapping.testStaticMapUpdate(TestShellBasedIdMapping.java:151)
Standard Output

2015-03-30 00:44:30,267 INFO  security.ShellBasedIdMapping (ShellBasedIdMapping.java:<init>(113)) - User configured user account update time is less than 1 minute. Use 1 minute instead.
2015-03-30 00:44:30,274 INFO  security.ShellBasedIdMapping (ShellBasedIdMapping.java:updateStaticMapping(322)) - Not doing static UID/GID mapping because 'D:\tmp\hadoop-dal\nfs-6561166579146979876.map' does not exist.
2015-03-30 00:44:30,274 ERROR security.ShellBasedIdMapping (ShellBasedIdMapping.java:checkSupportedPlatform(278)) - Platform is not supported:Windows Server 2008 R2. Can't update user map and group map and 'nobody' will be used for any user and group.
2015-03-30 00:44:30,275 INFO  security.ShellBasedIdMapping (ShellBasedIdMapping.java:<init>(113)) - User configured user account update time is less than 1 minute. Use 1 minute instead.
2015-03-30 00:44:30,275 INFO  security.ShellBasedIdMapping (ShellBasedIdMapping.java:updateStaticMapping(322)) - Not doing static UID/GID mapping because 'D:\tmp\hadoop-dal\nfs-6561166579146979876.map' does not exist.
2015-03-30 00:44:30,275 ERROR security.ShellBasedIdMapping (ShellBasedIdMapping.java:checkSupportedPlatform(278)) - Platform is not supported:Windows Server 2008 R2. Can't update user map and group map and 'nobody' will be used for any user and group.
{code}"
HADOOP-11787,OpensslSecureRandom.c pthread_threadid_np usage signature is wrong on 32-bit Mac,"In OpensslSecureRandom.c, pthread_threadid_np is being used with an unsigned long, but the type signature requires a uint64_t."
HADOOP-11777,`hadoop classpath` behaviour different from previous releases,"In 2.6 doing a `hadoop classpath` returns the following

classpath [--glob|--jar <path>|-h|--help] :
  Prints the classpath needed to get the Hadoop jar and the required
  libraries.
  Options:

  --glob       expand wildcards
  --jar <path> write classpath as manifest in jar named <path>
  -h, --help   print help

This is different from earlier releases.
However, passing any argument to the command will return the classpath as earlier
E.g. `hadoop classpath foo`

It would be nice to preserve the original behaviour for scripts that read the classpath using this command.

The fix seems to be a one line change in
hadoop-common-project/hadoop-common/src/main/bin/hadoop

"
HADOOP-11776,jdiff is broken in Hadoop 2,Seems like we haven't touch the API files from jdiff under dev-support for a while. For now we're missing the jdiff API files for hadoop 2. We're also missing YARN when generating the jdiff API files. 
HADOOP-11763,RM in insecure model get start failure after HADOOP-10670.,"TestDistributedShell get failed due to RM start failure.
The log exception:
{code}
2015-03-27 14:43:17,190 WARN  [RM-0] mortbay.log (Slf4jLog.java:warn(89)) - Failed startup of context org.mortbay.jetty.webapp.WebAppContext@2d2d0132{/,file:/Users/jdu/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-comm
on/target/classes/webapps/cluster}
javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/jdu/hadoop-http-auth-signature-secret
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)
        at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)
        at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)
        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
        at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)
        at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)
        at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)
        at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
        at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
        at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)
        at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)
        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
        at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)
        at org.mortbay.jetty.Server.doStart(Server.java:224)
        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
        at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:989)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1089)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.MiniYARNCluster$2.run(MiniYARNCluster.java:312)
Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/jdu/hadoop-http-auth-signature-secret
        at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)
        ... 23 more
{code}"
HADOOP-11761,Fix findbugs warnings in org.apache.hadoop.security.authentication,"As discovered in HADOOP-11748, we need to fix the findbugs warnings in org.apache.hadoop.security.authentication. "
HADOOP-11757,NFS gateway should shutdown when it can't start UDP or TCP server,"Unlike the Portmap, Nfs3 class does shutdown when the service can't start."
HADOOP-11754,RM fails to start in non-secure mode due to authentication filter failure,"RM fails to start in the non-secure mode with the following exception:

{noformat}
2015-03-25 22:02:42,526 WARN org.mortbay.log: failed RMAuthenticationFilter: javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret
2015-03-25 22:02:42,526 WARN org.mortbay.log: Failed startup of context org.mortbay.jetty.webapp.WebAppContext@6de50b08{/,jar:file:/Users/sjlee/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/cluster}
javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)
	at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)
	at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)
	at org.mortbay.jetty.Server.doStart(Server.java:224)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret
	at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)
	... 23 more
...
2015-03-25 22:02:42,538 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager
org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
Caused by: java.io.IOException: Problem in starting http server. Server handlers failed
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)
	... 4 more
{noformat}

This is likely a regression introduced by HADOOP-10670."
HADOOP-11748,The secrets of auth cookies should not be specified in configuration in clear text,"Based on the discussion on HADOOP-10670, this jira proposes to remove {{StringSecretProvider}} as it opens up possibilities for misconfiguration and security vulnerabilities.

{quote}

My understanding is that the use case of inlining the secret is never supported. The property is used to pass the secret internally. The way it works before HADOOP-10868 is the following:

* Users specify the initializer of the authentication filter in the configuration.
* AuthenticationFilterInitializer reads the secret file. The server will not start if the secret file does not exists. The initializer will set the property if it read the file correctly.
*There is no way to specify the secret in the configuration out-of-the-box – the secret is always overwritten by AuthenticationFilterInitializer.

{quote}

"
HADOOP-11738,Fix a link of Protocol Buffers 2.5 for download in BUILDING.txt ,"From REEF-216:

{quote}
Google recently switched off Google Code. They transferred the
Protocol Buffers project to
[GitHub|https://github.com/google/protobuf], and binaries are
available from [Google's developer
page|https://developers.google.com/protocol-buffers/docs/downloads].
However, only the most recent version is available. We use version 2.5
to be compatible with Hadoop. That version isn't available for
download. 
{quote}

Our BUILDING.txt has same issue."
HADOOP-11730,Regression: s3n read failure recovery broken,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output.

Here's a stack trace as an example.

{quote}
2015-03-13 20:17:24,835 [TezChild] INFO  org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor - Starting output org.apache.tez.mapreduce.output.MROutput@52008dbd to vertex scope-12
2015-03-13 20:17:24,866 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream threw an exception
org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 296587138; received: 155648
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184)
	at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)
	at org.jets3t.service.io.InterruptableInputStream.read(InterruptableInputStream.java:78)
	at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.read(HttpMethodReleaseInputStream.java:146)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.read(NativeS3FileSystem.java:145)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:259)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:204)
	at org.apache.tez.mapreduce.lib.MRReaderMapReduce.next(MRReaderMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:106)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:246)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNextTuple(POFilter.java:91)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:117)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:313)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:192)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-13 20:17:24,867 [TezChild] INFO  org.apache.hadoop.fs.s3native.NativeS3FileSystem - Received IOException while reading 'user/hadoop/tsato/readlarge/input/cloudian-s3.log.20141119', attempting to reopen.
2015-03-13 20:17:24,867 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream is fully consumed
2015-03-13 20:17:24,868 [TezChild] INFO  org.apache.tez.dag.app.TaskAttemptListenerImpTezDag - Commit go/no-go request from attempt_1426245338920_0001_1_00_000004_0
2015-03-13 20:17:24,868 [TezChild] INFO  org.apache.tez.dag.app.dag.impl.TaskImpl - attempt_1426245338920_0001_1_00_000004_0 given a go for committing the task output.
{quote}

It seems this is a regression, which was introduced by the following optimizations.

https://issues.apache.org/jira/browse/HADOOP-10589
https://issues.apache.org/jira/browse/HADOOP-10457

Also, test cases should be reviewed so that it covers this scenario."
HADOOP-11729,Fix link to cgroups doc in site.xml,s/NodeManagerCGroups/NodeManagerCgroups/
HADOOP-11724,DistCp throws NPE when the target directory is root.,"Distcp throws NPE when the target directory is root. It is due to {{CopyCommitter#cleanupTempFiles}} attempts to delete parent directory of root, which is {{null}}:

{code}
$ hadoop distcp pom.xml hdfs://localhost/
15/03/17 11:17:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/03/17 11:17:45 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, maxMaps=20, sslConfigurationFile='null', copyStrategy='uniformsize', sourceFileListing=null, sourcePaths=[pom.xml], targetPath=hdfs://localhost/, targetPathExists=true, preserveRawXattrs=false}
15/03/17 11:17:45 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
15/03/17 11:17:45 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
15/03/17 11:17:45 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb
15/03/17 11:17:45 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor
15/03/17 11:17:45 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
15/03/17 11:17:45 INFO mapreduce.JobSubmitter: number of splits:1
15/03/17 11:17:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local992233322_0001
15/03/17 11:17:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
15/03/17 11:17:46 INFO tools.DistCp: DistCp job-id: job_local992233322_0001
15/03/17 11:17:46 INFO mapreduce.Job: Running job: job_local992233322_0001
15/03/17 11:17:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null
15/03/17 11:17:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/03/17 11:17:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.tools.mapred.CopyCommitter
15/03/17 11:17:46 INFO mapred.LocalJobRunner: Waiting for map tasks
15/03/17 11:17:46 INFO mapred.LocalJobRunner: Starting task: attempt_local992233322_0001_m_000000_0
15/03/17 11:17:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/03/17 11:17:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.
15/03/17 11:17:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null
15/03/17 11:17:46 INFO mapred.MapTask: Processing split: file:/tmp/hadoop/mapred/staging/lei2046334351/.staging/_distcp-1889397390/fileList.seq:0+220
15/03/17 11:17:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
15/03/17 11:17:46 INFO mapred.CopyMapper: Copying file:/Users/lei/work/cloudera/s3a_cp_target/pom.xml to hdfs://localhost/pom.xml
15/03/17 11:17:46 INFO mapred.CopyMapper: Skipping copy of file:/Users/lei/work/cloudera/s3a_cp_target/pom.xml to hdfs://localhost/pom.xml
15/03/17 11:17:46 INFO mapred.LocalJobRunner:
15/03/17 11:17:46 INFO mapred.Task: Task:attempt_local992233322_0001_m_000000_0 is done. And is in the process of committing
15/03/17 11:17:46 INFO mapred.LocalJobRunner:
15/03/17 11:17:46 INFO mapred.Task: Task attempt_local992233322_0001_m_000000_0 is allowed to commit now
15/03/17 11:17:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_local992233322_0001_m_000000_0' to file:/tmp/hadoop/mapred/staging/lei2046334351/.staging/_distcp-1889397390/_logs/_temporary/0/task_local992233322_0001_m_000000
15/03/17 11:17:46 INFO mapred.LocalJobRunner: Copying file:/Users/lei/work/cloudera/s3a_cp_target/pom.xml to hdfs://localhost/pom.xml
15/03/17 11:17:46 INFO mapred.Task: Task 'attempt_local992233322_0001_m_000000_0' done.
15/03/17 11:17:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local992233322_0001_m_000000_0
15/03/17 11:17:46 INFO mapred.LocalJobRunner: map task executor complete.
15/03/17 11:17:46 INFO mapred.CopyCommitter: Remove parent: null for hdfs://localhost/
15/03/17 11:17:46 WARN mapred.CopyCommitter: Unable to cleanup temp files
java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:104)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.tools.mapred.CopyCommitter.deleteAttemptTempFiles(CopyCommitter.java:141)
	at org.apache.hadoop.tools.mapred.CopyCommitter.cleanupTempFiles(CopyCommitter.java:130)
	at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:83)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:538)
15/03/17 11:17:46 INFO mapred.CopyCommitter: Cleaning up temporary work folder: file:/tmp/hadoop/mapred/staging/lei2046334351/.staging/_distcp-1889397390
15/03/17 11:17:47 INFO mapreduce.Job: Job job_local992233322_0001 running in uber mode : false
15/03/17 11:17:47 INFO mapreduce.Job:  map 100% reduce 0%
15/03/17 11:17:47 INFO mapreduce.Job: Job job_local992233322_0001 completed successfully
15/03/17 11:17:47 INFO mapreduce.Job: Counters: 22
	File System Counters
		FILE: Number of bytes read=103917
		FILE: Number of bytes written=363277
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=0
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1
		Map output records=1
		Input split bytes=151
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=14
		Total committed heap usage (bytes)=163577856
	File Input Format Counters
		Bytes Read=252
	File Output Format Counters
		Bytes Written=70
	org.apache.hadoop.tools.mapred.CopyMapper$Counter
		BYTESSKIPPED=23491
		SKIP=1
{code}

The distcp task can still success. "
HADOOP-11722,Some Instances of Services using ZKDelegationTokenSecretManager go down when old token cannot be deleted,"The delete node code in {{ZKDelegationTokenSecretManager}} is as follows :
{noformat}
       while(zkClient.checkExists().forPath(nodeRemovePath) != null){
          zkClient.delete().guaranteed().forPath(nodeRemovePath);
       }
{noformat}

When instances of a Service using {{ZKDelegationTokenSecretManager}} try deleting a node simutaneously, It is possible that all of them enter into the while loop in which case, all peers will try to delete the node.. Only 1 will succeed and the rest will throw an exception.. which will bring down the node.

The Exception is as follows :
{noformat}
2015-03-15 10:24:54,000 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception
java.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28
	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)
	at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)
	... 4 more
{noformat}  "
HADOOP-11720,[JDK8] Fix javadoc errors caused by incorrect or illegal tags in hadoop-tools,"""mvn package -Pdist -DskipTests"" fails with JDK8, caused by incorrect or illegal tags in doc comments."
HADOOP-11714,Add more trace log4j messages to SpanReceiverHost,Add more trace log4j messages to SpanReceiverHost
HADOOP-11710,Make CryptoOutputStream behave like DFSOutputStream wrt synchronization,"per discussion on parent, as an intermediate solution make CryptoOutputStream behave like DFSOutputStream"
HADOOP-11693,Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.,"One of our customers' production HBase clusters was periodically throttled by Azure storage, when HBase was archiving old WALs. HMaster aborted the region server and tried to restart it.

However, since the cluster was still being throttled by Azure storage, the upcoming distributed log splitting also failed. Sometimes hbase:meta table was on this region server and finally showed offline, which cause the whole cluster in bad state.

{code}
2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error:
ABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller
Cause:
org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)
	at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)
	at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)
	... 8 more

2015-03-01 18:43:29,072 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN
java.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)
	at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)
	... 4 more
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)
	... 11 more

Sun Mar 01 18:59:51 GMT 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@aa93ac7, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
	at java.lang.Thread.run(Thread.java:745)
{code}

When archiving old WALs, WASB will do rename operation by copying src blob to destination blob and deleting the src blob. Copy blob is very costly in Azure storage and during Azure storage gc, it will be highly likely throttled. The throttling by Azure storage usually ends within 15mins. Current WASB retry policy is exponential retry, but only last at most for 2min. Short term fix will be adding a more intensive exponential retry when copy blob is throttled."
HADOOP-11691,X86 build of libwinutils is broken,"Hadoop-9922 recently fixed x86 build. After YARN-2190 compiling x86 results in error:
{code}
(Link target) ->
  E:\HW\project\hadoop-common\hadoop-common-project\hadoop-common\target/winutils/hadoopwinutilsvc_s.obj : fatal error LNK1112: module machine type 'x64' conflicts with target machine type 'X86' [E:\HW\project\hadoop-common\hadoop-common-project\hadoop-common\src\main\winutils\winutils.vcxproj]
{code}
"
HADOOP-11686,MiniKDC cannot change ORG_NAME or ORG_DOMAIN,"We addressing HBASE-7781 I found that I can not change the default realm EXAMPLE.COM. I will get a 'Client not found in Kerberos database (6) - Client not found in Kerberos database' if I change it.

This is because we do not set searchBaseDn when starting kdc, so it always search from ServerDNConstants.USER_EXAMPLE_COM_DN which is 'ou=users,dc=example,dc=com'."
HADOOP-11680,Deduplicate jars in convenience binary distribution,"Pulled from discussion on HADOOP-11656 Colin wrote:

{quote}
bq. Andrew wrote: One additional note related to this, we can spend a lot of time right now distributing 100s of MBs of jar dependencies when launching a YARN job. Maybe this is ameliorated by the new shared distributed cache, but I've heard this come up quite a bit as a complaint. If we could meaningfully slim down our client, it could lead to a nice win.

I'm frustrated that nobody responded to my earlier suggestion that we de-duplicate jars. This would drastically reduce the size of our install, and without rearchitecting anything.
In fact I was so frustrated that I decided to write a program to do it myself and measure the delta. Here it is:

Before:
{code}
du -h /h
249M    /h
{code}
After:
{code}
du -h /h
140M    /h
{code}

Seems like deduplicating jars would be a much better project than splitting into a client jar, if we really cared about this.
<snip>
{quote}"
HADOOP-11674,oneByteBuf in CryptoInputStream and CryptoOutputStream should be non static,"A common optimization in the io classes for Input/Output Streams is to save a single length-1 byte array to use in single byte read/write calls.

CryptoInputStream and CryptoOutputStream both attempt to follow this practice but mistakenly mark the array as static. That means that only a single instance of each can be present in a JVM safely."
HADOOP-11670,Regression: s3a auth setup broken ,"One big advantage provided by the s3a filesystem is the ability to use an IAM instance profile in order to authenticate when attempting to access an S3 bucket from an EC2 instance. This eliminates the need to deploy AWS account credentials to the instance or to provide them to Hadoop via the fs.s3a.awsAccessKeyId and fs.s3a.awsSecretAccessKey params.

The patch submitted to resolve HADOOP-10714 breaks this behavior by using the S3Credentials class to read the value of these two params. The change in question is presented below:

S3AFileSystem.java, lines 161-170:
{code}
    // Try to get our credentials or just connect anonymously
    S3Credentials s3Credentials = new S3Credentials();
    s3Credentials.initialize(name, conf);

    AWSCredentialsProviderChain credentials = new AWSCredentialsProviderChain(
        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),
                                        s3Credentials.getSecretAccessKey()),
        new InstanceProfileCredentialsProvider(),
        new AnonymousAWSCredentialsProvider()
    );
{code}

As you can see, the getAccessKey() and getSecretAccessKey() methods from the S3Credentials class are now used to provide constructor arguments to BasicAWSCredentialsProvider. These methods will raise an exception if the fs.s3a.awsAccessKeyId or fs.s3a.awsSecretAccessKey params are missing, respectively. If a user is relying on an IAM instance profile to authenticate to an S3 bucket and therefore doesn't supply values for these params, they will receive an exception and won't be able to access the bucket."
HADOOP-11666,Revert the format change of du output introduced by HADOOP-6857,"HADOOP-6857 did two things about `du` at the same time.
* Fix a bug for querying snapshottable directory
* Change the output format (incompatible change)

This issue is to revert the latter from branch-2 for keeping compatibility. The bug fix is left."
HADOOP-11663,Remove description about Java 6 from docs,"{{hadoop-auth/BuildingIt.md}} has:
{noformat}
Hadoop Auth, Java HTTP SPNEGO - Building It
Requirements
Java 6+
{noformat}"
HADOOP-11658,Externalize io.compression.codecs property,"A minor code refactoring, externalizing io.compression.codecs as configuration key."
HADOOP-11648,Set DomainSocketWatcher thread name explicitly,"while working at HADOOP-11604, seems the current DomainSocketWatcher thread name is not set explicitly, e.g. in our cluster, the format is like: Thread-25,  Thread-303670 or sth else. Here Thread-25 seems came from Datanode.initDataXceiver, and once this thread die, the Xceiver leak will be found. I think it'd better to set the thread name, so we can debug issue easier in further."
HADOOP-11642,Upgrade azure sdk version from 0.6.0 to 2.0.0,"hadoop-azure uses unsupported version of azure sdk (0.6.0).Upgrade it to 2.0.0
Breaking changes :https://github.com/Azure/azure-storage-java/blob/master/BreakingChanges.txt"
HADOOP-11639,Clean up Windows native code compilation warnings related to Windows Secure Container Executor.,YARN-2198 introduced additional code in Hadoop Common to support the NodeManager {{WindowsSecureContainerExecutor}}.  The patch introduced new compilation warnings that we need to investigate and resolve.
HADOOP-11638,OpensslSecureRandom.c pthreads_thread_id should support FreeBSD and Solaris in addition to Linux,"In OpensslSecureRandom.c you use Linux-specific syscall gettid():
static unsigned long pthreads_thread_id(void)
{
return (unsigned long)syscall(SYS_gettid);
}

Man page says:
gettid()  is Linux-specific and should not be used in programs that are
intended to be portable.


This breaks hadoop-2.6.0 compilation on FreeBSD (may be on other OSes too).
"
HADOOP-11634,Description of webhdfs' principal/keytab should switch places each other," *Need to interchnage Following Note for principal and keytab* 

{noformat}
Parameter	                                             Value	                    Notes
dfs.web.authentication.kerberos.principal	http/_HOST@REALM.TLD               Kerberos keytab file for the WebHDFS.
dfs.web.authentication.kerberos.keytab	/etc/security/keytab/http.service.keytab	Kerberos principal name for WebHDFS.
{noformat}
"
HADOOP-11633,Convert remaining branch-2 .apt.vm files to markdown,"We should convert the remaining branch-2 .apt.vm files to markdown.

Excluding the yarn files, which are covered by YARN-3168, we have remaining:
{code}
cmccabe@keter:~/hadoop> find -name '*.apt.vm'
./hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm
./hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm
./hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/ServerSetup.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/EncryptedShuffle.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/HadoopStreaming.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/MapredAppMasterRest.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/MapReduceTutorial.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/MapredCommands.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/MapReduce_Compatibility_Hadoop1_Hadoop2.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/DistributedCacheDeploy.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/apt/PluggableShuffleAndPluggableSort.apt.vm
./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/site/apt/HistoryServerRest.apt.vm
./hadoop-common-project/hadoop-common/src/site/apt/CommandsManual.apt.vm
./hadoop-common-project/hadoop-common/src/site/apt/ClusterSetup.apt.vm
./hadoop-common-project/hadoop-auth/src/site/apt/Configuration.apt.vm
./hadoop-common-project/hadoop-auth/src/site/apt/index.apt.vm
./hadoop-common-project/hadoop-auth/src/site/apt/BuildingIt.apt.vm
./hadoop-common-project/hadoop-auth/src/site/apt/Examples.apt.vm
./hadoop-common-project/hadoop-kms/src/site/apt/index.apt.vm
./hadoop-tools/hadoop-openstack/src/site/apt/index.apt.vm
./hadoop-tools/hadoop-sls/src/site/apt/SchedulerLoadSimulator.apt.vm
./hadoop-project/src/site/apt/index.apt.vm
{code}"
HADOOP-11632,Cleanup Find.java to remove SupressWarnings annotations,There are some SuppressWarnings annotations in Find.java. We should fix them.
HADOOP-11629,WASB filesystem should not start BandwidthGaugeUpdater if fs.azure.skip.metrics set to true,"In Hadoop-11248 we added configuration ""fs.azure.skip.metrics"". If set to true, we do not register Azure FileSystem metrics with the metrics system. However, BandwidthGaugeUpdater object is still created in AzureNativeFileSystemStore, resulting in unnecessary threads being spawned.

Under heavy load the system could be busy dealing with these threads and GC has to work on removing the thread objects. E.g. When multiple WebHCat clients submitting jobs to WebHCat server, we observed that the WebHCat server spawns ~400 daemon threads, which slows down the server and sometimes cause timeout.
"
HADOOP-11620,Add support for load balancing across a group of KMS for HA,"This patch needs to add support for :
* specification of multiple hostnames in the kms key provider uri
* KMS client to load balance requests across the hosts specified in the kms keyprovider uri."
HADOOP-11619,FTPFileSystem should override getDefaultPort,FTPFileSystem should override FileSystem#getDefaultPort to return FTP.DEFAULT_PORT 
HADOOP-11618,DelegateToFileSystem erroneously uses default FS's port in constructor  ,"DelegateToFileSystem constructor has the following code:

{code}
    super(theUri, supportedScheme, authorityRequired,
        FileSystem.getDefaultUri(conf).getPort());
{code}

The default port should be taken from theFsImpl instead.

{code}
    super(theUri, supportedScheme, authorityRequired,
        theFsImpl.getDefaultPort());
{code}"
HADOOP-11615,Update ServiceLevelAuth.md for YARN,"JobTracker should be ResourceManager, and {{hadoop mradmin}} should be {{yarn rmadmin}} in ServiceLevelAuth.md.
{code}
The service-level authorization configuration for the NameNode and JobTracker can be changed without restarting either of the Hadoop master daemons. The cluster administrator can change `$HADOOP_CONF_DIR/hadoop-policy.xml` on the master nodes and instruct the NameNode and JobTracker to reload their respective configurations via the `-refreshServiceAcl` switch to `dfsadmin` and `mradmin` commands respectively.

Refresh the service-level authorization configuration for the NameNode:

       $ bin/hadoop dfsadmin -refreshServiceAcl

Refresh the service-level authorization configuration for the JobTracker:

       $ bin/hadoop mradmin -refreshServiceAcl
{code}"
HADOOP-11612,Workaround for Curator's ChildReaper requiring Guava 15+,"HADOOP-11492 upped the Curator version to 2.7.1, which makes the {{ChildReaper}} class use a method that only exists in newer versions of Guava (we have 11.0.2, and it needs 15+).  As a workaround, we can copy the {{ChildReaper}} class into hadoop-common and make a minor modification to allow it to work with Guava 11.

The {{ChildReaper}} is used by Curator to cleanup old lock znodes.  Curator locks are needed by YARN-2942."
HADOOP-11609,Correct credential commands info in CommandsManual.html#credential,"
 ""-i"" is not supported, so would you remove ""-i"",,, ""
-v"" should be undocumented. The option is used only by test.

{noformat}
create alias [-v value][-provider provider-path]	Prompts the user for a credential to be stored as the given alias when a value is not provided via -v. The hadoop.security.credential.provider.path within the core-site.xml file will be used unless a -provider is indicated.
delete alias [-i][-provider provider-path]	Deletes the credential with the provided alias and optionally warns the user when --interactive is used. The hadoop.security.credential.provider.path within the core-site.xml file will be used unless a -provider is indicated.
list [-provider provider-path]	Lists all of the credential aliases The hadoop.security.credential.provider.path within the core-site.xml file will be used unless a -provider is indicated.
{noformat}"
HADOOP-11607,Reduce log spew in S3AFileSystem,"{{S3AFileSystem}} generates INFO level logs in {{open}} and {{rename}}, which are not necessary."
HADOOP-11605,FilterFileSystem#create with ChecksumOpt should propagate it to wrapped FS,"Current create code
{code}
  @Override
  public FSDataOutputStream create(Path f,
        FsPermission permission,
        EnumSet<CreateFlag> flags,
        int bufferSize,
        short replication,
        long blockSize,
        Progressable progress,
        ChecksumOpt checksumOpt) throws IOException {
    return fs.create(f, permission,
      flags, bufferSize, replication, blockSize, progress);
  }
{code}

does not propagate ChecksumOpt. However, it should be up to the wrapped FS implementation (default is to ignore)."
HADOOP-11604,Prevent ConcurrentModificationException while closing domain sockets during shutdown of DomainSocketWatcher thread.,"Our product cluster hit the Xceiver limit even w/ HADOOP-10404 & HADOOP-11333, i found it was caused by DomainSocketWatcher.watcherThread gone. Attached is a possible fix, please review, thanks"
HADOOP-11602,Fix toUpperCase/toLowerCase to use Locale.ENGLISH,"String#toLowerCase()/toUpperCase() without a locale argument can occur unexpected behavior based on the locale. It's written in [Javadoc|http://docs.oracle.com/javase/7/docs/api/java/lang/String.html#toLowerCase()]:
{quote}
For instance, ""TITLE"".toLowerCase() in a Turkish locale returns ""t\u0131tle"", where '\u0131' is the LATIN SMALL LETTER DOTLESS I character
{quote}

This issue is derived from HADOOP-10101.
"
HADOOP-11600,Fix up source codes to be compiled with Guava 17.0,"Removing usage of Guava's deprecated or missing methods in recent version(17.0) without updating pom file. This JIRA targets branch-2. On this JIRA, org.apache.hadoop.util.LimitInputStream will be used instead of com.google.common.io.LimitInputStream which is removed in Guava 15.0.

In the latest release of Guava(18.0), following changes are done:
* Objects#ToStringHelper was removed and we need to use MoreObjects#ToStringHelper. However, MoreObjects#ToStringHelper has been introduced since 18.0.
* Enums.valueOfFunction was removed and need to be replaced with Enums.stringConverter. However, MoreObjects#ToStringHelper has been introduced since 16.0.
* MoreExecutors.sameThreadExecutor() was removed at 18.0 and need to be replaced with MoreExecutors.newDirectExecutorService. However, MoreExecutors.newDirectExecutorService has been introduced since 18.0. 

We'll do these changes on HADOOP-10101."
HADOOP-11599,Client#getTimeout should use IPC_CLIENT_PING_DEFAULT when IPC_CLIENT_PING_KEY is not configured.,"Client#getTimeout should use IPC_CLIENT_PING_DEFAULT instead of  hard-coded value (true) when IPC_CLIENT_PING_KEY is not configured.
{code}
    if (!conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY, true)) {
{code}"
HADOOP-11595,Add default implementation for AbstractFileSystem#truncate,"As [~cnauroth] commented in HADOOP-11510, we should add a default implementation for AbstractFileSystem#truncate to avoid backwards-compatibility"
HADOOP-11592,"IPC error extraction fails ""getLength on uninitialized RpcWrapper""","I'm  seeing {{java.lang.IllegalArgumentException: getLength on uninitialized RpcWrapper}} in an operation; looking at the stack this is happening *in the code designed to extract exception text*.

This means the underlying exception is being lost"
HADOOP-11589,NetUtils.createSocketAddr should trim the input URI,"NetUtils.createSocketAddr does not trim the input URI, should be trimmed.
HDFS-7684 and HADOOP-9869 are trying to trim some URIs to be passed to the method, however, not all of the inputs have been trimmed already."
HADOOP-11587,TestMapFile#testMainMethodMapFile creates test files in hadoop-common project root,"After running TestMapFile#testMainMethodMapFile, two files (data and index) and a directory remain in hadoop-common project root dir. 

hadoop-common-project$ git status

Untracked files:
  (use ""git add <file>..."" to include in what will be committed)

  hadoop-common/mainMethodMapFile.mapfile/

...

hadoop-common-project$ tree hadoop-common/mainMethodMapFile.mapfile/
hadoop-common/mainMethodMapFile.mapfile/
├── data
└── index

The fix is to use ""path"" instead of ""mainMethodMapFile.mapfile"" as output file when calling MapFile.Main(). I will post a patch soon for it.
"
HADOOP-11586,Update use of Iterator to Iterable in AbstractMetricsContext.java,"Found these using the IntelliJ Findbugs-IDEA plugin, which uses findbugs3."
HADOOP-11584,s3a file block size set to 0 in getFileStatus,"The consequence is that mapreduce probably is not splitting s3a files in the expected way. This is similar to HADOOP-5861 (which was for s3n, though s3n was passing 5G rather than 0 for block size).

FileInputFormat.getSplits() relies on the FileStatus block size being set:
{code}
        if (isSplitable(job, path)) {
          long blockSize = file.getBlockSize();
          long splitSize = computeSplitSize(blockSize, minSize, maxSize);
{code}

However, S3AFileSystem does not set the FileStatus block size field. From S3AFileStatus.java:
{code}
  // Files
  public S3AFileStatus(long length, long modification_time, Path path) {
    super(length, false, 1, 0, modification_time, path);
    isEmptyDirectory = false;
  }
{code}

I think it should use S3AFileSystem.getDefaultBlockSize() for each file's block size (where it's currently passing 0)."
HADOOP-11579,Documentation for truncate,"With the addition of a major new feature to filesystems, the filesystem specification in hadoop-common/site is now out of sync. 

This means that
# there's no strict specification of what it should do
# you can't derive tests from that specification
# other people trying to implement the API will have to infer what to do from the HDFS source
# there's no way to decide whether or not the HDFS implementation does what it is intended.
# without matching tests against the raw local FS, differences between the HDFS impl and the Posix standard one won't be caught until it is potentially too late to fix.

The operation should be relatively easy to define (after a truncate, the files bytes [0...len-1] must equal the original bytes, length(file)==len, etc)

The truncate tests already written could then be pulled up into contract tests which any filesystem implementation can run against."
HADOOP-11571,Über-jira: S3a stabilisation phase I,"s3a shipped in 2.6; now its out various corner cases, scale and error handling issues are surfacing. 

fix them before 2.7 ships"
HADOOP-11570,S3AInputStream.close() downloads the remaining bytes of the object from S3,"Currently, S3AInputStream.close() calls S3Object.close().  But, S3Object.close() will read the remaining bytes of the S3 object, potentially transferring a lot of bytes from S3 that are discarded.  Instead, the wrapped stream should be aborted to avoid transferring discarded bytes (unless the preceding read() finished at contentLength).  For example, reading only the first byte of a 1 GB object and then closing the stream will result in all 1 GB transferred from S3."
HADOOP-11569,Provide Merge API for MapFile to merge multiple similar MapFiles to one MapFile,"If there are multiple similar MapFiles of the same keyClass and value classes, then these can be merged together to One MapFile to allow search easier.

Provide an API  similar to {{SequenceFile#merge()}}.
Merging will be easy with the fact that MapFiles are already sorted."
HADOOP-11558,Fix dead links to doc of hadoop-tools,Some links to dosc of hadoop-tools are dead by HADOOP-10976.
HADOOP-11549,flaky test detection tool failed to handle special control characters in test result,"When running the tool from HADOOP-11045 on latest Hadoop-hdfs-trunk job, I'm seeing a problem here:

{code}
[yzhang@localhost jenkinsftf]$ ./determine-flaky-tests-hadoop.py -j Hadoop-hdfs-trunk 
****Recently FAILED builds in url: https://builds.apache.org/job/Hadoop-hdfs-trunk
    THERE ARE 5 builds (out of 6) that have failed tests in the past 14 days, as listed below:

===>https://builds.apache.org/job/Hadoop-hdfs-trunk/2026/testReport (2015-02-04 03:30:00)
    Could not open testReport, check https://builds.apache.org/job/Hadoop-hdfs-trunk/2026/Console for why it was reported failed
...
{code}

I saw that the testReport can actually be opened from browser. After looking, I found that HDFS-7287 fix added the following test code:
{code}
//Create a directory whose name should be escaped in XML
Path invalidXMLDir = new Path(""/dirContainingInvalidXMLChar\u0000here"");
hdfs.mkdirs(invalidXMLDir);
...
{code}
And the output from this code caused the tool to choke. 

I found a solution here and I'm attaching a patch.
 "
HADOOP-11548,checknative should display a nicer error message when openssl support is not compiled in,"checknative should display a nicer error message when openssl support is not compiled in.  Currently, it displays this:

{code}
[cmccabe@keter hadoop]$ hadoop checknative
14/12/12 14:08:43 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native
14/12/12 14:08:43 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /usr/lib/hadoop/lib/native/libhadoop.so.1.0.0
zlib:    true /lib64/libz.so.1
snappy:  true /usr/lib64/libsnappy.so.1
lz4:     true revision:99
bzip2:   true /lib64/libbz2.so.1
openssl: false org.apache.hadoop.crypto.OpensslCipher.initIDs()V
{code}

Instead, we should display something like this, if openssl is not supported by the current build:
{code}
openssl: false Hadoop was built without openssl support.
{code}"
HADOOP-11547,hadoop-common native compilation fails on Windows due to missing support for __attribute__ declaration.,"HADOOP-11403 made a change to include exception.h in NativeIO.c.  This header includes use of the non-standard gcc {{\_\_attribute\_\_}} declaration, and thus fails compilation on Windows."
HADOOP-11546,Checkstyle failing: Unable to instantiate DoubleCheckedLockingCheck,"HDFS builds are failing in jenkins
{code}
Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.12.1:checkstyle (default-cli) on project hadoop-hdfs: An error has occurred in Checkstyle report generation. Failed during checkstyle configuration: cannot initialize module TreeWalker - Unable to instantiate DoubleCheckedLocking: Unable to instantiate DoubleCheckedLockingCheck -
{code}"
HADOOP-11545,"ArrayIndexOutOfBoundsException is thrown with ""hadoop credential list -provider""","Scenario:
========
Please run the following command . dn't give the provider path.

{noformat}
[hdfs@host194 bin]$ ./hadoop credential list -provider
java.lang.ArrayIndexOutOfBoundsException: 2
        at org.apache.hadoop.security.alias.CredentialShell.init(CredentialShell.java:117)
        at org.apache.hadoop.security.alias.CredentialShell.run(CredentialShell.java:63)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:427)
{noformat}"
HADOOP-11544,Remove unused configuration keys for tracing,CommonConfigurationKeys.HADOOP_TRACE_SAMPLER* are no longer used.
HADOOP-11543,Improve help message for hadoop/yarn command,Pls check the snapshot attached
HADOOP-11535,TableMapping related tests failed due to 'successful' resolving of invalid test hostname,"When mvn test in my environment, it reported the following.
{noformat}
Failed tests: 
  TestTableMapping.testClearingCachedMappings:144 expected:</[rack1]> but was:</[default-rack]>
  TestTableMapping.testTableCaching:79 expected:</[rack1]> but was:</[default-rack]>
  TestTableMapping.testResolve:56 expected:</[rack1]> but was:</[default-rack]>
{noformat}

It's caused by the good resolving for the 'bad test' hostname 'a.b.c' as follows.
{noformat}
[drankye@zkdesk hadoop-common-project]$ ping a.b.c
PING a.b.c (220.250.64.228) 56(84) bytes of data.
{noformat}

I understand it may happen in just my local environment, and document this just in case others also meet this. We may use even worse hostname than 'a.b.c' to avoid such situation."
HADOOP-11529,Fix findbugs warnings in hadoop-archives,"{code}
NP	Possible null pointer dereference of reader in org.apache.hadoop.tools.HadoopArchives$HArchiveInputFormat.getSplits(JobConf, int) on exception path
Dm	Found reliance on default encoding in org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.close(): String.getBytes()
Dm	Found reliance on default encoding in org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.configure(JobConf): String.getBytes()
Dm	Found reliance on default encoding in org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.reduce(IntWritable, Iterator, OutputCollector, Reporter): String.getBytes()
{code}
"
HADOOP-11526,Memory leak in Bzip2Compressor and Bzip2Decompressor,"The use of JNI's GetStringUTFChars should be paired with ReleaseStringUTFChars or else the utf-8 char* created by Java's JNI implementation is leaked. It isn't in Bzip2Decompressor.c:

https://apache.googlesource.com/hadoop-common/+/refs/heads/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.c#45

A less error-prone way of handling JNI resources like local references and UTF strings is to use a smart pointer like the Apache licensed code in Android's ScopedLocalRef and ScopedUtfChars:

https://android.googlesource.com/platform/libnativehelper/+/jb-mr1.1-dev-plus-aosp/include/nativehelper/ScopedLocalRef.h
https://android.googlesource.com/platform/libnativehelper/+/jb-mr1.1-dev-plus-aosp/include/nativehelper/ScopedUtfChars.h"
HADOOP-11525,"FileSystem should expose some performance characteristics for caller (e.g., FsShell) to choose the right algorithm.","When running {{hadoop fs -put}},  {{FsShell}} creates a {{._COPYING_.}} file on the target directory, and then renames it to target file when the write is done. However, for some targeted systems, such as S3, Azure and Swift, a partial failure write request (i.e., {{PUT}}) has not side effect, while the {{rename}} operation is expensive. 

{{FileSystem}} should expose some characteristics so that the operation such as {{CommandWithDestination#copyStreamToTarget()}} can detect and choose the right way to do.

"
HADOOP-11523,"StorageException complaining "" no lease ID"" when updating FolderLastModifiedTime in WASB","In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it.

In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail.

So we should acquire the lease when updating the folder property rather than pass ""null"" to it.

{code}
ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Couldn't rename wasb://xxx/hbase/data/default/tdelrowtbl/3c842e8823c192d1028dc72ac3f22886/recovered.edits/0000000000000000015.temp to wasb://xxx/hbase/data/default/tdelrowtbl/3c842e8823c192d1028dc72ac3f22886/recovered.edits/0000000000000000015
org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2558)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2569)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.updateParentFolderLastModifiedTime(NativeAzureFileSystem.java:2016)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1983)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1161)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1121)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}"
HADOOP-11522,Update S3A Documentation,"This patch updates the documentation to reflect the changes made in recent patches to s3a that lacked documentation (I am the major culprit): 
* contributed by me:: HADOOP-11261, HADOOP-11171
* contributed by [~tedyu]: HADOOP-11446, HADOOP-11463

It updates:
- index.md: new config params + explanation, testing guidelines (HADOOP-11520)
- core-default.xml: new config params"
HADOOP-11521,Make connection timeout configurable in s3a ,"Currently in s3a, only the socket timeout is configurable, i.e. how long to wait before an existing connection is declared dead. The aws sdk has a separate timeout for establishing a connection. This patch introduces a config option in s3a to pass this on. "
HADOOP-11520,Clean incomplete multi-part uploads in S3A tests,"As proposed in HADOOP-11488. This patch activates the purging functionality of s3a at the start of each test. This cleans up any in-progress multi-part uploads in the test bucket, preventing unknowing users from eternally paying Amazon for the space of the already uploaded parts of previous tests that failed during a multi-part upload. 

People who have run the s3a tests should run a single test (evidently after this patch is applied) against all their testbuckets (or manually abort multipart)."
HADOOP-11512,Use getTrimmedStrings when reading serialization keys,"In the file {{hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java}}, we grab the IO_SERIALIZATIONS_KEY config as Configuration#getStrings(…) which does not trim the input. This could cause confusing user issues if someone manually overrides the key in the XML files/Configuration object without using the dynamic approach.

The call should instead use Configuration#getTrimmedStrings(…), so the whitespace is trimmed before the class names are searched on the classpath."
HADOOP-11510,Expose truncate API via FileContext,We also need to expose truncate API via {{org.apache.hadoop.fs.FileContext}}.
HADOOP-11509,change parsing sequence in GenericOptionsParser to parse -D parameters first,"In GenericOptionsParser, we need to parse -D parameter first. In that case, the user input parameter (through -D) can be set into configuration object earlier and used to process other parameters."
HADOOP-11507,Hadoop RPC Authentication problem with different user locale,"When I try to use hadoop mapreduce framework with Turkish locale that set default  mapreduce.map.java.opts and  mapreduce.reduce.java.opts to -Duser.language=tr. It throws exception. 

After Long research, i found a little bug in org.apache.hadoop.security.SaslPropertiesResolver line 68. default setting of hadoop.rpc.protection is authentication. When i make locale Turkish authentication parameter with toUpperCase become AUTHENTİCATİON. Please attention to dotted big i. This is a Turkish letter. It is very similar big i, it just have addition dot. :)

IMHO who-one use upper or lowercase method for settings, it should use Locale.ENGLISH. I created a patch file. But I could not test it. I can not find which tests should i run. If you show me, I will be glad.  

BTW I think, Hadoop Tests  should be run different locale. 

{block}
ERROR operation.Operation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:147)
        at org.apache.hive.service.cli.operation.SQLOperation.access$000(SQLOperation.java:69)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:213)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

{block}

p.s. Exception is generated by a Hive query. It can be generated by any map reduce job. "
HADOOP-11506,Configuration variable expansion regex expensive for long values,"Profiling several large Hadoop jobs, we discovered that a surprising amount of time was spent inside Configuration.get, more specifically, in regex matching caused by the substituteVars call.

"
HADOOP-11502,SPNEGO to Web UI fails with headers larger than 4KB,"Thanks to [~bgooley] for reporting this:

""""""
When SSL and Kerberos Authentication is enabled for hadoop web GUIs, if the header is over 4KB in size, the browser shows a blank page.

Browser dev tools show that a 413 full head error is returned as a response from Jetty.

It seems that in HADOOP-8816, we only addressed non-ssl ports by setting ret.setHeaderBufferSize(1024*64);

However, with SSL enabled, we use SslSocketConnector() but don't set the HeaderBufferSize. I think this is why we are failing at the default Jetty max header buffer size of 4KB.
"""""""
HADOOP-11500,InputStream is left unclosed in ApplicationClassLoader,"{code}
    InputStream is = null;
    try {
      is = ApplicationClassLoader.class.getClassLoader().
          getResourceAsStream(PROPERTIES_FILE);
{code}
The InputStream is not closed in the static block."
HADOOP-11499,Check of executorThreadsStarted in ValueQueue#submitRefillTask() evades lock acquisition,"{code}
    if (!executorThreadsStarted) {
      synchronized (this) {
        // To ensure all requests are first queued, make coreThreads =
        // maxThreads
        // and pre-start all the Core Threads.
        executor.prestartAllCoreThreads();
        executorThreadsStarted = true;
      }
    }
{code}
It is possible that two threads executing the above code both see executorThreadsStarted as being false, leading to executor.prestartAllCoreThreads() called twice."
HADOOP-11498,Bump the version of HTrace to 3.1.0-incubating,The package is renamed from org.htrace to org.apache.htrace.
HADOOP-11497,Fix typo in ClusterSetup.html#Hadoop_Startup,"The command for starting DataNodes and NodeManagers is wrong. This took a while to find out. Instead of {{$HADOOP_PREFIX/sbin/hadoop-daemon.sh}} the commands should read {{$HADOOP_PREFIX/sbin/hadoop-daemons.sh}} otherwise only _one_ daemon will be started on the master.
"
HADOOP-11495,Convert site documentation from apt to markdown,"Almost Plain Text (aka APT) lost.  Markdown won.

As a result, there are a ton of tools and online resources for Markdown that would make editing and using our documentation much easier.  It would be extremely beneficial for the community as a whole to move from apt to markdown.

This JIRA proposes to do this migration for the common project."
HADOOP-11494,Lock acquisition on WrappedInputStream#unwrappedRpcBuffer may race with another thread,"In SaslRpcClient, starting at line 576:
{code}
    public int read(byte[] buf, int off, int len) throws IOException {
      synchronized(unwrappedRpcBuffer) {
        // fill the buffer with the next RPC message
        if (unwrappedRpcBuffer.remaining() == 0) {
          readNextRpcPacket();
        }
{code}
readNextRpcPacket() may assign another ByteBuffer to unwrappedRpcBuffer, making the lock on previous ByteBuffer not useful."
HADOOP-11493,Fix some typos in kms-acls.xml description,"""does is"":

  <property>
    <name>hadoop.kms.acl.ROLLOVER</name>
    <value>*</value>
    <description>
      ACL for rollover-key operations.
      If the user does is not in the GET ACL, the key material is not returned
      as part of the response.
    </description>
  </property>
"
HADOOP-11492,Bump up curator version to 2.7.1,"Curator 2.7.1 got released recently and contains CURATOR-111 that YARN-2716 requires. 

PS: Filing a common JIRA so folks from other sub-projects also notice this change and shout out if there are any reservations. "
HADOOP-11491,HarFs incorrectly declared as requiring an authority,"HarFs is inited with {{authorityRequired=true}}. However, obviously har uri may contain no authority (for the underlying default file system).

{code}
Caused by: org.apache.hadoop.HadoopIllegalArgumentException: FileSystem implementation error -  default port -1 is not valid
	at org.apache.hadoop.fs.AbstractFileSystem.getUri(AbstractFileSystem.java:301)
	at org.apache.hadoop.fs.AbstractFileSystem.<init>(AbstractFileSystem.java:261)
	at org.apache.hadoop.fs.DelegateToFileSystem.<init>(DelegateToFileSystem.java:49)
	at org.apache.hadoop.fs.HarFs.<init>(HarFs.java:30)
	... 17 more
{code}

and the HDFS HA URI's must not have ports either.
"
HADOOP-11490,Expose truncate API via FileSystem and shell command,Add truncate operation to FileSystem and expose it to users via shell command.
HADOOP-11489,Dropping dependency on io.netty from hadoop-nfs' pom.xml,"hadoop-nfs pom.xml has compile time dependency on io.netty

This dependency can be dropped."
HADOOP-11488,Difference in default connection timeout for S3A FS,"The core-default.xml defines fs.s3a.connection.timeout as 5000, and the code under hadoop-tools/hadoop-aws defines it as 50000.

We should update the former to 50s so it gets taken properly, as we're also noticing that 5s is often too low, especially in cases such as large DistCp operations (which fail with {{Read timed out}} errors from the S3 service)."
HADOOP-11483,HardLink.java should use the jdk7 createLink method,"Now that we are using jdk7, HardLink.java should use the jdk7 createLink method rather than our shell commands or JNI methods.

Note that we cannot remove all of the JNI / shell commands unless we remove the code which is checking the link count, something that jdk7 doesn't provide (at least, I don't think it does)"
HADOOP-11482,Use correct UGI when KMSClientProvider is called by a proxy user,"Long Living clients of HDFS (For eg. OOZIE) use cached DFSClients which in turn use a cached KMSClientProvider to talk to KMS.

Before an MR Job is run, the job client calls the {{DFClient.addDelegationTokens()}} method which calls {{addDelegationTokens()}} on the {{KMSClientProvider}} to get any delegation token associated to the user.

Unfortunately, this call uses a cached {{DelegationTokenAuthenticationURL.Token}} instance which can cause the {{SignerSecretProvider}} implementation of the {{AuthenticationFilter}} at the KMS Server end to fail validation. Which results in the MR job itself failing."
HADOOP-11481,ClassCastException while using a key created by keytool to create encryption zone. ,"I'm using transparent encryption. If I create a key for KMS keystore via keytool and use the key to create an encryption zone. I get a ClassCastException rather than an exception with decent error message. I know we should use 'hadoop key create' to create a key. It's better to provide an decent error message to remind user to use the right way to create a KMS key.

[LOG]
ERROR[user=hdfs] Method:'GET' Exception:'java.lang.ClassCastException: javax.crypto.spec.SecretKeySpec cannot be cast to org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata'"
HADOOP-11480,Typo in hadoop-aws/index.md uses wrong scheme for test.fs.s3.name,"Around line 270:
{code}
       <property>
         <name>test.fs.s3.name</name>
        <value>s3a://test-aws-s3/</value>
       </property>
{code}
The scheme should be s3."
HADOOP-11470,Remove some uses of obsolete guava APIs from the hadoop codebase,"Along the same vein as HADOOP-11286, there are now several remaining usages of guava APIs that are now incompatible with a more recent version (e.g. 16).

This JIRA proposes eliminating those usages. With this, the hadoop code base should run/compile cleanly even if guava 16 is used for example.

This JIRA doesn't propose upgrading the guava dependency version however (just making the codebase compatible with guava 16+)."
HADOOP-11469,KMS should skip default.key.acl and whitelist.key.acl when loading key acl,"KMSACLs#setKeyACLs, loads key ACLs from the configuration by checking if the key name contains ""key.acl"". However, this also matches ""default.key.acl"" and ""whitelist.key.acl"" which is incorrect."
HADOOP-11467,KerberosAuthenticator can connect to a non-secure cluster,"While looking at HADOOP-10895, we discovered that the {{KerberosAuthenticator}} can authenticate with a non-secure cluster, even without falling back.

The problematic code is here:
{code:java}
      if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) {    // <----- A
        LOG.debug(""JDK performed authentication on our behalf."");
        // If the JDK already did the SPNEGO back-and-forth for
        // us, just pull out the token.
        AuthenticatedURL.extractToken(conn, token);
        return;
      } else if (isNegotiate()) {                                   // <----- B
        LOG.debug(""Performing our own SPNEGO sequence."");
        doSpnegoSequence(token);
      } else {                                                      // <----- C
        LOG.debug(""Using fallback authenticator sequence."");
        Authenticator auth = getFallBackAuthenticator();
        // Make sure that the fall back authenticator have the same
        // ConnectionConfigurator, since the method might be overridden.
        // Otherwise the fall back authenticator might not have the information
        // to make the connection (e.g., SSL certificates)
        auth.setConnectionConfigurator(connConfigurator);
        auth.authenticate(url, token);
      }
    }
{code}
Sometimes the JVM does the SPNEGO for us, and path A is used.  However, if the {{KerberosAuthenticator}} tries to talk to a non-secure cluster, path A also succeeds in this case.  
More details can be found in this comment:
https://issues.apache.org/jira/browse/HADOOP-10895?focusedCommentId=14247476&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14247476

We've actually dealt with this before.  HADOOP-8883 tried to fix a related problem by adding another condition to path A that would look for a header.  However, the JVM hides this header, making path A never occur.  We reverted this change in HADOOP-10078, and didn't realize that there was still a problem until now."
HADOOP-11466,FastByteComparisons: do not use UNSAFE_COMPARER on the SPARC architecture because it is slower there,"One difference between Hadoop 2.x and Hadoop 1.x is a utility to compare two byte arrays at coarser 8-byte granularity instead of at the byte-level. The discussion at HADOOP-7761 says this fast byte comparison is somewhat faster for longer arrays and somewhat slower for smaller arrays ( AVRO-939). In order to do 8-byte reads on addresses not aligned to 8-byte boundaries, the patch uses Unsafe.getLong. The problem is that this call is incredibly expensive on SPARC. The reason is that the Studio compiler detects an unaligned pointer read and handles this read in software. x86 supports unaligned reads, so there is no penalty for this call on x86. "
HADOOP-11465,Fix findbugs warnings in hadoop-gridmix,Fix findbugs issues in hadoop-gridmix
HADOOP-11464,Reinstate support for launching Hadoop processes on Windows using Cygwin.,"The Windows compatibility work removed the dependency on Cygwin as a compatibility layer.  However, some users may still wish to run Cygwin on a personal machine for running Hadoop commands that interact with a cluster.  This issue proposes to reinstate support for launching Hadoop processes on Windows using Cygwin to call the bash scripts."
HADOOP-11463,Replace method-local TransferManager object with S3AFileSystem#transfers,"This is continuation of HADOOP-11446.
The following changes are made according to Thomas Demoor's comments:

1. Replace method-local TransferManager object with S3AFileSystem#transfers
2. Do not shutdown TransferManager after purging existing multipart file - otherwise the current transfer is unable to proceed
3. Shutdown TransferManager instance in the close method of S3AFileSystem"
HADOOP-11462,TestSocketIOWithTimeout needs change for PowerPC platform,"TestSocketIOWithTimeout uses a block size of 4192 bytes to simulate a partial write. This seems to be a valid in x86 architecture where the default minimum blocksize is 4096. 
This testcase fails in PowerPC where the default minimum block size is 65536 bytes (64KB). So for PowerPC, using a blocksize little more than 64K , say 65555(65536 + 19) holds good for this scenario.
I attached a patch here where i made it very general by introducing NativeIO.POSIX.getCacheManipulator().getOperatingSystemPageSize() to get the page size.
I tested my patch in both ppc64 and x86 linux machines."
HADOOP-11459,"Fix recent findbugs in ActiveStandbyElector, NetUtils and ShellBasedIdMapping","Fix findbugs in the latest jenkins which causing QA builds to fail.

{noformat}Return value of java.util.concurrent.CountDownLatch.await(long, TimeUnit) ignored in org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef.process(WatchedEvent){noformat}

{noformat}Sequence of calls to java.util.concurrent.ConcurrentHashMap may not be atomic in org.apache.hadoop.net.NetUtils.canonicalizeHost(String){noformat}

{noformat}Inconsistent synchronization of org.apache.hadoop.security.ShellBasedIdMapping.staticMapping; locked 88% of time{noformat}"
HADOOP-11455,KMS and Credential CLI should request confirmation for deletion by default,The hadoop key delete and hadoop credential delete currently only ask for confirmation of the delete if -i is specified. Asking for confirmation should be the default action for both.
HADOOP-11450,Cleanup DistCpV1 not to use deprecated methods and fix javadocs,"Currently, DistCpV1 is using deprecated methods and having wrong javadocs. We should fix them.

1. DistCpV1.copy doesn't have dstpath, but javadoc has it.
{code}
    /**
     * Copy a file to a destination.
     * @param srcstat src path and metadata
     * @param dstpath dst path
{code}

2. Removing deprecated methods.
{code}
         SequenceFile.Writer dir_writer = SequenceFile.createWriter(jobfs, jobConf, dstdirlist, Text.class, FilePair.class, SequenceFile.CompressionType.NONE);
{code}

{code}
        basedir = args.basedir.makeQualified(basefs);
{code}

"
HADOOP-11449,[JDK8] Cannot build on Windows: error: unexpected end tag: </ul>,"Tried on hadoop-2.6.0-src, branch-2.5 and branch-trunk-win. All gave this error:
```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.8.1:jar (module-javadocs) on project hadoop-annotations: MavenReportException: Error while creating archive:
[ERROR] Exit code: 1 - E:\Projects\hadoop-common\hadoop-common-project\hadoop-annotations\src\main\java\org\apache\hadoop\classification\InterfaceStability.java:27: error: unexpected end tag: </ul>
[ERROR] * </ul>
[ERROR] ^
[ERROR] 
[ERROR] Command line was: ""C:\Program Files\Java\jdk1.8.0_25\jre\..\bin\javadoc.exe"" @options @packages
[ERROR] 
[ERROR] Refer to the generated Javadoc files in 'E:\Projects\hadoop-common\hadoop-common-project\hadoop-annotations\target' dir.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-annotations
```"
HADOOP-11448,Fix findbugs warnings in FileBasedIPList,"Now there are 3 findbugs warnings in hadoop-common package.
https://builds.apache.org/job/PreCommit-HADOOP-Build/5336//artifact/patchprocess/newPatchFindbugsWarningshadoop-common.html
{code}
Bug type RV_RETURN_VALUE_IGNORED At ActiveStandbyElector.java:[line 1067]
Bug type AT_OPERATION_SEQUENCE_ON_CONCURRENT_ABSTRACTION At NetUtils.java:[line 291]
{code}
The above two warnings will be fixed by HADOOP-11433. This issue is to fix the last one.
{code}
Bug type DLS_DEAD_LOCAL_STORE At FileBasedIPList.java:[line 53]
{code}"
HADOOP-11447,Add a more meaningful toString method to SampleStat and MutableStat,SampleStat and MutableStat don't override the toString method. A more meaningful implementation could help with debugging.
HADOOP-11446,S3AOutputStream should use shared thread pool to avoid OutOfMemoryError,"When working with Terry Padgett who used s3a for hbase snapshot, the following issue was uncovered.
Here is part of the output including the OOME when hbase snapshot is exported to s3a (nofile ulimit was increased to 102400):
{code}
2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: OutputStream for key 'FastQueryPOC/2014-12-11/EVENT1-IDX-snapshot/.hbase-snapshot/.tmp/EVENT1_IDX_snapshot_2012_12_11/    650a5678810fbdaa91809668d11ccf09/.regioninfo' closed. Now beginning upload
2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: Minimum upload part size: 16777216 threshold2147483647
Exception in thread ""main"" java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:713)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)
        at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)
        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)
        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)
        at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)
        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)
        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)
{code}
In S3AOutputStream#close():
{code}
      TransferManager transfers = new TransferManager(client);
{code}
This results in each TransferManager creating its own thread pool, leading to the OOME.
One solution is to pass shared thread pool to TransferManager."
HADOOP-11445,Bzip2Codec: Data block is skipped when position of newly created stream is equal to start of split,"bz2 input files are handled by FileInputFormat+LineRecordReader. In LineRecordReader, bz2 specific compressed input stream is created to iterate over records. After every new creation, the stream points to the beginning of next data block. The logic to find the beginning of next block depends on start of the split. The search begins at 10 bytes behind the start of split. If the first search creates input stream whose position is before or at start of split, next block beginning is sought (assuming that the record reader for previous split would have already iterated over the the data block in which current start of split lies). If the split start is just at the byte where a newly created stream is positioned (start of data block), attempt is made to find beginning of next data block. This doesn't seem correct because this will result in jumping a whole block and will result in missing records."
HADOOP-11442,hadoop-azure: Create test jar,pom of hadoop-azure project to needs to be modified to create a test jar as well. This test jar is required to run test cases of Windowsazuretablesink 
HADOOP-11441,Hadoop-azure: Change few methods scope to public,"TestWindowsAzureTableSinkSetup class test cases have dependencies with hadoop-azure classes, however few functions in hadoop azure  classes are having default access and are not visible outside package.

AzureBlobStorageTestAccount.createTestAccount()
AzureNativeFileSystemStore.getAccountKeyFromConfiguration()


"
HADOOP-11440,"Use ""test.build.data"" instead of ""build.test.dir"" for testing in ClientBaseWithFixes","In ClientBaseWithFixes.java, the base directory for tests are set in the following:
{code}
    static final File BASETEST =
        new File(System.getProperty(""build.test.dir"", ""build""));
{code}
There is no property ""build.test.dir"", so {{BASETEST}} is always ""build"". We should use ""test.build.data"" instead of ""build.test.dir""."
HADOOP-11432,Fix SymlinkBaseTest#testCreateLinkUsingPartQualPath2,"seems it's a regression from HADOOP-11409, but weird to me it that why this testing could pass by QA robot at that jira.  it failed always in my dev box now w/o the change:)"
HADOOP-11431,clean up redundant maven-site-plugin configuration,"as seen during HADOOP-11420 review, there is some cleanup to do"
HADOOP-11430,"Add GenericTestUtils#disableLog, GenericTestUtils#setLogLevel","Now that we are using both commons-logging and slf4j, we can no longer rely on just casting the Log object to a {{Log4JLogger}} and calling {{setLevel}} on that.  With {{org.slf4j.Logger}} objects, we need to look up the underlying {{Log4JLogger}} using {{LogManager#getLogger}}.

This patch adds {{GenericTestUtils#disableLog}} and {{GenericTestUtils#setLogLevel}} functions which hide this complexity from unit tests, just allowing the tests to call {{disableLog}} or {{setLogLevel}}, and have {{GenericTestUtils}} figure out the right thing to do based on the log / logger type."
HADOOP-11429,Findbugs warnings in hadoop extras,9 new findbugs warnings in hadoop-extras. Check the attached report
HADOOP-11428,Remove obsolete reference to Cygwin in BUILDING.txt,The 'Building on Windows' section of BUILDING.txt has an obsolete reference to Cygwin. It should be removed to avoid confusion.
HADOOP-11427,ChunkedArrayList: fix removal via iterator and implement get,"ChunkedArrayList: implement removal via iterator and get.  Previously, calling remove on a ChunkedArrayList iterator would cause the returned size to be incorrect later."
HADOOP-11424,Fix failure for TestOsSecureRandom,"Recently I usually see failure of {{testOsSecureRandomSetConf}} in TestOsSecureRandom.
https://builds.apache.org/job/PreCommit-HADOOP-Build/5298//testReport/org.apache.hadoop.crypto.random/TestOsSecureRandom/testOsSecureRandomSetConf/
{code}
java.lang.Exception: test timed out after 120000 milliseconds
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:272)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:154)
	at java.io.BufferedReader.read1(BufferedReader.java:205)
	at java.io.BufferedReader.read(BufferedReader.java:279)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.parseExecResult(Shell.java:735)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:531)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
	at org.apache.hadoop.crypto.random.TestOsSecureRandom.testOsSecureRandomSetConf(TestOsSecureRandom.java:149)
{code}"
HADOOP-11422,Check CryptoCodec is AES-CTR for Crypto input/output stream,"{{CryptoInputStream}} and {{CryptoOutputStream}} require AES-CTR as the algorithm/mode, although there is only AES-CTR implementation currently, but we'd better to check it."
HADOOP-11421,Add IOUtils#listDirectory,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array."
HADOOP-11420,Use latest maven-site-plugin and replace link to svn with link to git,
HADOOP-11419,improve hadoop-maven-plugins,"little inconsistencies:
- property for Maven Plugin Tools http://maven.apache.org/plugin-tools/maven-plugin-plugin/ separate from Maven core, and use lates Plugin Tools version 3.3
- better execution configuration for maven-plugin-plugin to avoid twice execution
- default value for protoc command as parameter configuration instead of code"
HADOOP-11416,Move ChunkedArrayList into hadoop-common,"Move ChunkedArrayList into hadoop-common so that it can be used by classes in hadoop-common, not just hdfs"
HADOOP-11414,FileBasedIPList#readLines() can leak file descriptors,"{code}
          Reader fileReader = new InputStreamReader(
              new FileInputStream(file), Charsets.UTF_8);
          BufferedReader bufferedReader = new BufferedReader(fileReader);
          List<String> lines = new ArrayList<String>();
          String line = null;
          while ((line = bufferedReader.readLine()) != null) {
            lines.add(line);
          }
          bufferedReader.close();
{code}
Since bufferedReader.readLine() may throw IOE, so the close of bufferedReader should be enclosed within finally block."
HADOOP-11412,"POMs mention ""The Apache Software License"" rather than ""Apache License""",like JAMES-821 or RAT-128 or MPOM-48
HADOOP-11411,Hive build failure on hadoop-2.7 due to HADOOP-11356,"HADOOP-11356 removes org.apache.hadoop.fs.permission.AccessControlException, causing build break on Hive when compiling against hadoop-2.7:

{noformat}
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java:[808,63] cannot find symbol
  symbol:   class AccessControlException
  location: package org.apache.hadoop.fs.permission
[INFO] 1 error
{noformat}
"
HADOOP-11410,make the rpath of libhadoop.so configurable ,"We should make the rpath of {{libhadoop.so}} configurable, so that we can use a different rpath if needed.  The {{RPATH}} of {{libhadoop.so}} is primarily used to control where {{dlopen}} looks for shared libraries by default."
HADOOP-11409,FileContext.getFileContext can stack overflow if default fs misconfigured,"If the default filesystem is misconfigured such that it doesn't have a scheme then FileContext.getFileContext(URI, Configuration) will call FileContext.getFileContext(Configuration) which in turn calls the former and we loop until the stack explodes."
HADOOP-11403,"Avoid using sys_errlist on Solaris, which lacks support for it","sys_errlist has been removed from Solaris. The new interface is strerror.  Wherever sys_errlist is accessed we should change to using strerror instead.
We already have an interface function terror which can contain this functionality, so we should use it instead of directly accessing sys_errlist."
HADOOP-11402,Negative user-to-group cache entries are never cleared for never-again-accessed users,Negative user-to-group cache entries are never cleared for never-again-accessed users.  Use guava cache so that expired entries can be deleted automatically
HADOOP-11400,GraphiteSink does not reconnect to Graphite after 'broken pipe',"I see that after network error GraphiteSink does not reconnects to Graphite server and in effect metrics are not sent. 

Here is stacktrace I see (this is from nodemanager):

2014-12-11 16:39:21,655 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception, retry in 4806ms
org.apache.hadoop.metrics2.MetricsException: Error flushing metrics
        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)
        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)
Caused by: java.net.SocketException: Broken pipe
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)
        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)
        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)
        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)
        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)
        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)
        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)
        ... 5 more
2014-12-11 16:39:26,463 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception and over retry limit, suppressing further error messages
org.apache.hadoop.metrics2.MetricsException: Error flushing metrics
        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)
        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)
        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)
Caused by: java.net.SocketException: Broken pipe
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)
        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)
        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)
        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)
        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)
        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)
        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)
        ... 5 more


GraphiteSinkFixed.java is simply GraphiteSink.java from Hadoop 2.6.0 (with fixed https://issues.apache.org/jira/browse/HADOOP-11182) because I cannot simply upgrade Hadoop (I am using CDH5).

I see that GraphiteSink is using OutputStreamWriter which is created only in init method (which is probably called only once per application runtime) and there is no reconnection logic."
HADOOP-11399,Java Configuration file and .xml files should be automatically cross-compared,Update common in order to allow automatic comparison of Java Configuration classes and xxx-default.xml files within a unit test.  Changes here will be used in downstream JIRAs.
HADOOP-11396,Provide navigation in the site documentation linking to the Hadoop Compatible File Systems.,"We build site documentation for hadoop-aws and hadoop-openstack, and we'll soon have documentation for hadoop-azure.  This documentation is not linked from the main site though, so unless a user knows the direct URL, they won't be able to find it.  This issue proposes adding navigation to the site to make it easier to find these documents."
HADOOP-11395,Add site documentation for Azure Storage FileSystem integration.,The scope of this issue is to add site documentation covering our Azure Storage FileSystem integration.
HADOOP-11394,hadoop-aws documentation missing.,"In HADOOP-10714, the documentation source files for hadoop-aws were moved from src/site to src/main/site.  The build is no longer actually generating the HTML site from these source files, because src/site is the expected path."
HADOOP-11390,Metrics 2 ganglia provider to include hostname in unresolved address problems,"When metrics2/ganglia gets an unresolved hostname it doesn't include the hostname in question, making it harder to track down
"
HADOOP-11389,Clean up byte to string encoding issues in hadoop-common,"Much code in hadoop-common convert bytes to string using default charsets. The behavior of conversion depends on the platform settings of encoding, which is flagged by newer versions of findbugs. This jira proposes to fix the findbugs warnings."
HADOOP-11388,Remove deprecated o.a.h.metrics.file.FileContext,The {{o.a.h.metrics.file.FileContext}} has been deprecated. This jira proposes to remove it from the repository.
HADOOP-11386,Replace \n by %n in format hadoop-common format strings,It is recommended to use '%n' in format strings. We may want to replace all '\n' in hadoop-common. 
HADOOP-11385,Prevent cross site scripting attack on JMXJSONServlet,"JMXJSONServlet allows passing a callback parameter in the JMX response, which is introduced in HADOOP-8922:

{code}
        // ""callback"" parameter implies JSONP outpout
        jsonpcb = request.getParameter(CALLBACK_PARAM);
        if (jsonpcb != null) {
          response.setContentType(""application/javascript; charset=utf8"");
          writer.write(jsonpcb + ""("");
        } else {
          response.setContentType(""application/json; charset=utf8"");
        }
{code}

The code writes the callback parameter directly to the output, allowing cross-site scripting attack. This vulnerability allows the attacker easily stealing the credential of the user on the browser.

The original use case can be supported using Cross-origin resource sharing (CORS), which is used by the current NN web UI.

This jira proposes to move JMXJSONServlet to CORS."
HADOOP-11381,"Fix findbugs warnings in hadoop-distcp, hadoop-aws, hadoop-azure, and hadoop-openstack","When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-distcp, hadoop-aws, hadoop-azure, and hadoop-openstack. "
HADOOP-11379,Fix new findbugs warnings in hadoop-auth*,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and hadoop-auth-examples. "
HADOOP-11378,Fix new findbugs warnings in hadoop-kms,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-kms"
HADOOP-11377,"jdiff failing on java 7 and java 8, ""Null.java"" not found","Jdiff is having problems on Java 8, as it cannot find a javadoc for the new {{Null}} datatype

{code}
'<https://builds.apache.org/job/Hadoop-common-trunk-Java8/ws/hadoop-common-project/hadoop-common/dev-support/jdiff/Null.java'>

The ' characters around the executable and arguments are
not part of the command.
  [javadoc] javadoc: error - Illegal package name: """"
  [javadoc] javadoc: error - File not found: ""<https://builds.apache.org/job/Hadoop-common-trunk-Java8/ws/hadoop-common-project/hadoop-common/dev-support/jdiff/Null.java"">
{code}"
HADOOP-11372,Fix new findbugs warnings in mapreduce-examples,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in mapreduce-examples. "
HADOOP-11370,Fix new findbug warnings hadoop-yarn,
HADOOP-11369,"Fix new findbugs warnings in hadoop-mapreduce-client, non-core directories","When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in non-core directories of hadoop-mapreduce-client"
HADOOP-11368,Fix SSLFactory truststore reloader thread leak in KMSClientProvider,"When a {{KMSClientProvider}} is initialized in _ssl_ mode, It  initializes a {{SSLFactory}} object. This in-turn creates an instance of {{ReloadingX509TrustManager}} which, on initialization, starts a trust store reloader thread.

It is noticed that over time, as a number of short lived {{KMSClientProvider}} instances are created and destroyed, the trust store manager threads are not interrupted/killed and remain in TIMED_WAITING state. A Thread dump shows multiple:
{noformat}
""Truststore reloader thread"" daemon prio=10 tid=0x00007fb1cf942800 nid=0x4e99 waiting on condition [0x00007fb0485f5000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:189)
        at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
        - None
{noformat}
 
"
HADOOP-11367,Fix warnings from findbugs 3.0 in hadoop-streaming,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-streaming. "
HADOOP-11363,Hadoop maven surefire-plugin uses must set heap size,"Some of the hadoop tests (especially HBase) are running out of memory on Java 8, due to there not being enough heap for them

The heap size of surefire test runs is *not* set in {{MAVEN_OPTS}}, it needs to be explicitly set as an argument to the test run.

I propose

# {{hadoop-project/pom.xml}} defines the maximum heap size and test timeouts for surefire builds as properties
# modules which run tests use these values for their memory & timeout settings.
# these modules should also set the surefire version they want to use"
HADOOP-11361,Fix a race condition in MetricsSourceAdapter.updateJmxCache,"{noformat}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateAttrCache(MetricsSourceAdapter.java:247)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:177)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getAttribute(MetricsSourceAdapter.java:102)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
{noformat}"
HADOOP-11358,Tests for encryption/decryption with IV calculation overflow,"As discussed in HADOOP-11343, add more tests to cover encryption/decryption with IV calculation overflow"
HADOOP-11355,"When accessing data in HDFS and the key has been deleted, a Null Pointer Exception is shown.","When using the KMS with the file based keystore we can see this error when trying to access an encryption zone that got his key deleted:
{noformat}
KMSClientProvider[http://foo.bar.com:16000/kms/v1/] has been updated.
hdfs@foo:~$ hadoop fs -tail /user/systest/zones/n3bg0hfvyvjwvwuc/terasort/part-00000
-tail: Fatal internal error
java.lang.NullPointerException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:157)
        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:484)
        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:442)
        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:716)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)
        at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1317)
        at org.apache.hadoop.hdfs.DFSClient.createWrappedInputStream(DFSClient.java:1384)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:297)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:297)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
        at org.apache.hadoop.fs.shell.Tail.dumpFromOffset(Tail.java:92)
        at org.apache.hadoop.fs.shell.Tail.processPath(Tail.java:73)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:306)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:278)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:260)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:244)
        at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:287)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)
{noformat}
"
HADOOP-11354,ThrottledInputStream doesn't perform effective throttling,"This was first reported in HBASE-12632 by [~Tobi] :

I just transferred a ton of data using ExportSnapshot with bandwidth throttling from one Hadoop cluster to another Hadoop cluster, and discovered that ThrottledInputStream does not limit bandwidth.

The problem is that ThrottledInputStream sleeps once, for a fixed time (50 ms), at the start of each read call, disregarding the actual amount of data read.

ExportSnapshot defaults to a buffer size as big as the block size of the outputFs:

{code:java}
      // Use the default block size of the outputFs if bigger
      int defaultBlockSize = Math.max((int) outputFs.getDefaultBlockSize(), BUFFER_SIZE);
      bufferSize = conf.getInt(CONF_BUFFER_SIZE, defaultBlockSize);
      LOG.info(""Using bufferSize="" + StringUtils.humanReadableInt(bufferSize));
{code}

In my case, this was 256MB.

Hence, the ExportSnapshot mapper will attempt to read up to 256 MB at a time, each time sleeping only 50ms. Thus, in the worst case where each call to read fills the 256 MB buffer in negligible time, the ThrottledInputStream cannot reduce the bandwidth to under (256 MB) / (5 ms) = 5 GB/s.

Even in a more realistic case where read returns about 1 MB per call, it still cannot throttle the bandwidth to under 20 MB/s.

The issue is exacerbated by the fact that you need to set a low limit because the total bandwidth per host depends on the number of mapper slots as well.

A simple solution would change the if in throttle to a while, so that it keeps sleeping for 50 ms until the rate is finally low enough:

{code:java}
  private void throttle() throws IOException {
    while (getBytesPerSec() > maxBytesPerSec) {
      try {
        Thread.sleep(SLEEP_DURATION_MS);
        totalSleepTime += SLEEP_DURATION_MS;
      } catch (InterruptedException e) {
        throw new IOException(""Thread aborted"", e);
      }
    }
  }
{code}

This issue affects the ThrottledInputStream in hadoop as well.

Another way to see this is that for big enough buffer sizes, ThrottledInputStream will be throttling only the number of read calls to 20 per second, disregarding the number of bytes read."
HADOOP-11352,"Clean up test-patch.sh to disable ""+1 contrib tests""","Jenkins test-patch.sh always comments as ""{color:green}+1 contrib tests{color}.  The patch passed contrib unit tests."", however, the script runs no contrib tests.
This issue was found when fixing HDFS-7448."
HADOOP-11350,The size of header buffer of HttpServer is too small when HTTPS is enabled,"{code}
curl -k  -vvv -i -L --negotiate -u : https://<servername>:50070

< HTTP/1.1 413 FULL head
HTTP/1.1 413 FULL head
< Connection: close
Connection: close
< Server: Jetty(6.1.26)
Server: Jetty(6.1.26)
{code}

For some users, the spnego token too large for the default header buffer used by Jetty. 

Though the issue is fixed for HTTP connections (via HADOOP-8816), HTTPS connections needs to be fixed as well. "
HADOOP-11349,RawLocalFileSystem leaks file descriptor while creating a file if creat succeeds but chmod fails.,"{{RawLocalFileSystem}} currently implements some file creation operations as a sequence of 2 syscalls: create the file, followed by setting its permissions.  If creation succeeds, but then setting permission causes an exception to be thrown, then there is no attempt to close the previously opened file, resulting in a file descriptor leak."
HADOOP-11348,Remove unused variable from CMake error message for finding openssl,ERROR message for finding openssl should not print CUSTOM_OPENSSL_INCLUDE_DIR because this variable don't exist.
HADOOP-11344,KMS kms-config.sh sets a default value for the keystore password even in non-ssl setup,This results in kms always starting up in ssl mode.
HADOOP-11343,Overflow is not properly handled in caclulating final iv for AES CTR,"In the AesCtrCryptoCodec calculateIV, as the init IV is a random generated 16 bytes, 

final byte[] iv = new byte[cc.getCipherSuite().getAlgorithmBlockSize()];
      cc.generateSecureRandom(iv);

Then the following calculation of iv and counter on 8 bytes (64bit) space would easily cause overflow and this overflow gets lost.  The result would be the 128 bit data block was encrypted with a wrong counter and cannot be decrypted by standard aes-ctr.

{code}
/**
   * The IV is produced by adding the initial IV to the counter. IV length 
   * should be the same as {@link #AES_BLOCK_SIZE}
   */
  @Override
  public void calculateIV(byte[] initIV, long counter, byte[] IV) {
    Preconditions.checkArgument(initIV.length == AES_BLOCK_SIZE);
    Preconditions.checkArgument(IV.length == AES_BLOCK_SIZE);
    
    System.arraycopy(initIV, 0, IV, 0, CTR_OFFSET);
    long l = 0;
    for (int i = 0; i < 8; i++) {
      l = ((l << 8) | (initIV[CTR_OFFSET + i] & 0xff));
    }
    l += counter;
    IV[CTR_OFFSET + 0] = (byte) (l >>> 56);
    IV[CTR_OFFSET + 1] = (byte) (l >>> 48);
    IV[CTR_OFFSET + 2] = (byte) (l >>> 40);
    IV[CTR_OFFSET + 3] = (byte) (l >>> 32);
    IV[CTR_OFFSET + 4] = (byte) (l >>> 24);
    IV[CTR_OFFSET + 5] = (byte) (l >>> 16);
    IV[CTR_OFFSET + 6] = (byte) (l >>> 8);
    IV[CTR_OFFSET + 7] = (byte) (l);
  }
{code}"
HADOOP-11342,KMS key ACL should ignore ALL operation for default key ACL and whitelist key ACL,"KMS key ACL should ignore ALL operation for default key ACL and whitelist key ACL, while there is a bug in the code which causes that if {{default.key.acl.ALL}} is configured, it will be used."
HADOOP-11341,KMS support for whitelist key ACLs,"As reported by [~dian.fu] :
Key based ACL in KMS is currently implemented as whitelist. So if I configure as follows in kms-acl.xml,
{code}
 <property>
    <name>key.acl.testKey.DECRYPT_EEK</name>
    <value>testUser</value>
  </property>
{code}, then only {{testUser}} user can do {{DECRYPT_EEK}} call on key {{testKey}}. If I want {{yarn}} user can also do {{DECRYPT_EEK}} call on {{testKey}} key, I need add {{yarn}} user to the above configuration value manually. This means that if I want to configure key based ACL({{DECRYPT_EEK}}) for {{some key}}, I need also add {{yarn}} user to configuration {{DECRYPT_EEK}} for that key. As I don't know if {{yarn}} user will later need to do {{DECRYPT_EEK}} for this key.. This is inconvenient and tricky.

This can be alleviated by slightly modifying the key ACL logic in KMS first checks if the user, in this case {{yarn}}, is present in {{key.acl.<key-name>.<OP-name>}} list. And if not, then also check if the user is present in {{default.key.acl.<OP-name>}}. If yes, then grant access.. else deny.

Currently,  {{default.key.acl.<OP-name>}} is consulted only if NO {{key.acl.<key-name>.<OP-name>}} is specified."
HADOOP-11337,KeyAuthorizationKeyProvider access checks need to be done atomically,"In {{KeyAuthorizationKeyProvider#getMetadata}}, if firstly call {{KeyAuthorizationKeyProvider#doAccessCheck}} to check if client has the permission to do this operation. However, if the metadata is null when {{KeyAuthorizationKeyProvider#doAccessCheck}} is called and becomes not null after {{KeyAuthorizationKeyProvider#doAccessCheck}} called, key based ACL check will be skipped. The {{getMetadata}} operation should be atomic.
{code}
  public Metadata getMetadata(String name) throws IOException {
    doAccessCheck(name, KeyOpType.READ);
    return provider.getMetadata(name);
  }

  private void doAccessCheck(String keyName, KeyOpType opType) throws
      IOException {
    Metadata metadata = provider.getMetadata(keyName);
    if (metadata != null) {
      String aclName = metadata.getAttributes().get(KEY_ACL_NAME);
      checkAccess((aclName == null) ? keyName : aclName, getUser(), opType);
    }
  }
{code}"
HADOOP-11333,Fix deadlock in DomainSocketWatcher when the notification pipe is full,"I found some of our DataNodes will run ""exceeds the limit of concurrent xciever"", the limit is 4K.

After check the stack, I suspect that org.apache.hadoop.net.unix.DomainSocket.writeArray0 which called by DomainSocketWatcher.kick stuck:
{quote}
""DataXceiver for client unix:/var/run/hadoop-hdfs/dn [Waiting for operation #1]"" daemon prio=10 tid=0x00007f55c5576000 nid=0x385d waiting on condition [0x00007f558d5d4000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000740df9c90> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
        at org.apache.hadoop.net.unix.DomainSocketWatcher.add(DomainSocketWatcher.java:286)
        at org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.createNewMemorySegment(ShortCircuitRegistry.java:283)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitShm(DataXceiver.java:413)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm(Receiver.java:172)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:92)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
--
""DataXceiver for client unix:/var/run/hadoop-hdfs/dn [Waiting for operation #1]"" daemon prio=10 tid=0x00007f7de034c800 nid=0x7b7 runnable [0x00007f7db06c5000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.hadoop.net.unix.DomainSocket.writeArray0(Native Method)
	at org.apache.hadoop.net.unix.DomainSocket.access$300(DomainSocket.java:45)
	at org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream.write(DomainSocket.java:589)
	at org.apache.hadoop.net.unix.DomainSocketWatcher.kick(DomainSocketWatcher.java:350)
	at org.apache.hadoop.net.unix.DomainSocketWatcher.add(DomainSocketWatcher.java:303)
	at org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.createNewMemorySegment(ShortCircuitRegistry.java:283)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitShm(DataXceiver.java:413)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm(Receiver.java:172)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:92)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
	at java.lang.Thread.run(Thread.java:745)

""DataXceiver for client unix:/var/run/hadoop-hdfs/dn [Waiting for operation #1]"" daemon prio=10 tid=0x00007f55c5574000 nid=0x377a waiting on condition [0x00007f558d7d6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000740df9cb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at org.apache.hadoop.net.unix.DomainSocketWatcher.add(DomainSocketWatcher.java:306)
        at org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.createNewMemorySegment(ShortCircuitRegistry.java:283)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitShm(DataXceiver.java:413)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm(Receiver.java:172)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:92)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
        at java.lang.Thread.run(Thread.java:745)
             

""Thread-163852"" daemon prio=10 tid=0x00007f55c811c800 nid=0x6757 runnable [0x00007f55aef6e000]
   java.lang.Thread.State: RUNNABLE 
        at org.apache.hadoop.net.unix.DomainSocketWatcher.doPoll0(Native Method)
        at org.apache.hadoop.net.unix.DomainSocketWatcher.access$800(DomainSocketWatcher.java:52)
        at org.apache.hadoop.net.unix.DomainSocketWatcher$1.run(DomainSocketWatcher.java:457)
        at java.lang.Thread.run(Thread.java:745)
{quote}"
HADOOP-11332,KerberosAuthenticator#doSpnegoSequence should check if kerberos TGT is available in the subject ,"In {{KerberosAuthenticator#doSpnegoSequence}}, it first check if the subject is {{null}} before actually doing spnego, if the subject is {{null}}, it will first perform kerberos login before doing spnego. We should also check if kerberos TGT exists in the subject, if not, we should also perform kerberos login. This situation will occur when we configure KMS as kerberos enabled (via configure {{hadoop.kms.authentication.type}} as {{kerberos}}) and other hadoop services not kerberos enabled(via configure {{hadoop.security.authentication}} as {{simple}}). In this case, when client connect to KMS, KMS will trigger kerberos authentication and as {{hadoop.security.authentication}} is configured as {{simple}} in hadoop cluster, the client side haven't login with kerberos method currently, but maybe it has already login using simple method which will make {{subject}} not null."
HADOOP-11329,Add JAVA_LIBRARY_PATH to KMS startup options,"Currently, HADOOP_HOME isn't part of the start up options of KMS. If I add the the following configuration to core-site.xml of kms,
{code} <property>
  <name>hadoop.security.crypto.codec.classes.aes.ctr.nopadding</name>
  <value>org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec</value>
 </property>
{code} kms server will throw the following exception when receive ""generateEncryptedKey"" request
{code}
2014-11-24 10:23:18,189 DEBUG org.apache.hadoop.crypto.OpensslCipher: Failed to load OpenSSL Cipher.
java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCodeLoader.buildSupportsOpenssl()Z
        at org.apache.hadoop.util.NativeCodeLoader.buildSupportsOpenssl(Native Method)
        at org.apache.hadoop.crypto.OpensslCipher.<clinit>(OpensslCipher.java:85)
        at org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec.<init>(OpensslAesCtrCryptoCodec.java:50)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:129)
        at org.apache.hadoop.crypto.CryptoCodec.getInstance(CryptoCodec.java:67)
        at org.apache.hadoop.crypto.CryptoCodec.getInstance(CryptoCodec.java:100)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:256)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:371)
        at org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension$CryptoExtension$EncryptedQueueRefiller.fillQueueForKey(EagerKeyGeneratorKeyProviderCryptoExtension.java:77)
        at org.apache.hadoop.crypto.key.kms.ValueQueue$1.load(ValueQueue.java:181)
        at org.apache.hadoop.crypto.key.kms.ValueQueue$1.load(ValueQueue.java:175)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3965)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)
        at org.apache.hadoop.crypto.key.kms.ValueQueue.getAtMost(ValueQueue.java:256)
        at org.apache.hadoop.crypto.key.kms.ValueQueue.getNext(ValueQueue.java:226)
        at org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension$CryptoExtension.generateEncryptedKey(EagerKeyGeneratorKeyProviderCryptoExtension.java:126)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:371)
        at org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider.generateEncryptedKey(KeyAuthorizationKeyProvider.java:192)
        at org.apache.hadoop.crypto.key.kms.server.KMS$9.run(KMS.java:379)
        at org.apache.hadoop.crypto.key.kms.server.KMS$9.run(KMS.java:375
{code}
The reason is that it cannot find libhadoop.so. This will prevent KMS to response to ""generateEncryptedKey"" requests."
HADOOP-11327,"BloomFilter#not() omits the last bit, resulting in an incorrect filter","There's an off-by-one error in {{BloomFilter#not()}}:

{{BloomFilter#not}} calls {{BitSet#flip(0, vectorSize - 1)}}, but according to the javadoc for that method, {{toIndex}} is end-_exclusive_:
{noformat}
* @param  toIndex index after the last bit to flip
{noformat}

This means that the last bit in the bit array is not flipped.
Specifically, this was discovered in the following scenario:
1. A new/empty {{BloomFilter}} was created with vectorSize=7.
2. Invoke {{bloomFilter.not()}}; now expecting a bloom filter with all 7 bits (0 through 6) flipped to 1 and membershipTest(...) to always return true.
3. However, membershipTest(...) was found to often not return true, and upon inspection, the BitSet only had bits 0 through 5 flipped.

The fix should be simple: remove the ""- 1"" from the call to {{BitSet#flip}}."
HADOOP-11323,WritableComparator#compare keeps reference to byte array,"When the default compare is used on a WritableComparator a reference to the second passed in byte array is kept in the buffer. Since WritableComparator keeps a reference to the buffer the byte will never be garbage collected. This can lead to a higher heap use than needed.

The buffer should drop the reference to the byte array passed in. We can null out the byte array reference since the buffer is a private variable for the class."
HADOOP-11322,key based ACL check in KMS always check KeyOpType.MANAGEMENT even actual KeyOpType is not MANAGEMENT ,"In the method checkAccess of class KeyAuthorizationKeyProvider, there is following code:
{code}
private void checkAccess(String aclName, UserGroupInformation ugi,
      KeyOpType opType) throws AuthorizationException {
    Preconditions.checkNotNull(aclName, ""Key ACL name cannot be null"");
    Preconditions.checkNotNull(ugi, ""UserGroupInformation cannot be null"");
    if (acls.isACLPresent(aclName, KeyOpType.MANAGEMENT) &&
        (acls.hasAccessToKey(aclName, ugi, opType)
            || acls.hasAccessToKey(aclName, ugi, KeyOpType.ALL))) {
      return;
    }
...
}
{code}
Seems that {code}
acls.isACLPresent(aclName, KeyOpType.MANAGEMENT) {code}
should be replaced with {code}
acls.isACLPresent(aclName, opType) {code}"
HADOOP-11321,copyToLocal cannot save a file to an SMB share unless the user has Full Control permissions.,"In Hadoop 2, it is impossible to use {{copyToLocal}} to copy a file from HDFS to a destination on an SMB share.  This is because in Hadoop 2, the {{copyToLocal}} maps to 2 underlying {{RawLocalFileSystem}} operations: {{create}} and {{setPermission}}.  On an SMB share, the user may be authorized for the {{create}} but denied for the {{setPermission}}.  Windows denies the {{WRITE_DAC}} right required by {{setPermission}} unless the user has Full Control permissions.  Granting Full Control isn't feasible for most deployments, because it's insecure.  This is a regression from Hadoop 1, where {{copyToLocal}} only did a {{create}} and didn't do a separate {{setPermission}}."
HADOOP-11318,Update the document for hadoop fs -stat,"In FileSystemShell.apt.vm, 
{code}
stat

   Usage: <<<hdfs dfs -stat URI [URI ...]>>>

   Returns the stat information on the path.
{code}
Now {{-stat}} accepts the below formats.
 *   %b: Size of file in blocks
 *   %g: Group name of owner
 *   %n: Filename
 *   %o: Block size
 *   %r: replication
 *   %u: User name of owner
 *   %y: UTC date as &quot;yyyy-MM-dd HH:mm:ss&quot;
 *   %Y: Milliseconds since January 1, 1970 UTC

They should be documented."
HADOOP-11317,Increment SLF4J version to 1.7.10,YARN-2875 highllights some problems with SLF4J 1.7.5; needs to be pulled up to 1.7.7; 
HADOOP-11316,"""mvn package -Pdist,docs -DskipTests -Dtar"" fails because of non-ascii characters","The command fails because following files include non-ascii characters.
* ComparableVersion.java
* CommonConfigurationKeysPublic.java
* ComparableVersion.java

{code}
  [javadoc] /mnt/build/hadoop-2.6.0-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java:13: error: unmappable character for encoding ASCII
  [javadoc] //        author <a href=""mailto:hboutemy@apache.org"">Herv?? Boutemy</a>
  [javadoc]                                                           ^
  [javadoc] /mnt/build/hadoop-2.6.0-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java:13: error: unmappable character for encoding ASCII
  [javadoc] //        author <a href=""mailto:hboutemy@apache.org"">Herv?? Boutemy</a>
{code}

{code}
  [javadoc] /mnt/build/hadoop-2.6.0-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java:318: error: unmappable character for encoding ASCII
  [javadoc]   //  <!--- KMSClientProvider configurations ???>
  [javadoc]                                              ^
  [javadoc] /mnt/build/hadoop-2.6.0-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java:318: error: unmappable character for encoding ASCII
  [javadoc]   //  <!--- KMSClientProvider configurations ???>
  [javadoc]                                               ^
  [javadoc] /mnt/build/hadoop-2.6.0-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java:318: error: unmappable character for encoding ASCII
  [javadoc] Loading source files for package org.apache.hadoop.fs.crypto...
  [javadoc]   //  <!--- KMSClientProvider configurations ???>
{code}

{code}
  [javadoc] /mnt/build/hadoop-2.6.0-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java:13: error: unmappable character for encoding ASCII
  [javadoc] //        author <a href=""mailto:hboutemy@apache.org"">Herv?? Boutemy</a>
  [javadoc]                                                           ^
  [javadoc] /mnt/build/hadoop-2.6.0-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java:13: error: unmappable character for encoding ASCII
  [javadoc] //        author <a href=""mailto:hboutemy@apache.org"">Herv?? Boutemy</a>
{code}"
HADOOP-11313,Adding a document about NativeLibraryChecker,"NativeLibraryChecker is a good tool to check whether native libraries are loaded correctly. We don't have any docs about this, so we should add it to NativeLibraries.apt.vm."
HADOOP-11312,Fix unit tests to not use uppercase key names,"After HADOOP-11311 uppercase key names aren't allowed, breaking some unit tests. Let's fix them."
HADOOP-11311,Restrict uppercase key names from being created with JCEKS,The Java KeyStore spec is ambiguous about the requirements for case-sensitivity for KeyStore implementations. The JDK7 JCEKS is not case-sensitive. This makes it difficult to migrate from JCEKS to case-sensitive implementations.
HADOOP-11309,"System class pattern package.Foo should match package.Foo$Bar, too","Currently when job classloader is enabled and the user specifies {{package.Foo}} as a system class explicitly, nested classes are not considered system classes."
HADOOP-11307,create-release script should run git clean first,
HADOOP-11301,[optionally] update jmx cache to drop old metrics,"MetricsSourceAdapter::updateJmxCache() skips updating the info cache if no new metric is added since last time:
{code}
      int oldCacheSize = attrCache.size();
      int newCacheSize = updateAttrCache();
      if (oldCacheSize < newCacheSize) {
        updateInfoCache();
      }
{code}
This behavior is not desirable in some applications. For example nntop (HDFS-6982) reports the top users via jmx. The list is updated after each report. The previously reported top users hence should be removed from the cache upon each report request.

In our production run of nntop we made a change to ignore the size check and always perform updateInfoCache. I am planning to submit a patch including this change. The feature can be enabled by a configuration parameter.
"
HADOOP-11300,KMS startup scripts must not display the keystore / truststore passwords,"Sample output of the KMS startup scripts :

{noformat}
Setting KMS_HOME:          /usr/lib/hadoop-kms
Using   KMS_CONFIG:        /var/run/kms-config/
Using   KMS_LOG:           /var/log/kms-log
Using   KMS_TEMP:           /var/run/kms-tmp/
Using   KMS_HTTP_PORT:     16000
Using   KMS_ADMIN_PORT:     16001
Using   KMS_MAX_THREADS:     250
Using   KMS_SSL_KEYSTORE_FILE:     /etc/conf/kms-keystore.jks
Using   KMS_SSL_KEYSTORE_PASS:     keystorepass
Using   CATALINA_BASE:       /var/lib/kms/tomcat-deployment
Using   KMS_CATALINA_HOME:       /usr/lib/hadoop-kms/lib/bigtop-tomcat
Setting CATALINA_OUT:        /var/log/kms-log/kms-catalina.out
Setting CATALINA_PID:        /tmp/kms.pid

Using   CATALINA_OPTS:       ..... -Djavax.net.ssl.trustStorePassword=truststorepass ....
Adding to CATALINA_OPTS:     -Dkms.home.dir=......  -Dkms.ssl.keystore.pass= keystorepass ....
{noformat}

The keystore password and truststore password are in clear text.. which should be masked"
HADOOP-11295,RPC Server Reader thread can't shutdown if RPCCallQueue is full,"If RPC server is asked to stop when RPCCallQueue is full, {{reader.join()}} will just wait there. That is because

1. The reader thread is blocked on {{callQueue.put(call);}}.
2. When RPC server is asked to stop, it will interrupt all handler threads and thus no threads will drain the callQueue."
HADOOP-11294,"Nfs3FileAttributes should not change the values of rdev, nlink and size in the constructor ","In stead, it should just take the values passed in."
HADOOP-11291,Log the cause of SASL connection failures,"{{UGI#doAs}} will no longer log a PriviledgedActionException unless LOG.isDebugEnabled() == true. HADOOP-10015 made this change because it was decided that users calling {{UGI#doAs}} should be responsible for logging the error when catching an exception. Also, the log was confusing in certain situations (see more details in HADOOP-10015).

However, as Daryn noted, this log message was very helpful in cases of debugging security issues.

As an example, we would use to see this in the DN logs before HADOOP-10015:
{code}
2014-10-20 11:28:02,112 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs/hostA.com@REALM.COM (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Generic error (description in e-text) (60) - NO PREAUTH)]
2014-10-20 11:28:02,112 WARN org.apache.hadoop.ipc.Client: Couldn't setup connection for hdfs/hostA.com@REALM.COM to hostB.com/101.01.010:8022
2014-10-20 11:28:02,112 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs/hostA.com@REALM.COM (auth:KERBEROS) cause:java.io.IOException: Couldn't setup connection for hdfs/hostA.com@REALM.COM to hostB.com/101.01.010:8022
{code}

After the fix went in, the DN was upgraded, and only logs:
{code}
2014-10-20 14:11:40,712 WARN org.apache.hadoop.ipc.Client: Couldn't setup connection for hdfs/hostA.com@REALM.COM to hostB.com/101.01.010:8022
2014-10-20 14:11:40,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: hostB.com/101.01.010:8022
{code}

It'd be good to add more logging information about the cause of a SASL connection failure.

Thanks to [~qwertymaniac] for reporting this."
HADOOP-11289,Fix typo in RpcUtil log message,"From RpcUtil.java:

        LOG.info(""Malfromed RPC request from "" + e.getRemoteAddress());

s/Malfromed/malformed/"
HADOOP-11287,Simplify UGI#reloginFromKeytab for Java 7+,"HADOOP-10786 uses reflection to make {{UGI#reloginFromKeytab}} work with Java 6/7/8. In 2.7 Java 6 will no longer be supported, thus the code can be simplified."
HADOOP-11286,Map/Reduce dangerously adds Guava @Beta class to CryptoUtils,"See HDFS-7040 for more background/details.

In recent 2.6.0-SNAPSHOTs, the use of LimitInputStream was added to CryptoUtils. This is part of the API components of Hadoop, which severely impacts users who were utilizing newer versions of Guava, where the @Beta and @Deprecated class, LimitInputStream, has been removed (removed in version 15 and later), beyond the impact already experienced in 2.4.0 as identified in HDFS-7040."
HADOOP-11283,Potentially unclosed SequenceFile.Writer in DistCpV1#setup(),"{code}
    SequenceFile.Writer src_writer = SequenceFile.createWriter(jobfs, jobConf,
        srcfilelist, LongWritable.class, FilePair.class,
        SequenceFile.CompressionType.NONE);

    Path dstfilelist = new Path(jobDirectory, ""_distcp_dst_files"");
    SequenceFile.Writer dst_writer = SequenceFile.createWriter(jobfs, jobConf,
        dstfilelist, Text.class, Text.class,
        SequenceFile.CompressionType.NONE);
{code}
If creation of dst_writer throws exception, src_writer would be left unclosed since there is no finally clause doing that for the above code."
HADOOP-11282,Skip NFS TestShellBasedIdMapping tests that are irrelevant on Windows.,"{{TestShellBasedIdMapping}} contains several tests that fail on Windows.  The underlying implementation of {{ShellBasedIdMapping}} assumes a Unix-like environment, forking out to call things like {{bash}} and {{getent}}.  The output parsing also assumes Unix-like output from those commands.  The tests aren't currently relevant on Windows, because NFS isn't implemented for Windows.  Whenever that work is done, {{ShellBasedIdMapping}} will need a much different implementation to support Windows."
HADOOP-11280,TestWinUtils#testChmod fails after removal of NO_PROPAGATE_INHERIT_ACE.,"As part of the Windows YARN secure container executor changes in YARN-2198, {{chmod}} calls no longer use the {{NO_PROPAGATE_INHERIT_ACE}} flag.  This change in behavior violates one of the assertions in {{TestWinUtils#testChmod}}, so we need to update the test."
HADOOP-11274,ConcurrentModificationException in Configuration Copy Constructor,"Exception as below happens in doing some configuration update in parallel:
{noformat}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
	at java.util.HashMap$EntryIterator.next(HashMap.java:962)
	at java.util.HashMap$EntryIterator.next(HashMap.java:960)
	at java.util.HashMap.putAllForCreate(HashMap.java:554)
	at java.util.HashMap.<init>(HashMap.java:298)
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:703)
{noformat}
In a constructor of Configuration - public Configuration(Configuration other), the copy of updatingResource data structure in copy constructor is not synchronized properly. 
Configuration.get() eventually calls loadProperty() where updatingResource gets updated. So, whats happening here is one thread is trying to do copy of Configuration as demonstrated in stack trace and other thread is doing Configuration.get(key) and than ConcurrentModificationException occurs because copying of updatingResource is not synchronized in constructor. 
We should make the update to updatingResource get synchronized, and also fix other tiny synchronized issues there.
"
HADOOP-11273,TestMiniKdc failure: login options not compatible with IBM JDK,"When running test with IBM JDK, the testcase in /hadoop-common-project/hadoop-minikdc/src/test/java/org/apache/hadoop/minikdc/TestMiniKdc failed due to incompatible login options for IBM Java.
The login options needs to update to IBM Java options.

Testcases failed with the following stack:
javax.security.auth.login.LoginException: Bad JAAS configuration: unrecognized option: isInitiator
	at com.ibm.security.jgss.i18n.I18NException.throwLoginException(I18NException.java:165)
	at com.ibm.security.auth.module.Krb5LoginModule.checkForUnsupportedOptions(Krb5LoginModule.java:1406)
	at com.ibm.security.auth.module.Krb5LoginModule.parseJAASConfigOptions(Krb5LoginModule.java:954)
	at com.ibm.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
	at java.lang.reflect.Method.invoke(Method.java:619)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:796)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:211)
	at javax.security.auth.login.LoginContext$5.run(LoginContext.java:733)
	at javax.security.auth.login.LoginContext$5.run(LoginContext.java:731)
	at java.security.AccessController.doPrivileged(AccessController.java:366)
	at javax.security.auth.login.LoginContext.invokeCreatorPriv(LoginContext.java:730)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:600)
	at org.apache.hadoop.minikdc.TestMiniKdc.testKerberosLogin(TestMiniKdc.java:137)"
HADOOP-11272,Allow ZKSignerSecretProvider and ZKDelegationTokenSecretManager to use the same curator client,"Currently the {{AuthenticationFilter}} cannot be configured to use {{ZKDelegationTokenSecretManager}} and {{ZKSignerSecretProvider}} together if the Zookeeper is a kerberized cluster. 

The zookeeper client requires certain configuration parameters to be set as SystemProperties (The issue has also been reported [here|https://issues.apache.org/jira/browse/YARN-913?focusedCommentId=14142086&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14142086]). Thus if a single process needs to use both {{ZKDelegationTokenSecretManager}} and {{ZKSignerSecretProvider}} then it should use the same client.
"
HADOOP-11271,Use Time.monotonicNow() in Shell.java instead of Time.now(),"Use {{Time.monotonicNow()}} instead of {{Time.now()}} in Shell.java to keep track of the last executed time.

Using Time.monotonicNow() in elapsed time calculation usecases will be accurate and safe from system time changes."
HADOOP-11269,Add java 8 profile for hadoop-annotations,hadoop-annotations fails to build out-of-the-box under Java 8 because it lacks the profile to add {{tools.jar}} into the classpath of {{javac}}. This jira proposes to add a new build profile for Java 8 in hadoop-annotations.
HADOOP-11268,Update BUILDING.txt to remove the workaround for tools.jar,"After HADOOP-10563 lands in branch-2. The workaround for tools.jar documented in BUILDING.txt is no longer required.

We should update the document to reflect this change."
HADOOP-11267,TestSecurityUtil fails when run with JDK8 because of empty principal names,"Running {{TestSecurityUtil}} on JDK8 will fail:

{code}
java.lang.IllegalArgumentException: Empty nameString not allowed
	at sun.security.krb5.PrincipalName.validateNameStrings(PrincipalName.java:171)
	at sun.security.krb5.PrincipalName.<init>(PrincipalName.java:393)
	at sun.security.krb5.PrincipalName.<init>(PrincipalName.java:460)
	at javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:120)
	at org.apache.hadoop.security.TestSecurityUtil.isOriginalTGTReturnsCorrectValues(TestSecurityUtil.java:57)
{code}

In JDK8, PrincipalName checks that its name is not empty and throws an IllegalArgumentException if it is empty. This didn't happen in JDK6/7."
HADOOP-11266,Remove no longer supported activation properties for packaging from pom,"According to HADOOP-8925, packaging rpm and deb are no longer supported -- both options should be removed from the pom.
"
HADOOP-11265,Credential and Key Shell Commands not available on Windows,Must add the credential and key commands to the hadoop.cmd file for windows environments.
HADOOP-11261,Set custom endpoint for S3A,"Use a config setting to allow customizing the used AWS region. 

It also enables using a custom url pointing to an S3-compatible object store."
HADOOP-11260,Patch up Jetty to disable SSLv3,Hadoop uses an older version of Jetty that allows SSLv3. We should fix it up. 
HADOOP-11257,"Update ""hadoop jar"" documentation to warn against using it for launching yarn jars","We should update the ""hadoop jar"" documentation to warn against using it for launching yarn jars."
HADOOP-11256,Some site docs have inconsistent appearance,"The site docs of hadoop-gridmix, hadoop-rumen and hadoop-mapreduce-client-hs use default site.css because they do not have it in resources.
"
HADOOP-11254,Promoting AccessControlList to be public,"The motivation of promoting AccessControlList to be a public API is to facilitate the users to programmatically parse or construct an ACL string. A typical use case may be the timeline domain, where we have the client lib to accept strings complied with ACL string format to define the authorized readers/writers. It will be more convenient and less buggy if users can compose this string with AccessControlList."
HADOOP-11253,Hadoop streaming test TestStreamXmlMultipleRecords fails on Windows,"All the tests in TestStreamXmlMultipleRecords fail on Windows with errors similar to this -

{noformat}
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.streaming.TestStreaming.assertOutput(TestStreaming.java:177)
	at org.apache.hadoop.streaming.TestStreaming.checkOutput(TestStreaming.java:166)
	at org.apache.hadoop.streaming.TestStreaming.testCommandLine(TestStreaming.java:201)
	at org.apache.hadoop.streaming.TestStreamXmlMultipleRecords.testStreamXmlMultiInnerFast(TestStreamXmlMultipleRecords.java:113)
{noformat}"
HADOOP-11252,RPC client does not time out by default,"The RPC client has a default timeout set to 0 when no timeout is passed in. This means that the network connection created will not timeout when used to write data. The issue has shown in YARN-2578 and HDFS-4858. Timeouts for writes then fall back to the tcp level retry (configured via tcp_retries2) and timeouts between the 15-30 minutes. Which is too long for a default behaviour.

Using 0 as the default value for timeout is incorrect. We should use a sane value for the timeout and the ""ipc.ping.interval"" configuration value is a logical choice for it. The default behaviour should be changed from 0 to the value read for the ping interval from the Configuration.

Fixing it in common makes more sense than finding and changing all other points in the code that do not pass in a timeout.

Offending code lines:
https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java#L488
and 
https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java#L350"
HADOOP-11250,fix endmacro of set_find_shared_library_without_version in CMakeLists,"There is a small nit for {{set_find_shared_library_without_version}} in CMakeLists.txt:
{code}
endmacro(set_find_shared_library_version LVERS)
{code}

should be 
{code}
endmacro(set_find_shared_library_without_version)
{code}"
HADOOP-11248,Add hadoop configuration to disable Azure Filesystem metrics collection,"Today whenever Azure filesystem is used, metrics collection is enabled using class AzureFileSystemMetricsSystem. Metrics being collected includes bytes transferred and throughput.

In some situation, we do not want to collect metrics for Azure file system. E.g. for WebHCat server. We need to introduce a new configuration ""fs.azure.skip.metrics"" to disable metrics collection.
"
HADOOP-11247,Fix a couple javac warnings in NFS,"This JIRA is to fix 2 javac warnings, which are overlooked in HADOOP-11195.
{format}
241a242,243
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/conf/NfsConfiguration.java:[46,41] [deprecation] NFS_USERGROUP_UPDATE_MILLIS_KEY in org.apache.hadoop.nfs.nfs3.Nfs3Constant has been deprecated
> [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/conf/NfsConfiguration.java:[48,41] [deprecation] NFS_STATIC_MAPPING_FILE_KEY in org.apache.hadoop.nfs.nfs3.Nfs3Constant has been deprecated
{format}"
HADOOP-11246,Move jenkins to Java 7,"As hadoop 2.7 will drop the support of Java 6, the jenkins slaves should be compiling code using Java 7."
HADOOP-11243,SSLFactory shouldn't allow SSLv3,"We should disable SSLv3 in SSLFactory. This affects MR shuffle among others. 
See [CVE-2014-3566 |http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-3566]

We have {{context = SSLContext.getInstance(""TLS"");}} in SSLFactory, but when I checked, I could still connect with SSLv3."
HADOOP-11241,TestNMSimulator fails sometimes due to timing issue,"TestNMSimulator fails sometimes due to timing issues. From a failure -
{noformat}
2014-10-16 23:21:42,343 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:registerNodeManager(337)) - NodeManager from node node1(cmPort: 0 httpPort: 80) registered with capability: <memory:10240, vCores:10>, assigned nodeId node1:0
2014-10-16 23:21:42,397 ERROR delegation.AbstractDelegationTokenSecretManager (AbstractDelegationTokenSecretManager.java:run(642)) - ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2014-10-16 23:21:42,400 INFO  rmnode.RMNodeImpl (RMNodeImpl.java:handle(423)) - node1:0 Node Transitioned from NEW to RUNNING
2014-10-16 23:21:42,404 INFO  fair.FairScheduler (FairScheduler.java:addNode(825)) - Added node node1:0 cluster capacity: <memory:10240, vCores:10>
2014-10-16 23:21:42,407 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:18088
2014-10-16 23:21:42,409 ERROR delegation.AbstractDelegationTokenSecretManager (AbstractDelegationTokenSecretManager.java:run(642)) - ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2014-10-16 23:21:42,410 INFO  ipc.Server (Server.java:stop(2437)) - Stopping server on 18032
2014-10-16 23:21:42,412 INFO  ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 18032
2014-10-16 23:21:42,412 INFO  ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
{noformat}"
HADOOP-11238,Update the NameNode's Group Cache in the background when possible,"This patch addresses an issue where the namenode pauses during group resolution by only allowing a single group resolution query on expiry. There are two scenarios:
1. When there is not yet a value in the cache, all threads which make a request will block while a single thread fetches the value.
2. When there is already a value in the cache and it is expired, the new value will be fetched in the background while the old value is used by other threads
This is handled by guava's cache.

Negative caching is a feature built into the groups cache, and since guava's caches don't support different expiration times, we have a separate negative cache which masks the guava cache: if an element exists in the negative cache and isn't expired, we return it.

In total the logic for fetching a group is:
1. If username exists in static cache, return the value (this was already present)
2. If username exists in negative cache and negative cache is not expired, raise an exception as usual
3. Otherwise Defer to guava cache (see two scenarios above)


Original Issue Below:
----------------------------
Our namenode pauses for 12-60 seconds several times every hour. During these pauses, no new requests can come in.

Around the time of pauses, we have log messages such as:
2014-10-22 13:24:22,688 WARN org.apache.hadoop.security.Groups: Potential performance problem: getGroups(user=xxxxx) took 34507 milliseconds.

The current theory is:
1. Groups has a cache that is refreshed periodically. Each entry has a cache expiry.
2. When a cache entry expires, multiple threads can see this expiration and then we have a thundering herd effect where all these threads hit the wire and overwhelm our LDAP servers (we are using ShellBasedUnixGroupsMapping with sssd, how this happens has yet to be established)
3. group resolution queries begin to take longer, I've observed it taking 1.2 seconds instead of the usual 0.01-0.03 seconds when measuring in the shell `time groups myself`
4. If there is mutual exclusion somewhere along this path, a 1 second pause could lead to a 60 second pause as all the threads compete for the resource. The exact cause hasn't been established

Potential solutions include:
1. Increasing group cache time, which will make the issue less frequent
2. Rolling evictions of the cache so we prevent the large spike in LDAP queries
3. Gate the cache refresh so that only one thread is responsible for refreshing the cache

"
HADOOP-11236,NFS: Fix javadoc warning in RpcProgram.java,"Fix following javadoc warning during hadoop-nfs compilation:

{code}
:
:
[WARNING] Javadoc Warnings
[WARNING] /home/abutala/work/hadoop/hadoop-trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcProgram.java:73: warning - @param argument ""DatagramSocket"" is not a parameter name.
{code}
"
HADOOP-11233,hadoop.security.kms.client.encrypted.key.cache.expiry property spelled wrong in core-default,"Theres' a spurious {{""}} at the start of the kms cache entry
{code}
<property>
  <name>""hadoop.security.kms.client.encrypted.key.cache.expiry</name>
  <value>43200000</value>
  <description>
    Cache expiry time for a Key, after which the cache Queue for this
    key will be dropped. Default = 12hrs
  </description>
</property>
{code}"
HADOOP-11231,Remove dead code in ServletUtil,Some code in ServletUtil is dead after the JSP UI is phased out. This jira propose to clean them up.
HADOOP-11230,"Add missing dependency of bouncycastle for kms, httpfs, hdfs, MR and YARN","HADOOP-10847 removes uses of {{sun.security.x509.*}} packages, but causes tests that use {{KeyStoreTestUtil}} outside of hadoop-common to fail.  See [this comment|https://issues.apache.org/jira/browse/HADOOP-10847?focusedCommentId=14196756&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14196756] for details.

This was originally a duplicate of HADOOP-10847, but we've repurposed it to fix the {{NoClassDefFoundError}}."
HADOOP-11228,winutils task: unsecure path should not call AddNodeManagerAndUserACEsToObject,winutils task create path is broken after YARN-2198
HADOOP-11221,"JAVA specification for hashcode does not enforce it to be non-negative, but IdentityHashStore assumes System.identityHashCode() is non-negative","The following code snippet shows that IdentityHashStore assumes the hashCode is always non-negative.

{code:borderStyle=solid}
   private void putInternal(Object k, Object v) {
     int hash = System.identityHashCode(k);
     final int numEntries = buffer.length / 2;
     int index = hash % numEntries;
	 ...
   }
   
  private int getElementIndex(K k) {
     ...
     final int numEntries = buffer.length / 2;
     int hash = System.identityHashCode(k);
     int index = hash % numEntries;
     int firstIndex = index;
     ...
  }
{code}"
HADOOP-11217,Disable SSLv3 in KMS,"We should disable SSLv3 in KMS to protect against the POODLEbleed vulnerability.
See [CVE-2014-3566|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-3566]

We have {{sslProtocol=""TLS""}} set to only allow TLS in ssl-server.xml, but when I checked, I could still connect with SSLv3.  There documentation is somewhat unclear in the tomcat configs between {{sslProtocol}}, {{sslProtocols}}, and {{sslEnabledProtocols}} and what each value they take does exactly.  From what I can gather, {{sslProtocol=""TLS""}} actually includes SSLv3 and the only way to fix this is to explicitly list which TLS versions we support."
HADOOP-11216,Improve Openssl library finding,"When we compile Openssl 1.0.0\(x\) or 1.0.1\(x\) using default options, there will be {{libcrypto.so.1.0.0}} in output lib dir, so we expect this version suffix in cmake build file
{code}
SET(STORED_CMAKE_FIND_LIBRARY_SUFFIXES CMAKE_FIND_LIBRARY_SUFFIXES)
set_find_shared_library_version(""1.0.0"")
SET(OPENSSL_NAME ""crypto"")
....
{code}
If we don't bundle the crypto shared library in Hadoop distribution, then Hadoop will try to find crypto library in system path when running.
But in real linux distribution, there may be no {{libcrypto.so.1.0.0}} or {{libcrypto.so}} even the system embedded openssl is 1.0.1\(x\).  Then we need to make symbolic link.

This JIRA is to improve the Openssl library finding."
HADOOP-11213,Typos in html pages: SecureMode and EncryptedShuffle,"In SecureMode.html, 
{noformat}
banned.users	|   hfds,yarn,mapred,bin	
{noformat}
Here hfds should be hdfs.

In EncryptedShuffle.html,
{noformat}
hadoop.ssl.server.conf	|  ss-server.xml
hadoop.ssl.client.conf	|  ss-client.xml	
{noformat}
Here the two xml files should be ssl-*."
HADOOP-11211,mapreduce.job.classloader.system.classes semantics should be order-independent,"If we want to include package foo.bar.* but exclude all sub packages named foo.bar.tar.* in system classes, configuring ""mapreduce.job.classloader.system.classes=foo.bar.,-foo.bar.tar."" won't work. foo.bar.tar will still be pulled in. But if we change the order:

""mapreduce.job.classloader.system.classes=-foo.bar.tar.,foo.bar."", then it will work.

This bug is due to the implementation of ApplicationClassLoaser#isSystemClass in hadoop-common, where we simply return the matching result immediately when the class name hits the first match (either positive or negative)."
HADOOP-11209,Configuration#updatingResource/finalParameters are not thread-safe,"{{Configuration}} objects are not fully thread-safe, which causes problems in multi-threaded frameworks like Spark that use these configurations to interact with existing Hadoop APIs (such as InputFormats).

SPARK-2546 is an example of a problem caused by this lack of thread-safety.  In that bug, multiple concurrent modifications of the same Configuration (in third-party code) caused an infinite loop because Configuration's internal {{java.util.HashMap}} is not thread-safe.

One workaround is for our code to clone Configuration objects; unfortunately, this also suffers from thread-safety issues on older Hadoop versions because Configuration's constructor wasn't thread-safe (HADOOP-10456).

[Looking at a recent version of Configuration.java|https://github.com/apache/hadoop/blob/d989ac04449dc33da5e2c32a7f24d59cc92de536/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L666], it seems that the private {{updatingResource}} HashMap and {{finalParameters}} HashSet fields the only non-thread-safe collections in Configuration (Java's {{Properties}} class is thread-safe), so I don't think that it would be hard to make Configuration fully thread-safe."
HADOOP-11207,DelegationTokenAuthenticationHandler needs to support DT operations for proxy user,"Currently, DelegationTokenAuthenticationHandler only support DT operations for the request user after it passes the authentication. However, it should also support the request user to do DT operations on behalf of the proxy user.

Timeline server is using the authentication filter for DT operations instead of traditional RPC-based ones. It needs this feature to enable the proxy user to use the timeline service (YARN-2676)."
HADOOP-11201,Hadoop Archives should support globs resolving to files,"Consider the following scenario:
{code}
$ hadoop fs -ls /tmp/harsrc/dir2/dir3
Found 5 items
-rw-r--r--   1 blah blah          0 2014-10-13 20:59 /tmp/harsrc/dir2/dir3/file31
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 /tmp/harsrc/dir2/dir3/file32
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 /tmp/harsrc/dir2/dir3/file33
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 /tmp/harsrc/dir2/dir3/file34
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 /tmp/harsrc/dir2/dir3/file35
{code}

Archive 'dir3/file3*':
{code}
$ hadoop archive -Dmapreduce.framework.name=local -archiveName fileStar.har -p /tmp/harsrc 'dir2/dir3/file*' /tmp/hardst_local
$ hadoop fs -ls -R har:/tmp/hardst_local/fileStar.har
drwxr-xr-x   - blah blah          0 2014-10-13 22:32 har:///tmp/hardst_local/fileStar.har/dir2
{code}

Archiving dir3 (directory) which is equivalent to the above works.
{code}
$ hadoop archive -Dmapreduce.framework.name=local -archiveName dir3.har -p /tmp/harsrc 'dir2/dir3' /tmp/hardst_local
$ hadoop fs -ls -R har:/tmp/hardst_local/dir3.har
14/10/14 02:06:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - blah blah          0 2014-10-13 22:32 har:///tmp/hardst_local/dir3.har/dir2
drwxr-xr-x   - blah blah          0 2014-10-14 01:51 har:///tmp/hardst_local/dir3.har/dir2/dir3
-rw-r--r--   1 blah blah          0 2014-10-13 20:59 har:///tmp/hardst_local/dir3.har/dir2/dir3/file31
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 har:///tmp/hardst_local/dir3.har/dir2/dir3/file32
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 har:///tmp/hardst_local/dir3.har/dir2/dir3/file33
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 har:///tmp/hardst_local/dir3.har/dir2/dir3/file34
-rw-r--r--   1 blah blah          0 2014-10-14 01:51 har:///tmp/hardst_local/dir3.har/dir2/dir3/file35
{code}


"
HADOOP-11200,"HttpFS proxyuser, doAs param is case sensitive",It appears that the doAs processing in HttpFS for proxyusers is case sensitive.
HADOOP-11198,Fix typo in javadoc for FileSystem#listStatus(),"{code}
   * @return the statuses of the files/directories in the given patch
{code}
'patch' should be path"
HADOOP-11195,Move Id-Name mapping in NFS to the hadoop-common area for better maintenance,"Per [~aw]'s suggestion in HDFS-7146, creating this jira to move the id-name mapping implementation (IdUserGroup.java) to the framework that cache user and group info in hadoop-common area (hadoop-common/src/main/java/org/apache/hadoop/security) 

Thanks [~brandonli] and [~aw] for the review and discussion in HDFS-7146.
"
HADOOP-11194,Ignore .keep files,"Given we don't need to keep empty directories, I suppose we can get rid of the .keep files. "
HADOOP-11193,Fix uninitialized variables in NativeIO.c,"This was caught by TestNativeIO#testFstat test. 

Looks like an uninitialized variable mode below in NativeIO.c#Java_org_apache_hadoop_io_nativeio_NativeIO_00024POSIX_fstat around line ~278. 

ifdef WINDOWS
  LPWSTR owner = NULL;
  LPWSTR group = NULL;
  int mode;
  jstring jstr_owner = NULL;
... 


Test Report:

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.io.nativeio.TestNativeIO
Tests run: 18, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 2.214 sec <<< FAILURE! - in org.apache.hadoop.io.native
io.TestNativeIO
testFstat(org.apache.hadoop.io.nativeio.TestNativeIO)  Time elapsed: 0.514 sec  <<< FAILURE!
java.lang.AssertionError: Stat mode field should indicate a regular file expected:<32768> but was:<53248>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.apache.hadoop.io.nativeio.TestNativeIO.testFstat(TestNativeIO.java:96)


Results :

Failed tests:
  TestNativeIO.testFstat:96 Stat mode field should indicate a regular file expected:<32768> but was:<53248>

Tests run: 18, Failures: 1, Errors: 0, Skipped: 1"
HADOOP-11188,hadoop-azure: automatically expand page blobs when they become full,"Right now, page blobs are initialized to a fixed size (fs.azure.page.blob.size) and cannot be expanded. This task is to make them automatically expand when they get to be nearly full.

Design: if a write occurs that does not have enough room in the file to finish, then flush all preceding operations, extend the file, and complete the write. This will be synchronized (to have exclusive access) in access to PageBlobOutputStream so there won't be race conditions.

The file will be extended by fs.azure.page.blob.extension.size bytes, which must be a multiple of 512. The internal default for fs.azure.page.blob.extension size will be 128 * 1024 * 1024. The minimum extension size will be 4 * 1024 * 1024 which is the maximum write size, so the new write will finish. 

Extension will stop when the file size reaches 1TB. The final extension may be less than fs.azure.page.blob.extension.size if the remainder (1TB - current_file_size) is smaller than fs.azure.page.blob.extension.size.

An alternative to this is to make the default size 1TB. This is much simpler to implement. It's a one-line change. Or even simpler, don't change it at all because it is adequate for HBase.

Rationale for this file size extension feature:

1) be able to download files to local disk easily with CloudXplorer and similar tools. Downloading a 1TB page blob is not practical if you don't have 1TB disk space since on the local side it expands to the full file size, locally filled with zeros where there is no valid data.

2) don't make customers uncomfortable when they see large 1TB files. They often ask if they have to pay for it, even though they only pay for the space actually used in the page blob.

I think rationale 2 is a relatively minor issue, because 98% of customers for HBase will never notice. They will just use it and not look at what kind of files are used for the logs. They don't pay for the unused space, so it is not a problem for them. We can document this. Also, if they use hadoop fs -ls, they will see the actual size of the files since I put in a fix for that.

Rationale 1 is a minor issue because you cannot interpret the data on your local file system anyway due to the data format. So really, the only reason to copy data locally in its binary format would be if you are moving it around or archiving it. Copying a 1TB page blob from one location in the cloud to another is pretty fast with smart copy utilities that don't actually move the 0-filled parts of the file.

Nevertheless, this is a convenience feature for users. They won't have to worry about setting fs.azure.page.blob.size under normal circumstances and can make the files grow as big as they want.

If we make the change to extend the file size on the fly, that introduces new possible error or failure modes for HBase. We should included retry logic. 
"
HADOOP-11187,NameNode - KMS communication fails after a long period of inactivity,"As reported by [~atm] :

The issue is due to the authentication token that the NN has to talk to the KMS is expiring, AND the signature secret provider in the KMS authentication filter is discarding the old secret after 2x the authentication token validity period.
If the token being supplied is under 1x the validity lifetime then the token will authenticate just fine. If the token being supplied is between 1x-2x the validity lifetime, then the token can be validated but it will be expired, so a 401 will be returned to the client and it will get a new token. But if the token being supplied is greater than 2x the validity lifetime, then the KMS authentication filter will not even be able to validate the token, and will return a 403, which will cause the client to not retry authentication to the KMS.

The KMSClientProvider needs to be modified to retry authentication even in the above case"
HADOOP-11186,"documentation should talk about hadoop.htrace.spanreceiver.classes, not hadoop.trace.spanreceiver.classes","The documentation should talk about hadoop.htrace.spanreceiver.classes, not hadoop.trace.spanreceiver.classes (note the H)"
HADOOP-11184,Update Hadoop's lz4 to r123,We should update Hadoop's copy of the lz4 compression library to the latest version.
HADOOP-11183,Memory-based S3AOutputstream,"Currently s3a buffers files on disk(s) before uploading. This JIRA investigates adding a memory-based upload implementation.

The motivation is evidently performance: this would be beneficial for users with high network bandwidth to S3 (EC2?) or users that run Hadoop directly on an S3-compatible object store (FYI: my contributions are made in name of Amplidata). "
HADOOP-11182,GraphiteSink emits wrong timestamps,"the org.apache.hadoop.metrics2.sink.GraphiteSink class emits metrics at the configured time period, but the timestamps written only change every 128 seconds, even it the configured time period in the configuration file is much shorter.

This is due to a bug in line 93:

{code:java}
092            // Round the timestamp to second as Graphite accepts it in such format.
093            int timestamp = Math.round(record.timestamp() / 1000.0f);
{code}

The timestamp property is a long and is divided by a float which yields a result that is not precise enough and yields same valued results for timestamps that lie up to 128 seconds apart. Also, the result is then written into an int variable.

One solution would be to divide by 1000.0d, but the best fix would be to not even convert to a decimal format in the first place. Instead one could replace the line with the following:

{code:java}
   long timestamp = record.timestamp() / 1000L;
{code}"
HADOOP-11181,o.a.h.security.token.delegation.DelegationTokenManager should be more generalized to handle other DelegationTokenIdentifier,"While DelegationTokenManager can set external secretManager, it have the assumption that the token is going to be o.a.h.security.token.delegation.DelegationTokenIdentifier, and use DelegationTokenIdentifier method to decode a token. 
{code}
  @SuppressWarnings(""unchecked"")
  public UserGroupInformation verifyToken(Token<DelegationTokenIdentifier>
      token) throws IOException {
    ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());
    DataInputStream dis = new DataInputStream(buf);
    DelegationTokenIdentifier id = new DelegationTokenIdentifier(tokenKind);
    id.readFields(dis);
    dis.close();
    secretManager.verifyToken(id, token.getPassword());
    return id.getUser();
  }
{code}

It's not going to work it the token kind is other than web.DelegationTokenIdentifier. For example, RM want to reuse it but hook it to RMDelegationTokenSecretManager and RMDelegationTokenIdentifier, which has the customized way to decode a token."
HADOOP-11179,Tarball as local resource type archive fails to localize on Windows ,"When trying Tez out on Windows, the tez tar.gz failed to localize on Windows env. 

Also. reproduced the same issue with a similar tarball when used with distributed cache with an MR sleep job."
HADOOP-11178,Fix findbugs exclude file,The findbugs exclude file in {{hadoop-common-project/hadoop-common/dev-support/findbugsExcludeFile.xml}} is missing a matching close tag
HADOOP-11176,KMSClientProvider authentication fails when both currentUgi and loginUgi are a proxied user,"In a secure environment, with kerberos, when the KMSClientProvider instance is created in the context of a proxied user, The initial SPNEGO handshake is made with the currentUser (the proxied user) as the Principal.. this will fail, since the proxied user is not logged in.
The handshake must be done using the real user.
 "
HADOOP-11175,Fix several issues of hadoop security configuration in user doc.,"There are several issues of secure mode in user doc:

{{dfs.namenode.secondary.keytab.file}} should be {{dfs.secondary.namenode.keytab.file}}, 
{{dfs.namenode.secondary.kerberos.principal}} should be 
{{dfs.secondary.namenode.kerberos.principal}}.

{{dfs.namenode.kerberos.https.principal}} doesn't exist, it should be {{dfs.namenode.kerberos.internal.spnego.principal}}.
{{dfs.namenode.secondary.kerberos.https.principal}} doesn't exist, it should be {{dfs.secondary.namenode.kerberos.internal.spnego.principal}}.
{{dfs.datanode.kerberos.https.principal}} doesn't exist, we can remove it."
HADOOP-11174,Delegation token for KMS should only be got once if it already exists,"When submit MapReduce job in security mode, we need to collect delegation tokens (i.e. delegation token for NameNode, KMS).

{{addDelegationTokens}} is invoked several times, currently dt for NN is got only once if exists.  But  dt for KMS is got every time, we should fix this.

Running in real cluster:
{quote}
14/10/08 02:30:52 INFO security.TokenCache: Got dt for hdfs://hnode1.sh.intel.com:9000; Kind: HDFS_DELEGATION_TOKEN, Service: 10.239.47.8:9000, Ident: (HDFS_DELEGATION_TOKEN token 13 for user)
14/10/08 02:30:52 WARN token.Token: Cannot find class for token kind kms-dt
14/10/08 02:30:52 INFO security.TokenCache: Got dt for hdfs://hnode1.sh.intel.com:9000; Kind: kms-dt, Service: 10.239.47.8:16000, Ident: 00 04 75 73 65 72 04 79 61 72 6e 00 8a 01 48 eb e0 d2 76 8a 01 49 0f ed 56 76 06 02
14/10/08 02:30:53 WARN token.Token: Cannot find class for token kind kms-dt
14/10/08 02:30:53 INFO security.TokenCache: Got dt for hdfs://hnode1.sh.intel.com:9000; Kind: kms-dt, Service: 10.239.47.8:16000, Ident: 00 04 75 73 65 72 04 79 61 72 6e 00 8a 01 48 eb e0 d3 85 8a 01 49 0f ed 57 85 07 02
14/10/08 02:30:53 WARN token.Token: Cannot find class for token kind kms-dt
14/10/08 02:30:53 INFO security.TokenCache: Got dt for hdfs://hnode1.sh.intel.com:9000; Kind: kms-dt, Service: 10.239.47.8:16000, Ident: 00 04 75 73 65 72 04 79 61 72 6e 00 8a 01 48 eb e0 d3 8e 8a 01 49 0f ed 57 8e 08 02
14/10/08 02:30:53 WARN token.Token: Cannot find class for token kind kms-dt
14/10/08 02:30:53 INFO security.TokenCache: Got dt for hdfs://hnode1.sh.intel.com:9000; Kind: kms-dt, Service: 10.239.47.8:16000, Ident: 00 04 75 73 65 72 04 79 61 72 6e 00 8a 01 48 eb e0 d4 ef 8a 01 49 0f ed 58 ef 09 02
{quote}"
HADOOP-11173,Improve error messages for some KeyShell commands,"A few KeyShell commands don't print the exception messages and just swallow the exception, resulting in a non-specific error message."
HADOOP-11172,Improve error message in Shell#runCommand on OutOfMemoryError,"TestWebHdfsFileSystemContract failed with the following error. But the real reason is not. Instead, it's because of ""max user processes"" of ulimit setting is too low. Need a more informative error message here.

{code}
2014-09-04 20:09:08,396 ERROR mortbay.log (Slf4jLog.java:warn(87)) - /webhdfs/v1/test/testSeek/zero
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:693)
        at org.apache.hadoop.hdfs.LeaseRenewer.put(LeaseRenewer.java:320)
        at org.apache.hadoop.hdfs.DFSClient.beginFileLease(DFSClient.java:789)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1605)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1526)
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.put(DatanodeWebHdfsMethods.java:234)
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.access$000(DatanodeWebHdfsMethods.java:86)
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods$1.run(DatanodeWebHdfsMethods.java:205)
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods$1.run(DatanodeWebHdfsMethods.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1626)
        at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.put(DatanodeWebHdfsMethods.java:202)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1203)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.messageComplete(HttpConnection.java:959)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:792)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}

"
HADOOP-11171,Enable using a proxy server to connect to S3a.,This exposes the AWS SDK config for a proxy (host and port) to s3a through config settings.  
HADOOP-11170,ZKDelegationTokenSecretManager fails to renewToken created by a peer,"When a ZKDTSM tries to renew a token created by a peer, It throws an Exception with message : 
""bar is trying to renew a token with wrong password"""
HADOOP-11169,Fix DelegationTokenAuthenticatedURL to pass the connection Configurator to the authenticator ,"KMS DelegationToken operation keeps throwing ""unable to find valid certification path to requested target"". It looks very much like the truststore is not being picked up."
HADOOP-11168,"Remove duplicated entry ""dfs.webhdfs.enabled"" in the user doc","The default value for {{dfs.webhdfs.enabled}} should be {{true}}, not _http/_HOST@REALM.TLD_."
HADOOP-11166,Remove ulimit from test-patch.sh,"We set a ulimit in test-patch.sh on the number of open files. We also hit this limit all the time leading to test failures.

Let's remove this. Thanks [~abayer] for finding this issue."
HADOOP-11165,TestUTF8 fails when run against java 8,"Using jdk1.8.0_20 , I got:
{code}
testGetBytes(org.apache.hadoop.io.TestUTF8)  Time elapsed: 0.007 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<쑼ь⣄鬘㟻햫紖燺[?炀⃍⑰풸낓⨵ἲꬌホ쭷㛕曬䟊⁍䴥䳠領蟭뱻宭竕昚鍳튇ꊕ혶齲쏈㠮胨䩦隼᢯䍻᝴킿喝벁ࢼ듿饭玳Մ剌䒤?䳛슟녚沖᯳?訨
牙⍖?䎠旘薑春觀葝礫⁑ﻱ⣽゚굿뒦ݦ︀偆?]古絥萟浐> but was:<쑼ь⣄鬘㟻햫紖燺[�炀⃍⑰풸낓⨵ἲꬌホ쭷㛕曬䟊⁍䴥䳠領蟭뱻宭竕昚鍳튇ꊕ혶齲쏈㠮胨䩦隼᢯䍻᝴킿喝벁ࢼ듿饭玳Մ剌䒤�䳛슟녚᯳�訨牙⍖�䎠旘薑春觀葝礫⁑ﻱ⣽゚굿뒦ݦ︀偆�]古絥萟浐>
        at junit.framework.Assert.assertEquals(Assert.java:100)
        at junit.framework.Assert.assertEquals(Assert.java:107)
        at junit.framework.TestCase.assertEquals(TestCase.java:269)
        at org.apache.hadoop.io.TestUTF8.testGetBytes(TestUTF8.java:58)

testIO(org.apache.hadoop.io.TestUTF8)  Time elapsed: 0.002 sec  <<< FAILURE!
junit.framework.ComparisonFailure: expected:<...ᨍ⁖粩⧬车﹂脖朷䝄懒댵突疼資⍣眠畠忁[?]䪐ゑ鬍鍅遻ꈸ釡> but was:<...ᨍ⁖粩⧬车﹂脖朷䝄懒댵突疼資⍣眠畠忁[�]䪐ゑ鬍鍅遻>ꈸ釡>
        at junit.framework.Assert.assertEquals(Assert.java:100)
        at junit.framework.Assert.assertEquals(Assert.java:107)
        at junit.framework.TestCase.assertEquals(TestCase.java:269)
        at org.apache.hadoop.io.TestUTF8.testIO(TestUTF8.java:86)
{code}"
HADOOP-11163,MetricsSystemImpl may miss a registered source,"Calling to {{MetricsSystemImpl#register(String name, String desc, T source)}} could miss when the name is {{null}}. This is related to my previous change in HADOOP-11105."
HADOOP-11161,Expose close method in KeyProvider to give clients of Provider implementations a hook to release resources,"The {{KMSClientProvider}} class needs to be have a {{close()}} method to shutdown back ground executor threads.

The {{DFSClient}} creates an instance of a {{KeyProvider}} during initialization. If this KP is a {{KMSClientProvider}}, this needs to be closed to prevent thread leakage"
HADOOP-11160,Fix  typo in nfs3 server duplicate entry reporting,"Fix typo: ""systms""."
HADOOP-11157,ZKDelegationTokenSecretManager never shuts down listenerThreadPool,I'm trying to integrate Solr with the DelegationTokenAuthenticationFilter and running into this issue.  The solr unit tests look for leaked threads and when I started using the ZKDelegationTokenSecretManager it started reporting leaks.  Shuting down the listenerThreadPool after the objects that use it resolves the leak threads errors.
HADOOP-11156,DelegateToFileSystem should implement getFsStatus(final Path f).,"DelegateToFileSystem only implemented getFsStatus() and didn't implement getFsStatus(final Path f). So if you call getFsStatus(final Path f), it will call  AbstractFileSystem.getFsStatus(final Path f) which will also call DelegateToFileSystem.getFsStatus(). It should implement getFsStatus(final Path f) to call fsImpl.getStatus(f) instead of calling fsImpl.getStatus() from getFsStatus().
Also based on the following description for FileContext.getFsStatus:
{code} 
/**
   * Returns a status object describing the use and capacity of the
   * file system denoted by the Parh argument p.
   * If the file system has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * 
   * @param f Path for which status should be obtained. null means the
   * root partition of the default file system. 
   *
   * @return a FsStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   */
  public FsStatus getFsStatus(final Path f) throws AccessControlException,
      FileNotFoundException, UnsupportedFileSystemException, IOException {
    if (f == null) {
      return defaultFS.getFsStatus();
    }
    final Path absF = fixRelativePart(f);
    return new FSLinkResolver<FsStatus>() {
      @Override
      public FsStatus next(final AbstractFileSystem fs, final Path p) 
        throws IOException, UnresolvedLinkException {
        return fs.getFsStatus(p);
      }
    }.resolve(this, absF);
  }
{code}
we should differentiate getFsStatus(final Path f) from getFsStatus() in DelegateToFileSystem."
HADOOP-11154,Update BUILDING.txt to state that CMake 3.0 or newer is required on Mac.,"The native code can be built on Mac now, but CMake 3.0 or newer is required.  This differs from our minimum stated version of 2.6 in BUILDING.txt.  I'd like to update BUILDING.txt to state that 3.0 or newer is required if building on Mac."
HADOOP-11153,Make number of KMS threads configurable,"Would be nice to make the # of KMS threads configurable. The Tomcat default is 200, but we can also up this."
HADOOP-11151,Automatically refresh auth token and retry on auth failure,"Enable CFS and KMS service in the cluster, initially it worked to put/copy file into encryption zone. But after a while (might be one day), it fails to put/copy file into the encryption zone with the error
java.util.concurrent.ExecutionException: java.io.IOException: HTTP status [403], message [Forbidden]

The kms.log shows below
AbstractDelegationTokenSecretManager - Updating the current master key for generating delegation tokens
2014-09-29 13:18:46,599 WARN  AuthenticationFilter - AuthenticationToken ignored: org.apache.hadoop.security.authentication.util.SignerException: Invalid signature
2014-09-29 13:18:46,599 WARN  AuthenticationFilter - Authentication exception: Anonymous requests are disallowed
org.apache.hadoop.security.authentication.client.AuthenticationException: Anonymous requests are disallowed
        at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)
        at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)
        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)
        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)
        at java.lang.Thread.run(Thread.java:745)"
HADOOP-11145,TestFairCallQueue fails,"TestFairCallQueue#testPutBlocksWhenAllFull fails on trunk and branch-2.
{code}
Running org.apache.hadoop.ipc.TestFairCallQueue
Tests run: 22, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.174 sec <<< FAILURE! - in org.apache.hadoop.ipc.TestFairCallQueue
testPutBlocksWhenAllFull(org.apache.hadoop.ipc.TestFairCallQueue)  Time elapsed: 0.239 sec  <<< FAILURE!
junit.framework.AssertionFailedError: expected:<10> but was:<0>
	at junit.framework.Assert.fail(Assert.java:57)
	at junit.framework.Assert.failNotEquals(Assert.java:329)
	at junit.framework.Assert.assertEquals(Assert.java:78)
	at junit.framework.Assert.assertEquals(Assert.java:234)
	at junit.framework.Assert.assertEquals(Assert.java:241)
	at junit.framework.TestCase.assertEquals(TestCase.java:409)
	at org.apache.hadoop.ipc.TestFairCallQueue.assertCanPut(TestFairCallQueue.java:337)
	at org.apache.hadoop.ipc.TestFairCallQueue.testPutBlocksWhenAllFull(TestFairCallQueue.java:353)
{code}"
HADOOP-11143,NetUtils.wrapException loses inner stack trace on BindException,"{{NetUtils.wrapException}} is designed to aid debugging by including exception diagnostics in the wrapped & relayed exception.

When a BindException is caught, we build the new exception  but don't include the original as the inner cause.

this means it doesn't get logged, and while the host:port problem may be identifiable, the bit of the code playing up is now harder to track down.
"
HADOOP-11140,hadoop-aws only need test-scoped dependency on hadoop-common's tests jar,
HADOOP-11133,Should trim the content of keystore password file for JavaKeyStoreProvider,"In linux, when we edit file using vi/vim, or echo something to a file, {{LF}} char will be added at the end when saving file. The keystore is load failed in {{JavaKeyStoreProvider}}:
{quote}
java.io.IOException: Keystore was tampered with, or password was incorrect
{quote}
We should trim the content of keystore password file."
HADOOP-11130,NFS updateMaps OS check is reversed,"getent is fairly standard, dscl is not.  Yet the code logic prefers dscl for non-Linux platforms. This code should for OS X and use dscl and, if not, then use getent.  See comments."
HADOOP-11125,Remove redundant tests in TestOsSecureRandom,"From https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1897/console :
{code}
Running org.apache.hadoop.crypto.random.TestOsSecureRandom
Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 120.516 sec <<< FAILURE! - in org.apache.hadoop.crypto.random.TestOsSecureRandom
testOsSecureRandomSetConf(org.apache.hadoop.crypto.random.TestOsSecureRandom)  Time elapsed: 120.013 sec  <<< ERROR!
java.lang.Exception: test timed out after 120000 milliseconds
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:220)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:264)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:306)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:158)
	at java.io.InputStreamReader.read(InputStreamReader.java:167)
	at java.io.BufferedReader.fill(BufferedReader.java:136)
	at java.io.BufferedReader.read1(BufferedReader.java:187)
	at java.io.BufferedReader.read(BufferedReader.java:261)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.parseExecResult(Shell.java:715)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:524)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702)
	at org.apache.hadoop.crypto.random.TestOsSecureRandom.testOsSecureRandomSetConf(TestOsSecureRandom.java:149)
{code}"
HADOOP-11122,Fix findbugs in ZK DelegationTokenSecretManagers ,HADOOP-11017 adds ZK implementation for DelegationTokenSecretManager. This is a follow-up JIRA to address review comments there - findbugs and order of updates to the {{currentKey}}
HADOOP-11113,Namenode not able to reconnect to KMS after KMS restart,"It is observed that if KMS is restarted without the Namenode being restarted, NN will not be able to reconnect with KMS.
It seems that the KMS auth cookie goes stale and it does not get flushed, so the KMSClient in the NN cannot reconnect with the new KMS."
HADOOP-11112,TestKMSWithZK does not use KEY_PROVIDER_URI,HDFS-7004 missed updating {{TestKMSWithZK}} to use {{KEY_PROVIDER_URI}} to set the keyprovider URI.
HADOOP-11111,MiniKDC to use locale EN_US for case conversions,"The miniKDC cluster uses {{.equalsIgnoreCase()}}, and {{.toLower()}}, {{.toUpper}} everywhere. While nobody uses this in production, it should be fixed to ensure tests run consistently across all locales."
HADOOP-11110,JavaKeystoreProvider should not report a key as created if it was not flushed to the backing file,"Testing with the KMS backed by JKS reveals the following:

{noformat}
[root@dlo-4 ~]# hadoop key create testkey -provider kms://http@localhost:16000/kms
testkey has not been created. Mkdirs failed to create file:xxxxx
....<stack trace>....

[root@dlo-4 ~]# hadoop key list -provider kms://http@localhost:16000/kms
Listing keys for KeyProvider: KMSClientProvider[http://localhost:16000/kms/v1/]
testkey
{noformat}

The JKS still has the key in memory and serves it up, but will disappear if the KMS is restarted since it's not flushed to the file."
HADOOP-11109,Site build is broken ,
HADOOP-11106,Document considerations of HAR and Encryption,Minor changes to the HAR documentation need to be made discussing HAR and encryption.
HADOOP-11105,MetricsSystemImpl could leak memory in registered callbacks,"In {{MetricsSystemImpl.register(final String name, final String description, final T sink)}} method, we will also add a callback function to the {{callbacks}} array list. However, upon unregistering, the callback function is not removed from the list in existing code. As a result, the memory allocated for the callback function could accumulate in the {{MetricsSystemImpl}} list. In one of our cluster, we have seen a case where there are about 874 MB memory retained by {{MetricsSystemImpl}}."
HADOOP-11101,How about inputstream close statement from catch block to finally block in FileContext#copy() ?,"If IOException is happended, can be catched exception block.. 
But another excpetion is happended, can't be catched exception block.. also Stream object can't be closed..

{code}
        try {
          in = open(qSrc);
          EnumSet<CreateFlag> createFlag = overwrite ? EnumSet.of(
              CreateFlag.CREATE, CreateFlag.OVERWRITE) : 
                EnumSet.of(CreateFlag.CREATE);
          out = create(qDst, createFlag);
          IOUtils.copyBytes(in, out, conf, true);
        } catch (IOException e) {
          IOUtils.closeStream(out);
          IOUtils.closeStream(in);
          throw e;
        }
{code}"
HADOOP-11099,KMS return HTTP UNAUTHORIZED 401 on ACL failure,"The usual error, HTTP UNAUTHORIZED means is for authentication, not for authorization.

KMS should return HTTP FORBIDDEN in case of ACL failure."
HADOOP-11098,[JDK8] Max Non Heap Memory default changed between JDK7 and 8,"I noticed this because the NameNode UI shows ""Max Non Heap Memory"" which after some digging I found correlates to MaxDirectMemorySize.

JDK7
{noformat}
Heap Memory used 16.75 GB of 23 GB Heap Memory. Max Heap Memory is 23.7 GB.
Non Heap Memory used 57.32 MB of 67.38 MB Commited Non Heap Memory. Max Non Heap Memory is 130 MB. 
{noformat}

JDK8
{noformat}
Heap Memory used 3.02 GB of 7.65 GB Heap Memory. Max Heap Memory is 23.7 GB.
Non Heap Memory used 103.12 MB of 104.41 MB Commited Non Heap Memory. Max Non Heap Memory is -1 B. 
{noformat}

More information in first comment."
HADOOP-11097,"kms docs say proxyusers, not proxyuser for config params","The KMS docs have the proxy parameters named proxyusers, not proxyuser.
"
HADOOP-11096,KMS: KeyAuthorizationKeyProvider should verify the keyversion belongs to the keyname on decrypt,"when decrypting a EEK the {{KeyAuthorizationKeyProvider}} must  verify the keyversionname belongs to the keyname.
"
HADOOP-11091,Eliminate old configuration parameter names from s3a,"This JIRA is to track eliminating the configuration parameter names with ""old"" in the name from s3a."
HADOOP-11088,"Unittest TestKeyShell, TestCredShell and TestKMS assume UNIX path separator for JECKS key store path","TestKeyShell and TestCredShell assume UNIX path separator for JECKS key store path. This will fail the tests on Windows which uses a different path separator. The fix should be something like:

{code}
-    jceksProvider = ""jceks://file"" + tmpDir + ""/keystore.jceks"";
+    final Path jksPath = new Path(tmpDir.toString(), ""keystore.jceks"");
+    jceksProvider = ""jceks://file"" + jksPath.toUri();
{code}


"
HADOOP-11085,Excessive logging by org.apache.hadoop.util.Progress when value is NaN,"MAPREDUCE-5671 fixed the ""illegal"" progress values that do not fall into (0,1) interval when the progress value is given. Whenever illegal value was encountered, LOG.warn would log that incident.

As  a result, each of the task's syslog will be full of  WARN [main] org.apache.hadoop.util.Progress: Illegal progress value found, progress is Float.NaN. Progress will be changed to 0

Each input record will contribute to one line of such log, leading to most
of the tasks' syslog > 1GB.

We will need to change the log level to debug to avoid such excessive logging."
HADOOP-11083,"After refactoring of HTTP proxyuser to common, doAs param is case sensitive",In HADOOP-10835 I've overlooked that the {{doAs}} parameter was been handled as case insensitive.
HADOOP-11082,Resolve findbugs warnings in hadoop-aws module,Currently hadoop-aws module has the findbugs exclude file from hadoop-common.  It would be nice to address the findbugs bugs eventually.
HADOOP-11077,NPE if hosts not specified in ProxyUsers,"When using the TokenDelegationAuthenticationFilter, I noticed if I don't specify the hosts for a user/groups proxy user and then try to authenticate, I get an NPE rather than an AuthorizationException."
HADOOP-11074,Move s3-related FS connector code to hadoop-aws,"Now that hadoop-aws has been created, we should actually move the relevant code into that module, similar to what was done with hadoop-openstack, etc."
HADOOP-11073,Credential Provider related Unit Tests Failure on Windows,"Reported by: Xiaomara and investigated by [~cnauroth].

The credential provider related unit tests failed on Windows. The tests try to set up a URI by taking the build test directory and concatenating it with other strings containing the rest of the URI format, i.e.:

{code}
  public void testFactory() throws Exception {
    Configuration conf = new Configuration();
    conf.set(CredentialProviderFactory.CREDENTIAL_PROVIDER_PATH,
        UserProvider.SCHEME_NAME + "":///,"" +
            JavaKeyStoreProvider.SCHEME_NAME + ""://file"" + tmpDir + ""/test.jks"");
{code}

This logic is incorrect on Windows, because the file path separator will be '\', which violates URI syntax. Forward slash is not permitted. 

The proper fix is to always do path/URI construction through the org.apache.hadoop.fs.Path class, specifically using the constructors that take explicit parent and child arguments.

The affected unit tests are:

{code}
* TestKeyProviderFactory
* TestLdapGroupsMapping
* TestCredentialProviderFactory
* KeyStoreTestUtil
* TestSSLFactory
{code}
"
HADOOP-11071,KMSClientProvider should drain the local generated EEK cache on key rollover,This is for formal correctness and to enable HDFS EZ to verify a rollover when testing with KMS
HADOOP-11070,Create MiniKMS for testing,This will facilitate testing HDFS and MR with HDFS encryption fully reproducing a real deployment setup.
HADOOP-11069,KMSClientProvider should use getAuthenticationMethod() to determine if in proxyuser mode or not,"Currently is checking the login UGI being different from the current UGI, it should check if the current UGI auth method is PROXY."
HADOOP-11068,Match hadoop.auth cookie format to jetty output,"See: https://issues.apache.org/jira/browse/HADOOP-10911?focusedCommentId=14111626&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14111626

I posted the cookie format that jetty generates, but I attached a version of the patch with an older format.  Note, because the tests are pretty comprehensive, this cookie format works (it fixes the issue we were having with Solr), but it would probably be better to match the format that jetty generates."
HADOOP-11067,warning message 'ssl.client.truststore.location has not been set' gets printed for hftp command,"hftp command prints 'WARN ssl.FileBasedKeyStoresFactory: The property 'ssl.client.truststore.location' has not been set, no TrustStore will be loaded' in Windows unsecure environment. 

This issue only exists in Windows environment.

{noformat}
hdfs dfs -cat hftp://XXXX:50070/user/yesha/1409773968/L1/a.txt
WARN ssl.FileBasedKeyStoresFactory: The property 'ssl.client.truststore.location' has not been set, no TrustStore will be loaded
HEllo World..!!
{noformat}
"
HADOOP-11065,Rat check should exclude **/build/**,"https://builds.apache.org/job/HADOOP2_Release_Artifacts_Builder/ copies the nightly scripts under build/. For the rat-check to pass here, we should exclude the build directory. "
HADOOP-11064,UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes,"The private native method names and signatures in {{NativeCrc32}} were changed in HDFS-6561 ... as a result hadoop-common-2.4 JARs get unsatisifed link errors when they try to perform checksums. 

This essentially stops Hadoop 2.4 applications running on Hadoop 2.6 unless rebuilt and repackaged with the hadoop- 2.6 JARs"
HADOOP-11063,"KMS cannot deploy on Windows, because class names are too long.","Windows has a maximum path length of 260 characters.  KMS includes several long class file names.  During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure.  The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows."
HADOOP-11062,CryptoCodec testcases requiring OpenSSL should be run only if -Pnative is used,"there are a few testcases, cryptocodec related that require Hadoop native code and OpenSSL.

These tests should be skipped if -Pnative is not used when running the tests."
HADOOP-11060,Create a CryptoCodec test that verifies interoperability between the JCE and OpenSSL implementations,"We should have a test that verifies writing with one codec implementation and reading with other works, including some random seeks. This should be tested in both directions.
"
HADOOP-11057,checknative command to probe for winutils.exe on windows,"hadoop's {{checknative}} command looks for native binaries and returns an error code if one is missing.

But it doesn't check for {{winutils.exe}} on windows, which turns out to be essential for some operations. 

Adding this check to the -a (or default) operation would allow the check to be used as a health check on windows installations"
HADOOP-11056,OsSecureRandom.setConf() might leak file descriptors.,"OsSecureRandom.setConf() might leak resource, the stream is not closed when:

1. if setConf() is called a second time
2. if {{fillReservoir(0);}} throw exception.

 "
HADOOP-11054,Add a KeyProvider instantiation based on a URI,"Currently there is no way to instantiate a {{KeyProvider}} given a URI.

In the case of HDFS encryption, it would be desirable to be explicitly specify a KeyProvider URI to avoid obscure misconfigurations."
HADOOP-11049,javax package system class default is too broad,"The system class default defined in ApplicationClassLoader has ""javax."". This is too broad. The intent of the system classes is to exempt classes that are provided by the JDK along with hadoop and minimally necessary dependencies that are guaranteed to be on the system classpath. ""javax."" is too broad for that.

For example, JSR-330 which is part of JavaEE (not JavaSE) has ""javax.inject"". Packages like them should not be declared as system classes, as they will result in ClassNotFoundException if they are needed and present on the user classpath."
HADOOP-11048,user/custom LogManager fails to load if the client classloader is enabled,"If the client classloader is enabled (HADOOP-10893) and you happen to use a user-provided log manager via -Djava.util.logging.manager, it fails to load the custom log manager:

{noformat}
Could not load Logmanager ""org.foo.LogManager""
java.lang.ClassNotFoundException: org.foo.LogManager
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at java.util.logging.LogManager$1.run(LogManager.java:191)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.util.logging.LogManager.<clinit>(LogManager.java:181)
    at java.util.logging.Logger.demandLogger(Logger.java:339)
    at java.util.logging.Logger.getLogger(Logger.java:393)
    at com.google.common.collect.MapMakerInternalMap.<clinit>(MapMakerInternalMap.java:136)
    at com.google.common.collect.MapMaker.makeCustomMap(MapMaker.java:602)
    at com.google.common.collect.Interners$CustomInterner.<init>(Interners.java:59)
    at com.google.common.collect.Interners.newWeakInterner(Interners.java:103)
    at org.apache.hadoop.util.StringInterner.<clinit>(StringInterner.java:49)
    at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2293)
    at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2185)
    at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2102)
    at org.apache.hadoop.conf.Configuration.get(Configuration.java:851)
    at org.apache.hadoop.util.RunJar.run(RunJar.java:179)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}

This is caused because Configuration.loadResources() is invoked before the client classloader is created and made available."
HADOOP-11045,Introducing a tool to detect flaky tests of hadoop jenkins test job,"File this jira to introduce a tool to detect flaky tests of hadoop jenkins test jobs. Certainly it can be adapted to projects other than hadoop.

I developed the tool on top of some initial work [~tlipcon] did. We find it quite useful. With Todd's agreement, I'd like to push it to upstream so all of us can share (thanks Todd for the initial work and support). I hope you find the tool useful too.

The idea is, when one has the need to see if the test failure s/he is seeing in a pre-build jenkins run is flaky or not, s/he could run this tool to get a good idea. Also, if one wants to look at the failure trend of a testcase in a given jenkins job, the tool can be used too. I hope people find it useful.

This tool is for hadoop contributors rather than hadoop users. Thanks [~tedyu] for the advice to put to dev-support dir.

Description of the tool:
{code}
#
# Given a jenkins test job, this script examines all runs of the job done
# within specified period of time (number of days prior to the execution
# time of this script), and reports all failed tests.
#
# The output of this script includes a section for each run that has failed
# tests, with each failed test name listed.
#
# More importantly, at the end, it outputs a summary section to list all failed
# tests within all examined runs, and indicate how many runs a same test
# failed, and sorted all failed tests by how many runs each test failed in.
#
# This way, when we see failed tests in PreCommit build, we can quickly tell 
# whether a failed test is a new failure or it failed before, and it may just 
# be a flaky test.
#
# Of course, to be 100% sure about the reason of a failed test, closer look 
# at the failed test for the specific run is necessary.
#
{code}

How to use the tool:
{code}
Usage: determine-flaky-tests-hadoop.py [options]

Options:
  -h, --help            show this help message and exit
  -J JENKINS_URL, --jenkins-url=JENKINS_URL
                        Jenkins URL
  -j JOB_NAME, --job-name=JOB_NAME
                        Job name to look at
  -n NUM_PREV_DAYS, --num-days=NUM_PREV_DAYS
                        Number of days to examine
{code}

Example command line:
{code}
./determine-flaky-tests-hadoop.py -J https://builds.apache.org -j PreCommit-HDFS-Build -n 2 
{code}
"
HADOOP-11040,Return value of read(ByteBuffer buf) in CryptoInputStream is incorrect in some cases,"In {{CryptoInputStream}}, for {{int read(ByteBuffer buf))}}, if there is unread value in outBuffer, then the current return value is incorrect. This case happens when caller uses bytes array read firstly and then do the ByteBuffer read."
HADOOP-11039,ByteBufferReadable API doc is inconsistent with the implementations.,"In {{ByteBufferReadable}}, API doc of {{int read(ByteBuffer buf)}} says:
{quote}
After a successful call, buf.position() and buf.limit() should be unchanged, and therefore any data can be immediately read from buf. buf.mark() may be cleared or updated.
{quote}
{quote}
@param buf
                the ByteBuffer to receive the results of the read operation. Up to
                buf.limit() - buf.position() bytes may be read.
{quote}

But actually the implementations (e.g. {{DFSInputStream}}, {{RemoteBlockReader2}}) would be: 
*Upon return, buf.position() will be advanced by the number of bytes read.*
code implementation of {{RemoteBlockReader2}} is as following:
{code}
@Override
  public int read(ByteBuffer buf) throws IOException {
    if (curDataSlice == null || curDataSlice.remaining() == 0 && bytesNeededToFinish > 0) {
      readNextPacket();
    }
    if (curDataSlice.remaining() == 0) {
      // we're at EOF now
      return -1;
    }

    int nRead = Math.min(curDataSlice.remaining(), buf.remaining());
    ByteBuffer writeSlice = curDataSlice.duplicate();
    writeSlice.limit(writeSlice.position() + nRead);
    buf.put(writeSlice);
    curDataSlice.position(writeSlice.position());

    return nRead;
  }
{code}

This description is very important and will guide user how to use this API, and all the implementations should keep the same behavior. We should fix the javadoc."
HADOOP-11036,Add build directory to .gitignore,"After building hadoop, build directory is generated under hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/build/. It's useful to ignore the directory to avoid wrong operations.
{code}
hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/build/
{code}
"
HADOOP-11032,Replace use of Guava's Stopwatch with Hadoop's StopWatch,"This patch reduces Hadoop's dependency on an old version of guava. Stopwatch.elapsedMillis() isn't part of guava past v16 and the tools I'm working on use v17. 

To remedy this and also reduce Hadoop's reliance on old versions of guava, we can implement original StopWatch based on Guava's one and Apache's one."
HADOOP-11030,Define a variable jackson.version instead of using constant at multiple places,
HADOOP-11026,add FileSystem contract specification for FSDataInputStream and FSDataOutputStream#isEncrypted,"Following on to HDFS-6843, the contract specification for FSDataInputStream and FSDataOutputStream needs to be updated to reflect the addition of isEncrypted."
HADOOP-11021,Configurable replication factor in the hadoop archive command,"Due to be below hard-coded replication degree in {{HadoopArchives}}, the {{archive}} command will fail if HDFS maximum replication has already been configured to a number lower than 10. 

{code:java}
    //increase the replication of src files
    jobfs.setReplication(srcFiles, (short) 10);
{code}

This Jira will make the {{archive}} command configurable with desired replication degree."
HADOOP-11017,KMS delegation token secret manager should be able to use zookeeper as store,This will allow supporting multiple KMS instances behind a load balancer.
HADOOP-11016,KMS should support signing cookies with zookeeper secret manager,"This will allow supporting multiple KMS instances behind a load-balancer.
"
HADOOP-11015,Http server/client utils to propagate and recreate Exceptions from server to client,"While doing HADOOP-10771, while discussing it with [~daryn], a suggested improvement was to propagate the server side exceptions to the client in the same way WebHDFS does it.

This JIRA is to provide a utility class to do the same and refactor HttpFS and KMS to use it."
HADOOP-11014,Potential resource leak in JavaKeyStoreProvider due to unclosed stream,"From hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java :
{code}
  private void writeToNew(Path newPath) throws IOException {
    FSDataOutputStream out =
        FileSystem.create(fs, newPath, permissions);
    try {
      keyStore.store(out, password);
    } catch (KeyStoreException e) {
      throw new IOException(""Can't store keystore "" + this, e);
    } catch (NoSuchAlgorithmException e) {
      throw new IOException(
          ""No such algorithm storing keystore "" + this, e);
    } catch (CertificateException e) {
      throw new IOException(
          ""Certificate exception storing keystore "" + this, e);
    }
    out.close();
{code}
IOException is not among the catch blocks.
According to http://docs.oracle.com/javase/7/docs/api/java/security/KeyStore.html#store(java.io.OutputStream,%20char[]), IOException may be thrown from the store() call. In that case, out would be left unclosed.

In loadFromPath():
{code}
    keyStore.load(fs.open(p), password);
{code}
The InputStream should be closed upon return from load()"
HADOOP-11012,hadoop fs -text of zero-length file causes EOFException,"List:
$ $HADOOP_PREFIX/bin/hadoop fs -ls /user/ericp/foo
-rw-------   3 ericp hdfs          0 2014-08-22 16:37 /user/ericp/foo

Cat:
$ $HADOOP_PREFIX/bin/hadoop fs -cat /user/ericp/foo

Text:
$ $HADOOP_PREFIX/bin/hadoop fs -text /user/ericp/foo
text: java.io.EOFException
	at java.io.DataInputStream.readShort(DataInputStream.java:315)
	at org.apache.hadoop.fs.shell.Display$Text.getInputStream(Display.java:130)
	at org.apache.hadoop.fs.shell.Display$Cat.processPath(Display.java:98)
	at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:306)
	at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:278)
	at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:260)
	at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:244)
	at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:287)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)
"
HADOOP-11009,Add Timestamp Preservation to DistCp,"Currently access and modification times are not preserved on files copied using DistCp. This patch adds an option to DistCp for timestamp preservation. 

The patch ready, but I understand there is a Contributor form I need to sign before I can upload it. Can someone point me in the right direction for this form? Thanks!"
HADOOP-11008,Remove duplicated description about proxy-user in site documents,The one should be pointer to the other.
HADOOP-11007,Reinstate building of ant tasks support,The ant tasks support from HADOOP-1508 is still present under hadoop-hdfs/src/ant/ but is no longer being built.  It would be nice if this was reinstated in the build and distributed as part of the release.
HADOOP-11005,Fix HTTP content type for ReconfigurationServlet,"The reconfiguration framework introduced from HDFS-7001 supports reload configuration from HTTP servlet, using {{ReconfigurableServlet}}. {{ReconfigurableServlet}} processes a HTTP GET request to list the differences between old and new configurations in HTML, with a form that allows the user to submit to confirm the configuration changes. However since the response lacks HTTP ""content-type"", the browser renders the page as text file, which makes it impossible to submit the form. "
HADOOP-11001,Fix test-patch to work with the git repo,We want the precommit builds to run against the git repo after the transition. 
HADOOP-11000,HAServiceProtocol's health state is incorrectly transitioned to SERVICE_NOT_RESPONDING,"When HAServiceProtocol.monitorHealth throws a HealthCheckFailedException, the actual exception from protocol buffer RPC is a RemoteException that wraps the real exception. Thus the state is incorrectly transitioned to SERVICE_NOT_RESPONDING

{noformat}
HealthMonitor.java
doHealthChecks

      try {
        status = proxy.getServiceStatus();
        proxy.monitorHealth();
        healthy = true;
      } catch (HealthCheckFailedException e) {
        .....
        enterState(State.SERVICE_UNHEALTHY);
      } catch (Throwable t) {
        .....
        enterState(State.SERVICE_NOT_RESPONDING);
        .....
      }

{noformat}"
HADOOP-10998,Fix bash tab completion code to work,"The included version of bash completion has not been updated in almost two years and does not work correctly with more recent versions of Hadoop.

The command substitutions in the script were using backticks, which were interpolating variables before they were executed and breaking the script.  In particular the awk commands were breaking due to this interpolation.

The attached patch changes the backticks to {{$(command)}} style for command substitution and updates the script to work properly with newer versions of hadoop.

I tested the script using {{zsh 5.0.5}}, {{zsh 4.3.10]}, {{bash-4.1.2}} and {{bash-3.2}}.  Hadoop versions tested were cloudera version {{2.0.0-cdh4.2.1}} and briefly Apache {{2.5.0}}."
HADOOP-10994,KeyProviderCryptoExtension should use CryptoCodec for generation/decryption of keys,"Currently is using JDK Cipher, with fs-encryption branch merged into trunk we can swap to CryptoCodec."
HADOOP-10992,Merge KMS to branch-2,"A pre-requisite for getting HDFS encryption in branch-2 is KMS, we need to merge all related JIRAs:

{code}
052932e7299ff64d36287b368f94ccf8698d5c9d HADOOP-10141. Create KeyProvider API to separate encryption key storage from the applications. (omalley)
b72026617b038f588581d43c323718fe8120b400 HADOOP-10201. Add listing to KeyProvider API. (Larry McCay via omalley)
4a178b6736d54e1b1940babd7cbda34921957d01 HADOOP-10177. Create CLI tools for managing keys. (Larry McCay via omalley)
0cf6ccf606fceb6c06f35d72b2c2b679d71ad96c HADOOP-10237. JavaKeyStoreProvider needs to set keystore permissions correctly. (Larry McCay via omalley)
56d349b81d24ef1421ffcdfb822a8fe122f05c80 HADOOP-10432. Refactor SSLFactory to expose static method to determine HostnameVerifier. (tucu)
0d66663cb277937eb7ec6a281dc7f236efe387fd HADOOP-10429. KeyStores should have methods to generate the materials themselves, KeyShell should use them. (tucu)
d9c1c42fdfddb810ebe2ec151f751d05e987f25e HADOOP-10427. KeyProvider implementations should be thread safe. (tucu)
98be41ff908acd2fa55c0b302c8a3def55987e41 HADOOP-10428. JavaKeyStoreProvider should accept keystore password via configuration falling back to ENV VAR. (tucu)
b2b05181682c2a55f5ed1cfa2c44f3390eebd5c4 HADOOP-10244. TestKeyShell improperly tests the results of delete (Larry McCay via omalley)
83f057e8e1d16949b94fe2e99f4232ced8156e6a HADOOP-10430. KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys. (tucu)
f6f52ca1c2df57d13fa596e074accc0f3549ff58 HADOOP-10431. Change visibility of KeyStore.Options getter methods to public. (tucu)
05e59fd8058f21a52d4a268af3a189c89ebad2fe HADOOP-10534. KeyProvider getKeysMetadata should take a list of names rather than returning all keys. (omalley)
16be41a63e4b3bd79b1cee4edce6df374666ca58 HADOOP-10433. Key Management Server based on KeyProvider API. (tucu)
4bcaa45a2ea36fb440069c7a458cdc225cb862ca HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)
1727e235c3d3317b2ac6d7c25ea01505853653ca HADOOP-10586. KeyShell doesn't allow setting Options via CLI. (clamb via tucu)
6b410f3b2e185fca963c7db664395e97d76cd6ee HADOOP-10645. TestKMS fails because race condition writing acl files. (tucu)
7868054902590af6dbda941f2cc8324267c8bef8 HADOOP-10611. KMS, keyVersion name should not be assumed to be keyName@versionNumber. (tucu)
725f087f3f2fc31190810344d0e508e34b4a126e HADOOP-10607. Create API to separate credential/password storage from applications. (Larry McCay via omalley)
097254f094b004404ba4754f97f906f46a12b0e4 HADOOP-10696. Add optional attributes to KeyProvider Options and Metadata. (tucu)
a283b91add9e9230b9597fd33355822517a1852e HADOOP-10695. KMSClientProvider should respect a configurable timeout. (yoderme via tucu)
6cef126f29673704c345c52995890ff48395ec1a HADOOP-10757. KeyProvider KeyVersion should provide the key name. (asuresh via tucu)
9b7a1cb122c6a6041e718986085ec7f6bab422c4 HADOOP-10719. Add generateEncryptedKey and decryptEncryptedKey methods to KeyProvider. (asuresh via tucu)
9c03a4b321db7950d5652ba03022f9ee3ebd2d6f HADOOP-10769. Create KeyProvider extension to handle delegation tokens. Contributed by Arun Suresh.
db91ab3d02fddfd325fd308e46f65075c2c6cd93 HADOOP-10812. Delegate KeyProviderExtension#toString to underlying KeyProvider. (wang)
7c7911bbd63d30932df71af536f45c20adba88ff HADOOP-10736. Add key attributes to the key shell. Contributed by Mike Yoder.
cfb5943d356fef911f424ed8250a9c02b706ecc6 HADOOP-10824. Refactor KMSACLs to avoid locking. (Benoy Antony via umamahesh)
6b9b985233c293d22f89a4deadf871230f09d7ed HADOOP-10816. KeyShell returns -1 on error to the shell, should be 1. (Mike Yoder via wang)
ceea01cff5762115c58817ab696cd11641bc9a98 HADOOP-10841. EncryptedKeyVersion should have a key name property. (asuresh via tucu)
468a4fc00921ea7bc61bb60666e9352b0ad3928b HADOOP-10842. CryptoExtension generateEncryptedKey method should receive the key name. (asuresh via tucu)
c6d60c6db8b22d6dc45e63073bc5bb52dc041a8c HADOOP-10750. KMSKeyProviderCache should be in hadoop-common. (asuresh via tucu)
c3eca9f2504ed619a3edcf3d3eafc286133911d0 HADOOP-10720. KMS: Implement generateEncryptedKey and decryptEncryptedKey in the REST API. (asuresh via tucu)
6ae46e601290a094019fdd8e241a90a6f269203c HADOOP-10826. Iteration on KeyProviderFactory.serviceLoader is thread-unsafe. (benoyantony viat tucu)
22bbb1e1b1ad076cb2cac22b7863904aea903586 HADOOP-10881. Clarify usage of encryption and encrypted encryption key in KeyProviderCryptoExtension. (wang)
8eafb8915177261d6560c365c5cac6f7dad12e55 HADOOP-10891. Add EncryptedKeyVersion factory method to KeyProviderCryptoExtension. (wang)
cae52dee46a57da40a811129781a3664beb0fe42 HADOOP-10756. KMS audit log should consolidate successful similar requests. (asuresh via tucu)
9704e448046a95949d6da6c894f729130821f88b HADOOP-10793. KeyShell args should use single-dash style. (wang)
13e092f3ecfb11e9bc33cae7f81768f393f9ac64 HADOOP-10920. site plugin couldn't parse hadoop-kms index.apt.vm. (Akira Ajisaka via wang)
362bc16eaa7d83a3ef9dde5e6c69f21f753b8a80 HADOOP-10937. Need to set version name correctly before decrypting EEK. Contributed by Arun Suresh.
66af8b0ed51f082889be3d39f63e28f5920e5cb6 HADOOP-10936. Change default KeyProvider bitlength to 128. (wang)
e1eb546528ee4d5c1c44f8d785bf0c0378090645 HADOOP-10918. JMXJsonServlet fails when used within Tomcat. (tucu)
b4706add323b7fb195844d4b4ec10d445f7122fd HADOOP-10939. Fix TestKeyProviderFactory testcases to use default 128 bit length keys. Contributed by Arun Suresh.
75abed80c6314623e4eb842d003c6613e493a16b HADOOP-10862. Miscellaneous trivial corrections to KMS classes. (asuresh via tucu)
0d2970300a4074dbc448d6d79946444afa6e66d9 HADOOP-10224. JavaKeyStoreProvider has to protect against corrupting underlying store. (asuresh via tucu)
d8663c28e0f26af9b34fdead2fe4cd7ed628e2e2 HADOOP-10770. KMS add delegation token support. (tucu)
859fe45e4e22d96f22dd35649cd25ab7c94ba444 HADOOP-10967. Improve DefaultCryptoExtension#generateEncryptedKey performance. (hitliuyi via tucu)
9e87d275322482133054454bea8c34d49703105f HADOOP-10698. KMS, add proxyuser support. (tucu)
45b61bfa07007e3807ee8ee5ed36c058f8042983 HADOOP-10488. TestKeyProviderFactory fails randomly. (tucu)
{code}"
HADOOP-10990,Add missed NFSv3 request and response classes,Two RPC calls were missed in original NFS implementation: LINK and MKNOD. This JIRA is to track the effort of adding the missed RPC calls.
HADOOP-10989,Work around buggy getgrouplist() implementations on Linux that return 0 on failure,"HADOOP-10781 corrected the handling of the return value from {{getgrouplist}} to work on FreeBSD.  However, it also regressed fixes that had been put in place to work around issues with {{getgrouplist}} on Linux.  This issue will restore that behavior, but still retain compatibility with FreeBSD."
HADOOP-10987,Provide an iterator-based listing API for FileSystem,"Iterator based listing methods already exist in {{FileContext}} for both simple listing and listing with locations. However, {{FileSystem}} lacks the former.  From what I understand, it wasn't added to {{FileSystem}} because it was believed to be phased out soon. Since {{FileSystem}} is very well alive today and new features are getting added frequently, I propose adding an iterator based {{listStatus}} method. As for the name of the new method, we can use the same name used in {{FileContext}} : {{listStatusIterator()}}.

It will be particularly useful when listing giant directories. Without this, the client has to build up a huge data structure and hold it in memory. We've seen client JVMs running out of memory because of this.

Once this change is made, we can modify FsShell, etc. in followup jiras."
HADOOP-10982,KMS: Support for multiple Kerberos principals,"The Key Management Server should support multiple Kerberos principals.

This is required for KMS multi instance setups behind a  VIP. In such scenarios there must a principal with the VIP hostname and one with the actual hostname of the KMS instance. Regular users will use the VIP entry point, monitoring agents must hit a specific instance.
"
HADOOP-10976,moving the source code of hadoop-tools docs to the directory under hadoop-tools,Some of the doc files of hadoop-tools are placed in the mapreduce project. It should be moved for the ease of maintenance.
HADOOP-10975,org.apache.hadoop.util.DataChecksum should support native checksum calculation,
HADOOP-10973,Native Libraries Guide contains format error,"The move from xdocs to APT introduced a formatting bug so that the sub-list under Usage point 4 was merged into the text itself and no longer appeared as a sub-list. Compare xdocs version http://hadoop.apache.org/docs/r1.2.1/native_libraries.html#Native+Hadoop+Library to APT version http://hadoop.apache.org/docs/r2.5.0/hadoop-project-dist/hadoop-common/NativeLibraries.html#Native_Hadoop_Library.

The patch is to trunk, but is also valid for released versions 0.23.11, 2.2.0, 2.3.0, 2.4.0, 2.4.1, and 2.5.0, and could be back-ported to them if deemed necessary."
HADOOP-10972,Native Libraries Guide contains mis-spelt build line,"The Native Libraries Guide mis-spells the define 'skipTests' with a lowercase 't' in the build line. The correct build line is:

{code:none}
$ mvn package -Pdist,native -DskipTests -Dtar
{code}

Patch is to trunk, but is also valid for released versions 2.2.0, 2.3.0, 2.4.0, 2.4.1, and 2.5.0, and could be back-ported to them if deemed necessary."
HADOOP-10970,Cleanup KMS configuration keys,"It'd be nice to add descriptions to the config keys in kms-site.xml.

Also, it'd be good to rename key.provider.path to key.provider.uri for clarity, or just drop "".path""."
HADOOP-10968,hadoop native build fails to detect java_libarch on ppc64le,"[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (make) @ hadoop-common ---
[INFO] Executing tasks

main:
     [exec] -- The C compiler identification is GNU 4.8.3
     [exec] -- The CXX compiler identification is GNU 4.8.3
     [exec] -- Check for working C compiler: /usr/bin/cc
     [exec] -- Check for working C compiler: /usr/bin/cc -- works
     [exec] -- Detecting C compiler ABI info
     [exec] -- Detecting C compiler ABI info - done
     [exec] -- Check for working CXX compiler: /usr/bin/c++
     [exec] -- Check for working CXX compiler: /usr/bin/c++ -- works
     [exec] JAVA_HOME=, JAVA_JVM_LIBRARY=JAVA_JVM_LIBRARY-NOTFOUND
     [exec] JAVA_INCLUDE_PATH=/usr/lib64/jvm/java-1.7.0-openjdk-1.7.0/include, JAVA_INCLUDE_PATH2=/usr/lib64/jvm/java-1.7.0-openjdk-1.7.0/include/linux
     [exec] CMake Error at JNIFlags.cmake:114 (MESSAGE):
     [exec]   Failed to find a viable JVM installation under JAVA_HOME.
     [exec] Call Stack (most recent call first):
     [exec]   CMakeLists.txt:24 (include)
     [exec] 
     [exec] 
     [exec] -- Detecting CXX compiler ABI info
     [exec] -- Detecting CXX compiler ABI info - done
     [exec] -- Configuring incomplete, errors occurred!
     [exec] See also ""/root/bigtop/build/hadoop/rpm/BUILD/hadoop-2.3.0-src/hadoop-common-project/hadoop-common/target/native/CMakeFiles/CMakeOutput.log"".
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................. SUCCESS [ 10.680 s]
[INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.716 s]
[INFO] Apache Hadoop Annotations .......................... SUCCESS [  3.270 s]
[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.274 s]
[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  1.819 s]
[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  3.284 s]
[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  2.863 s]
[INFO] Apache Hadoop Auth ................................. SUCCESS [  4.032 s]
[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  2.475 s]
[INFO] Apache Hadoop Common ............................... FAILURE [ 10.458 s]
[INFO] Apache Hadoop NFS .................................. SKIPPED
[INFO] Apache Hadoop Common Project ....................... SKIPPED
[INFO] Apache Hadoop HDFS ................................. SKIPPED
[INFO] Apache Hadoop HttpFS ............................... SKIPPED
[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SKIPPED
[INFO] Apache Hadoop HDFS-NFS ............................. SKIPPED
[INFO] Apache Hadoop HDFS Project ......................... SKIPPED
[INFO] hadoop-yarn ........................................ SKIPPED
[INFO] hadoop-yarn-api .................................... SKIPPED
[INFO] hadoop-yarn-common ................................. SKIPPED
[INFO] hadoop-yarn-server ................................. SKIPPED
[INFO] hadoop-yarn-server-common .......................... SKIPPED
[INFO] hadoop-yarn-server-nodemanager ..................... SKIPPED
[INFO] hadoop-yarn-server-web-proxy ....................... SKIPPED
[INFO] hadoop-yarn-server-resourcemanager ................. SKIPPED
[INFO] hadoop-yarn-server-tests ........................... SKIPPED
[INFO] hadoop-yarn-client ................................. SKIPPED
[INFO] hadoop-yarn-applications ........................... SKIPPED
[INFO] hadoop-yarn-applications-distributedshell .......... SKIPPED
[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SKIPPED
[INFO] hadoop-yarn-site ................................... SKIPPED
[INFO] hadoop-yarn-project ................................ SKIPPED
[INFO] hadoop-mapreduce-client ............................ SKIPPED
[INFO] hadoop-mapreduce-client-core ....................... SKIPPED
[INFO] hadoop-mapreduce-client-common ..................... SKIPPED
[INFO] hadoop-mapreduce-client-shuffle .................... SKIPPED
[INFO] hadoop-mapreduce-client-app ........................ SKIPPED
[INFO] hadoop-mapreduce-client-hs ......................... SKIPPED
[INFO] hadoop-mapreduce-client-jobclient .................. SKIPPED
[INFO] hadoop-mapreduce-client-hs-plugins ................. SKIPPED
[INFO] Apache Hadoop MapReduce Examples ................... SKIPPED
[INFO] hadoop-mapreduce ................................... SKIPPED
[INFO] Apache Hadoop MapReduce Streaming .................. SKIPPED
[INFO] Apache Hadoop Distributed Copy ..................... SKIPPED
[INFO] Apache Hadoop Archives ............................. SKIPPED
[INFO] Apache Hadoop Rumen ................................ SKIPPED
[INFO] Apache Hadoop Gridmix .............................. SKIPPED
[INFO] Apache Hadoop Data Join ............................ SKIPPED
[INFO] Apache Hadoop Extras ............................... SKIPPED
[INFO] Apache Hadoop Pipes ................................ SKIPPED
[INFO] Apache Hadoop OpenStack support .................... SKIPPED
[INFO] Apache Hadoop Client ............................... SKIPPED
[INFO] Apache Hadoop Mini-Cluster ......................... SKIPPED
[INFO] Apache Hadoop Scheduler Load Simulator ............. SKIPPED
[INFO] Apache Hadoop Tools Dist ........................... SKIPPED
[INFO] Apache Hadoop Tools ................................ SKIPPED
[INFO] Apache Hadoop Distribution ......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 42.273 s
[INFO] Finished at: 2014-08-13T00:43:06+02:00
[INFO] Final Memory: 61M/465M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-common: An Ant BuildException has occured: exec returned: 1
[ERROR] around Ant part ...<exec dir=""/root/bigtop/build/hadoop/rpm/BUILD/hadoop-2.3.0-src/hadoop-common-project/hadoop-common/target/native"" executable=""cmake"" failonerror=""true"">... @ 4:156 in /root/bigtop/build/hadoop/rpm/BUILD/hadoop-2.3.0-src/hadoop-common-project/hadoop-common/target/antrun/build-main.xml
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-common
error: Bad exit status from /var/tmp/rpm-tmp.suXMUs (%build)"
HADOOP-10967,Improve DefaultCryptoExtension#generateEncryptedKey performance ,"This JIRA is to improve generateEncryptedKey performance:

*1.* SecureRandom#generateSeed is very slow, we should use SecureRandom#nextBytes to generate the {{IV}} which is much faster. 

*2.* Define SecureRandom as threadlocal object which can improve the performance a bit.

*3.* Use {{new SecureRandom()}} instead of SHA1PRNG, the former has better entropy."
HADOOP-10966,Hadoop Common native compilation broken in windows,"After HADOOP-10962 hadoop common native compilation broken in windows

{noformat}
src\org\apache\hadoop\io\nativeio\NativeIO.c(181): error C2065: 'POSIX_FADV_NORMAL' : undeclared identifier 
src\org\apache\hadoop\io\nativeio\NativeIO.c(184): error C2065: 'POSIX_FADV_RANDOM' : undeclared identifier 
src\org\apache\hadoop\io\nativeio\NativeIO.c(187): error C2065: 'POSIX_FADV_SEQUENTIAL' : undeclared identifier 
src\org\apache\hadoop\io\nativeio\NativeIO.c(190): error C2065: 'POSIX_FADV_WILLNEED' : undeclared identifier 
src\org\apache\hadoop\io\nativeio\NativeIO.c(193): error C2065: 'POSIX_FADV_DONTNEED' : undeclared identifier 
src\org\apache\hadoop\io\nativeio\NativeIO.c(196): error C2065: 'POSIX_FADV_NOREUSE' : undeclared identifier 
{noformat}"
HADOOP-10964,Small fix for NetworkTopologyWithNodeGroup#sortByDistance,"{{nodes.length}} should be {{activeLen}}.
{code}
  @Override
  public void sortByDistance(Node reader, Node[] nodes, int activeLen,
      long seed, boolean randomizeBlockLocationsPerBlock) {
    // If reader is not a datanode (not in NetworkTopology tree), we need to
    // replace this reader with a sibling leaf node in tree.
    if (reader != null && !this.contains(reader)) {
      Node nodeGroup = getNode(reader.getNetworkLocation());
      if (nodeGroup != null && nodeGroup instanceof InnerNode) {
        InnerNode parentNode = (InnerNode) nodeGroup;
        // replace reader with the first children of its parent in tree
        reader = parentNode.getLeaf(0, null);
      } else {
        return;
      }
    }
    super.sortByDistance(reader, nodes, nodes.length, seed,
        randomizeBlockLocationsPerBlock);
  }
{code}"
HADOOP-10963,Move compile-time dependency to JDK7,"As discussed on the *-dev@hadoop.apache.org mailing list, this jira tracks moving to JDK7 and dropping support for JDK6."
HADOOP-10962,Flags for posix_fadvise are not valid in some architectures,"In org.apache.hadoop.io.nativeio.NativeIO.java, the posix_fadvise flag parameter is hardcoded to the most common values in fcntl.h.

However, not all architectures use the same values (in this case, System z/Linux) A better approach would be to not make assumptions about fcntl.h values (or any other system constants).

This bug results in calls to posix_fadvise failing in zLinux."
HADOOP-10957,The globber will sometimes erroneously return a permission denied exception when there is a non-terminal wildcard,"The globber will sometimes erroneously return a permission denied exception when there is a non-terminal wildcard.  The existing unit tests don't catch this, because it doesn't happen for superusers."
HADOOP-10956,Fix create-release script to include docs and necessary txt files,The create-release script doesn't include docs in the binary tarball. We should fix that. 
HADOOP-10954,Adding site documents of hadoop-tools,There are no pages for hadoop-tools in the site documents of branch-2 or later.
HADOOP-10953,NetworkTopology#add calls NetworkTopology#toString without holding the netlock,"Found this issue while reading the related code. In NetworkTopology.toString() method, there is no thread safety guarantee directly, it's called by add/remove, and inside add/remove, most of this.toString() calls are protected by rwlock, except a couple of error handling codes, one possible fix is that moving them into lock as well, due to not heavy operations, so no obvious downgration should be observed per my current knowledge."
HADOOP-10946,Fix a bunch of typos in log messages,There are a bunch of typos in various log messages.  These need cleaning up.
HADOOP-10939,Fix TestKeyProviderFactory testcases to use default 128 bit length keys,[HADOOP-10936|https://issues.apache.org/jira/browse/HADOOP-10936] changed the default KeyProvider bit length to 128. {{TestKeyProviderFactory}} needs to be fixed to use new key length.
HADOOP-10938,Remove thread-safe description in PositionedReadable javadoc,"According to discussion in HDFS-6813, we may need to remove thread-safe description in PositionedReadable javadoc, since DFSInputStream, WebhdfsFileSystem#inputStream, HarInputStream don't implement them with thread-safe."
HADOOP-10937,Need to set version name correctly before decrypting EEK,"Touchz-ing a file results in a Null Pointer Exception

{noformat}
[hdfs@mynode hadoop-common]$ hdfs dfs -touchz /enc3/touchFIle
2014-08-01 08:45:10,148 INFO  [main] hdfs.DFSClient (DFSClient.java:<init>(605)) - Found KeyProvider: KeyProviderCryptoExtension: KMSClientProvider[http://mynode.myhost.com:16000/kms/v1/]
-touchz: Fatal internal error
java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)
	at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)
	at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)
	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)
	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
{noformat}"
HADOOP-10936,Change default KeyProvider bitlength to 128,"You need to download unlimited strength JCE to work with 256-bit keys. It'd be good to change the default to 128 to avoid needing the unlimited strength JCE, and print out the bitlength being used in places."
HADOOP-10933,FileBasedKeyStoresFactory Should use Configuration.getPassword for SSL Passwords,"As part of HADOOP-10904, this jira represents the ability to leverage the credential provider API when clear text passwords on disk are unacceptable. By using the Configuration.getPassword method, the credential provider API may be used while maintaining backward compatibility for passwords stored in config/files."
HADOOP-10932,"compile error on project ""Apache Hadoop OpenStack support""","compile hadoop, it has error as below:
[ERROR] /home/git_repo/hadoop-common/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/SwiftTestUtils.java:[41,7] cannot access org.hamcrest.Matcher
class file for org.hamcrest.Matcher not found
public class SwiftTestUtils extends org.junit.Assert {"
HADOOP-10931,"compile error on project ""Apache Hadoop OpenStack support""","use command ”mvn package -Pdist,native -DskipTests -Dtar“ to compile hadoop, it have errors as below:
[ERROR] /home/hadoop-common/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/SwiftTestUtils.java:[41,7] cannot access org.hamcrest.Matcher
class file for org.hamcrest.Matcher not found"
HADOOP-10929,Typo in Configuration.getPasswordFromCredentialProviders,"Transposed letters in getPasswordFromCredenitalProviders need to be fixed to be ""Credential""."
HADOOP-10928,Incorrect usage on 'hadoop credential list',"{{hadoop credential list}}'s usage message states a mandatory {{alias}} argument. The code does not actually accept an alias.

Fix the message."
HADOOP-10927,Fix CredentialShell help behavior and error codes,"{noformat}
$ hadoop credential
java.lang.NullPointerException
	at org.apache.hadoop.security.alias.CredentialShell.run(CredentialShell.java:67)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:420)
{noformat}

Ran a no-arg version of {{hadoop credential}} expecting to get the usage/help message (like other commands act), and got the above exception instead."
HADOOP-10925,Compilation fails in native link0 function on Windows.,HDFS-6482 introduced a new native code function for creating hard links.  The Windows implementation of this function does not compile due to an incorrect call to {{CreateHardLink}}.
HADOOP-10922,User documentation for CredentialShell,The CredentialShell needs end user documentation for the website.
HADOOP-10920,site plugin couldn't parse hadoop-kms index.apt.vm,"From log of https://builds.apache.org/job/Hadoop-Common-trunk/1193 :
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.3:site (docs) on project hadoop-kms: Error during page generation: Error parsing '<https://builds.apache.org/job/Hadoop-Common-trunk/ws/trunk/hadoop-common-project/hadoop-kms/src/site/apt/index.apt.vm'>: line [126] expected SECTION2, found SECTION3 -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.3:site (docs) on project hadoop-kms: Error during page generation
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:108)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:76)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:116)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:361)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: Error during page generation
        at org.apache.maven.plugins.site.SiteMojo.execute(SiteMojo.java:143)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:133)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 19 more
Caused by: org.apache.maven.doxia.siterenderer.RendererException: Error parsing '<https://builds.apache.org/job/Hadoop-Common-trunk/ws/trunk/hadoop-common-project/hadoop-kms/src/site/apt/index.apt.vm'>: line [126] expected SECTION2, found SECTION3
        at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.renderDocument(DefaultSiteRenderer.java:414)
        at org.apache.maven.doxia.siterenderer.DoxiaDocumentRenderer.renderDocument(DoxiaDocumentRenderer.java:53)
        at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.renderModule(DefaultSiteRenderer.java:319)
        at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render(DefaultSiteRenderer.java:135)
        at org.apache.maven.plugins.site.SiteMojo.renderLocale(SiteMojo.java:175)
        at org.apache.maven.plugins.site.SiteMojo.execute(SiteMojo.java:138)
        ... 21 more
Caused by: org.apache.maven.doxia.module.apt.AptParseException: expected SECTION2, found SECTION3
        at org.apache.maven.doxia.module.apt.AptParser.parse(AptParser.java:235)
        at org.apache.maven.doxia.DefaultDoxia.parse(DefaultDoxia.java:65)
        at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.renderDocument(DefaultSiteRenderer.java:406)
        ... 26 more
Caused by: org.apache.maven.doxia.module.apt.AptParseException: expected SECTION2, found SECTION3
        at org.apache.maven.doxia.module.apt.AptParser.expectedBlock(AptParser.java:1404)
        at org.apache.maven.doxia.module.apt.AptParser.traverseSection(AptParser.java:787)
        at org.apache.maven.doxia.module.apt.AptParser.traverseSection(AptParser.java:823)
        at org.apache.maven.doxia.module.apt.AptParser.traverseBody(AptParser.java:765)
        at org.apache.maven.doxia.module.apt.AptParser.parse(AptParser.java:230)
        ... 28 more
[ERROR]
{code}"
HADOOP-10918,JMXJsonServlet fails when used within Tomcat,"{{JMXJsonServlet.doGet()}} has the following check:

{code}
      if (!HttpServer2.isInstrumentationAccessAllowed(getServletContext(),
                                                     request, response)) {
{code}

Loading the class {{HttpServer2}} triggers loading Jetty specific classes:

{code}
SEVERE: Servlet.service() for servlet jmx-servlet threw exception
java.lang.ClassNotFoundException: org.mortbay.jetty.servlet.Context
        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680)
        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526)
        at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:157)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:617)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.hadoop.crypto.key.kms.server.KMSMDCFilter.doFilter(KMSMDCFilter.java:84)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:438)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:255)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:408)
        at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:128)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)
        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)
        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)
        at java.lang.Thread.run(Thread.java:695)
Jul 31, 2014 2:46:24 PM org.apache.catalina.core.StandardWrapperValve invoke
{code}

Because of this the JMX servlet fails to work in KMS"
HADOOP-10911,hadoop.auth cookie after HADOOP-10710 still not proper according to RFC2109,"I'm seeing the same problem reported in HADOOP-10710 (that is, httpclient is unable to authenticate with servers running the authentication filter), even with HADOOP-10710 applied.

From my reading of the spec, the problem is as follows:
Expires is not a valid directive according to the RFC, though it is mentioned for backwards compatibility with netscape draft spec.  When httpclient sees ""Expires"", it parses according to the netscape draft spec, but note from RFC2109:
{code}
Note that the Expires date format contains embedded spaces, and that ""old"" cookies did not have quotes around values. 
{code}
and note that AuthenticationFilter puts quotes around the value:
https://github.com/apache/hadoop-common/blob/6b11bff94ebf7d99b3a9e513edd813cb82538400/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java#L437-L439

So httpclient's parsing appears to be kosher."
HADOOP-10910,Increase findbugs maxHeap size,The release build fails because of an obscure findbugs error. Testing reveals that this is related to the findbugs heap size.
HADOOP-10905,LdapGroupsMapping Should use configuration.getPassword for SSL and LDAP Passwords,"By using Configuration.getPassword the credential provider API can be used to provide an alternative to storing the ssl and ldap bind passwords in clear text within the configuration files.

getPassword will enable us to not sure clear text passwords when required and provide backward compatibility for when it is not necessary and existing deployments."
HADOOP-10903,Enhance hadoop classpath command to expand wildcards or write classpath into jar manifest.,"The ""hadoop classpath"" shell command currently prints the classpath variable established by the shell scripts and then exits.  A few enhancements to this command would be desirable to support a few other use cases."
HADOOP-10902,Deletion of directories with snapshots will not output reason for trash move failure,"When using trash-enabled FsShell to delete a directory that has snapshots, we se an error message saying ""Failed to move to trash"" but no  explanation.

{code}
[hdfs@schu-enc2 ~]$ hdfs dfs -rm -r snap
2014-07-28 05:45:29,527 INFO  [main] fs.TrashPolicyDefault (TrashPolicyDefault.java:initialize(92)) - Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes.
rm: Failed to move to trash: hdfs://schu-enc2.vpc.com:8020/user/hdfs/snap. Consider using -skipTrash option
{code}

If we use -skipTrash, then we'll get the explanation: ""rm: The directory /user/hdfs/snap cannot be deleted since /user/hdfs/snap is snapshottable and already has snapshots""

It'd be an improvement to make it clear that dirs with snapshots cannot be deleted when we're using the trash."
HADOOP-10900,CredentialShell args should use single-dash style,"As was discussed in HADOOP-10793 related to KeyShell, we should standardize on single-dash flags for things in branch-2. CredentialShell also needs to be updated."
HADOOP-10896,Update compatibility doc to capture visibility of un-annotated classes/ methods,"From discussion on email thread, we should add something to the effect of 

""Classes without annotations are to considered @Private. Class members without specific annotations inherit the annotations of the class."" "
HADOOP-10894,Fix dead link in ToolRunner documentation,"Looking at http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/util/ToolRunner.html, at least one of the links is non-existent. This issue is very similar to HADOOP-10864."
HADOOP-10893,isolated classloader on the client side,"We have the job classloader on the mapreduce tasks that run on the cluster. It has a benefit of being able to isolate class space for user code and avoid version clashes.

Although it occurs less often, version clashes do occur on the client JVM. It would be good to introduce an isolated classloader on the client side as well to address this. A natural point to introduce this may be through RunJar, as that's how most of hadoop jobs are run."
HADOOP-10891,Add EncryptedKeyVersion factory method to KeyProviderCryptoExtension,"For fs-encryption, we need to create a EncryptedKeyVersion from its component parts for decryption. We also need a way of getting a KPCE from a conf. Both of these can be done with factory methods."
HADOOP-10890,TestDFVariations.testMount fails intermittently,"Failure message:

{code}
Error Message

Specified path /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/datadoes not exist

Stacktrace

java.io.FileNotFoundException: Specified path /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/datadoes not exist
	at org.apache.hadoop.fs.DF.getMount(DF.java:109)
	at org.apache.hadoop.fs.TestDFVariations.testMount(TestDFVariations.java:54)

Standard Output

java.io.IOException: Fewer lines of output than expected: Filesystem     1K-blocks     Used Available Use% Mounted on
java.io.IOException: Unexpected empty line
java.io.IOException: Could not parse line:        19222656 10597036   7649060  59% /
{code}


"
HADOOP-10887,Add XAttrs to ViewFs and make XAttrs + ViewFileSystem internal dir behavior consistent,"This is very similar to the work done in HADOOP-10845 (Add common tests for ACLs in combination with viewfs)

Here we make the XAttrs + ViewFileSystem internal dir behavior consistent. Right now, when users attempt XAttr operation on an internal dir, they will get an UnsupportedOperationException. Instead, we should throw the ReadOnlyMountTable AccessControlException or the NotInMountPointException.

We also add the XAttrs APIs to ViewFs. This involves adding them to ChRootedFs as well. Also, {{listXAttrs}} is missing from FileContext, so we should add that in."
HADOOP-10886,CryptoCodec#getCodecclasses throws NPE when configurations not loaded.,"There are some test cases which will not load the xml defaults. In this case, CryptoCodec#getCodecclasses will fail with NPE.

{noformat}
java.lang.NullPointerException: null
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
        at com.google.common.base.Splitter.split(Splitter.java:371)
        at org.apache.hadoop.crypto.CryptoCodec.getCodecClasses(CryptoCodec.java:100)
        at org.apache.hadoop.crypto.CryptoCodec.getInstance(CryptoCodec.java:54)
        at org.apache.hadoop.crypto.CryptoCodec.getInstance(CryptoCodec.java:91)
        at org.apache.hadoop.crypto.TestCryptoStreamsForLocalFS.init(TestCryptoStreamsForLocalFS.java:53)
{noformat}

https://builds.apache.org/job/Hadoop-fs-encryption-nightly/71/"
HADOOP-10884,Fix dead link in Configuration javadoc,"In [Configuration javadoc|http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/conf/Configuration.html], the link to core-default.xml is dead. We should fix it."
HADOOP-10882,Move DirectBufferPool into common util,"MAPREDUCE-2841 uses a direct buffer pool to pass data back and forth between native and Java code. The branch has an implementation which appears to be derived from the one in HDFS. Instead of copy-pasting, we should move the HDFS DirectBufferPool into Common so that MR can make use of it."
HADOOP-10881,Clarify usage of encryption and encrypted encryption key in KeyProviderCryptoExtension,
HADOOP-10880,Move HTTP delegation tokens out of URL querystring to a header,"Following up on a discussion in HADOOP-10799.

Because URLs are often logged, delegation tokens may end up in LOG files while they are still valid. 

We should move the tokens to a header.

We should still support tokens in the querystring for backwards compatibility."
HADOOP-10872,"TestPathData fails intermittently with ""Mkdirs failed to create d1""","A bunch of TestPathData tests failed intermittently, e.g.
https://builds.apache.org/job/PreCommit-HDFS-Build/7416//testReport/

Example failure log:
{code}
Failed

org.apache.hadoop.fs.shell.TestPathData.testUnqualifiedUriContents
Failing for the past 1 build (Since Failed#7416 )
Took 0.46 sec.
Error Message

Mkdirs failed to create d1

Stacktrace

java.io.IOException: Mkdirs failed to create d1
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:426)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:849)
	at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:1149)
	at org.apache.hadoop.fs.shell.TestPathData.initialize(TestPathData.java:54)
{code}

"
HADOOP-10868,Create a ZooKeeper-backed secret provider,Create a secret provider (see HADOOP-10791) that is backed by ZooKeeper and can synchronize amongst different servers.
HADOOP-10866,RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses non-curly quotes,"Symlink tests failure happened from time to time,

https://builds.apache.org/job/PreCommit-HDFS-Build/7383//testReport/
https://builds.apache.org/job/PreCommit-HDFS-Build/7376/testReport/

{code}
Failed

org.apache.hadoop.fs.TestSymlinkLocalFSFileContext.testDanglingLink

Failing for the past 1 build (Since Failed#7376 )
Took 83 ms.
Error Message

Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link
Stacktrace

java.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link
	at org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)
	at org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Standard Output

2014-07-17 23:31:37,770 WARN  fs.FileUtil (FileUtil.java:symLink(829)) - Command 'ln -s /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/file /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test2/linkToFile' failed 1 with: ln: failed to create symbolic link '/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test2/linkToFile': No such file or directory

2014-07-17 23:31:38,109 WARN  fs.FileUtil (FileUtil.java:symLink(829)) - Command 'ln -s /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/file /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile' failed 1 with: ln: failed to create symbolic link '/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile': File exists
{code}
"
HADOOP-10864,Tool documentenation is broken,"Looking at http://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/util/Tool.html, at least one of the links is non-existent.  There are likely other bugs in this documentation too."
HADOOP-10863,KMS should have a blacklist for decrypting EEKs,"In particular, we'll need to put HDFS admin user there by default to prevent an HDFS admin from getting file encryption keys."
HADOOP-10862,Miscellaneous trivial corrections to KMS classes,"{{KMSRESTConstants.java}}, {{KEY_OP}} should be {{KEYS}} and value should be {{keys}}.

{{KMS.java}} should be annotated with Jersey {{@Singleton}} to avoid creating an instance on every request, it is thread safe already.

Make sure all KMS related classes are annotated with private audience."
HADOOP-10857,Native Libraries Guide doen't mention a dependency on openssl-development package,{{maven compile -Pnative}} fails without installing openssl-development package(e.g. libssl-dev). We should describe it in Native Libraries Guide.
HADOOP-10855,Allow Text to be read with a known length,"For the native task work (MAPREDUCE-2841) it is useful to be able to store strings in a different fashion than the default (varint-prefixed) serialization. We should provide a ""read"" method in Text which takes an already-known length to support this use case while still providing Text objects back to the user."
HADOOP-10852,NetgroupCache is not thread-safe,"_NetgroupCache_ internally uses two ConcurrentHashMaps and a boolean variable to signal updates on one of the ConcurrentHashMap
None of the functions are synchronized  and hence is possible to have unexpected results due to race condition between different threads.

As an example, consider the following sequence:

Thread1 :
{{add}} a group
{{netgroupToUsersMap}} is updated.
{{netgroupToUsersMapUpdated}} is set to true.
Thread 2:
calls {{getNetgroups}} for a user
Due to re-ordering, {{netgroupToUsersMapUpdated=true}} is visible, but updates in {{netgroupToUsersMap}} is not visible.
Does a wrong update with older {{netgroupToUsersMap}} values. "
HADOOP-10851,NetgroupCache does not remove group memberships,"_NetgroupCache_ is used by _GroupMappingServiceProvider_ implementations based on net groups.
But it has a serious flaw in that once a user to group membership is established, it remains forever even if user is actually removed from the netgroup and cache is cleared.  It is cleared only if the server is restarted.

To reproduce this: 

* Cache a group with a set of users.
* Test membership correctness.
* Clear cache, remove a user from the group and cache the group again
* Expected result : user’s groups should not include the group from which he/she is removed. 
* Actual result : user’s groups includes the group from which he/she was removed.

It is also noted that _NetgroupCache_ has concurrency issues and a separate jira is filed to rectify them.
"
HADOOP-10847,Remove the usage of sun.security.x509.* in testing code,"As was told by Max (Oracle), JDK9 is likely to block all accesses to sun.* classes.

Below is from email of Andrew Purtell:
{quote}
The use of sun.* APIs to create a certificate in Hadoop and HBase test code can be removed. Someone (Intel? Oracle?) can submit a JIRA that replaces the programmatic construction with a stringified binary cert for use in the relevant unit tests. 
{quote}

In Hadoop, the calls in question are below:
{code}
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:24:import sun.security.x509.CertificateIssuerName;
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:25:import sun.security.x509.CertificateSerialNumber;
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:26:import sun.security.x509.CertificateSubjectName;
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:27:import sun.security.x509.CertificateValidity;
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:28:import sun.security.x509.CertificateVersion;
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:29:import sun.security.x509.CertificateX509Key;
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:30:import sun.security.x509.X500Name; 
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:31:import sun.security.x509.X509CertImpl; 
hadoop-common/src/test/java/org/apache/hadoop/security/ssl/KeyStoreTestUtil.java:32:import sun.security.x509.X509CertInfo;
{code}"
HADOOP-10845,Add common tests for ACLs in combination with viewfs.,Add tests in Hadoop Common for the ACL APIs in combination with viewfs.
HADOOP-10843,TestGridmixRecord unit tests failure on PowerPC,"In TestGridmixRecord#binSortTest, the test expects the comparison result of WritableComparator.compareBytes, which uses UnsafeComparer, to be the integer difference rather than the documented ""@return 0 if equal, < 0 if left is less than right, etc."". 

TestGridmixRecord#binSortTest code snippet
{code}
      final int chk = WritableComparator.compareBytes(
          out1.getData(), 0, out1.getLength(),
          out2.getData(), 0, out2.getLength());
      assertEquals(chk, x.compareTo(y));
      assertEquals(chk, cmp.compare(
            out1.getData(), 0, out1.getLength(),
            out2.getData(), 0, out2.getLength()));
{code}


The code snippet below shows the Unsafe comparator behavior for non-little-endian machines. 
{code}
	if (!littleEndian) {
	  return lessThanUnsigned(lw, rw) ? -1 : 1;
	}
{code}
"
HADOOP-10842,CryptoExtension generateEncryptedKey method should receive the key name,"Generating an EEK should be done using always the current keyversion of a key name. We should enforce that by API by handing off EEKs for the last keyversion of a keyname only, thus we should ask for EEKs for a keyname and the {{CryptoExtension}} should use the last keyversion."
HADOOP-10841,EncryptedKeyVersion should have a key name property,"having a keyname will help the NN to efficiently (without additional keyprovider calls, which can translate into remote calls) determine the key name of an EDEK."
HADOOP-10840,Fix OutOfMemoryError caused by metrics system in Azure File System,"In Hadoop 2.x the Hadoop File System framework changed and no cache is implemented (refer to HADOOP-6356). This means for every WASB access, a new NativeAzureFileSystem is created, along which a Metrics source created and added to MetricsSystemImpl. Over time the sources accumulated, eating memory and causing Java OutOfMemoryError.

The fix is to utilize the unregisterSource() method added to MetricsSystem in HADOOP-10839."
HADOOP-10839,Add unregisterSource() to MetricsSystem API,"Currently the MetrisSystem API has register() method to register a MetricsSource but doesn't have unregister() method. This means once a MetricsSource is registered with the MetricsSystem, it will be there forever until the MetricsSystem is shut down. This in some cases can cause Java OutOfMemoryError.

One such case is in file system metrics implementation. The new AbstractFileSystem/FileContext framework does not implement a cache so every file system access can lead to the creation of a NativeFileSystem instance. (refer to HADOOP-6356). And all these NativeFileSystem needs to share the same instance of MetricsSystemImpl, which means we cannot shut down MetricsSystem to clean up all the MetricsSources that has been registered but no longer active. Over time the MetricsSource instance accumulates and eventually we saw OutOfMemoryError."
HADOOP-10838,Byte array native checksumming,
HADOOP-10835,Implement HTTP proxyuser support in HTTP authentication client/server libraries,This is to implement generic handling of proxyuser in the {{DelegationTokenAuthenticatedURL}} and {{DelegationTokenAuthenticationFilter}} classes and to wire properly UGI on the server side.
HADOOP-10833,Remove unused cache in UserProvider,"_UserProvider_ contains the field cache.
{code}
  private final Map<String, CredentialEntry> cache = new HashMap<String, 
      CredentialEntry>();
{code}
	
It is referenced only in {{deleteCredentialEntry}} and so there is no real usage of {{cache}}."
HADOOP-10830,Missing lock in JavaKeyStoreProvider.createCredentialEntry,"_JavaKeyStoreProvider_ uses _ReentrantReadWriteLock_  to provide thread safety.
The {{createCredentialEntry}} should hold _writeLock_ before adding the entry.

"
HADOOP-10826,Iteration on KeyProviderFactory.serviceLoader  is thread-unsafe,"KeyProviderFactory uses _ServiceLoader_ framework to load _KeyProviderFactory_
{code}
  private static final ServiceLoader<KeyProviderFactory> serviceLoader =
      ServiceLoader.load(KeyProviderFactory.class);
{code}
The _ServiceLoader_ framework does lazy initialization of services which makes it thread unsafe. If accessed from multiple threads, it is better to synchronize the access.
Similar synchronization has been done while loading compression codec providers via HADOOP-8406.
"
HADOOP-10824,Refactor KMSACLs to avoid locking,"Currently _KMSACLs_ is made thread safe using _ReadWriteLock_. It is possible to safely publish the _acls_ collection using _volatile_.
Similar refactoring has been done in [HADOOP-10448|https://issues.apache.org/jira/browse/HADOOP-10448?focusedCommentId=13980112&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13980112]
"
HADOOP-10821,Prepare the release notes for Hadoop 2.5.0,"The release notes for 2.3.0+ (http://hadoop.apache.org/docs/r2.4.1/index.html) still talk about federation and MRv2
being new features. We should update them."
HADOOP-10820,Throw an exception in GenericOptionsParser when passed an empty Path,"An empty token (e.g. ""a.jar,,b.jar"") in the -libjars option causes the current working directory to be recursively localized.

Here's an example of this in action (using Hadoop 2.2.0):

{code}
# create a temp directory and touch three JAR files
mkdir -p tmp/path && cd tmp && touch a.jar b.jar c.jar path/d.jar

# Run an example job only specifying two of the JARs.
# Include an empty entry in libjars.
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar pi -libjars a.jar,,c.jar 2 1000000000

# As the job is running examine the localized directory in HDFS.
# Notice that not only are the two JAR's specified in libjars copied,
# but in addition the contents of the working directory are also recursively copied.
$ hadoop fs -lsr /tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/a.jar
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/c.jar
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/tmp
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/tmp/a.jar
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/tmp/b.jar
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/tmp/c.jar
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/tmp/path
/tmp/hadoop-yarn/staging/aholmes/.staging/job_1404752711144_0018/libjars/tmp/path/d.jar
{code}"
HADOOP-10817,ProxyUsers configuration should support configurable prefixes ,"Currently {{ProxyUsers}} and the {{ImpersonationProvider}} are hardcoded to use {{hadoop.proxyuser.}} prefixes for loading proxy user configuration.

Adding the possibility of using a custom prefix will enable reusing the {{ProxyUsers}} class from other components (i.e. HttpFS and KMS).
"
HADOOP-10816,"KeyShell returns -1 on error to the shell, should be 1","I've seen this in several places now - commands returning -1 on failure to the shell. It's a bug. Someone confused their posix style returns (0 on success, < 0 on failure) with program returns, which are an unsigned character. Thus, a return of -1 actually becomes 255 to the shell.
{noformat}
$ hadoop key create happykey2 --provider kms://http@localhost:16000/kms --attr ""a=a"" --attr ""a=b""

Each attribute must correspond to only one value:
atttribute ""a"" was repeated

...

$ echo $?
255
{noformat}

A return value of 1 instead of -1 does the right thing."
HADOOP-10815,Implement Windows equivalent of mlock.,"To support HDFS Centralized Cache Management on Windows, we need a native code function for Windows that is equivalent to POSIX {{mlock}}."
HADOOP-10814,Update Tomcat version used by HttpFS and KMS to latest 6.x version,"KMS and HttpFS are using Tomcat 6.0.37, we should move it to 6.0.41 to get bug fixes and security fixes.

We should add a property with the tomcat version in the hadoop-project POM and use that property from KMS and HttpFS.

"
HADOOP-10812,Delegate KeyProviderExtension#toString to underlying KeyProvider,"Would be nice to delegate this, else we get the default reference toString"
HADOOP-10810,Clean up native code compilation warnings.,There are several compilation warnings coming from the native code on both Linux and Windows.
HADOOP-10809,hadoop-azure: page blob support,"Azure Blob Storage provides two flavors: block-blobs and page-blobs.  Block-blobs are the general purpose kind that support convenient APIs and are the basis for the Azure Filesystem for Hadoop (see HADOOP-9629).

Page-blobs use the same namespace as block-blobs but provide a different low-level feature set.  Most importantly, page-blobs can cope with an effectively infinite number of small accesses whereas block-blobs can only tolerate 50K appends before relatively manual rewriting of the data is necessary.  A simple analogy is that page-blobs are like a regular disk and the basic API is like a low-level device driver.

See http://msdn.microsoft.com/en-us/library/azure/ee691964.aspx for some introductory material.

The primary driving scenario for page-blob support is for HBase transaction log files which require an access pattern of many small writes.  Additional scenarios can also be supported.

Configuration:
The Hadoop Filesystem abstraction needs a mechanism so that file-create can determine whether to create a block- or page-blob.  To permit scenarios where application code doesn't know about the details of azure storage we would like the configuration to be Aspect-style, ie configured by the Administrator and transparent to the application. The current solution is to use hadoop configuration to declare a list of page-blob folders -- Azure Filesystem for Hadoop will create files in these folders using page-blob flavor.  The configuration key is ""fs.azure.page.blob.dir"", and description can be found in AzureNativeFileSystemStore.java.

Code changes:
- refactor of basic Azure Filesystem code to use a general BlobWrapper and specialized BlockBlobWrapper vs PageBlobWrapper
- introduction of PageBlob support (read, write, etc)
- miscellaneous changes such as umask handling, implementation of createNonRecursive(), flush/hflush/hsync.
- new unit tests.

Credit for the primary patch: Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, Mike Liddell.

Also included in the patch is support for atomic folder rename over the Azure blob store through the Azure file system layer for Hadoop. See the README file for more details, including how to use the fs.azure.atomic.rename.dir configuration variable to control where atomic folder rename logic is applied. By default, folders under /hbase have atomic rename applied, which is needed for correct operation of HBase."
HADOOP-10808,Remove unused native code for munlock.,"The Centralized Cache Management project added a native function for calling {{munlock}}.  This function is unused though, because Centralized Cache Management calls {{munmap}}, which implicitly unlocks the memory too.  Let's remove the unused code.  This is a private/unstable class, so there is no backwards-compatibility concern."
HADOOP-10801,Fix dead link in site.xml,"Documents for FileSystem API definition were created in HADOOP-9361 but not linked.
In hadoop-project/src/site/site.xml,
{code}
      <item name=""FileSystem Specification""
        href=""hadoop-project-dist/hadoop-common/index.html""/>
{code}
should be
{code}
      <item name=""FileSystem Specification""
        href=""hadoop-project-dist/hadoop-common/filesystem/index.html""/>
{code}"
HADOOP-10793,KeyShell args should use single-dash style,"Follow-on from HADOOP-10736 as per [~andrew.wang] - the key shell uses the gnu double dash style for command line args, while other command line programs use a single dash.  Consider changing this, and consider another argument parsing scheme, like the CommandLine class."
HADOOP-10791,AuthenticationFilter should support externalizing the secret for signing and provide rotation support,"It should be possible to externalize the secret used to sign the hadoop-auth cookies.

In the case of WebHDFS the shared secret used by NN and DNs could be used. In the case of Oozie HA, the secret could be stored in Oozie HA control data in ZooKeeper.

In addition, it is desirable for the secret to change periodically, this means that the AuthenticationService should remember a previous secret for the max duration of hadoop-auth cookie."
HADOOP-10786,Fix UGI#reloginFromKeytab on Java 8,"Krb5LoginModule changed subtly in java 8: in particular, if useKeyTab and storeKey are specified, then only a KeyTab object is added to the Subject's private credentials, whereas in java <= 7 both a KeyTab and some number of KerberosKey objects were added.

The UGI constructor checks whether or not a keytab was used to login by looking if there are any KerberosKey objects in the Subject's private credentials. If there are, then isKeyTab is set to true, and otherwise it's set to false.

Thus, in java 8 isKeyTab is always false given the current UGI implementation, which makes UGI#reloginFromKeytab fail silently.

Attached patch will check for a KeyTab object on the Subject, instead of a KerberosKey object. This fixes relogins from kerberos keytabs on Oracle java 8, and works on Oracle java 7 as well."
HADOOP-10782,Typo in DataChecksum classs,
HADOOP-10781,Unportable getgrouplist() usage breaks FreeBSD,"getgrouplist() has different return values on Linux and FreeBSD:
Linux: either the number of groups (positive) or -1 on error
FreeBSD: 0 on success or -1 on error

The return value of getgrouplist() is analyzed in Linux-specific way in 
hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/security/hadoop_user_info.c, in function hadoop_user_info_getgroups() which breaks FreeBSD.

In this function you have 3 choices for the return value 
ret = getgrouplist(uinfo->pwd.pw_name, uinfo->pwd.pw_gid,
                         uinfo->gids, &ngroups);

1) ret > 0 : OK for Linux, it will be zero on FreeBSD.  I propose to change this to ret >= 0
2) First condition is false and ret != -1:  impossible according to manpage
3) ret == 1 -- OK for both Linux and FreeBSD

So I propose to change ""ret > 0"" to ""ret >= 0"" and (optionally) return 2nd case."
HADOOP-10780,hadoop_user_info_alloc fails on FreeBSD due to incorrect sysconf use,"I am trying hadoop-2.4.1 on FreeBSD-10/stable.
namenode starts up, but after first datanode contacts it, it throws an exception.
All limits seem to be high enough:

% limits -a
Resource limits (current):
  cputime              infinity secs
  filesize             infinity kB
  datasize             33554432 kB
  stacksize              524288 kB
  coredumpsize         infinity kB
  memoryuse            infinity kB
  memorylocked         infinity kB
  maxprocesses           122778
  openfiles              140000
  sbsize               infinity bytes
  vmemoryuse           infinity kB
  pseudo-terminals     infinity
  swapuse              infinity kB

14944  1  S        0:06.59 /usr/local/openjdk7/bin/java -Dproc_namenode -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop -Dhadoop.log.file=hadoop-hdfs-namenode-nezabudka3-00.log -Dhadoop.home.dir=/usr/local -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx32768m -Xms32768m -Djava.library.path=/usr/local/lib -Xmx32768m -Xms32768m -Djava.library.path=/usr/local/lib -Xmx32768m -Xms32768m -Djava.library.path=/usr/local/lib -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode


From the namenode's log:

2014-07-03 23:28:15,070 WARN  [IPC Server handler 5 on 8020] ipc.Server (Server.java:run(2032)) - IPC Server handler 5 on 8020, call org.apache.hadoop.hdfs.server.protocol.Datano
deProtocol.versionRequest from 5.255.231.209:57749 Call#842 Retry#0
java.lang.OutOfMemoryError
        at org.apache.hadoop.security.JniBasedUnixGroupsMapping.getGroupsForUser(Native Method)
        at org.apache.hadoop.security.JniBasedUnixGroupsMapping.getGroups(JniBasedUnixGroupsMapping.java:80)
        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
        at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1417)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:81)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3331)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkSuperuserPrivilege(FSNamesystem.java:5491)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.versionRequest(NameNodeRpcServer.java:1082)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.versionRequest(DatanodeProtocolServerSideTranslatorPB.java:234)
        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28069)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)


I did not have such an issue with hadoop-1.2.1."
HADOOP-10774,Update KerberosTestUtils for hadoop-auth tests when using IBM Java,"There are two issues if IBM Java is used while testing hadoop-auth tests.
 
Looks like there are lot of changes haven been done to properly handle the kerbserose authentication using the JIRA defect: HADOOP-9446 for IBM JAVA.
But their are still some failures can been seen in ""Apache Hadoop Common"" tests in case of IBM JAVA.
Available patch for HADOOP-10774 will solve the authentication issues plus the path issues.

Two issue issue related to IBM java are.

1) Bad JAAS configuration: unrecognized option: isInitiator
2) Cannot retrieve key from keytab HTTP/localhost@EXAMPLE.COM

#1 Is caused as isInitiator isn't defined when we use IBM JAVA. 
#2 IS caused as, For IBM_JAVA keytab file must be a absolute path with file:// as the prefix for the useKeytab option.
   But the file path is relative. This change will work with both openjdk & IBM_JAVA. 
  
Attached patch will resolve all failures happening if we use IBM Java.   
"
HADOOP-10771,Refactor HTTP delegation support out of httpfs to common,"HttpFS implements delegation token support in {{AuthenticationFilter}} & {{AuthenticationHandler}} subclasses.

For HADOOP-10770 we need similar functionality for KMS.

Not to duplicate code, we should refactor existing code to common."
HADOOP-10770,KMS add delegation token support,This is a follow up on HADOOP-10769 for KMS itself.
HADOOP-10769,Create KeyProvider extension to handle delegation tokens,"The KeyProvider API needs to return delegation tokens to enable access to the KeyProvider from processes without Kerberos credentials (ie Yarn containers).

This is required for HDFS encryption and KMS integration."
HADOOP-10767,Clean up unused code in Ls shell command.,"After recent ACL changes, there is some unused code left in {{Ls}} that we can clean up."
HADOOP-10758,KMS: add ACLs on per key basis.,The KMS server should enforce ACLs on per key basis.
HADOOP-10757,KeyProvider KeyVersion should provide the key name,"Currently the {{KeyVersion}} does not provide a way to get the key name to do a reverse lookup to get the metadata of the key.

For the {{JavaKeyStoreProvider}} and the {{UserProvider}} this is not an issue because the key name is encoded in the key version name. 

This encoding of the key name in the key version name cannot be expected in all KeyProvider implementations. It is common for key management systems to use UUID to refer to specific key materials (KeyVersions in Hadoop parlance).

"
HADOOP-10756,KMS audit log should consolidate successful similar requests,"Every rejected access should be audited, but successful accesses should be consolidated within a given amount of time if the request is from the same user for he same key. "
HADOOP-10755,Support negative caching of user-group mapping,"We've seen a situation at a couple of our customers where interactions from an unknown user leads to a high-rate of group mapping calls. In one case, this was happening at a rate of 450 calls per second with the shell-based group mapping, enough to severely impact overall namenode performance and also leading to large amounts of log spam (prints a stack trace each time).

Let's consider negative caching of group mapping, as well as quashing the rate of this log message."
HADOOP-10754,Reenable several HA ZooKeeper-related tests on Windows.,"Now that our version of ZooKeeper has been upgraded in HADOOP-9555, we can reenable several tests that had been broken on Windows."
HADOOP-10750,KMSKeyProviderCache should be in hadoop-common,"KMS has {{KMSCacheKeyProvider}}, this class should be available in hadoop-common for users of  {{KeyProvider}} instances to wrap them and avoid several, potentially expensive, key retrievals.

"
HADOOP-10748,HttpServer2 should not load JspServlet,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.
HADOOP-10747,Support configurable retries on SASL connection failures in RPC client.,"The RPC client includes a retry loop around SASL connection failures.  Currently, this is hard-coded to a maximum of 5 retries.  Let's make this configurable."
HADOOP-10746,TestSocketIOWithTimeout#testSocketIOWithTimeout fails on Power PC ,"SocketOutputStream closes its writer if it's partial written. But on PPC, after writing for some time, buf.capacity still equals buf.remaining. The reason might be what's written on PPC is buffered,so the buf.remaining will not change till a flush."
HADOOP-10744,LZ4 Compression fails to recognize PowerPC Little Endian Architecture,"Lz4 Compression fails to identify the PowerPC Little Endian Architecture. It recognizes it as Big Endian and several testcases( TestCompressorDecompressor, TestCodec, TestLz4CompressorDecompressor)  fails due to this.

Running org.apache.hadoop.io.compress.TestCompressorDecompressor
Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 0.435 sec <<< FAILURE! - in org.apache.hadoop.io.compress.TestCompressorDecompressor
testCompressorDecompressor(org.apache.hadoop.io.compress.TestCompressorDecompressor)  Time elapsed: 0.308 sec  <<< FAILURE!
org.junit.internal.ArrayComparisonFailure: org.apache.hadoop.io.compress.lz4.Lz4Compressor_org.apache.hadoop.io.compress.lz4.Lz4Decompressor-  byte arrays not equals error !!!: arrays first differed at element [1428]; expected:<4> but was:<10>
        at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)
        at org.junit.Assert.internalArrayEquals(Assert.java:473)
        at org.junit.Assert.assertArrayEquals(Assert.java:294)
        at org.apache.hadoop.io.compress.CompressDecompressTester$CompressionTestStrategy$2.assertCompression(CompressDecompressTester.java:325)
        at org.apache.hadoop.io.compress.CompressDecompressTester.test(CompressDecompressTester.java:135)
        at org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor(TestCompressorDecompressor.java:58)
.......
.......
....."
HADOOP-10739,Renaming a file into a directory containing the same filename results in a confusing I/O error,"Renaming a file to another existing filename says ""File
exists"" but colliding with a file in a directory results in the cryptic
""Input/output error""."
HADOOP-10737,"S3n silent failure on copy, data loss on rename","Jets3tNativeFileSystemStore.copy(String, String) handles its exceptions with handleServiceException(String, ServiceException), which behaves like:

1) Throw FileNotFoundException if the exception's error code is NoSuchKey
2) Otherwise, throw IOException if the exception's cause is an IOException
3) Otherwise, LOG.debug a message and throw nothing

So S3 exceptions other than NoSuchKey (like RequestTimeout, ServiceUnavailable) are suppressed. This makes ""copy"" fail while still returning as if it succeeded. Furthermore since NativeS3FileSystem's ""rename"" is implemented as a copy followed by a delete, this means ""rename"" can delete the source key even though the copy has failed."
HADOOP-10736,Add key attributes to the key shell,The recent work in HADOOP-10696 added attribute-value pairs to keys in the KMS.  Now the key shell needs to be updated to set/get these attributes.
HADOOP-10733,Potential null dereference in CredentialShell#promptForCredential(),"{code}
      char[] newPassword1 = c.readPassword(""Enter password: "");
      char[] newPassword2 = c.readPassword(""Enter password again: "");
      noMatch = !Arrays.equals(newPassword1, newPassword2);
      if (noMatch) {
        Arrays.fill(newPassword1, ' ');
{code}
newPassword1 might be null, leading to NullPointerException in Arrays.fill() call.
Similar issue for the following call on line 381:
{code}
      Arrays.fill(newPassword2, ' ');
{code}"
HADOOP-10732,Update without holding write lock in JavaKeyStoreProvider#innerSetCredential(),"In hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java,
innerSetCredential() doesn't wrap update with writeLock.lock() / writeLock.unlock()."
HADOOP-10731,Remove @date JavaDoc comment in ProgramDriver class,Remove JavaDoc @date in the ProgramDriver class for consistency.
HADOOP-10728,Metrics system for Windows Azure Storage Filesystem,"Add a metrics2 source for the Windows Azure Filesystem driver that was introduced with HADOOP-9629.

AzureFileSystemInstrumentation is the new MetricsSource.  

AzureNativeFilesystemStore and NativeAzureFilesystem have been modified to record statistics and some machinery is added for the accumulation of 'rolling average' statistics.

Primary new code appears in org.apache.hadoop.fs.azure.metrics namespace.

h2. Credits and history
Credit for this work goes to the early team: [~minwei], [~davidlao], [~lengningliu] and [~stojanovic] as well as multiple people who have taken over this work since then (hope I don't forget anyone): [~dexterb], Johannes Klein, [~ivanmi], Michael Rys, [~mostafae], [~brian_swan], [~mikelid], [~xifang], and [~chuanliu].
"
HADOOP-10720,KMS: Implement generateEncryptedKey and decryptEncryptedKey in the REST API,KMS client/server should implement support for generating encrypted keys and decrypting them via the REST API being introduced by HADOOP-10719.
HADOOP-10719,Add generateEncryptedKey and decryptEncryptedKey methods to KeyProvider,"This is a follow up on [HDFS-6134|https://issues.apache.org/jira/browse/HDFS-6134?focusedCommentId=14036044&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14036044]

KeyProvider API should  have 2 new methods:

* KeyVersion generateEncryptedKey(String keyVersionName, byte[] iv)
* KeyVersion decryptEncryptedKey(String keyVersionName, byte[] iv, KeyVersion encryptedKey)

The implementation would do a known transformation on the IV (i.e.: xor with 0xff the original IV).
"
HADOOP-10717,HttpServer2 should load jsp DTD from local jars instead of going remote,"When user want to start NameNode, user would got the following exception, it is caused by missing org.mortbay.jetty:jsp-2.1-jetty:jar:6.1.26 in the pom.xml

14/06/18 14:55:30 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
14/06/18 14:55:30 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
14/06/18 14:55:30 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
14/06/18 14:55:30 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
14/06/18 14:55:30 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
14/06/18 14:55:30 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
14/06/18 14:55:30 INFO http.HttpServer2: Jetty bound to port 50070
14/06/18 14:55:30 INFO mortbay.log: jetty-6.1.26
14/06/18 14:55:30 INFO mortbay.log: NO JSP Support for /, did not find org.apache.jasper.servlet.JspServlet
14/06/18 14:57:38 WARN mortbay.log: EXCEPTION
java.net.ConnectException: Connection timed out
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at java.net.Socket.connect(Socket.java:478)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:163)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:395)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:530)
        at sun.net.www.http.HttpClient.<init>(HttpClient.java:234)
        at sun.net.www.http.HttpClient.New(HttpClient.java:307)
        at sun.net.www.http.HttpClient.New(HttpClient.java:324)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:970)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:911)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:836)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1172)
        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:677)
        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.startEntity(XMLEntityManager.java:1315)
        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.startDTDEntity(XMLEntityManager.java:1282)
        at com.sun.org.apache.xerces.internal.impl.XMLDTDScannerImpl.setInputSource(XMLDTDScannerImpl.java:283)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$DTDDriver.dispatch(XMLDocumentScannerImpl.java:1194)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$DTDDriver.next(XMLDocumentScannerImpl.java:1090)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:1003)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:648)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:511)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:808)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
        at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)
        at javax.xml.parsers.SAXParser.parse(SAXParser.java:395)
        at org.mortbay.xml.XmlParser.parse(XmlParser.java:188)
        at org.mortbay.xml.XmlParser.parse(XmlParser.java:204)
        at org.mortbay.jetty.webapp.TagLibConfiguration.configureWebApp(TagLibConfiguration.java:238)
        at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1279)
        at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
        at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
        at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)
        at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)
        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
        at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)
        at org.mortbay.jetty.Server.doStart(Server.java:224)
        at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
        at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:795)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:142)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:690)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:581)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:748)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:732)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1386)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1452)
"
HADOOP-10716,Cannot use more than 1 har filesystem,"Filesystems are cached purely on scheme + authority.  Har filesystems actually need further differentiation based on path to the har file itself.  For this reason, the fs cache used to be explicitly disable for har via ""fs.har.impl.cache.disable"" in core-default.xml."
HADOOP-10715,Remove public GraphiteSink#setWriter(),"During review of HADOOP-10660, Ravi brought up the notion of making GraphiteSink#setWriter() private.

This JIRA is to address Ravi's comment."
HADOOP-10714,AmazonS3Client.deleteObjects() need to be limited to 1000 entries per call,"In the patch for HADOOP-10400, calls to AmazonS3Client.deleteObjects() need to have the number of entries at 1000 or below. Otherwise we get a Malformed XML error similar to:

com.amazonaws.services.s3.model.AmazonS3Exception: Status Code: 400, AWS Service: Amazon S3, AWS Request ID: 6626AD56A3C76F5B, AWS Error Code: MalformedXML, AWS Error Message: The XML you provided was not well-formed or did not validate against our published schema, S3 Extended Request ID: DOt6C+Y84mGSoDuaQTCo33893VaoKGEVC3y1k2zFIQRm+AJkFH2mTyrDgnykSL+v
at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:798)
at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:421)
at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)
at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)
at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3480)
at com.amazonaws.services.s3.AmazonS3Client.deleteObjects(AmazonS3Client.java:1739)
at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:388)
at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:829)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:874)
at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:878)

Note that this is mentioned in the AWS documentation:
http://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html

""The Multi-Object Delete request contains a list of up to 1000 keys that you want to delete. In the XML, you provide the object key names, and optionally, version IDs if you want to delete a specific version of the object from a versioning-enabled bucket. For each key, Amazon S3….”

Thanks to Matteo Bertozzi and Rahul Bhartia from AWS for identifying the problem."
HADOOP-10711,Cleanup some extra dependencies from hadoop-auth,"HADOOP-10322 added {{apacheds-kerberos-codec}} as a dependency, which brought in some additional dependencies.  
{noformat}
[INFO] \- org.apache.directory.server:apacheds-kerberos-codec:jar:2.0.0-M15:compile
[INFO]    +- org.apache.directory.server:apacheds-i18n:jar:2.0.0-M15:compile
[INFO]    +- org.apache.directory.api:api-asn1-api:jar:1.0.0-M20:compile
[INFO]    +- org.apache.directory.api:api-asn1-ber:jar:1.0.0-M20:compile
[INFO]    +- org.apache.directory.api:api-i18n:jar:1.0.0-M20:compile
[INFO]    +- org.apache.directory.api:api-ldap-model:jar:1.0.0-M20:compile
[INFO]    |  +- org.apache.mina:mina-core:jar:2.0.0-M5:compile
[INFO]    |  +- antlr:antlr:jar:2.7.7:compile
[INFO]    |  +- commons-lang:commons-lang:jar:2.6:compile
[INFO]    |  \- commons-collections:commons-collections:jar:3.2.1:compile
[INFO]    +- org.apache.directory.api:api-util:jar:1.0.0-M20:compile
[INFO]    \- net.sf.ehcache:ehcache-core:jar:2.4.4:compile
{noformat}
It looks like we don't need most of them."
HADOOP-10710,hadoop.auth cookie is not properly constructed according to RFC2109,"It seems that HADOOP-10379 introduced a bug on how hadoop.auth cookies are being constructed.

Before HADOOP-10379, cookies were constructed using Servlet's {{Cookie}} class and corresponding {{HttpServletResponse}} methods. This was taking care of setting attributes like 'Version=1' and double-quoting the cookie value if necessary.

HADOOP-10379 changed the Cookie creation to use a {{StringBuillder}} and setting values and attributes by hand. This is not taking care of setting required attributes like Version and escaping the cookie value.

While this is not breaking HadoopAuth {{AuthenticatedURL}} access, it is breaking access done using {{HtttpClient}}. I.e. Solr uses HttpClient and its access is broken since this change.

It seems that HADOOP-10379 main objective was to set the 'secure' attribute. Note this can be done using the {{Cookie}} API.

We should revert the cookie creation logic to use the {{Cookie}} API and take care of the security flag via {{setSecure(boolean)}}.
"
HADOOP-10703,HttpServer2 creates multiple authentication filters,"The HttpServer2.defineFilter creates a Filter instance for each context. By default, there are 3 contexts.
So there will be 3 separate AuthenticationFilter instances and corresponding AuthenticationHandler instances. This also results in 3 separate initializations of AuthenticationHandler.
The log file illustrating this repeated initialization is attached.
"
HADOOP-10702,KerberosAuthenticationHandler does not log the principal names correctly,"With HADOOP-10158, it is possible to load multiple principal names or all HTTP principals in the key tab by specifying “*”.
Each principal name is logged when when the principal is loaded from key tab. But there is a bug due to which principal name is logged each time  as either “*” or the full list of principals.

The log snippet is as below:

2014-06-15 00:19:13,288 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab, for principal *
2014-06-15 00:19:13,292 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab, for principal *
2014-06-15 00:19:13,294 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab, for principal *
2014-06-15 00:19:13,295 INFO org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: Login using keytab /etc/hadoop/hadoop.keytab, for principal *"
HADOOP-10701,NFS should not validate the access premission only based on the user's primary group,The bug is while accessing NFS Mounted File System the permission is always validated based on the primary Unix group the user is associated with and Secondary Unix groups are ignored.
HADOOP-10699,Fix build native library on mac osx,Some patches for fixing build a hadoop native library on os x 10.7/10.8.
HADOOP-10698,"KMS, add proxyuser support",Add proxyuser support to KMS as per discussion in HDFS-6134.
HADOOP-10696,Add optional attributes to KeyProvider Options and Metadata,"In addition to having an optional description, KeyProvider Options and Metadata should support optional key value pairs to help categorize keys.

This would be useful for visualization purposes."
HADOOP-10695,KMSClientProvider should respect a configurable timeout.,"It'd be good if KMSClientProvider used a timeout, so it doesn't hang forever if the KMServer is down."
HADOOP-10691,Improve the readability of 'hadoop fs -help',"'hadoop fs -help` displays help informations with numbers of different formats. 

This patch borrows the format used in `hdfs cacheadmin -help`: all options are formatted by using org.apache.hadoop.tools.TableListing."
HADOOP-10690,Lack of synchronization on access to InputStream in NativeAzureFileSystem#NativeAzureFsInputStream#close(),"{code}
    public void close() throws IOException {
      in.close();
    }
{code}
The close() method should be protected by synchronized keyword."
HADOOP-10689,InputStream is not closed in AzureNativeFileSystemStore#retrieve(),"In the catch block:
{code}
        if(in != null){
          inDataStream.close();
        }
{code}
We check against in but try to close inDataStream which should have been closed by the if statement above."
HADOOP-10688,Expose thread-level FileSystem StatisticsData,FileSystems collect data on the bytes read and written by each thread.  It would be helpful for it to make this data public.
HADOOP-10686,Writables are not always configured,"Seeing the following exception:
{noformat}
java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:403)
Caused by: java.lang.NullPointerException
	at org.apache.sqoop.job.io.SqoopWritable.readFields(SqoopWritable.java:59)
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:129)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.compare(MapTask.java:1248)
	at org.apache.hadoop.util.QuickSort.fix(QuickSort.java:35)
	at org.apache.hadoop.util.QuickSort.sortInternal(QuickSort.java:87)
	at org.apache.hadoop.util.QuickSort.sort(QuickSort.java:63)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1582)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1467)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:699)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:769)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:339)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

It turns out that WritableComparator does not configure Writable objects :https://github.com/apache/hadoop-common/blob/branch-2.3.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java. This is during the sort phase for an MR job."
HADOOP-10683,Users authenticated with KERBEROS are recorded as being authenticated with SIMPLE,"We have enabled kerberos authentication in our clusters, but we see the following in the log files 

2014-06-11 11:07:05,903 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for X@Y.COM (*auth:SIMPLE*)
2014-06-11 11:07:05,914 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for X@Y.COM (auth:KERBEROS) for protocol=interface 

This is quite confusing for administrators.
"
HADOOP-10681,Remove synchronized blocks from SnappyCodec and ZlibCodec buffering inner loop,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers.

!perf-top-spill-merge.png!

The bottleneck was found to be java monitor code inside SnappyCompressor.

The methods are neatly inlined by the JIT into the parent caller (BlockCompressorStream::write), which unfortunately does not flatten out the synchronized blocks.

!compress-cmpxchg-small.png!

The loop does a write of small byte[] buffers (each IFile key+value). 

I counted approximately 6 monitor enter/exit blocks per k-v pair written."
HADOOP-10678,SecurityUtil has unnecessary synchronization on collection used for only tests,"The function _SecurityUtil.getKerberosInfo()_  is a function used during authentication and authorization. 

It has two synchronized blocks and one of them is on testProviders. This is an unnecessary lock given that the testProviders is empty in real scenario."
HADOOP-10677,ExportSnapshot fails on kerberized cluster using s3a,"When using HBase ExportSnapshot on a kerberized cluster, exporting to s3a using HADOOP-10400, we see the following problem:

Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: patch283two
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:414)

The problem seems to be that the patch in HADOOP-10400 does not have getCanonicalServiceName()."
HADOOP-10676,S3AOutputStream not reading new config knobs for multipart configs,S3AOutputStream.java does not have the code to read the new config knobs for multipart configs.  This patch will add that.
HADOOP-10675,Add server-side encryption functionality to s3a,"The current patch for s3a in HADOOP-10400 does not have the capability to specify server-side encryption.  This JIRA will track the addition of such functionality to HADOOP-10400, similar to what was done in HADOOP-10568 for s3n."
HADOOP-10674,Rewrite the PureJavaCrc32 loop for performance improvement,"Below are some performance improvement opportunities performance improvement in PureJavaCrc32.
- eliminate ""off += 8; len -= 8;""
- replace T8_x_start with hard coded constants
- eliminate c0 - c7 local variables

In my machine, there are 30% to 50% improvement for most of the cases."
HADOOP-10673,Update rpc metrics when the call throws an exception,Currently RPC metrics isn't updated when the call throws an exception. We can either update the existing metrics or have a new set of metrics in the case of exception.
HADOOP-10670,Allow AuthenticationFilters to load secret from signature secret files,"In Hadoop web console, by using AuthenticationFilterInitializer, it's allowed to configure AuthenticationFilter for the required signature secret by specifying signature.secret.file property. This improvement would also allow this when AuthenticationFilterInitializer isn't used in situations like webhdfs."
HADOOP-10668,TestZKFailoverControllerStress#testExpireBackAndForth occasionally fails,"From https://builds.apache.org/job/PreCommit-HADOOP-Build/4018//testReport/org.apache.hadoop.ha/TestZKFailoverControllerStress/testExpireBackAndForth/ :
{code}
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode
	at org.apache.zookeeper.server.DataTree.getData(DataTree.java:648)
	at org.apache.zookeeper.server.ZKDatabase.getData(ZKDatabase.java:371)
	at org.apache.hadoop.ha.MiniZKFCCluster.expireActiveLockHolder(MiniZKFCCluster.java:199)
	at org.apache.hadoop.ha.MiniZKFCCluster.expireAndVerifyFailover(MiniZKFCCluster.java:234)
	at org.apache.hadoop.ha.TestZKFailoverControllerStress.testExpireBackAndForth(TestZKFailoverControllerStress.java:84)
{code}"
HADOOP-10666,Remove Copyright /d/d/d/d Apache Software Foundation from the source files license header,"Some of the files have ""Copyright 2011 Apache Software Foundation"" in the license header comment.

This is not the right license header per ASF rule [1]

From the ASF header rule [1] :
""Source File Headers for Code Developed at the ASF""
""This section refers only to works submitted directly to the ASF by the copyright owner or owner's agent.""

Seems like the one without Copyright notice is used for software developed directly by ASF which Apache Hadoop is.

When it was under external organization it does need to use the one from ASF 2.0 license [2] which requires Copyright included.

[1] http://www.apache.org/legal/src-headers.html
[2] http://www.apache.org/licenses/LICENSE-2.0
"
HADOOP-10665,Make Hadoop Authentication Handler loads case in-sensitive,"The authentication mechanism for http specified via _hadoop.http.authentication.type_ is matched in in a case sensitive way. If one specifies the type {simple,kerberos}  in the wrong case, the following exception is thrown . 

Caused by: java.lang.ClassNotFoundException: Simple
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:148)
	... 24 more


I believe , it is safe to ignore case during this comparison.



"
HADOOP-10664,TestNetUtils.testNormalizeHostName fails,"java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.net.TestNetUtils.testNormalizeHostName(TestNetUtils.java:617)"
HADOOP-10660,GraphiteSink should implement Closeable,"GraphiteSink wraps OutputStreamWriter around socket's output stream.
Currently the socket is never closed.

GraphiteSink should implement Closeable such that MetricsSystem can close the socket when it is stopped."
HADOOP-10659,Refactor AccessControlList to reuse utility functions and to improve performance,"Minor improvements can be done on _AccessControlList_.

Code Reusability:
_AccessControlList_ sanitizes the input list to remove duplicate entries, trim entries. _StringUtils.getTrimmedStringCollection_ can be used in this case.

Performance:
_AccessControlList_  uses TreeSet to maintain set of users and groups.
HashSet improves the performance slightly."
HADOOP-10658,SSLFactory expects truststores being configured,"The {{FileBasedKeyStoresFactory}} used by the {{SSLFactory}} expects truststores to be always present, if using CA signed certificates, they are not needed."
HADOOP-10657,Have RetryInvocationHandler log failover attempt at INFO level,"RetryInovcationHandler uses worthLogging flag to decide if it will do logging. worthLogging will be false for first fails over given invocationFailoverCount is zero. That addresses the log noise where the second-listed NN is active.

For other failover scenarios, it will be useful to log the error message at info level for analysis purpose."
HADOOP-10656,The password keystore file is not picked by LDAP group mapping,"The user configured password file(LDAP_KEYSTORE_PASSWORD_FILE_KEY) will not be picked by LdapGroupsMapping:

In setConf():

{noformat}
    keystorePass =
        conf.get(LDAP_KEYSTORE_PASSWORD_KEY, LDAP_KEYSTORE_PASSWORD_DEFAULT);
    if (keystorePass.isEmpty()) {
      keystorePass = extractPassword(
        conf.get(LDAP_KEYSTORE_PASSWORD_KEY, LDAP_KEYSTORE_PASSWORD_DEFAULT)); 
    }
{noformat}"
HADOOP-10652,Refactor Proxyusers to use AccessControlList  ,Currently Proxyuser specification  accepts a list of users and groups including wildcard values. Same functionality is already encapsulated in _AccessControlList_ . It will be better to refactor _ProxyUsers_ to use _AccessControlList_ instead of maintaining separate logic.
HADOOP-10651,Add ability to restrict service access using IP addresses and hostnames,"In some use cases, it make sense to authorize the usage of some services only from specific hosts. Just like ACLS for Service Authorization , there can be a list of hosts for each service and this list can be checked during authorization. 

Similar to ACLS, there can be a whitelist of ip and blacklist of ips. The default whitelist will be * and default blacklist will be empty. It should be possible to override the default whitelist and default blacklist. It should be possible to define whitelist and blacklist per service.
It should be possible to define ip ranges in blacklists and whitelists"
HADOOP-10650,Add ability to specify a reverse ACL (black list) of users and groups,"Currently , it is possible to define a ACL (user and groups) for a service. To temporarily remove authorization for a set of users, administrator needs to remove the users from the specific group and this may be a lengthy process ( update ldap groups, flush caches on machines).

 If there is a facility to define a reverse ACL for services, then administrator can disable users by specifying the users in reverse ACL. In other words, one can specify a whitelist of users and groups as well as a blacklist of users and groups. 

One can also specify a default blacklist to disable the users from accessing any service."
HADOOP-10649,Allow overriding the default ACL for service authorization ,"The default service authorization for a protocol is “*” and this authorizes everyone for the specific protocol.
It should be possible to override the default ACL and specify a different acl as the default ACL value.
"
HADOOP-10647,String Format Exception in SwiftNativeFileSystemStore.java,"If Swift.debug is given a string containing a % character, a format exception will occur. This happens when the path for any of the FileStatus objects contain a % encoded character. The bug is located at hadoop/src/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftNativeFileSystemStore.java:931.
"
HADOOP-10645,TestKMS fails because race condition writing acl files,"The {{TestKMS#testACLs()}} test randomly fails because a race condition while updating the acls files which is hot-reloaded.

We should disable the background thread that does the reload and do it manually for the purposes of the test."
HADOOP-10639,FileBasedKeyStoresFactory initialization is not using default for SSL_REQUIRE_CLIENT_CERT_KEY,The FileBasedKeyStoresFactory initialization is defaulting SSL_REQUIRE_CLIENT_CERT_KEY to true instead of the default DEFAULT_SSL_REQUIRE_CLIENT_CERT (false).
HADOOP-10638,Updating hadoop-daemon.sh to work as expected when nfs is started as a privileged user. ,"When NFS is started as a privileged user, this change sets up required environment variables:
HADOOP_PID_DIR = $HADOOP_PRIVILEGED_NFS_PID_DIR
HADOOP_LOG_DIR = $HADOOP_PRIVILEGED_NFS_LOG_DIR
HADOOP_IDENT_STRING = $HADOOP_PRIVILEGED_NFS_USER

Also, along with the above, we also now collect ulimits for the right user.
"
HADOOP-10635,Add a method to CryptoCodec to generate SRNs for IV,"SRN generators are provided by crypto libraries. the CryptoCodec gives access to a crypto library, thus it makes sense to expose the SRN generator on the CryptoCodec API."
HADOOP-10632,Minor improvements to Crypto input and output streams,Minor follow up feedback on the crypto streams
HADOOP-10630,Possible race condition in RetryInvocationHandler,"In one of our system tests with NameNode HA setup, we ran 300 threads in LoadGenerator. While one of the NameNodes was already in the active state and started to serve, we still saw one of the client thread failed all the retries in a 20 seconds window. In the meanwhile, we saw a lot of following warning msg in the log:
{noformat}
WARN retry.RetryInvocationHandler: A failover has occurred since the start of this method invocation attempt.
{noformat}

After checking the code, we see the following code in RetryInvocationHandler:
{code}
  while (true) {
      // The number of times this invocation handler has ever been failed over,
      // before this method invocation attempt. Used to prevent concurrent
      // failed method invocations from triggering multiple failover attempts.
      long invocationAttemptFailoverCount;
      synchronized (proxyProvider) {
        invocationAttemptFailoverCount = proxyProviderFailoverCount;
      }
      ......
      if (action.action == RetryAction.RetryDecision.FAILOVER_AND_RETRY) {
            // Make sure that concurrent failed method invocations only cause a
            // single actual fail over.
            synchronized (proxyProvider) {
              if (invocationAttemptFailoverCount == proxyProviderFailoverCount) {
                proxyProvider.performFailover(currentProxy.proxy);
                proxyProviderFailoverCount++;
                currentProxy = proxyProvider.getProxy();
              } else {
                LOG.warn(""A failover has occurred since the start of this method""
                    + "" invocation attempt."");
              }
            }
            invocationFailoverCount++;
          }
     ......
{code}

We can see we refresh the value of currentProxy only when the thread performs the failover (while holding the monitor of the proxyProvider). Because ""currentProxy"" is not volatile,  a thread that does not perform the failover (in which case it will log the warning msg) may fail to get the new value of currentProxy."
HADOOP-10626,Limit Returning Attributes for LDAP search,"When using Hadoop Ldap Group mappings in an enterprise environment, searching groups and returning all members can take a long time causing a timeout.  This causes not all groups to be returned for a user.  Because the first search only searches for the user dn and the second search retrieves the group member attribute, we only need to return the group member attribute on the search speeding up the search."
HADOOP-10625,Configuration: names should be trimmed when putting/getting to properties,"Currently, Hadoop will not trim name when putting a pair of k/v to property. But when loading configuration from file, names will be trimmed:
(In Configuration.java)
{code}
          if (""name"".equals(field.getTagName()) && field.hasChildNodes())
            attr = StringInterner.weakIntern(
                ((Text)field.getFirstChild()).getData().trim());
          if (""value"".equals(field.getTagName()) && field.hasChildNodes())
            value = StringInterner.weakIntern(
                ((Text)field.getFirstChild()).getData());
{code}
With this behavior, following steps will be problematic:
1. User incorrectly set "" hadoop.key=value"" (with a space before hadoop.key)
2. User try to get ""hadoop.key"", cannot get ""value""
3. Serialize/deserialize configuration (Like what did in MR)
4. User try to get ""hadoop.key"", can get ""value"", which will make inconsistency problem."
HADOOP-10622,Shell.runCommand can deadlock,Ran into a deadlock in Shell.runCommand.  Stacktrace details to follow.
HADOOP-10620,/docs/current doesn't point to the latest version 2.4.0,http://hadoop.apache.org/docs/current/ points to 2.3.0 while 2.4.0's out.
HADOOP-10618,Remove SingleNodeSetup.apt.vm,"http://hadoop.apache.org/docs/r2.4.0/hadoop-project-dist/hadoop-common/SingleNodeSetup.html is deprecated and not linked from the left side page.
We should remove the document and use http://hadoop.apache.org/docs/r2.4.0/hadoop-project-dist/hadoop-common/SingleCluster.html instead."
HADOOP-10614,CBZip2InputStream is not threadsafe,"Hadoop uses CBZip2InputStream to decode bzip2 files. However, the implementation is not threadsafe. This is not a really problem for Hadoop MapReduce because Hadoop runs each task in a separate JVM. But for other libraries that utilize multithreading and use Hadoop's InputFormat, e.g., Spark, it will cause exceptions like the following:

{code}
java.lang.ArrayIndexOutOfBoundsException: 6 org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.recvDecodingTables(CBZip2InputStream.java:729) org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:795) org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:499) org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:330) org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:394) org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:428) java.io.InputStream.read(InputStream.java:101) org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:205) org.apache.hadoop.util.LineReader.readLine(LineReader.java:169) org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:176) org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:43) org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:198) org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:181) org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:35) scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1000) org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:847) org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:847) org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1077) org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1077) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111) org.apache.spark.scheduler.Task.run(Task.scala:51) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:724)
{code}"
HADOOP-10612,NFS failed to refresh the user group id mapping table,Found by Preetham Kukillaya. The user/group id mapping table is not updated periodically.
HADOOP-10611,"KMS, keyVersion name should not be assumed to be keyName@versionNumber","Some KMS classes are assuming the keyVersion is keyName@versionNumber. 

The keyVersion should be handled as an opaque value.

"
HADOOP-10610,Upgrade S3n fs.s3.buffer.dir to support multi directories,"fs.s3.buffer.dir defines the tmp folder where files will be written to before getting sent to S3.  Right now this is limited to a single folder which causes to major issues.

1. You need a drive with enough space to store all the tmp files at once
2. You are limited to the IO speeds of a single drive

This solution will resolve both and has been tested to increase the S3 write speed by 2.5x with 10 mappers on hs1.

"
HADOOP-10609,.gitignore should ignore .orig and .rej files,.gitignore file should ignore .orig and .rej files
HADOOP-10607,Create an API to Separate Credentials/Password Storage from Applications,"As with the filesystem API, we need to provide a generic mechanism to support multiple credential storage mechanisms that are potentially from third parties. 

We need the ability to eliminate the storage of passwords and secrets in clear text within configuration files or within code.

Toward that end, I propose an API that is configured using a list of URLs of CredentialProviders. The implementation will look for implementations using the ServiceLoader interface and thus support third party libraries.

Two providers will be included in this patch. One using the credentials cache in MapReduce jobs and the other using Java KeyStores from either HDFS or local file system. 

A CredShell CLI will also be included in this patch which provides the ability to manage the credentials within the stores.

"
HADOOP-10602,"Documentation has broken ""Go Back"" hyperlinks.","Multiple pages of our documentation have ""Go Back"" links that are broken, because they point to an incorrect relative path."
HADOOP-10591,Compression codecs must used pooled direct buffers or deallocate direct buffers when stream is closed,"Currently direct buffers allocated by compression codecs like Gzip (which allocates 2 direct buffers per instance) are not deallocated when the stream is closed. Eventually for long running processes which create a huge number of files, these direct buffers are left hanging till a full gc, which may or may not happen in a reasonable amount of time - especially if the process does not use a whole lot of heap.

Either these buffers should be pooled or they should be deallocated when the stream is closed."
HADOOP-10590,ServiceAuthorizationManager  is not threadsafe,"The mutators in ServiceAuthorizationManager  are synchronized. The accessors are not synchronized.
This results in visibility issues when  ServiceAuthorizationManager's state is accessed from different threads.
"
HADOOP-10589,NativeS3FileSystem throw NullPointerException when the file is empty,"An empty file in the s3 path.

NativeS3FsInputStream dose not check the InputStream .


2014-05-06 20:29:26,961 INFO [main] org.apache.hadoop.hive.ql.exec.ReduceSinkOperator: 4 forwarded 0 rows
2014-05-06 20:29:26,961 INFO [main] org.apache.hadoop.hive.ql.exec.GroupByOperator: 3 Close done
2014-05-06 20:29:26,961 INFO [main] org.apache.hadoop.hive.ql.exec.SelectOperator: 2 Close done
2014-05-06 20:29:26,961 INFO [main] org.apache.hadoop.hive.ql.exec.FilterOperator: 1 Close done
2014-05-06 20:29:26,961 INFO [main] org.apache.hadoop.hive.ql.exec.TableScanOperator: 0 Close done
2014-05-06 20:29:26,961 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: 5 Close done
2014-05-06 20:29:26,961 INFO [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: ExecMapper: processed 0 rows: used memory = 602221488
2014-05-06 20:29:26,964 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.NullPointerException
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.close(NativeS3FileSystem.java:147)
	at java.io.BufferedInputStream.close(BufferedInputStream.java:472)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at org.apache.hadoop.util.LineReader.close(LineReader.java:150)
	at org.apache.hadoop.mapred.LineRecordReader.close(LineRecordReader.java:244)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doClose(CombineHiveRecordReader.java:72)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.close(HiveContextAwareRecordReader.java:96)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.close(HadoopShimsSecure.java:248)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.close(MapTask.java:209)
	at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:1950)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:445)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)

2014-05-06 20:29:26,970 INFO [main] org.apache.hadoop.mapred.Task: Runnning cleanup for the task
"
HADOOP-10588,Workaround for jetty6 acceptor startup issue,"When a cluster is restarted, jetty is not functioning for a small percentage of datanodes, requiring restart of those datanodes.  This is caused by JETTY-1316.

We've tried overriding isRunning() and retrying on super.isRunning() returning false, as the reporter of JETTY-1316 mentioned in the description.  It looks like the code was actually exercised (i.e. the issue was caused by this jetty bug)  and the acceptor was working fine after retry.

Since we will probably move to a later version of jetty after branch-3 is cut, we can put this workaround in branch-2 only."
HADOOP-10586,KeyShell doesn't allow setting Options via CLI,You should be able to set any of the Options passed to the KeyProvider via the CLI.
HADOOP-10585,Retry polices ignore interrupted exceptions,Retry polices should not use {{ThreadUtil.sleepAtLeastIgnoreInterrupts}}.  This prevents {{FsShell}} commands from being aborted during retries.  It also causes orphaned webhdfs DN DFSClients to keep running after the webhdfs client closes the connection.  Jetty goes into a loop constantly sending interrupts to the handler thread.  Webhdfs retries cause multiple nodes to have these orphaned clients.  The DN cannot shutdown until orphaned clients complete.
HADOOP-10583,bin/hadoop key throws NPE with no args and assorted other fixups,bin/hadoop key throws NPE.
HADOOP-10581,TestUserGroupInformation#testGetServerSideGroups fails because groups stored in Set and ArrayList are compared,"The test fails on some machines that has variety of user groups.
Initially the groups are extracted and stored in a set
{{Set<String> groups = new LinkedHashSet<String> ();}}
when the user groups are collected by calling the {{login.getGroupNames()}}, they are stored in an array list
{{String[] gi = login.getGroupNames();}}
Because these groups are stored in different structure, there will be inconsistency in the group count. Sets have unique list of keys while array list emits everything they have.
{{assertEquals(groups.size(), gi.length);}} fails when there are more than one groups with same name as the count in sets will be less than the arraylist."
HADOOP-10574,Bump the maven plugin versions too -moving the numbers into properties,"the maven build plugins are a bit dated -example, compiler plugin is at 2.5..1, when the latest is 3.1

# move the version definition from inline into properties
# increment the versions

This has no effect on downstream projects, so is low risk"
HADOOP-10572,Example NFS mount command must pass noacl as it isn't supported by the server yet,"Use of the documented default mount command results in the below server side log WARN event, cause the client tries to locate the ACL program (#100227):

{code}
12:26:11.975 AM	TRACE	org.apache.hadoop.oncrpc.RpcCall	
Xid:-1114380537, messageType:RPC_CALL, rpcVersion:2, program:100227, version:3, procedure:0, credential:(AuthFlavor:AUTH_NONE), verifier:(AuthFlavor:AUTH_NONE)
12:26:11.976 AM	TRACE	org.apache.hadoop.oncrpc.RpcProgram	
NFS3 procedure #0
12:26:11.976 AM	WARN	org.apache.hadoop.oncrpc.RpcProgram	
Invalid RPC call program 100227
{code}

The client mount command must pass {{noacl}} to avoid this."
HADOOP-10568,Add s3 server-side encryption,"Add s3 server-side encryption as described here:

http://docs.aws.amazon.com/AmazonS3/latest/dev/SSEUsingJavaSDK.html"
HADOOP-10566,Refactor proxyservers out of ProxyUsers,"HADOOP-10498 added proxyservers feature in ProxyUsers. It is beneficial to treat this as a separate feature since 

1> The ProxyUsers is per proxyuser where as proxyservers is per cluster. The cardinality is different. 

2> The ProxyUsers.authorize() and ProxyUsers.isproxyUser() are synchronized and hence share the same lock  and impacts performance.

Since these are two separate features, it will be an improvement to keep them separate. It also enables one to fine-tune each feature independently.
"
HADOOP-10565,Support IP ranges (CIDR) in  proxyuser.hosts,"In some use cases, there will be many hosts from which the user can impersonate. 
This requires specifying many ips  in the XML configuration. 
It is cumbersome to specify and maintain long list of ips in proxyuser.hosts
The problem can be solved if we enable proxyuser.hosts to accept ip ranges in CIDR format.

In addition, the current ip authorization involve a liner scan of the ips and an attempt to do InetAddress.getByName()  for each ip/host. 

It may be beneficial to group this functionality of ip authorization by looking up  ""ip addresses/host names/ip-ranges"" into a separate class. This could be reused in other usecases which require similar functionality"
HADOOP-10563,Remove the dependency of jsp in trunk,"After HDFS-6252 neither hdfs nor yarn uses jsp, thus the dependency of the jsp can be removed from the pom."
HADOOP-10562,Namenode exits on exception without printing stack trace in AbstractDelegationTokenSecretManager,Not printing the stack trace makes debugging harder.
HADOOP-10561,Copy command with preserve option should handle Xattrs,"The design docs for Xattrs stated that we handle preserve options with copy commands

From doc:
Preserve option of commands like “cp -p” shell command and “distcp -p” should work on XAttrs. 
In the case of source fs supports XAttrs but target fs does not support, exception about XAttrs not supported will be thrown."
HADOOP-10557,FsShell -cp -pa option for preserving extended ACLs,This issue tracks enhancing FsShell cp to add a new command-line option (-pa) for preserving extended ACLs.
HADOOP-10556,Add toLowerCase support to auth_to_local rules for service name,"When using Vintela to integrate Linux with AD, principals are lowercased. If the accounts in AD have uppercase characters (ie FooBar) the Kerberos principals have also uppercase characters (ie FooBar/<HOST>). Because of this, when a service (Yarn/HDFS) extracts the service name from the Kerberos principal (FooBar) and uses it for obtain groups the user is not found because via Linux the user FooBar is unknown, it has been converted to foobar.
"
HADOOP-10552,Fix usage and example at FileSystemShell.apt.vm,"Usage at moveFromLocal needs ""hdfs"" command, and example for touchz should use ""hdfs dfs"".
"
HADOOP-10549,MAX_SUBST and varPat should be final in Configuration.java,"In Configuration, expansion of variables is handled using the following constants that are not declared final:

{code}
  private static Pattern varPat = Pattern.compile(""\\$\\{[^\\}\\$\u0020]+\\}"");
  private static int MAX_SUBST = 20;
{code}

"
HADOOP-10547,Give SaslPropertiesResolver.getDefaultProperties() public scope,Trying to use SaslPropertiesResolver.getDefaultProperties() in Hive project but the method has protected scope. Please make this a public method if appropriate.
HADOOP-10543,RemoteException's unwrapRemoteException method failed for PathIOException,"If the cause of a RemoteException is PathIOException, RemoteException's unwrapRemoteException methods would fail, because some PathIOException constructors initialize the cause to null, which makes Throwable to throw exception at
{code}
    public synchronized Throwable initCause(Throwable cause) {
        if (this.cause != this)
            throw new IllegalStateException(""Can't overwrite cause"");
{code} 
"
HADOOP-10542,Potential null pointer dereference in Jets3tFileSystemStore#retrieveBlock(),"{code}
      in = get(blockToKey(block), byteRangeStart);
      out = new BufferedOutputStream(new FileOutputStream(fileBlock));
      byte[] buf = new byte[bufferSize];
      int numRead;
      while ((numRead = in.read(buf)) >= 0) {
{code}
get() may return null.
The while loop dereferences in without null check."
HADOOP-10541,InputStream in MiniKdc#initKDCServer for minikdc.ldiff is not closed,"The same InputStream variable is used for minikdc.ldiff and minikdc-krb5.conf :
{code}
    InputStream is = cl.getResourceAsStream(""minikdc.ldiff"");
...
    is = cl.getResourceAsStream(""minikdc-krb5.conf"");
{code}
Before the second assignment, is should be closed."
HADOOP-10540,Datanode upgrade in Windows fails with hardlink error.,"I try to upgrade Hadoop from 1.x and 2.4, but DataNode failed to start due to hard link exception.
Repro steps:
*Installed Hadoop 1.x
*hadoop dfsadmin -safemode enter
*hadoop dfsadmin -saveNamespace
*hadoop namenode -finalize
*Stop all services
*Uninstall Hadoop 1.x 
*Install Hadoop 2.4 
*Start namenode with -upgrade option
*Try to start datanode, begin to see Hardlink exception in datanode service log.

{code}

2014-04-10 22:47:11,655 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8010: starting
2014-04-10 22:47:11,656 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2014-04-10 22:47:11,999 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -56
2014-04-10 22:47:12,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on d:\hadoop\data\hdfs\dn\in_use.lock acquired by nodename 7268@myhost
2014-04-10 22:47:12,011 INFO org.apache.hadoop.hdfs.server.common.Storage: Recovering storage directory D:\hadoop\data\hdfs\dn from previous upgrade
2014-04-10 22:47:12,017 INFO org.apache.hadoop.hdfs.server.common.Storage: Upgrading storage directory d:\hadoop\data\hdfs\dn.
   old LV = -44; old CTime = 0.
   new LV = -55; new CTime = 1397168400373
2014-04-10 22:47:12,021 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-39008719-10.0.0.1-1397168400092 directory d:\hadoop\data\hdfs\dn\current\BP-39008719-10.0.0.1-1397168400092\current
2014-04-10 22:47:12,254 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (Datanode Uuid unassigned) service to myhost/10.0.0.1:8020
java.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.
	at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)
	at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:815)
	at java.lang.Thread.run(Thread.java:722)
2014-04-10 22:47:12,258 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to myhost/10.0.0.1:8020
2014-04-10 22:47:12,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN
java.lang.Exception: trace
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:91)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:859)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)
	at java.lang.Thread.run(Thread.java:722)
2014-04-10 22:47:12,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2014-04-10 22:47:12,360 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN
java.lang.Exception: trace
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:861)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)
	at java.lang.Thread.run(Thread.java:722)
2014-04-10 22:47:14,360 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2014-04-10 22:47:14,361 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2014-04-10 22:47:14,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at myhost/10.0.0.1
************************************************************/
{code}"
HADOOP-10539,Provide backward compatibility for ProxyUsers.authorize() call,"HADOOP-10499  removed the unused _Configuration_ parameter from _ProxyUsers.authorize()_  method. 
This broke few components like HBase who invoked that function and forced it to rely on Reflection.

"
HADOOP-10535,Make the retry numbers in ActiveStandbyElector configurable,"Currently in ActiveStandbyElector, when its zookeeper client cannot successfully communicate with ZooKeeper service, the retry number is hard coded to 3 (ActiveStandbyElector#NUM_RETRIES). After retrying 3 times a fatal error will be thrown and the ZKFC will quit. It will be better to make the retry times configurable."
HADOOP-10534,KeyProvider API should using windowing for retrieving metadata,"The KeyProvider API should refrain from bulk operations that fetch information about all keys. in HADOOP-10430, we settled on providing an accessor that given a set of key names returns a list of metadata."
HADOOP-10533,S3 input stream NPEs in MapReduce job,"I'm running a wordcount MR as follows

hadoop jar WordCount.jar wordcount.WordCountDriver s3n://bucket/wordcount/input s3n://bucket/wordcount/output
 
s3n://bucket/wordcount/input is a s3 object that contains other input files.

However I get following NPE error

12/10/02 18:56:23 INFO mapred.JobClient:  map 0% reduce 0%
12/10/02 18:56:54 INFO mapred.JobClient:  map 50% reduce 0%
        12/10/02 18:56:56 INFO mapred.JobClient: Task Id : attempt_201210021853_0001_m_000001_0, Status : FAILED
java.lang.NullPointerException
        at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.close(NativeS3FileSystem.java:106)
        at java.io.BufferedInputStream.close(BufferedInputStream.java:451)
        at java.io.FilterInputStream.close(FilterInputStream.java:155)
        at org.apache.hadoop.util.LineReader.close(LineReader.java:83)
        at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.close(LineRecordReader.java:144)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.close(MapTask.java:497)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:765)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)

MR runs fine if i specify more specific input path such as s3n://bucket/wordcount/input/file.txt

MR fails if I pass s3 folder as a parameter


In summary,
This works
 hadoop jar ./hadoop-examples-1.0.3.jar wordcount /user/hadoop/wordcount/input/ s3n://bucket/wordcount/output/

This doesn't work
 hadoop jar ./hadoop-examples-1.0.3.jar wordcount s3n://bucket/wordcount/input/ s3n://bucket/wordcount/output/

(both input path are directories)

"
HADOOP-10531,hadoop-config.sh - bug in --hosts argument,"I think there's a typo in the hadoop-config.sh which broke slave start in cluster mode.

To reproduce it :

/usr/local/hadoop/sbin/hadoop-daemons.sh --config /etc/hadoop/conf --hosts slaves start datanode

here's the patch :

--- hadoop-dist/target/hadoop-2.4.0/libexec/hadoop-config.sh    2014-04-22 12:20:45.000000000 -0400
+++ hadoop-dist/target/hadoop-2.4.0/libexec/hadoop-config.sh    2014-04-22 12:21:00.000000000 -0400
@@ -93,7 +93,7 @@
     if [ ""--hosts"" = ""$1"" ]
     then
         shift
-        export HADOOP_SLAVES=""${HADOOP_CONF_DIR}/$$1""
+        export HADOOP_SLAVES=""${HADOOP_CONF_DIR}/$1""
         shift
     elif [ ""--hostnames"" = ""$1"" ]
     then
"
HADOOP-10530,Make hadoop trunk build on Java7+ only,"As discussed on hadoop-common, hadoop 3 is envisaged to be Java7+ *only* -this JIRA covers switching the build for this

# maven enforcer plugin to set Java version = {{[1.7)}}
# compiler to set language to java 1.7"
HADOOP-10527,Fix incorrect return code and allow more retries on EINTR,"After HADOOP-10522, user/group look-up will only try up to 5 times on EINTR.  More retries should be allowed just in case.

Also, when a user/group lookup returns no entries, the wrapper methods are returning EIO, instead of ENOENT."
HADOOP-10526,Chance for Stream leakage in CompressorStream,"In CompressorStream.close , finish() can throw IOException . But out will not be closed in that situation since it is not in finally "
HADOOP-10525,Remove DRFA.MaxBackupIndex config from log4j.properties,"From [hadoop-user mailing list|http://mail-archives.apache.org/mod_mbox/hadoop-user/201404.mbox/%3C534FACD3.8040907%40corp.badoo.com%3E].
{code}
# 30-day backup
# log4j.appender.DRFA.MaxBackupIndex=30
{code}
In {{log4j.properties}}, the above lines should be removed because DailyRollingFileAppender(DRFA) doesn't support MaxBackupIndex config."
HADOOP-10522,JniBasedUnixGroupMapping mishandles errors,"The mishandling of errors in the jni user-to-groups mapping modules can cause segmentation faults in subsequent calls.  Here are the bugs:

1) If {{hadoop_user_info_fetch()}} returns an error code that is not ENOENT, the error may not be handled at all.  This bug was found by [~cnauroth].

2)  In {{hadoop_user_info_fetch()}} and {{hadoop_group_info_fetch()}}, the global {{errno}} is directly used. This is not thread-safe and could be the cause of some failures that disappeared after enabling the big lookup lock.

3) In the above methods, there is no limit on retries."
HADOOP-10517,InputStream is not closed in two methods of JarFinder,"JarFinder#jarDir() and JarFinder#zipDir() have such code:
{code}
                 InputStream is = new FileInputStream(f);
                 copyToZipStream(is, anEntry, zos);
{code}
The InputStream is closed in copyToZipStream() but should be enclosed in finally block."
HADOOP-10514,Common side changes to support  HDFS extended attributes (HDFS-2006),This is an umbrella issue for tracking all Hadoop Common changes required to support HDFS extended attributes implementation
HADOOP-10508,RefreshCallQueue fails when authorization is enabled,"When hadoop.security.authorization=true, the callqueue cannot be refreshed with hadoop dfsadmin -refreshCallQueue (protocol not found). 

"
HADOOP-10507,FsShell setfacl can throw ArrayIndexOutOfBoundsException when no perm is specified,"If users don't specify the perm of an acl when using the FsShell's setfacl command, a fatal internal error ArrayIndexOutOfBoundsException will be thrown.

{code}
[root@hdfs-nfs ~]# hdfs dfs -setfacl -m user:bob: /user/hdfs/td1
-setfacl: Fatal internal error
java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.hadoop.fs.permission.AclEntry.parseAclEntry(AclEntry.java:285)
	at org.apache.hadoop.fs.permission.AclEntry.parseAclSpec(AclEntry.java:221)
	at org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand.processOptions(AclCommands.java:260)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:255)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:308)
[root@hdfs-nfs ~]# 
{code}

An improvement would be if it returned something like this:

{code}
[root@hdfs-nfs ~]# hdfs dfs -setfacl -m user:bob:rww /user/hdfs/td1
-setfacl: Invalid permission in <aclSpec> : user:bob:rww
Usage: hadoop fs [generic options] -setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]
[root@hdfs-nfs ~]# 
{code}"
HADOOP-10503,Move junit up to v 4.11,"JUnit 4.11 has been out for a while; other projects are happy with it, so update it."
HADOOP-10500,TestDoAsEffectiveUser fails on JDK7 due to failure to reset proxy user configuration.,"The proxy user configuration is held in a static variable.  Various tests in {{TestDoAsEffectiveUser}} mutate this data.  JDK7 executes tests in a different order than JDK6, so this can cause a failure if the state hasn't been set explicitly to what the test needs."
HADOOP-10499,Remove unused parameter from ProxyUsers.authorize(),"The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated.

Attaching the simple patch which removes the unused _conf_ parameter and updates the callers.
The ProxyUsers is defined as a private audience and so there shouldn't be any external callers. 
"
HADOOP-10498,Add support for proxy server,HDFS-6218 & HDFS-6219 require support for configurable proxy servers.
HADOOP-10496,Metrics system FileSink can leak file descriptor.,"{{FileSink}} opens a file.  If the {{MetricsSystem}} is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak."
HADOOP-10495,TestFileUtil fails on Windows due to bad permission assertions.,"{{TestFileUtil}} contains some assertions that check {{java.io.File#canRead}}.  These assertions fail on Windows.  The {{java.io.File}} methods for checking permissions are known to be buggy in JDK 6.  We can replace these calls with our own custom {{FileUtil#canRead}}, which is correct on all platforms."
HADOOP-10490,TestMapFile and TestBloomMapFile leak file descriptors.,"Multiple tests in {{TestMapFile}} and {{TestBloomMapFile}} open files but don't close them.  On Windows, the leaked file descriptors cause subsequent tests to fail, because file locks are still held while trying to delete the test data directory."
HADOOP-10489,UserGroupInformation#getTokens and UserGroupInformation#addToken can lead to ConcurrentModificationException,"Currently UserGroupInformation#getTokens and UserGroupInformation#addToken uses UGI's monitor to protect the iteration and modification of Credentials#tokenMap. Per [discussion|https://issues.apache.org/jira/browse/HADOOP-10475?focusedCommentId=13965851&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13965851] in HADOOP-10475, this can still lead to ConcurrentModificationException."
HADOOP-10488,TestKeyProviderFactory fails randomly,"This test is fail randomly depending on the order of execution of the test methods, the reason is that the keystore used by the different testmethods is the same. We should either delete it before/after each test, or we should use a diff diff or each run."
HADOOP-10482,Fix various findbugs warnings in hadoop-common,"The following findbugs warnings need to be fixed:

{noformat}
[INFO] --- findbugs-maven-plugin:2.5.3:check (default-cli) @ hadoop-common ---
[INFO] BugInstance size is 97
[INFO] Error size is 0
[INFO] Total bugs: 97
[INFO] Found reliance on default encoding in org.apache.hadoop.conf.Configuration.getConfResourceAsReader(String): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.conf.Configuration""] At Configuration.java:[lines 169-2642]
[INFO] Null passed for nonnull parameter of set(String, String) in org.apache.hadoop.conf.Configuration.setPattern(String, Pattern) [""org.apache.hadoop.conf.Configuration""] At Configuration.java:[lines 169-2642]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.conf.ReconfigurationServlet.printHeader(PrintWriter, String) [""org.apache.hadoop.conf.ReconfigurationServlet""] At ReconfigurationServlet.java:[lines 44-234]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.conf.ReconfigurationServlet.printHeader(PrintWriter, String) [""org.apache.hadoop.conf.ReconfigurationServlet""] At ReconfigurationServlet.java:[lines 44-234]
[INFO] Found reliance on default encoding in new org.apache.hadoop.crypto.key.KeyProvider$Metadata(byte[]): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.crypto.key.KeyProvider$Metadata""] At KeyProvider.java:[lines 110-204]
[INFO] Found reliance on default encoding in org.apache.hadoop.crypto.key.KeyProvider$Metadata.serialize(): new java.io.OutputStreamWriter(OutputStream) [""org.apache.hadoop.crypto.key.KeyProvider$Metadata""] At KeyProvider.java:[lines 110-204]
[INFO] Redundant nullcheck of clazz, which is known to be non-null in org.apache.hadoop.fs.FileSystem.createFileSystem(URI, Configuration) [""org.apache.hadoop.fs.FileSystem""] At FileSystem.java:[lines 89-3017]
[INFO] Unread public/protected field: org.apache.hadoop.fs.HarFileSystem$Store.endHash [""org.apache.hadoop.fs.HarFileSystem$Store""] At HarFileSystem.java:[lines 492-500]
[INFO] Unread public/protected field: org.apache.hadoop.fs.HarFileSystem$Store.startHash [""org.apache.hadoop.fs.HarFileSystem$Store""] At HarFileSystem.java:[lines 492-500]
[INFO] Found reliance on default encoding in org.apache.hadoop.fs.HardLink.createHardLink(File, File): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.fs.HardLink""] At HardLink.java:[lines 51-546]
[INFO] Found reliance on default encoding in org.apache.hadoop.fs.HardLink.createHardLinkMult(File, String[], File, int): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.fs.HardLink""] At HardLink.java:[lines 51-546]
[INFO] Found reliance on default encoding in org.apache.hadoop.fs.HardLink.getLinkCount(File): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.fs.HardLink""] At HardLink.java:[lines 51-546]
[INFO] Bad attempt to compute absolute value of signed random integer in org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(String, long, Configuration, boolean) [""org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext""] At LocalDirAllocator.java:[lines 247-549]
[INFO] Null passed for nonnull parameter of org.apache.hadoop.conf.Configuration.set(String, String) in org.apache.hadoop.fs.ftp.FTPFileSystem.initialize(URI, Configuration) [""org.apache.hadoop.fs.ftp.FTPFileSystem""] At FTPFileSystem.java:[lines 51-593]
[INFO] Redundant nullcheck of dirEntries, which is known to be non-null in org.apache.hadoop.fs.ftp.FTPFileSystem.delete(FTPClient, Path, boolean) [""org.apache.hadoop.fs.ftp.FTPFileSystem""] At FTPFileSystem.java:[lines 51-593]
[INFO] Redundant nullcheck of org.apache.hadoop.fs.ftp.FTPFileSystem.getFileStatus(FTPClient, Path), which is known to be non-null in org.apache.hadoop.fs.ftp.FTPFileSystem.exists(FTPClient, Path) [""org.apache.hadoop.fs.ftp.FTPFileSystem""] At FTPFileSystem.java:[lines 51-593]
[INFO] Found reliance on default encoding in org.apache.hadoop.fs.shell.Display$AvroFileInputStream.read(): String.getBytes() [""org.apache.hadoop.fs.shell.Display$AvroFileInputStream""] At Display.java:[lines 259-309]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.fs.shell.Display$Checksum.processPath(PathData) [""org.apache.hadoop.fs.shell.Display$Checksum""] At Display.java:[lines 169-196]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.fs.shell.Display$Checksum.processPath(PathData) [""org.apache.hadoop.fs.shell.Display$Checksum""] At Display.java:[lines 169-196]
[INFO] Found reliance on default encoding in org.apache.hadoop.fs.shell.Display$TextRecordInputStream.read(): String.getBytes() [""org.apache.hadoop.fs.shell.Display$TextRecordInputStream""] At Display.java:[lines 207-244]
[INFO] Call to method of static java.text.DateFormat in org.apache.hadoop.fs.shell.Ls.processPath(PathData) [""org.apache.hadoop.fs.shell.Ls""] At Ls.java:[lines 45-207]
[INFO] org.apache.hadoop.fs.shell.Ls.dateFormat is a static field of type java.text.DateFormat, which isn't thread safe [""org.apache.hadoop.fs.shell.Ls""] At Ls.java:[lines 45-207]
[INFO] Call to method of static java.text.DateFormat in org.apache.hadoop.fs.shell.Stat.processPath(PathData) [""org.apache.hadoop.fs.shell.Stat""] At Stat.java:[lines 46-128]
[INFO] org.apache.hadoop.fs.shell.Stat.timeFmt is a static field of type java.text.DateFormat, which isn't thread safe [""org.apache.hadoop.fs.shell.Stat""] At Stat.java:[lines 46-128]
[INFO] Switch statement found in org.apache.hadoop.fs.shell.Test.processPath(PathData) where default case is missing [""org.apache.hadoop.fs.shell.Test""] At Test.java:[lines 33-94]
[INFO] return value of java.util.concurrent.CountDownLatch.await(long, TimeUnit) ignored in org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef.process(WatchedEvent) [""org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef""] At ActiveStandbyElector.java:[lines 1017-1074]
[INFO] Switch statement found in org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter.log(int, String) where default case is missing [""org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter""] At SshFenceByTcpPort.java:[lines 273-314]
[INFO] Found reliance on default encoding in org.apache.hadoop.ha.StreamPumper.pump(): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.ha.StreamPumper""] At StreamPumper.java:[lines 32-89]
[INFO] Found reliance on default encoding in org.apache.hadoop.http.HtmlQuoting.<static initializer for HtmlQuoting>(): String.getBytes() [""org.apache.hadoop.http.HtmlQuoting""] At HtmlQuoting.java:[lines 27-206]
[INFO] Found reliance on default encoding in org.apache.hadoop.http.HtmlQuoting.needsQuoting(String): String.getBytes() [""org.apache.hadoop.http.HtmlQuoting""] At HtmlQuoting.java:[lines 27-206]
[INFO] Found reliance on default encoding in org.apache.hadoop.http.HtmlQuoting.quoteHtmlChars(String): java.io.ByteArrayOutputStream.toString() [""org.apache.hadoop.http.HtmlQuoting""] At HtmlQuoting.java:[lines 27-206]
[INFO] Found reliance on default encoding in org.apache.hadoop.http.HtmlQuoting.quoteHtmlChars(String): String.getBytes() [""org.apache.hadoop.http.HtmlQuoting""] At HtmlQuoting.java:[lines 27-206]
[INFO] Found reliance on default encoding in org.apache.hadoop.io.DefaultStringifier.toString(Object): new String(byte[]) [""org.apache.hadoop.io.DefaultStringifier""] At DefaultStringifier.java:[lines 60-202]
[INFO] org.apache.hadoop.io.LongWritable$DecreasingComparator.compare(WritableComparable, WritableComparable) negates the return value of org.apache.hadoop.io.WritableComparator.compare(WritableComparable, WritableComparable) [""org.apache.hadoop.io.LongWritable$DecreasingComparator""] At LongWritable.java:[lines 98-106]
[INFO] org.apache.hadoop.io.LongWritable$DecreasingComparator.compare(byte[], int, int, byte[], int, int) negates the return value of org.apache.hadoop.io.LongWritable$Comparator.compare(byte[], int, int, byte[], int, int) [""org.apache.hadoop.io.LongWritable$DecreasingComparator""] At LongWritable.java:[lines 98-106]
[INFO] Found reliance on default encoding in new org.apache.hadoop.io.SequenceFile$Writer(Configuration, SequenceFile$Writer$Option[]): String.getBytes() [""org.apache.hadoop.io.SequenceFile$Writer""] At SequenceFile.java:[lines 822-1361]
[INFO] Found reliance on default encoding in new org.apache.hadoop.io.SequenceFile$Writer(FileSystem, Configuration, Path, Class, Class): String.getBytes() [""org.apache.hadoop.io.SequenceFile$Writer""] At SequenceFile.java:[lines 822-1361]
[INFO] Found reliance on default encoding in new org.apache.hadoop.io.SequenceFile$Writer(FileSystem, Configuration, Path, Class, Class, int, short, long, Progressable, SequenceFile$Metadata): String.getBytes() [""org.apache.hadoop.io.SequenceFile$Writer""] At SequenceFile.java:[lines 822-1361]
[INFO] Found reliance on default encoding in new org.apache.hadoop.io.SequenceFile$Writer(FileSystem, Configuration, Path, Class, Class, Progressable, SequenceFile$Metadata): String.getBytes() [""org.apache.hadoop.io.SequenceFile$Writer""] At SequenceFile.java:[lines 822-1361]
[INFO] Switch statement found in org.apache.hadoop.io.Text.bytesToCodePoint(ByteBuffer) where default case is missing [""org.apache.hadoop.io.Text""] At Text.java:[lines 56-672]
[INFO] Switch statement found in org.apache.hadoop.io.Text.validateUTF8(byte[], int, int) where default case is missing [""org.apache.hadoop.io.Text""] At Text.java:[lines 56-672]
[INFO] Found reliance on default encoding in org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.readStreamHeader(): new String(byte[]) [""org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream""] At BZip2Codec.java:[lines 361-539]
[INFO] Found reliance on default encoding in org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream.writeStreamHeader(): String.getBytes() [""org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream""] At BZip2Codec.java:[lines 273-335]
[INFO] Write to static field org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.skipDecompression from instance method org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(byte[], int, int) [""org.apache.hadoop.io.compress.bzip2.CBZip2InputStream""] At CBZip2InputStream.java:[lines 82-1173]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.io.file.tfile.TFile.main(String[]) [""org.apache.hadoop.io.file.tfile.TFile""] At TFile.java:[lines 134-2361]
[INFO] Found reliance on default encoding in org.apache.hadoop.io.file.tfile.TFileDumper.dumpInfo(String, PrintStream, Configuration): new String(byte[], int, int) [""org.apache.hadoop.io.file.tfile.TFileDumper""] At TFileDumper.java:[lines 43-294]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.io.file.tfile.TFileDumper.dumpInfo(String, PrintStream, Configuration) [""org.apache.hadoop.io.file.tfile.TFileDumper""] At TFileDumper.java:[lines 43-294]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.io.file.tfile.TFileDumper.dumpInfo(String, PrintStream, Configuration) [""org.apache.hadoop.io.file.tfile.TFileDumper""] At TFileDumper.java:[lines 43-294]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.io.file.tfile.TFileDumper.dumpInfo(String, PrintStream, Configuration) [""org.apache.hadoop.io.file.tfile.TFileDumper""] At TFileDumper.java:[lines 43-294]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.io.file.tfile.TFileDumper.dumpInfo(String, PrintStream, Configuration) [""org.apache.hadoop.io.file.tfile.TFileDumper""] At TFileDumper.java:[lines 43-294]
[INFO] Found reliance on default encoding in org.apache.hadoop.ipc.RpcConstants.<static initializer for RpcConstants>(): String.getBytes() [""org.apache.hadoop.ipc.RpcConstants""] At RpcConstants.java:[lines 26-56]
[INFO] Found reliance on default encoding in org.apache.hadoop.ipc.Server.<static initializer for Server>(): String.getBytes() [""org.apache.hadoop.ipc.Server""] At Server.java:[lines 133-2685]
[INFO] org.apache.hadoop.ipc.Server$Connection.close() might ignore java.lang.Exception [""org.apache.hadoop.ipc.Server$Connection"", ""java.lang.Exception""] At Server.java:[lines 1101-2053]At Exception.java:[lines 54-123]
[INFO] Found reliance on default encoding in org.apache.hadoop.ipc.Server$Connection.setupHttpRequestOnIpcPortResponse(): String.getBytes() [""org.apache.hadoop.ipc.Server$Connection""] At Server.java:[lines 1101-2053]
[INFO] Increment of volatile field org.apache.hadoop.ipc.Server$Connection.rpcCount in org.apache.hadoop.ipc.Server$Connection.decRpcCount() [""org.apache.hadoop.ipc.Server$Connection""] At Server.java:[lines 1101-2053]
[INFO] Increment of volatile field org.apache.hadoop.ipc.Server$Connection.rpcCount in org.apache.hadoop.ipc.Server$Connection.incRpcCount() [""org.apache.hadoop.ipc.Server$Connection""] At Server.java:[lines 1101-2053]
[INFO] HTTP parameter written to Servlet output in org.apache.hadoop.jmx.JMXJsonServlet.doGet(HttpServletRequest, HttpServletResponse) [""org.apache.hadoop.jmx.JMXJsonServlet""] At JMXJsonServlet.java:[lines 120-422]
[INFO] Found reliance on default encoding in org.apache.hadoop.log.LogLevel.process(String): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.log.LogLevel""] At LogLevel.java:[lines 38-86]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics.file.FileContext.startMonitoring(): new java.io.FileWriter(File, boolean) [""org.apache.hadoop.metrics.file.FileContext""] At FileContext.java:[lines 58-158]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics.file.FileContext.startMonitoring(): new java.io.PrintWriter(OutputStream) [""org.apache.hadoop.metrics.file.FileContext""] At FileContext.java:[lines 58-158]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics.ganglia.GangliaContext.xdr_string(String): String.getBytes() [""org.apache.hadoop.metrics.ganglia.GangliaContext""] At GangliaContext.java:[lines 64-254]
[INFO] Redundant nullcheck of units, which is known to be non-null in org.apache.hadoop.metrics.ganglia.GangliaContext31.emitMetric(String, String, String) [""org.apache.hadoop.metrics.ganglia.GangliaContext31""] At GangliaContext31.java:[lines 39-144]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics2.impl.MetricsConfig.toString(Configuration): java.io.ByteArrayOutputStream.toString() [""org.apache.hadoop.metrics2.impl.MetricsConfig""] At MetricsConfig.java:[lines 51-280]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics2.impl.MetricsConfig.toString(Configuration): new java.io.PrintStream(OutputStream) [""org.apache.hadoop.metrics2.impl.MetricsConfig""] At MetricsConfig.java:[lines 51-280]
[INFO] Increment of volatile field org.apache.hadoop.metrics2.lib.MutableCounterInt.value in org.apache.hadoop.metrics2.lib.MutableCounterInt.incr() [""org.apache.hadoop.metrics2.lib.MutableCounterInt""] At MutableCounterInt.java:[lines 35-64]
[INFO] Increment of volatile field org.apache.hadoop.metrics2.lib.MutableCounterLong.value in org.apache.hadoop.metrics2.lib.MutableCounterLong.incr() [""org.apache.hadoop.metrics2.lib.MutableCounterLong""] At MutableCounterLong.java:[lines 36-65]
[INFO] Increment of volatile field org.apache.hadoop.metrics2.lib.MutableGaugeInt.value in org.apache.hadoop.metrics2.lib.MutableGaugeInt.decr() [""org.apache.hadoop.metrics2.lib.MutableGaugeInt""] At MutableGaugeInt.java:[lines 36-89]
[INFO] Increment of volatile field org.apache.hadoop.metrics2.lib.MutableGaugeInt.value in org.apache.hadoop.metrics2.lib.MutableGaugeInt.incr() [""org.apache.hadoop.metrics2.lib.MutableGaugeInt""] At MutableGaugeInt.java:[lines 36-89]
[INFO] Increment of volatile field org.apache.hadoop.metrics2.lib.MutableGaugeLong.value in org.apache.hadoop.metrics2.lib.MutableGaugeLong.decr() [""org.apache.hadoop.metrics2.lib.MutableGaugeLong""] At MutableGaugeLong.java:[lines 36-89]
[INFO] Increment of volatile field org.apache.hadoop.metrics2.lib.MutableGaugeLong.value in org.apache.hadoop.metrics2.lib.MutableGaugeLong.incr() [""org.apache.hadoop.metrics2.lib.MutableGaugeLong""] At MutableGaugeLong.java:[lines 36-89]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics2.sink.FileSink.init(SubsetConfiguration): new java.io.FileWriter(File, boolean) [""org.apache.hadoop.metrics2.sink.FileSink""] At FileSink.java:[lines 39-83]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics2.sink.FileSink.init(SubsetConfiguration): new java.io.PrintWriter(OutputStream) [""org.apache.hadoop.metrics2.sink.FileSink""] At FileSink.java:[lines 39-83]
[INFO] Found reliance on default encoding in org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink.xdr_string(String): String.getBytes() [""org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink""] At AbstractGangliaSink.java:[lines 45-289]
[INFO] Sequence of calls to java.util.concurrent.ConcurrentHashMap may not be atomic in org.apache.hadoop.net.NetUtils.canonicalizeHost(String) [""org.apache.hadoop.net.NetUtils""] At NetUtils.java:[lines 63-905]
[INFO] Found reliance on default encoding in org.apache.hadoop.net.TableMapping$RawTableMapping.load(): new java.io.FileReader(String) [""org.apache.hadoop.net.TableMapping$RawTableMapping""] At TableMapping.java:[lines 85-171]
[INFO] Found reliance on default encoding in org.apache.hadoop.record.compiler.CGenerator.genCode(String, ArrayList, ArrayList, String, ArrayList): new java.io.FileWriter(String) [""org.apache.hadoop.record.compiler.CGenerator""] At CGenerator.java:[lines 32-71]
[INFO] Found reliance on default encoding in org.apache.hadoop.record.compiler.CppGenerator.genCode(String, ArrayList, ArrayList, String, ArrayList): new java.io.FileWriter(String) [""org.apache.hadoop.record.compiler.CppGenerator""] At CppGenerator.java:[lines 32-74]
[INFO] Found reliance on default encoding in org.apache.hadoop.record.compiler.JRecord$JavaRecord.genCode(String, ArrayList): new java.io.FileWriter(File) [""org.apache.hadoop.record.compiler.JRecord$JavaRecord""] At JRecord.java:[lines 42-478]
[INFO] Found reliance on default encoding in org.apache.hadoop.security.AuthenticationFilterInitializer.initFilter(FilterContainer, Configuration): new java.io.FileReader(String) [""org.apache.hadoop.security.AuthenticationFilterInitializer""] At AuthenticationFilterInitializer.java:[lines 46-112]
[INFO] Found reliance on default encoding in org.apache.hadoop.security.Credentials.<static initializer for Credentials>(): String.getBytes() [""org.apache.hadoop.security.Credentials""] At Credentials.java:[lines 58-323]
[INFO] Found reliance on default encoding in org.apache.hadoop.security.LdapGroupsMapping.extractPassword(String): new java.io.FileReader(String) [""org.apache.hadoop.security.LdapGroupsMapping""] At LdapGroupsMapping.java:[lines 71-360]
[INFO] Found reliance on default encoding in org.apache.hadoop.security.SaslRpcServer.decodeIdentifier(String): String.getBytes() [""org.apache.hadoop.security.SaslRpcServer""] At SaslRpcServer.java:[lines 65-214]
[INFO] Found reliance on default encoding in org.apache.hadoop.security.SaslRpcServer.encodeIdentifier(byte[]): new String(byte[]) [""org.apache.hadoop.security.SaslRpcServer""] At SaslRpcServer.java:[lines 65-214]
[INFO] Found reliance on default encoding in org.apache.hadoop.security.SaslRpcServer.encodePassword(byte[]): new String(byte[]) [""org.apache.hadoop.security.SaslRpcServer""] At SaslRpcServer.java:[lines 65-214]
[INFO] Switch statement found in new org.apache.hadoop.util.ComparableVersion$StringItem(String, boolean) where default case is missing [""org.apache.hadoop.util.ComparableVersion$StringItem""] At ComparableVersion.java:[lines 162-257]
[INFO] Found reliance on default encoding in org.apache.hadoop.util.HostsFileReader.readFileToSetWithFileInputStream(String, String, InputStream, Set): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.util.HostsFileReader""] At HostsFileReader.java:[lines 41-176]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.util.NativeLibraryChecker.main(String[]) [""org.apache.hadoop.util.NativeLibraryChecker""] At NativeLibraryChecker.java:[lines 32-97]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.util.NativeLibraryChecker.main(String[]) [""org.apache.hadoop.util.NativeLibraryChecker""] At NativeLibraryChecker.java:[lines 32-97]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.util.NativeLibraryChecker.main(String[]) [""org.apache.hadoop.util.NativeLibraryChecker""] At NativeLibraryChecker.java:[lines 32-97]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.util.NativeLibraryChecker.main(String[]) [""org.apache.hadoop.util.NativeLibraryChecker""] At NativeLibraryChecker.java:[lines 32-97]
[INFO] Format string should use %n rather than \n in org.apache.hadoop.util.NativeLibraryChecker.main(String[]) [""org.apache.hadoop.util.NativeLibraryChecker""] At NativeLibraryChecker.java:[lines 32-97]
[INFO] org.apache.hadoop.util.PrintJarMainClass.main(String[]) may fail to close stream [""org.apache.hadoop.util.PrintJarMainClass""] At PrintJarMainClass.java:[lines 31-54]
[INFO] Switch statement found in org.apache.hadoop.util.PureJavaCrc32.update(byte[], int, int) where default case is missing [""org.apache.hadoop.util.PureJavaCrc32""] At PureJavaCrc32.java:[lines 45-118]
[INFO] Switch statement found in org.apache.hadoop.util.PureJavaCrc32C.update(byte[], int, int) where default case is missing [""org.apache.hadoop.util.PureJavaCrc32C""] At PureJavaCrc32C.java:[lines 41-115]
[INFO] Found reliance on default encoding in org.apache.hadoop.util.ReflectionUtils.logThreadInfo(Log, String, long): java.io.ByteArrayOutputStream.toString() [""org.apache.hadoop.util.ReflectionUtils""] At ReflectionUtils.java:[lines 52-339]
[INFO] Found reliance on default encoding in org.apache.hadoop.util.ReflectionUtils.logThreadInfo(Log, String, long): new java.io.PrintWriter(OutputStream) [""org.apache.hadoop.util.ReflectionUtils""] At ReflectionUtils.java:[lines 52-339]
[INFO] Found reliance on default encoding in org.apache.hadoop.util.Shell.runCommand(): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.util.Shell""] At Shell.java:[lines 45-753]
{noformat}"
HADOOP-10479,Fix new findbugs warnings in hadoop-minikdc,"The following findbugs warnings need to be fixed:

{noformat}
[INFO] --- findbugs-maven-plugin:2.5.3:check (default-cli) @ hadoop-minikdc ---
[INFO] BugInstance size is 2
[INFO] Error size is 0
[INFO] Total bugs: 2
[INFO] Found reliance on default encoding in org.apache.hadoop.minikdc.MiniKdc.initKDCServer(): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.minikdc.MiniKdc""] At MiniKdc.java:[lines 112-557]
[INFO] Found reliance on default encoding in org.apache.hadoop.minikdc.MiniKdc.main(String[]): new java.io.FileReader(File) [""org.apache.hadoop.minikdc.MiniKdc""] At MiniKdc.java:[lines 112-557]
{noformat}"
HADOOP-10478,Fix new findbugs warnings in hadoop-maven-plugins,"The following findbug warning needs to be fixed:

{noformat}
[INFO] --- findbugs-maven-plugin:2.5.3:check (default-cli) @ hadoop-maven-plugins ---
[INFO] BugInstance size is 1
[INFO] Error size is 0
[INFO] Total bugs: 1
[INFO] Found reliance on default encoding in new org.apache.hadoop.maven.plugin.util.Exec$OutputBufferThread(InputStream): new java.io.InputStreamReader(InputStream) [""org.apache.hadoop.maven.plugin.util.Exec$OutputBufferThread""] At Exec.java:[lines 89-114]
{noformat}"
HADOOP-10476,Bumping the findbugs version to 3.0.0,"The findbug version used by hadoop is pretty old (1.3.9). The old version of Findbugs itself have some bugs (like http://sourceforge.net/p/findbugs/bugs/918/, hit by HADOOP-10474).

Futhermore, Java 8 is only supported by findbugs 3.0.0 or newer.

It's a good time to bump the findbugs version to 3.0.0."
HADOOP-10475,ConcurrentModificationException in AbstractDelegationTokenSelector.selectToken(),While running a hive job on a HA cluster saw ConcurrentModificationException in AbstractDelegationTokenSelector.selectToken()
HADOOP-10473,TestCallQueueManager is still flaky,"testSwapUnderContention counts the calls and then interrupts as shown below.  There could be call after counting the call but before interrupt.
{code}
    for (Taker t : consumers) {
      totalCallsConsumed += t.callsTaken;
      threads.get(t).interrupt();
    }
{code}"
HADOOP-10471,Reduce the visibility of constants in ProxyUsers,"Most of the constants in proxyusers have public visibility unnecessarily. 
These public constants should be set to private  and their external usage should be replaced by the corresponding functions.
"
HADOOP-10468,TestMetricsSystemImpl.testMultiThreadedPublish fails intermediately,"{{TestMetricsSystemImpl.testMultiThreadedPublish}} can fail intermediately due to the insufficient size of the sink queue:

{code}
2014-04-06 21:34:55,269 WARN  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:putMetricsImmediate(107)) - Collector has a full queue and can't consume the given metrics.
2014-04-06 21:34:55,270 WARN  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:putMetricsImmediate(107)) - Collector has a full queue and can't consume the given metrics.
2014-04-06 21:34:55,271 WARN  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:putMetricsImmediate(107)) - Collector has a full queue and can't consume the given metrics.
{code}

The unit test should increase the default queue size to avoid intermediate failure."
HADOOP-10467,Enable proxyuser specification to support list of users in addition to list of groups.,"Today , the proxy user specification supports only list of groups. In some cases, it is useful to specify the list of users in addition to list of groups. 
"
HADOOP-10466,Lower the log level in UserGroupInformation,That's more or less the continuation of HADOOP-10015... Some info should be debug if we apply the same policy.
HADOOP-10462,DF#getFilesystem is not parsing the command output,"{{DF#getFileSystem}} returns null if {{DF#getMount}} is not called before. This is because {{DF#getFileSystem}} is not parsing the df command output.
This causes {{NameNodeResourceChecker}} to print the log as follows if the available space on the volume used for saving fsimage is less than the specified value (100MB by default).
{code}
Space available on volume 'null' is 92274688, which is below the configured reserved amount 104857600
{code}
"
HADOOP-10459,distcp V2 doesn't preserve root dir's attributes when -p is specified,"Two issues were observed with distcpV2

ISSUE 1. when copying a source dir to target dir with ""-pu"" option using command 

  ""distcp -pu source-dir target-dir""
 
The source dir's owner is not preserved at target dir. Simiarly other attributes of source dir are not preserved.  Supposedly they should be preserved when no -update and no -overwrite specified. 

There are two scenarios with the above command:

a. when target-dir already exists. Issuing the above command will  result in target-dir/source-dir (source-dir here refers to the last component of the source-dir path in the command line) at target file system, with all contents in source-dir copied to under target-dir/src-dir. The issue in this case is, the attributes of src-dir is not preserved.

b. when target-dir doesn't exist. It will result in target-dir with all contents of source-dir copied to under target-dir. This issue in this  case is, the attributes of source-dir is not carried over to target-dir.

For multiple source cases, e.g., command 

  ""distcp -pu source-dir1 source-dir2 target-dir""

No matter whether the target-dir exists or not, the multiple sources are copied to under the target dir (target-dir is created if it didn't exist). And their attributes are preserved. 

ISSUE 2. with the following command:

  ""distcp source-dir target-dir""

when source-dir is an empty directory, and when target-dir doesn't exist, source-dir is not copied, actually the command behaves like a no-op. However, when the source-dir is not empty, it would be copied and results in target-dir at the target file system containing a copy of source-dir's children.

To be consistent, empty source dir should be copied too. Basically the  above distcp command should cause target-dir get created at target file system, and the source-dir's attributes are preserved at target-dir when -p is passed.
"
HADOOP-10458,swifts should throw FileAlreadyExistsException on attempt to overwrite file,"the swift:// filesystem checks for and rejects {{create()}} calls over an existing file if overwrite = false, but it throws a custom exception. {{SwiftPathExistsException}}

If it threw a {{org.apache.hadoop.fs.FileAlreadyExistsException}} it would match HDFS"
HADOOP-10457,S3N NPEs if you do a read() after a seek() past the EOF,"if you do a seek past the EOF of an S3n file
# it doesn't throw any exception
# on the next read, you get to see a stack trace"
HADOOP-10456,Bug in Configuration.java exposed by Spark (ConcurrentModificationException),"The following exception occurs non-deterministically:
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
        at java.util.HashMap$KeyIterator.next(HashMap.java:960)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:341)
        at java.util.HashSet.<init>(HashSet.java:117)
        at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:671)
        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:439)
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:110)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:154)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
"
HADOOP-10455,"When there is an exception, ipc.Server should first check whether it is an terse exception","ipc.Server allows application servers to define terse exceptions; see Server.addTerseExceptions.  For terse exception, it only prints a short message but not the stack trace.  However, if an exception is both RuntimeException and terse exception, it still prints out the stack trace of the exception."
HADOOP-10454,Provide FileContext version of har file system,"Add support for HarFs, the FileContext version of HarFileSystem."
HADOOP-10451,Remove unused field and imports from SaslRpcServer,"There were unused fields and import remained on SaslRpcServer.

This jira is to remove cleanup those  fields from SaslRpcServer. "
HADOOP-10450,Build zlib native code bindings in hadoop.dll for Windows.,"Currently, the zlib native code bindings are not built in to hadoop.dll for Windows.  This patch will include this in the Windows build."
HADOOP-10449,Fix the javac warnings in the security packages.,The are a few minor javac warnings.
HADOOP-10448,Support pluggable mechanism to specify proxy user settings,"We have a requirement to support large number of superusers. (users who impersonate as another user) (http://hadoop.apache.org/docs/r1.2.1/Secure_Impersonation.html) 

Currently each  superuser needs to be defined in the core-site.xml via proxyuser settings. This will be cumbersome when there are 1000 entries.

It seems useful to have a pluggable mechanism to specify  proxy user settings with the current approach as the default. 

"
HADOOP-10442,Group look-up can cause segmentation fault when certain JNI-based mapping module is used.,"When JniBasedUnixGroupsNetgroupMapping or JniBasedUnixGroupsMapping is used, we get segmentation fault very often. The same system ran 2.2 for months without any problem, but as soon as upgrading to 2.3, it started crashing.  This resulted in multiple name node crashes per day.

The server was running nslcd (nss-pam-ldapd-0.7.5-15.el6_3.2). We did not see this problem on the servers running sssd. 

There was one change in the C code and it modified the return code handling after getgrouplist() call. If the function returns 0 or a negative value less than -1, it will do realloc() instead of returning failure."
HADOOP-10441,"Namenode metric ""rpc.RetryCache/NameNodeRetryCache.CacheHit"" can't be correctly processed by Ganglia","The issue is reported by [~dsen]:

Recently added Namenode metric ""rpc.RetryCache/NameNodeRetryCache.CacheHit"" can't be correctly processed by Ganglia because its name contains ""/""

Proposal: Namenode metric ""rpc.RetryCache/NameNodeRetryCache.CacheHit"" should be renamed to ""rpc.RetryCache.NameNodeRetryCache.CacheHit""
Here - org.apache.hadoop.ipc.metrics.RetryCacheMetrics#RetryCacheMetrics
{code}
  RetryCacheMetrics(RetryCache retryCache) {
    name = ""RetryCache/""+ retryCache.getCacheName();
    registry = new MetricsRegistry(name);
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Initialized ""+ registry);
    }
  }
{code}"
HADOOP-10440,"HarFsInputStream of HarFileSystem, when reading data, computing the position has bug","In the HarFsInputStream of HarFileSystem, when reading data by interface ""int read(byte[] b)"", ""int read(byte[] b, int offset, int len)"" wille be called and position wille be update, so position  need not be update in interface ""int read(byte[] b)""
{code}
//HarFileSystem.HarFsInputStream
      public synchronized int read(byte[] b) throws IOException {
        int ret = read(b, 0, b.length);
        if (ret != -1) {
          position += ret;
        }
        return ret;
      }

      /**
       *
       */
      public synchronized int read(byte[] b, int offset, int len)
        throws IOException {
        int newlen = len;
        int ret = -1;
        if (position + len > end) {
          newlen = (int) (end - position);
        }
        // end case
        if (newlen == 0)
          return ret;
        ret = underLyingStream.read(b, offset, newlen);
        position += ret;
        return ret;
      }
{code}"
HADOOP-10439,Fix compilation error in branch-2 after HADOOP-10426,"HADOOP-10426 removes the import of {{java.io.File}} in branch-2, which causes compilation error."
HADOOP-10437,Fix the javac warnings in the conf and the util package,There are a few minor javac warnings in org.apache.hadoop.conf and org.apache.hadoop.util.  We should fix them.
HADOOP-10433,Key Management Server based on KeyProvider API,"(from HDFS-6134 proposal)

Hadoop KMS is the gateway, for Hadoop and Hadoop clients, to the underlying KMS. It provides an interface that works with existing Hadoop security components (authenticatication, confidentiality).

Hadoop KMS will be implemented leveraging the work being done in HADOOP-10141 and HADOOP-10177.

Hadoop KMS will provide an additional implementation of the Hadoop KeyProvider class. This implementation will be a client-server implementation.

The client-server protocol will be secure:

* Kerberos HTTP SPNEGO (authentication)
* HTTPS for transport (confidentiality and integrity)
* Hadoop ACLs (authorization)

The Hadoop KMS implementation will not provide additional ACL to access encrypted files. For sophisticated access control requirements, HDFS ACLs (HDFS-4685) should be used.

Basic key administration will be supported by the Hadoop KMS via the, already available, Hadoop KeyShell command line tool

There are minor changes that must be done in Hadoop KeyProvider functionality:

The KeyProvider contract, and the existing implementations, must be thread-safe

KeyProvider API should have an API to generate the key material internally
JavaKeyStoreProvider should use, if present, a password provided via configuration

KeyProvider Option and Metadata should include a label (for easier cross-referencing)

To avoid overloading the underlying KeyProvider implementation, the Hadoop KMS will cache keys using a TTL policy.

Scalability and High Availability of the Hadoop KMS can achieved by running multiple instances behind a VIP/Load-Balancer. For High Availability, the underlying KeyProvider implementation used by the Hadoop KMS must be High Available.
"
HADOOP-10432,Refactor SSLFactory to expose static method to determine HostnameVerifier,"The {{SSFactory.getHostnameVerifier()}} method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, DEFAULT_AND_LOCALHOST, STRICT, STRICT_IE6, ALLOW_ALL).

"
HADOOP-10431,Change visibility of KeyStore.Options getter methods to public,Making Options getter methods public will enable {{KeyProvider}} implementations to use those classes.
HADOOP-10430,"KeyProvider Metadata should have an optional description, there should be a method to retrieve the metadata from all keys",Being able to attach an optional description (and show it when displaying metadata) will enable giving some context on the keys.
HADOOP-10429,"KeyStores should have methods to generate the materials themselves, KeyShell should use them","Currently, the {{KeyProvider}} API expects the caller to provide the key materials. And, the {{KeyShell}} generates key materials.

For security reasons, {{KeyProvider}} implementations may want to generate and hide (from the user generating the key) the key materials."
HADOOP-10428,JavaKeyStoreProvider should accept keystore password via configuration falling back to ENV VAR,"Currently the password for the {{JavaKeyStoreProvider}} must be set in an ENV VAR.

Allowing the password to be set via configuration enables applications to interactively ask for the password before initializing the {{JavaKeyStoreProvider}}."
HADOOP-10427,KeyProvider implementations should be thread safe,The {{KeyProvider}} API should be thread-safe so it can be used safely in server apps.
HADOOP-10426,CreateOpts.getOpt(..) should declare with generic type argument,"Similar to CreateOpts.setOpt(..), the CreateOpts.getOpt(..) should also declare with a generic type parameter <T extends CreateOpts>.  Then, all the casting from CreateOpts to its subclasses can be avoided."
HADOOP-10425,Incompatible behavior of LocalFileSystem:getContentSummary,"Unlike in Hadoop1, FilterFileSystem overrides getContentSummary, which causes content summary to be called on rawLocalFileSystem in Local mode.

This impacts the computations of Stats in Hive with getting back FileSizes that include the size of the crc files.
"
HADOOP-10423,Clarify compatibility policy document for combination of new client and old server.,"As discussed on the dev mailing lists and MAPREDUCE-4052, we need to update the text of the compatibility policy to discuss a new client combined with an old server."
HADOOP-10422,Remove redundant logging of RPC retry attempts.,{{RetryUtils}} logs each retry attempt at both info level and debug level.
HADOOP-10419,BufferedFSInputStream NPEs on getPos() on a closed stream,"if you call getPos on a {{ChecksumFileSystem}} after a {{close()}} you get an NPE.

While throwing an exception in this states is legitimate (HDFS does, RawLocal does not), it should be an {{IOException}}"
HADOOP-10418,SaslRpcClient should not assume that remote principals are in the default_realm,"In SaslRpcClient#getServerPrincipal, when constructing the KerberosPrincipal to compare to the configured value, we just assume that the remote principal is in the default realm configured in /etc/krb5.conf. This will not always be the case, however. Instead, we should use the configured domain_realm mapping to determine the realm of the remote principal."
HADOOP-10414,Incorrect property name for RefreshUserMappingProtocol in hadoop-policy.xml,"In HDFS-1096 and MAPREDUCE-1836, the name of the ACL property for the RefreshUserMappingsProtocol service changed from security.refresh.usertogroups.mappings.protocol.acl to security.refresh.user.mappings.protocol.acl, but the example in hadoop-policy.xml was not updated. The example should be fixed to avoid confusion."
HADOOP-10407,Fix the javac warnings in the ipc package.,Fix the javac warnings in the org.apache.hadoop.ipc package.  Most of them are generic warnings.
HADOOP-10404,Some accesses to DomainSocketWatcher#closed are not protected by lock,"{code}
   * Lock which protects toAdd, toRemove, and closed.
   */
  private final ReentrantLock lock = new ReentrantLock();
{code}
There're two places, NotificationHandler.handle() and kick(), where access to closed is without holding lock.
"
HADOOP-10402,Configuration.getValByRegex does not substitute for variables,"When using Configuration.getValByRegex(...), variables are not resolved.  

For example:
{code:xml}
<property>
   <name>bar</name>
   <value>woot</value>
</property>
<property>
   <name>foo3</name>
   <value>${bar}</value>
</property>
{code}
If you then try to do something like {{Configuration.getValByRegex(foo.*)}}, it will return a Map containing ""foo3=$\{bar}"" instead of ""foo3=woot"""
HADOOP-10401,ShellBasedUnixGroupsMapping#getGroups does not always return primary group first,{{ShellBasedUnixGroupsMapping#getGroups}} does not always return the primary group first.  It should do this so that clients who expect it don't get the wrong result.  We should also document that the primary group is returned first in the API.  Note that {{JniBasedUnixGroupsMapping}} does return the primary group first.
HADOOP-10400,Incorporate new S3A FileSystem implementation,"The s3native filesystem has a number of limitations (some of which were recently fixed by HADOOP-9454). This patch adds an s3a filesystem which uses the aws-sdk instead of the jets3t library. There are a number of improvements over s3native including:

- Parallel copy (rename) support (dramatically speeds up commits on large files)
- AWS S3 explorer compatible empty directories files ""xyz/"" instead of ""xyz_$folder$"" (reduces littering)
- Ignores s3native created _$folder$ files created by s3native and other S3 browsing utilities
- Supports multiple output buffer dirs to even out IO when uploading files
- Supports IAM role-based authentication
- Allows setting a default canned ACL for uploads (public, private, etc.)
- Better error recovery handling
- Should handle input seeks without having to download the whole file (used for splits a lot)

This code is a copy of https://github.com/Aloisius/hadoop-s3a with patches to various pom files to get it to build against trunk. I've been using 0.0.1 in production with CDH 4 for several months and CDH 5 for a few days. The version here is 0.0.2 which changes around some keys to hopefully bring the key name style more inline with the rest of hadoop 2.x.

*Tunable parameters:*

    fs.s3a.access.key - Your AWS access key ID (omit for role authentication)
    fs.s3a.secret.key - Your AWS secret key (omit for role authentication)
    fs.s3a.connection.maximum - Controls how many parallel connections HttpClient spawns (default: 15)
    fs.s3a.connection.ssl.enabled - Enables or disables SSL connections to S3 (default: true)
    fs.s3a.attempts.maximum - How many times we should retry commands on transient errors (default: 10)
    fs.s3a.connection.timeout - Socket connect timeout (default: 5000)
    fs.s3a.paging.maximum - How many keys to request from S3 when doing directory listings at a time (default: 5000)
    fs.s3a.multipart.size - How big (in bytes) to split a upload or copy operation up into (default: 104857600)
    fs.s3a.multipart.threshold - Until a file is this large (in bytes), use non-parallel upload (default: 2147483647)
    fs.s3a.acl.default - Set a canned ACL on newly created/copied objects (private | public-read | public-read-write | authenticated-read | log-delivery-write | bucket-owner-read | bucket-owner-full-control)
    fs.s3a.multipart.purge - True if you want to purge existing multipart uploads that may not have been completed/aborted correctly (default: false)
    fs.s3a.multipart.purge.age - Minimum age in seconds of multipart uploads to purge (default: 86400)
    fs.s3a.buffer.dir - Comma separated list of directories that will be used to buffer file writes out of (default: uses ${hadoop.tmp.dir}/s3a )


*Caveats*:

Hadoop uses a standard output committer which uploads files as filename.COPYING before renaming them. This can cause unnecessary performance issues with S3 because it does not have a rename operation and S3 already verifies uploads against an md5 that the driver sets on the upload request. While this FileSystem should be significantly faster than the built-in s3native driver because of parallel copy support, you may want to consider setting a null output committer on our jobs to further improve performance.

Because S3 requires the file length and MD5 to be known before a file is uploaded, all output is buffered out to a temporary file first similar to the s3native driver.

Due to the lack of native rename() for S3, renaming extremely large files or directories make take a while. Unfortunately, there is no way to notify hadoop that progress is still being made for rename operations, so your job may time out unless you increase the task timeout.

This driver will fully ignore _$folder$ files. This was necessary so that it could interoperate with repositories that have had the s3native driver used on them, but means that it won't recognize empty directories that s3native has been used on.

Statistics for the filesystem may be calculated differently than the s3native filesystem. When uploading a file, we do not count writing the temporary file on the local filesystem towards the local filesystem's written bytes count. When renaming files, we do not count the S3->S3 copy as read or write operations. Unlike the s3native driver, we only count bytes written when we start the upload (as opposed to the write calls to the temporary local file). The driver also counts read & write ops, but they are done mostly to keep from timing out on large s3 operations.

The AWS SDK unfortunately passes the multipart threshold as an int which means
fs.s3a.multipart.threshold can not be greater than 2^31-1 (2147483647).

This is currently implemented as a FileSystem and not a AbstractFileSystem."
HADOOP-10399,FileContext API for ACLs.,Add new methods to AbstractFileSystem and FileContext for manipulating ACLs.
HADOOP-10395,TestCallQueueManager is flaky,"{{TestCallQueueManager#testSwapUnderContention}} fails occasionally on a test VM with this assert.
{code}
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.hadoop.ipc.TestCallQueueManager.testSwapUnderContention(TestCallQueueManager.java:193)
{code}

Although it appears unlikely, it is possible for the queue to be intermittently empty while the putters and getters are running. The assert can be removed or coded differently.
"
HADOOP-10394,TestAuthenticationFilter is flaky,"We have seen this assert cause occasional failures on Ubuntu.

{code}
        Assert.assertEquals(System.currentTimeMillis() + 1000 * 1000,
                     token.getExpires(), 100);
{code}

The expected fudge is up to 100ms, we have seen up to ~110ms in practice."
HADOOP-10393,Fix hadoop-auth javac warnings,There are quite a few generic warnings and other javac warnings in hadoop-auth.  All of them are minor.
HADOOP-10386,Log proxy hostname in various exceptions being thrown in a HA setup,"In a HA setup any time we see an exception such as safemode or namenode in standby etc we dont know which namenode it came from. The user has to go to the logs of the namenode and determine which one was active and/or standby around the same time.
I think it would help with debugging if any such exceptions could include the namenode hostname so the user could know exactly which namenode served the request."
HADOOP-10383,InterfaceStability annotations should have RetentionPolicy.RUNTIME,Same as in HADOOP-10374. Forgot to make the change there. 
HADOOP-10379,Protect authentication cookies with the HttpOnly and Secure flags,"Browser vendors have adopted proposals to enhance the security of HTTP cookies. For example, the server can mark a cookie as {{Secure}} so that it will not be transfer via plain-text HTTP protocol, and the server can mark a cookie as {{HttpOnly}} to prohibit the JavaScript to access that cookie.

This jira proposes to adopt these flags in Hadoop to protect the HTTP cookie used for authentication purposes."
HADOOP-10378,Typo in help printed by hdfs dfs -help,"There is a typo in the description of the following command

hdfs dfs -help

{noformat}
-count [-q] <path> ...:	Count the number of directories, files and bytes under the paths
		that match the specified file pattern.  The output columns are:
		DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or
		QUOTA REMAINING_QUATA SPACE_QUOTA REMAINING_SPACE_QUOTA 
		      DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME
{noformat}

""REMAINING_QUATA"" should be ""REMAINING_QUOTA"""
HADOOP-10376,Refactor refresh*Protocols into a single generic refreshConfigProtocol,"See https://issues.apache.org/jira/browse/HADOOP-10285

There are starting to be too many refresh*Protocols We can refactor them to use a single protocol with a variable payload to choose what to do.

Thereafter, we can return an indication of success or failure."
HADOOP-10374,InterfaceAudience annotations should have RetentionPolicy.RUNTIME,"There are valid use cases for accessing the InterfaceAudience annotations programatically. 

In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. 

Related: HBASE-8546, HBASE-10462, HBASE-8275"
HADOOP-10373,create tools/hadoop-amazon for aws/EMR support,"After HADOOP-9565 adds a marker interface for blobstores, move s3 & s3n into their own hadoop-amazon library

# keeps the S3 dependencies out of the standard hadoop client dependency graph.
# lets people switch this for alternative implementations.

feature #2 would let you swap over to another s3n impl (e.g. amazon's) without rebuilding everything"
HADOOP-10368,InputStream is not closed in VersionInfo ctor,"{code}
      InputStream is = Thread.currentThread().getContextClassLoader()
        .getResourceAsStream(versionInfoFile);
      if (is == null) {
        throw new IOException(""Resource not found"");
      }
      info.load(is);
{code}
is should be closed at the end of the method."
HADOOP-10365,BufferedOutputStream in FileUtil#unpackEntries() should be closed in finally block,"{code}
    BufferedOutputStream outputStream = new BufferedOutputStream(
        new FileOutputStream(outputFile));
...
    outputStream.flush();
    outputStream.close();
{code}

outputStream should be closed in finally block."
HADOOP-10361,Correct alignment in CLI output for ACLs.,This patch will correct a few lingering issues in the alignment of the text output from the CLI related to ACLs.
HADOOP-10360,Use 2 network adapter In hdfs read and write,
HADOOP-10355,TestLoadGenerator#testLoadGenerator fails,"From https://builds.apache.org/job/PreCommit-HDFS-Build/6194//testReport/

{code}
java.io.IOException: Stream closed
	at java.io.BufferedReader.ensureOpen(BufferedReader.java:97)
	at java.io.BufferedReader.readLine(BufferedReader.java:292)
	at java.io.BufferedReader.readLine(BufferedReader.java:362)
	at org.apache.hadoop.fs.loadGenerator.LoadGenerator.loadScriptFile(LoadGenerator.java:511)
	at org.apache.hadoop.fs.loadGenerator.LoadGenerator.init(LoadGenerator.java:418)
	at org.apache.hadoop.fs.loadGenerator.LoadGenerator.run(LoadGenerator.java:324)
	at org.apache.hadoop.fs.loadGenerator.TestLoadGenerator.testLoadGenerator(TestLoadGenerator.java:231)
{code}"
HADOOP-10354,TestWebHDFS fails after merge of HDFS-4685 to trunk,"After merging HDFS-4685 to trunk, some dev environments are experiencing a failure to parse a permission string in TestWebHDFS.  The problem appears to occur only in environments with security extensions enabled on the local file system, such as Smack or ACLs."
HADOOP-10353,FsUrlStreamHandlerFactory is not thread safe,"The {{FsUrlStreamHandlerFactory}} class uses a plain {{HashMap}} for caching.

When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key.

The result is a NPE potentially being thrown when calling {{FsUrlStreamHandlerFactory#createURLStreamHandler(String protocol)}} concurrently."
HADOOP-10352,Recursive setfacl erroneously attempts to apply default ACL to files.,"When calling setfacl -R with an ACL spec containing default ACL entries, the command can fail if there is a mix of directories and files underneath the specified path.  It attempts to set the default ACL entries on the files, but only directories can have a default ACL."
HADOOP-10350,BUILDING.txt should mention openssl dependency required for hadoop-pipes,BUILDING.txt should mention openssl dependency required for hadoop-pipes
HADOOP-10348,"Deprecate hadoop.ssl.configuration in branch-2, and remove it in trunk","As discussed in

https://issues.apache.org/jira/browse/HADOOP-8581?focusedCommentId=13786567&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13786567

The configuration hadoop.ssl.enabled should be deprecated. We need to mark them as deprecated in CommonConfigurationKeysPublic"
HADOOP-10346,Deadlock while logging tokens,Ran into a deadlock between two threads that were both wielding Tokens.  One was trying to log a token while the other was trying to set the token service on a different token.
HADOOP-10345,Sanitize the the inputs (groups and hosts) for the proxyuser configuration,"Currently there are no input cleansing done on  hadoop.proxyuser.<user-name>.groups  and hadoop.proxyuser.<user-name>.hosts .
It will be an improvement to trim each value, remove duplicate and empty values during init/refresh.

"
HADOOP-10343,Change info to debug log in LossyRetryInvocationHandler,in LossyRetryInvocationHandler we print logs at info level when we drop responses. This causes lot of noise on the console.
HADOOP-10342,Extend UserGroupInformation to return a UGI given a preauthenticated kerberos Subject,"We need the ability to use a Subject that was created inside an embedding application through a kerberos authentication. For example, an application that uses JAAS to authenticate to a KDC should be able to provide the resulting Subject and get a UGI instance to call doAs on.

Example: 
{code}
        UserGroupInformation.setConfiguration(conf);

		LoginContext context = new LoginContext(""com.sun.security.jgss.login"", new UserNamePasswordCallbackHandler(userName, password));
		context.login();
		
		Subject subject = context.getSubject();

	    final UserGroupInformation ugi2 = UserGroupInformation.getUGIFromSubject(subject);

        ugi2.doAs(new PrivilegedExceptionAction<Object>() {
            @Override
            public Object run() throws Exception {
                final FileSystem fs = FileSystem.get(conf);
                int i=0;

                for (FileStatus status : fs.listStatus(new Path(""/user""))) {
                    System.out.println(status.getPath());
                    System.out.println(status);
                    if (i++ > 10) {
                        System.out.println(""only first 10 showed..."");
                        break;
                    }
                }
                return null;
            }
        });
{code}
"
HADOOP-10338,Cannot get the FileStatus of the root inode from the new Globber,"We can no longer get the correct FileStatus of the root inode ""/"" from the Globber."
HADOOP-10337,ConcurrentModificationException from MetricsDynamicMBeanBase.createMBeanInfo(),"This stack trace came from our HBase 0.94.3 production env:
2014-02-11,17:34:46,562 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute tbl.micloud_gallery_albumsharetag_v2.region.96a6d0bc9f0153e0e1ec0318b39ecc45.next_histogram_99th_percentile of hadoop:service=RegionServer,name=RegionServerDynamicStatistics threw an exception
javax.management.RuntimeMBeanException: java.util.ConcurrentModificationException
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:856)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:869)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:670)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:315)
        at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:293)
        at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:193)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1057)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$ValueIterator.next(HashMap.java:822)
        at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:87)
        at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:78)
        at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.getAttribute(MetricsDynamicMBeanBase.java:138)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        ... 27 more"
HADOOP-10335,An ip whitelist based implementation to resolve Sasl properties per connection,"As noted in HADOOP-10221, it is sometimes required for a Hadoop Server to communicate with some client over encrypted channel and with some other clients over unencrypted channel. 

Hadoop-10221 introduced an interface _SaslPropertiesResolver_  and the changes required to plugin and use _SaslPropertiesResolver_  to identify the SaslProperties to be used for a connection. 

In this jira, an ip-whitelist based implementation of _SaslPropertiesResolver_  is attempted."
HADOOP-10333,Fix grammatical error in overview.html document,The file trunk/hadoop-common-project/hadoop-common/src/main/java/overview.html contains a typo. I will (try to) create a patch for this.
HADOOP-10332,HttpServer's jetty audit log always logs 200 OK,"HttpServer inserts the audit logger handler  _before_ the actual servlet handlers, so the default 200 is always logged even if the operation fails.
{code}
handlers.setHandlers(new Handler[] {requestLogHandler, contexts});
{code}
"
HADOOP-10330,TestFrameDecoder fails if it cannot bind port 12345,{{TestFrameDecoder}} fails if port 12345 is in use.
HADOOP-10328,loadGenerator exit code is not reliable,"LoadGenerator exit code is determined using the following logic

{code}
int exitCode = init(args);
    if (exitCode != 0) {
      return exitCode;
    }
{code}

At the end of the run we just return the exitCode. So essentially if you are arguments are correct you will always get 0 back."
HADOOP-10327,Trunk windows build broken after HDFS-5746,Hadoop build broken with Native code errors in windows.
HADOOP-10326,M/R jobs can not access S3 if Kerberos is enabled,"With Kerberos enabled, any job that is taking as input or output s3 files fails.

It can be easily reproduced with wordcount shipped in hadoop-examples.jar and a public S3 file:
{code}
/opt/hadoop/bin/hadoop --config /opt/hadoop/conf/ jar /opt/hadoop/hadoop-examples-1.0.0.jar wordcount s3n://ubikodpublic/test out01
{code}

returns:
{code}
12/08/10 12:40:19 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 192 for hadoop on 10.85.151.233:9000
12/08/10 12:40:19 INFO security.TokenCache: Got dt for hdfs://aws04.machine.com:9000/mapred/staging/hadoop/.staging/job_201208101229_0004;uri=10.85.151.233:9000;t.service=10.85.151.233:9000
12/08/10 12:40:19 INFO mapred.JobClient: Cleaning up the staging area hdfs://aws04.machine.com:9000/mapred/staging/hadoop/.staging/job_201208101229_0004
java.lang.IllegalArgumentException: java.net.UnknownHostException: ubikodpublic
        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:293)
        at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:317)
        at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:189)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:92)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:79)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:197)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:252)
<SNIP>
{code}

"
HADOOP-10322,Add ability to read principal names from a keytab,It will be useful to have an ability to enumerate the principals stored in a keytab.
HADOOP-10320,Javadoc in InterfaceStability.java lacks final </ul>,
HADOOP-10319,Unable to run Hadoop (2.2.0) commands on Cygwin (2.831) on Windows XP 3,"Did Following on starting Shell

1. ssh localhost
2. cd /cygdrive/e/hadoop-2.2.0
3. export JAVA_HOME=/cygdrive/e/JDK
4. export HADOOP_INSTALL=/cygdrive/e/hadoop-2.2.o
5. export PATH=$PATH:$JAVA_HOME:$HADOOP_INSTALL/bin:$HADOOP_INSTALL/sbin

I have installed JDK1.7.0_51

At $hadoop version (throws) Error: Could not find or load main class org.apache.hadoop.util.Versioninfo

Similar errors are thrown for fs and jar. I have not made any changes to any environment variables or scripts.

I am new to Hadoop-2.2.0 and following text Hadoop- The Definitive Guide by Tom white where it has been suggested that Cygwin can be used with Windows and Hadoop 2.

Advise appreciated. Thanks

Anand
"
HADOOP-10317,Rename branch-2.3 release version from 2.4.0-SNAPSHOT to 2.3.0-SNAPSHOT,Right now the pom.xml's refer to 2.4 rather than 2.3 in branch-2.3. We need to update them.
HADOOP-10314,The ls command help still shows outdated 0.16 format.,"The description of output format is vastly outdated. It was changed after version 0.16.

{noformat}
$ hadoop fs -help ls
-ls [-d] [-h] [-R] [<path> ...]:	List the contents that match the specified file pattern. If
		path is not specified, the contents of /user/<currentUser>
		will be listed. Directory entries are of the form 
			dirName (full path) <dir> 
		and file entries are of the form 
			fileName(full path) <r n> size 
		where n is the number of replicas specified for the file 
		and size is the size of the file, in bytes.
		  -d  Directories are listed as plain files.
		  -h  Formats the sizes of files in a human-readable fashion
		      rather than a number of bytes.
		  -R  Recursively list the contents of directories.
{noformat}"
HADOOP-10313,Script and jenkins job to produce Hadoop release artifacts,"As discussed in the dev mailing lists, we should have a jenkins job to build the release artifacts."
HADOOP-10312,Shell.ExitCodeException to have more useful toString,"Shell's ExitCodeException doesn't include the exit code in the toString value, so isn't that useful in diagnosing container start failures in YARN"
HADOOP-10311,Cleanup vendor names from the code base,
HADOOP-10310,SaslRpcServer should be initialized even when no secret manager present,"HADOOP-8783 made a change which caused the SaslRpcServer not to be initialized if there is no secret manager present. This works fine for most Hadoop daemons because they need a secret manager to do their business, but JournalNodes do not. The result of this is that JournalNodes are broken and will not handle RPCs in a Kerberos-enabled environment, since the SaslRpcServer will not be initialized."
HADOOP-10305,"Add ""rpc.metrics.quantile.enable"" and ""rpc.metrics.percentiles.intervals"" to core-default.xml","""rpc.metrics.quantile.enable"" and ""rpc.metrics.percentiles.intervals"" were added in HADOOP-9420, but these two parameters are not written in core-default.xml."
HADOOP-10301,AuthenticationFilter should return Forbidden for failed authentication,"The hadoop-auth AuthenticationFilter returns a 401 Unauthorized without a WWW-Authenticate headers.  The is illegal per the HTTP RPC and causes a NPE in the HttpUrlConnection.

This is half of a fix that affects webhdfs.  See HDFS-4564."
HADOOP-10295,Allow distcp to automatically identify the checksum type of source files and use it for the target,"Currently while doing distcp, users can use ""-Ddfs.checksum.type"" to specify the checksum type in the target FS. This works fine if all the source files are using the same checksum type. If files in the source cluster have mixed types of checksum, users have to either use ""-skipcrccheck"" or have checksum mismatching exception. Thus we may need to consider adding a new option to distcp so that it can automatically identify the original checksum type of each source file and use the same checksum type in the target FS. "
HADOOP-10292,Restore HttpServer from branch-2.2 in branch-2,This jira is a follow-up jira of HADOOP-10255. It brings in the HttpServer in branch-2.2 directly into branch-2 to restore the compatibility of HBase.
HADOOP-10291,TestSecurityUtil#testSocketAddrWithIP fails,"testSocketAddrWithIP fails with Assertion Error

{noformat}
Running org.apache.hadoop.security.TestSecurityUtil
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.389 sec <<< FAILURE!
testSocketAddrWithIP(org.apache.hadoop.security.TestSecurityUtil)  Time elapsed: 275 sec  <<< FAILURE!
java.lang.AssertionError: expected:<127.0.0.1:123> but was:<localhost:123>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:147)
	at org.apache.hadoop.security.TestSecurityUtil.verifyTokenService(TestSecurityUtil.java:271)
	at org.apache.hadoop.security.TestSecurityUtil.verifyAddress(TestSecurityUtil.java:290)
	at org.apache.hadoop.security.TestSecurityUtil.verifyServiceAddr(TestSecurityUtil.java:306)
	at org.apache.hadoop.security.TestSecurityUtil.testSocketAddrWithIP(TestSecurityUtil.java:334)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:242)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:137)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)


Results :

Failed tests:   testSocketAddrWithIP(org.apache.hadoop.security.TestSecurityUtil): expected:<127.0.0.1:123> but was:<localhost:123>
{noformat}"
HADOOP-10288,Explicit reference to Log4JLogger breaks non-log4j users,"In HttpRequestLog, we make an explicit reference to the Log4JLogger class for an instanceof check. If the log4j implementation isn't actually on the classpath, the instanceof check throws NoClassDefFoundError instead of returning false. This means that dependent projects that don't use log4j can no longer embed HttpServer -- typically this is an issue when they use MiniDFSCluster as part of their testing."
HADOOP-10285,Admin interface to swap callqueue at runtime,"We wish to swap the active call queue during runtime in order to do performance tuning without restarting the namenode.
This patch adds the ability to refresh the call queue on the namenode, through dfsadmin -refreshCallQueue

"
HADOOP-10282,Create a FairCallQueue: a multi-level call queue which schedules incoming calls and multiplexes outgoing calls,"The FairCallQueue ensures quality of service by altering the order of RPC calls internally. 

It consists of three parts:
1. a Scheduler (`HistoryRpcScheduler` is provided) which provides a priority number from 0 to N (0 being highest priority)
2. a Multi-level queue (residing in `FairCallQueue`) which provides a way to keep calls in priority order internally
3. a Multiplexer (`WeightedRoundRobinMultiplexer` is provided) which provides logic to control which queue to take from

Currently the Mux and Scheduler are not pluggable, but they probably should be (up for discussion).

This is how it is used:

// Production
1. Call is created and given to the CallQueueManager
2. CallQueueManager requests a `put(T call)` into the `FairCallQueue` which implements `BlockingQueue`
3. `FairCallQueue` asks its scheduler for a scheduling decision, which is an integer e.g. 12
4. `FairCallQueue` inserts Call into the 12th queue: `queues.get(12).put(call)`

// Consumption
1. CallQueueManager requests `take()` or `poll()` on FairCallQueue
2. `FairCallQueue` asks its multiplexer for which queue to draw from, which will also be an integer e.g. 2
3. `FairCallQueue` draws from this queue if it has an available call (or tries other queues if it is empty)

Additional information is available in the linked JIRAs regarding the Scheduler and Multiplexer's roles."
HADOOP-10281,"Create a scheduler, which assigns schedulables a priority level","The Scheduler decides which sub-queue to assign a given Call. It implements a single method getPriorityLevel(Schedulable call) which returns an integer corresponding to the subqueue the FairCallQueue should place the call in.

The HistoryRpcScheduler is one such implementation which uses the username of each call and determines what % of calls in recent history were made by this user.

It is configured with a historyLength (how many calls to track) and a list of integer thresholds which determine the boundaries between priority levels.

For instance, if the scheduler has a historyLength of 8; and priority thresholds of 4,2,1; and saw calls made by these users in order:
Alice, Bob, Alice, Alice, Bob, Jerry, Alice, Alice

* Another call by Alice would be placed in queue 3, since she has already made >= 4 calls
* Another call by Bob would be placed in queue 2, since he has >= 2 but less than 4 calls
* A call by Carlos would be placed in queue 0, since he has no calls in the history

Also, some versions of this patch include the concept of a 'service user', which is a user that is always scheduled high-priority. Currently this seems redundant and will probably be removed in later patches, since its not too useful.

----------------

As of now, the current scheduler is the DecayRpcScheduler, which only keeps track of the number of each type of call and decays these counts periodically."
HADOOP-10280,Make Schedulables return a configurable identity of user or group,"In order to intelligently schedule incoming calls, we need to know what identity it falls under.

We do this by defining the Schedulable interface, which has one method, getIdentity(IdentityType idType)

The scheduler can then query a Schedulable object for its identity, depending on what idType is. 

For example:
Call 1: Made by user=Alice, group=admins
Call 2: Made by user=Bob, group=admins
Call 3: Made by user=Carlos, group=users
Call 4: Made by user=Alice, group=admins

Depending on what the identity is, we would treat these requests differently. If we query on Username, we can bucket these 4 requests into 3 sets for Alice, Bob, and Carlos. If we query on Groupname, we can bucket these 4 requests into 2 sets for admins and users.

In this initial version, idType can be username or primary group. In future versions, it could be jobID, request class (read or write), or some explicit QoS field. These are user-defined, and will be reloaded on callqueue refresh."
HADOOP-10279,"Create multiplexer, a requirement for the fair queue","The Multiplexer helps the FairCallQueue decide which of its internal sub-queues to read from during a poll() or take(). It controls the penalty of being in a lower queue. Without the mux, the FairCallQueue would have issues with starvation of low-priority requests.

The WeightedRoundRobinMultiplexer is an implementation which uses a weighted round robin approach to muxing the sub-queues. It is configured with an integer list pattern.

For example: 10, 5, 5, 2 means:
* Read queue 0 10 times
* Read queue 1 5 times
* Read queue 2 5 times
* Read queue 3 2 times
* Repeat"
HADOOP-10278,Refactor to make CallQueue pluggable,"* Refactor CallQueue into an interface, base, and default implementation that matches today's behavior
* Make the call queue impl configurable, keyed on port so that we minimize coupling"
HADOOP-10274,Lower the logging level from ERROR to WARN for UGI.doAs method,"Recently we got the error msg ""Request is a replay (34) - PROCESS_TGS"" while we are using the HBase client API to put data into HBase-0.94.16 with krb5-1.6.1 enabled. The related msg as follows...
{code}
[2014-01-15 09:40:38,452][hbase-tablepool-1-thread-3][ERROR][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1124)): PriviledgedActionException as:takeshi_miao@LAB cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Request is a replay (34) - PROCESS_TGS)]
[2014-01-15 09:40:38,453][hbase-tablepool-1-thread-3][DEBUG][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation.logPriviledgedAction(UserGroupInformation.java:1143)): PriviledgedAction as:takeshi_miao@LAB from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)                                                                                          
[2014-01-15 09:40:38,453][hbase-tablepool-1-thread-3][DEBUG][org.apache.hadoop.ipc.SecureClient](org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection$1.run(SecureClient.java:213)): Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Request is a replay (34) - PROCESS_TGS)]
[2014-01-15 09:40:38,454][hbase-tablepool-1-thread-3][INFO ][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation.reloginFromTicketCache(UserGroupInformation.java:657)): Initiating logout for takeshi_miao@LAB
[2014-01-15 09:40:38,454][hbase-tablepool-1-thread-3][DEBUG][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.logout(UserGroupInformation.java:154)): hadoop logout
[2014-01-15 09:40:38,454][hbase-tablepool-1-thread-3][INFO ][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation.reloginFromTicketCache(UserGroupInformation.java:667)): Initiating re-login for takeshi_miao@LAB
[2014-01-15 09:40:38,455][hbase-tablepool-1-thread-3][DEBUG][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:146)): hadoop login
[2014-01-15 09:40:38,456][hbase-tablepool-1-thread-3][DEBUG][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:95)): hadoop login commit
[2014-01-15 09:40:38,456][hbase-tablepool-1-thread-3][DEBUG][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:100)): using existing subject:[takeshi_miao@LAB, UnixPrincipal: takeshi_miao, UnixNumericUserPrincipal: 501, UnixNumericGroupPrincipal [Primary Group]: 501, UnixNumericGroupPrincipal [Supplementary Group]: 502, takeshi_miao@LAB]
{code}

Finally, we found that the HBase would doing the retry (5 * 10 times) and recovery this _'request is a replay (34)'_ issue, but based on the HBase user viewpoint, the error msg at first line may be frightful, as we were afraid there was any data loss occurring at the first sight...
{code}
[2014-01-15 09:40:38,452][hbase-tablepool-1-thread-3][ERROR][org.apache.hadoop.security.UserGroupInformation](org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1124)): PriviledgedActionException as:takeshi_miao@LAB cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Request is a replay (34) - PROCESS_TGS)]
{code}

So I'd like to suggest to change the logging level from '_ERROR_' to '_WARN_' for _o.a.hadoop.security.UserGroupInformation#doAs(PrivilegedExceptionAction<T>)_ method
{code}
  public <T> T doAs(PrivilegedExceptionAction<T> action
                    ) throws IOException, InterruptedException {
    try {
      // ...
    } catch (PrivilegedActionException pae) {
      Throwable cause = pae.getCause();
      LOG.error(""PriviledgedActionException as:""+this+"" cause:""+cause); // I mean here
      // ...
    }
  }
{code}
Due to this method already throws _checked exception_s which can be handled by the client code, so the error may not really be an error if client  code can handle it...such as this case.

For more details pls refer to HBASE-10379"
HADOOP-10273,Fix 'mvn site',"'mvn site' fails with

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.0:site (default-site) on project hadoop-main: Execution default-site of goal org.apache.maven.plugins:maven-site-plugin:3.0:site failed: A required class was missing while executing org.apache.maven.plugins:maven-site-plugin:3.0:site: org/sonatype/aether/graph/DependencyFilter

[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/AetherClassNotFound
{code}

Looks related to https://cwiki.apache.org/confluence/display/MAVEN/AetherClassNotFound

Bumping the maven-site-plugin version should fix it."
HADOOP-10267,hadoop2.2 building error,"while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10266,hadoop2.2 building error,"while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10265,hadoop2.2 building error,"while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10264,hadoop2.2 building error,"while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10263,hadoop2.2 building error,"while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10262,hadoop2.2 building error,"while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10261,hadoop2.2 building error,"
while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10260,hadoop2.2 building error,"
while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10259,hadoop2.2 building error,"
while running: 
mvn clean install -DskipTests

i got these errors
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,48] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,48] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] OutputFormat is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] XMLSerializer is internal proprietary API and may be removed in a future release
[ERROR] /var/software/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java:[93,0] error: error while writing FsDatasetImpl: No space left on device
[ERROR] -> [Help 1]


i google for a long time, and have no idea how to resole it. can someone help me? thank you very much"
HADOOP-10255,Rename HttpServer to HttpServer2 to retain older HttpServer in branch-2 for compatibility,"As suggested in HADOOP-10253, HBase needs a temporary copy of {{HttpServer}} from branch-2.2 to make sure it works across multiple 2.x releases.

This patch renames the current {{HttpServer}} into {{HttpServer2}}, and bring  the {{HttpServer}} in branch-2.2 into the repository."
HADOOP-10252,HttpServer can't start if hostname is not specified,"HADOOP-8362 added a checking to make sure configuration values are not null. By default, we don't specify the hostname for the HttpServer. So we could not start info server due to

{noformat}
2014-01-22 08:43:05,969 FATAL [M:0;localhost:48573] master.HMaster(2187): Unhandled exception. Starting shutdown.
java.lang.IllegalArgumentException: Property value must not be null
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:958)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:940)
	at org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)
	at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)
	at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)
	at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)
	at org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)
	at java.lang.Thread.run(Thread.java:722){noformat}"
HADOOP-10251,Both NameNodes could be in STANDBY State if SNN network is unstable,"Following corner scenario happened in one of our cluster.

1. NN1 was Active and NN2 was Standby
2. NN2 machine's network was slow 
3. NN1 got shutdown.
4. NN2 ZKFC got the notification and trying to check for old active for fencing. (This took little more time, again due to slow network)
5. In between, NN1 got restarted by our automatic monitoring, and ZKFC made it Active.
6. Now NN2 ZKFC got Old Active as NN1 and it did graceful fencing of NN1 to STANBY.
7. Before writing ActiveBreadCrumb to ZK, NN2 ZKFC got session timeout and got shutdown before making NN2 Active.


*Now cluster having both NameNodes as STANDBY.*
NN1 ZKFC still thinks that its nameNode is in Active state. 
NN2 ZKFC waiting for election."
HADOOP-10250,VersionUtil returns wrong value when comparing two versions,"VersionUtil.compareVersions(""1.0.0-beta-1"", ""1.0.0"") returns 7 instead of negative number, which is wrong, because 1.0.0-beta-1 older than 1.0.0.


 
"
HADOOP-10249,LdapGroupsMapping should trim ldap password read from file," org.apache.hadoop.security.LdapGroupsMapping allows specifying ldap connection password in a file using property key

hadoop.security.group.mapping.ldap.bind.password.file

The code in LdapGroupsMapping  that reads the content of the password file does not trim the password value. This causes ldap connection failure as the password in the password file ends up having a trailing newline.

Most of the text editors and echo adds a new line at the end of file.
So, LdapGroupsMapping should trim the password read from the file.
"
HADOOP-10248,Property name should be included in the exception where property value is null,"I saw the following when trying to determine startup failure:
{code}
2014-01-21 06:07:17,871 FATAL [master:h2-centos6-uns-1390276854-hbase-10:60000] master.HMaster: Unhandled exception. Starting shutdown.
java.lang.IllegalArgumentException: Property value must not be null
at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)
at org.apache.hadoop.conf.Configuration.set(Configuration.java:958)
at org.apache.hadoop.conf.Configuration.set(Configuration.java:940)
at org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)
at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)
at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)
at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)
at org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)
{code}
Property name should be included in the following exception:
{code}
    Preconditions.checkArgument(
        value != null,
        ""Property value must not be null"");
{code}"
HADOOP-10244,TestKeyShell improperly tests the results of a Delete,"The TestKeyShell.testKeySuccessfulKeyLifecycle test is supposed to ensure that the deleted key is no longer in the results of a subsequent delete command. Mistakenly, it is testing that it is STILL there.

The delete command is actually working but the stdout capture should be reset instead of flushed. Therefore, the test is picking up the existence of the key name from the deletion message in the previous command."
HADOOP-10240,Windows build instructions incorrectly state requirement of protoc 2.4.1 instead of 2.5.0,"When we upgraded to protoc 2.5.0, we updated BUILDING.txt to state the new requirement.  However, there is another section of the document for Windows builds, and that section still lists version 2.4.1."
HADOOP-10237,JavaKeyStoreProvider needs to set keystore permissions properly,In order protect access to the created keystores permissions should initially be set to 700 by the JavaKeyStoreProvider. Subsequent permission changes can then be done using FS.
HADOOP-10236,Fix typo in o.a.h.ipc.Client#checkResponse,"There's a typo in o.a.h.ipc.Client.java. 
{code}
          throw new IOException(""Client IDs not matched: local ID=""
              + StringUtils.byteToHexString(clientId) + "", ID in reponse=""
              + StringUtils.byteToHexString(header.getClientId().toByteArray()));
{code}
It should be fixed as follows:
{code}
          throw new IOException(""Client IDs not matched: local ID=""
              + StringUtils.byteToHexString(clientId) + "", ID in response=""
              + StringUtils.byteToHexString(header.getClientId().toByteArray()));
{code}"
HADOOP-10235,Hadoop tarball has 2 versions of stax-api JARs,"They are:

stax-api-1.0-2.jar
stax-api-1.0.2.jar

Maven cannot resolve them property because they are published under different groupIds, because of that for maven are different things.

we need to exclude one of them explicitly"
HADOOP-10234,"""hadoop.cmd jar"" does not propagate exit code.","Running ""hadoop.cmd jar"" does not always propagate the exit code to the caller.  In interactive use, it works fine.  However, in some usages (notably Hive), it gets called through {{Shell#getRunScriptCommand}}, which needs to do an intermediate ""cmd /c"" to execute the script.  In that case, the last exit code is getting dropped, so Hive can't detect job failures."
HADOOP-10231,Add some components in Native Libraries document,"The documented components in Native Libraries are only zlib and gzip.
Now Native Libraries includes some other components such as other compression formats (lz4, snappy), libhdfs and fuse module. These components should be documented."
HADOOP-10228,FsPermission#fromShort() should cache FsAction.values(),"FsPermission#fromShort() calls FsAction.values() every time, which causes unnecessary performance penalty."
HADOOP-10224,JavaKeyStoreProvider has to protect against corrupting underlying store,Java keystores get corrupted at times. A key management operation that writes the store to disk could cause a corruption and all protected data would then be unaccessible.
HADOOP-10223,MiniKdc#main() should close the FileReader it creates,"FileReader is used to read MiniKDC properties.

This FileReader should be closed after reading."
HADOOP-10221,Add a plugin to specify SaslProperties for RPC protocol based on connection properties,"Add a plugin to specify SaslProperties for RPC protocol based on connection properties.

HADOOP-10211 enables client and server to specify and support multiple QOP.  Some connections needs to be restricted to a specific set of QOP based on connection properties.
Eg. connections from client from a specific subnet needs to be encrypted (QOP=privacy)
"
HADOOP-10214,Fix multithreaded correctness warnings in ActiveStandbyElector,"When i worked at HADOOP-9420, found the unrelated findbugs warning:
https://builds.apache.org/job/PreCommit-HADOOP-Build/3408//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html"
HADOOP-10212,Incorrect compile command in Native Library document,"The following old command still exists in Native Library document.
{code}
   $ ant -Dcompile.native=true <target>
{code}
Now maven is used instead of ant."
HADOOP-10211,Enable RPC protocol to negotiate SASL-QOP values between clients and servers,"SASL allows different types of protection are referred to as the quality of protection (qop). It is negotiated between the client and server during the authentication phase of the SASL exchange. Currently hadoop allows specifying a single QOP value  via _hadoop.rpc.protection_. 
The enhancement enables a user to specify multiple QOP values -  _authentication_, _integrity_, _privacy_ as a comma separated list via _hadoop.rpc.protection_
The client and server can have different set of values for  _hadoop.rpc.protection_ and they will negotiate to determine the QOP to be used for communication."
HADOOP-10208,Remove duplicate initialization in StringUtils.getStringCollection,"The *values* is initialized twice.

{code:title=StringUtils.java|borderStyle=solid}
 public static Collection<String> getStringCollection(String str){
    List<String> values = new ArrayList<String>();
    if (str == null)
      return values;
    StringTokenizer tokenizer = new StringTokenizer (str,"","");
    values = new ArrayList<String>();
{code}"
HADOOP-10207,TestUserGroupInformation#testLogin is flaky,"This test depends on the execution order of tests. If TestUserGroupInformation#testGetServerSideGroups() runs first, TestUserGroupInformation#testLogin will fail."
HADOOP-10203,Connection leak in Jets3tNativeFileSystemStore#retrieveMetadata ,"Jets3tNativeFileSystemStore#retrieveMetadata  is leaking connections. 

This affects any client that tries to read many small files very quickly (e.g. distcp from s3 to hdfs with small files blocks due to connection pool starvation). 

This is not a problem for larger files because when the GC runs any connection that's out of scope will be released in #finalize().

We are seeing the following log messages as a symptom of this problem:

{noformat}
13/12/26 13:40:01 WARN httpclient.HttpMethodReleaseInputStream: Attempting to release HttpMethod in finalize() as its response data stream has gone out of scope. This attempt will not always succeed and cannot be relied upon! Please ensure response data streams are always fully consumed or closed to avoid HTTP connection starvation.
13/12/26 13:40:01 WARN httpclient.HttpMethodReleaseInputStream: Successfully released HttpMethod in finalize(). You were lucky this time... Please ensure response data streams are always fully consumed or closed.
{noformat}
"
HADOOP-10201,Add Listing Support to Key Management APIs,Extend the key management APIs from HADOOP-10141 to include the ability to list the available keys.
HADOOP-10198,DomainSocket: add support for socketpair,Add support for {{DomainSocket#socketpair}}.  This function uses the POSIX function of the same name to create two UNIX domain sockets which are connected to each other.  This will be useful for HDFS-5182.
HADOOP-10193,hadoop-auth's PseudoAuthenticationHandler can consume getInputStream,"I'm trying to use the AuthenticationFilter in front of Apache Solr.  The issue I'm running into is that the PseudoAuthenticationHandler calls ServletRequest.getParameter which affects future calls to ServletRequest.getInputStream.  I.e. from the javadoc:

{code}
If the parameter data was sent in the request body, such as occurs with an HTTP POST request, then reading the body directly via getInputStream() or getReader() can interfere with the execution of this method. 
{code}

Solr calls getInputStream after the filter and errors result."
HADOOP-10191,Missing executable permission on viewfs internal dirs,"ViewFileSystem allows 1) unconditional listing of internal directories (mount points) and 2) and changing work directories.

1) requires read permission
2) requires executable permission

However, the hardcoded PERMISSION_RRR == 444 for FileStatus representing an internal dir does not have executable bit set.

This confuses YARN localizer for public resources on viewfs because it requires executable permission for ""other"" on all of the ancestor directories of the resource. 

{code}
java.io.IOException: Resource viewfs:/pubcache/cache.txt is not publicly accessable and as such cannot be part of the public cache.
        at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:182)
        at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:51)
        at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:279)
        at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:277)
{code}"
HADOOP-10184,Hadoop Common changes required to support HDFS ACLs.,This is an umbrella issue for tracking all Hadoop Common changes required to support the HDFS ACLs project.
HADOOP-10181,GangliaContext does not work with multicast ganglia setup,"The GangliaContext class which is used to send Hadoop metrics to Ganglia uses a DatagramSocket to send these metrics.  This works fine for Ganglia multicast setups that are all on the same VLAN.  However, when working with multiple VLANs, a packet sent via DatagramSocket to a multicast address will end up with a TTL of 1.  Multicast TTL indicates the number of network hops for which a particular multicast packet is valid.  The packets sent by GangliaContext do not make it to ganglia aggregrators on the same multicast group, but in different VLANs.

To fix, we'd need a configuration property that specifies that multicast is to be used, and another that allows setting of the multicast packet TTL.  With these set, we could then use MulticastSocket setTimeToLive() instead of just plain ol' DatagramSocket.
"
HADOOP-10178,"Configuration deprecation always emit ""deprecated"" warnings when a new key is used","Even if you use any new configuration properties, you still find ""deprecated"" warnings in your logs. E.g.:
13/12/14 01:00:51 INFO Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
"
HADOOP-10177,Create CLI tools for managing keys via the KeyProvider API,"The KeyProvider API provides access to keys, but we need CLI tools to provide the ability to create and delete keys. I'd think it would look something like:

{code}
% hadoop key create key1
% hadoop key roll key1
% hadoop key list key1
% hadoop key delete key1
{code}"
HADOOP-10175,Har files system authority should preserve userinfo,"When Har file system parse the URI to get the authority at initialization, the userinfo is not preserved. This may lead to failures if the underlying file system relies on the userinfo to work properly. E.g. har://file-user:passwd@localhost:80/test.har will be parsed to har://file-localhost:80/test.har, where user:passwd is lost in the processing."
HADOOP-10173,Remove UGI from DIGEST-MD5 SASL server creation,"Instantiation of SASL server instances within the readers threads is performed within a {{UGI.getCurrentUser().doAs}}.  {{getCurrentUser}} is synchronized, and doAs also degrades performance.  GSSAPI (kerberos) requires instantiation within a doAs, but DIGEST-MD5 (token) does not."
HADOOP-10172,Cache SASL server factories,"Performance for SASL server creation is _atrocious_.  {{Sasl.createSaslServer}} does not cache the provider resolution for the factories.  Factory resolution and server instantiation has 3 major contention points.  During bursts of connections, one reader accepting a connection stalls other readers accepting connections, in turn stalling all existing connections handled by those readers.

I benched 5 threads at 187 instances/s - total, not per thread.  With this and another change, I've boosted it to 33K instances/s."
HADOOP-10171,TestRPC fails intermittently on jkd7,Branch-2 runs JDK7 which has a random test order. So we get an error in TestRPC (testStopsAllThreads) failing on the AssertEquals.
HADOOP-10169,remove the unnecessary  synchronized in JvmMetrics class,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential."
HADOOP-10168,fix javadoc of ReflectionUtils.copy ,"In the javadoc of ReflectionUtils.copy, the return value is not documented, the arguments are named incorrectly.

{code}
  /**                                                                                                                                                                                                                                                                
   * Make a copy of the writable object using serialization to a buffer                                                                                                                                                                                              
   * @param dst the object to copy from                                                                                                                                                                                                                              
   * @param src the object to copy into, which is destroyed                                                                                                                                                                                                          
   * @throws IOException                                                                                                                                                                                                                                             
   */
  @SuppressWarnings(""unchecked"")
  public static <T> T copy(Configuration conf,
                                T src, T dst) throws IOException {
{code}"
HADOOP-10167,Mark hadoop-common source as UTF-8 in Maven pom files / refactoring,"While looking at BIGTOP-831, turned out that the way Bigtop calls maven build / site:site generation causes the errors like this:

[ERROR] Exit code: 1 - /home/user/jenkins/workspace/BigTop-RPM/label/centos-6-x86_64-HAD-1-buildbot/bigtop-repo/build/hadoop/rpm/BUILD/hadoop-2.0.2-alpha-src/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/source/JvmMetricsInfo.java:31: error: unmappable character for encoding ANSI_X3.4-1968
[ERROR] JvmMetrics(""JVM related metrics etc.""), // record info??

Making the whole hadoop-common to use UTF-8 fixes that and seems in general good thing to me.

Attaching first version of patch for review.

Original issue was observed on openjdk 7 (x86-64)."
HADOOP-10164,Allow UGI to login with a known Subject,For storm I would love to let Hadoop initialize based off of credentials that were already populated in a Subject.  This is not currently possible because logging in a user always creates a new blank Subject.  This is to allow a user to be logged in based off a pre-existing subject through a new method.
HADOOP-10162,Fix symlink-related test failures in TestFileContextResolveAfs and TestStat in branch-2,"We need to backport HADOOP-10052  to branch-2 as I found that the TestFileContextResolveAfs is failing after HADOOP-10020 went in.
Also the test TestStat is failing for the same reason. It needs to be fixed as well."
HADOOP-10158,SPNEGO should work with multiple interfaces/SPNs.,"This is the list of internal servlets added by namenode.

| Name | Auth | Need to be accessible by end users |
| StartupProgressServlet | none | no |
| GetDelegationTokenServlet | internal SPNEGO | yes |
| RenewDelegationTokenServlet | internal SPNEGO | yes |
|  CancelDelegationTokenServlet | internal SPNEGO | yes |
|  FsckServlet | internal SPNEGO | yes |
|  GetImageServlet | internal SPNEGO | no |
|  ListPathsServlet | token in query | yes |
|  FileDataServlet | token in query | yes |
|  FileChecksumServlets | token in query | yes |
| ContentSummaryServlet | token in query | yes |

GetDelegationTokenServlet, RenewDelegationTokenServlet, CancelDelegationTokenServlet and FsckServlet are accessed by end users, but hard-coded to use the internal SPNEGO filter.

If a name node HTTP server binds to multiple external IP addresses, the internal SPNEGO service principal name may not work with an address to which end users are connecting.  The current SPNEGO implementation in Hadoop is limited to use a single service principal per filter.

If the underlying hadoop kerberos authentication handler cannot easily be modified, we can at least create a separate auth filter for the end-user facing servlets so that their service principals can be independently configured. If not defined, it should fall back to the current behavior."
HADOOP-10150,Hadoop cryptographic file system,"There is an increasing need for securing data when Hadoop customers use various upper layer applications, such as Map-Reduce, Hive, Pig, HBase and so on.

HADOOP CFS (HADOOP Cryptographic File System) is used to secure data, based on HADOOP “FilterFileSystem” decorating DFS or other file systems, and transparent to upper layer applications. It’s configurable, scalable and fast.

High level requirements:
1.	Transparent to and no modification required for upper layer applications.
2.	“Seek”, “PositionedReadable” are supported for input stream of CFS if the wrapped file system supports them.
3.	Very high performance for encryption and decryption, they will not become bottleneck.
4.	Can decorate HDFS and all other file systems in Hadoop, and will not modify existing structure of file system, such as namenode and datanode structure if the wrapped file system is HDFS.
5.	Admin can configure encryption policies, such as which directory will be encrypted.
6.	A robust key management framework.
7.	Support Pread and append operations if the wrapped file system supports them."
HADOOP-10148,backport hadoop-10107 to branch-0.23,"Found this in [build #5440|https://builds.apache.org/job/PreCommit-HDFS-Build/5440/testReport/junit/org.apache.hadoop.hdfs.server.blockmanagement/TestUnderReplicatedBlocks/testSetrepIncWithUnderReplicatedBlocks/]

Caused by: java.lang.NullPointerException
	at org.apache.hadoop.ipc.Server.getNumOpenConnections(Server.java:2434)
	at org.apache.hadoop.ipc.metrics.RpcMetrics.numOpenConnections(RpcMetrics.java:74)"
HADOOP-10147,Upgrade to commons-logging 1.1.3 to avoid potential deadlock in MiniDFSCluster,"There is a deadlock in commons-logging 1.1.1 (see LOGGING-119) that can manifest itself while running {{MiniDFSCluster}} JUnit tests.

This deadlock has been fixed in commons-logging 1.1.2.  The latest version available is commons-logging 1.1.3, and Hadoop should upgrade to that in order to address this deadlock."
HADOOP-10146,Workaround JDK7 Process fd close bug,"JDK7's {{Process}} output streams have an async fd-close race bug.  This manifests as commands run via o.a.h.u.Shell causing threads to hang, OOM, or cause other bizarre behavior.  The NM is likely to encounter the bug under heavy load.

Specifically, {{ProcessBuilder}}'s {{UNIXProcess}} starts a thread to reap the process and drain stdout/stderr to avoid a lingering zombie process.  A race occurs if the thread using the stream closes it, the underlying fd is recycled/reopened, while the reaper is draining it.  {{ProcessPipeInputStream.drainInputStream}}'s will OOM allocating an array if {{in.available()}} returns a huge number, or may wreak havoc by incorrectly draining the fd.

"
HADOOP-10143,replace WritableFactories's hashmap with ConcurrentHashMap,"We observed a lock contend hotspot from a HBase cluster:

""IPC Reader 9 on port 12600"" daemon prio=10 tid=0x00007f85b8aceed0 nid=0x4be8 waiting for monitor entry [0x00007f8501c57000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.io.WritableFactories.getFactory(WritableFactories.java:44)
        - locked <0x00000007fd1328a8> (a java.lang.Class for org.apache.hadoop.io.WritableFactories)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:680)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)
        at org.apache.hadoop.hbase.client.MultiAction.readFields(MultiAction.java:116)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)
        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:126)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processData(SecureServer.java:618)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processOneRpc(SecureServer.java:596)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.saslReadAndProcess(SecureServer.java:362)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.readAndProcess(SecureServer.java:492)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:770)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:561)
        - locked <0x000000043da3fea0> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:536)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)



""IPC Reader 7 on port 12600"" daemon prio=10 tid=0x00007f85b8a99df0 nid=0x4be6 waiting for monitor entry [0x00007f8501cd9000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.io.WritableFactories.getFactory(WritableFactories.java:44)
        - locked <0x00000007fd1328a8> (a java.lang.Class for org.apache.hadoop.io.WritableFactories)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:680)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)
        at org.apache.hadoop.hbase.client.MultiAction.readFields(MultiAction.java:116)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)
        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:126)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processData(SecureServer.java:618)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processOneRpc(SecureServer.java:596)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.saslReadAndProcess(SecureServer.java:362)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.readAndProcess(SecureServer.java:492)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:770)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:561)
        - locked <0x000000043da232e8> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:536)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


""IPC Reader 5 on port 12600"" daemon prio=10 tid=0x00007f85b8a64d40 nid=0x4be2 runnable [0x00007f8501d5b000]
   java.lang.Thread.State: RUNNABLE
        at org.apache.hadoop.io.WritableFactories.getFactory(WritableFactories.java:44)
        - locked <0x00000007fd1328a8> (a java.lang.Class for org.apache.hadoop.io.WritableFactories)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:680)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)
        at org.apache.hadoop.hbase.client.Action.readFields(Action.java:103)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:586)
        at org.apache.hadoop.hbase.client.MultiAction.readFields(MultiAction.java:116)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)
        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:126)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processData(SecureServer.java:618)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.processOneRpc(SecureServer.java:596)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.saslReadAndProcess(SecureServer.java:362)
        at org.apache.hadoop.hbase.ipc.SecureServer$SecureConnection.readAndProcess(SecureServer.java:492)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:770)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:561)
        - locked <0x000000043da27300> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:536)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

All those three threads just wanted to get/read the factory, so to me,  it looks like a perfect use case for ConcurrentHashMap here."
HADOOP-10142,"Avoid groups lookup for unprivileged users such as ""dr.who""","Reduce the logs generated by ShellBasedUnixGroupsMapping.
For ex: Using WebHdfs from windows generates following log for each request

{noformat}2013-12-03 11:34:56,589 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user dr.who
org.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)
        at org.apache.hadoop.util.Shell.run(Shell.java:417)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)
        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
        at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)
        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)
        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
2013-12-03 11:34:56,590 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user dr.who{noformat}"
HADOOP-10141,Create an API to separate encryption key storage from applications,"As with the filesystem API, we need to provide a generic mechanism to support multiple key storage mechanisms that are potentially from third parties. 

An additional requirement for long term data lakes is to keep multiple versions of each key so that keys can be rolled periodically without requiring the entire data set to be re-written. Rolling keys provides containment in the event of keys being leaked.

Toward that end, I propose an API that is configured using a list of URLs of KeyProviders. The implementation will look for implementations using the ServiceLoader interface and thus support third party libraries.

Two providers will be included in this patch. One using the credentials cache in MapReduce jobs and the other using Java KeyStores from either HDFS or local file system. 

"
HADOOP-10140,Specification of HADOOP_CONF_DIR via the environment in hadoop_config.cmd,"It would be nice if HADOOP_CONF_DIR could be set in the environment like YARN_CONF_DIR. This could be done in lib-exec/hadoop.cmd by setting HADOOP_CONF_DIR conditionally.

Using the Windows command shell, the modification would be as follows:
if not defined HADOOP_CONF_DIR (
set HADOOP_CONF_DIR=%HADOOP_HOME%\etc\hadoop
)

This would allow the Hadoop configuration to be placed in ProgramData more easily. 
"
HADOOP-10139,Update and improve the Single Cluster Setup document,"The document should be understandable to a newcomer because the first place he will go is ""setup a single node""."
HADOOP-10135,writes to swift fs over partition size leave temp files and empty output file,"The OpenStack/swift filesystem produces incorrect output when the written objects exceed the configured partition size. After job completion, the expected files in the swift container have length == 0 and a collection of temporary files remain with names that appear to be URLs.

This can be replicated with teragen against the minicluster using the following command line:
bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar teragen 100000 swift://mycontainer.myservice/teradata

Where core-site.xml contains:
  <property>
    <name>fs.swift.impl</name>
    <value>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</value>
  </property>
  <property>
    <name>fs.swift.partsize</name>
    <value>1024</value>
  </property>
  <property>
    <name>fs.swift.service.myservice.auth.url</name>
    <value>https://auth.api.rackspacecloud.com/v2.0/tokens</value>
  </property>
  <property>
    <name>fs.swift.service.myservice.username</name>
    <value>[[your-cloud-username]]</value>
  </property>
  <property>
    <name>fs.swift.service.myservice.region</name>
    <value>DFW</value>
  </property>
  <property>
    <name>fs.swift.service.myservice.apikey</name>
    <value>[[your-api-key]]</value>
  </property>
  <property>
    <name>fs.swift.service.myservice.public</name>
    <value>true</value>
  </property>

Container ""mycontainer"" should have a collection of objects with names starting with ""teradata/part-m-00000"".  Instead, that file is empty and there is a collection of objects with names like ""swift://mycontainer.myservice/teradata/_temporary/0/_temporary/attempt_local415043862_0001_m_000000_0/part-m-00000/000010"""
HADOOP-10134,[JDK8] Fix Javadoc errors caused by incorrect or illegal tags in doc comments ,Javadoc is more strict by default in JDK8 and will error out on malformed or illegal tags found in doc comments. Although tagged as JDK8 all of the required changes are generic Javadoc cleanups.
HADOOP-10132,RPC#stopProxy() should log the class of proxy when IllegalArgumentException is encountered,"When investigating HBASE-10029, [~szetszwo] made the suggestion of logging the class of proxy when IllegalArgumentException is thrown."
HADOOP-10131,NetWorkTopology#countNumOfAvailableNodes() is returning wrong value if excluded nodes passed are not part of the cluster tree,"I got ""File /hdfs_COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation."" in the following case

1. 2 DNs cluster,
2. One of the datanodes was not responding from last 10 min, but about to detect as dead at NN.
3. Tried to write one file, for the block NN allocated both DNs.
4. Client While creating the pipeline took some time to detect one node failure.
5. Before client detects pipeline failure, NN side dead node was removed from cluster map.
6. Now, client has abandoned previous block and asked for new block with dead node in excluded list and got above exception even though one more node was available live.

When I dig this more, found that,
{{NetWorkTopology#countNumOfAvailableNodes()}} is not giving correct count when the excludeNodes passed from client are not part of the cluster map.


Adding to this one more case where count is wrong.
1. If there is no node present for the normalized scope in cluster."
HADOOP-10130,RawLocalFS::LocalFSFileInputStream.pread does not track FS::Statistics,RawLocalFS::LocalFSFileInputStream.pread does not track FS::Statistics
HADOOP-10129,Distcp may succeed when it fails,"Distcp uses {{IOUtils.cleanup}} to close its output streams w/o previously attempting to close the streams.  {{IOUtils.cleanup}} will swallow close or implicit flush on close exceptions.  As a result, distcp may silently skip files when a partial file listing is generated, and/or appear to succeed when individual copies fail."
HADOOP-10127,Add ipc.client.connect.retry.interval to control the frequency of connection retries,"Currently, {{ipc.Client}} client attempts to connect to the server every 1 second. It would be nice to make this configurable to be able to connect more/less frequently. Changing the number of retries alone is not granular enough."
HADOOP-10126,"LightWeightGSet log message is confusing : ""2.0% max memory = 2.0 GB""","Following message log message from LightWeightGSet is confusing.
{noformat}2013-11-21 18:00:21,198 INFO org.apache.hadoop.util.GSet: 2.0% max memory = 2.0 GB{noformat}, 
where 2GB is max JVM memory, but log message confuses like 2% of max memory is 2GB. 

It can be better like this
""2.0% of max memory 2.0 GB = 40.9 MB"""
HADOOP-10125,no need to process RPC request if the client connection has been dropped,"If the client has dropped the connection before the RPC is processed, RPC server doesn't need to process the RPC call. We have encountered issues where bad applications can bring down the NN. https://issues.apache.org/jira/i#browse/Hadoop-9640 tries to address that. When this occurs, NN's RPC queues are filled up with client requests and DN requests, sometimes we want to stop the flooding by stopping the bad applications and/or DNs. Some RPC processing like DatanodeProtocol::blockReport could take couple hundred milliseconds. So it is worthwhile to have NN skip the RPC calls if DNs have been stopped."
HADOOP-10121,Fix javadoc spelling for HadoopArchives#writeTopLevelDirs,"There's a misspelling at HadoopArchives.java. It should be fixed as follows: 

{code}
-  * @param parentPath the parent path that you wnat the archives
+  * @param parentPath the parent path that you want the archives
{code}"
HADOOP-10112,har file listing  doesn't work with wild card,"[test@test001 root]$ hdfs dfs -ls har:///tmp/filename.har/*
-ls: Can not create a Path from an empty string
Usage: hadoop fs [generic options] -ls [-d] [-h] [-R] [<path> ...]

It works without ""*"".
"
HADOOP-10111,Allow DU to be initialized with an initial value,"When a DU object is created, the du command runs right away. If the target directory contains a huge number of files and directories, its constructor may not return for many seconds.  It will be nice if it can be told to delay the initial scan and use a specified initial ""used"" value."
HADOOP-10110,hadoop-auth has a build break due to missing dependency,"We have a build break in hadoop-auth if build with maven cache cleaned. The error looks like the follows. The problem exists on both Windows and Linux. If you have old jetty jars in your maven cache, you won't see the error.

{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:29.469s
[INFO] Finished at: Mon Nov 18 12:30:36 PST 2013
[INFO] Final Memory: 37M/120M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:testCompile (default-testCompile) on project hadoop-auth: Compilation failure: Compilation failure:
[ERROR] /home/chuan/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[84,13] cannot access org.mortbay.component.AbstractLifeCycle
[ERROR] class file for org.mortbay.component.AbstractLifeCycle not found
[ERROR] server = new Server(0);
[ERROR] /home/chuan/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[94,29] cannot access org.mortbay.component.LifeCycle
[ERROR] class file for org.mortbay.component.LifeCycle not found
[ERROR] server.getConnectors()[0].setHost(host);
[ERROR] /home/chuan/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[96,10] cannot find symbol
[ERROR] symbol  : method start()
[ERROR] location: class org.mortbay.jetty.Server
[ERROR] /home/chuan/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java:[102,12] cannot find symbol
[ERROR] symbol  : method stop()
[ERROR] location: class org.mortbay.jetty.Server
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-auth

{noformat}"
HADOOP-10109,Fix test failure in TestOfflineEditsViewer introduced by HADOOP-10052,Fix test failure in TestOfflineEditsViewer introduced by HADOOP-10052
HADOOP-10107,Server.getNumOpenConnections may throw NPE,"Found this in [build #5440|https://builds.apache.org/job/PreCommit-HDFS-Build/5440/testReport/junit/org.apache.hadoop.hdfs.server.blockmanagement/TestUnderReplicatedBlocks/testSetrepIncWithUnderReplicatedBlocks/]

Caused by: java.lang.NullPointerException
	at org.apache.hadoop.ipc.Server.getNumOpenConnections(Server.java:2434)
	at org.apache.hadoop.ipc.metrics.RpcMetrics.numOpenConnections(RpcMetrics.java:74)"
HADOOP-10106,Incorrect thread name in RPC log messages,"INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647

This is thrown by a reader thread, so the message should be like

INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception org.apache.hadoop.ipc.RpcServerException: Unknown out of band call #-2147483647

Another example is Responder.processResponse, which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread."
HADOOP-10104,Update jackson to 1.9.13,"Jackson is now at 1.9.13, [apparently|http://mvnrepository.com/artifact/org.codehaus.jackson/jackson-core-asl], hadoop 2.2 at 1.8.8.

jackson isn't used that much in the code so risk from an update *should* be low

"
HADOOP-10103,update commons-lang to 2.6,update commons-lang from 2.5 to 2.6
HADOOP-10102,update commons IO from 2.1 to 2.4,"commons IO is at v2.4, release notes: http://commons.apache.org/proper/commons-io/upgradeto2_4.html

One change is marked as source & semantically incompatible with 2.2: [IO-318| https://issues.apache.org/jira/browse/IO-318]"
HADOOP-10100,MiniKDC shouldn't use apacheds-all artifact,"The MiniKDC currently depends on the {{apacheds-all}} artifact:
{code:xml}
    <dependency>
      <groupId>org.apache.directory.server</groupId>
      <artifactId>apacheds-all</artifactId>
      <version>2.0.0-M15</version>
      <scope>compile</scope>
    </dependency>
{code}

However, this artifact includes, inside of itself, a lot of other packages, including antlr, ehcache, apache commons, and mina (you can see a full list of the packages in the jar [here|http://mvnrepository.com/artifact/org.apache.directory.server/apacheds-all/2.0.0-M15]).  This can be problematic if other projects (e.g. Oozie) try to use MiniKDC and have a different version of one of those dependencies (in my case, ehcache).  Because the packages are included inside the {{apacheds-all}} jar, we can't override their version.  

Instead, we should remove {{apacheds-all}} and use dependencies that only include org.apache.directory.* packages; the other necessary dependencies should be included normally."
HADOOP-10095,Performance improvement in CodecPool,"CodecPool shows up when profiling HBase with a mixed workload (it says we spend 1% of the time there).

It could be a profiler side effect, but on the other hand we can save some 'Map#contains'."
HADOOP-10094,NPE in GenericOptionsParser#preProcessForWindows(),"main() in java guarantees that args is not null, but on some uses of Tool interface from java, people seem to pass around null as args, causing a NPE in GenericOptionsParser. 

Although, passing null is not recommended, we can do a trivial fix. "
HADOOP-10093,hadoop-env.cmd sets HADOOP_CLIENT_OPTS with a max heap size that is too small.,HADOOP-9211 increased the default max heap size set by hadoop-env.sh to 512m.  The same change needs to be applied to hadoop-env.cmd for Windows.
HADOOP-10090,Jobtracker metrics not updated properly after execution of a mapreduce job,"After executing a wordcount mapreduce sample job, jobtracker metrics are not updated properly. Often times the response from the jobtracker has higher number of job_completed than job_submitted (for example 8 jobs completed and 7 jobs submitted). 

Issue reported by Toma Paunovic."
HADOOP-10088,copy-nativedistlibs.sh needs to quote snappy lib dir,copy-nativedistlibs.sh needs to quote snappy lib dir. Currently this fails for directories with 'spaces' or 'windows path seperartor'
HADOOP-10087,UserGroupInformation.getGroupNames() fails to return primary group first when JniBasedUnixGroupsMappingWithFallback is used,"When JniBasedUnixGroupsMappingWithFallback is used as the group mapping resolution provider, UserGroupInformation.getGroupNames() fails to return the primary group first in the list as documented."
HADOOP-10086,User document for authentication in secure cluster,"There are no independent section for basic security features such as authentication and group mapping in the user documentation, though there are sections for ""Service Level Authorization"" and ""HTTP Authentication"".
Creating independent section for authentication and moving contents about secure cluster currently residing in ""Cluster Setup"" section could be good starting point.
"
HADOOP-10085,CompositeService should allow adding services while being inited,"We can add services to a CompositeService. However, if we do that while initing the CompositeService, it leads to a ConcurrentModificationException.

It would be nice to allow adding services even during the init of CompositeService."
HADOOP-10081,Client.setupIOStreams can leak socket resources on exception or error,"The setupIOStreams method in org.apache.hadoop.ipc.Client can leak socket resources if an exception is thrown before the inStream and outStream local variables are assigned to this.in and this.out, respectively.  "
HADOOP-10079,log a warning message if group resolution takes too long.,We should log a warning message if group resolution takes too long.
HADOOP-10078,KerberosAuthenticator always does SPNEGO,"HADOOP-8883 made this change to {{KerberosAuthenticator}}
{code:java}
@@ -158,7 +158,7 @@ public class KerberosAuthenticator implements Authenticator {
       conn.setRequestMethod(AUTH_HTTP_METHOD);
       conn.connect();
       
-      if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) {
+      if (conn.getRequestProperty(AUTHORIZATION) != null && conn.getResponseCode() == HttpURLConnection.HTTP_OK) {
         LOG.debug(""JDK performed authentication on our behalf."");
         // If the JDK already did the SPNEGO back-and-forth for
         // us, just pull out the token.
{code}
to fix OOZIE-1010.  However, as [~aklochkov] pointed out recently, this inadvertently made the if statement always false because it turns out that the JDK excludes some headers, including the ""Authorization"" one that we're checking (see discussion [here|https://issues.apache.org/jira/browse/HADOOP-8883?focusedCommentId=13807596&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13807596]).  This means that it was always either calling {{doSpnegoSequence(token);}} or {{getFallBackAuthenticator().authenticate(url, token);}}, which is actually the old behavior that existed before HADOOP-8855 changed it in the first place.

In any case, I tried removing the ""Authorization"" check and Oozie still works with and without Kerberos; the NPE reported in OOZIE-1010 has since been properly fixed due as a side effect for a similar issue in OOZIE-1368."
HADOOP-10072,TestNfsExports#testMultiMatchers fails due to non-deterministic timing around cache expiry check.,{{TestNfsExports#testMultiMatchers}} has been failing sporadically in my environment.  The final step of this test is to check that a cache entry has expired.  It looks like the timing is too tight around this check.
HADOOP-10070,RPC client doesn't use per-connection conf to determine server's expected Kerberos principal name,"Currently, RPC client caches the {{Configuration}} object that was passed in to its constructor and uses that same conf for every connection it sets up thereafter. This can cause problems when security is enabled if the {{Configuration}} object provided when the first RPC connection was made does not contain all possible entries for all server principals that will later be used by subsequent connections. When this happens, it will result in later RPC connections incorrectly failing with the error ""Failed to specify server's Kerberos principal name"" even though the principal name was specified in the {{Configuration}} object provided on later RPC connection attempts.

I believe this means that we've inadvertently reintroduced HADOOP-6907."
HADOOP-10067,Missing POM dependency on jsr305,Compiling for Fedora revels a missing declaration for javax.annotation.Nullable.  This is the result of a missing explicit dependency on jsr305.
HADOOP-10064,Upgrade to maven antrun plugin version 1.7,"v1.6 does not respect 'mvn -q'. 

I have been building with 1.7 on my dev machine and haven't encountered any problems so far."
HADOOP-10062,race condition in MetricsSystemImpl#publishMetricsNow that causes incorrect results,"TestMetricsSystemInpl#testMultiThreadedPublish failed with ""Metrics not collected""

{code}
Running org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl
Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.688 sec <<< FAILURE! - in org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl
testMultiThreadedPublish(org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl)  Time elapsed: 0.056 sec  <<< FAILURE!
java.lang.AssertionError: Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Passed
        at org.junit.Assert.fail(Assert.java:93)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testMultiThreadedPublish(TestMetricsSystemImpl.java:232)


Results :

Failed tests:
  TestMetricsSystemImpl.testMultiThreadedPublish:232 Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Metric not collected!
Passed

Tests run: 6, Failures: 1, Errors: 0, Skipped: 0

{code}"
HADOOP-10059,RPC authentication and authorization metrics overflow to negative values on busy clusters,The RPC metrics for authorization and authentication successes can easily overflow to negative values on a busy cluster that has been up for a long time.  We should consider providing 64-bit values for these counters.
HADOOP-10058,TestMetricsSystemImpl#testInitFirstVerifyStopInvokedImmediately fails on trunk,"Here is the output when I executed ""mvn test -Dtest=TestMetricsSystemImpl"".

{code}
Failed tests: 
  TestMetricsSystemImpl.testInitFirstVerifyStopInvokedImmediately:114 
Wanted at most 2 times but was 3

Tests run: 6, Failures: 1, Errors: 0, Skipped: 0
{code}

I found the test doesn't always fail. The test sometimes success."
HADOOP-10055,"FileSystemShell.apt.vm doc has typo ""numRepicas"" ","HDFS-5139 added ""numRepicas"" to FileSystemShell.apt.vm, should be ""numReplicas""."
HADOOP-10052,Temporarily disable client-side symlink resolution,"As a follow-on to the JIRA that disabled creation of symlinks on the server-side, we should also disable client-side resolution so old clients talking to a new server behave properly."
HADOOP-10047,Add a directbuffer Decompressor API to hadoop,"With the Zero-Copy reads in HDFS (HDFS-5260), it becomes important to perform all I/O operations without copying data into byte[] buffers or other buffers which wrap over them.

This is a proposal for adding a DirectDecompressor interface to the io.compress, to indicate codecs which want to surface the direct buffer layer upwards.

The implementation should work with direct heap/mmap buffers and cannot assume .array() availability."
HADOOP-10046,Print a log message when SSL is enabled,It would be nice to have a log message that indicates that SSL is enabled in org.apache.hadoop.http.HttpServer.
HADOOP-10044,Improve the javadoc of rpc code,
HADOOP-10040,hadoop.cmd in UNIX format and would not run by default on Windows,"The hadoop.cmd currently checked in into hadoop-common is in UNIX format, same as most of other src files. However, the hadoop.cmd is meant to be used on Windows only, the fact that it is in UNIX format makes it unrunnable as is on Window platform.

An exception shall be made on hadoop.cmd (and other cmd files for what matters) to make sure they are in DOS format, for them to be runnable as is when checked out from source repository."
HADOOP-10039,Add Hive to the list of projects using AbstractDelegationTokenSecretManager,org.apache.hadoop.hive.thrift.DelegationTokenSecretManager extends AbstractDelegationTokenSecretManager. This should be captured in the InterfaceAudience annotation of AbstractDelegationTokenSecretManager.
HADOOP-10037,"s3n read truncated, but doesn't throw exception ","For months now we've been finding that we've been experiencing frequent data truncation issues when reading from S3 using the s3n:// protocol.  I finally was able to gather some debugging output on the issue in a job I ran last night, and so can finally file a bug report.


The job I ran last night was on a 16-node cluster (all of them AWS EC2 cc2.8xlarge machines, running Ubuntu 13.04 and Cloudera CDH4.3.0).  The job was a Hadoop streaming job, which reads through a large number (i.e., ~55,000) of files on S3, each of them approximately 300K bytes in size.

All of the files contain 46 columns of data in each record.  But I added in an extra check in my mapper code to count and verify the number of columns in every record - throwing an error and crashing the map task if the column count is wrong.

If you look in the attached task logs, you'll see 2 attempts on the same task.  The first one fails due to data truncated (i.e., my job intentionally fails the map task due to the current record failing the column count check).  The task then gets retried on a different machine and runs to a succesful completion.

You can see further evidence of the truncation further down in the task logs, where it displays the count of the records read:  the failed task says 32953 records read, while the successful task says 63133.

Any idea what the problem might be here and/or how to work around it?  This issue is a very common occurrence on our clusters.  E.g., in the job I ran last night before I had gone to bed I had already encountered 8 such failuers, and the job was only 10% complete.  (~25,000 out of ~250,000 tasks.)

I realize that it's common for I/O errors to occur - possibly even frequently - in a large Hadoop job.  But I would think that if an I/O failure (like a truncated read) did occur, that something in the underlying infrastructure code (i.e., either in NativeS3FileSystem or in jets3t) should detect the error and throw an IOException accordingly.  It shouldn't be up to the calling code to detect such failures, IMO."
HADOOP-10031,FsShell -get/copyToLocal/moveFromLocal should support Windows local path,"In current trunk code, running 'hadoop fs -get /hdfs/path c:\windows\path' will lead to an error put: unexpected URISyntaxException. This is a regression to the 1.0 fs shell commands. FsShell get, copyToLocal, and moveFromLocal should support Windows local path formats as localSrc argument to the two commands."
HADOOP-10030,FsShell -put/copyFromLocal should support Windows local path,"In current trunk code, running 'hadoop fs -put c:\windows\path /hdfs/path' will lead to an error {{put: unexpected URISyntaxException}}. This is a regression to the 1.0 fs shell commands. FsShell put and copyFromLocal should support Windows local path formats as localSrc argument to the two commands."
HADOOP-10029,Specifying har file to MR job fails in secure cluster,This is an issue found by [~rramya]. See the exception stack trace in the following comment.
HADOOP-10028,Malformed ssl-server.xml.example ,"The ssl-server.xml.example file has malformed XML leading to DN start error if the example file is reused.

{code}
2013-10-07 16:52:01,639 FATAL conf.Configuration (Configuration.java:loadResource(2151)) - error parsing conf ssl-server.xml
org.xml.sax.SAXParseException: The element type ""description"" must be terminated by the matching end-tag ""</description>"".
        at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:249)
        at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)
        at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:153)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:1989)
{code}"
HADOOP-10020,disable symlinks temporarily,disable symlinks temporarily until we can make them production-ready in Hadoop 2.3
HADOOP-10017,Fix NPE in DFSClient#getDelegationToken when doing Distcp from a secured cluster to an insecured cluster,"Currently if we run Distcp from a secured cluster and copy data to an insecured cluster, DFSClient#getDelegationToken will throw NPE when processing the NULL token returned by the NN in the insecured cluster. We should be able to handle the NULL token here. "
HADOOP-10015,UserGroupInformation prints out excessive ERROR warnings,"In UserGroupInformation::doAs(), it prints out a log at ERROR level whenever it catches an exception.

However, it prints benign warnings in the following paradigm:

{noformat}
 try {
        ugi.doAs(new PrivilegedExceptionAction<FileStatus>() {
          @Override
          public FileStatus run() throws Exception {
            return fs.getFileStatus(nonExist);
          }
        });
      } catch (FileNotFoundException e) {
      }
{noformat}

For example, FileSystem#exists() follows this paradigm. Distcp uses this paradigm too. The exception is expected therefore there should be no ERROR logs printed in the namenode logs.

Currently, the user quickly finds out that the namenode log is quickly filled by _benign_ ERROR logs when he or she runs distcp in secure set up. This behavior confuses the operators.

This jira proposes to move the log to DEBUG level."
HADOOP-10012,Secure Oozie jobs fail with delegation token renewal exception in Namenode HA setup,
HADOOP-10011,NPE if the system can't determine its own name and you go DNS.getDefaultHost(null),"In a test case that I am newly writing, on my infamous ""home machine with broken DNS"", I cant call getByName(null) without seeing a stack trace:
Testcase: testNullInterface took 0.014 sec
	Caused an ERROR
null
java.lang.NullPointerException
	at java.net.NetworkInterface.getByName(NetworkInterface.java:226)
	at org.apache.hadoop.net.DNS.getIPs(DNS.java:94)
	at org.apache.hadoop.net.DNS.getHosts(DNS.java:141)
	at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:218)
	at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:235)
	at org.apache.hadoop.net.TestDNS.testNullInterface(TestDNS.java:62)"
HADOOP-10006,Compilation failure in trunk for o.a.h.fs.swift.util.JSONUtil,"The error is like following:
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-openstack: Compilation failure: Compilation failure:
[ERROR] /home/jdu/bdc/hadoop-trunk/hadoop-common/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/JSONUtil.java:[97,33] type parameters of <T>T cannot be determined; no unique maximal instance exists for type variable T with upper bounds T,java.lang.Object
[ERROR] /home/jdu/bdc/hadoop-trunk/hadoop-common/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/JSONUtil.java:[115,33] type parameters of <T>T cannot be determined; no unique maximal instance exists for type variable T with upper bounds T,java.lang.Object
"
HADOOP-10005,No need to check INFO severity level is enabled or not,"As a convention in developers, INFO is the default level and INFO logs should be always available. So no need to check it for most of the cases."
HADOOP-10003,HarFileSystem.listLocatedStatus() fails,"It looks like HarFileSystem.listLocatedStatus() doesn't work properly because it is inheriting FilterFileSystem's implementation.  This is causing archive unit tests to fail in Hive when using hadoop 2.1.1.

If HarFileSystem overrides listLocatedStatus() to use FileSystem's implementation, the Hive unit tests pass."
HADOOP-9998,Provide methods to clear only part of the DNSToSwitchMapping,"After HDFS-4521, once a DN is registered with invalid networktopology, all cached rack info in DNSToSwitchMapping will be cleared. We should only clear cache on specific nodes."
HADOOP-9992,Modify the NN loadGenerator to optionally run as a MapReduce job,
HADOOP-9989,"Bug introduced in HADOOP-9374, which parses the -tokenCacheFile as binary file but set it to the configuration as JSON file.","The code in JIRA HADOOP-9374's patch introduced a bug, where the value of the 
tokenCacheFile parameter is being parsed as a binary file and set it to the
mapreduce.job.credentials.json parameter in GenericOptionsParser, which cannot be parsed by JobSubmitter when it gets the value."
HADOOP-9982,Fix dead links in hadoop site docs,"For example, the hyperlink 'Single Node Setup' doesn't work correctly in ['Cluster Setup' document|http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html].
I also found other dead links. I'll try to fix them."
HADOOP-9981,globStatus should minimize its listStatus and getFileStatus calls,"After HADOOP-9652, listStatus() or globStatus() calls against a local file system directory is very slow.  A user was loading data from local file system to Hive and it took about 30 seconds. The same operation took less than a second pre-HADOOP-9652. 

The input path had many other files beside the input files and strace showed that fork & exec of stat against each and every one of them. jstack confirmed that this was being done from getNativeFileLinkStatus().

"
HADOOP-9977,Hadoop services won't start with different keypass and keystorepass when https is enabled,"Enable ssl in the configuration. While creating keystore, give different keypass and keystore password. (here, keypass = hadoop and storepass=hadoopKey)

keytool -genkey -alias host1 -keyalg RSA -keysize 1024 -dname ""CN=host1,OU=cm,O=cm,L=san jose,ST=ca,C=us"" -keypass hadoop -keystore keystore.jks -storepass hadoopKey

In , ssl-server.xml set below two properties.
<property><name>ssl.server.keystore.keypassword</name><value>hadoop</value></property>
<property><name>ssl.server.keystore.password</name><value>hadoopKey</value></property>

Namenode, ResourceManager, Datanode, Nodemanager, SecondaryNamenode fails to start with below error.

2013-09-17 21:39:00,794 FATAL namenode.NameNode (NameNode.java:main(1325)) - Exception in namenode join
java.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key
        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)
        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)
Caused by: java.security.UnrecoverableKeyException: Cannot recover key
        at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)
        at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)
        at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)
        at java.security.KeyStore.getKey(KeyStore.java:792)
        at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)
        at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)
        at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:259)
        at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)
        at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)
        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)
        ... 9 more"
HADOOP-9976,Different versions of avro and avro-maven-plugin,"Post HADOOP-9672, the versions for avro and avro-maven-plugin are different - 1.7.4 and 1.5.3 respectively. "
HADOOP-9974,Maven OutOfMemory exception when building with protobuf 2.5.0,"Recently Hadoop upgraded to use Protobuf 2.5.0. To build the trunk, I updated my installed Protobuf 2.5.0. With this upgrade, I didn't encounter the build failure due to protoc, but failed when building HDFS sub-project. Bellow is failure message. I'm using Mac OS X.

{code}
INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................ SUCCESS [1.075s]
[INFO] Apache Hadoop Project POM ......................... SUCCESS [0.805s]
[INFO] Apache Hadoop Annotations ......................... SUCCESS [2.283s]
[INFO] Apache Hadoop Assemblies .......................... SUCCESS [0.343s]
[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [1.913s]
[INFO] Apache Hadoop Maven Plugins ....................... SUCCESS [2.390s]
[INFO] Apache Hadoop Auth ................................ SUCCESS [2.597s]
[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [1.868s]
[INFO] Apache Hadoop Common .............................. SUCCESS [55.798s]
[INFO] Apache Hadoop NFS ................................. SUCCESS [3.549s]
[INFO] Apache Hadoop MiniKDC ............................. SUCCESS [1.788s]
[INFO] Apache Hadoop Common Project ...................... SUCCESS [0.044s]
[INFO] Apache Hadoop HDFS ................................ FAILURE [25.219s]
[INFO] Apache Hadoop HttpFS .............................. SKIPPED
[INFO] Apache Hadoop HDFS BookKeeper Journal ............. SKIPPED
[INFO] Apache Hadoop HDFS-NFS ............................ SKIPPED
[INFO] Apache Hadoop HDFS Project ........................ SKIPPED
[INFO] hadoop-yarn ....................................... SKIPPED
[INFO] hadoop-yarn-api ................................... SKIPPED
[INFO] hadoop-yarn-common ................................ SKIPPED
[INFO] hadoop-yarn-server ................................ SKIPPED
[INFO] hadoop-yarn-server-common ......................... SKIPPED
[INFO] hadoop-yarn-server-nodemanager .................... SKIPPED
[INFO] hadoop-yarn-server-web-proxy ...................... SKIPPED
[INFO] hadoop-yarn-server-resourcemanager ................ SKIPPED
[INFO] hadoop-yarn-server-tests .......................... SKIPPED
[INFO] hadoop-yarn-client ................................ SKIPPED
[INFO] hadoop-yarn-applications .......................... SKIPPED
[INFO] hadoop-yarn-applications-distributedshell ......... SKIPPED
[INFO] hadoop-mapreduce-client ........................... SKIPPED
[INFO] hadoop-mapreduce-client-core ...................... SKIPPED
[INFO] hadoop-yarn-applications-unmanaged-am-launcher .... SKIPPED
[INFO] hadoop-yarn-site .................................. SKIPPED
[INFO] hadoop-yarn-project ............................... SKIPPED
[INFO] hadoop-mapreduce-client-common .................... SKIPPED
[INFO] hadoop-mapreduce-client-shuffle ................... SKIPPED
[INFO] hadoop-mapreduce-client-app ....................... SKIPPED
[INFO] hadoop-mapreduce-client-hs ........................ SKIPPED
[INFO] hadoop-mapreduce-client-jobclient ................. SKIPPED
[INFO] hadoop-mapreduce-client-hs-plugins ................ SKIPPED
[INFO] Apache Hadoop MapReduce Examples .................. SKIPPED
[INFO] hadoop-mapreduce .................................. SKIPPED
[INFO] Apache Hadoop MapReduce Streaming ................. SKIPPED
[INFO] Apache Hadoop Distributed Copy .................... SKIPPED
[INFO] Apache Hadoop Archives ............................ SKIPPED
[INFO] Apache Hadoop Rumen ............................... SKIPPED
[INFO] Apache Hadoop Gridmix ............................. SKIPPED
[INFO] Apache Hadoop Data Join ........................... SKIPPED
[INFO] Apache Hadoop Extras .............................. SKIPPED
[INFO] Apache Hadoop Pipes ............................... SKIPPED
[INFO] Apache Hadoop Tools Dist .......................... SKIPPED
[INFO] Apache Hadoop Tools ............................... SKIPPED
[INFO] Apache Hadoop Distribution ........................ SKIPPED
[INFO] Apache Hadoop Client .............................. SKIPPED
[INFO] Apache Hadoop Mini-Cluster ........................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:40.880s
[INFO] Finished at: Thu Aug 15 16:02:56 PDT 2013
[INFO] Final Memory: 49M/123M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure
[ERROR] Failure executing javac, but could not parse the error:
[ERROR] 
[ERROR] 
[ERROR] The system is out of resources.
[ERROR] Consult the following stack trace for details.
[ERROR] java.lang.OutOfMemoryError: Java heap space
[ERROR] at com.sun.tools.javac.code.Scope$ImportScope.makeEntry(Scope.java:385)
[ERROR] at com.sun.tools.javac.code.Scope.enter(Scope.java:196)
[ERROR] at com.sun.tools.javac.code.Scope.enter(Scope.java:183)
[ERROR] at com.sun.tools.javac.comp.MemberEnter.importAll(MemberEnter.java:132)
[ERROR] at com.sun.tools.javac.comp.MemberEnter.visitTopLevel(MemberEnter.java:509)
[ERROR] at com.sun.tools.javac.tree.JCTree$JCCompilationUnit.accept(JCTree.java:446)
[ERROR] at com.sun.tools.javac.comp.MemberEnter.memberEnter(MemberEnter.java:387)
[ERROR] at com.sun.tools.javac.comp.MemberEnter.complete(MemberEnter.java:819)
[ERROR] at com.sun.tools.javac.code.Symbol.complete(Symbol.java:384)
[ERROR] at com.sun.tools.javac.code.Symbol$ClassSymbol.complete(Symbol.java:766)
[ERROR] at com.sun.tools.javac.comp.Enter.complete(Enter.java:464)
[ERROR] at com.sun.tools.javac.comp.Enter.main(Enter.java:442)
[ERROR] at com.sun.tools.javac.main.JavaCompiler.enterTrees(JavaCompiler.java:822)
[ERROR] at com.sun.tools.javac.main.JavaCompiler.compile(JavaCompiler.java:727)
[ERROR] at com.sun.tools.javac.main.Main.compile(Main.java:353)
[ERROR] at com.sun.tools.javac.main.Main.compile(Main.java:279)
[ERROR] at com.sun.tools.javac.main.Main.compile(Main.java:270)
[ERROR] at com.sun.tools.javac.Main.compile(Main.java:87)
[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[ERROR] at java.lang.reflect.Method.invoke(Method.java:597)
[ERROR] at org.codehaus.plexus.compiler.javac.JavacCompiler.compileInProcess0(JavacCompiler.java:551)
[ERROR] at org.codehaus.plexus.compiler.javac.JavacCompiler.compileInProcess(JavacCompiler.java:526)
[ERROR] at org.codehaus.plexus.compiler.javac.JavacCompiler.compile(JavacCompiler.java:167)
[ERROR] at org.apache.maven.plugin.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:678)
[ERROR] at org.apache.maven.plugin.CompilerMojo.execute(CompilerMojo.java:128)
[ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-hdfs
{code}"
HADOOP-9968,ProxyUsers does not work with NetGroups,"It is possible to use NetGroups for ACLs. This requires specifying  the config property hadoop.security.group.mapping as  org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping or org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping.

The authorization to proxy a user by another user is specified as a list of groups hadoop.proxyuser.<user-name>.groups. The Group resolution does not work  if we are using NetGroups."
HADOOP-9964,O.A.H.U.ReflectionUtils.printThreadInfo() is not thread-safe which cause TestHttpServer pending 10 minutes or longer.,The printThreadInfo() in ReflectionUtils is not thread-safe which cause two or more threads calling this method from StackServlet to deadlock. 
HADOOP-9962,in order to avoid dependency divergence within Hadoop itself lets enable DependencyConvergence,In order to avoid the likes of HADOOP-9961 it may be useful for us to enable DependencyConvergence check in  maven-enforcer-plugin.
HADOOP-9961,versions of a few transitive dependencies diverged between hadoop subprojects,"I've noticed a few divergences between secondary dependencies of the various hadoop subprojects. For example:
{noformat}
[ERROR]
Dependency convergence error for org.apache.commons:commons-compress:1.4.1 paths to dependency are:
+-org.apache.hadoop:hadoop-client:3.0.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:3.0.0-20130913.204420-3360
    +-org.apache.avro:avro:1.7.4
      +-org.apache.commons:commons-compress:1.4.1
and
+-org.apache.hadoop:hadoop-client:3.0.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:3.0.0-20130913.204420-3360
    +-org.apache.commons:commons-compress:1.4
{noformat}"
HADOOP-9960,Upgrade Jersey version to 1.9,HCatalog uses version 1.14 of Jersery and cannot use version 1.8 which hadoop uses. Hcatalog requires at least 1.9 due to this issue: https://java.net/jira/browse/JERSEY-721
HADOOP-9958,Add old constructor back to DelegationTokenInformation to unbreak downstream builds,"HDFS-4680 added an argument to the constructor of DelegationTokenInformation, which is an incompatible change for downstreams. Let's add the old one back in.

See: HIVE-5281"
HADOOP-9956,RPC listener inefficiently assigns connections to readers,"The socket listener and readers use a complex synchronization to update the reader's NIO {{Selector}}.  Updating active selectors is not thread-safe so precautions are required.

However, the current locking choreography results in a serialized distribution of new connections to the parallel socket readers.  A slower/busier reader can stall the listener and throttle performance.

The problem manifests as unexpectedly low cpu utilization by the listener and readers (~20-30%) under heavy load.  The call queue is shallow when it should be overflowing."
HADOOP-9955,RPC idle connection closing is extremely inefficient,"The RPC server listener loops accepting connections, distributing the new connections to socket readers, and then conditionally & periodically performs a scan for idle connections.  The idle scan choses a _random index range_ to scan in a _synchronized linked list_.

With 20k+ connections, walking the range of indices in the linked list is extremely expensive.  During the sweep, other threads (socket responder and readers) that want to close connections are blocked, and no new connections are being accepted."
HADOOP-9954,Hadoop 2.0.5 doc build failure - OutOfMemoryError exception,"When run hadoop build with command line options:
{code}
mvn package -Pdist,native,docs -DskipTests -Dtar 
{code}

Build failed adn OutOfMemoryError Exception is thrown:
{code}
[INFO] --- maven-source-plugin:2.1.2:test-jar (default) @ hadoop-hdfs ---
[INFO] 
[INFO] --- findbugs-maven-plugin:2.3.2:findbugs (default) @ hadoop-hdfs ---
[INFO] ****** FindBugsMojo execute *******
[INFO] canGenerate is true
[INFO] ****** FindBugsMojo executeFindbugs *******
[INFO] Temp File is /var/lib/jenkins/workspace/Hadoop-Client-2.0.5-T-RPM/rpms/hadoop-devel.x86_64/BUILD/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/findbugsTemp.xml
[INFO] Fork Value is true
     [java] Out of memory
     [java] Total memory: 477M
     [java]  free memory: 68M
     [java] Analyzed: /var/lib/jenkins/workspace/Hadoop-Client-2.0.5-T-RPM/rpms/hadoop-devel.x86_64/BUILD/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/target/classes
     [java]      Aux: /home/henkins-service/.m2/repository/org/codehaus/mojo/findbugs-maven-plugin/2.3.2/findbugs-maven-plugin-2.3.2.jar
     [java]      Aux: /home/henkins-service/.m2/repository/com/google/code/findbugs/bcel/1.3.9/bcel-1.3.9.jar
 ...
     [java]      Aux: /home/henkins-service/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar
     [java] Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
     [java] 	at java.util.HashMap.<init>(HashMap.java:226)
     [java] 	at edu.umd.cs.findbugs.ba.deref.UnconditionalValueDerefSet.<init>(UnconditionalValueDerefSet.java:68)
     [java] 	at edu.umd.cs.findbugs.ba.deref.UnconditionalValueDerefAnalysis.createFact(UnconditionalValueDerefAnalysis.java:650)
     [java] 	at edu.umd.cs.findbugs.ba.deref.UnconditionalValueDerefAnalysis.createFact(UnconditionalValueDerefAnalysis.java:82)
     [java] 	at edu.umd.cs.findbugs.ba.BasicAbstractDataflowAnalysis.getFactOnEdge(BasicAbstractDataflowAnalysis.java:119)
     [java] 	at edu.umd.cs.findbugs.ba.AbstractDataflow.getFactOnEdge(AbstractDataflow.java:54)
     [java] 	at edu.umd.cs.findbugs.ba.npe.NullDerefAndRedundantComparisonFinder.examineNullValues(NullDerefAndRedundantComparisonFinder.java:297)
     [java] 	at edu.umd.cs.findbugs.ba.npe.NullDerefAndRedundantComparisonFinder.execute(NullDerefAndRedundantComparisonFinder.java:150)
     [java] 	at edu.umd.cs.findbugs.detect.FindNullDeref.analyzeMethod(FindNullDeref.java:278)
     [java] 	at edu.umd.cs.findbugs.detect.FindNullDeref.visitClassContext(FindNullDeref.java:205)
     [java] 	at edu.umd.cs.findbugs.DetectorToDetector2Adapter.visitClass(DetectorToDetector2Adapter.java:68)
     [java] 	at edu.umd.cs.findbugs.FindBugs2.analyzeApplication(FindBugs2.java:979)
     [java] 	at edu.umd.cs.findbugs.FindBugs2.execute(FindBugs2.java:230)
     [java] 	at edu.umd.cs.findbugs.FindBugs.runMain(FindBugs.java:348)
     [java] 	at edu.umd.cs.findbugs.FindBugs2.main(FindBugs2.java:1057)
     [java] Java Result: 1
[INFO] No bugs found

{code}"
HADOOP-9948,Add a config value to CLITestHelper to skip tests on Windows,"We want to add a configuration to CLITestHelper so we can skip some tests on Windows. We want this feature because some test cases in TestHDFSCLI should not be tested on Windows. Or more specifically, the globbing test cases. The differences are explained in HDFS-4632. The proposed syntax looks like follows.

{noformat}
<test> <!-- TESTED -->
      <description>ls: Negative test for quoted /*/* globbing </description>
      <windows>false</windows>
{noformat}

When Windows set to false, we will skip running the test on Windows."
HADOOP-9945,HAServiceState should have a state for stopped services,"HAServiceState, currently, has states for Initializing, Active, and Standby. 

For RM HA, it is useful to have another state ""STOPPING"". This is because YARN has a separate service model where Services are stopped and can't be restarted. When the Active/Standby RM is stopped, it can no longer go back to the Active/Standby state; setting the HAServiceState to Initializing is misleading."
HADOOP-9944,RpcRequestHeaderProto defines callId as uint32 while ipc.Client.CONNECTION_CONTEXT_CALL_ID is signed (-3),RpcRequestHeaderProto defines callId as uint32 while ipc.Client.CONNECTION_CONTEXT_CALL_ID is signed (-3).
HADOOP-9935,set junit dependency to test scope,"junit should be set to scope test in hadoop-mapreduce-project and hadoop-yarn-project. This patch will fix the problem, that hadoop always pulls in its own version of junit and that junit is even included in the tarballs."
HADOOP-9932,Improper synchronization in RetryCache,"In LightWeightCache#evictExpiredEntries(), the precondition check can fail. [~patwhitey2007] ran a HA failover test and it occurred while the SBN was catching up with edits during a transition to active. This caused NN to terminate.

Here is my theory: If an RPC handler calls waitForCompletion() and it happens to remove the head of the queue in get(), it will race with evictExpiredEntries() frrom put()."
HADOOP-9929,Insufficient permissions for a path reported as file not found,"Using ""hadoop fs -ls"" to list a path where the permissions of a parent directory are insufficient ends up reporting ""no such file or directory"" on the full path rather than reporting the permission issue.  For example:

{noformat}
$ hadoop fs -ls /user/abc/tests/data
ls: `/user/abc/tests/data': No such file or directory
$ hadoop fs -ls /user/abc
ls: Permission denied: user=somebody, access=READ_EXECUTE, inode=""/user/abc"":abc:hdfs:drwx------
{noformat}"
HADOOP-9924,FileUtil.createJarWithClassPath() does not generate relative classpath correctly,"On Windows, FileUtil.createJarWithClassPath() is called to generate a manifest jar file to pack classpath - to avoid the problem of classpath being too long.
However, the relative classpath is not handled correctly. It relies on Java's File(relativePath) to resolve the relative path. But it really should be using the given pwd parameter to resolve the relative path.

To reproduce this bug, you can try some pig job on Windows, it will fail and the pig log on the application master will look like this:

2013-08-29 23:25:55,498 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat not found
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat not found

This is because the PigOutputFormat class is in the job.jar file but the classpath manifest has:
file:/c:/apps/dist/hadoop-2.1.0-beta/bin/job.jar/job.jar
When it really should be:
file:/<job container folder>/job.jar/job.jar"
HADOOP-9922,hadoop windows native build will fail in 32 bit machine,Building Hadoop in windows 32 bit machine fails as native project is not having Win32 configuration
HADOOP-9921,daemon scripts should remove pid file on stop call after stop or process is found not running,"daemon scripts should remove the pid file on stop call using daemon script.

Should remove the pid file, even though process is not running.

same pid file will be used by start command. At that time, if the same pid is assigned to some other process, then start may fail."
HADOOP-9919,Update hadoop-metrics2.properties examples to Yarn,"The config for JobTracker and TaskTracker (comment outed) still exists in hadoop-metrics2.properties as follows:

{code}
#jobtracker.sink.file_jvm.context=jvm
#jobtracker.sink.file_jvm.filename=jobtracker-jvm-metrics.out
#jobtracker.sink.file_mapred.context=mapred
#jobtracker.sink.file_mapred.filename=jobtracker-mapred-metrics.out

#tasktracker.sink.file.filename=tasktracker-metrics.out
{code}

These lines should be removed and a config for NodeManager should be added instead."
HADOOP-9918,Add addIfService() to CompositeService,"Some YARN and MR classes implement their own version of {{addIfService(Object object)}} that adds the service to CompositeService if the object is a service. 

It makes more sense to move this helper to CompositeService itself."
HADOOP-9916,Race condition in ipc.Client causes TestIPC timeout,"TestIPC timeouts occasionally, for example: 
[https://issues.apache.org/jira/browse/HDFS-5130?focusedCommentId=13749870&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13749870]
[https://issues.apache.org/jira/browse/HADOOP-9915?focusedCommentId=13753302&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13753302]

Look into the code, there is race condition in oah.ipc.Client, the race condition happen between RPC call thread and connection read response thread:

{code}
        if (status == RpcStatusProto.SUCCESS) {
          Writable value = ReflectionUtils.newInstance(valueClass, conf);
          value.readFields(in);                 // read value
          call.setRpcResponse(value);
          calls.remove(callId);
{code}

Read Thread: 
Connection.receiveRpcResponse-> call.setRpcResponse(value) -> notify Call Thread

Call Thread:
Client.call -> Connection.addCall(retry with the same callId) -> notify read thread

Read Thread:
calls.remove(callId) # intend to remove old call, but removes newly added call...
Connection.waitForWork end up wait maxIdleTime and close the connection. The call never get response and dead.

The problem doesn't show because previously callId is unique, we never accidentally remove newly added calls, but when retry added this race condition became possible.

To solve this, we can simply change order, remove the call first, then notify call thread.
Note there are many places need this order change(normal case, error case, cleanup case)

And there are some minor issues in TestIPC:
1. there are two method with same name:
   void testSerial()
   void testSerial(int handlerCount, boolean handlerSleep, ...)
   the second is not a test case(so should not use testXXX prefix), but somehow it causes testSerial(first one) run two times, see test report:
{code}
  <testcase time=""26.896"" classname=""org.apache.hadoop.ipc.TestIPC"" name=""testSerial""/>
  <testcase time=""25.426"" classname=""org.apache.hadoop.ipc.TestIPC"" name=""testSerial""/>
{code}

2. timeout annotation should be added, so next time related log is available.

"
HADOOP-9915,o.a.h.fs.Stat support on Macosx,"Support macosx in o.a.h.fs.Stat.
The stat cmd in macosx seems the same as stat in freebsd. I make mac to use the same ExecString as freebsd, it seems to work fine.
"
HADOOP-9910,proxy server start and stop documentation wrong,I was trying to run a distributed cluster and found two little problems in the documentation on how to start and stop the proxy server. Attached patch fixes it.
HADOOP-9909,org.apache.hadoop.fs.Stat should permit other LANG,"I executed ""hdfs dfs -put"" command and displayed following warning message. And ""hdfs dfs -put"" command was success.
This is because Stat.parseExecResult() check a message of stat command from only English.

{code}
[hadoop@trunk ~]$ hdfs dfs -put fugafuga.txt .
13/08/27 16:24:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/08/27 16:24:37 WARN fs.FSInputChecker: Problem opening checksum file: file:/home/hadoop/fugafuga.txt.  Ignoring exception:
java.io.IOException: Unexpected stat output: stat: cannot stat `/home/hadoop/.fugafuga.txt.crc': そのようなファイルやディレクトリはありません
        at org.apache.hadoop.fs.Stat.parseExecResult(Stat.java:163)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:489)
        at org.apache.hadoop.util.Shell.run(Shell.java:417)
        at org.apache.hadoop.fs.Stat.getFileStatus(Stat.java:68)
        at org.apache.hadoop.fs.RawLocalFileSystem.getNativeFileLinkStatus(RawLocalFileSystem.java:806)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:738)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:523)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1397)
        at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:210)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:143)
        at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:339)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:763)
        at org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:239)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:183)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:168)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:310)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:282)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:163)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:264)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:248)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:140)
        at org.apache.hadoop.fs.shell.CopyCommands$Put.processArguments(CopyCommands.java:224)
        at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:194)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:155)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:255)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:308)
Caused by: java.lang.NumberFormatException: For input string: ""stat: cannot stat `/home/hadoop/.fugafuga.txt.crc': そのようなファイルやディレクトリはありません""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:441)
        at java.lang.Long.parseLong(Long.java:483)
        at org.apache.hadoop.fs.Stat.parseExecResult(Stat.java:128)
        ... 27 more
{code}"
HADOOP-9908,Fix NPE when versioninfo properties file is missing,When running tests in Eclipse I ran into an NPE in VersionInfo since the version info properties file didn't properly make it to the classpath. This is because getResourceAsStream can return null if the file is not found.
HADOOP-9907,Webapp http://hostname:port/metrics  link is not working ,This link is not working which just shows a blank page.
HADOOP-9906,Move HAZKUtil to o.a.h.util.ZKUtil and make inner-classes public,"HAZKUtil defines a couple of exceptions - BadAclFormatException and BadAuthFormatException - that can be made public for use in other components. 

For instance, YARN-353 could use it in tests and ACL validation."
HADOOP-9899,Remove the debug message added by HADOOP-8855,"HADOOP-8855 added a debug message which was printed to System.out.
{code}
//KerberosAuthenticator.java
  private void sendToken(byte[] outToken) throws IOException, AuthenticationException {
    new Exception(""sendToken"").printStackTrace(System.out);
    ...
}
{code}"
HADOOP-9898,Set SO_KEEPALIVE on all our sockets,"We recently saw an issue where network issues between slaves and the NN caused ESTABLISHED TCP connections to pile up and leak on the NN side. It looks like the RST packets were getting dropped, which meant that the client thought the connections were closed, while they hung open forever on the server.

Setting the SO_KEEPALIVE option on our sockets would prevent this kind of leak from going unchecked."
HADOOP-9897,Add method to get path start position without drive specifier in o.a.h.fs.Path  ,"There are a lot of code in Path to get start position after skipping drive specifier, like:

{code}
    int start = hasWindowsDrive(uri.getPath()) ? 3 : 0;
{code}

Also there is a minor bug in mergePaths:
mergePath(""/"", ""/foo"") will yield Path(""//foo"") which will be parsed as uri authority, not path."
HADOOP-9889,Refresh the Krb5 configuration when creating a new kdc in Hadoop-MiniKDC,"Krb5 Config uses a singleton and once initialized it does not refresh automatically. Without refresh, there are failures if you are using MiniKDCs with different configurations (such as different realms) within the same test run or if the Krb5 Config singleton is called before the MiniKDC is started for the first time."
HADOOP-9887,globStatus does not correctly handle paths starting with a drive spec on Windows,"Recent file system changes have caused globStatus to stop working for paths starting with a drive spec on Windows.  The problem is most easily visible by running {{TestFileUtil#createJarWithClassPath}} on Windows.  This method attempts a globStatus with pattern {{*\{.jar,.JAR\}}}, and it no longer correctly identifies files at the path ending in .jar or .JAR."
HADOOP-9886,Turn warning message in RetryInvocationHandler to debug,"Currently if debug is not enabled we display a warning message when the client fails over to another namenode.

This will happen for every call that goes to the failed over namenode."
HADOOP-9880,SASL changes from HADOOP-9421 breaks Secure HA NN ,"buildSaslNegotiateResponse() will create a SaslRpcServer with TOKEN auth. When create() is called against it, secretManager.checkAvailableForRead() is called, which fails in HA standby. Thus HA standby nodes cannot be transitioned to active."
HADOOP-9879,Move the version info of zookeeper dependencies to hadoop-project/pom,"As different projects (HDFS, YARN) depend on zookeeper, it is better to keep the version information in hadoop-project/pom.xml."
HADOOP-9877,Fix listing of snapshot directories in globStatus,"{code}
decster:~/hadoop> bin/hadoop fs -ls ""/foo/.snapshot""
13/08/16 01:17:22 INFO hdfs.DFSClient: +++++ listPath(/)
13/08/16 01:17:22 INFO hdfs.DFSClient: +++++ listPath(/foo)
ls: `/foo/.snapshot': No such file or directory
{code}

HADOOP-9817 refactor some globStatus code, but forgot to handle special case that .snapshot dir is not show up in listStatus but exists, so we need to explicitly check path existence using getFileStatus, rather than depending on listStatus results.
"
HADOOP-9875,TestDoAsEffectiveUser can fail on JDK 7,Another issue with the test method execution order changing between JDK 6 and 7.
HADOOP-9872,Improve protoc version handling and detection,"HADOOP-9845 bumped up protoc from 2.4.1 to 2.5.0, but we run into a few quirks:

* 'protoc --version' in 2.4.1 exits with 1
* 'protoc --version' in 2.5.0 exits with 0
* if you have multiple protoc in your environment, you have to the the one you want to use in the PATH before building hadoop
* build documentation and requirements of protoc are outdated

This patch does:

* handles protoc version correctly independently of the exit code
* if HADOOP_PROTOC_PATH env var is defined, it uses it as the protoc executable * if HADOOP_PROTOC_PATH is not defined, it picks protoc from the PATH
* documentation updated to reflect 2.5.0 is required
* enforces the version of protoc and protobuf JAR are the same
* Added to VersionInfo the protoc version used (sooner or later this will be useful for in a troubleshooting situation).

[~vicaya] suggested to make the version check for protoc lax (i.e. 2.5.*). While working on the patch I've thought about that. But that would introduce a potential mismatch between protoc and protobuff  JAR.

Still If you want to use different version of protoc/protobuff from the one defined in the POM, you can use the -Dprotobuf.version=#### to specify your alternate version. But I would recommend not to do this, because if you publish the artifacts to a Maven repo, the fact you used -Dprotobuf.version=#### will be lost and the version defined in the POM properties will be used (IMO Maven should use the effective POM on deploy, but they don't).

"
HADOOP-9871,Fix intermittent findbug warnings in DefaultMetricsSystem,Findbugs sometimes picks up warnings from DefaultMetricsSystem due to some of the fields not being transient for serializable class (DefaultMetricsSystem is an Enum (which is serializable)). 
HADOOP-9869, Configuration.getSocketAddr()/getEnum() should use getTrimmed(),"YARN-1059 has shown that the hostname:port string used for the address of things like the RM isn't trimmed before its parsed, leading to errors that aren't that obvious. 

We should trim it -it's clearly not going to break any existing (valid) configurations"
HADOOP-9868,Server must not advertise kerberos realm,"HADOOP-9789 broke kerberos authentication by making the RPC server advertise the kerberos service principal realm.  SASL clients and servers do not support specifying a realm, so it must be removed from the advertisement."
HADOOP-9866,convert hadoop-auth testcases requiring kerberos to use minikdc,
HADOOP-9865,FileContext.globStatus() has a regression with respect to relative path,"I discovered the problem when running unit test TestMRJobClient on Windows. The cause is indirect in this case. In the unit test, we try to launch a job and list its status. The job failed, and caused the list command get a result of 0, which triggered the unit test assert. From the log and debug, the job failed because we failed to create the Jar with classpath (see code around {{FileUtil.createJarWithClassPath}}) in {{ContainerLaunch}}. This is a Windows specific step right now; so the test still passes on Linux. This step failed because we passed in a relative path to {{FileContext.globStatus()}} in {{FileUtil.createJarWithClassPath}}. The relevant log looks like the following.

{noformat}
2013-08-12 16:12:05,937 WARN  [ContainersLauncher #0] launcher.ContainerLaunch (ContainerLaunch.java:call(270)) - Failed to launch container.
org.apache.hadoop.HadoopIllegalArgumentException: Path is relative
	at org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)
	at org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)
	at org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:128)
	at org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)
	at org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

I think this is a regression from HADOOP-9817. I modified some code and the unit test passed. (See the attached patch.) However, I think the impact is larger. I will add some unit tests to verify the behavior, and work on a more complete fix."
HADOOP-9860,Remove class HackedKeytab and HackedKeytabEncoder from hadoop-minikdc once jira DIRSERVER-1882 solved,"Remove class {{HackedKeytab}} and {{HackedKeytabEncoder}} from hadoop-minikdc (HADOOP-9848) once jira DIRSERVER-1882 solved.

Also update the apacheds version in the pom.xml."
HADOOP-9858,Remove unused private RawLocalFileSystem#execCommand method from branch-2.,"{{RawLocalFileSystem#execCommand}} is a private method that is no longer used.  It does not exist on trunk, but we need to remove it from branch-2 and branch-2.1-beta.  This was likely caused by a merge error during YARN-316."
HADOOP-9857,Tests block and sometimes timeout on Windows due to invalid entropy source.,"Tests are configured to set the entropy source to {{file:///dev/urandom}} to prevent blocking on machines with low entropy.  We've observed that on Windows, this will cause tests to block for 5 seconds trying to access this non-existent path the first time the process does something that needs entropy, such as using a {{SecureRandom}} or creating a {{UUID}}.  This can cause tests to fail due to timeouts.  In addition to the initial 5-second blocking, this causes the JVM to fall back to a slower, non-native seed generation implementation."
HADOOP-9850,RPC kerberos errors don't trigger relogin,"Hadoop auto-renews a ticket cache TGT.  However, a TGT acquired via keytab is just allowed to expire.  To compensate, any exception during a kerberos RPC connection triggers a relogin.

Prior to HADOOP-9698, the RPC client ""knew"" the SASL client was attempting authMethod kerberos.  Now the SASL client negotiates and returns the authMethod to the RPC Client.  When an exception occurs, such as TGT expired, the Client doesn't know what the SASL client was attempting so no relogin is attempted.  After 24 hours, keytab based services that act as clients (ex. RM for token renewal) go dead."
HADOOP-9848,Create a MiniKDC for use with security testing,"Create a MiniKDC using Apache Directory Server. MiniKDC builds an embedded KDC (key distribution center), and allows to create principals and keytabs on the fly. MiniKDC can be integrated for Hadoop security unit testing."
HADOOP-9847,TestGlobPath symlink tests fail to cleanup properly,"On our internal trunk Jenkins runs, I've seen failures like the following:

{noformat}

Error Message:
Cannot delete /user/jenkins. Name node is in safe mode. Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use ""hdfs dfsadmin -safemode leave"" to turn safe mode off.  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:3138)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInt(FSNamesystem.java:3097)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3081)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:671)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:491)  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:48087)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2031)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2027)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:396)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1493)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2025)

Stack Trace:
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot delete /user/jenkins. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use ""hdfs dfsadmin -safemode leave"" to turn safe mode off.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:3138)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInt(FSNamesystem.java:3097)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3081)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:671)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:491)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:48087)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2031)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2027)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1493)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2025)

        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.Client.call(Client.java:1352)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at $Proxy15.delete(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
        at $Proxy15.delete(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:449)
        at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1575)
        at org.apache.hadoop.hdfs.DistributedFileSystem$11.doCall(DistributedFileSystem.java:585)
        at org.apache.hadoop.hdfs.DistributedFileSystem$11.doCall(DistributedFileSystem.java:581)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:581)
        at org.apache.hadoop.fs.TestGlobPaths.cleanupDFS(TestGlobPaths.java:788)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
        at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
{noformat}

I believe this is because the {{@After}} in {{TestGlobPaths}} is trying to delete the test dir even though the symlink tests have already shutdown the minicluster."
HADOOP-9845,Update protobuf to 2.5 from 2.4.x,"protobuf 2.5 is a bit faster with a new Parse to avoid a builder step and a few other goodies that we'd like to take advantage of over in hbase especially now we are all pb all the time.  Unfortunately the protoc generated files are no longer compatible w/ 2.4.1 generated files.  Hadoop uses 2.4.1 pb.  This latter fact makes it so we cannot upgrade until hadoop does.

This issue suggests hadoop2 move to protobuf 2.5.

I can do the patch no prob. if there is interest.

(When we upgraded our build broke with complaints like the below:
{code}
java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.
	at com.google.protobuf.GeneratedMessage.getUnknownFields(GeneratedMessage.java:180)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto.getSerializedSize(ClientNamenodeProtocolProtos.java:21566)
	at com.google.protobuf.AbstractMessageLite.toByteString(AbstractMessageLite.java:49)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.constructRpcRequest(ProtobufRpcEngine.java:149)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:193)
	at com.sun.proxy.$Proxy14.getDatanodeReport(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at com.sun.proxy.$Proxy14.getDatanodeReport(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDatanodeReport(ClientNamenodeProtocolTranslatorPB.java:488)
	at org.apache.hadoop.hdfs.DFSClient.datanodeReport(DFSClient.java:1887)
	at org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:1798
...
{code}

More over in HBASE-8165 if interested."
HADOOP-9833,move slf4j to version 1.7.5,"Hadoop depends on SLF4J 1.6.1 ; 1.7.5 is the latest, which adds varags support in the logging.

As SLF4J is visible downstream, updating it gives hadoop apps the more modern version of the SLF4J APIs. hadoop-auth uses these APIs too."
HADOOP-9832,Add RPC header to client ping,Splitting out the ping part of the umbrella jira.
HADOOP-9831,Make checknative shell command accessible on Windows.,The checknative command was implemented in HADOOP-9162 and HADOOP-9164 to print information about availability of native libraries.  We already have the native code to do this on Windows.  We just need to update hadoop.cmd to expose the checknative command and pass through to the correct command class.
HADOOP-9830,Typo at http://hadoop.apache.org/docs/current/,"Strange symbols at http://hadoop.apache.org/docs/current/
{code} 
ApplicationMaster manages the application‚Äôs scheduling and coordination. 
{code}
Sorry for posting here, could not find any other way to report."
HADOOP-9821,ClientId should have getMsb/getLsb methods,Both ClientId and RetryCache have the same logic to calculate msb and lsb. We should not have same logics in separate classes but have utility methods to do so in one class.
HADOOP-9820,RPCv9 wire protocol is insufficient to support multiplexing,"RPCv9 is intended to allow future support of multiplexing.  This requires all wire messages to be tagged with a RPC header so a demux can decode and route the messages accordingly.

RPC ping packets and SASL QOP wrapped data is known to not be tagged with a header."
HADOOP-9817,FileSystem#globStatus and FileContext#globStatus need to work with symlinks,"FileSystem#globStatus and FileContext#globStatus need to work with symlinks.  Currently, they resolve all links, so that if you have:
{code}
/alpha/beta
/alphaLink -> alpha
{code}

and you take {{globStatus(/alphaLink/*)}}, you will get {{/alpha/beta}}, rather than the expected {{/alphaLink/beta}}.

We even resolve terminal symlinks, which would prevent listing a symlink in FSShell, for example.  Instead, we should build up the path incrementally.  This will allow the shell to behave as expected, and also allow custom globbers to ""see"" the correct paths for symlinks."
HADOOP-9816,RPC Sasl QOP is broken,HADOOP-9421 broke the handling of SASL wrapping for RPC QOP integrity and privacy options.
HADOOP-9806,PortmapInterface should check if the procedure is out-of-range,
HADOOP-9803,Add generic type parameter to RetryInvocationHandler,"{code}
//RetryInvocationHandler.java
private final FailoverProxyProvider proxyProvider;
{code}
FailoverProxyProvider in the field above requires a generic type parameter.  So RetryInvocationHandler should also has a generic type parameter."
HADOOP-9802,Support Snappy codec on Windows.,Build and test the existing Snappy codec on Windows.
HADOOP-9801,"Configuration#writeXml uses platform defaulting encoding, which may mishandle multi-byte characters.","The overload of {{Configuration#writeXml}} that accepts an {{OutputStream}} does not set encoding explicitly, so it chooses the platform default encoding.  Depending on the platform's default encoding, this can cause incorrect output data when encoding multi-byte characters."
HADOOP-9792,Retry the methods that are tagged @AtMostOnce along with @Idempotent,"Currently the operations marked as @Idempotent are retried. Now that we have added new operations that are marked @AtMostOnce that guaranteed to be executed only once with the help of RetryCache on server implementation, these methods should also be retried."
HADOOP-9791,Add a test case covering long paths for new FileUtil access check methods,We've seen historically that paths longer than 260 chars can cause things not to work on Windows if not properly handled. Filing a tracking Jira to add a native io test case with long paths for new FileUtil access check methods added with HADOOP-9413. 
HADOOP-9789,Support server advertised kerberos principals,"The RPC client currently constructs the kerberos principal based on the a config value, usually with an _HOST substitution.  This means the service principal must match the hostname the client is using to connect.  This causes problems:
* Prevents using HA with IP failover when the servers have distinct principals from the failover hostname
* Prevents clients from being able to access a service bound to multiple interfaces.  Only the interface that matches the server's principal may be used.

The client should be able to use the SASL advertised principal (HADOOP-9698), with appropriate safeguards, to acquire the correct service ticket."
HADOOP-9787,ShutdownHelper util to shutdown threads and threadpools,"Several classes spawn threads and threadpools and shut them down on close() and serviceStop() etc. A helper class helps standardize the wait time after a thread/threadpool is interrupted/shutdown for it to actually terminate.

One example of this is MAPREDUCE-5428."
HADOOP-9786,RetryInvocationHandler#isRpcInvocation should support ProtocolTranslator ,"Currently the RetryInvocationHandler uses the same RPC ids (i.e., clientId + callId) only when the invocation is a RPC invocation. To check whether an invocation is RPC, RetryInvocationHandler#isRpcInvocation directly apply Proxy#isProxyClass on RetryInvocation#currentProxy. However, if currentProxy is an instance of ProtocolTranslator (e.g., ClientNamenodeProtocolTranslatorPB), the real dynamically-generated proxy object is contained within currentProxy and needs to be retrieved by calling ProtocolTranslator#getUnderlyingProxyObject. Failing to recognize a RPC invocation can cause a retry request to have different ""clientId + callId"" with its initial call, and fail to hit the corresponding retry cache entry in the NameNode side."
HADOOP-9785,"LZ4 code may need upgrade (lz4.c embedded in libHadoop is r43 18 months ago, while latest version is r98)","While analyzing compression performance of different Hadoop codecs I noticed that the LZ4 code was taken from revision 43 of https://code.google.com/p/lz4/. The latest version is r98 and there may be extra performance benefits we can gain from using r98. 

We may involve the original LZ4 author Yann Collet on these discussions, as the current LZ4 code includes additional algorithms and parameters. 

To start the investigation, I ran preliminary experiments with the Silesia corpus and there seems to be an improvement on throughput for compression and decompression in the latest release when compared with r43 (haven't done enough analysis to conclude anything statistically, but looks good).  

Here is raw output using LZ4 from r43 with a SUBSET of the silesia corpus (http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia)

File: silesia/dickens
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Compressed 10192446 bytes into 6433123 bytes ==> 63.12%
Done in 0.07 s ==> 138.86 MB/s
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Successfully decoded 10192446 bytes
Done in 0.02 s ==> 486.01 MB/s

File: silesia/mozilla
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Compressed 51220480 bytes into 26379814 bytes ==> 51.50%
Done in 0.25 s ==> 195.39 MB/s
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Successfully decoded 51220480 bytes
Done in 0.12 s ==> 407.06 MB/s

File: silesia/mr
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Compressed 9970564 bytes into 5669268 bytes ==> 56.86%
Done in 0.04 s ==> 237.72 MB/s
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Successfully decoded 9970564 bytes
Done in 0.02 s ==> 475.43 MB/s

File: silesia/nci
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Compressed 33553445 bytes into 5880292 bytes ==> 17.53%
Done in 0.08 s ==> 399.99 MB/s
*** Compression CLI using LZ4 algorithm , by Yann Collet (Jul 29 2013) ***
Successfully decoded 33553445 bytes
Done in 0.06 s ==> 533.32 MB/s

And here raw output of LZ4 from the latest release r98

File: silesia/dickens
*** Full LZ4 speed analyzer , by Yann Collet (Jul 29 2013) ***
Loading silesia/dickens...
1-LZ4_compress        :  10192446 ->^M1-LZ4_compress        :  10192446 ->   6434313 (63.13%),  172.3 MB/s
1-LZ4_decompress_fast :  10192446 ->^M1-LZ4_decompress_fast :  10192446 ->   676.0 MB/s^MLZ4_decompress_fast   :  10192446 ->   676.0 MB/s

File: silesia/mozilla
*** Full LZ4 speed analyzer , by Yann Collet (Jul 29 2013) ***
Loading silesia/mozilla...
1-LZ4_compress        :  51220480 ->^M1-LZ4_compress        :  51220480 ->  26382113 (51.51%),  281.7 MB/s
1-LZ4_decompress_fast :  51220480 ->^M1-LZ4_decompress_fast :  51220480 ->  1003.1 MB/s^MLZ4_decompress_fast   :  51220480 ->  1003.1 MB/s

File: silesia/mr
*** Full LZ4 speed analyzer , by Yann Collet (Jul 29 2013) ***
Loading silesia/mr...
1-LZ4_compress        :   9970564 ->^M1-LZ4_compress        :   9970564 ->   5669255 (56.86%),  268.3 MB/s
1-LZ4_decompress_fast :   9970564 ->^M1-LZ4_decompress_fast :   9970564 ->   788.7 MB/s^MLZ4_decompress_fast   :   9970564 ->   788.7 MB/s

File: silesia/nci
*** Full LZ4 speed analyzer , by Yann Collet (Jul 29 2013) ***
Loading silesia/nci...
1-LZ4_compress        :  33553445 ->^M1-LZ4_compress        :  33553445 ->   5883923 (17.54%),  584.9 MB
1-LZ4_decompress_fast :  33553445 ->^M1-LZ4_decompress_fast :  33553445 ->  1208.3 MB/s^MLZ4_decompress_fast   :  33553445 ->  1208.3 MB/s
"
HADOOP-9784,Add a builder for HttpServer,There are quite a lot of constructors in class of HttpServer to create instance. Create a builder class to abstract the building steps which helps to avoid more constructors in the future.
HADOOP-9776,HarFileSystem.listStatus() returns invalid authority if port number is empty,"If the given har URI is ""har://<scheme>-localhost/usr/my.har/a"", the result of HarFileSystem.listStatus() will have a "":"" appended after localhost, like this: ""har://<scheme>-localhost:/usr/my.har/a"". it should return ""har://<scheme>-localhost/usr/my.bar/a"" instead.

This creates problem when running a hive unit test TestCliDriver (archive_excludeHadoop20.q), generating the following error:

	java.io.IOException: cannot find dir = har://pfile-localhost:/GitHub/hive-monarch/build/ql/test/data/warehouse/tstsrcpart/ds=2008-04-08/hr=12/data.har/000000_0 in pathToPartitionInfo: [pfile:/GitHub/hive-monarch/build/ql/test/data/warehouse/tstsrcpart/ds=2008-04-08/hr=11, har://pfile-localhost/GitHub/hive-monarch/build/ql/test/data/warehouse/tstsrcpart/ds=2008-04-08/hr=12/data.har]
	    [junit] 	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:298)
	    [junit] 	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:260)
	    [junit] 	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CombineHiveInputSplit.<init>(CombineHiveInputFormat.java:104)
"
HADOOP-9774,RawLocalFileSystem.listStatus() return absolute paths when input path is relative on Windows,"On Windows, when using RawLocalFileSystem.listStatus() to enumerate a relative path (without drive spec), e.g., ""file:///mydata"", the resulting paths become absolute paths, e.g., [""file://E:/mydata/t1.txt"", ""file://E:/mydata/t2.txt""...].
Note that if we use it to enumerate an absolute path, e.g., ""file://E:/mydata"" then the we get the same results as above.

This breaks some hive unit tests which uses local file system to simulate HDFS when testing, therefore the drive spec is removed. Then after listStatus() the path is changed to absolute path, hive failed to find the path in its map reduce job.

You'll see the following exception:
[junit] java.io.IOException: cannot find dir = pfile:/E:/GitHub/hive-monarch/build/ql/test/data/warehouse/src/kv1.txt in pathToPartitionInfo: [pfile:/GitHub/hive-monarch/build/ql/test/data/warehouse/src]
[junit] 	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:298)


This problem is introduced by this JIRA:
HADOOP-8962

Prior to the fix for HADOOP-8962 (merged in 0.23.5), the resulting paths are relative paths if the parent paths are relative, e.g., [""file:///mydata/t1.txt"", ""file:///mydata/t2.txt""...]

This behavior change is a side effect of the fix in HADOOP-8962, not an intended change. The resulting behavior, even though is legitimate from a function point of view, break consistency from the caller's point of view. When the caller use a relative path (without drive spec) to do listStatus() the resulting path should be relative. Therefore, I think this should be fixed."
HADOOP-9773,TestLightWeightCache fails,"It fails on some size limit tests when the random seed is 1374774736885L, "
HADOOP-9770,Make RetryCache#state non volatile,See the comment - https://issues.apache.org/jira/browse/HDFS-4979?focusedCommentId=13719111&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13719111
HADOOP-9768,chown and chgrp reject users and groups with spaces on platforms where spaces are otherwise acceptable,"The chown and chgrp commands enforce a check on a valid set of characters for user and group.  The set of valid characters does not include space.  On some platforms (notably Windows), the space character is acceptable.  We've seen this cause test failures in {{TestFsShellReturnCode}} when running on Windows if the logged-in user is a member of a group like Remote Desktop Users."
HADOOP-9763,Extends LightWeightGSet to support eviction of expired elements,We extends LightWeightGSet to support eviction of expired elements so that namenode could use it as the RetryCache.
HADOOP-9762,RetryCache utility for implementing RPC retries,HDFS-4979 has the RetryCache implementation in uncommitted patches. This jira moves this utility to hadoop-common.
HADOOP-9761,ViewFileSystem#rename fails when using DistributedFileSystem,"ViewFileSystem currently passes unqualified paths (no scheme or authority) to underlying FileSystems when doing a rename. DistributedFileSystem symlink support added in HADOOP-9418 needs to qualify and check rename sources and destinations since cross-filesystem renames aren't supported, so this breaks in the following way

- Default FS URI is configured to viewfs://<viewfs>
- When doing a rename, ViewFileSystem checks to make sure both src and dst FileSystems are the same (which they are, both in same DFS), and then calls DistributedFileSystem#rename with unqualified ""remainder"" paths
- Since these paths are unqualified, DFS qualifies them with the default FS to check that it can do the rename. This turns it into viewfs://<viewfs>/<path>
- Since viewfs://<viewfs> is not the DFS's URI, DFS errors out the rename."
HADOOP-9760,Move GSet and LightWeightGSet to hadoop-common,GSet and related classes are useful for all of Hadoop. This jira proposes moving it to hadoop-common.
HADOOP-9759,Add support for NativeCodeLoader#getLibraryName on Windows,HADOOP-9164 introduced a new native API to get the library path for the Hadoop native libraries and other third party native libraries. There is no equivalent support of this native API on Windows in the original patch. This JIRA is created to track the work to support {{NativeCodeLoader#getLibraryName}}. We also have a build break on Windows due to the change.
HADOOP-9758,Provide configuration option for FileSystem/FileContext symlink resolution,"With FileSystem symlink support incoming in HADOOP-8040, some clients will wish to not transparently resolve symlinks. This is somewhat similar to O_NOFOLLOW in open(2).

Rationale for is for a security model where a user can invoke a third-party service running as a service user to operate on the user's data. For instance, users might want to use Hive to query data in their homedirs, where Hive runs as the Hive user and the data is readable by the Hive user. This leads to a security issue with symlinks:

# User Mallory invokes Hive to process data files in {{/user/mallory/hive/}}
# Hive checks permissions on the files in {{/user/mallory/hive/}} and allows the query to proceed.
# RACE: Mallory replaces the files in {{/user/mallory/hive}} with symlinks that point to user Ann's Hive files in {{/user/ann/hive}}. These files aren't readable by Mallory, but she can create whatever symlinks she wants in her own scratch directory.
# Hive's MR jobs happily resolve the symlinks and accesses Ann's private data.

This is also potentially useful for clients using FileContext, so let's add it there too."
HADOOP-9757,Har metadata cache can grow without limit,"MAPREDUCE-2459 added a metadata cache to the har filesystem, but the cache has no upper limits.  A long-running process that accesses many har archives will eventually run out of memory due to a har metadata cache that never retires entries."
HADOOP-9756,Additional cleanup RPC code,"HADOOP-9754 already did good job to address most of work for cleanup RPC code. Here is some additional work, include:
- Remove some unused deprecated code.
- Narrow ""throws Exception"" to throw some specific exception and remove some unnecessary exceptions
- Fix a generic warning and correct spell issue."
HADOOP-9754,Clean up RPC code,"Cleanup the RPC code for the following problems:
- Remove unnecessary ""throws IOException/InterruptedException"".
- Fix generic warnings.
- Fix other javac warnings."
HADOOP-9751,Add clientId and retryCount to RpcResponseHeaderProto,"We have clientId, callId and retryCount in RpcRequestHeaderProto.  However, we only have callId but not clientId and retryCount in RpcResponseHeaderProto.

It is useful to have clientId and retryCount in the responses for applications like rpc proxy server."
HADOOP-9748,Reduce blocking on UGI.ensureInitialized,"EnsureInitialized is always sync'ed on the class, when it should only sync if it actually has to initialize."
HADOOP-9740,FsShell's Text command does not read avro data files stored on HDFS,"HADOOP-8597 added support for reading avro data files from FsShell Text command. However, it does not work with files stored on HDFS. Here is the error message:
{code}
$hadoop fs -text hdfs://localhost:8020/test.avro
-text: URI scheme is not ""file""
Usage: hadoop fs [generic options] -text [-ignoreCrc] <src> ...
{code}

The problem is because the File constructor complains not able to recognize hdfs:// scheme in during AvroFileInputStream initialization. 

There is a unit TestTextCommand.java under hadoop-common project. However it only tested files in local file system. I created a similar one under hadoop-hdfs project using MiniDFSCluster. Please see attached maven unit test error message with full stack trace for more details.
 


 "
HADOOP-9738,TestDistCh fails,"{noformat}
junit.framework.AssertionFailedError: expected:<r---w-rw-> but was:<r---wxrwx>
	at junit.framework.Assert.fail(Assert.java:50)
	at junit.framework.Assert.failNotEquals(Assert.java:287)
	at junit.framework.Assert.assertEquals(Assert.java:67)
	at junit.framework.Assert.assertEquals(Assert.java:74)
	at org.apache.hadoop.tools.TestDistCh.checkFileStatus(TestDistCh.java:197)
	at org.apache.hadoop.tools.TestDistCh.testDistCh(TestDistCh.java:180)
{noformat}

It has been broken since Jun 14. "
HADOOP-9734,"Common protobuf definitions for GetUserMappingsProtocol, RefreshAuthorizationPolicyProtocol and RefreshUserMappingsProtocol","Many of the Hadoop server daemons support the concept of refreshing user mappings or getting the currently cached user-group mappings.  Currently there are protocol buffer definitions of RefreshUserMappingsProtocol and GetUserMappingsProtocol in HDFS, but using it requires packages to depend upon HDFS when they may otherwise have no reason to do so.  We should move the protocol buffer definitions and glue code to common for easier reuse."
HADOOP-9730,fix hadoop.spec to add task-log4j.properties ,
HADOOP-9720,Rename Client#uuid to Client#clientId,To address the comment - https://issues.apache.org/jira/browse/HADOOP-9688?focusedCommentId=13705032&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13705032
HADOOP-9717,Add retry attempt count to the RPC requests,RetryCache lookup on server side implementation can be optimized if Rpc request indicates if the request is being retried. This jira proposes adding an optional field to Rpc request that indicates if request is being retried.
HADOOP-9716,Move the Rpc request call ID generation to client side InvocationHandler,"Currently when RetryInvocationHandler is used to retry an RPC request, a new RPC request call ID is generated. This jira proposes moving call ID generation to InvocationHandler so that retried RPC requests retain the same call ID. This is needed for RetryCache functionality proposed in HDFS-4942."
HADOOP-9712,"Write contract tests for FTP filesystem, fix places where it breaks","implement the abstract contract tests for ftp, identify where it is failing to meet expectations and, where possible, fix. 

FTPFS appears to be the least tested (& presumably used) hadoop filesystem implementation; there may be some bug reports that have been around for years that could drive test cases and fixes."
HADOOP-9711,Write contract tests for S3Native; fix places where it breaks,"implement the abstract contract tests for S3, identify where it is failing to meet expectations and, where possible, fix. Blobstores tend to treat 0 byte files as directories, so tests overwriting files with dirs and vice versa may fail and have to be skipped"
HADOOP-9707,Fix register lists for crc32c inline assembly,"The inline assembly used for the crc32 instructions has an incorrect clobber list: the computed CRC values are ""in-out"" variables and thus need to use the ""matching constraint"" syntax in the clobber list.

This doesn't seem to cause a problem now in Hadoop, but may break in a different compiler version which allocates registers differently, or may break when the same code is used in another context."
HADOOP-9705,FsShell cp -p does not preserve directory attibutes,"HADOOP-9338 added the -p flag to preserve file attributes when copying.

However, cp -p does not preserve directory attributes. It'd be useful to add this functionality.

For example, the following shows that the modified time is not preserved
{code}
[schu@hdfs-snapshots-1 ~]$ $HADOOP_HOME/bin/hdfs dfs -mkdir /user/schu/testDir1
[schu@hdfs-snapshots-1 ~]$ $HADOOP_HOME/bin/hdfs dfs -ls /user/schu/
Found 1 items
drwxr-xr-x   - schu supergroup          0 2013-07-07 20:25 /user/schu/testDir1
[schu@hdfs-snapshots-1 ~]$ $HADOOP_HOME/bin/hdfs dfs -cp -p /user/schu/testDir1 /user/schu/testDir2
[schu@hdfs-snapshots-1 ~]$ $HADOOP_HOME/bin/hdfs dfs -ls /user/schu
Found 2 items
drwxr-xr-x   - schu supergroup          0 2013-07-07 20:25 /user/schu/testDir1
drwxr-xr-x   - schu supergroup          0 2013-07-07 20:35 /user/schu/testDir2
[schu@hdfs-snapshots-1 ~]$ 
{code}

The preserve logic is in CommandWithDestination#copyFileToTarget, which is only called with files.

{code}
  protected void processPath(PathData src, PathData dst) throws IOException {
    if (src.stat.isSymlink()) {
      // TODO: remove when FileContext is supported, this needs to either                                                                                                     
      // copy the symlink or deref the symlink                                                                                                                                
      throw new PathOperationException(src.toString());
    } else if (src.stat.isFile()) {
      copyFileToTarget(src, dst);
    } else if (src.stat.isDirectory() && !isRecursive()) {
      throw new PathIsDirectoryException(src.toString());
    }
  }
{code}

{code}
  /**                                                                                                                                                                         
   * Copies the source file to the target.                                                                                                                                    
   * @param src item to copy                                                                                                                                                  
   * @param target where to copy the item                                                                                                                                     
   * @throws IOException if copy fails                                                                                                                                        
   */
  protected void copyFileToTarget(PathData src, PathData target) throws IOException {
    src.fs.setVerifyChecksum(verifyChecksum);
    if (src != null) {
      throw new PathExistsException(""hi"");
    }
    InputStream in = null;
    try {
      in = src.fs.open(src.path);
      copyStreamToTarget(in, target);
      if(preserve) {
        target.fs.setTimes(
          target.path,
          src.stat.getModificationTime(),
          src.stat.getAccessTime());
        target.fs.setOwner(
          target.path,
          src.stat.getOwner(),
          src.stat.getGroup());
        target.fs.setPermission(
          target.path,
          src.stat.getPermission());
        System.out.println(""Preserving"");

        if (src.fs.equals(target.fs)) {
            System.out.println(""Same filesystems"");
          src.fs.preserveAttributes(src.path, target.path);
        }
        throw new IOException(""hi"");
      }
    } finally {
      IOUtils.closeStream(in);
    }
  }
{code}"
HADOOP-9704,Write metrics sink plugin for Hadoop/Graphite,"Write a metrics sink plugin for Hadoop to send metrics directly to Graphite in additional to the current ganglia and file ones.
"
HADOOP-9703,org.apache.hadoop.ipc.Client leaks threads on stop.,"org.apache.hadoop.ipc.Client#stop says ""Stop all threads related to this client."" but does not shutdown the static SEND_PARAMS_EXECUTOR, so usage of this class always leaks threads rather than cleanly closing or shutting down.

"
HADOOP-9701,mvn site ambiguous links in hadoop-common,"{code}
[INFO] Rendering site with org.apache.maven.skins:maven-stylus-skin:jar:1.2 skin.
[WARNING] [APT Parser] Ambiguous link: 'InterfaceClassification.html'. If this is a local link, prepend ""./""!
{code}

Also, noticed a warning in SingleNodeSetup.apt"
HADOOP-9698,RPCv9 client must honor server's SASL negotiate response,"As of HADOOP-9421, a RPCv9 server will advertise its authentication methods.  This is meant to support features such as IP failover, better token selection, and interoperability in a heterogenous security environment.

Currently the client ignores the negotiate response and just blindly attempts to authenticate instead of choosing a mutually agreeable auth method."
HADOOP-9693,Shell should add a probe for OSX,"the {{Shell}} class looks for Windows and Linux and sets static fields to show this. For tests, it's sometimes useful to know if a host is OS/X, as its filesystem is different. Although this is easy to test in code [https://developer.apple.com/library/mac/#technotes/tn2002/tn2110.html], having this in one place makes it easier to maintain if future java versions ever behave differently"
HADOOP-9691,RPC clients can generate call ID using AtomicInteger instead of synchronizing on the Client instance.,"As noted in discussion on HADOOP-9688, we can optimize generation of call ID in the RPC client code.  Currently, it synchronizes on the {{Client}} instance to coordinate access to a shared {{int}}.  We can switch this to {{AtomicInteger}} to avoid lock contention."
HADOOP-9688,Add globally unique Client ID to RPC requests,This is a subtask in hadoop-common related to HDFS-4942 to add unique request ID to RPC requests.
HADOOP-9686,Easy access to final parameters in Configuration,It would be nice if there was an easy way to get final parameters within a Configuration.  This would allow clients who wrap Configuration to easily determine which properties should not be changed and implement stricter semantics for them (e.g.: throw an exception when attempts to change them are made).
HADOOP-9683,Wrap IpcConnectionContext in RPC headers,"After HADOOP-9421, all RPC exchanges (including SASL) are wrapped in RPC headers except IpcConnectionContext, which is still raw protobuf, which makes request pipelining (a desirable feature for things like HDFS-2856) impossible to achieve in a backward compatible way. Let's finish the job and wrap IpcConnectionContext with the RPC request header with the call id of SET_IPC_CONNECTION_CONTEXT. Or simply make it an optional field in the RPC request header that gets set for the first RPC call of a given stream.

"
HADOOP-9681,FileUtil.unTarUsingJava() should close the InputStream upon finishing,"In {{FileUtil.unTarUsingJava()}} method, we did not close input steams explicitly upon finish. This could lead to a file handle leak on Windows.

I discovered this when investigating the unit test case failure of {{TestFSDownload.testDownloadArchive()}}. FSDownload class will use {{FileUtil.unTarUsingJava()}} to unpack some temporary archive file. Later, the temporary file should be deleted. Because of the file handle leak, the {{File.delete()}} method fails. The test case then fails because it assert the temporary file should not exist."
HADOOP-9678,TestRPC#testStopsAllThreads intermittently fails on Windows,"Exception:
{noformat}
junit.framework.AssertionFailedError: null
	at org.apache.hadoop.ipc.TestRPC.testStopsAllThreads(TestRPC.java:440)
{noformat}"
HADOOP-9676,make maximum RPC buffer size configurable,"Currently the RPC server just allocates however much memory the client asks for, without validating.  It would be nice to make the maximum RPC buffer size configurable.  This would prevent a rogue client from bringing down the NameNode (or other Hadoop daemon) with a few requests for 2 GB buffers.  It would also make it easier to debug issues with super-large RPCs or malformed headers, since OOMs can be difficult for developers to reproduce."
HADOOP-9673,"NetworkTopology: when a node can't be added, print out its location for diagnostic purposes",It would be nice if NetworkTopology would print out the network location of a node if it couldn't be added.
HADOOP-9672,Upgrade Avro dependency to 1.7.4,"Hadoop still depends on Avro-1.5.3, when the latest release is 1.7.4.  I've observed this cause problems when using Hadoop 2 with Crunch, which uses a more recent version of Avro."
HADOOP-9669,Reduce the number of byte array creations and copies in XDR data manipulation,"XDR.writeXxx(..) methods ultimately use the static XDR.append(..) for writing each data type.  The static append creates a new array and copy data.  Therefore, for a singe reply such as RpcAcceptedReply.voidReply(..), there are multiple array creations and array copies.  For example, there are at least 6 array creations and array copies for RpcAcceptedReply.voidReply(..).
"
HADOOP-9665,BlockDecompressorStream#decompress will throw EOFException instead of return -1 when EOF,"BlockDecompressorStream#decompress ultimately calls rawReadInt, which will throw EOFException instead of return -1 when encountering end of a stream. Then, decompress will be called by read. However, InputStream#read is supposed to return -1 instead of throwing EOFException to indicate the end of a stream. This explains why in LineReader,
{code}
      if (bufferPosn >= bufferLength) {
        startPosn = bufferPosn = 0;
        if (prevCharCR)
          ++bytesConsumed; //account for CR from previous read
        bufferLength = in.read(buffer);
        if (bufferLength <= 0)
          break; // EOF
      }
{code}
-1 is checked instead of catching EOFException.

Now the problem will occur with SnappyCodec. If an input file is compressed with SnappyCodec, it needs to be decompressed through BlockDecompressorStream when it is read. Then, if it empty, EOFException will been thrown from rawReadInt and break LineReader."
HADOOP-9661,Allow metrics sources to be extended,My use case is to create an FSQueueMetrics that extends QueueMetrics and includes some additional fair-scheduler-specific information.
HADOOP-9660,"[WINDOWS] Powershell / cmd parses -Dkey=value from command line as [-Dkey, value] which breaks GenericsOptionParser","When parsing parameters to a class implementing Tool, and using ToolRunner, we can pass 
{code}
bin/hadoop <tool_class> -Dkey=value 
{code}
However, powershell parses the '=' sign itself, and sends it to  java as [""-Dkey"", ""value""] which breaks GenericOptionsParser. 

Using ""-Dkey=value"" or '-Dkey=value' does not fix the problem. The only workaround seems to trick PS by using: 
'""-Dkey=value""' (single + double quote)

In cmd, ""-Dkey=value"" works, but not '""-Dkey=value""'. 

http://stackoverflow.com/questions/4940375/how-do-i-pass-an-equal-sign-when-calling-a-batch-script-in-powershell"
HADOOP-9658,SnappyCodec#checkNativeCodeLoaded may unexpectedly fail when native code is not loaded,"{code}
  public static void checkNativeCodeLoaded() {
      if (!NativeCodeLoader.buildSupportsSnappy()) {
        throw new RuntimeException(""native snappy library not available: "" +
            ""this version of libhadoop was built without "" +
            ""snappy support."");
      }
      if (!SnappyCompressor.isNativeCodeLoaded()) {
        throw new RuntimeException(""native snappy library not available: "" +
            ""SnappyCompressor has not been loaded."");
      }
      if (!SnappyDecompressor.isNativeCodeLoaded()) {
        throw new RuntimeException(""native snappy library not available: "" +
            ""SnappyDecompressor has not been loaded."");
      }
  }
{code}
buildSupportsSnappy is native method. If the native code is not loaded, the method will be missing. Therefore, whether the native code is loaded or not, the first runtime exception will not be thrown."
HADOOP-9656,Gridmix unit tests fail on Windows and Linux,"The following three Gridmix unit tests fail on both Windows and Linux:

* TestGridmixSubmission
* TestLoadJob
* TestSleepJob
* TestDistCacheEmulation

For the first three unit tests, one common cause of failure for both Windows and Linux is that -1 was passed to {{scaleConfigParameter()}} as the default per-task memory request in {{GridmixJob.configureHighRamProperties()}} method.

In additional to the memory setting issue, Windows also have a path issue. In {{CommonJobTest.doSubmission()}} method, ""root"" path is an HDFS path. However, it is initialized as a local file path. This lead to later failure to create ""root"" on HDFS.

For TestDistCacheEmulation, the test asserts a different permission (0644) than the permission that GridmixGenerateDistCacheData job sets on those files."
HADOOP-9652,Allow RawLocalFs#getFileLinkStatus to fill in the link owner and mode if requested,"{{RawLocalFs#getFileLinkStatus}} does not actually get the owner and mode of the symlink, but instead uses the owner and mode of the symlink target.  If the target can't be found, it fills in bogus values (the empty string and FsPermission.getDefault) for these.

Symlinks have an owner distinct from the owner of the target they point to, and getFileLinkStatus ought to expose this.

In some operating systems, symlinks can have a permission other than 0777.  We ought to expose this in RawLocalFilesystem and other places, although we don't necessarily have to support this behavior in HDFS."
HADOOP-9649,Promote YARN service life-cycle libraries into Hadoop Common,"YARN service lifecycle is a good independent library and can benefit all hadoop sub-projects. Therefore, it is good to move it to hadoop-common."
HADOOP-9643,org.apache.hadoop.security.SecurityUtil calls toUpperCase(Locale.getDefault()) as well as toLowerCase(Locale.getDefault()) on hadoop.security.authentication value.,"With the wrong locale, something like hadoop.security.authentication=simple will cause an IllegalArgumentException because ""simple"".toUpperCase(Locale.getDefault()) may not equal SIMPLE."
HADOOP-9638,parallel test changes caused invalid test path for several HDFS tests on Windows,"HADOOP-9287 made changes to the tests to support running multiple tests in parallel.  Part of that patch accidentally reverted a prior change to use paths of the form ""/tmp/<test name>"" when running tests against HDFS.  On Windows, use of the test root will contain a drive spec (i.e. C:\dir), and the colon character is rejected as invalid by HDFS."
HADOOP-9637,Adding Native Fstat for Windows as needed by YARN,"In the YARN, nodemanager need to enforce the log file can only be accessed by the owner. At various places, {{SecureIOUtils.openForRead()}} was called to enforce this check. We don't have {{NativeIO.Posix.getFstat()}} used by {{SecureIOUtils.openForRead()}} on Windows, and this make the check fail on Windows. The YARN unit tests TestAggregatedLogFormat.testContainerLogsFileAccess and TestContainerLogsPage.testContainerLogPageAccess fail on Windows because of this.

The JIRA try to provide a Windows implementation of {{NativeIO.Posix.getFstat()}}.

TestAggregatedLogFormat.testContainerLogsFileAccess test case fails on Windows. The test case try to simulate a situation where first log file is owned by different user (probably symlink) and second one by the user itself. In this situation, the attempt to try to aggregate the logs should fail with the error message ""Owner ... for path ... did not match expected owner ..."".

The check on file owner happens at {{AggregatedLogFormat.write()}} method. The method calls {{SecureIOUtils.openForRead()}} to read the log files before writing out to the OutputStream.

{{SecureIOUtils.openForRead()}} use {{NativeIO.Posix.getFstat()}} to get the file owner and group. We don't have {{NativeIO.Posix.getFstat()}} implementation on Windows; thus, the failure."
HADOOP-9635,Fix Potential Stack Overflow in DomainSocket.c,"When I was running on OSX, the DataNode was segfaulting. On investigation, it was tracked down to this code. A potential stack overflow was also identified. 

{code}
   utfLength = (*env)->GetStringUTFLength(env, jstr);
   if (utfLength > sizeof(path)) {
     jthr = newIOException(env, ""path is too long!  We expected a path ""
         ""no longer than %zd UTF-8 bytes."", sizeof(path));
     goto done;
   }
  // GetStringUTFRegion does not pad with NUL
   (*env)->GetStringUTFRegion(env, jstr, 0, utfLength, path);

...

  //strtok_r can set rest pointer to NULL when no tokens found.
  //Causes JVM to crash in rest[0]
   for (check[0] = '/', check[1] = '\0', rest = path, token = """";
       token && rest[0];
        token = strtok_r(rest, ""/"", &rest)) {
{code}"
HADOOP-9632,TestShellCommandFencer will fail if there is a 'host' machine in the network,"TestShellCommandFencer will fail if there is a machine named ‘host’ in the network. The %target_address% environment variable used in the test was from result of InetSocketAddress.getAddress(). The method will return 'host/ip' instead of only 'host' when the host actually exists in the network. When the test comparing the log output, it assumes there is no ip in the address."
HADOOP-9630,Remove IpcSerializationType,"IpcSerializationType is assumed to be protobuf for the forseeable future. Not to be confused with RpcKind which still supports different RpcEngines. Let's remove the dead code, which can be confusing to maintain."
HADOOP-9629,Support Windows Azure Storage - Blob as a file system in Hadoop,"h2. Description
This JIRA incorporates adding a new file system implementation for accessing Windows Azure Storage - Blob from within Hadoop, such as using blobs as input to MR jobs or configuring MR jobs to put their output directly into blob storage.

h2. High level design
At a high level, the code here extends the FileSystem class to provide an implementation for accessing blob storage; the scheme wasb is used for accessing it over HTTP, and wasbs for accessing over HTTPS. We use the URI scheme: {code}wasb[s]://<container>@<account>/path/to/file{code} to address individual blobs. We use the standard Azure Java SDK (com.microsoft.windowsazure) to do most of the work. In order to map a hierarchical file system over the flat name-value pair nature of blob storage, we create a specially tagged blob named path/to/dir whenever we create a directory called path/to/dir, then files under that are stored as normal blobs path/to/dir/file. We have many metrics implemented for it using the Metrics2 interface. Tests are implemented mostly using a mock implementation for the Azure SDK functionality, with an option to test against a real blob storage if configured (instructions provided inside in README.txt).

h2. Credits and history
This has been ongoing work for a while, and the early version of this work can be seen in HADOOP-8079. This JIRA is a significant revision of that and we'll post the patch here for Hadoop trunk first, then post a patch for branch-1 as well for backporting the functionality if accepted. Credit for this work goes to the early team: [~minwei], [~davidlao], [~lengningliu] and [~stojanovic] as well as multiple people who have taken over this work since then (hope I don't forget anyone): [~dexterb], Johannes Klein, [~ivanmi], Michael Rys, [~mostafae], [~brian_swan], [~mikelid], [~xifang], and [~chuanliu].

h2. Test
Besides unit tests, we have used WASB as the default file system in our service product. (HDFS is also used but not as default file system.) Various different customer and test workloads have been run against clusters with such configurations for quite some time. The current version reflects to the version of the code tested and used in our production environment."
HADOOP-9625,HADOOP_OPTS not picked up by hadoop command,"When migrating from hadoop 1 to hadoop 2, one thing caused our users grief are those non-backward-compatible changes. This JIRA is to fix one of those changes:
  HADOOP_OPTS is not picked up any more by hadoop command

With Hadoop 1, HADOOP_OPTS will be picked up by hadoop command. With Hadoop 2, HADOOP_OPTS will be overwritten by the line in conf/hadoop_env.sh :

export HADOOP_OPTS=""-Djava.net.preferIPv4Stack=true""

We should fix this."
HADOOP-9624,"TestFSMainOperationsLocalFileSystem failed when the Hadoop test root path has ""X"" in its name","TestFSMainOperationsLocalFileSystem extends Class FSMainOperationsBaseTest. PathFilter FSMainOperationsBaseTest#TEST_X_FILTER checks if a path has ""x"" and ""X"" in its name. 
{code}
final private static PathFilter TEST_X_FILTER = new PathFilter() {
  public boolean accept(Path file) {
    if(file.getName().contains(""x"") || file.toString().contains(""X""))
      return true;
    else
      return false;
{code}




Some of the test cases construct a path by combining path ""TEST_ROOT_DIR"" with a customized partial path. 
The problem is that ""TEST_ROOT_DIR"" may also has ""X"" in its name. The path check will pass even if the customized partial path doesn't have ""X"". However, for this case the path filter is supposed to reject this path.

An easy fix is to change ""file.toString().contains(""X"")"" to ""file.getName().contains(""X"")"". Note that org.apache.hadoop.fs.Path.getName() only returns the final component of this path.
"
HADOOP-9623,Update jets3t dependency to  0.9.0 ,"Current version referenced in pom is 0.6.1 (Aug 2008), updating to 0.9.0 enables mvn-rpmbuild to build against system dependencies. http://jets3t.s3.amazonaws.com/RELEASE_NOTES.html"
HADOOP-9619,Mark stability of .proto files,
HADOOP-9618,Add thread which detects JVM pauses,"Often times users struggle to understand what happened when a long JVM pause (GC or otherwise) causes things to malfunction inside a Hadoop daemon. For example, a long GC pause while logging an edit to the QJM may cause the edit to timeout, or a long GC pause may make other IPCs to the NameNode timeout. We should add a simple thread which loops on 1-second sleeps, and if the sleep ever takes significantly longer than 1 second, log a WARN. This will make GC pauses obvious in logs."
HADOOP-9614,smart-test-patch.sh hangs for new version of patch (2.7.1),"patch -p0 -E --dry-run prints ""checking file "" for the new version of patch(2.7.1) rather than ""patching file"" as it did for older versions. This causes TMP2 to become empty, which causes the script to hang on this command forever:
    PREFIX_DIRS_AND_FILES=$(cut -d '/' -f 1 | sort | uniq)
"
HADOOP-9611,mvn-rpmbuild against google-guice > 3.0 yields missing cglib dependency,"Google guice 3.0 repackaged some external dependencies (cglib), which are broken out and exposed when running a mvn-rpmbuild against a stock Fedora 18 machine (3.1.2-6).  By adding the explicit dependency, it fixes the error and causes no impact to normal mvn builds."
HADOOP-9607,Fixes in Javadoc build,"It appears that some non-ascii characters have crept into the code, which cause an issue when building javadocs.  "
HADOOP-9605,Update junit dependency,"Simple update of the junit dependency to use newer version.  E.g. when running maven-rpmbuild on Fedora 18.
"
HADOOP-9604,Wrong Javadoc of FSDataOutputStream,"The following Javadoc of FSDataOutputStream is wrong.
{quote}
  buffers output through a \{@link BufferedOutputStream\} and creates a checksum file.
{quote}
FSDataOutputStream has nothing to do with a BufferedOutputStream. Neither it create a checksum file.
"
HADOOP-9599,hadoop-config.cmd doesn't set JAVA_LIBRARY_PATH correctly,"In Windows, hadoop-config.cmd uses the non-existent-variable HADOOP_CORE_HOME when setting the JAVA_LIBRAR_PATH variable. It should use HADOOP_HOME or HADOOP_COMMON_HOME.
The net effect is that running e.g. ""hdfs namenode"" directly (outside of hadoop command prompt) would error out with UnsatisfiedLinkError because it can't access hadoop.dll."
HADOOP-9598,Improve code coverage of RMAdminCLI,
HADOOP-9597,FileSystem open() API is not clear if FileNotFoundException is thrown when the path does not exist,"The current FileSystem open() method throws a generic IOException in its API specification.

Some FileSystem implementations (DFS, RawLocalFileSystem ...) throws more specific FileNotFoundException if the path does not exist.  Some throws IOException only (FTPFileSystem, HftpFileSystem ...). 

If we have a new FileSystem implementation, what should we follow exactly for open()?

What should the application expect in this case.

 "
HADOOP-9594,Update apache commons math dependency,"Current dependency is against 2.1 which has been deprecated, and numerous bugz have been fixed since then.  See http://commons.apache.org/proper/commons-math/changes-report.html for details.  This also affects mvn-rpmbuild when running on newer platforms e.g. Fedora 18. 
"
HADOOP-9593,stack trace printed at ERROR for all yarn clients without hadoop.home set,"This is the problem of HADOOP-9482 now showing up in a different application -one whose log4j settings haven't turned off all Shell logging.

Unless you do that, all yarn clients will have a stack trace at error in their logs, which is generating false alarms and is utterly pointless. Why does this merit a stack trace? Why log it at error? It's not an error for a client app to not have these values set as long as they have the relevant JARs on their classpath. And if they don't, they'll get some classpath error instead"
HADOOP-9582,"Non-existent file to ""hadoop fs -conf"" doesn't throw error","When we run :
hadoop fs -conf BAD_FILE -ls /
we expect hadoop to throw an error,but it doesn't."
HADOOP-9581,hadoop --config non-existent directory should result in error ,"Courtesy : [~cwchung]

{quote}Providing a non-existent config directory should result in error.

$ hadoop dfs -ls /  : shows Hadoop DFS directory
$ hadoop --config bad_config_dir dfs -ls : successful, showing Linux directory
{quote}"
HADOOP-9576,Make NetUtils.wrapException throw EOFException instead of wrapping it as IOException,"In case of EOFException, NetUtils is now wrapping it as IOException, we may want to throw EOFException as it is, since EOFException can happen when connection is lost in the middle, the client may want to explicitly handle such exception"
HADOOP-9574,Add new methods in AbstractDelegationTokenSecretManager for restoring RMDelegationTokens on RMRestart,"we're considering to add the following methods in AbstractDelegationTokenSecretManager for restoring RMDelegationTokens, these methods can also possibly be reused by hdfsDelegationTokenSecretManager, see YARN-638

  protected void storeNewMasterKey(DelegationKey key) throws IOException {
    return;
  }
  protected void removeStoredMasterKey(DelegationKey key) {
    return;
  }
  protected void storeNewToken(TokenIdent ident, long renewDate) {
    return;
  }
  protected void removeStoredToken(TokenIdent ident) throws IOException {

  }
  protected void updateStoredToken(TokenIdent ident, long renewDate) {
    return;
  }

Also move addPersistedDelegationToken in hdfs.DelegationTokenSecretManager, to AbstractDelegationTokenSecretManager
"
HADOOP-9566,Performing direct read using libhdfs sometimes raises SIGPIPE (which in turn throws SIGABRT) causing client crashes,Reading using libhdfs sometimes raises SIGPIPE (which in turn throws SIGABRT from JVM_handle_linux_signal). This can lead to crashes in the client application. It would be nice if libhdfs handled this signal internally.
HADOOP-9563,Fix incompatibility introduced by HADOOP-9523,"HADOOP-9523 changed the output format of the platform name util, so hbase won't start unless platform name is specifically set.

We could add a command line option to output in the new extended format and keep the old format when no option is given."
HADOOP-9560,metrics2#JvmMetrics should have max memory size of JVM,"metrics#JvmMetrics have the max memory size of JVM specified by -Xmx option.
metrics2#JvmMetrics doesn't have it but it should also have max memory size of JVM, because it's useful for users."
HADOOP-9559,When metrics system is restarted MBean names get incorrectly flagged as dupes,"In the Metrics2 system, every source gets registered as an MBean name, which gets put into a unique name pool in the singleton DefaultMetricsSystem object. The problem is that when the metrics system is shutdown (which unregisters the MBeans) this unique name pool is left as is, so if the metrics system is started again every attempt to register the same MBean names fails (exception is eaten and a warning is logged).
I think the fix here is to remove the name from the unique name pool if an MBean is unregistered, since it's OK at this point to add it again."
HADOOP-9557,hadoop-client excludes commons-httpclient,"hadoop-client pom excludes commons-httpclient jar, while this is used while running in pig in local mode and also httpfs clients
"
HADOOP-9556,disable HA tests on Windows that fail due to ZooKeeper client connection management bug,"ZOOKEEPER-1702 tracks a client connection management bug that manifests frequently on Windows.  Until a patch is available in a ZooKeeper release, we need to disable the failing tests temporarily on Windows."
HADOOP-9555,HA functionality that uses ZooKeeper may experience inadvertent TCP RST and miss session expiration event due to bug in client connection management,ZOOKEEPER-1702 tracks a client connection management bug.  The bug can cause an unexpected TCP RST that ultimately prevents delivery of a session expiration event.  The symptoms of the bug seem to show up more frequently on Windows than on other platforms (though it's not really a Windows-specific bug).
HADOOP-9553,TestAuthenticationToken fails on Windows,"Likely test issue.
"
HADOOP-9550,Remove aspectj dependency,"Per discussion on HDFS-2261 and Nicholas' suggestion, we should remove the aspectj dependencies for the moment till the AOP fault-injection tests are fixed to work with maven."
HADOOP-9549,WebHdfsFileSystem hangs on close(),"When close() is called via fs shoutdown hook, the synchronized method, removeRenewAction() hangs. This is because DelegationTokenRenewer calls DelayQueue.take() inside a synchronized block. Since this is a blocking call, it hangs forever."
HADOOP-9544,backport UTF8 encoding fixes to branch-1,"The trunk code has received numerous bug fixes related to UTF8 encoding.  I recently observed a branch-1-based cluster fail to load its fsimage due to these bugs.  I've confirmed that the bug fixes existing on trunk will resolve this, so I'd like to backport to branch-1."
HADOOP-9543,TestFsShellReturnCode may fail in branch-1,"There is a hardcoded username ""admin"" in TestFsShellReturnCode. If ""admin"" does not exist in the local fs, the test may fail.  Before HADOOP-9502, the failure of the command is ignored silently, i.e. the command returns success even if it indeed failed."
HADOOP-9540,Expose the InMemoryS3 and S3N FilesystemStores implementations for Unit testing.,"The stub implementations available for InMemoryFileSytemStores for S3 and S3N are currently restricted in package scope. These are quiet handy utilities for unit testing and nice to be exposed in a public scope. 

Or even conveniently I have added simple wrapper InMemoryFileSystem implementations for these stores so that it can be easily leveraged by any interested developers."
HADOOP-9537,Backport AIX patches to branch-1,"Backport couple of trivial Jiras to branch-1.

HADOOP-9305  Add support for running the Hadoop client on 64-bit AIX
HADOOP-9283  Add support for running the Hadoop client on AIX
"
HADOOP-9532,HADOOP_CLIENT_OPTS is appended twice by Windows cmd scripts,"This problem was reported initially for the shell scripts in HADOOP-9455.  This issue tracks the same problem for the Windows cmd scripts.  Appending HADOOP_CIENT_OPTS twice can cause an incorrect JVM launch, particularly if trying to set remote debugging flags."
HADOOP-9527,Add symlink support to LocalFileSystem on Windows,"Multiple test cases are broken. I didn't look at each failure in detail.

The main cause of the failures appears to be that RawLocalFS.readLink() does not work on Windows. We need ""winutils readlink"" to fix the test."
HADOOP-9526,TestShellCommandFencer and TestShell fail on Windows,"The following TestShellCommandFencer tests fail on Windows.
# testTargetAsEnvironment
# testConfAsEnvironment
# testTargetAsEnvironment

TestShell#testInterval also fails.

All failures look like test issues."
HADOOP-9525,Add tests that validate winutils chmod behavior on folders,As part of HADOOP-9413 and HDFS-4610 I realized that we don't have tests that validate the behavior of winutils chmod on folders. It would be good to add additional tests to both validate the functionality and use them as means to document some subtle differences in behavior between Unix and Windows chmod.
HADOOP-9524,Fix ShellCommandFencer to work on Windows,ShellcommandFencer has a hard-coded dependency on bash. Since we no longer require cygwin/bash on Windows we must fix it to use cmd.exe instead.
HADOOP-9523,Provide a generic IBM java vendor flag in PlatformName.java to support non-Sun JREs,"There are several different points between Sun jdk & IBM jdk, so there is a need to provide a generic IBM java vendor flag. So enhance PlatformName.java to add Java vendor information."
HADOOP-9517,Document Hadoop Compatibility,"As we get ready to call hadoop-2 stable we need to better define 'Hadoop Compatibility'.

http://wiki.apache.org/hadoop/Compatibility is a start, let's document requirements clearly and completely."
HADOOP-9515,Add general interface for NFS and Mount,"These is the general interface implementation for NFS and Mount protocol, e.g., some protocol related data structures and etc. It doesn't include the file system specific implementations."
HADOOP-9511,Adding support for additional input streams (FSDataInputStream and RandomAccessFile) in SecureIOUtils.,"At present we can only get secured InputStream. However we want other types of input streams to avoid possible security attacks.
YARN-578 needs this fix."
HADOOP-9509,Implement ONCRPC and XDR,This is to track the implementation of ONCRPC(rfc5531) and XDR(rfc4506). 
HADOOP-9507,LocalFileSystem rename() is broken in some cases when destination exists,"The rename() method in RawLocalFileSystem uses FileUtil.copy() without realizing that FileUtil.copy() has a special behavior that if you're copying /foo to /bar and /bar exists and is a directory, it'll copy /foo inside /bar instead of overwriting it, which is not what rename() wants. So you end up with weird behaviors like in this repro:

{code}
c:
cd \
md Foo
md Bar
md Foo\X
md Bar\X
hadoop fs -mv file:///c:/Foo file:///c:/Bar
{code}

At the end of this, you would expect to find only Bar\X, but you instead find Bar\X\X."
HADOOP-9504,MetricsDynamicMBeanBase has concurrency issues in createMBeanInfo,"Please see HBASE-8416 for detail information.
we need to take care of the synchronization for HashMap put(), otherwise it may lead to spin loop."
HADOOP-9503,Remove sleep between IPC client connect timeouts,"HADOOP-9106 introduced a configurable retry timeout when there are socket timeouts errors.

Since the IPC client is used by a wide variety of applications including realtime/online ones (like hbase + hdfs), we sometimes need fast fail timeouts which are < 3 seconds. Achieving such a timeout for connect is very difficult with a 1 second sleep in b/w retries. It would be good to remove this sleep. "
HADOOP-9502,chmod does not return error exit codes for some exceptions,"When some dfs operations fail due to SnapshotAccessControlException, valid exit codes are not returned.

E.g:
{noformat}
-bash-4.1$  hadoop dfs -chmod -R 755 /user/foo/hdfs-snapshots/test0/.snapshot/s0
chmod: changing permissions of 'hdfs://<namenode>:8020/user/foo/hdfs-snapshots/test0/.snapshot/s0':org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotAccessControlException: Modification on read-only snapshot is disallowed

-bash-4.1$ echo $?
0

-bash-4.1$  hadoop dfs -chown -R hdfs:users /user/foo/hdfs-snapshots/test0/.snapshot/s0
chown: changing ownership of 'hdfs://<namenode>:8020/user/foo/hdfs-snapshots/test0/.snapshot/s0':org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotAccessControlException: Modification on read-only snapshot is disallowed

-bash-4.1$ echo $?
0
{noformat}

Similar problems exist for some other exceptions such as SafeModeException."
HADOOP-9500,TestUserGroupInformation#testGetServerSideGroups fails on Windows due to failure to find winutils.exe,"The test attempts to run the winutils groups command, but the initialization logic fails to find the path to winutils.exe."
HADOOP-9496,Bad merge of HADOOP-9450 on branch-2 breaks all bin/hadoop calls that need HADOOP_CLASSPATH ,"Merge of HADOOP-9450 to branch-2 is broken for hadoop-config.sh

on trunk

http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh?r1=1453486&r2=1469214&pathrev=1469214

vs on branch-2

http://svn.apache.org/viewvc/hadoop/common/branches/branch-2/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh?r1=1390222&r2=1469215

This is breaking all hadoop client code which needs HADOOP_CLASSPATH to be set correctly."
HADOOP-9495,"Define behaviour of Seekable.seek(), write tests, fix all hadoop implementations for compliance","{{Seekable.seek()}} seems a good starting point for specifying, testing and implementing FS API compliance: one method, relatively non-ambiguous semantics, easily assessed used in the Hadoop codebase. Specify and test it first"
HADOOP-9494,Excluded auto-generated and examples code from clover reports,"applicable to branch-0.23, branch-2, trunk"
HADOOP-9492,Fix the typo in testConf.xml to make it consistent with FileUtil#copy(),HADOOP-9473 fixed a typo in FileUtil#copy(). We need to fix the same typo in testConf.xml accordingly. Otherwise TestCLI will fail in branch-1.
HADOOP-9490,LocalFileSystem#reportChecksumFailure not closing the checksum file handle before rename,"LocalFileSystem#reportChecksumFailure is not closing the open stream on the checksum file before it moves it to the bad_files folder, what causes the operation to fail on Windows.

TestLocalFileSystem fail on Windows because of this:
{code}
testReportChecksumFailure(org.apache.hadoop.fs.TestLocalFileSystem)  Time elapsed: 31 sec  <<< FAILURE!
java.lang.AssertionError: 
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.hadoop.fs.TestLocalFileSystem.testReportChecksumFailure(TestLocalFileSystem.java:335)
{code}"
HADOOP-9488,FileUtil#createJarWithClassPath only substitutes environment variables from current process environment/does not support overriding when launching new process,"{{FileUtil#createJarWithClassPath}} always uses {{System#getEnv}} for substitution of environment variables in the classpath bundled into the jar manifest.  YARN launches container processes with a different set of environment variables, so the method needs to support providing environment variables different from the current process."
HADOOP-9487,Deprecation warnings in Configuration should go to their own log or otherwise be suppressible,"Running local pig jobs triggers large quantities of warnings about deprecated properties -something I don't care about as I'm not in a position to fix without delving into Pig. 

I can suppress them by changing the log level, but that can hide other warnings that may actually matter.

If there was a special Configuration.deprecated log for all deprecation messages, this log could be suppressed by people who don't want noisy logs"
HADOOP-9486,Promote Windows and Shell related utils from YARN to Hadoop Common,"This is happening as part of YARN-493, this is a tracking ticket for common changes."
HADOOP-9485,No default value in the code for hadoop.rpc.socket.factory.class.default,"In {{core-default.xml}}, {{hadoop.rpc.socket.factory.class.default}} defaults to {{org.apache.hadoop.net.StandardSocketFactory}}.  However, in {{CommonConfigurationKeysPublic.java}}, there is no default for this key.  This is inconsistent (defaults in the code versus defaults in the XML files should match.)  It also leads to problems with {{RemoteBlockReader2}}, since the default {{SocketFactory}} creates a {{Socket}} without an associated channel.  {{RemoteBlockReader2}} cannot use such a {{Socket}}.

This bug only really becomes apparent when you create a {{Configuration}} using the {{Configuration(loadDefaults=true)}} constructor.  Thanks to AB Srinivasan for his help in discovering this bug."
HADOOP-9483,winutils support for readlink command,The current codebase relies on the Unix readlink command to determine the target of a symlink on the local file system.  winutils currently does not support this functionality on Windows.  Adding the command to winutils will prevent the need to use GnuWin32 or Cygwin for readlink support.
HADOOP-9481,Broken conditional logic with HADOOP_SNAPPY_LIBRARY,"The problem is a regression introduced by recent fix https://issues.apache.org/jira/browse/HADOOP-8562 .
That fix makes some improvements for Windows platform, but breaks native code work on Unix.
Namely, let's see the diff HADOOP-8562 of the file hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c :  
{noformat}
--- hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c
+++ hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c
@@ -16,12 +16,18 @@
  * limitations under the License.
  */

-#include <dlfcn.h>
+
+#if defined HADOOP_SNAPPY_LIBRARY
+
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>

+#ifdef UNIX
+#include <dlfcn.h>
 #include ""config.h""
+#endif // UNIX
+
 #include ""org_apache_hadoop_io_compress_snappy.h""
 #include ""org_apache_hadoop_io_compress_snappy_SnappyCompressor.h""

@@ -81,7 +87,7 @@ JNIEXPORT jint JNICALL Java_org_apache_hadoop_io_compress_snappy_SnappyCompresso
   UNLOCK_CLASS(env, clazz, ""SnappyCompressor"");

   if (uncompressed_bytes == 0) {
-    return 0;
+    return (jint)0;
   }

   // Get the output direct buffer
@@ -90,7 +96,7 @@ JNIEXPORT jint JNICALL Java_org_apache_hadoop_io_compress_snappy_SnappyCompresso
   UNLOCK_CLASS(env, clazz, ""SnappyCompressor"");

   if (compressed_bytes == 0) {
-    return 0;
+    return (jint)0;
   }

   /* size_t should always be 4 bytes or larger. */
@@ -109,3 +115,5 @@ JNIEXPORT jint JNICALL Java_org_apache_hadoop_io_compress_snappy_SnappyCompresso
   (*env)->SetIntField(env, thisj, SnappyCompressor_uncompressedDirectBufLen, 0);
   return (jint)buf_len;
 }
+
+#endif //define HADOOP_SNAPPY_LIBRARY
{noformat}

Here we see that all the class implementation got enclosed into ""if defined HADOOP_SNAPPY_LIBRARY"" directive, and the point is that ""HADOOP_SNAPPY_LIBRARY"" is *not* defined. 
This causes the class implementation to be effectively empty, what, in turn, causes the UnsatisfiedLinkError to be thrown in the runtime upon any attempt to invoke the native methods implemented there.
The actual intention of the authors of HADOOP-8562 was (as we suppose) to invoke ""include config.h"", where ""HADOOP_SNAPPY_LIBRARY"" is defined. But currently it is *not* included because it resides *inside* ""if defined HADOOP_SNAPPY_LIBRARY"" block.

Similar situation with ""ifdef UNIX"", because UNIX or WINDOWS variables are defined in ""org_apache_hadoop.h"", which is indirectly included through ""include ""org_apache_hadoop_io_compress_snappy.h"""", and in the current code this is done *after* code ""ifdef UNIX"", so in the current code the block ""ifdef UNIX"" is *not* executed on UNIX.

The suggested patch fixes the described problems by reordering the ""include"" and ""if"" preprocessor directives accordingly, bringing the methods of class org.apache.hadoop.io.compress.snappy.SnappyCompressor back to work again.

Of course, Snappy native libraries must be installed to build and invoke snappy native methods.

(Note: there was a mistype in commit message: 8952 written in place of 8562: 
HADOOP-8952. Enhancements to support Hadoop on Windows Server and Windows Azure environments. Contributed by Ivan Mitic, Chuan Liu, Ramya Sunil, Bikas Saha, Kanna Karanam, John Gordon, Brandon Li, Chris Nauroth, David Lao, Sumadhur Reddy Bolli, Arpit Agarwal, Ahmed El Baz, Mike Liddell, Jing Zhao, Thejas Nair, Steve Maine, Ganeshan Iyer, Raja Aluri, Giridharan Kesavan, Ramya Bharathi Nimmagadda.
   git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453486 13f79535-47bb-0310-9956-ffa450edef68
)"
HADOOP-9478,Fix race conditions during the initialization of Configuration related to deprecatedKeyMap,"When we lanuch the client appliation which use kerberos security,the FileSystem can't be create because the exception ' java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.SecurityUtil'.

I check the exception stack trace,it maybe caused by the unsafe get operation of the deprecatedKeyMap which used by the org.apache.hadoop.conf.Configuration.

So I write a simple test case:

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.hdfs.HdfsConfiguration;

public class HTest {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.addResource(""core-site.xml"");
        conf.addResource(""hdfs-site.xml"");
        FileSystem fileSystem = FileSystem.get(conf);
        System.out.println(fileSystem);
        System.exit(0);
    }
}

Then I launch this test case many times,the following exception is thrown:

Exception in thread ""TGT Renewer for XXX"" java.lang.ExceptionInInitializerError
     at org.apache.hadoop.security.UserGroupInformation.getTGT(UserGroupInformation.java:719)
     at org.apache.hadoop.security.UserGroupInformation.access$1100(UserGroupInformation.java:77)
     at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:746)
     at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 16
     at java.util.HashMap.getEntry(HashMap.java:345)
     at java.util.HashMap.containsKey(HashMap.java:335)
     at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1989)
     at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1867)
     at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1785)
     at org.apache.hadoop.conf.Configuration.get(Configuration.java:712)
     at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:731)
     at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1047)
     at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:76)
     ... 4 more
Exception in thread ""main"" java.io.IOException: Couldn't create proxy provider class org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
     at org.apache.hadoop.hdfs.NameNodeProxies.createFailoverProxyProvider(NameNodeProxies.java:453)
     at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:133)
     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
     at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
     at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
     at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:162)
     at HTest.main(HTest.java:11)
Caused by: java.lang.reflect.InvocationTargetException
     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
     at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
     at org.apache.hadoop.hdfs.NameNodeProxies.createFailoverProxyProvider(NameNodeProxies.java:442)
     ... 11 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.SecurityUtil
     at org.apache.hadoop.net.NetUtils.createSocketAddrForHost(NetUtils.java:231)
     at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:211)
     at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:159)
     at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)
     at org.apache.hadoop.hdfs.DFSUtil.getAddressesForNameserviceId(DFSUtil.java:452)
     at org.apache.hadoop.hdfs.DFSUtil.getAddresses(DFSUtil.java:434)
     at org.apache.hadoop.hdfs.DFSUtil.getHaNnRpcAddresses(DFSUtil.java:496)
     at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.<init>(ConfiguredFailoverProxyProvider.java:88)
     ... 16 more


If the HashMap used at multi-thread enviroment,not only the put operation be synchronized,the get operation(eg. containKey) should be synchronzied too.

The simple solution is trigger the init of SecurityUtil before creating the FileSystem,but I think it's should be synchronized for get of deprecatedKeyMap.

Thanks. "
HADOOP-9476,Some test cases in TestUserGroupInformation fail if ran after testSetLoginUser.,"HADOOP-9352 added a new test case testSetLoginUser. If it runs prior to other test cases, some of them fail."
HADOOP-9473,typo in FileUtil copy() method,"typo:
{code}
Index: src/core/org/apache/hadoop/fs/FileUtil.java
===================================================================
--- src/core/org/apache/hadoop/fs/FileUtil.java	(revision 1467295)
+++ src/core/org/apache/hadoop/fs/FileUtil.java	(working copy)
@@ -178,7 +178,7 @@
     // Check if dest is directory
     if (!dstFS.exists(dst)) {
       throw new IOException(""`"" + dst +""': specified destination directory "" +
-                            ""doest not exist"");
+                            ""does not exist"");
     } else {
       FileStatus sdst = dstFS.getFileStatus(dst);
       if (!sdst.isDir()) 
{code}"
HADOOP-9471,"hadoop-client wrongfully excludes jetty-util JAR, breaking webhdfs","WebHdfsFileSystem uses jetty-util's JSON class.

hadoop-client excludes that JAR, applications built using hadoop-client POM fail:

{code}java.lang.NoClassDefFoundError: org/mortbay/util/ajax/JSON
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:277)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$Runner.getResponse(WebHdfsFileSystem.java:561)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$Runner.run(WebHdfsFileSystem.java:480)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:413)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getHdfsFileStatus(WebHdfsFileSystem.java:580)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getFileStatus(WebHdfsFileSystem.java:591)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1332)
{code}
"
HADOOP-9470,eliminate duplicate FQN tests in different Hadoop modules,"In different modules of Hadoop project there are tests with identical FQNs (fully qualified name).
For example, test with FQN org.apache.hadoop.util.TestRunJar is contained in 2 modules:
 ./hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestRunJar.java
 ./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/util/TestRunJar.java 

Such situation causes certain problems with test result reporting and other code analysis tools (such as Clover, e.g.) because almost all the tools identify the tests by their Java FQN.

So, I suggest to rename all such test classes to avoid duplicate FQNs in different modules. I'm attaching simple shell script that can find all such problematic test classes. Currently Hadoop trunk has 9 such test classes, they are:
$ ~/bin/find-duplicate-fqns.sh
# Module [./hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-classes] has 7 duplicate FQN tests:
org.apache.hadoop.ipc.TestSocketFactory
org.apache.hadoop.mapred.TestFileOutputCommitter
org.apache.hadoop.mapred.TestJobClient
org.apache.hadoop.mapred.TestJobConf
org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter
org.apache.hadoop.util.TestReflectionUtils
org.apache.hadoop.util.TestRunJar
# Module [./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/target/test-classes] has 2 duplicate FQN tests:
org.apache.hadoop.yarn.TestRecordFactory
org.apache.hadoop.yarn.TestRPCFactories
"
HADOOP-9469,mapreduce/yarn source jars not included in dist tarball,the mapreduce and yarn sources jars don't get included into the distribution tarball.  It seems they get built by default just aren't assembled.
HADOOP-9467,Metrics2 record filtering (.record.filter.include/exclude) does not filter by name,Filtering by record considers only the record's tag for filtering and not the record's name.
HADOOP-9459,"ActiveStandbyElector can join election even before Service HEALTHY, and results in null data at ActiveBreadCrumb","ActiveStandbyElector can store null at ActiveBreadCrumb in the below race condition. At further all failovers will fail resulting NPE.

1. ZKFC restarted.
2. due to machine busy, first zk connection is expired even before the health monitoring returned the status.
3. On re-establishment transitionToActive will be called, at this time appData will be null,
4. So now ActiveBreadCrumb will have null.
5. After this any failovers will fail throwing 

{noformat}java.lang.NullPointerException
	at org.apache.hadoop.util.StringUtils.byteToHexString(StringUtils.java:171)
	at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:892)
	at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:797)
	at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:475)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:545)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497){noformat}


Should not join the election before service is HEALTHY
"
HADOOP-9458,"In branch-1, RPC.getProxy(..) may call proxy.getProtocolVersion(..) without retry",RPC.getProxy(..) may call proxy.getProtocolVersion(..) without retry even when client has specified retry in the conf.
HADOOP-9457,add an SCM-ignored XML filename to keep secrets in (auth-keys.xml?),"to avoid accidentally checking in secrets, I keep auth keys for things like AWS in file called {{auth-keys.xml}} alongside the {{test/resources/core-site.xml}} file, then XInclude them. I also have a global gitignore set up ignore files with that name.

I propose having a standard name for XML files containing such secrets (we could use auth-keys.xml or something else, and set up {{hadoop-trunk/.gitignore}} and {{svn:ignore}} to ignore them. That way, nobody else will check them in by accident"
HADOOP-9455,HADOOP_CLIENT_OPTS appended twice causes JVM failures,"If you set HADOOP_CLIENT_OPTS and run hadoop, you'll find that the HADOOP_CLIENT_OPTS value gets appended twice, and leads to JVM start failures for cases like adding debug flags.

For example,

{noformat}
HADOOP_CLIENT_OPTS='-agentlib:jdwp=transport=dt_socket,address=localhost:9009,server=y,suspend=y' hadoop jar anything

ERROR: Cannot load this JVM TI agent twice, check your java command line for duplicate jdwp options.
Error occurred during initialization of VM
agent library failed to init: jdwp
{noformat}"
HADOOP-9454,Support multipart uploads for s3native,"The s3native filesystem is limited to 5 GB file uploads to S3, however the newest version of jets3t supports multipart uploads to allow storing multi-TB files. While the s3 filesystem lets you bypass this restriction by uploading blocks, it is necessary for us to output our data into Amazon's publicdatasets bucket which is shared with others.

Amazon has added a similar feature to their distribution of hadoop as has MapR.

Please note that while this supports large copies, it does not yet support parallel copies because jets3t doesn't expose an API yet that allows it without hadoop controlling the threads unlike with upload.

By default, this patch does not enable multipart uploads. To enable them and parallel uploads:

add the following keys to your hadoop config:

<property>
  <name>fs.s3n.multipart.uploads.enabled</name>
  <value>true</value>
</property>
<property>
  <name>fs.s3n.multipart.uploads.block.size</name>
  <value>67108864</value>
</property>
<property>
  <name>fs.s3n.multipart.copy.block.size</name>
  <value>5368709120</value>
</property>

create a /etc/hadoop/conf/jets3t.properties file with or similar to:

storage-service.internal-error-retry-max=5
storage-service.disable-live-md5=false
threaded-service.max-thread-count=20
threaded-service.admin-max-thread-count=20
s3service.max-thread-count=20
s3service.admin-max-thread-count=20"
HADOOP-9451,Node with one topology layer should be handled as fault topology when NodeGroup layer is enabled,"Currently, nodes with one layer topology are allowed to join in the cluster that with enabling NodeGroup layer which cause some exception cases. 
When NodeGroup layer is enabled, the cluster should assumes that at least two layer (Rack/NodeGroup) is valid topology for each nodes, so should throw exceptions for one layer node in joining."
HADOOP-9450,HADOOP_USER_CLASSPATH_FIRST is not honored; CLASSPATH is PREpended instead of APpended,"On line 133 of the hadoop shell wrapper, CLASSPATH is set as:

CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}

Notice that the built-up CLASSPATH, along with all the libs and unwanted JARS are pre-pended BEFORE the user's HADOOP_CLASSPATH.  Therefore there is no way to put your own JARs in front of those that the hadoop wrapper script sets.

We propose a patch that reverses this order.  Failing that, we would like to add a command line option to override this behavior and enable a user's JARs to be found before the wrong ones in the Hadoop library paths.

We always welcome your opinions."
HADOOP-9446,Support Kerberos HTTP SPNEGO authentication for non-SUN JDK,"Class KerberosAuthenticator and KerberosAuthenticationHandler currently only support running with SUN JDK when Kerberos is enabled. In order to support  alternative JDKs like IBM JDK which has different options supported by Krb5LoginModule and different login module classes, the HTTP Kerberos authentication classes need to be changed.

In addition, NT_GSS_KRB5_PRINCIPAL, which is used in KerberosAuthenticator to get the corresponding oid instance, is a field defined in SUN JDK, but not in IBM JDK.

This JIRA is to fix the existing problems and add support for Kerberos HTTP SPNEGO authentication with non-SUN　JDK."
HADOOP-9444,$var shell substitution in properties are not expanded in hadoop-policy.xml,"During BigTop 0.6.0 release test cycle, [~rvs] came around the following problem:
{noformat}
013-03-26 15:37:03,573 FATAL
org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting
NodeManager
org.apache.hadoop.yarn.YarnException: Failed to Start
org.apache.hadoop.yarn.server.nodemanager.NodeManager
        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:199)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:359)
Caused by: org.apache.avro.AvroRuntimeException:
java.lang.reflect.UndeclaredThrowableException
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:162)
        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
        ... 3 more
Caused by: java.lang.reflect.UndeclaredThrowableException
        at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:128)
        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:61)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:199)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:158)
        ... 4 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException):
User yarn/ip-10-46-37-244.ec2.internal@BIGTOP (auth:KERBEROS) is not
authorized for protocol interface
org.apache.hadoop.yarn.server.api.ResourceTrackerPB, expected client
Kerberos principal is yarn/ip-10-46-37-244.ec2.internal@BIGTOP
        at org.apache.hadoop.ipc.Client.call(Client.java:1235)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
        at $Proxy26.registerNodeManager(Unknown Source)
        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)
        ... 6 more

{noformat}

The most significant part is 
{{User yarn/ip-10-46-37-244.ec2.internal@BIGTOP (auth:KERBEROS) is not authorized for protocol interface  org.apache.hadoop.yarn.server.api.ResourceTrackerPB}} indicating that ResourceTrackerPB hasn't been annotated with {{@KerberosInfo}} nor {{@TokenInfo}}"
HADOOP-9443,Port winutils static code analysis change to trunk,"We hit a problem in winutils when running tests on Windows. The static code analysis change will fix the problem. More specifically, the old code always assumes the security descriptor get from GetSecurityDescriptorControl() is relative, and will make an absolute security descriptor out of it. The new absolute security descriptor will then pass to SetSecurityDescriptorDacl() to set permissions on the file. If the security descriptor is absolute, the new absolute security descriptor will be NULL, and we will run into the problem. This is what happened exactly in our case. The fix from static code analysis will solve the problem."
HADOOP-9439,JniBasedUnixGroupsMapping: fix some crash bugs,"JniBasedUnixGroupsMapping has some issues.

* sometimes on error paths variables are freed prior to being initialized
* re-allocate buffers less frequently (can reuse the same buffer for multiple calls to getgrnam)
* allow non-reentrant functions to be used, to work around client bugs
* don't throw IOException from JNI functions if the JNI functions do not declare this checked exception.
* don't bail out if only one group name among all the ones associated with a user can't be looked up."
HADOOP-9437,TestNativeIO#testRenameTo fails on Windows due to assumption that POSIX errno is embedded in NativeIOException,"HDFS-4428 added a detailed error message for failures to rename files by embedding the POSIX errno in the {{NativeIOException}}.  On Windows, the mapping of errno is not performed, so the errno enum value will not be present in the {{NativeIOException}}."
HADOOP-9435,Support building the JNI code against the IBM JVM,"When native build hadoop-common-project with IBM java using command like: 
mvn package -Pnative
it will exist the following errors.
     [exec] -- Configuring incomplete, errors occurred!
     [exec] JAVA_HOME=, JAVA_JVM_LIBRARY=/home/louis/ibm-java-i386-60/jre/lib/i386/classic/libjvm.so
     [exec] JAVA_INCLUDE_PATH=/home/louis/ibm-java-i386-60/include, JAVA_INCLUDE_PATH2=JAVA_INCLUDE_PATH2-NOTFOUND
     [exec] CMake Error at JNIFlags.cmake:113 (MESSAGE):
     [exec]   Failed to find a viable JVM installation under JAVA_HOME.
     [exec] Call Stack (most recent call first):
     [exec]   CMakeLists.txt:24 (include)

The reason is that IBM java uses $JAVA_HOME/include/jniport.h instead of $JAVA_HOME/include/jni_md.h in non-IBM java.

[exec] /usr/lib/jvm/java-1.6.0-ibm-1.6.0.12.0.x86_64/jre/lib/amd64/default/libjvm.so: undefined reference to `dlsym'
     [exec] /usr/lib/jvm/java-1.6.0-ibm-1.6.0.12.0.x86_64/jre/lib/amd64/default/libjvm.so: undefined reference to `dlerror'
     [exec] /usr/lib/jvm/java-1.6.0-ibm-1.6.0.12.0.x86_64/jre/lib/amd64/default/libjvm.so: undefined reference to `dladdr'
     [exec] /usr/lib/jvm/java-1.6.0-ibm-1.6.0.12.0.x86_64/jre/lib/amd64/default/libjvm.so: undefined reference to `dlopen'
     [exec] /usr/lib/jvm/java-1.6.0-ibm-1.6.0.12.0.x86_64/jre/lib/amd64/default/libjvm.so: undefined reference to `dlclose'
     [exec] collect2: ld returned 1 exit status
     [exec] make[2]: *** [test_libhdfs_ops] Error 1
     [exec] make[1]: *** [CMakeFiles/test_libhdfs_ops.dir/all] Error 2
     [exec] make: *** [all] Error 

The reason is libjvm.so need libdl when linking."
HADOOP-9434,Backport HADOOP-9267 to branch-1,"Currently in hadoop 1.1.2, if user issue ""bin/hadoop help"" in command line, it will throw below exception. We can improve this to print the usage message.
===============================================
Exception in thread ""main"" java.lang.NoClassDefFoundError: help
===============================================

This issue is already resolved in HADOOP-9267 in trunk, so we only need to backport it into branch-1"
HADOOP-9432,Add support for markdown .md files in site documentation,"The ""markdown"" syntax for marking up text is something which the {{mvn site}} build can be set up to support alongside the existing APT formatted text.

Markdown offers many advantages
 # It's more widely understood.
 # There's tooling support in various text editors (TextMate, an IDEA plugin and others)
 # It can be directly rendered in github
 # the {{.md}} files can be named {{.md.vm}} to trigger velocity preprocessing, at the expense of direct viewing in github

feature #3 is good as it means that you can point people directly at a doc via a github mirror, and have it rendered. 

I propose adding the options to Maven to enable content be written as {{.md}} and {{.md.vm}} files in the directory {{src/site/markdown}}. This does not require any changes to the existing {{.apt}} files, which can co-exist and cross-reference each other.

"
HADOOP-9430,TestSSLFactory fails on IBM JVM,
HADOOP-9429,TestConfiguration fails with IBM JAVA ,"failure in assertTrue(""Result has proper header"", result.startsWith(
		        ""<?xml version=\""1.0\"" encoding=\""UTF-8\"" standalone=\""no\""?><configuration>""));
"
HADOOP-9425,Add error codes to rpc-response,
HADOOP-9421,Convert SASL to use ProtoBuf and provide negotiation capabilities,
HADOOP-9420,"Add percentile or max metric for rpcQueueTime, processing time","Currently, we only export averages for rpcQueueTime and rpcProcessingTime. These metrics are most useful when looking at timeouts and slow responses, which in my experience are often caused by momentary spikes in load, which won't show up in averages over the 15+ second time intervals often used by metrics systems. We should collect at least the max queuetime and processing time over each interval, or the percentiles if it's not too expensive."
HADOOP-9418,Add symlink resolution support to DistributedFileSystem,Add symlink resolution support to DistributedFileSystem as well as tests.
HADOOP-9417,Support for symlink resolution in LocalFileSystem / RawLocalFileSystem,Add symlink resolution support to LocalFileSystem/RawLocalFileSystem as well as tests.
HADOOP-9416,Add new symlink resolution methods in FileSystem and FileSystemLinkResolver,"Add new methods for symlink resolution to FileSystem, and add resolution support for FileSystem to FileSystemLinkResolver."
HADOOP-9414,Refactor out FSLinkResolver and relevant helper methods,Can reuse the existing FsLinkResolver within FileContext for FileSystem as well. Also move around / pull out some other reusable functions.
HADOOP-9413,Introduce common utils for File#setReadable/Writable/Executable and File#canRead/Write/Execute that work cross-platform,"So far, we've seen many unittest and product bugs in Hadoop on Windows because Java's APIs that manipulate with permissions do not work as expected. We've addressed many of these problems on one-by-one basis (by either changing code a bit or disabling the test). While debugging the remaining unittest failures we continue to run into the same patterns of problems, and instead of addressing them one-by-one, I propose that we expose a set of equivalent wrapper APIs that will work well for all platforms. 

Scanning thru the codebase, this will actually be a simple change as there are very few places that use File#setReadable/Writable/Executable and File#canRead/Write/Execute (5 files in Common, 9 files in HDFS). 

HADOOP-8973 contains additional context on the problem."
HADOOP-9408,misleading description for net.topology.table.file.name property in core-default.xml,"<property>
  <name>net.topology.table.file.name</name>
  <value></value>
  <description> The file name for a topology file, which is used when the
    *net.topology.script.file.name* property is set to
    org.apache.hadoop.net.TableMapping. The file format is a two column text
    file, with columns separated by whitespace. The first column is a DNS or
    IP address and the second column specifies the rack where the address maps.
    If no entry corresponding to a host in the cluster is found, then 
    /default-rack is assumed.
  </description>
</property>

Marked property name(net.topology.script.file.name) should be
{code}
net.topology.node.switch.mapping.impl
{code}
"
HADOOP-9407,commons-daemon 1.0.3 dependency has bad group id causing build issues,"The commons-daemon dependency of the hadoop-hdfs module has been at version 1.0.3 for a while. However, 1.0.3 has a pretty well-known groupId error in its pom (""org.apache.commons"" as opposed to ""commons-daemon""). This problem has since been corrected on commons-daemon starting 1.0.4.

This causes build problems for many who depend on hadoop-hdfs directly and indirectly, however. Maven can skip over this metadata inconsistency. But other less forgiving build systems such as ivy and gradle have much harder time working around this problem. For example, in gradle, pretty much the only obvious way to work around this is to override this dependency version."
HADOOP-9406,hadoop-client leaks dependency on JDK tools jar,"hadoop-client leaks out JDK tools jar as dependency. 

JDK tools jar is defined as a system dependency for hadoop-annotation/jdiff/javadocs purposes, if not done javadoc generation fails.

The problem is that in the way it is defined now, this dependency ends up leaking to hadoop-client and downstream projects that depend on hadoop-client may end up including/bundling JDK tools JAR."
HADOOP-9405,TestGridmixSummary#testExecutionSummarizer is broken,"HADOOP-9252 changed how human readable numbers are printed, and required updating a number of test cases. This one was missed because the Jenkins precommit job apparently isn't running the tests in hadoop-tools."
HADOOP-9401,CodecPool: Add counters for number of (de)compressors leased out,"CodecPool enables reusing compressors/decompressors created. However, the onus is on the user to return the compressors/decompressors to the pool, and can be easily missed.

It would be quite handy to keep track of the total number of compressors/decompressors created and helper methods to check outstanding (not returned) compressors/decompressors. It immediately allows Hadoop and downstream projects to write unit tests for the same."
HADOOP-9399,protoc maven plugin doesn't work on mvn 3.0.2,"On my machine with mvn 3.0.2, I get a ClassCastException trying to use the maven protoc plugin. The issue seems to be that mvn 3.0.2 sees the List<File> parameter, and doesn't see the generic type argument, and stuffs Strings inside instead. So, we get ClassCastException trying to use the objects as Files."
HADOOP-9397,Incremental dist tar build fails,Building a dist tar build when the dist tarball already exists from a previous build fails.
HADOOP-9388,TestFsShellCopy fails on Windows,"Test fails on below test cases:
{code}
Tests run: 11, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 4.343 sec <<< FAILURE!
testMoveDirFromLocal(org.apache.hadoop.fs.TestFsShellCopy)  Time elapsed: 29 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<1>
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.failNotEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:126)
        at org.junit.Assert.assertEquals(Assert.java:470)
        at org.junit.Assert.assertEquals(Assert.java:454)
        at org.apache.hadoop.fs.TestFsShellCopy.testMoveDirFromLocal(TestFsShellCopy.java:392)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
        at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

testMoveDirFromLocalDestExists(org.apache.hadoop.fs.TestFsShellCopy)  Time elapsed: 25 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<1>
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.failNotEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:126)
        at org.junit.Assert.assertEquals(Assert.java:470)
        at org.junit.Assert.assertEquals(Assert.java:454)
        at org.apache.hadoop.fs.TestFsShellCopy.testMoveDirFromLocalDestExists(TestFsShellCopy.java:410)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
        at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
{code}

"
HADOOP-9387,TestDFVariations fails on Windows after the merge,"Test fails with the following errors:
{code}
Running org.apache.hadoop.fs.TestDFVariations
Tests run: 4, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.186 sec <<< FAILURE!
testOSParsing(org.apache.hadoop.fs.TestDFVariations)  Time elapsed: 109 sec  <<< ERROR!
java.io.IOException: Fewer lines of output than expected
        at org.apache.hadoop.fs.DF.parseOutput(DF.java:203)
        at org.apache.hadoop.fs.DF.getMount(DF.java:150)
        at org.apache.hadoop.fs.TestDFVariations.testOSParsing(TestDFVariations.java:59)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$1.run(FailOnTimeout.java:28)

testGetMountCurrentDirectory(org.apache.hadoop.fs.TestDFVariations)  Time elapsed: 1 sec  <<< ERROR!
java.io.IOException: Fewer lines of output than expected
        at org.apache.hadoop.fs.DF.parseOutput(DF.java:203)
        at org.apache.hadoop.fs.DF.getMount(DF.java:150)
        at org.apache.hadoop.fs.TestDFVariations.testGetMountCurrentDirectory(TestDFVariations.java:139)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$1.run(FailOnTimeout.java:28)
{code}"
HADOOP-9384,Update S3 native fs implementation to use AWS SDK to support authorization through roles,"Currently the S3 native implementation {{org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore}} requires credentials to be set explicitly. Amazon allows setting credentials for instances instead of users, via roles. Such are rotated frequently and kept in a local cache all of which is handled by the AWS SDK in this case the {{AmazonS3Client}}. The SDK follows a specific order to establish whether credentials are set explicitly or via a role:
- Environment Variables: AWS_ACCESS_KEY_ID and AWS_SECRET_KEY
- Java System Properties: aws.accessKeyId and aws.secretKey
- Instance Metadata Service, which provides the credentials associated with the IAM role for the EC2 instance
as seen in http://docs.aws.amazon.com/IAM/latest/UserGuide/role-usecase-ec2app.html

To support this feature the current {{NativeFileSystemStore}} implementation needs to be altered to use the AWS SDK instead of the JetS3t S3 libraries.

A request for this feature has previously been raised as part of the Flume project (FLUME-1691) where the HDFS on top of S3 implementation is used as a manner of logging into S3 via an HDFS Sink.
"
HADOOP-9381,Document dfs cp -f option,"dfs cp should document -f (overwrite) option in the page displayed by -help. Additionally, the HTML documentation page should also document this option and all the options should all be formatted the same."
HADOOP-9380,Add totalLength to rpc response,
HADOOP-9379,capture the ulimit info after printing the log to the console,"Based on the discussions in HADOOP-9253 people prefer if we dont print the ulimit info to the console but still have it in the logs.

Just need to move the head statement to before the capture of ulimit code."
HADOOP-9376,TestProxyUserFromEnv fails on a Windows domain joined machine,"TestProxyUserFromEnv#testProxyUserFromEnvironment fails with the following error on my machine:

org.junit.ComparisonFailure: expected:<[redmond\]ivanmi> but was:<[]ivanmi>
	at org.junit.Assert.assertEquals(Assert.java:123)
	at org.junit.Assert.assertEquals(Assert.java:145)
	at org.apache.hadoop.security.TestProxyUserFromEnv.testProxyUserFromEnvironment(TestProxyUserFromEnv.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)




"
HADOOP-9375,Port HADOOP-7290 to branch-1 to fix TestUserGroupInformation failure,Unit test failure in TestUserGroupInformation.testGetServerSideGroups. port HADOOP-7290 to branch-1.1 
HADOOP-9374,Add tokens from -tokenCacheFile into UGI,"{{GenericOptionsParser}} accepts a {{-tokenCacheFile}} option.  However, it only sets the {{mapreduce.job.credentials.json}} conf value instead of also adding the tokens to the UGI so they are usable by the command being executed."
HADOOP-9373,Merge CHANGES.branch-trunk-win.txt to CHANGES.txt,This is to merge the changes from CHANGES.branch-trunk-win.txt to appropriate CHANGES.txt files.
HADOOP-9372,Fix bad timeout annotations on tests,"Following two tests have bad timeout annotations:

org.apache.hadoop.util.TestWinUtils
org.apache.hadoop.mapreduce.v2.TestMRJobs


"
HADOOP-9371,Define Semantics of FileSystem more rigorously,"The semantics of {{FileSystem}} and {{FileContext}} are not completely defined in terms of 
# core expectations of a filesystem
# consistency requirements.
# concurrency requirements.
# minimum scale limits

Furthermore, methods are not defined strictly enough in terms of their outcomes and failure modes.

The requirements and method semantics should be defined more strictly."
HADOOP-9369,DNS#reverseDns() can return hostname with . appended at the end,"DNS#reverseDns uses javax.naming.InitialDirContext to do a reverse DNS lookup. This can sometimes return hostnames with a . at the end.

Saw this happen on hadoop-1: two nodes with tasktracker.dns.interface set to eth0"
HADOOP-9365,TestHAZKUtil fails on Windows,"TestHAZKUtil#testConfIndirectionfails on the following validation:

assertTrue(fnfe.getMessage().startsWith(BOGUS_FILE));

because the path separators do not match:

Expected: /xxxx-this-does-not-exist
Actual: \xxxx-this-does-not-exist"
HADOOP-9364,PathData#expandAsGlob does not return correct results for absolute paths on Windows,"This causes {{FsShell ls}} not to work properly for absolute paths. For example:

{code}
-fs hdfs://127.0.0.1:58559 -ls -R /dir0
{code}

returns

{code}
drwxr-xr-x   - ivanmi supergroup          0 2013-03-05 11:15 ../../dir0/dir1
{code}"
HADOOP-9361,Strictly define the expected behavior of filesystem APIs and write tests to verify compliance,"{{FileSystem}} and {{FileContract}} aren't tested rigorously enough -while HDFS gets tested downstream, other filesystems, such as blobstore bindings, don't.

The only tests that are common are those of {{FileSystemContractTestBase}}, which HADOOP-9258 shows is incomplete.

I propose 
# writing more tests which clarify expected behavior
# testing operations in the interface being in their own JUnit4 test classes, instead of one big test suite. 
# Having each FS declare via a properties file what behaviors they offer, such as atomic-rename, atomic-delete, umask, immediate-consistency -test methods can downgrade to skipped test cases if a feature is missing."
HADOOP-9358,"""Auth failed"" log should include exception string","Currently, when authentication fails, we see a WARN message like:
{code}
2013-02-28 22:49:03,152 WARN  ipc.Server (Server.java:saslReadAndProcess(1056)) - Auth failed for 1.2.3.4:12345:null
{code}
This is not useful to understand the underlying cause. The WARN entry should additionally include the exception text, eg:
{code}
2013-02-28 22:49:03,152 WARN  ipc.Server (Server.java:saslReadAndProcess(1056)) - Auth failed for 1.2.3.4:12345:null (GSS initiate failed [Caused by GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))])
{code}"
HADOOP-9355,Abstract symlink tests to use either FileContext or FileSystem,We'd like to run the symlink tests using both FileContext and the upcoming FileSystem implementation. The first step here is abstracting the test logic to run on an abstract filesystem implementation.
HADOOP-9353,Activate native-win profile by default on Windows,"Hadoop on Windows requires native components to be available so the native-win profile should be activated by default.

The fix should update BUILDING.txt appropriately."
HADOOP-9352,Expose UGI.setLoginUser for tests,"The {{UGI.setLoginUser}} method is not publicly exposed, which makes it impossible to correctly test code executed outside of an explicit {{doAs}}.  {{getCurrentUser}}/{{getLoginUser}} will always vivify the login user from the user running the test, and not an arbitrary user to be determined by the test.  The method is documented with why it's not ready for prime-time, but it's good enough for tests."
HADOOP-9350,Hadoop not building against Java7 on OSX ,Maven stack-traces out in the {{jspc}} compilation as the JSPC plugin doesn't work against the new JDK7 JAR layout. Needs a symlink set up to fix
HADOOP-9349,Confusing output when running hadoop version from one hadoop installation when HADOOP_HOME points to another,"Hadoop version X is downloaded to ~/hadoop-x, and Hadoop version Y is downloaded to ~/hadoop-y.  HADOOP_HOME is set to hadoop-x.  A user running hadoop-y/bin/hadoop might expect to be running the hadoop-y jars, but, because of HADOOP_HOME, will actually be running hadoop-x jars.

""hadoop version"" could help clear this up a little by reporting the current HADOOP_HOME."
HADOOP-9343,Allow additional exceptions through the RPC layer,"The RPC layer currently only allows IOException, RuntimeException, InterruptedException and their derivatives - which limits exceptions declared by protocols.
Other exceptions end up at the client as an UndeclaredThrowableException wrapped in RemoteException.
Additional exception types should be allowed."
HADOOP-9342,Remove jline from distribution,"Dist bundles ancient jline-0.9.94 as transitive dependency of zookeeper-3.4.2 (it is used in the ZK CLI and not in Hadoop). Please exclude this jline dependency so that other projects that rely on hadoop classpath can use their own version (there is a newer JLine 2.x that contains history search etc.)
"
HADOOP-9339,IPC.Server incorrectly sets UGI auth type,"For non-secure servers, {{IPC.Server#processConnectionContext}} will explicitly set the UGI's auth type to SIMPLE.  However the auth type has already been set by this point, and this explicit set causes proxy UGIs to be SIMPLE/SIMPLE instead of PROXY/SIMPLE."
HADOOP-9338,FsShell Copy Commands Should Optionally Preserve File Attributes,The attached patch adds a -p flag to the copyFromLocal and copyToLocal FsShell commands that behaves (as far as possible) like the unix 'cp' command's -p flag (i.e. preserves file last access and last modification times).
HADOOP-9337,org.apache.hadoop.fs.DF.getMount() does not work on Mac OS,"test org.apache.hadoop.fs.TestLocalFileSystem.testReportChecksumFailure() (added in HADOOP-9067) appears to fail on MacOS because 
method org.apache.hadoop.fs.DF.getMount() does not work correctly.
The problem is that ""df -k <path>"" command returns on MacOS output like the following:
-------
Filesystem   1024-blocks      Used Available Capacity  iused    ifree %iused  Mounted on
/dev/disk0s4   194879828 100327120  94552708    52% 25081778 23638177   51%   /Volumes/Data
-------
while the following is expected:
-------
Filesystem         1024-blocks      Used Available Capacity Mounted on
/dev/mapper/vg_iveselovskyws-lv_home 420545160  15978372 383204308       5% /home
-------
So, we see that Mac's output has 3 additional tokens.

I can suggest 2 ways to fix the problem.
(a) use ""-P"" (POSIX) option when invoking df command. This will probably ensure unifirm output on all Unix systems;
(b) move Mac branch to specific ""case"" branch and treat it specifically (like we currently have for AIX, DF.java, line 214)"
HADOOP-9336,Allow UGI of current connection to be queried,"Querying {{UGI.getCurrentUser}} is synch'ed and inefficient for short-lived RPC requests.  Since the connection already contains the UGI, there should be a means to query it directly and avoid a call to {{UGI.getCurrentUser}}."
HADOOP-9334,Update netty version,"There are newer version available. HBase for example depends on the 3.5.9.
Latest 3.5 is 3.5.11, there is the 3.6.3 as well.

While there is no point in trying to have exactly the same version, things are more comfortable if the gap in version is minimal, as the dependency is client side as well (i.e. HBase has to choose a version anyway).

Attached a patch for the branch 2.

I haven't executed the unit tests, but HBase works ok with Hadoop on Netty 3.5.9.

"
HADOOP-9329,document native build dependencies in BUILDING.txt,"{{BUILDING.txt}} describes {{-Pnative}}, but it does not specify what native libraries are needed for the build.  We should address this."
HADOOP-9323,Typos in API documentation,"Some typos are as follows:
http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/ChecksumFileSystem.html
basice->basic

http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileContext.html
sytem->system

http://hadoop.apache.org/docs/current/api/index.html?org/apache/hadoop/fs/RawLocalFileSystem.html
http://hadoop.apache.org/docs/current/api/index.html?org/apache/hadoop/fs/FilterFileSystem.html
inital->initial

http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/TrashPolicy.html
paramater->parameter

http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/PositionedReadable.html
equalt->equal

http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/BytesWritable.html
http://hadoop.apache.org/docs/current/api/org/apache/hadoop/record/Buffer.html
seqeunce->sequence

http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Text.html
instatiation->instantiation

http://hadoop.apache.org/docs/current/api/org/apache/hadoop/record/RecordOutput.html
alll->all

Please revise the documentation. "
HADOOP-9322,LdapGroupsMapping doesn't seem to set a timeout for its directory search,"We don't appear to be setting a timeout via http://docs.oracle.com/javase/6/docs/api/javax/naming/directory/SearchControls.html#setTimeLimit(int) before we search with http://docs.oracle.com/javase/6/docs/api/javax/naming/directory/DirContext.html#search(javax.naming.Name,%20java.lang.String,%20javax.naming.directory.SearchControls).

This may occasionally lead to some unwanted NN pauses due to lock-holding on the operations that do group lookups. A timeout is better to define than rely on ""0"" (infinite wait)."
HADOOP-9319,Update bundled lz4 source to latest version,"There is a newer version available at https://code.google.com/p/lz4/source/detail?r=89

Among other fixes, r75 fixes compile warnings generated by Visual Studio."
HADOOP-9318,"when exiting on a signal, print the signal name first","On UNIX, it would be nice to know when a Hadoop daemon had exited on a signal.  For example, if a daemon exited because the system administrator sent SIGTERM (i.e. {{killall java}}), it would be nice to know that.  Although some of this can be deduced from context and {{SHUTDOWN_MSG}}, it would be nice to have it be explicit."
HADOOP-9315,Port HADOOP-9249 hadoop-maven-plugins Clover fix to branch-2 to fix build failures,"clone of https://issues.apache.org/jira/browse/HADOOP-9249 for branch-2

Running Maven with the -Pclover option for code coverage causes the build to fail because of not finding a Clover class while running hadoop-maven-plugins version-info."
HADOOP-9307,BufferedFSInputStream.read returns wrong results after certain seeks,"After certain sequences of seek/read, BufferedFSInputStream can silently return data from the wrong part of the file. Further description in first comment below."
HADOOP-9305,Add support for running the Hadoop client on 64-bit AIX,"HADOOP-9283 added support for running the Hadoop client on AIX, but only with 32-bit JREs. This JIRA is to add support for 64-bit JREs as well."
HADOOP-9304,remove addition of avro genreated-sources dirs to build,"The avro maven plugin automatically adds those dirs to the source dirs of the module.

this is just a POM clean up."
HADOOP-9303,command manual dfsadmin missing entry for restoreFailedStorage option,"Generating the latest site docs it doesn't show the -restoreFailedStorage option under the dfsadmin section of commands_manual.html

Also it appears the table header is concatenated with the first row:

COMMAND_OPTION -report"
HADOOP-9302,HDFS docs not linked from top level,"HADOOP-9221 and others converted docs to apt format. After that they aren't linked to the top level menu like: http://hadoop.apache.org/docs/current/

I only see the hadoop commands manual and the Filesystem shell. It used to be you clicked on say the commands manual and you would go to the old style documentation where it had a menu with links to the Superusers, native libraries, etc, but I don't see that any more since converted. "
HADOOP-9301,hadoop client servlet/jsp/jetty/tomcat JARs creating conflicts in Oozie & HttpFS,"Here's how to reproduce:

{noformat}
$ cd hadoop-client
$ mvn dependency:tree | egrep 'jsp|jetty'
[INFO] |  +- org.mortbay.jetty:jetty:jar:6.1.26.cloudera.2:compile
[INFO] |  +- org.mortbay.jetty:jetty-util:jar:6.1.26.cloudera.2:compile
[INFO] |  +- javax.servlet.jsp:jsp-api:jar:2.1:compile
{noformat}

This has a potential for completely screwing up clients like Oozie, etc – hence a blocker.

It seems that while common excludes those JARs, they are sneaking in via hdfs, we need to exclude them too."
HADOOP-9299,kerberos name resolution is kicking in even when kerberos is not configured,"Here's what I'm observing on a fully distributed cluster deployed via Bigtop from the RC0 2.0.3-alpha tarball:

{noformat}
528077-oozie-tucu-W@mr-node] Error starting action [mr-node]. ErrorType [TRANSIENT], ErrorCode [JA009], Message [JA009: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to yarn/localhost@LOCALREALM
        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.<init>(AbstractDelegationTokenIdentifier.java:68)
        at org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier.<init>(MRDelegationTokenIdentifier.java:51)
        at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.getDelegationToken(HistoryClientService.java:336)
        at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.getDelegationToken(MRClientProtocolPBServiceImpl.java:210)
        at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:240)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1735)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1731)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1729)
Caused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to yarn/localhost@LOCALREALM
        at org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:378)
        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.<init>(AbstractDelegationTokenIdentifier.java:66)
        ... 12 more
]
{noformat}

This is submitting a mapreduce job via Oozie 3.3.1. The reason I think this is a Hadoop issue rather than the oozie one is because when I hack /etc/krb5.conf to be:

{noformat}
[libdefaults]
   ticket_lifetime = 600
   default_realm = LOCALHOST
   default_tkt_enctypes = des3-hmac-sha1 des-cbc-crc
   default_tgs_enctypes = des3-hmac-sha1 des-cbc-crc

[realms]
   LOCALHOST = {
       kdc = localhost:88
       default_domain = .local
   }

[domain_realm]
   .local = LOCALHOST

[logging]
   kdc = FILE:/var/log/krb5kdc.log
   admin_server = FILE:/var/log/kadmin.log
   default = FILE:/var/log/krb5lib.log
{noformat}

The issue goes away. 

Now, once again -- the kerberos auth is NOT configured for Hadoop, hence it should NOT pay attention to /etc/krb5.conf to begin with.

"
HADOOP-9297,remove old record IO generation and tests,"Remove their processing from the common POM and delete the following files:

{code}
hadoop-common-project/hadoop-common/src/test/ddl/buffer.jr
hadoop-common-project/hadoop-common/src/test/ddl/int.jr
hadoop-common-project/hadoop-common/src/test/ddl/string.jr
hadoop-common-project/hadoop-common/src/test/ddl/test.jr
hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/record/FromCpp.java
hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/record/RecordBench.java
hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/record/TestBuffer.java
hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/record/TestRecordIO.java
hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/record/TestRecordVersioning.java
hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/record/ToCpp.java
hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/typedbytes/TestIO.java
{code}

All these code is used exclusively within the files being removed. It does not affect any component in a live cluster.
"
HADOOP-9294,GetGroupsTestBase fails on Windows,Test failure is due to not handling the platform-specific line ending.
HADOOP-9291,enhance unit-test coverage of package o.a.h.metrics2,
HADOOP-9290,Some tests cannot load native library,"Some tests are unable to load the native DLL on Windows. The culprit appears to be an incorrect PATH configuration for the native-win profile in hadoop-project/pom.xml.


{code:xml}
    <profile>
      <id>native-win</id>
      <activation>
        <os>
          <family>Windows</family>
        </os>
      </activation>
      <build>
        <plugins>
          <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-surefire-plugin</artifactId>
            <configuration>
              <environmentVariables>
                <!-- Specify where to look for the native DLL on Windows -->
                <PATH>${env.PATH};${basedir}\..\..\hadoop-common-project\hadoop-common\target\bin;</PATH>
              </environmentVariables>
            </configuration>
          </plugin>
        </plugins>
      </build>
    </profile>
{code}

This is evaluated independently by each project and the relative path is not always correct."
HADOOP-9289,FsShell rm -f fails for non-matching globs,"Rm -f isn't supposed to error for paths that don't exist.  It works as expected for exact paths, but fails for non-matching globs."
HADOOP-9287,Parallel testing hadoop-common,"The maven surefire plugin supports parallel testing feature. By using it, the tests can be run more faster.
"
HADOOP-9283,Add support for running the Hadoop client on AIX,"The UserGroupInformation class currently supports running with either Sun Java or IBM Java on Windows or Linux. This JIRA is to add support for using the Hadoop client on AIX. Since there is no Sun Java available for AIX, only IBM Java will be supported on AIX."
HADOOP-9279,Document the need to build hadoop-maven-plugins for eclipse and separate project builds,"In current hadoop-trunk, compile fails when hadoop-maven-plugins-3.0.0-SNAPSHOT.jar doesn't exist in local repository or at maven central repository.

The affected components are:
./hadoop-common-project/hadoop-common/pom.xml
./hadoop-maven-plugins/pom.xml
./hadoop-project/pom.xml
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml

The error log is as follows
{quote}
[ERROR] Plugin org.apache.hadoop.maven.plugin:hadoop-maven-plugins:1.0 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.hadoop.maven.plugin:hadoop-maven-plugins:jar:1.0: Could not find artifact org.apache.hadoop.maven.plugin:hadoop-maven-plugins:pom:1.0 in central (http://repo.maven.apache.org/maven2) -> [Help 1]
{quote}

This can be avoidable if hadoop-maven-plugins is installed before packaging. This is undocumented, so this should get documented."
HADOOP-9278,HarFileSystem may leak file handle,TestHarFileSystemBasics fails on Windows due to invalid HAR URI and file handle leak.  We need to change the tests to use valid HAR URIs and fix the file handle leak.
HADOOP-9276,Allow BoundedByteArrayOutputStream to be resettable,Allow BoundedByteArrayOutputStream's byte[] to be resettable.
HADOOP-9267,"hadoop -help, -h, --help should show usage instructions","It's not friendly for new users when the command line scripts don't show usage instructions when passed the defacto Unix usage flags. Imagine this sequence of commands:

{noformat}
-> % hadoop --help
Error: No command named `--help' was found. Perhaps you meant `hadoop -help'
-> % hadoop -help
Error: No command named `-help' was found. Perhaps you meant `hadoop help'
-> % hadoop help
Exception in thread ""main"" java.lang.NoClassDefFoundError: help
Caused by: java.lang.ClassNotFoundException: help
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: help.  Program will exit.
{noformat}

Same applies for the `hdfs` script."
HADOOP-9264,port change to use Java untar API on Windows from branch-1-win to trunk,"HADOOP-8847 originally introduced this change on branch-1-win.  HADOOP-9081 ported the change to branch-trunk-win.  This should be simple to port to trunk, which would simplify the merge and test activity happening on HADOOP-8562."
HADOOP-9260,Hadoop version may be not correct when starting name node or data node,"1. Check out the trunk from http://svn.apache.org/repos/asf/hadoop/common/trunk/ -r 1439752
2. Compile package
   m2 package -Pdist -Psrc -Pnative -Dtar -DskipTests
3. Hadoop version of compiled dist shows the following:

Hadoop ${pom.version}
Subversion ${version-info.scm.uri} -r ${version-info.scm.commit}
Compiled by ${user.name} on ${version-info.build.time}
From source with checksum ${version-info.source.md5}

And in a real cluster, the log in name node shows:

2013-01-29 15:23:42,738 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = bdpe01.sh.intel.com/10.239.47.101
STARTUP_MSG:   args = []
STARTUP_MSG:   version = ${pom.version}
STARTUP_MSG:   classpath = ...
STARTUP_MSG:   build = ${version-info.scm.uri} -r ${version-info.scm.commit}; compiled by '${user.name}' on ${version-info.build.time}
STARTUP_MSG:   java = 1.6.0_33

While some data nodes with the same binary shows the correct version information.
"
HADOOP-9258,Add stricter tests to FileSystemContractTestBase,"The File System Contract contains implicit assumptions that aren't checked in the contract test base. Add more tests to define the contract's assumptions more rigorously for those filesystems that are tested by this (not Local, BTW)"
HADOOP-9255,relnotes.py missing last jira,"generating the release notes for 0.23.6 via "" python ./dev-support/relnotes.py -v 0.23.6 "" misses the last jira that was committed.  In this case it was YARN-354.


"
HADOOP-9254,"Cover packages org.apache.hadoop.util.bloom, org.apache.hadoop.util.hash",
HADOOP-9253,Capture ulimit info in the logs at service start time,output of ulimit -a is helpful while debugging issues on the system.
HADOOP-9252,StringUtils.humanReadableInt(..) has a race condition,"humanReadableInt(..) incorrectly uses oneDecimal without synchronization.

Also, limitDecimalTo2(double) correctly uses decimalFormat with synchronization.  However, synchronization can be avoided for a better performance."
HADOOP-9248,Allow configuration of Amazon S3 Endpoint,"http://wiki.apache.org/hadoop/AmazonS3 page describes configuration of Hadoop with S3 as storage. Other systems like EMC Atmos now implement S3 Interface, but in order to be able to connect to them, the endpoint needs to be configurable. Please add a configuration parameter that would be propagated  to underlying jets3t library as s3service.s3-endpoint param."
HADOOP-9247,"parametrize Clover ""generateXxx"" properties to make them re-definable via -D in mvn calls","The suggested parametrization is needed in order 
to be able to re-define these properties with ""-Dk=v"" maven options.
For some reason the expressions declared in clover 
docs like ""${maven.clover.generateHtml}"" (see http://docs.atlassian.com/maven-clover2-plugin/3.0.2/clover-mojo.html) do not work in that way. 
However, the parametrized properties are confirmed to work: e.g. 
-DcloverGenHtml=false switches off the Html generation, if defined <generateHtml>${cloverGenHtml}</generateHtml>.

The default values provided here exactly correspond to Clover defaults, so
the behavior is 100% backwards compatible."
HADOOP-9246,Execution phase for hadoop-maven-plugin should be process-resources,"Per discussion on HADOOP-9245, the execution phase of hadoop-maven-plugin should be _process-resources_ and not _compile_."
HADOOP-9245,mvn clean without running mvn install before fails,"HADOOP-8924 introduces plugin dependency on hadoop-maven-plugins in hadoop-common and hadoop-yarn-common.

Calling mvn clean on a fresh m2/repository (missing hadoop-maven-plugins) fails due to this dependency."
HADOOP-9242,Duplicate surefire plugin config in hadoop-common,"Unfortunately in HADOOP-9217 a duplicated configuration of Surefire plugin was introduced in hadoop-common/pom.xml, effectively discarding a part of configuration. "
HADOOP-9241,DU refresh interval is not configurable,"While the {{DF}} class's refresh interval is configurable, the {{DU}}'s isn't. We should ensure both be configurable."
HADOOP-9233,Cover package org.apache.hadoop.io.compress.zlib with unit tests,
HADOOP-9231,Parametrize staging URL for the uniformity of distributionManagement,"The build's {{distributionManagement}} section currently uses parametrization for the snapshot repository. It is convenient and allows to override the value from a developer's custom profile.

The same isn't available for release artifacts to make the parametrization symmetric for both types.  "
HADOOP-9230,TestUniformSizeInputFormat fails intermittently,"TestUniformSizeFileInputFormat fails intermittently. I ran the test 50 times and noticed 5 failures.

Haven't noticed any particular pattern to which runs fail.

A sample stack trace is as follows:

{noformat}
java.lang.AssertionError: expected:<1944> but was:<1820>
        at org.junit.Assert.fail(Assert.java:91)
        at org.junit.Assert.failNotEquals(Assert.java:645)
        at org.junit.Assert.assertEquals(Assert.java:126)
        at org.junit.Assert.assertEquals(Assert.java:470)
        at org.junit.Assert.assertEquals(Assert.java:454)
        at org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat.checkAgainstLegacy(TestUniformSizeInputFormat.java:244)
        at org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat.testGetSplits(TestUniformSizeInputFormat.java:126)
        at org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat.testGetSplits(TestUniformSizeInputFormat.java:252)
{noformat}
"
HADOOP-9228,FileSystemContractTestBase never verifies that files are files,{{FileSystemContractTestBase}} never verifies that a newly created file has a file status where {{isFile()}} returns true
HADOOP-9227,FileSystemContractBaseTest doesn't test filesystem's mkdir/isDirectory() logic rigorously enough,"The {{FileSystemContractBaseTest.mkdirs()}} asserts that a newly created directory is true, but way of {{FileStatus.isFile()}}, but doesn't assert that the directory is a dir by way of {{FileStatus.isDir()}}.

The assertion used is slightly weaker, as the {{isFile()}} test is actually
{{!isdir && !isSymlink()}}. if an implementation of {{FileSystem.mkdirs()}} created symlinks then the test would still pass.

There is one test that looks at the {{isDirectory()}} logic, {{testMkdirsWithUmask()}} -but as that test is skipped for the s3 filesystems, it is possible for those filesystems (or similar) to not have their directory creation logic stressed enough.

The fix would be a trivial single line."
HADOOP-9225,Cover package org.apache.hadoop.compress.Snappy,
HADOOP-9222,Cover package with org.apache.hadoop.io.lz4 unit tests,"Add test class TestLz4CompressorDecompressor with method for Lz4Compressor, Lz4Decompressor testing "
HADOOP-9221,Convert remaining xdocs to APT,"The following Forrest XML documents are still present in trunk:
{noformat}
hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/Superusers.xml
hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/deployment_layout.xml
hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/native_libraries.xml
hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/service_level_auth.xml
hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/single_node_setup.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/SLG_user_guide.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/faultinject_framework.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/hdfs_editsviewer.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/hdfs_imageviewer.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/hdfs_permissions_guide.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/hdfs_quota_admin_guide.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/hdfs_user_guide.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/hftp.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/libhdfs.xml
hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/webhdfs.xml
{noformat}

Several of them are leftover cruft, and all of them are out of date to one degree or another, but it's easiest to simply convert them all to APT and move forward with editing thereafter."
HADOOP-9220,Unnecessary transition to standby in ActiveStandbyElector,"When performing a manual failover from one HA node to a second, under some circumstances the second node will transition from standby -> active -> standby -> active. This is with automatic failover enabled, so there is a ZK cluster doing leader election.
"
HADOOP-9219,coverage fixing for org.apache.hadoop.tools.rumen,"coverage fixing for org.apache.hadoop.tools.rumen 
HADOOP-9219-trunk.patch for trunk, brunch-2 and branch-0.23
"
HADOOP-9218,Document the Rpc-wrappers used internally,
HADOOP-9217,Print thread dumps when hadoop-common tests fail,"Printing thread dumps when tests fail due to timeouts was introduced in HADOOP-8755, but was enabled in M/R, HDFS and Yarn only. 
It makes sense to enable in hadoop-common as well. In particular, TestZKFailoverController seems to be one of the most flaky tests in trunk currently and having thread dumps may help debugging this."
HADOOP-9216,CompressionCodecFactory#getCodecClasses should trim the result of parsing by Configuration.,"CompressionCodecFactory#getCodecClasses doesn't trim its input.
This can confuse users of CompressionCodecFactory. For example, The setting as follows can cause error because of spaces in the values.

{quote}
     conf.set(""io.compression.codecs"", 
        ""  org.apache.hadoop.io.compress.GzipCodec , "" +
        "" org.apache.hadoop.io.compress.DefaultCodec  , "" +
        ""org.apache.hadoop.io.compress.BZip2Codec   "");
{quote}


This ticket deals with this problem."
HADOOP-9215,"when using cmake-2.6, libhadoop.so doesn't get created (only libhadoop.so.1.0.0)","Looks like none of the .so files are being built. They all have .so.1.0.0 but no just .so file.  branch-0.23 works fine but trunk and branch-2 are broke.

This actually applies to libhadoop.so and libhdfs.so"
HADOOP-9212,Potential deadlock in FileSystem.Cache/IPC/UGI,jcarder found a cycle which could lead to a potential deadlock.
HADOOP-9211,"HADOOP_CLIENT_OPTS default setting fixes max heap size at 128m, disregards HADOOP_HEAPSIZE","hadoop-env.sh as included in the 2.0.2alpha release tarball contains:
export HADOOP_CLIENT_OPTS=""-Xmx128m $HADOOP_CLIENT_OPTS""

This overrides any heap settings in HADOOP_HEAPSIZE."
HADOOP-9209,Add shell command to dump file checksums,"Occasionally while working with tools like distcp, or debugging certain issues, it's useful to be able to quickly see the checksum of a file. We currently have the APIs to efficiently calculate a checksum, but we don't expose it to users. This JIRA is to add a ""fs -checksum"" command which dumps the checksum information for the specified file(s)."
HADOOP-9203,RPCCallBenchmark should find a random available port,"RPCCallBenchmark insists on port 12345 by default. It should find a random ephemeral range port instead if one isn't specified.

{noformat}
testBenchmarkWithProto(org.apache.hadoop.ipc.TestRPCCallBenchmark)  Time elapsed: 5092 sec  <<< ERROR!
java.net.BindException: Problem binding to [0.0.0.0:12345] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:710)
	at org.apache.hadoop.ipc.Server.bind(Server.java:361)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:459)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:1877)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:982)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:376)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:351)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:825)
	at org.apache.hadoop.ipc.RPCCallBenchmark.startServer(RPCCallBenchmark.java:230)
	at org.apache.hadoop.ipc.RPCCallBenchmark.run(RPCCallBenchmark.java:264)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
	at org.apache.hadoop.ipc.TestRPCCallBenchmark.testBenchmarkWithProto(TestRPCCallBenchmark.java:43)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$1.run(FailOnTimeout.java:28)
{noformat}
"
HADOOP-9199,Cover package org.apache.hadoop.io with unit tests,
HADOOP-9194,RPC Support for QoS,"One of the next frontiers of Hadoop performance is QoS (Quality of Service). We need QoS support to fight the inevitable ""buffer bloat"" (including various queues, which are probably necessary for throughput) in our software stack. This is important for mixed workload with different latency and throughput requirements (e.g. OLTP vs OLAP, batch and even compaction I/O) against the same DFS.

Any potential bottleneck will need to be managed by QoS mechanisms, starting with RPC. 

How about adding a one byte DS (differentiated services) field (a la the 6-bit DS field in IP header) in the RPC header to facilitate the QoS mechanisms (in separate JIRAs)? The byte at a fixed offset (how about 0?) of the header is helpful for implementing high performance QoS mechanisms in switches (software or hardware) and servers with minimum decoding effort."
HADOOP-9193,hadoop script can inadvertently expand wildcard arguments when delegating to hdfs script,"The hadoop front-end script will print a deprecation warning and defer to the hdfs front-end script for certain commands, like fsck, dfs.  If a wildcard appears as an argument then it can be inadvertently expanded by the shell to match a local filesystem path before being sent to the hdfs script, which can be very confusing to the end user.

For example, the following two commands usually perform very different things, even though they should be equivalent:
{code}
hadoop fs -ls /tmp/\*
hadoop dfs -ls /tmp/\*
{code}
The former lists everything in the default filesystem under /tmp, while the latter expands /tmp/\* into everything in the *local* filesystem under /tmp and passes those as arguments to try to list in the default filesystem.
"
HADOOP-9192,Move token related request/response messages to common,"Get, Renew and Cancel delegation token requests and responses are repeated in HDFS, Yarn and MR. This jira proposes to move these messages into Security.proto in common."
HADOOP-9191,TestAccessControlList and TestJobHistoryConfig fail with JDK7,"Individual test cases have dependencies on a specific order of execution and fail when the order is changed.

TestAccessControlList.testNetGroups relies on Groups being initialized with a hard-coded test class that subsequent test cases depend on.

TestJobHistoryConfig.testJobHistoryLogging fails to shutdown the MiniDFSCluster on exit."
HADOOP-9190,packaging docs is broken,"It looks like after the docs got converted to apt format in HADOOP-8427, mvn site package -Pdist,docs no longer works.   If you run mvn site or mvn site:stage by itself they work fine, its when you go to package it that it breaks.

The error is with broken links, here is one of them:

broken-links>
  <link message=""hadoop-common-project/hadoop-common/target/docs-src/src/documentation/content/xdocs/HttpAuthentication.xml (No such file or directory)"" uri=""HttpAuthentication.html"">
    <referrer uri=""linkmap.html""/>
    <referrer uri=""index.html""/>
    <referrer uri=""single_node_setup.html""/>
    <referrer uri=""native_libraries.html""/>
    <referrer uri=""Superusers.html""/>
    <referrer uri=""service_level_auth.html""/>
    <referrer uri=""deployment_layout.html""/>
  </link>"
HADOOP-9183,Potential deadlock in ActiveStandbyElector,"A jcarder run found a potential deadlock in the locking of ActiveStandbyElector and ActiveStandbyElector.WatcherWithClientRef. No deadlock has been seen in practice, this is just a theoretical possibility at the moment."
HADOOP-9181,Set daemon flag for HttpServer's QueuedThreadPool,"we hit HBASE-6031 again, after looking into thread dump, it was caused by the threads from QueuedThreadPool are user thread, not daemon thread, so the hbase shutdownhook never be called and the hbase instance was hung.

Furthermore, i saw daemon be set in fb-20 branch, let's set in trunk codebase as well, it should be safe:)"
HADOOP-9179,TestFileSystem fails with open JDK7,This is a test order-dependency bug as pointed out in HADOOP-8390. This JIRA is to track the fix in branch-1.
HADOOP-9178,src/main/conf is missing hadoop-policy.xml,"src/main/conf contains hadoop-env.sh and core-site.xml, but is missing hadoop-policy.xml"
HADOOP-9175,TestWritableName fails with Open JDK 7,"TestWritableName.testAddName fails due to a test order execution dependency on testSetName.

java.io.IOException: WritableName can't load class: mystring
at org.apache.hadoop.io.WritableName.getClass(WritableName.java:73)
at org.apache.hadoop.io.TestWritableName.testAddName(TestWritableName.java:92)
Caused by: java.lang.ClassNotFoundException: mystring
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:264)
at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:820)
at org.apache.hadoop.io.WritableName.getClass(WritableName.java:71)"
HADOOP-9174,TestSecurityUtil fails on Open JDK 7,"TestSecurityUtil.TestBuildTokenServiceSockAddr fails due to implicit dependency on the test case execution order.

Testcase: testBuildTokenServiceSockAddr took 0.003 sec
	Caused an ERROR
expected:<[127.0.0.1]:123> but was:<[localhost]:123>
	at org.apache.hadoop.security.TestSecurityUtil.testBuildTokenServiceSockAddr(TestSecurityUtil.java:133)


Similar bug exists in TestSecurityUtil.testBuildDTServiceName.

The root cause is that a helper routine (verifyAddress) used by some test cases has a side effect. It resets a static variable (SecurityUtil.useIpForTokenService). 

The broken test cases assume that the flag will be set to true when they are invoked. The fix is to explicitly initialize the flag to its expected value instead of depending on the execution order."
HADOOP-9173,Add security token protobuf definition to common and use it in hdfs,"The same definition is repeated across HDFS, MR and YARN. We could use the protobuf definitions."
HADOOP-9164,Print paths of loaded native libraries in NativeLibraryChecker,
HADOOP-9163,The rpc msg in  ProtobufRpcEngine.proto should be moved out to avoid an extra copy,
HADOOP-9162,Add utility to check native library availability,"Many times, after deploy hadoop or when trouble shooting, we need to check whether native library(along with native compression libraries) can work properly, and I just want to use one command to check that, like this:

hadoop org.apache.hadoop.util.NativeCodeLoader

and it shows:

Native library loading test:
hadoop: false
zlib:   false
snappy: false
lz4:    false
"
HADOOP-9155,"FsPermission should have different default value, 777 for directory and 666 for file","The default permission for {{FileSystem#create}} is the same default as for {{FileSystem#mkdirs}}, namely {{0777}}. It would make more sense for the default to be {{0666}} for files and {{0777}} for directories.  The current default leads to a lot of files being created with the executable bit that really should not be.  One example is anything created with FsShell's copyToLocal.

For reference, {{fopen}} creates files with a mode of {{0666}} (minus whatever bits are set in the umask; usually {{0022}}.  This seems to be the standard behavior and we should follow it.  This is also a regression since branch-1."
HADOOP-9154,SortedMapWritable#putAll() doesn't add key/value classes to the map,"In the following code from {{SortedMapWritable}}, #putAll() doesn't add key/value classes to the class-id maps.

{code}

  @Override
  public Writable put(WritableComparable key, Writable value) {
    addToMap(key.getClass());
    addToMap(value.getClass());
    return instance.put(key, value);
  }

  @Override
  public void putAll(Map<? extends WritableComparable, ? extends Writable> t){
    for (Map.Entry<? extends WritableComparable, ? extends Writable> e:
      t.entrySet()) {
      
      instance.put(e.getKey(), e.getValue());
    }
  }
{code}"
HADOOP-9153,Support createNonRecursive in ViewFileSystem,"Implement createNonRecursive in ViewFileSystem.  Currently an ""Unsupported..."" exception is thrown when it's called."
HADOOP-9152,HDFS can report negative DFS Used on clusters with very small amounts of data,"I had a near empty HDFS instance where I was creating a file and deleting it very quickly. I noticed that HDFS sometimes reported a negative DFS used.

{noformat}
root@brock0-1 ~]# sudo -u hdfs -i hdfs dfsadmin -report
Configured Capacity: 97233235968 (90.56 GB)
Present Capacity: 84289609707 (78.5 GB)
DFS Remaining: 84426645504 (78.63 GB)
DFS Used: -137035797 (-133824.02 KB)
DFS Used%: -0.16%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 1 (1 total, 0 dead)

Live datanodes:
Name: 127.0.0.1:50010 (localhost)
Hostname: brock0-1.ent.cloudera.com
Decommission Status : Normal
Configured Capacity: 97233235968 (90.56 GB)
DFS Used: -137035797 (-133824.02 KB)
Non DFS Used: 12943626261 (12.05 GB)
DFS Remaining: 84426645504 (78.63 GB)
DFS Used%: -0.14%
DFS Remaining%: 86.83%
Last contact: Thu Nov 22 18:25:37 PST 2012




[root@brock0-1 ~]# sudo -u hdfs -i hdfs dfsadmin -report
Configured Capacity: 97233235968 (90.56 GB)
Present Capacity: 84426973184 (78.63 GB)
DFS Remaining: 84426629120 (78.63 GB)
DFS Used: 344064 (336 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 1 (1 total, 0 dead)

Live datanodes:
Name: 127.0.0.1:50010 (localhost)
Hostname: brock0-1.ent.cloudera.com
Decommission Status : Normal
Configured Capacity: 97233235968 (90.56 GB)
DFS Used: 344064 (336 KB)
Non DFS Used: 12806262784 (11.93 GB)
DFS Remaining: 84426629120 (78.63 GB)
DFS Used%: 0%
DFS Remaining%: 86.83%
Last contact: Thu Nov 22 18:28:47 PST 2012
{noformat}"
HADOOP-9151,Include RPC error info in RpcResponseHeader instead of sending it separately,
HADOOP-9150,Unnecessary DNS resolution attempts for logical URIs,"In the FileSystem code, we accidentally try to DNS-resolve the logical name before it is converted to an actual domain name. In some DNS setups, this can cause a big slowdown - eg in one misconfigured cluster we saw a 2-3x drop in terasort throughput, since every task wasted a lot of time waiting for slow ""not found"" responses from DNS."
HADOOP-9147,Add missing fields to FIleStatus.toString,"The FileStatus.toString method is missing the following fields:
- modification_time
- access_time
- symlink

These should be added in to aid debugging."
HADOOP-9140,Cleanup rpc PB protos,
HADOOP-9137,Support connection limiting in IPC server,
HADOOP-9135,JniBasedUnixGroupsMappingWithFallback should log at debug rather than info during fallback,"{{JniBasedUnixGroupsMappingWithFallback}} should log at debug rather than info during fallback.  Otherwise, every time an HDFS client does something (like DFSShell), the message about fallback is printed out."
HADOOP-9131,TestLocalFileSystem#testListStatusWithColons cannot run on Windows,"HADOOP-8962 added a new test, {{TestLocalFileSystem#testListStatusWithColons}}, covering the case of files that contain ':'.  This test cannot pass on Windows, because on Windows, the local file system does not support ':' in file names."
HADOOP-9127,Update documentation for ZooKeeper Failover Controller,Documentation needs to be updated as per the configurations of HA health monitor and failover controller.
HADOOP-9125,LdapGroupsMapping threw CommunicationException after some idle time,"LdapGroupsMapping threw exception as below after some idle time. During the idle time no call to the group mapping provider should be made to repeat it.

2012-12-07 02:20:59,738 WARN org.apache.hadoop.security.LdapGroupsMapping: Exception trying to get groups for user aduser2
javax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'
        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)
        at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)
        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)
        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)
        at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)
        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)
        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)
        at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)
        at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)
        at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)
        at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)
        at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)
        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)
        at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)
        at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)
        at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)
        at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)
Caused by: java.io.IOException: connection closed
        at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)
        at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)
        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)
        ... 28 more
2012-12-07 02:20:59,739 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user aduser2
"
HADOOP-9124,SortedMapWritable violates contract of Map interface for equals() and hashCode(),"This issue is similar to HADOOP-7153. It was found when using MRUnit - see MRUNIT-158, specifically https://issues.apache.org/jira/browse/MRUNIT-158?focusedCommentId=13501985&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13501985

--
o.a.h.io.SortedMapWritable implements the java.util.Map interface, however it does not define an implementation of the equals() or hashCode() methods; instead the default implementations in java.lang.Object are used.

This violates the contract of the Map interface which defines different behaviour for equals() and hashCode() than Object does. More information here: http://download.oracle.com/javase/6/docs/api/java/util/Map.html#equals(java.lang.Object)

The practical consequence is that SortedMapWritables containing equal entries cannot be compared properly. We were bitten by this when trying to write an MRUnit test for a Mapper that outputs MapWritables; the MRUnit driver cannot test the equality of the expected and actual MapWritable objects."
HADOOP-9121,InodeTree.java has redundant check for vName while throwing exception,"{code}
337	    if (!gotMountTableEntry) {
338	      throw new IOException(
339	          ""ViewFs: Cannot initialize: Empty Mount table in config for "" + 
340	             vName == null ? ""viewfs:///"" : (""viewfs://"" + vName + ""/""));
341	    }
{code}

The vName is always non-null due to checks/assignments done prior to this code segment."
HADOOP-9119,Add test to FileSystemContractBaseTest to verify integrity of overwritten files,The test {{FileSystemContractBaseTest.testOverwrite()}} is meant to verify that overwrites work -but it only overwrites a zero byte file with some data and only checks for length. This can hide problems where there overwrite was somehow corrupted.
HADOOP-9118,FileSystemContractBaseTest test data for read/write isn't rigorous enough,Every byte in the {{FileSystemContractBaseTest}} test data array is always in the range 0-9. A filesystem that dropped the highest four bits of every byte would still pass the read and write tests -the test data should include every bit in a byte being true and false.
HADOOP-9117,replace protoc ant plugin exec with a maven plugin,The protoc compiler is currently invoked using ant plugin exec. There is a bug in the ant plugin exec task which does not consume the STDOUT or STDERR appropriately making the build to stop sometimes (you need to press enter to continue).
HADOOP-9115,Deadlock in configuration when writing configuration to hdfs,"This was noticed when using hive with hadoop-1.1.1 and running 

{code}
select count(*) from tbl;
{code}

This would cause a deadlock configuration. "
HADOOP-9114,"After defined the dfs.checksum.type as the NULL, write file and hflush will through java.lang.ArrayIndexOutOfBoundsException","when I test the characteristic parameter about dfs.checksum.type. The value can be defined as NULL,CRC32C,CRC32. It's ok when the value is CRC32C or CRC32, but the client will through java.lang.ArrayIndexOutOfBoundsException when the value is configured NULL."
HADOOP-9113,o.a.h.fs.TestDelegationTokenRenewer is failing intermittently,"In the following code snippets, the test tries to check the renewCount for the token to verify if the FileSystem has been de-queued.

{code}
    @Override
    public long renew(Configuration conf) {
      if (renewCount == MAX_RENEWALS) {
        Thread.currentThread().interrupt();
      } else {
        renewCount++;
      }
      return renewCount;
    }

testAddRemoveRenewAction() {
  // some test code
  assertTrue(""Token not removed"", (tfs.testToken.renewCount < MAX_RENEWALS));
}
{code}

On slower machines, the renewCount can actually reach MAX_RENEWALS resulting in a test failure.

renewCount should not be used to verify this."
HADOOP-9111,Fix failed testcases with @ignore annotation In branch-1,"Currently in branch-1, several failed testcases have @ignore annotation which does not take effect because these testcases are still using JUnit3. This jira plans to change these testcases to JUnit4 to let @ignore work."
HADOOP-9106,Allow configuration of IPC connect timeout,"Currently the connection timeout in Client.setupConnection() is hard coded to 20seconds. This is unreasonable in some scenarios, such as HA failover, if we want a faster failover time. We should allow this to be configured per-client."
HADOOP-9105,FsShell -moveFromLocal erroneously fails,"The move successfully completes, but then reports error upon trying to delete the local source directory even though it succeeded."
HADOOP-9103,UTF8 class does not properly decode Unicode characters outside the basic multilingual plane,"this the log information  of the  exception  from the SecondaryNameNode: 
2012-03-28 00:48:42,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for
 non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????
??????????tor.qzone.qq.com/keypart-00174
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)
        at java.lang.Thread.run(Thread.java:619)

this is the log information  about the file from namenode:
2012-03-28 00:32:26,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss	ip=/10.131.16.34	cmd=create	src=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174	dst=null	perm=boss:boss:rw-r--r--
2012-03-28 00:37:42,387 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174. blk_2751836614265659170_184668759
2012-03-28 00:37:42,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174 is closed by DFSClient_attempt_201203271849_0016_r_000174_0
2012-03-28 00:37:50,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss	ip=/10.131.16.34	cmd=rename	src=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174	dst=/user/boss/pgv/fission/task16/split/  @?            tor.qzone.qq.com/keypart-00174	perm=boss:boss:rw-r--r--


after check the code that save FSImage,I found there are a problem that maybe a bug of HDFS Code,I past below:
-------------this is the saveFSImage method  in  FSImage.java, I make some mark at the problem code------------

/**
   * Save the contents of the FS image to the file.
   */
  void saveFSImage(File newFile) throws IOException {
    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();
    FSDirectory fsDir = fsNamesys.dir;
    long startTime = FSNamesystem.now();
    //
    // Write out data
    //
    DataOutputStream out = new DataOutputStream(
                                                new BufferedOutputStream(
                                                                         new FileOutputStream(newFile)));
    try {
      .........
    
      // save the rest of the nodes
      saveImage(strbuf, 0, fsDir.rootDir, out);------------------problem
      fsNamesys.saveFilesUnderConstruction(out);------------------problem  detail is below
      strbuf = null;
    } finally {
      out.close();
    }

    LOG.info(""Image file of size "" + newFile.length() + "" saved in "" 
        + (FSNamesystem.now() - startTime)/1000 + "" seconds."");
  }

 /**
   * Save file tree image starting from the given root.
   * This is a recursive procedure, which first saves all children of
   * a current directory and then moves inside the sub-directories.
   */
  private static void saveImage(ByteBuffer parentPrefix,
                                int prefixLength,
                                INodeDirectory current,
                                DataOutputStream out) throws IOException {
    int newPrefixLength = prefixLength;
    if (current.getChildrenRaw() == null)
      return;
    for(INode child : current.getChildren()) {
      // print all children first
      parentPrefix.position(prefixLength);
      parentPrefix.put(PATH_SEPARATOR).put(child.getLocalNameBytes());------------------problem
      saveINode2Image(parentPrefix, child, out);
    }
   ..........
  }


 // Helper function that writes an INodeUnderConstruction
  // into the input stream
  //
  static void writeINodeUnderConstruction(DataOutputStream out,
                                           INodeFileUnderConstruction cons,
                                           String path) 
                                           throws IOException {
    writeString(path, out);------------------problem
    ..........
  }
  
  static private final UTF8 U_STR = new UTF8();
  static void writeString(String str, DataOutputStream out) throws IOException {
    U_STR.set(str);
    U_STR.write(out);------------------problem 
  }

  /**
   * Converts a string to a byte array using UTF8 encoding.
   */
  static byte[] string2Bytes(String str) {
    try {
      return str.getBytes(""UTF8"");------------------problem 
    } catch(UnsupportedEncodingException e) {
      assert false : ""UTF8 encoding is not supported "";
    }
    return null;
  }
------------------------------------------below is the explain------------------------
in  saveImage method:  child.getLocalNameBytes(),the  bytes use the method of str.getBytes(""UTF8"");

but in writeINodeUnderConstruction, the bytes user the method of Class  UTF8 to get the bytes.

I make a test use our messy code file name , found the the two bytes arrsy are not equal. so I both use the class UTF8,then the problem desappare.

I think this is a bug of HDFS or UTF8."
HADOOP-9099,NetUtils.normalizeHostName fails on domains where UnknownHost resolves to an IP address,"I just hit this failure. We should use some more unique string for ""UnknownHost"":

Testcase: testNormalizeHostName took 0.007 sec
	FAILED
expected:<[65.53.5.181]> but was:<[UnknownHost]>
junit.framework.AssertionFailedError: expected:<[65.53.5.181]> but was:<[UnknownHost]>
	at org.apache.hadoop.net.TestNetUtils.testNormalizeHostName(TestNetUtils.java:347)

Will post a patch in a bit."
HADOOP-9098,Add missing license headers,There are missing license headers in some source files (e.g. TestUnderReplicatedBlocks.java is one) according to the RAT report.
HADOOP-9097,Maven RAT plugin is not checking all source files,"Running 'mvn apache-rat:check' passes, but running RAT by hand (by downloading the JAR) produces some warnings for Java files, amongst others."
HADOOP-9095,TestNNThroughputBenchmark fails in branch-1,"{noformat}
java.lang.StringIndexOutOfBoundsException: String index out of range: 0
    at java.lang.String.charAt(String.java:686)
    at org.apache.hadoop.net.NetUtils.normalizeHostName(NetUtils.java:539)
    at org.apache.hadoop.net.NetUtils.normalizeHostNames(NetUtils.java:562)
    at org.apache.hadoop.net.CachedDNSToSwitchMapping.resolve(CachedDNSToSwitchMapping.java:88)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1047)
    ...
    at org.apache.hadoop.hdfs.server.namenode.NNThroughputBenchmark$StatsDaemon.run(NNThroughputBenchmark.java:377)
{noformat}"
HADOOP-9093,Move all the Exception in PathExceptions to o.a.h.fs package,The exceptions in PathExceptions are useful for non shell related functionality as well. Making this available as exceptions under fs will help move some of the HDFS implementation code throw more specific exception than throwing IOException (for example see HDFS-4209).
HADOOP-9090,Support on-demand publish of metrics,"Updated description based on feedback:

We have a need to publish metrics out of some short-living processes, which is not really well-suited to the current metrics system implementation which periodically publishes metrics asynchronously (a behavior that works great for long-living processes). Of course I could write my own metrics system, but it seems like such a waste to rewrite all the awesome code currently in the MetricsSystemImpl and supporting classes.
The way this JIRA solves this problem is adding a new method publishMetricsNow() to the MetricsSystemImpl() class, that does a synchronous out-of-band push of the metrics from the sources to the sink. I also add a method to MetricsSinkAdapter (putMetricsImmediate) to support that change."
HADOOP-9087,Queue size metric for metric sinks isn't actually maintained,Came across this while looking at the code for the metrics system: the qsize gauge in the MetricsSinkAdapter is not updated/maintained so it will always be zero (there's a compiler warning about it actually).
HADOOP-9078,enhance unit-test coverage of class org.apache.hadoop.fs.FileContext,
HADOOP-9072,Hadoop-Common-0.23-Build Fails to build in Jenkins,"[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:testCompile (default-testCompile) on project hadoop-common: Compilation failure: Compilation failure:
[ERROR] /home/jenkins/jenkins-slave/workspace/Hadoop-Common-0.23-Build/trunk/hadoop-common-project/hadoop-common/target/clover/src-test-instrumented/org/apache/hadoop/record/RecInt.java:[4,7] duplicate class: org.apache.hadoop.record.RecInt
[ERROR] /home/jenkins/jenkins-slave/workspace/Hadoop-Common-0.23-Build/trunk/hadoop-common-project/hadoop-common/target/generated-test-sources/java/org/apache/hadoop/record/RecRecordOld.java:[4,7] duplicate class: org.apache.hadoop.record.RecRecordOld
[ERROR] /home/jenkins/jenkins-slave/workspace/Hadoop-Common-0.23-Build/trunk/hadoop-common-project/hadoop-common/target/clover/src-test-instrumented/org/apache/hadoop/record/RecString.java:[4,7] duplicate class: org.apache.hadoop.record.RecString
[ERROR] /home/jenkins/jenkins-slave/workspace/Hadoop-Common-0.23-Build/trunk/hadoop-common-project/hadoop-common/target/generated-test-sources/java/org/apache/hadoop/record/RecBuffer.java:[4,7] duplicate class: org.apache.hadoop.record.RecBuffer
[ERROR] /home/jenkins/jenkins-slave/workspace/Hadoop-Common-0.23-Build/trunk/hadoop-common-project/hadoop-common/target/clover/src-test-instrumented/org/apache/hadoop/record/RecRecord1.java:[4,7] duplicate class: org.apache.hadoop.record.RecRecord1
[ERROR] /home/jenkins/jenkins-slave/workspace/Hadoop-Common-0.23-Build/trunk/hadoop-common-project/hadoop-common/target/clover/src-test-instrumented/org/apache/hadoop/record/RecRecordNew.java:[4,7] duplicate class: org.apache.hadoop.record.RecRecordNew
[ERROR] /home/jenkins/jenkins-slave/workspace/Hadoop-Common-0.23-Build/trunk/hadoop-common-project/hadoop-common/target/clover/src-test-instrumented/org/apache/hadoop/record/RecRecord0.java:[4,7] duplicate class: org.apache.hadoop.record.RecRecord0

"
HADOOP-9071,configure ivy log levels for resolve/retrieve,
HADOOP-9070,Kerberos SASL server cannot find kerberos key,HADOOP-9015 inadvertently removed a {{doAs}} block around instantiation of the sasl server which renders a server incapable of accepting kerberized connections.
HADOOP-9067,"provide test for method org.apache.hadoop.fs.LocalFileSystem.reportChecksumFailure(Path, FSDataInputStream, long, FSDataInputStream, long)",this method is not covered by the existing unit tests. Provide a test to cover it.
HADOOP-9064,Augment DelegationTokenRenewer API to cancel the tokens on calls to removeRenewAction,"Post HADOOP-9049, FileSystems register with DelegationTokenRenewer (a singleton), to renew tokens. 

To avoid a bunch of defunct tokens clog the NN, we should augment the API to {{#removeRenewAction(boolean cancel)}} and cancel the token appropriately."
HADOOP-9063,enhance unit-test coverage of class org.apache.hadoop.fs.FileUtil,Some methods of class org.apache.hadoop.fs.FileUtil are covered by unit-tests poorly or not covered at all. Enhance the coverage.
HADOOP-9054,Add AuthenticationHandler that uses Kerberos but allows for an alternate form of authentication for browsers,"It would be useful for some Oozie users if, when using Kerberos, that browser access to the oozie web UI could be authenticated in a different way (w/o Kerberos).  This may be useful for other projects using Hadoop-Auth, so this feature is to add a new AuthenticationHandler that uses Kerberos by default, unless a browser (user-agents are configurable) is used, in which case some other form of authentication can be used.  "
HADOOP-9051,“ant test” will build failed for  trying to delete a file,"Run ""ant test"" on branch-1 of hadoop-common.
When the test process reach ""test-core-excluding-commit-and-smoke""

It will invoke the ""macro-test-runner"" to clear and rebuild the test environment.
Then the ant task command  <delete dir=""@{test.dir}/logs"" />
failed for trying to delete an non-existent file.

following is the test result logs:
test-core-excluding-commit-and-smoke:
   [delete] Deleting: /home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build/test/testsfailed
   [delete] Deleting directory /home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build/test/data
    [mkdir] Created dir: /home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build/test/data
   [delete] Deleting directory /home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build/test/logs

BUILD FAILED
/home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build.xml:1212: The following error occurred while executing this line:
/home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build.xml:1166: The following error occurred while executing this line:
/home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build.xml:1057: Unable to delete file /home/jdu/bdc/hadoop-topology-branch1-new/hadoop-common/build/test/logs/userlogs/job_20121112223129603_0001/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/attempt_20121112223129603_0001_r_000000_0/stdout
"
HADOOP-9049,DelegationTokenRenewer needs to be Singleton and FileSystems should register/deregister to/from.,"Currently, DelegationTokenRenewer is not singleton.

Each filesystem using it spawns its own DelegationTokenRenewer. Also, they don't stop the Renewer leading to other problems.

A single DelegationTokenRenewer should be sufficient for all FileSystems."
HADOOP-9043,disallow in winutils creating symlinks with forwards slashes,"In general, the winutils symlink command rejects attempts to create symlinks targeting a destination file that does not exist.  However, if given a symlink destination with forward slashes pointing at a file that does exist, then it creates the symlink with the forward slashes, and then attempts to open the file through the symlink will fail."
HADOOP-9042,Add a test for umask in FileSystemContractBaseTest,Add a unit test to make sure {{umask}} is working correctly in FileSystemContractBaseTest.
HADOOP-9041,FileSystem initialization can go into infinite loop,"More information is there: https://jira.springsource.org/browse/SHDP-111

Referenced source code from example is: https://github.com/SpringSource/spring-hadoop/blob/master/src/main/java/org/springframework/data/hadoop/configuration/ConfigurationFactoryBean.java

from isolating that cause it looks like if you register: org.apache.hadoop.fs.FsUrlStreamHandlerFactory before calling FileSystem.loadFileSystems() then it goes into infinite loop."
HADOOP-9038,provide unit-test coverage of class org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.PathIterator,The class org.apache.hadoop.fs.LocalDirAllocator.AllocatorPerContext.PathIterator currently has zero unit-test coverage. Add/enhance the tests to provide one.
HADOOP-9036,TestSinkQueue.testConcurrentConsumers fails intermittently (Backports HADOOP-7292),"org.apache.hadoop.metrics2.impl.TestSinkQueue.testConcurrentConsumers
 

Error Message

should've thrown
Stacktrace

junit.framework.AssertionFailedError: should've thrown
	at org.apache.hadoop.metrics2.impl.TestSinkQueue.shouldThrowCME(TestSinkQueue.java:229)
	at org.apache.hadoop.metrics2.impl.TestSinkQueue.testConcurrentConsumers(TestSinkQueue.java:195)
Standard Output

2012-10-03 16:51:31,694 INFO  impl.TestSinkQueue (TestSinkQueue.java:consume(243)) - sleeping
"
HADOOP-9035,Generalize setup of LoginContext,The creation of the {{LoginContext}} in {{UserGroupInformation}} has specific cases for specific authentication types.  This is inflexible.
HADOOP-9025,org.apache.hadoop.tools.TestCopyListing failing,"https://builds.apache.org/job/PreCommit-HADOOP-Build/1732//testReport/org.apache.hadoop.tools/TestCopyListing/testDuplicates/

"
HADOOP-9022,Hadoop distcp tool fails to copy file if -m 0 specified,"When trying to copy file using distcp on H23, if -m 0 is specified, distcp will just spawn 0 mapper tasks and the file will not be copied.
But this used to work before H23, even when -m 0 specified, distcp will always copy the files.
Checked the code of DistCp.java
Before the rewrite, it set the number maps at least to 1
job.setNumMapTasks(Math.max(numMaps, 1));

But in the newest code, it just takes the input from user:
job.getConfiguration().set(JobContext.NUM_MAPS,
                  String.valueOf(inputOptions.getMaxMaps()));"
HADOOP-9021,Enforce configured SASL method on the server,The RPC needs to restrict itself to only using the configured SASL method.
HADOOP-9020,Add a SASL PLAIN server,Java includes a SASL PLAIN client but not a server.
HADOOP-9017,fix hadoop-client-pom-template.xml and hadoop-client-pom-template.xml for version ,"hadoop-client-pom-template.xml and hadoop-client-pom-template.xml references to project.version variable, instead they should refer to @version token."
HADOOP-9016,org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.skip(long) must never return negative value.,"The patch fixes bug in method org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.skip(long):
// the contract is described in java.io.InputStream.skip(long):
// this method returns the number of bytes actually skipped, so,
// the return value should never be negative. 

The patch adds tests for the fixed functionality + other tests to cover other uncovered methods of classes org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream,
org.apache.hadoop.fs.HarFileSystem.HarFSDataInputStream.HarFsInputStream.
"
HADOOP-9015,Standardize creation of SaslRpcServers,"To ease adding additional SASL support, need to merge the multiple switches for mechanism type and server creation into a single switch with a single call to createSaslServer."
HADOOP-9014,Standardize creation of SaslRpcClients,"To ease adding additional SASL support, need to change the chained conditionals into a switch and make one standard call to createSaslClient."
HADOOP-9013,UGI should not hardcode loginUser's authenticationType,"{{UGI.loginUser}} assumes that the user's auth type for security on = kerberos, security off = simple.  It should instead use the configured auth type."
HADOOP-9012,IPC Client sends wrong connection context,The IPC client will send the wrong connection context when asked to switch to simple auth.
HADOOP-9010,Map UGI authenticationMethod to RPC authMethod,The UGI's authenticationMethod needs a forward mapping to the RPC/SASL authMethod.  This will allow for the RPC client to eventually use the UGI's authenticationMethod to derive the authMethod instead of assuming security on is kerberos and security off is simple.
HADOOP-9009,Add SecurityUtil methods to get/set authentication method,The authentication method is handled as a string when an enum is available.  Adding methods to get/set the conf value based on the enum will simplify adding new SASL auths such as PLAIN.
HADOOP-9004,Allow security unit tests to use external KDC,"I want to add the option of allowing security-related unit tests to use an external KDC.

In HADOOP-8078, we add the ability to start and use an ApacheDS KDC for security-related unit tests. It would be good to allow users to validate the use of their own KDC, keytabs, and principals and to test different KDCs and not rely on the ApacheDS KDC."
HADOOP-9003,StringIntern not handling null correctly,Tracking the change to StringIntern that went in with MAPREDUCE-4752 to fix the handling of null in StringIntern.
HADOOP-8999,SASL negotiation is flawed,"The RPC protocol used for SASL negotiation is flawed.  The server's RPC response contains the next SASL challenge token, but a SASL server can return null (I'm done) or a N-many byte challenge.  The server currently will not send a RPC success response to the client if the SASL server returns null, which causes the client to hang until it times out."
HADOOP-8998,set Cache-Control no-cache header on all dynamic content,"We should send a Cache-Control header on JSP pages so that HTTP/1.1 compliant caches can properly manage cached data.

Currently our JSPs send:
{noformat}
% curl -v http://nn1:50070/dfshealth.jsp
...
< HTTP/1.1 200 OK
< Content-Type: text/html; charset=utf-8
< Expires: Thu, 01-Jan-1970 00:00:00 GMT
< Set-Cookie: JSESSIONID=xtblchjm7o7j1y1f33r0mpmqp;Path=/
< Content-Length: 3651
< Server: Jetty(6.1.26)
{noformat}

Based on a quick reading of RFC 2616 http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html I think we want to send {{Cache-Control: private, no-cache}} but I could be wrong.  The Jetty docs http://docs.codehaus.org/display/JETTY/LastModifiedCacheControl indicate this is fairly straightforward."
HADOOP-8995,Remove unnecessary bogus exception log from Configuration,"In Configuration#Configuration(boolean) and Configuration#Configuration(Configuration), bogus exceptions are thrown when Log level is DEBUG."
HADOOP-8994,"TestDFSShell creates file named ""noFileHere"", making further tests hard to understand",While working on HDFS-1331 I added a test to {{TestDFSShell}} which used {{noFileHere}} in the negative case.  This failed mysteriously because the earlier tests run {{-touchz noFileHere}} for no good reason.
HADOOP-8992,Enhance unit-test coverage of class HarFileSystem,"New unit test TestHarFileSystem2 provided in order to enhance coverage of class HarFileSystem.
Also some unused methods deleted from class HarFileSystem."
HADOOP-8989,hadoop fs -find feature,"Both sysadmins and users make frequent use of the unix 'find' command, but Hadoop has no correlate. Without this, users are writing scripts which make heavy use of hadoop dfs -lsr, and implementing find one-offs. I think hdfs -lsr is somewhat taxing on the NameNode, and a really slow experience on the client side. Possibly an in-NameNode find operation would be only a bit more taxing on the NameNode, but significantly faster from the client's point of view?

The minimum set of options I can think of which would make a Hadoop find command generally useful is (in priority order):
* -type (file or directory, for now)
* -atime/-ctime-mtime (... and -creationtime?) (both + and - arguments)
* -print0 (for piping to xargs -0)
* -depth
* -owner/-group (and -nouser/-nogroup)
* -name (allowing for shell pattern, or even regex?)
* -perm
* -size

One possible special case, but could possibly be really cool if it ran from within the NameNode:
* -delete
The ""hadoop dfs -lsr | hadoop dfs -rm"" cycle is really, really slow.

Lower priority, some people do use operators, mostly to execute -or searches such as:
* find / \(-nouser -or -nogroup\)

Finally, I thought I'd include a link to the [Posix spec for find|http://www.opengroup.org/onlinepubs/009695399/utilities/find.html]
"
HADOOP-8988,Backport HADOOP-8343 to branch-1,"Backport HADOOP-8343 to branch-1 so as to specifically control the authorization requirements for accessing /jmx, /metrics, and /conf in branch-1."
HADOOP-8986,Server$Call object is never released after it is sent,When an IPC response cannot be returned without blocking an Server$Call object is attached to the SelectionKey of the write selector.  However the call object is never removed from the SlectionKey. So for a connection that rarely has large responses but is long lived there is a lot of data.
HADOOP-8985,Add namespace declarations in .proto files for languages other than java,"Currently .proto files use java_package to specify java packages in proto files, but namespace are not specified for other languages such as cpp, this causes name collision in cpp. we can add namespace declarations to avoid this. 

In Java, the package specifier is used as the Java package, unless you explicitly provide a option java_package in your .proto file. So the original java package will not be affected.

About namespace name, how about ""hadoop.common""(hadoop::common in cpp) for all common sub-project proto files, and ""hadoop.hdfs""(hadoop::hdfs in cpp) for all hdfs sub-project proto files?
"
HADOOP-8982,TestSocketIOWithTimeout fails on Windows,This is a possible race condition or difference in socket handling on Windows.
HADOOP-8981,TestMetricsSystemImpl fails on Windows,The test is failing on an expected mock interaction.
HADOOP-8973,DiskChecker cannot reliably detect an inaccessible disk on Windows with NTFS ACLs,"DiskChecker.checkDir uses File.canRead, File.canWrite, and File.canExecute to check if a directory is inaccessible.  These APIs are not reliable on Windows with NTFS ACLs due to a known JVM bug."
HADOOP-8971,Backport: hadoop.util.PureJavaCrc32 cache hit-ratio is low for static data (HADOOP-8926),"Backport (HADOOP-8926) cache-aware improvements made to PureJavaCrc32 to branch-1.

With patch, observed significant improvement in crc throughput at larger chunk sizes on branch-1

java.version = 1.6.0_37
java.runtime.name = Java(TM) SE Runtime Environment
java.runtime.version = 1.6.0_37-b06
java.vm.version = 20.12-b01
java.vm.vendor = Sun Microsystems Inc.
java.vm.name = Java HotSpot(TM) 64-Bit Server VM
java.vm.specification.version = 1.0
java.specification.version = 1.6
os.arch = amd64
os.name = Linux
os.version = 3.5.0-17-generic

Performance Table (The unit is MB/sec)
|| Num Bytes ||CRC32 || PureJavaCrc32 || PureJavaCrc32+patch ||
|1 |16.548 |163.916 |122.289 |
|2 |30.046 |213.591 |246.657 |
|4 |59.853 |301.521 |322.862 |
|8 | 107.435 |491.757 |512.487 |
| 16 | 179.278 |509.099 |709.656 |
| 32 | 265.496 |651.438 |908.673 |
| 64 | 350.994 |766.566 | 1000.775 |
|128 | 410.514 |795.719 | 1073.625 |
|256 | 434.938 |828.010 | 1188.074 |
|512 | 451.997 |883.219 | 1239.929 |
| 1024 | 443.369 |902.318 | 1250.906 |
| 2048 | 427.923 |864.701 | 1228.599 |
| 4096 | 430.814 |863.258 | 1210.633 |
| 8192 | 437.079 |883.766 | 1242.756 |
|16384 | 433.349 |851.653 | 1211.161 |
|32768 | 433.188 |906.142 | 1171.551 |
|65536 | 437.420 |890.266 | 1089.157 |
| 131072 | 431.189 |869.041 | 1180.171 |
| 262144 | 430.636 |910.133 | 1212.344 |
| 524288 | 429.365 |889.867 | 1197.842 |
|1048576 | 433.012 |887.909 | 1153.719 |
|2097152 | 432.520 |861.701 | 1179.221 |
|4194304 | 439.791 |899.085 | 1178.736 |
|8388608 | 442.422 |856.197 | 1211.953 |
| 16777216 | 419.015 |866.560 | 1152.946 |
"
HADOOP-8968,Add a flag to completely disable the worker version check,"The current logic in the TaskTracker and the DataNode to allow a relax version check with the JobTracker and NameNode works only if the versions of Hadoop are exactly the same.

We should add a switch to disable version checking completely, to enable rolling upgrades between compatible versions (typically patch versions)."
HADOOP-8963,CopyFromLocal doesn't always create user directory,"When you use the command ""hadoop fs -copyFromLocal filename ."" before the /user/username directory has been created, the file is created with name /user/username instead of a directory being created with file /user/username/filename.  The command ""hadoop fs -copyFromLocal filename filename"" works as expected, creating /user/username and /user/username/filename, and ""hadoop fs -copyFromLocal filename ."" works as expected if the /user/username directory already exists."
HADOOP-8962,RawLocalFileSystem.listStatus fails when a child filename contains a colon,"If listStatus is called on a directory that contains a file with a ':' in its name then it mistakenly thinks there is a scheme being specified and an exception is thrown because of a relative URI.
"
HADOOP-8958,ViewFs:Non absolute mount name failures when running multiple tests on Windows,This appears to be an issue with parsing a Windows-specific path.
HADOOP-8957,AbstractFileSystem#IsValidName should be overridden for embedded file systems like ViewFs,"This appears to be a problem with parsing a Windows-specific path, ultimately throwing InvocationTargetException from AbstractFileSystem.newInstance."
HADOOP-8951,RunJar to fail with user-comprehensible error message if jar missing,"When the RunJar JAR is missing or not a file, exit with a meaningful message."
HADOOP-8948,TestFileUtil.testGetDU fails on Windows due to incorrect assumption of line separator,"The test asserts 8 bytes of disk usage, based on 2 files, each containing a single line of 3 characters followed by line terminator.  This is fine for Unix line endings (1 additional byte) but incorrect for Windows line endings (2 additional bytes)."
HADOOP-8944,Shell command fs -count should include human readable option,"The shell command fs -count report sizes in bytes.  The command should accept a -h option to display the sizes in a human readable format, i.e. K, M, G, etc."
HADOOP-8943,Support multiple group mapping providers,"  Discussed with Natty about LdapGroupMapping, we need to improve it so that: 
1. It's possible to do different group mapping for different users/principals. For example, AD user should go to LdapGroupMapping service for group, but service principals such as hdfs, mapred can still use the default one ShellBasedUnixGroupsMapping; 

2. Multiple ADs can be supported to do LdapGroupMapping; 

3. It's possible to configure what kind of users/principals (regarding domain/realm is an option) should use which group mapping service/mechanism.

4. It's possible to configure and combine multiple existing mapping providers without writing codes implementing new one."
HADOOP-8932,JNI-based user-group mapping modules can be too chatty on lookup failures,"On a user/group lookup failure, JniBasedUnixGroupsMapping and JniBasedUnixGroupsNetgroupMapping are logging the
full stack trace at WARN level.  Since the caller of these methods is already logging errors, this is not needed.  In branch-1, just one line is logged, so we don't need this change there.

"
HADOOP-8931,Add Java version to startup message,"I often look at logs and have to track down the java version they were run with, it would be useful if we logged this as part of the startup message."
HADOOP-8930,Cumulative code coverage calculation,"When analyzing code coverage in Hadoop Core, we noticed that some coverage gaps are caused by the way the coverage calculation is done currently. More specifically, right now coverage can not be calculated for the whole Core at once, but can only be calculated separately for top level modules like common-project, hadoop-hdfs-project etc. 

At the same time, some code in particular modules is tested by tests in other modules of Core. For example, ""org.apache.hadoop.fs"" from hadoop-common-project/hadoop-common is not covered there but it's covered by tests under hadoop-hdfs-project.

To enable calculation of ""cumulative"" code coverage it's needed to move Clover profile definition up one level, from hadoop-project/pom.xml to the top level pom.xml (hadoop-main).

Patch both for 0.23 and 2.x will be attached shortly."
HADOOP-8929,"Add toString, other improvements for SampleQuantiles","The new SampleQuantiles class is useful in the context of benchmarks, but currently there is no way to print it out outside the context of a metrics sink. It would be nice to have a convenient way to stringify it for logging, etc.

Also:
- made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible.
- changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.
"
HADOOP-8926,hadoop.util.PureJavaCrc32 cache hit-ratio is low for static data,"While running microbenchmarks for HDFS write codepath, a significant part of the CPU fraction was consumed by the DataChecksum.update(). 

The attached patch converts the static arrays in CRC32 into a single linear array for a performance boost in the inner loop.

milli-seconds for 1Gig (16400 loop over a 64kb chunk) 

|| platform || original || cache-aware || improvement ||
| x86 | 3894 | 2304 | 40.83 |
| x86_64 | 2131 | 1826 | 14 | 

The performance improvement on x86 is rather larger than the 64bit case, due to the extra register/stack pressure caused by the static arrays.

A closer analysis of the PureJavaCrc32 JIT code shows the following assembly fragment

{code}
  0x40f1e345: mov    $0x184,%ecx
  0x40f1e34a: mov    0x4415b560(%ecx),%ecx  ;*getstatic T8_5
                                        ; - PureJavaCrc32::update@95 (line 61)
                                        ;   {oop('PureJavaCrc32')}
  0x40f1e350: mov    %ecx,0x2c(%esp)
{code}

Basically, the static variables T8_0 through to T8_7 are being spilled to the stack because of register pressure. The x86_64 case has a lower likelihood of such pessimistic JIT code due to the increased number of registers."
HADOOP-8925,Remove the packaging,"Per discussion on HADOOP-8809, now that Bigtop is TLP and supports Hadoop v2 let's remove the Hadoop packaging from trunk and branch-2. We should remove it anyway since it no longer part of the build post mavenization, was not updated post MR1 (there's no MR2/YARN packaging) and is not maintained."
HADOOP-8924,Add maven plugin alternative to shell script to save package-info.java,"Currently, the build process relies on saveVersion.sh to generate package-info.java with a version annotation.  The sh binary may not be available on all developers' machines (e.g. Windows without Cygwin). This issue tracks removal of that dependency in Hadoop Common."
HADOOP-8922,Provide alternate JSONP output for JMXJsonServlet to allow javascript in browser dashboard,"JMXJsonServlet may provide a JSONP alternative to JSON to allow javascript in browser GUI to make requests.
For security purpose about XSS, browser limit request on other domain[¹|#ref1] so that metrics from cluster nodes cannot be used in a full js interface.
An example of this kind of dashboard is the bigdesk[²|#ref2] plugin for ElasticSearch.

In order to achieve that the servlet should detect a GET parameter (callback=xxxx) and modify the response by surrounding the Json value with ""xxxx("" and "");"" [³|#ref3]
value ""xxxx"" is variable and should be provide by client as callback parameter value.

{anchor:ref1}[1] https://developer.mozilla.org/en-US/docs/Same_origin_policy_for_JavaScript
{anchor:ref2}[2] https://github.com/lukas-vlcek/bigdesk
{anchor:ref3}[3] http://en.wikipedia.org/wiki/JSONP"
HADOOP-8917,add LOCALE.US to toLowerCase in SecurityUtil.replacePattern,"Webhdfs and fsck when getting the kerberos principal use Locale.US in toLowerCase. We should do the same in replacePattern as this method is used when service prinicpals log in.

see https://issues.apache.org/jira/browse/HADOOP-8878?focusedCommentId=13472245&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13472245 for more details"
HADOOP-8913,hadoop-metrics2.properties should give units in comment for sampling period,"the default hadoop-metrics2.properties contains the lines
#default sampling period
*.period=10

it should be made clear that the units are seconds"
HADOOP-8912,adding .gitattributes file to prevent CRLF and LF mismatches for source and text files,"Source code in hadoop-common repo has a bunch of files that have CRLF endings.
With more development happening on windows there is a higher chance of more CRLF files getting into the source tree.
I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files.

I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it.

# http://git-scm.com/docs/gitattributes#_checking_out_and_checking_in
# http://stackoverflow.com/questions/170961/whats-the-best-crlf-handling-strategy-with-git
 
This issue for adding .gitattributes file to the tree."
HADOOP-8911,CRLF characters in source and text files,"Source code in hadoop-common repo has a bunch of files that have CRLF endings.
With more development happening on windows there is a higher chance of more CRLF files getting into the source tree.
I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files.
But before adding the .gitattributes file we need to normalize the existing tree, so that people when they sync after .giattributes change wont end up with a bunch of modified files in their workspace.
I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it.
# http://git-scm.com/docs/gitattributes#_checking_out_and_checking_in
# http://stackoverflow.com/questions/170961/whats-the-best-crlf-handling-strategy-with-git

I will submit a separate bug and patch for .gitattributes

"
HADOOP-8909,Hadoop Common Maven protoc calls must not depend on external sh script,"Currently, several pom.xml files rely on external shell scripting to call protoc.  The sh binary may not be available on all developers' machines (e.g. Windows without Cygwin).  This issue tracks removal of that dependency in Hadoop Common."
HADOOP-8906,paths with multiple globs are unreliable,"Let's say we have have a structure of ""$date/$user/stuff/file"".  Multiple globs are unreliable unless every directory in the structure exists.

These work:
date*/user
date*/user/stuff
date*/user/stuff/file

These fail:
date*/user/*
date*/user/*/*
date*/user/stu*
date*/user/stu*/*
date*/user/stu*/file
date*/user/stuff/*
date*/user/stuff/f*"
HADOOP-8901,GZip and Snappy support may not work without unversioned libraries,"Currently, we use {{dlopen}} to open {{libz.so}} and {{libsnappy.so}}, to get Gzip and Snappy support, respectively.

However, this is not correct; we should be dlopening {{libsnappy.so.1}} instead.  The versionless form of the shared library is not commonly installed except by development packages.  Also, we may run into subtle compatibility problems if a new version of libsnappy comes out.

Thanks to Brandon Vargo for reporting this bug."
HADOOP-8900,BuiltInGzipDecompressor throws IOException - stored gzip size doesn't match decompressed size,"Encountered failure when processing large GZIP file
• Gz: Failed in 1hrs, 13mins, 57sec with the error:
 ¸java.io.IOException: IO error in map input file hdfs://localhost:9000/Halo4/json_m/gz/NewFileCat.txt.gz
 at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:242)
 at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:216)
 at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
 at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:435)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:371)
 at org.apache.hadoop.mapred.Child$4.run(Child.java:266)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:415)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
 at org.apache.hadoop.mapred.Child.main(Child.java:260)
 Caused by: java.io.IOException: stored gzip size doesn't match decompressed size
 at org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeTrailerState(BuiltInGzipDecompressor.java:389)
 at org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:224)
 at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:82)
 at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:76)
 at java.io.InputStream.read(InputStream.java:102)
 at org.apache.hadoop.util.LineReader.readLine(LineReader.java:134)
 at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:136)
 at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:40)
 at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:66)
 at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:32)
 at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:67)
 at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:236)
 ... 9 more"
HADOOP-8896,Javadoc points to Wrong Reader and Writer classes in SequenceFile,"Line 56 of org.apache.hadoop.io.SequenceFile refers to {@link Writer}, {@link Reader} in the javadoc comment describing the class SequenceFile. When the javadoc is built Reader and Writer link to java.io.Reader and java.io.Writer, respectively. However, they should instead refer to {@link SequenceFile.Reader} and {@link SequenceFile.Writer}."
HADOOP-8894,GenericTestUtils.waitFor should dump thread stacks on timeout,"Many tests use this utility to wait for a condition to become true. In the event that it times out, we should dump all the thread stack traces, in case the timeout was due to a deadlock. This should make it easier to debug scenarios like HDFS-4001."
HADOOP-8889,Upgrade to Surefire 2.12.3,"Surefire 2.12.3 has a couple improvements which are helpful for us. In particular, it fixes http://jira.codehaus.org/browse/SUREFIRE-817 which has been aggravating in the past."
HADOOP-8886,Remove KFS support,"KFS is no longer maintained (is replaced by QFS, which HADOOP-8885 is adding), let's remove it."
HADOOP-8883,Anonymous fallback in KerberosAuthenticator is broken,HADOOP-8855 changed KerberosAuthenticator to handle when the JDK did the SPNEGO already; but this change broke using the fallback authenticator (PseudoAuthenticator) with an anonymous user (see OOZIE-1010).  
HADOOP-8882,uppercase namenode host name causes fsck to fail when useKsslAuth is on,"{code}
 public static void fetchServiceTicket(URL remoteHost) throws IOException {
    if(!UserGroupInformation.isSecurityEnabled())
      return;
    
    String serviceName = ""host/"" + remoteHost.getHost();
{code}

the hostname should be converted to lower case. Saw this in branch 1, will look at trunk and update the bug accordingly."
HADOOP-8881,FileBasedKeyStoresFactory initialization logging should be debug not info,"When hadoop.ssl.enabled is set to true hadoop client invocations get a log message on the terminal with the initialization of the keystores, switching to debug will disable this log message by default."
HADOOP-8880,Missing jersey jars as dependency in the pom causes hive tests to fail,ivy.xml has the dependency included where as the same dependency is not updated in the pom template.
HADOOP-8878,uppercase namenode hostname causes hadoop dfs calls with webhdfs filesystem and fsck to fail when security is on,"This was noticed on a secure cluster where the namenode had an upper case hostname and the following command was issued

hadoop dfs -ls webhdfs://NN:PORT/PATH

the above command failed because delegation token retrieval failed.

Upon looking at the kerberos logs it was determined that we tried to get the ticket for kerberos principal with upper case hostnames and that host did not exit in kerberos. We should convert the hostnames to lower case. Take a look at HADOOP-7988 where the same fix was applied on a different class.

I have noticed this issue exists on branch-1. Will investigate trunk and branch-2 and update accordingly."
HADOOP-8870,NullPointerException when glob doesn't return files,"Reading

{code}s3n://bucket/{a/,b/,c/}{code}

if one of the globs matches nothing, I get:

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:992)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:177)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)
	at spark.HadoopRDD.<init>(HadoopRDD.scala:51)
	at spark.SparkContext.hadoopFile(SparkContext.scala:186)
	at spark.SparkContext.textFile(SparkContext.scala:155)
	at com.celtra.analyzer.LogAnalyzer.analyzeSufficientS3Logs(LogAnalyzer.scala:52)
	at com.celtra.analyzer.App$.main(App.scala:164)
	at com.celtra.analyzer.App.main(App.scala)
{code}

I'm not sure whether this is specific to S3 or all filesystems.

This was occuring in 0.20.205 and I confirmed it's still present in 1.0.3."
HADOOP-8866,SampleQuantiles#query is O(N^2) instead of O(N),"SampleQuantiles#query() does O(N) calls LinkedList#get() in a loop, rather than using an iterator. This makes query O(N^2), rather than O(N)."
HADOOP-8861,FSDataOutputStream.sync should call flush() if the underlying wrapped stream is not Syncable,"Currently FSDataOutputStream.sync is a no-op if the wrapped stream is not Syncable. Instead it should call flush() if the wrapped stream is not syncable.

This behavior is already present in trunk, but branch-1 does not have this."
HADOOP-8860,Split MapReduce and YARN sections in documentation navigation,This JIRA is to change the navigation on http://hadoop.apache.org/docs/r2.0.1-alpha/ to reflect the fact that MapReduce and YARN are separate modules/sub-projects.
HADOOP-8855,SSL-based image transfer does not work when Kerberos is disabled,"In SecurityUtil.openSecureHttpConnection, we first check {{UserGroupInformation.isSecurityEnabled()}}. However, this only checks the kerberos config, which is independent of {{hadoop.ssl.enabled}}. Instead, we should check {{HttpConfig.isSecure()}}.

Credit to Wing Yew Poon for discovering this bug"
HADOOP-8851,Use -XX:+HeapDumpOnOutOfMemoryError JVM option in the forked tests,"This can help to reveal the cause of issue in the event of OOME in tests.
Suggested patch attached."
HADOOP-8849,FileUtil#fullyDelete should grant the target directories +rwx permissions before trying to delete them,"2 improvements are suggested for implementation of methods org.apache.hadoop.fs.FileUtil.fullyDelete(File) and org.apache.hadoop.fs.FileUtil.fullyDeleteContents(File):
 
1) We should grant +rwx permissions the target directories before trying to delete them.
The mentioned methods fail to delete directories that don't have read or execute permissions.
Actual problem appears if an hdfs-related test is timed out (with a short timeout like tens of seconds), and the forked test process is killed, some directories are left on disk that are not readable and/or executable. This prevents next tests from being executed properly because these directories cannot be deleted with FileUtil#fullyDelete(), so many subsequent tests fail. So, its recommended to grant the read, write, and execute permissions the directories whose content is to be deleted.

2) Generic reliability improvement: we shouldn't rely upon File#delete() return value, use File#exists() instead. 
FileUtil#fullyDelete() uses return value of method java.io.File#delete(), but this is not reliable because File#delete() returns true only if the file was deleted as a result of the #delete() method invocation. E.g. in the following code
if (f.exists()) { // 1
  return f.delete(); // 2
}
if the file f was deleted by another thread or process between calls ""1"" and ""2"", this fragment will return ""false"", while the file f does not exist upon the method return.
So, better to write
if (f.exists()) {
  f.delete();
  return !f.exists();
}"
HADOOP-8843,Old trash directories are never deleted on upgrade from 1.x,"The older format of the trash checkpoint for 1.x is yyMMddHHmm the new format is yyMMddHHmmss(-\d+)? so if you upgrade from an old cluster to a new one, all of the entires in .trash will never be deleted because they currently are always ignored on deletion.

We should support deleting the older format as well."
HADOOP-8833,fs -text should make sure to call inputstream.seek(0) before using input stream,"From Muddy Dixon on HADOOP-8449:

Hi
We found the changes in order of switch and guard block in
{code}
private InputStream forMagic(Path p, FileSystem srcFs) throws IOException
{code}
Because of this change, return value of
{code}
codec.createInputStream(i)
{code}
is changed if codec exists.

{code}
private InputStream forMagic(Path p, FileSystem srcFs) throws IOException {
    FSDataInputStream i = srcFs.open(p);

    // check codecs
    CompressionCodecFactory cf = new CompressionCodecFactory(getConf());
    CompressionCodec codec = cf.getCodec(p);
    if (codec != null) {
      return codec.createInputStream(i);
    }

    switch(i.readShort()) {
       // cases
    }
{code}

New:

{code}
private InputStream forMagic(Path p, FileSystem srcFs) throws IOException {
    FSDataInputStream i = srcFs.open(p);

    switch(i.readShort()) { // <=== index (or pointer) processes!!
      // cases
      default: {
        // Check the type of compression instead, depending on Codec class's
        // own detection methods, based on the provided path.
        CompressionCodecFactory cf = new CompressionCodecFactory(getConf());
        CompressionCodec codec = cf.getCodec(p);
        if (codec != null) {
          return codec.createInputStream(i);
        }
        break;
      }
    }

    // File is non-compressed, or not a file container we know.
    i.seek(0);
    return i;
  }
{code}

Fix is to use i.seek(0) before we use i anywhere. I missed that."
HADOOP-8832,backport serviceplugin to branch-1,The original patch was only partially back ported to branch-1. This JIRA is to back port the rest of it.
HADOOP-8827,Upgrade jets3t to the latest,"As of hadoop-2.0.1-alpha, the bundles jets3t-0.6.1.jar, but the latest jets3t is already at 0.9.0 <http://jets3t.s3.amazonaws.com/downloads.html>. Perhaps it would be good to upgrade to use a more up-to-date version."
HADOOP-8826,Docs still refer to 0.20.205 as stable line,"The main docs page still refers to 0.20.205 as the stable line, 1.0 is the stable line now."
HADOOP-8823,ant package target should not depend on cn-docs,"In branch-1, the package target depends on cn-docs but the doc is already outdated."
HADOOP-8822,relnotes.py was deleted post mavenization,"relnotes.py was removed post mavinization.  It needs to be added back in so we can generate release notes, and it should be updated to deal with YARN and the separate release notes files."
HADOOP-8820,"Backport HADOOP-8469 and HADOOP-8470: add ""NodeGroup"" layer in new NetworkTopology (also known as NetworkTopologyWithNodeGroup)","This patch backport HADOOP-8469 and HADOOP-8470 to branch-1 and includes:
1. Make NetworkTopology class pluggable for extension.
2. Implement a 4-layer NetworkTopology class (named as NetworkTopologyWithNodeGroup) to use in virtualized environment (or other situation with additional layer between host and rack)."
HADOOP-8819,"Should use && instead of  & in a few places in FTPFileSystem,FTPInputStream,S3InputStream,ViewFileSystem,ViewFs","Should use && instead of  & in a few places in FTPFileSystem,FTPInputStream,S3InputStream,ViewFileSystem,ViewFs."
HADOOP-8818,Should use equals() rather than == to compare String or Text in MD5MD5CRC32FileChecksum and TFileDumper,Should use equals() rather than == to compare String or Text in MD5MD5CRC32FileChecksum and TFileDumper.
HADOOP-8817,Backport Network Topology Extension for Virtualization (HADOOP-8468) to branch-1,"HADOOP-8468 propose network topology changes for running on virtualized infrastructure, which includes:
1. Add ""NodeGroup"" layer in new NetworkTopology (also known as NetworkTopologyWithNodeGroup): HADOOP-8469, HADOOP-8470
2. Update Replica Placement/Removal Policy to reflect new topology layer: HDFS-3498, HDFS-3601
3. Update balancer policy:HDFS-3495
4. Update Task Scheduling Policy to reflect new topology layer and support the case that compute nodes (NodeManager or TaskTracker) and data nodes are separated into different VMs, but still benefit from physical host locality: YARN-18, YARN-19.
This JIRA will address the backport work on branch-1 which will be divided into 4 issues/patches in related jira issues."
HADOOP-8816,HTTP Error 413 full HEAD if using kerberos authentication,"The HTTP Authentication: header is too large if using kerberos and the request is rejected by Jetty because Jetty has a too low default header size limit.

Can be fixed by adding ret.setHeaderBufferSize(1024*128); in org.apache.hadoop.http.HttpServer.createDefaultChannelConnector

"
HADOOP-8815,RandomDatum overrides equals(Object) but no hashCode(),"Override equal() but not hashCode() is a violation of the general contract for Object.hashCode will occur, which can have unexpected repercussions when this class is in conjunction with all hash-based collections.

This test class is used in multiple places, so it may be worth fixing."
HADOOP-8814,Inefficient comparison with the empty string. Use isEmpty() instead,"Prior to JDK 6, we can check if a string is empty by doing """".equals(s) or s.equals("""").

Starting from JDK 6, String class has a new convenience and efficient method isEmpty() to check string's length."
HADOOP-8812,ExitUtil#terminate should print Exception#toString ,Per Steve's feedback on ExitUtil#terminate should print Exception#toString rather than use getMessage as the latter may return null.  
HADOOP-8811,Compile hadoop native library in FreeBSD,Native hadoop library do not compiles in FreeBSD because setnetgrent returns void and assembler do not supports SSE4 instructions.
HADOOP-8810,when building with documentation build stops waiting for ENTER on terminal,"When building the docs {{mvn clean package -Pdocs -DskipTests site site:stage -DstagingDirectory=/tmp/hadoop-site}}, in OSX (and I've seen it a few times in Ubuntu as well), the build stops, if you press ENTER it continues. It happens twice.

I've traced this down to the exec-maven-plugin invocation of protoc for hadoop-yarn-api module (and other YARN module I don't recall at the moment).

jstacking the Maven process it seems the exec-maven-plugin has some locking issues consuming the STDOUT/STDERR of the process being executed.

I've converted the protoc invocation in the hadoop-yarn-api to use the antrun plugin instead and then another module running protoc using exec-maven-puglin hang.


"
HADOOP-8808,"Update FsShell documentation to mention deprecation of some of the commands, and mention alternatives","In HADOOP-7286, we deprecated the following 3 commands dus, lsr and rmr, in favour of du -s, ls -r and rm -r respectively. The FsShell documentation should be updated to mention these, so that users can start switching. Also, there are places where we refer to the deprecated commands as alternatives. This can be changed as well."
HADOOP-8806,"libhadoop.so: dlopen should be better at locating libsnappy.so, etc.","libhadoop calls {{dlopen}} to load {{libsnappy.so}} and {{libz.so}}.  These libraries can be bundled in the {{$HADOOP_ROOT/lib/native}} directory.  For example, the {{-Dbundle.snappy}} build option copies {{libsnappy.so}} to this directory.  However, snappy can't be loaded from this directory unless {{LD_LIBRARY_PATH}} is set to include this directory.

Can we make this configuration ""just work"" without needing to rely on {{LD_LIBRARY_PATH}}?"
HADOOP-8804,Improve Web UIs when the wildcard address is used,"When IPC addresses are bound to the wildcard (ie the default config) the NN, JT (and probably RM etc) Web UIs are a little goofy. Eg ""0 Hadoop Map/Reduce Administration"" and ""NameNode '0.0.0.0:18021' (active)"". Let's improve them."
HADOOP-8801,ExitUtil#terminate should capture the exception stack trace,"ExitUtil#terminate(status,Throwable) should capture and log the stack trace of the given throwable. This will help debug issues like HDFS-3933."
HADOOP-8795,"BASH tab completion doesn't look in PATH, assumes path to executable is specified","bash-tab-completion/hadoop.sh checks that the first token in the command is an existing, executable file - which assumes that the path to the hadoop executable is specified (or that it's in the working directory). If the executable is somewhere else in PATH, tab completion will not work.

I propose that the first token be passed through 'which' so that any executables in the path also get detected. I've tested that this technique will work in the event that relative and absolute paths are used as well."
HADOOP-8794,Modifiy bin/hadoop to point to HADOOP_YARN_HOME,YARN-9 renames YARN_HOME to HADOOP_YARN_HOME. bin/hadoop script also needs to do the same.
HADOOP-8793,hadoop-core has dependencies on two different versions of commons-httpclient,"hadoop-core fails to enforce dependency convergence, resulting in potential conflicts.

At the very least, there appears to be a direct dependency on
{code}commons-httpclient:commons-httpclient:3.0.1{code}
but a transitive dependency on
{code}commons-httpclient:commons-httpclient:3.1{code}
via
{code}net.java.dev.jets3t:jets3t:0.7.1{code}

See http://maven.apache.org/enforcer/enforcer-rules/dependencyConvergence.html for details on how to enforce dependency convergence in Maven.

Please enforce dependency convergence... it helps projects that depend on hadoop libraries build much more reliably and safely."
HADOOP-8791,"rm ""Only deletes non empty directory and files.""","The documentation (1.0.3) is describing the opposite of what rm does.
It should be  ""Only delete files and empty directories.""

With regards to file, the size of the file should not matter, should it?

OR I am totally misunderstanding the semantic of this command and I am not the only one."
HADOOP-8789,Tests setLevel(Level.OFF) should be Level.ERROR,"Multiple tests have code like
{code}
((Log4JLogger)LogFactory.getLog(FSNamesystem.class)).getLogger().setLevel(Level.OFF);
{code}
Completely disabling logs from given classes with {{Level.OFF}} is a bad idea and makes debugging other test failures, especially intermittent test failures like HDFS-3664, difficult.  Instead the code should use {{Level.ERROR}} to reduce verbosity."
HADOOP-8786,HttpServer continues to start even if AuthenticationFilter fails to init,"As seen in HDFS-3904, if the AuthenticationFilter fails to initialize, the web server will continue to start up. We need to check for context initialization errors after starting the server."
HADOOP-8784,Improve IPC.Client's token use,"If present, tokens should be sent for all auth types including simple auth."
HADOOP-8783,Improve RPC.Server's digest auth,RPC.Server should always allow digest auth (tokens) if a secret manager if present.
HADOOP-8781,hadoop-config.sh should add JAVA_LIBRARY_PATH to LD_LIBRARY_PATH,Snappy SO fails to load properly if LD_LIBRARY_PATH does not include the path where snappy SO is. This is observed in setups that don't have an independent snappy installation (not installed by Hadoop)
HADOOP-8780,Update DeprecatedProperties apt file,The current list of deprecated properties is not up-to-date. I'll will upload a patch momentarily.
HADOOP-8775,MR2 distcp permits non-positive value to -bandwidth option which causes job never to complete,"The likelihood that someone would want to enter a non-positive value for -bandwidth seems really low. However, the job would never complete if a non-positive value was specified. It'd just get stuck at map 100%. Luckily, a positive value would always lead to the job completing.

{noformat}
bash-4.1$ hadoop distcp -bandwidth 0 hdfs://c1204.hal.cloudera.com:17020/user/hdfs/in-dir hdfs://c1204.hal.cloudera.com:17020/user/hdfs/in-dir58
hadoop distcp -bandwidth 0 hdfs://c1204.hal.cloudera.com:17020/user/hdfs/in-dir hdfs://c1204.hal.cloudera.com:17020/user/hdfs/in-dir58
12/05/23 15:53:01 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, maxMaps=20, sslConfigurationFile='null', copyStrategy='uniformsiz\
e', sourceFileListing=null, sourcePaths=[hdfs://c1204.hal.cloudera.com:17020/user/hdfs/in-dir], targetPath=hdfs://c1204.hal.cloudera.com:17020/user/hdfs/in-dir58}
12/05/23 15:53:02 WARN conf.Configuration: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb
12/05/23 15:53:02 WARN conf.Configuration: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor
12/05/23 15:53:02 INFO util.NativeCodeLoader: Loaded the native-hadoop library
12/05/23 15:53:03 INFO mapreduce.JobSubmitter: number of splits:3
12/05/23 15:53:04 WARN conf.Configuration: mapred.jar is deprecated. Instead, use mapreduce.job.jar
12/05/23 15:53:04 WARN conf.Configuration: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
12/05/23 15:53:04 WARN conf.Configuration: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
12/05/23 15:53:04 WARN conf.Configuration: mapred.mapoutput.value.class is deprecated. Instead, use mapreduce.map.output.value.class
12/05/23 15:53:04 WARN conf.Configuration: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class
12/05/23 15:53:04 WARN conf.Configuration: mapred.job.name is deprecated. Instead, use mapreduce.job.name
12/05/23 15:53:04 WARN conf.Configuration: mapreduce.inputformat.class is deprecated. Instead, use mapreduce.job.inputformat.class
12/05/23 15:53:04 WARN conf.Configuration: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
12/05/23 15:53:04 WARN conf.Configuration: mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
12/05/23 15:53:04 WARN conf.Configuration: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
12/05/23 15:53:04 WARN conf.Configuration: mapred.mapoutput.key.class is deprecated. Instead, use mapreduce.map.output.key.class
12/05/23 15:53:04 WARN conf.Configuration: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
12/05/23 15:53:04 INFO mapred.ResourceMgrDelegate: Submitted application application_1337808305464_0014 to ResourceManager at c1204.hal.cloudera.com/172.29.98.195:8040
12/05/23 15:53:04 INFO mapreduce.Job: The url to track the job: http://auto0:8088/proxy/application_1337808305464_0014/
12/05/23 15:53:04 INFO tools.DistCp: DistCp job-id: job_1337808305464_0014
12/05/23 15:53:04 INFO mapreduce.Job: Running job: job_1337808305464_0014
12/05/23 15:53:09 INFO mapreduce.Job: Job job_1337808305464_0014 running in uber mode : false
12/05/23 15:53:09 INFO mapreduce.Job:  map 0% reduce 0%
12/05/23 15:53:14 INFO mapreduce.Job:  map 33% reduce 0%
12/05/23 15:53:19 INFO mapreduce.Job:  map 100% reduce 0%
{noformat}"
HADOOP-8770,NN should not RPC to self to find trash defaults (causes deadlock),"When transitioning a SBN to active, I ran into the following situation:
- the TrashPolicy first gets loaded by an IPC Server Handler thread. The {{initialize}} function then tries to make an RPC to the same node to find out the defaults.
- This is happening inside the NN write lock (since it's part of the active initialization). Hence, all of the other handler threads are already blocked waiting to get the NN lock.
- Since no handler threads are free, the RPC blocks forever and the NN never enters active state.

We need to have a general policy that the NN should never make RPCs to itself for any reason, due to potential for deadlocks like this."
HADOOP-8767,secondary namenode on slave machines,when the default value for HADOOP_SLAVES is changed in hadoop-env.sh the hdfs starting (with start-dfs.sh) creates secondary namenodes on all the machines in the file conf/slaves instead of conf/masters.
HADOOP-8766,FileContextMainOperationsBaseTest should randomize the root dir ,"FileContextMainOperationsBaseTest should randomize the name of the root directory it creates. It currently hardcodes LOCAL_FS_ROOT_URI to {{/tmp/test}}.

This causes the job to fail if it clashes with another jobs that also uses that path. Eg

{noformat}
org.apache.hadoop.fs.FileAlreadyExistsException: Parent path is not a directory: file:/tmp/test
        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:362)
        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:373)
        at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:931)
        at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)
        at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)
        at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)
        at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)
        at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2333)
        at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)
        at org.apache.hadoop.fs.FileContextMainOperationsBaseTest.testWorkingDirectory(FileContextMainOperationsBaseTest.java:178)
{noformat}
"
HADOOP-8764,CMake: HADOOP-8737 broke ARM build,"ARM build is broken again: CMAKE_SYSTEM_PROCESSOR comes from {{uname -p}}, which reports values like ""armv7l"" for the ARMv7 architecture. However, the OpenJDK and Oracle ARM JREs both use ""jre/lib/arm"" for the JVM directory."
HADOOP-8757,Metrics should disallow names with invalid characters,"Just spent a couple hours trying to figure out why a metric I added didn't show up in JMX, only to eventually realize it was because I had a whitespace in the property name. This didn't cause any errors to be logged -- the metric just didn't show up in JMX. We should check that the name is valid and log an error, or replace invalid characters with something like an underscore."
HADOOP-8756,Fix SEGV when libsnappy is in java.library.path but not LD_LIBRARY_PATH,"We use {{System.loadLibrary(""snappy"")}} from the Java side.  However in libhadoop, we use {{dlopen}} to open libsnappy.so dynamically.  System.loadLibrary uses {{java.library.path}} to resolve libraries, and {{dlopen}} uses {{LD_LIBRARY_PATH}} and the system paths to resolve libraries.  Because of this, the two library loading functions can be at odds.

We should fix this so we only load the library once, preferably using the standard Java {{java.library.path}}.

We should also log the search path(s) we use for {{libsnappy.so}} when loading fails, so that it's easier to diagnose configuration issues."
HADOOP-8755,Print thread dump when tests fail due to timeout ,"When a test fails due to timeout it's often not clear what is the root cause. See HDFS-3364 as an example.

We can print dump of all threads in this case, this may help finding causes."
HADOOP-8754,Deprecate all the RPC.getServer() variants,"In HADOOP-8736, a Builder is introduced to replace all the getServer() variants. This JIRA is to delete all the getServer() variants once all the other components replaced this method with the Builder."
HADOOP-8753,"LocalDirAllocator throws ""ArithmeticException: / by zero"" when there is no available space on configured local dir","12/08/09 13:59:49 INFO mapreduce.Job: Task Id : attempt_1344492468506_0023_m_000000_0, Status : FAILED
java.lang.ArithmeticException: / by zero
        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:371)
        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:150)
        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:131)
        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:115)
        at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.getLocalPathForWrite(LocalDirsHandlerService.java:257)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:849)

Instead of throwing exception directly we can log a warning saying no available space on configured local dir"
HADOOP-8749,HADOOP-8031 changed the way in which relative xincludes are handled in Configuration.,The patch from HADOOP-8031 changed the xml parsing to use DocumentBuilder#parse(InputStream uri.openStream()) instead of DocumentBuilder#parse(String uri.toString()).I looked into the implementation of javax.xml.parsers.DocumentBuilder and org.xml.sax.InputSource and there is a difference when the DocumentBuilder parse(String) method is used versus parse(InputStream).
HADOOP-8748,Move dfsclient retry to a util class,HDFS-3504 introduced mechanisms to retry RPCs. I want to move that to common to allow MAPREDUCE-4603 to share it too. Should be a trivial patch.
HADOOP-8747,Syntax error on cmake version 2.6 patch 2 in JNIFlags.cmake,"On SUSE Linux Enterprise Server 11 SP1, ""cmake version 2.6 patch 2"" is installed.

It seems to have trouble parsing this ""if"" statement in JNIFlags.cmake:
{code}
IF((NOT JAVA_JVM_LIBRARY) OR (NOT JAVA_INCLUDE_PATH) OR (NOT JAVA_INCLUDE_PATH2))
{code}

We should rephrase this if statement so that it will work on all versions of cmake above or equal to 2.6."
HADOOP-8745,Incorrect version numbers in hadoop-core POM,"The hadoop-core POM as published to Maven central has different dependency versions than Hadoop actually has on its runtime classpath. This can lead to client code working in unit tests but failing on the cluster and vice versa.

The following version numbers are incorrect: jackson-mapper-asl, kfs, and jets3t. There's also a duplicate dependency to commons-net."
HADOOP-8737,"cmake: always use JAVA_HOME to find libjvm.so, jni.h, jni_md.h","We should always use the {{libjvm.so}}, {{jni.h}}, and {{jni_md.h}} under {{JAVA_HOME}}, rather than trying to look for them in system paths.  Since we compile with Maven, we know that we'll have a valid {{JAVA_HOME}} at all times.  There is no point digging in system paths, and it can lead to host contamination if the user has multiple JVMs installed."
HADOOP-8736,Add Builder for building an RPC server,There are quite a few variants of getServer() method to create an RPC server. Create a builder class to abstract the building steps and avoid more getServer() variants in the future.
HADOOP-8727,Gracefully deprecate dfs.umaskmode in 2.x onwards,"While HADOOP-6234 added dfs.umaskmode in 0.21.0, the subsequent HADOOP-6233 simply renamed it, again in 0.21.0, without any deprecation mechanism (understandable).

However, 1.x now carries dfs.umaskmode but there isn't a graceful deprecation when one upgrades to 2.x. We should recreate this prop and add it to the deprecated list."
HADOOP-8726,The Secrets in Credentials are not available to MR tasks,"Though secrets are passed in Credentials, the secrets are not available to the MR tasks.

This issue  exists with security on/off. 

This is related to the change in HADOOP-8225
"
HADOOP-8725,MR is broken when security is off,"HADOOP-8225 broke MR when security is off.  MR was changed to stop re-reading the credentials that UGI had already read, and to stop putting those tokens back into the UGI where they already were.  UGI only reads a credentials file when security is enabled, but MR uses tokens (ie. job token) even when security is disabled..."
HADOOP-8722,Update BUILDING.txt with latest snappy info,"HADOOP-8620 changed the default of snappy.lib from snappy.prefix/lib to empty.  This, in turn, means that you can't use {{-Dbundle.snappy}} without setting {{-Dsnappy.lib}}"
HADOOP-8721,ZKFC should not retry 45 times when attempting a graceful fence during a failover,"Scenario:
Active NN on machine1
Standby NN on machine2
Machine1 is isolated from the network (machine1 network cable unplugged)
After zk session timeout ZKFC at machine2 side gets notification that NN1 is not there.
ZKFC tries to failover NN2 as active.
As part of this during fencing it tries to connect to machine1 and kill NN1. (sshfence technique configured)
This connection retry happens for 45 times( as it takes  ipc.client.connect.max.socket.retries)
Also after that standby NN is not able to take over as active (because of fencing failure).
Suggestion: If ZKFC is not able to reach other NN for specified time/no of retries it can consider that NN as dead and instruct the other NN to take over as active as there is no chance of the other NN (NN1) retaining its state as active after zk session timeout when its isolated from network

From ZKFC log:
{noformat}
2012-06-21 17:46:14,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 22 time(s).
2012-06-21 17:46:35,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 23 time(s).
2012-06-21 17:46:56,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 24 time(s).
2012-06-21 17:47:17,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 25 time(s).
2012-06-21 17:47:38,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 26 time(s).
2012-06-21 17:47:59,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 27 time(s).
2012-06-21 17:48:20,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 28 time(s).
2012-06-21 17:48:41,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 29 time(s).
2012-06-21 17:49:02,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 30 time(s).
2012-06-21 17:49:23,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 31 time(s).
{noformat}
 

"
HADOOP-8720,TestLocalFileSystem should use test root subdirectory,"During unit test root directory of ""test.build.data"" is deleted possibly affecting other tests."
HADOOP-8718,org.apache.hadoop.fs.viewfs.InodeTree.createLink(...) may throw java.lang.ArrayIndexOutOfBoundsException,"org.apache.hadoop.fs.viewfs.InodeTree.createLink(...) may throw java.lang.ArrayIndexOutOfBoundsException as breakIntoPathComponents(src) may return Empty Array ""[]"". this happened when run the test case of viewfs on my jenkins server. In my situation, ""/"" is passed into breakIntoPathComponents() as its param ""src"". 

Here is the Message given by Jenkins:

java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:237)
        at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:178)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:178)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2150)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:80)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2184)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2166)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:302)
        at org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:64)
        at org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
        at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
        at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)

Standard Output
        2012-08-22 10:31:40,487 INFO  mortbay.log (Slf4jLog.java:info(67)) - Home dir base /"
HADOOP-8713,TestRPCCompatibility fails intermittently with JDK7,"TestRPCCompatibility can fail intermittently with errors like the following when tests are not run in declaration order:

{noformat}
testVersion2ClientVersion1Server(org.apache.hadoop.ipc.TestRPCCompatibility): expected:<3> but was:<-3>
{noformat}

Moving the reset of the ProtocolSignature cache from ad-hoc usage in testVersion2ClientVersion2Server to tearDown fixes the issue."
HADOOP-8712,Change default hadoop.security.group.mapping,Change the hadoop.security.group.mapping in core-site to JniBasedUnixGroupsNetgroupMappingWithFallback
HADOOP-8711,provide an option for IPC server users to avoid printing stack information for certain exceptions,"Currently it's hard coded in the server that it doesn't print the exception stack for StandbyException. 

Similarly, other components may have their own exceptions which don't need to save the stack trace in log. One example is HDFS-3817."
HADOOP-8710,Remove ability for users to easily run the trash emptier,"Users can currently run the emptier via {{hadoop org.apache.hadoop.fs.Trash}}, which seems error prone as there's nothing in that command that suggests it runs the emptier and nothing that asks you before deleting the trash for all users (that the current user is capable of deleting). Given that the trash emptier runs server side (eg on the NN) let's remove the ability to easily run it client side.  Marking as an incompatible change since someone expecting the hadoop command with this class specified to empty trash will no longer be able to (they'll need to create their own class that does this)."
HADOOP-8709,globStatus changed behavior from 0.20/1.x,"In 0.20 or 1.x, globStatus will return an empty array if the glob pattern does not match any files.  After HADOOP-6201 it throws FileNotFoundException.  The javadoc states it will return an empty array."
HADOOP-8704,add request logging to jetty/httpserver,We have been requested to log all the requests coming into Jetty/HttpServer for security and auditing purposes. 
HADOOP-8703,distcpV2: turn CRC checking off for 0 byte size,"DistcpV2 (hadoop-tools/hadoop-distcp/..) can fail from checksum failure, sometimes when copying a 0 byte file. Root cause of this may have to do with an inconsistent nature of HDFS when creating 0 byte files, however distcp can avoid this issue by not checking CRC when size is zero.

This issue was reported as part of HADOOP-8233, though it seems like a better idea to treat this particular aspect on it's own."
HADOOP-8700,Move the checksum type constants to an enum,"In DataChecksum, there are constants for crc types, crc names and crc sizes.  We should move them to an enum for better coding style."
HADOOP-8699,some common testcases create core-site.xml in test-classes making other testcases to fail,"Some of the testcases (HADOOP-8581, MAPREDUCE-4417) create core-site.xml files on the fly in test-classes, overriding the core-site.xml that is part of the test/resources.

Things fail/pass depending on the order testcases are run (which seems dependent on the platform/jvm you are using).
"
HADOOP-8697,TestWritableName fails intermittently with JDK7,"On JDK7, {{testAddName}} can run before {{testSetName}}, which causes it to fail with:

{noformat}
testAddName(org.apache.hadoop.io.TestWritableName): WritableName can't load class: mystring
{noformat}
"
HADOOP-8695,TestPathData fails intermittently with JDK7,"Failed tests:   testWithDirStringAndConf(org.apache.hadoop.fs.shell.TestPathData): checking exist
  testToFile(org.apache.hadoop.fs.shell.TestPathData): expected:<file:/tmp> but was:</data/md0/hadoop-common/hadoop-common-project/hadoop-common/target/test/data/testPD/d1>

{{testWithStringAndConfForBuggyPath}} (which is declared last and therefore runs last with JDK6) overwrites the static variable {{testDir}} with {{file:///tmp}}. With JDK7, test methods run in an undefined order, and the other tests will fail if run after this one."
HADOOP-8693,TestSecurityUtil fails intermittently with JDK7,"Failed tests:   testBuildDTServiceName(org.apache.hadoop.security.TestSecurityUtil): expected:<[127.0.0.1]:123> but was:<[localhost]:123>
  testBuildTokenServiceSockAddr(org.apache.hadoop.security.TestSecurityUtil): expected:<[127.0.0.1]:123> but was:<[localhost]:123>

Test methods run in an arbitrary order with JDK7. In this case, these tests fail because tests like {{testSocketAddrWithName}} (which run afterward with JDK6) are calling {{SecurityUtil.setTokenServiceUseIp(false)}}."
HADOOP-8692,TestLocalDirAllocator fails intermittently with JDK7,"Failed tests:   test0[0](org.apache.hadoop.fs.TestLocalDirAllocator): Checking for build/test/temp/RELATIVE1 in build/test/temp/RELATIVE0/block2860496281880890121.tmp - FAILED!
  test0[1](org.apache.hadoop.fs.TestLocalDirAllocator): Checking for /data/md0/hadoop-common/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE1 in /data/md0/hadoop-common/hadoop-common-project/hadoop-common/build/test/temp/ABSOLUTE0/block7540717865042594902.tmp - FAILED!
  test0[2](org.apache.hadoop.fs.TestLocalDirAllocator): Checking for file:/data/md0/hadoop-common/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED1 in /data/md0/hadoop-common/hadoop-common-project/hadoop-common/build/test/temp/QUALIFIED0/block591739547204821805.tmp - FAILED!

The recently added {{testRemoveContext()}} (MAPREDUCE-4379) does not clean up after itself, so if it runs before test0 (due to undefined test ordering on JDK7), test0 fails. This can be fixed by wrapping it with {code}try { ... } finally { rmBufferDirs(); }{code}"
HADOOP-8691,"FsShell can print ""Found xxx items"" unnecessarily often","The ""Found xxx items"" header that is printed with a file listing will often appear multiple times in not-so-helpful ways in light of globbing.  For example:

{noformat}
$ hadoop fs -ls 'teradata/*'  
Found 1 items
-rw-r--r--   1 someuser somegroup          0 2012-08-06 16:55 teradata/_SUCCESS
Found 1 items
-rw-r--r--   1 someuser somegroup       5000 2012-08-06 16:55 teradata/part-m-00000
Found 1 items
-rw-r--r--   1 someuser somegroup       5000 2012-08-06 16:55 teradata/part-m-00001
{noformat}

Seems like it should just print ""Found 3 items"" once at the top, or maybe not even print a header at all."
HADOOP-8689,Make trash a server side configuration option,Per ATM's suggestion in HADOOP-8598 for v2 let's make {{fs.trash.interval}} configured server side. If it is not configured server side then the client side configuration is used. The {{fs.trash.checkpoint.interval}} option is already server side as the emptier runs in the NameNode. Clients may manually run an emptier via hadoop org.apache.hadoop.fs.Trash but it's OK if it uses a separate interval. 
HADOOP-8687,Upgrade log4j to 1.2.17,"Let's bump log4j from 1.2.15 to version 1.2.17. It and 16 are maintenance releases with good fixes and also remove some jar dependencies (javamail, jmx, jms, though we're already excluding them).

http://logging.apache.org/log4j/1.2/changes-report.html#a1.2.17"
HADOOP-8686,Fix warnings in native code,"Fix warnings about const-correctness, improper conversion between pointers of different sizes, implicit declaration of sync_file_range, variables being used with uninitialized values, and so forth in the hadoop-common native code."
HADOOP-8684,Deadlock between WritableComparator and WritableComparable,"Classes implementing WriableComparable in Hadoop call the method WritableComparator.define() in their static initializers. This means, the classes call the method define() while thier class loading, under locking their class objects. And, the method WritableComparator.define() locks the WritableComaprator class object.

On the other hand, WritableComparator.get() also locks the WritableComparator class object, and the method may create instances of the targeted comparable class, involving loading the targeted comparable class if any. This means, the method might try to lock the targeted comparable class object under locking the WritableComparator class object.

There are reversed orders of locking objects, and you might fall in deadlock."
HADOOP-8660,TestPseudoAuthenticator failing with NPE,"This test started failing recently, on top of trunk:

testAuthenticationAnonymousAllowed(org.apache.hadoop.security.authentication.client.TestPseudoAuthenticator)  Time elapsed: 0.241 sec  <<< ERROR!
java.lang.NullPointerException
        at org.apache.hadoop.security.authentication.client.PseudoAuthenticator.authenticate(PseudoAuthenticator.java:75)
        at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:232)
        at org.apache.hadoop.security.authentication.client.AuthenticatorTestCase._testAuthentication(AuthenticatorTestCase.java:127)
        at org.apache.hadoop.security.authentication.client.TestPseudoAuthenticator.testAuthenticationAnonymousAllowed(TestPseudoAuthenticator.java:65)
"
HADOOP-8659,Native libraries must build with soft-float ABI for Oracle JVM on ARM,"There was recently an ABI (application binary interface) change in most Linux distributions for modern ARM processors (ARMv7). Historically, hardware floating-point (FP) support was optional/vendor-specific for ARM processors, so for software compatibility, the default ABI required that processors with FP units copy FP arguments into integer registers (or memory) when calling a shared library function. Now that hardware floating-point has been standardized for some time, Linux distributions such as Ubuntu 12.04 have changed the default ABI to leave FP arguments in FP registers, since this can significantly improve performance for FP libraries.

Unfortunately, Oracle has not yet released a JVM (as of 7u4) that supports the new ABI, presumably since this involves some non-trivial changes to components like JNI. While the soft-float JVM can run on systems with multi-arch support (currently Debian/Ubuntu) using compatibility libraries, this configuration requires that any third-party JNI libraries also be compiled using the soft-float ABI. Since hard-float systems default to compiling for hard-float, an extra argument to GCC (and installation of a compatibility library) is required to build soft-float Hadoop native libraries that work with the Oracle JVM.

Note that OpenJDK on hard-float systems does use the new ABI, and expects JNI libraries to use it as well. Therefore the fix for this issue requires detecting the float ABI of the current JVM."
HADOOP-8656,backport forced daemon shutdown of HADOOP-8353 into branch-1,the init.d service shutdown code doesn't work if the daemon is hung -backporting the portion of HADOOP-8353 that edits bin/hadoop-daemon.sh corrects this
HADOOP-8655,"In TextInputFormat, while specifying textinputformat.record.delimiter the character/character sequences in data file similar to starting character/starting character sequence in delimiter were found missing in certain cases in the Map Output","Set textinputformat.record.delimiter as ""</entity>""

Suppose the input is a text file with the following content
<entity><id>1</id><name>User1</name></entity><entity><id>2</id><name>User2</name></entity><entity><id>3</id><name>User3</name></entity><entity><id>4</id><name>User4</name></entity><entity><id>5</id><name>User5</name></entity>

Mapper was expected to get value as 

Value 1 - <entity><id>1</id><name>User1</name>
Value 2 - <entity><id>2</id><name>User2</name>
Value 3 - <entity><id>3</id><name>User3</name>
Value 4 - <entity><id>4</id><name>User4</name>
Value 5 - <entity><id>5</id><name>User5</name>

According to this bug Mapper gets value

Value 1 - entity><id>1</id><name>User1</name>
Value 2 - <entity>id>2</id><name>User2</name>
Value 3 - <entity><id>3id><name>User3</name>
Value 4 - <entity><id>4</id><name>User4name>
Value 5 - <entity><id>5</id><name>User5</name>

The pattern shown above need not occur for value 1,2,3 necessarily. The bug occurs at some random positions in the map input.
 "
HADOOP-8654,TextInputFormat delimiter  bug:- Input Text portion ends with & Delimiter starts with same char/char sequence,"TextInputFormat delimiter  bug scenario , a character sequence of the input text,  in which the first character matches with the first character of delimiter, and the remaining input text character sequence  matches with the entire delimiter character sequence from the  starting position of the delimiter.

eg   delimiter =""record"";
and Text ="" record 1:- name = Gelesh e mail = gelesh.hadoop@gmail.com Location Bangalore record 2: name = sdf  ..  location =Bangalorrecord 3: name .... "" 

Here string ""=Bangalorrecord 3: "" satisfy two conditions 
1) contains the delimiter ""record""
2) The character / character sequence immediately before the delimiter (ie ' r ') matches with first character (or character sequence ) of delimiter.  (ie ""=Bangalor"" ends with and Delimiter starts with same character/char sequence 'r' ),

Here the delimiter is not encountered by the program resulting in improper value text in map that contains the delimiter   "
HADOOP-8648,libhadoop:  native CRC32 validation crashes when io.bytes.per.checksum=1,"The native CRC32 code, found in {{pipelined_crc32c}}, crashes when chunksize is set to 1.

{code}
12:27:14,886  INFO NativeCodeLoader:50 - Loaded the native-hadoop library
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fa00ee5a340, pid=24100, tid=140326058854144
#
# JRE version: 6.0_29-b11
# Java VM: Java HotSpot(TM) 64-Bit Server VM (20.4-b02 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libhadoop.so.1.0.0+0x8340]  pipelined_crc32c+0xa0
#
# An error report file with more information is saved as:
# /h/hs_err_pid24100.log
#
# If you would like to submit a bug report, please visit:
#   http://java.sun.com/webapps/bugreport/crash.jsp
#
Aborted
{code}

The Java CRC code works fine in this case.

Choosing blocksize=1 is a __very__ odd choice.  It means that we're storing a 4-byte checksum for every byte. 
{code}
-rw-r--r--  1 cmccabe users  49398 Aug  3 11:33 blk_4702510289566780538
-rw-r--r--  1 cmccabe users 197599 Aug  3 11:33 blk_4702510289566780538_1199.meta
{code}

However, obviously crashing is never the right thing to do."
HADOOP-8644,AuthenticatedURL should be able to use SSLFactory,This is required to enable the use of HTTPS with SPNEGO using Hadoop configured keystores. This is required by HADOOP-8581.
HADOOP-8642,Document that io.native.lib.available only controls native bz2 and zlib compression codecs,"Per core-default.xml {{io.native.lib.available}} indicates ""Should native hadoop libraries, if present, be used"" however it looks like it only affects bzip2 and zlib. Even if we set {{io.native.lib.available}} to false, native libraries are loaded and the libraries other than bzip2 and zlib are actually used. We should document that."
HADOOP-8637,FilterFileSystem#setWriteChecksum is broken,{{FilterFileSystem#setWriteChecksum}} is being passed through as {{fs.setVERIFYChecksum}}.  Example of impact is checksums cannot be disabled for LFS if a filter fs (like {{ChRootedFileSystem}}) is applied.
HADOOP-8635,Cannot cancel paths registered deleteOnExit,"{{FileSystem#deleteOnExit}} does not have a symmetric method to unregister files.  Since it's used to register temporary files during a copy operation, this can lead to a lot of unnecessary rpc operations for files successfully copied when the {{FileSystem}} is closed."
HADOOP-8634,Ensure FileSystem#close doesn't squawk for deleteOnExit paths,{{FileSystem#deleteOnExit}} doesn't check if the path exists before attempting to delete.  Errors may cause unnecessary INFO log squawks.
HADOOP-8633,Interrupted FsShell copies may leave tmp files,"Interrupting a copy, ex. via SIGINT, may cause tmp files to not be removed.  If the user is copying large files then the remnants will eat into the user's quota."
HADOOP-8632,Configuration leaking class-loaders,"The newly introduced CACHE_CLASSES leaks class loaders causing associated classes to not be reclaimed.

One solution is to remove the cache itself since each class loader implementation caches the classes it loads automatically and preventing an exception from being raised is just a micro-optimization that, as one can tell, causes bugs instead of improving anything.
In fact, I would argue in a highly-concurrent environment, the weakhashmap synchronization/lookup probably costs more then creating the exception itself.

Another is to prevent the leak from occurring, by inserting the loadedclass into the WeakHashMap wrapped in a WeakReference. Otherwise the class has a strong reference to its classloader (the key) meaning neither gets GC'ed.
And since the cache_class is static, even if the originating Configuration instance gets GC'ed, its classloader won't."
HADOOP-8627,FS deleteOnExit may delete the wrong path,"{{FilterFileSystem}} is incorrectly delegating {{deleteOnExit}} to the raw underlying fs.  This is wrong, because each fs instance is intended to maintain its own pool of temp files.  Worse yet, this means registering a file via {{ChRootedFileSystem#deleteOnExit}} will delete the file w/o the chroot path prepended!"
HADOOP-8626,Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user,(&amp;(objectClass=user)(sAMAccountName={0}) should have a trailing parenthesis at the end
HADOOP-8624,ProtobufRpcEngine should log all RPCs if TRACE logging is enabled,"Since all RPC requests/responses are now ProtoBufs, it's easy to add a TRACE level logging output for ProtobufRpcEngine that actually shows the full content of all calls. This is very handy especially when writing/debugging unit tests, but might also be useful to enable at runtime for short periods of time to debug certain production issues."
HADOOP-8623,hadoop jar command should respect HADOOP_OPTS,The jar command to the hadoop script should use any set HADOOP_OPTS and HADOOP_CLIENT_OPTS environment variables like all the other commands.
HADOOP-8620,Add -Drequire.fuse and -Drequire.snappy,"We have some optional build components which don't get built if they are not installed on the build machine.  One of those components is fuse_dfs.  Another is the snappy support in libhadoop.so.

Unfortunately, since these components are silently ignored if they are not present, it's easy to unintentionally create an incomplete build.

We should add two flags, -Drequire.fuse and -Drequire.snappy, that do exactly what the names suggest.  This will make the build more repeatable.  Those who want a complete build can specify these system properties to maven.  If the build cannot be created as requested, it will be a hard error."
HADOOP-8617,backport pure Java CRC32 calculator changes to branch-1,"Multiple efforts have been made gradually to improve the CRC performance in Hadoop. This JIRA is to back port these changes to branch-1, which include HADOOP-6166, HADOOP-6148, HADOOP-7333.

The related HDFS and MAPREDUCE patches are uploaded to their original JIRAs HDFS-496 and MAPREDUCE-782."
HADOOP-8616,ViewFS configuration requires a trailing slash,"If the viewfs config doesn't have a trailing slash commands like the following fail:

{noformat}
bash-3.2$ hadoop fs -ls
-ls: Can not create a Path from an empty string
Usage: hadoop fs [generic options] -ls [-d] [-h] [-R] [<path> ...]
{noformat}

We hit this problem with the following configuration because ""hdfs://ha-nn-uri"" does not have a trailing ""/"".

{noformat}
  <property>
  <name>fs.viewfs.mounttable.foo.link./nameservices/ha-nn-uri</name>
  <value>hdfs://ha-nn-uri</value>
  </property>
{noformat}"
HADOOP-8614,IOUtils#skipFully hangs forever on EOF,"IOUtils#skipFully contains this code:

{code}
  public static void skipFully(InputStream in, long len) throws IOException {
    while (len > 0) {
      long ret = in.skip(len);
      if (ret < 0) {
        throw new IOException( ""Premature EOF from inputStream"");
      }
      len -= ret;
    }
  }
{code}

The Java documentation is silent about what exactly skip is supposed to do in the event of EOF.  However, I looked at both InputStream#skip and ByteArrayInputStream#skip, and they both simply return 0 on EOF (no exception).  So it seems safe to assume that this is the standard Java way of doing things in an InputStream.

Currently IOUtils#skipFully will loop forever if you ask it to skip past EOF!"
HADOOP-8613,AbstractDelegationTokenIdentifier#getUser() should set token auth type,"{{AbstractDelegationTokenIdentifier#getUser()}} returns the UGI associated with a token.  The UGI's auth type will either be SIMPLE for non-proxy tokens, or PROXY (effective user) and SIMPLE (real user).  Instead of SIMPLE, it needs to be TOKEN."
HADOOP-8612,Backport HADOOP-8599 to branch-1 (Non empty response when read beyond eof),"When FileSystem.getFileBlockLocations(file,start,len) is called with ""start"" argument equal to the file size, the response is not empty. See HADOOP-8599 for details and tiny patch."
HADOOP-8611,Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails,"When the JNI-based users-group mapping is enabled, the process/command will fail if the native library, libhadoop.so, cannot be found. This mostly happens at client-side where users may use hadoop programatically. Instead of failing, falling back to the shell-based implementation will be desirable. Depending on how cluster is configured, use of the native netgroup mapping cannot be subsituted by the shell-based default. For this reason, this behavior must be configurable with the default being ""disabled""."
HADOOP-8609,IPC server logs a useless message when shutting down socket,"I occasionally see this WARN message out of the NameNode:
{code}
12/07/19 10:37:37 WARN ipc.Server: Ignoring socket shutdown exception
{code}
with no further details. This message isn't useful - it either needs to have more contextual information (eg what client it's talking about and the exception ignored), or should be reduced to debug level."
HADOOP-8608,Add Configuration API for parsing time durations,"Hadoop has a lot of configurations which specify durations or intervals of time. Unfortunately these different configurations have little consistency in units - eg some are in milliseconds, some in seconds, and some in minutes. This makes it difficult for users to configure, since they have to always refer back to docs to remember the unit for each property.

The proposed solution is to add an API like {{Configuration.getTimeDuration}} which allows the user to specify the units with a postfix. For example, ""10ms"", ""10s"", ""10m"", ""10h"", or even ""10d"". For backwards-compatibility, if the user does not specify a unit, the API can specify the default unit, and warn the user that they should specify an explicit unit instead."
HADOOP-8606,FileSystem.get may return the wrong filesystem,"{{FileSystem.get(URI, conf)}} will return the default fs if the scheme is null, regardless of whether the authority is null too.  This causes URIs of ""//authority/path"" to _always_ refer to ""/path"" on the default fs.  To the user, this appears to ""work"" if the authority in the null-scheme URI matches the authority of the default fs.  When the authorities don't match, the user is very surprised that the default fs is used."
HADOOP-8599,Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file ,"When FileSystem.getFileBlockLocations(file,start,len) is called with ""start"" argument equal to the file size, the response is not empty. There is a test TestGetFileBlockLocations.testGetFileBlockLocations2 which uses randomly generated ""start"" and ""len"" arguments when calling FileSystem.getFileBlockLocations and the test fails randomly (when the generated start value equals to the file size)."
HADOOP-8597,FsShell's Text command should be able to read avro data files,"Similar to SequenceFiles are Apache Avro's DataFiles. Since these are getting popular as a data format, perhaps it would be useful if {{fs -text}} were to add some support for reading it, like it reads SequenceFiles. Should be easy since Avro is already a dependency and provides the required classes.

Of discussion is the output we ought to emit. Avro DataFiles aren't simple as text, nor have they the singular Key-Value pair structure of SequenceFiles. They usually contain a set of fields defined as a record, and the usual text emit, as available from avro-tools via http://avro.apache.org/docs/current/api/java/org/apache/avro/tool/DataFileReadTool.html, is in proper JSON format.

I think we should use the JSON format as the output, rather than a delimited form, for there are many complex structures in Avro and JSON is the easiest and least-work-to-do way to display it (Avro supports json dumping by itself)."
HADOOP-8589,ViewFs tests fail when tests and home dirs are nested,"TestFSMainOperationsLocalFileSystem fails in case when the test root directory is under the user's home directory, and the user's home dir is deeper than 2 levels from /. This happens with the default 1-node installation of Jenkins. 

This is the failure log:

{code}
org.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here
	at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)
	at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)
	at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2128)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)
	at org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)
	at org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)

...

Standard Output
2012-07-11 22:07:20,239 INFO  mortbay.log (Slf4jLog.java:info(67)) - Home dir base /var/lib
{code}

The reason for the failure is that the code tries to mount links for both ""/var"" and ""/var/lib"", and it fails for the 2nd one as the ""/var"" is mounted already.

The fix was provided in HADOOP-8036 but later it was reverted in HADOOP-8129."
HADOOP-8587,HarFileSystem access of harMetaCache isn't threadsafe,HarFileSystem's use of the static harMetaCache map is not threadsafe. Credit to Todd for pointing this out.
HADOOP-8586,Fixup a bunch of SPNEGO misspellings,"SPNEGO is misspelled as ""SPENGO"" a bunch of places."
HADOOP-8585,Fix initialization circularity between UserGroupInformation and HadoopConfiguration,"Fix findbugs warning about initialization circularity between UserGroupInformation and UserGroupInformation#HadoopConfiguration.

From the findbugs text:
{code}
Initialization circularity between org.apache.hadoop.security.UserGroupInformation and org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration
	

Bug type IC_INIT_CIRCULARITY (click for details)
In class org.apache.hadoop.security.UserGroupInformation
In class org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration
At UserGroupInformation.java:[lines 76-1395]
{code}"
HADOOP-8581,add support for HTTPS to the web UIs,"HDFS/MR web UIs don't work over HTTPS, there are places where 'http://' is hardcoded."
HADOOP-8580,ant compile-native fails with automake version 1.11.3,"The following:

{code}
ant -d -v -DskipTests -Dcompile.native=true clean compile-native
{code}

works with GNU automake version 1.11.1, but fails with automake version 1.11.3. 

Relevant lines of failure seem to be these:

{code}
[exec] make[1]: Leaving directory `/tmp/hadoop-common/build/native/Linux-amd64-64'
     [exec] Current OS is Linux
     [exec] Executing 'sh' with arguments:
     [exec] '/tmp/hadoop-common/build/native/Linux-amd64-64/libtool'
     [exec] '--mode=install'
     [exec] 'cp'
     [exec] '/tmp/hadoop-common/build/native/Linux-amd64-64/libhadoop.la'
     [exec] '/tmp/hadoop-common/build/native/Linux-amd64-64/lib'
     [exec] 
     [exec] The ' characters around the executable and arguments are
     [exec] not part of the command.
     [exec] /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: 3212: /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: install_prog+=cp: not found
     [exec] /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: 3232: /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: files+= /tmp/hadoop-common/build/native/Linux-amd64-64/libhadoop.la: not found
     [exec] libtool: install: you must specify an install program
     [exec] libtool: install: Try `libtool --help --mode=install' for more information.
  [antcall] Exiting /tmp/hadoop-common/build.xml.

BUILD FAILED
{code}


Created transcript showing entire output of the above ant command for both automake 1.11.1 (successful) versus 1.11.3 (failed) here:

https://gist.github.com/3078988"
HADOOP-8573,Configuration tries to read from an inputstream resource multiple times. ,"If someone calls Configuration.addResource(InputStream) and then reloadConfiguration is called for any reason, Configruation will try to reread the contents of the InputStream, after it has already closed it.

This never showed up in 1.0 because the framework itself does not call addResource with an InputStream, and typically by the time user code starts running that might call this, all of the default and site resources have already been loaded.

In 0.23 mapreduce is now a client library, and mapred-site.xml and mapred-default.xml are loaded much later in the process."
HADOOP-8569,CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE,"In the native code, we should define _GNU_SOURCE and _LARGEFILE_SOURCE so that all of the functions on Linux are available.

_LARGEFILE enables fseeko and ftello; _GNU_SOURCE enables a variety of Linux-specific functions from glibc, including sync_file_range."
HADOOP-8567,Port conf servlet to dump running configuration  to branch 1.x,"HADOOP-6408 provide conf servlet that can dump running configuration which great helps admin to trouble shooting the configuration issue. However, that patch works on branch after 0.21 only and should be backport to branch 1.x."
HADOOP-8566,AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays),the accept() method should consider the case where the class getPackage() returns NULL.
HADOOP-8563,don't package hadoop-pipes examples/bin,Let's not package hadoop-pipes examples/bin
HADOOP-8562,Enhancements to support Hadoop on Windows Server and Windows Azure environments,This JIRA tracks the work that needs to be done on trunk to enable Hadoop to run on Windows Server and Azure environments. This incorporates porting relevant work from the similar effort on branch 1 tracked via HADOOP-8079.
HADOOP-8561,Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes,"To solve the problem for an authenticated user to type hadoop shell commands in a web console, we can introduce an HADOOP_PROXY_USER environment variable to allow proper impersonation in the child hadoop client processes."
HADOOP-8552,Conflict: Same security.log.file for multiple users. ,"In log4j.properties, hadoop.security.log.file is set to SecurityAuth.audit. In the presence of multiple users, this can lead to a potential conflict.

Adding username to the log file would avoid this scenario."
HADOOP-8551,fs -mkdir creates parent directories without the -p option,hadoop fs -mkdir foo/bar will work even if bar is not present.  It should only work if -p is given and foo is not present.
HADOOP-8550,hadoop fs -touchz automatically created parent directories,"Recently many of the fsShell commands were updated to be more POSIX compliant.  touchz appears to have been missed, or has regressed.  If it has regressed then the target version should be 0.23.3."
HADOOP-8547,Package hadoop-pipes examples/bin directory (again),"It looks like since MAPREDUCE-4267, we're no longer exporting the hadooppipes examples/bin directory to hadoop-dist as part of a ""mvn package"" build.  This seems unintentional, so we should export those binaries again."
HADOOP-8545,Filesystem Implementation for OpenStack Swift,",Add a filesystem implementation for OpenStack Swift object store, similar to the one which exists today for S3."
HADOOP-8543,Invalid pom.xml files on 0.23 branch,This is backport of HADOOP-8268 to 0.23 branch. It fixes invalid pom.xml which allows them to be uploaded into artifactory maven repository management and adds schema declarations which allows to use XML validating tools.
HADOOP-8541,Better high-percentile latency metrics,"Based on discussion in HBASE-6261 and with some HDFS devs, I'd like to make better high-percentile latency metrics a part of hadoop-common.

I've already got a working implementation of [1], an efficient algorithm for estimating quantiles on a stream of values. It allows you to specify arbitrary quantiles to track (e.g. 50th, 75th, 90th, 95th, 99th), along with tight error bounds. This estimator can be snapshotted and reset periodically to get a feel for how these percentiles are changing over time.

I propose creating a new MutableQuantiles class that does this. [1] isn't completely without overhead (~1MB memory for reasonably sized windows), which is why I hesitate to add it to the existing MutableStat class.

[1] Cormode, Korn, Muthukrishnan, and Srivastava. ""Effective Computation of Biased Quantiles over Data Streams"" in ICDE 2005."
HADOOP-8538,CMake builds fail on ARM,"CMake native builds fail with this error:

cc1: error: unrecognized command line option '-m32'

-m32 is only defined by GCC for x86, PowerPC, and SPARC.

The following files specify -m32 when the JVM data model is 32-bit:

hadoop-common-project/hadoop-common/src/CMakeLists.txt
hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt
hadoop-tools/hadoop-pipes/src/CMakeLists.txt
hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/CMakeLists.txt

This is a partial regression of HDFS-1920."
HADOOP-8537,Two TFile tests failing recently,TestTFileJClassComparatorByteArrays and TestTFileByteArrays are failing in some recent patch builds (seems to have started in the middle of May). These tests previously failed in HADOOP-7111 - perhaps something regressed there?
HADOOP-8535,Cut hadoop build times in half (upgrade maven-compiler-plugin to 2.5.1),"starting with 2.4.0 maven-compiler-plugin started caching the class loader for the javac compiler. This patch upgrades the compiler to 2.5.1. On my box, build times are reduced from 5 minutes to 2 minutes. "
HADOOP-8533,Remove Parallel Call in IPC,"From what I know, I do not think anyone uses Parallel Call. I also think it is not tested very well."
HADOOP-8531,SequenceFile Writer can throw out a better error if a serializer or deserializer isn't available,"Currently, if the provided Key/Value class lacks a proper serializer in the loaded config for the SequenceFile.Writer, we get an NPE as the null return goes unchecked.

Hence we get:
{code}
java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1163)
	at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1079)
	at org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1331)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:271)
{code}

We can provide a better message + exception in such cases. This is slightly related to MAPREDUCE-2584."
HADOOP-8525,Provide Improved Traceability for Configuration,"Configuration provides basic traceability to see where a config setting came from, but once the configuration is written out that information is written to a comment in the XML and then lost the next time the configuration is read back in.  It would really be great to be able to store a complete history of where the config came from in the XML, so that it can then be retrieved later for debugging."
HADOOP-8524,Allow users to get source of a Configuration parameter,"When we load the various XMLs via the Configuration class, the source of the XML file (filename) is usually kept in the Configuration class but not exposed programmatically. It is presently exposed as comments such as ""Loaded from mapred-site.xml"" in the XML dump/serialization but can't be accessed otherwise (Via the Configuration API).

For debugging/etc. purposes, it may be useful to expose this safely (such as an API for ""where did this property come from?"" queries for a specific property, via an API."
HADOOP-8512,AuthenticatedURL should reset the Token when the server returns other than OK on authentication,"Currently the token is not being reset and if using AuthenticatedURL, it will keep sending the invalid token as Cookie. There is not security concern with this, the main inconvenience is the logging being generated on the server side."
HADOOP-8509,JarFinder duplicate entry: META-INF/MANIFEST.MF exception,"Calling JarFinder.getJar() throws an exception if user has a ""META-INF/MANIFEST.MF"" in the same directory of the .class specified.

Jarfinder.getJar() call createJar() that always create a new Manifest and then add all the files contained in the .class directory.

To reproduce the problem in a simple way you can create a SimpleClass.java in the same directory create a META-INF/MANIFEST.MF and then call JarFinder.getJar(SimpleClass.class)

{code}
>> Caused by: java.util.zip.ZipException: duplicate entry: META-INF/MANIFEST.MF
>> at java.util.zip.ZipOutputStream.putNextEntry(ZipOutputStream.java:175)
>> at java.util.jar.JarOutputStream.putNextEntry(JarOutputStream.java:92)
>> at org.apache.hadoop.util.JarFinder.zipDir(JarFinder.java:66)
>> at org.apache.hadoop.util.JarFinder.zipDir(JarFinder.java:62)
>> at org.apache.hadoop.util.JarFinder.zipDir(JarFinder.java:44)
>> at org.apache.hadoop.util.JarFinder.createJar(JarFinder.java:93)
>> at org.apache.hadoop.util.JarFinder.getJar(JarFinder.java:134)
{code}"
HADOOP-8507,Avoid OOM while deserializing DelegationTokenIdentifer,avoid OOM while deserializing DelegationTokenIdentifer
HADOOP-8501,Gridmix fails to compile on OpenJDK7u4,"[INFO] Compiling 36 source files to /tmp/hadoop-common/hadoop-tools/hadoop-gridmix/target/classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /tmp/hadoop-common/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java:[612,40] error: type argument ? extends T is not within bounds of type-variable E
[ERROR]
    E extends Enum<E> declared in class Enum
/tmp/hadoop-common/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java:[615,14] error: type argument ? extends T is not within bounds of type-variable E
[INFO] 2 errors
"
HADOOP-8499,Lower min.user.id to 500 for the tests,"On Linux platforms where user IDs start at 500 rather than 1000, the build currently is broken.  This includes CentOS, RHEL, Fedora, SuSE, and probably most other Linux platforms.  It does happen to work on Debian and Ubuntu, which explains why Jenkins hasn't caught it yet.

Other users will see something like this:

{code}
[INFO] Requested user cmccabe has id 500, which is below the minimum allowed 1000
[INFO] FAIL: test-container-executor
[INFO] ================================================
[INFO] 1 of 1 test failed
[INFO] Please report to mapreduce-dev@hadoop.apache.org
[INFO] ================================================
[INFO] make[1]: *** [check-TESTS] Error 1
[INFO] make[1]: Leaving directory `/home/cmccabe/hadoop4/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn
-server/hadoop-yarn-server-nodemanager/target/native/container-executor'
{code}

And then the build fails.  Since native unit tests are currently unskippable (HADOOP-8480) this makes the project unbuildable.

The easy solution to this is to relax the constraint for the unit test.  Since the unit test already writes its own configuration file, we just need to change it there.

In general, I believe that it would make sense to change this to 500 across the board.  I'm not aware of any Linuxes that create system users with IDs higher than or equal to 500.  System user IDs tend to be below 200.

However, if we do nothing else, we should at least fix the build by relaxing the constraint for unit tests."
HADOOP-8495,Update Netty to avoid leaking file descriptors during shuffle,Netty 3.2.3.Final has a known bug where writes to a closed channel do not have their futures invoked.  See [Netty-374|https://issues.jboss.org/browse/NETTY-374].  This can lead to file descriptor leaks during shuffle as noted in MAPREDUCE-4298.
HADOOP-8488,test-patch.sh gives +1 even if the native build fails.,It seems that Jenkins doesn't fail the build if the native part of the build doesn't succeed.  This should be fixed!
HADOOP-8485,"Don't hardcode ""Apache Hadoop 0.23"" in the docs","The docs currently hardcode the string ""Apache Hadoop 0.23"" and ""hadoop-0.20.205"" in the main page."
HADOOP-8484,Prevent Configuration getter methods that are passed a default value from throwing RuntimeException,"Configuration getter methods that are passed default values can throw RuntimeExceptions if the value provided is invalid (e.g. NumberFormatException).
In many cases such exception results in more serious consequences (failure to sart a service, see for example NodeManager DeletionService). This can be avoided by returning the default value and just printing a warning message."
HADOOP-8481,update BUILDING.txt to talk about cmake rather than autotools,update BUILDING.txt to talk about cmake rather than autotools
HADOOP-8480,The native build should honor -DskipTests,"Currently, the native build does not honor -DskipTests.  The native unit tests will be run even when you specify:

{code}
mvn compile -Pnative -DskipTests -X -e
{code}

This seems inconsistent; shouldn't we fix this to work like the other tests do?"
HADOOP-8470,Implementation of 4-layer subclass of NetworkTopology (NetworkTopologyWithNodeGroup),"To support the four-layer hierarchical topology shown in attached figure as a subclass of NetworkTopology, NetworkTopologyWithNodeGroup was developed along with unit tests. NetworkTopologyWithNodeGroup overriding the methods add, remove, and pseudoSortByDistance were the most relevant to support the four-layer topology. The method seudoSortByDistance selects the nodes to use for reading data and sorts the nodes in sequence of node-local, nodegroup-local, rack- local, rack–off. Another slightly change to seudoSortByDistance is to support cases of separation data node and node manager: if the reader cannot be found in NetworkTopology tree (formed by data nodes only), then it will try to sort according to reader's sibling node in the tree.
The distance calculation changes the weights from 0 (local), 2 (rack- local), 4 (rack-off) to: 0 (local), 2 (nodegroup-local), 4 (rack-local), 6 (rack-off).
The additional node group layer should be specified in the topology script or table mapping, e.g. input 10.1.1.1, output: /rack1/nodegroup1
A subclass on InnerNode, InnerNodeWithNodeGroup, was also needed to support NetworkTopologyWithNodeGroup."
HADOOP-8469,Make NetworkTopology class pluggable,"The class NetworkTopology is where the three-layer hierarchical topology is modeled in the current code base and is instantiated directly by the DatanodeManager and Balancer.
To support alternative topologies, changes were make the topology class pluggable, that is to support using a user specified topology class specified in the Hadoop configuration file core-defaul.xml. The user specified topology class is instantiated using reflection in the same manner as other customizable classes in Hadoop. If no use specified topology class is found, the fallback is to use the NetworkTopology to preserve current behavior. To make it possible to reuse code in NetworkTopology several minor changes were made to make the class more extensible. The NetworkTopology class is currently annotated with @InterfaceAudience.LimitedPrivate({""HDFS"", ""MapReduce""}) and @InterfaceStability.Unstable.
The proposed changes in NetworkTopology listed below
1. Some fields were changes from private to protected
2. Added some protected methods so that sub classes could override behavior
3. Added a new method,isNodeGroupAware,to NetworkTopology
4. The inner class InnerNode was made a package protected class to it would be easier to subclass"
HADOOP-8466,hadoop-client POM incorrectly excludes avro,"avro is used by Serializers initialization, thus it must be there"
HADOOP-8465,hadoop-auth should support ephemeral authentication,"Currently, once a client is authenticated the generated authentication-token (& cookie) are valid for a given (service configurable) lifespan.

Once the authentication-token (& cookie) is issued, the authentication logic will not be triggered until the authentication-token expires.

This behavior does not work well with delegation tokens expected behavior where delegation tokens can be canceled at any time.

Having ephemeral authentication (which is check on every request) would address this issue."
HADOOP-8463,hadoop.security.auth_to_local needs a key definition and doc ,"hadoop.security.auth_to_local should be defined in CommonConfigurationKeysPublic.java, and update the uses of the raw string in common and hdfs with the key. 
It's definition in core-site.xml should also be updated with a description.
"
HADOOP-8462,Native-code implementation of bzip2 codec,"The bzip2 codec supplied with Hadoop is currently available only as a Java implementation.  A version that uses the system bzip2 library can provide improved performance and a better memory footprint.  This will also make it feasible to utilize alternative bzip2 libraries that may perform better for specific jobs.
"
HADOOP-8460,Document proper setting of HADOOP_PID_DIR and HADOOP_SECURE_DN_PID_DIR,"We should document that in a properly setup cluster HADOOP_PID_DIR and HADOOP_SECURE_DN_PID_DIR should not point to /tmp, but should point to a directory that normal users do not have access to."
HADOOP-8459,make SecurityUtil.setTokenServiceUseIp method public,Changing the scope of this method to be able to use it for HttpFS delegation token support HDFS-3113
HADOOP-8458,Add management hook to AuthenticationHandler to enable delegation token operations support,"Currently hadoop-auth AuthenticationHandler only authenticates a request.

While it can easily be extended to authenticate delegation tokens, it cannot handle the delegation token get/renew/cancel operations.

The motivation of this new feature is that the above delegation token operations should be handled by a security component (hadoop-auth) instead of a functional component (httpfs implementation). Ideally we should have a complete separation of concerns between delegation token management and FileSystem/MapReduce/YARN API, but we don't. This change is a step on that directory for HTTP based services (like HttpFS)."
HADOOP-8452,DN logs backtrace when running under jsvc and /jmx is loaded,"Running the data node under jsvc and requesting /jmx falls victim to a kernel bug http://marc.info/?l=linux-kernel&m=133788505209725&w=2 which results in EACCES when open()ing /proc/self/fd to attempt to count the open filedescriptors.

Hopefully someday we will have kernels with this bug fixed; in the meantime, the log spew which results is unpleasant:

12270 2012-05-15 21:04:41,683 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute OpenFileDescriptorCount of java.lang:type=OperatingSystem threw an exception
12271 javax.management.RuntimeErrorException: java.lang.InternalError: errno: 13 error: Unable to open directory /proc/self/fd
12272 
12273         at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:858)
12274         at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:869)
12275         at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:670)
12276         at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
12277         at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:314)
12278         at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:292)
12279         at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:192)
12280         at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
12281         at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
12282         at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
12283         at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
12284         at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:932)
12285         at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
12286         at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
12287         at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
12288         at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
12289         at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
12290         at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
12291         at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
12292         at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
12293         at org.mortbay.jetty.Server.handle(Server.java:326)
12294         at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
12295         at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
12296         at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
12297         at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
12298         at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
12299         at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
12300         at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
12301 Caused by: java.lang.InternalError: errno: 13 error: Unable to open directory /proc/self/fd
12302 
12303         at com.sun.management.UnixOperatingSystem.getOpenFileDescriptorCount(Native Method)
12304         at sun.reflect.GeneratedMethodAccessor42.invoke(Unknown Source)
12305         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
12306         at java.lang.reflect.Method.invoke(Method.java:597)
12307         at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:167)
12308         at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:96)
12309         at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:33)
12310         at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
12311         at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
12312         at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
12313         at javax.management.StandardMBean.getAttribute(StandardMBean.java:358)
12314         at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
12315         ... 25 more

we should catch the RuntimeErrorException and make it a debug()."
HADOOP-8450,Remove src/test/system,"runAs is a binary in the hadoop-common project.  It seems to allow you to run a hadoop daemon as another user.  However, it requires setuid privileges in order to do this.

* Do we still need this binary?  Who is using it and are there better solutions out there?  My understanding is that setuid binaries are something we generally try to avoid.

* If we do need it, can we add some documentation about it somewhere?

* Should this binary be in the src/test directory?  It doesn't seem to be testing anything.

Hopefully that covers everything..."
HADOOP-8449,hadoop fs -text fails with compressed sequence files with the codec file extension,"When the -text command is run on a file and the file ends in the default extension for a codec (e.g. snappy or gz), but is a compressed sequence file, the command will fail.

The issue is that it assumes that if it matches the extension, then it's plain compressed file. It might be more helpful to check if it's a sequence file first, and then check the file extension second."
HADOOP-8445,Token should not print the password in toString,This JIRA is for porting HADOOP-6622 to branch-1 since 6622 is already closed.
HADOOP-8444,Fix the tests FSMainOperationsBaseTest.java and F ileContextMainOperationsBaseTest.java to avoid potential test failure,"The above mentioned tests have a path filter to filter file names that contain either the string ""x"" or ""X"".  The filter contains the following if statement:
{code}
if(file.getName().contains(""x"") || file.toString().contains(""X""))
{code}
It should be corrected to:
{code}
if(file.getName().contains(""x"") || file.getName().contains(""X""))
{code}

Note that toString() returns the full path name.  These tests will fail when a directory name contains the string ""X"" and the base file name does not.  This was the case when we ran the tests on our system."
HADOOP-8440,HarFileSystem.decodeHarURI fails for URIs whose host contains numbers,"For example, HarFileSystem.decodeHarURI will fail for the following URI:

har://hdfs-127.0.0.1:51040/user"
HADOOP-8438,hadoop-validate-setup.sh refers to examples jar file which doesn't exist,"hadoop-validate-setup.sh is trying to find the file with the name hadoop-examples-\*.jar and it is failing to find because the examples jar is renamed to hadoop-mapreduce-examples-\*.jar.

{code:xml}
linux-rj72:/home/hadoop/hadoop-3.0.0-SNAPSHOT/sbin # ./hadoop-validate-setup.sh
find: `/usr/share/hadoop': No such file or directory
Did not find hadoop-examples-*.jar under '/home/hadoop-3.0.0-SNAPSHOT or '/usr/share/hadoop'
linux-rj72:/home/hadoop-3.0.0-SNAPSHOT/sbin #
{code}
"
HADOOP-8433,Don't set HADOOP_LOG_DIR in hadoop-env.sh,"It's better to comment the following in hadoop-env.sh

# Where log files are stored.  $HADOOP_HOME/logs by default.
export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}/$USER

Because of this logs are placing under root($user) and this getting called two times while starting process.
hence logs are placing at /root/root/"
HADOOP-8431,Running distcp wo args throws IllegalArgumentException,"Running distcp w/o args results in the following:

{noformat}
hadoop-3.0.0-SNAPSHOT $ ./bin/hadoop distcp
12/05/23 18:49:04 ERROR tools.DistCp: Invalid arguments: 
java.lang.IllegalArgumentException: Target path not specified
	at org.apache.hadoop.tools.OptionsParser.parse(OptionsParser.java:86)
	at org.apache.hadoop.tools.DistCp.run(DistCp.java:102)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.tools.DistCp.main(DistCp.java:368)
Invalid arguments: Target path not specified
{noformat}"
HADOOP-8430,Backport new FileSystem methods introduced by HADOOP-8014 to branch-1 ,"Per HADOOP-8422 let's backport the new FileSystem methods from HADOOP-8014 to branch-1 so users can transition over in Hadoop 1.x releases, which helps upstream projects like HBase work against federation (see HBASE-6067). "
HADOOP-8427,"Convert Forrest docs to APT, incremental",Some of the forrest docs content in src/docs/src/documentation/content/xdocs has not yet been converted to APT and moved to src/site/apt. Let's convert the forrest docs that haven't been converted yet to new APT content in hadoop-common/src/site/apt (and link the new content into hadoop-project/src/site/apt/index.apt.vm) and remove all forrest dependencies.
HADOOP-8423,MapFile.Reader.get() crashes jvm or throws EOFException on Snappy or LZO block-compressed data,"I am using Cloudera distribution cdh3u1.

When trying to check native codecs for better decompression
performance such as Snappy or LZO, I ran into issues with random
access using MapFile.Reader.get(key, value) method.
First call of MapFile.Reader.get() works but a second call fails.

Also  I am getting different exceptions depending on number of entries
in a map file.
With LzoCodec and 10 record file, jvm gets aborted.

At the same time the DefaultCodec works fine for all cases, as well as
record compression for the native codecs.

I created a simple test program (attached) that creates map files
locally with sizes of 10 and 100 records for three codecs: Default,
Snappy, and LZO.
(The test requires corresponding native library available)

The summary of problems are given below:

Map Size: 100
Compression: RECORD
==================
DefaultCodec:  OK
SnappyCodec: OK
LzoCodec: OK

Map Size: 10
Compression: RECORD
==================
DefaultCodec:  OK
SnappyCodec: OK
LzoCodec: OK

Map Size: 100
Compression: BLOCK
================
DefaultCodec:  OK

SnappyCodec: java.io.EOFException  at
org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:114)

LzoCodec: java.io.EOFException at
org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:114)

Map Size: 10
Compression: BLOCK
==================
DefaultCodec:  OK

SnappyCodec: java.lang.NoClassDefFoundError: Ljava/lang/InternalError
at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect(Native
Method)

LzoCodec:
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00002b068ffcbc00, pid=6385, tid=47304763508496
#
# JRE version: 6.0_21-b07
# Java VM: Java HotSpot(TM) 64-Bit Server VM (17.0-b17 mixed mode linux-amd64 )
# Problematic frame:
# C  [liblzo2.so.2+0x13c00]  lzo1x_decompress+0x1a0
#


"
HADOOP-8422,Deprecate FileSystem#getDefault* and getServerDefault methods that don't take a Path argument ,"The javadocs for FileSystem#getDefaultBlockSize and FileSystem#getDefaultReplication claim that ""The given path will be used to locate the actual filesystem"" however they both ignore the path."
HADOOP-8419,GzipCodec NPE upon reset with IBM JDK,"The GzipCodec will NPE upon reset after finish when the native zlib codec is not loaded. When the native zlib is loaded the codec creates a CompressorOutputStream that doesn't have the problem, otherwise, the GZipCodec uses GZIPOutputStream which is extended to provide the resetState method. Since IBM JDK 6 SR9 FP2 including the current JDK 6 SR10, GZIPOutputStream#finish will release the underlying deflater, which causes NPE upon reset. This seems to be an IBM JDK quirk as Sun JDK and OpenJDK doesn't have this issue."
HADOOP-8418,Fix UGI for IBM JDK running on Windows,The login module and user principal classes are different for 32 and 64-bit Windows in IBM J9 JDK 6 SR10. Hadoop 1.0.3 does not run on either because it uses the 32 bit login module and the 64-bit user principal class.
HADOOP-8417,HADOOP-6963 didn't update hadoop-core-pom-template.xml,"HADOOP-6963 introduced commons-io 2.1 in ivy.xml but forgot to update the hadoop-core-pom-template.xml.

This has caused map reduce jobs in downstream projects to fail with:
{code}
Caused by: java.lang.ClassNotFoundException: org.apache.commons.io.FileUtils
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 15 more
{code}
This caused a regression for 1.0.3 because downstream projects used to not directly depend on commons-io"
HADOOP-8415,getDouble() and setDouble() in org.apache.hadoop.conf.Configuration,"In the org.apache.hadoop.conf.Configuration class, methods exist to set Integers, Longs, Booleans, Floats and Strings, but methods for Doubles are absent. Are they not there for a reason or should they be added? In the latter case, the attached patch contains the missing functions."
HADOOP-8408,MR doesn't work with a non-default ViewFS mount table and security enabled,"With security enabled, if one sets up a ViewFS mount table using the default mount table name, everything works as expected. However, if you try to create a ViewFS mount table with a non-default name, you'll end up getting an error like the following (in this case ""vfs-cluster"" was the name of the mount table) when running an MR job:

{noformat}
java.lang.IllegalArgumentException: java.net.UnknownHostException: vfs-cluster
{noformat}"
HADOOP-8406,CompressionCodecFactory.CODEC_PROVIDERS iteration is thread-unsafe,"CompressionCodecFactory defines CODEC_PROVIDERS as:
{code}
  private static final ServiceLoader<CompressionCodec> CODEC_PROVIDERS =
    ServiceLoader.load(CompressionCodec.class);
{code}
but this is a lazy collection which is thread-unsafe to iterate. We either need to synchronize when we iterate over it, or we need to materialize it during class-loading time by copying to a non-lazy collection"
HADOOP-8403,bump up POMs version to 2.0.1-SNAPSHOT,
HADOOP-8400,"All commands warn ""Kerberos krb5 configuration not found"" when security is not enabled","Post HADOOP-8086 I get ""Kerberos krb5 configuration not found, setting default realm to empty"" warnings when running Hadoop commands even though I don't have kerb enabled."
HADOOP-8399,Remove JDK5 dependency from Hadoop 1.0+ line,"This issues has been fixed in Hadoop starting from 0.21 (see HDFS-1552).
I propose to make the same fix for 1.0 line and get rid of JDK5 dependency all together."
HADOOP-8398,Cleanup BlockLocation,"Minor BlockLocation cleanup. Remove dead imports, cleanup some incorrect comment and write a better javadoc."
HADOOP-8393,"hadoop-config.sh missing variable exports, causes Yarn jobs to fail with ClassNotFoundException MRAppMaster","If you start a pseudo distributed yarn using ""start-yarn.sh"" you need to specify exports for HADOOP_COMMON_HOME, HADOOP_HDFS_HOME, YARN_HOME, YARN_CONF_DIR, and HADOOP_MAPRED_HOME in hadoop-env.sh (or elsewhere), otherwise the spawned node manager will be missing these in it's environment. This is due to start-yarn using yarn-daemons. With this fix it's possible to start yarn (etc...) with only HADOOP_CONF_DIR specified in the environment. Took some time to track down this failure, so seems worthwhile to fix."
HADOOP-8390,TestFileSystemCanonicalization fails with JDK7,"Failed tests:
 testShortAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization):
expected:<myfs://host.a.b:123> but was:<myfs://host:123>
 testPartialAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization):
expected:<myfs://host.a.b:123> but was:<myfs://host.a:123>

Passes on same machine with JDK 1.6.0_32.
"
HADOOP-8388,Remove unused BlockLocation serialization,"BlockLocation has write, read methods but it's never serialized anywhere except in a unit test.  We should removed this dead code."
HADOOP-8386,hadoop script doesn't work if 'cd' prints to stdout (default behavior in Ubuntu),"if the 'hadoop' script is run as 'bin/hadoop' on a distro where the 'cd' command prints to stdout, the script will fail due to this line: 'bin=`cd ""$bin""; pwd`'

Workaround: execute from the bin/ directory as './hadoop'

Fix: change that line to 'bin=`cd ""$bin"" > /dev/null; pwd`'"
HADOOP-8373,Port RPC.getServerAddress to 0.23,"{{RPC.getServerAddress}} was introduced in trunk/2.0 as part of larger HA changes.  0.23 does not have HA, but this non-HA specific method is needed."
HADOOP-8372,normalizeHostName() in NetUtils is not working properly in resolving a hostname start with numeric character,"A valid host name can start with numeric value (You can refer RFC952, RFC1123 or http://www.zytrax.com/books/dns/apa/names.html), so it is possible in a production environment, user name their hadoop nodes as: 1hosta, 2hostb, etc. But normalizeHostName() will recognise this hostname as IP address and return directly rather than resolving the real IP address. These nodes will be failed to get correct network topology if topology script/TableMapping only contains their IPs (without hostname)."
HADOOP-8370,Native build failure: javah: class file for org.apache.hadoop.classification.InterfaceAudience not found,"[INFO] --- native-maven-plugin:1.0-alpha-7:javah (default) @ hadoop-common ---
[INFO] /bin/sh -c cd /build/hadoop-common/hadoop-common-project/hadoop-common && /usr/lib/jvm/jdk1.7.0_02/bin/javah -d /build/hadoop-common/hadoop-common-project/hadoop-common/target/native/javah -classpath <...> org.apache.hadoop.io.compress.zlib.ZlibDecompressor org.apache.hadoop.security.JniBasedUnixGroupsMapping org.apache.hadoop.io.nativeio.NativeIO org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping org.apache.hadoop.io.compress.snappy.SnappyCompressor org.apache.hadoop.io.compress.snappy.SnappyDecompressor org.apache.hadoop.io.compress.lz4.Lz4Compressor org.apache.hadoop.io.compress.lz4.Lz4Decompressor org.apache.hadoop.util.NativeCrc32
Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate': class file for org.apache.hadoop.classification.InterfaceAudience not found
Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'
Error: cannot access org.apache.hadoop.classification.InterfaceStability
  class file for org.apache.hadoop.classification.InterfaceStability not found

The fix for me was to changing the scope of hadoop-annotations from
""provided"" to ""compile"" in pom.xml:

   <dependency>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-annotations</artifactId>
     <scope>compile</scope>
   </dependency>

For some reason, it was the only dependency with scope ""provided""."
HADOOP-8368,Use CMake rather than autotools to build native code,"It would be good to use cmake rather than autotools to build the native (C/C++) code in Hadoop.

Rationale:
1. automake depends on shell scripts, which often have problems running on different operating systems.  It would be extremely difficult, and perhaps impossible, to use autotools under Windows.  Even if it were possible, it might require horrible workarounds like installing cygwin.  Even on Linux variants like Ubuntu 12.04, there are major build issues because /bin/sh is the Dash shell, rather than the Bash shell as it is in other Linux versions.  It is currently impossible to build the native code under Ubuntu 12.04 because of this problem.

CMake has robust cross-platform support, including Windows.  It does not use shell scripts.

2. automake error messages are very confusing.  For example, ""autoreconf: cannot empty /tmp/ar0.4849: Is a directory"" or ""Can't locate object method ""path"" via package ""Autom4te..."" are common error messages.  In order to even start debugging automake problems you need to learn shell, m4, sed, and the a bunch of other things.  With CMake, all you have to learn is the syntax of CMakeLists.txt, which is simple.

CMake can do all the stuff autotools can, such as making sure that required libraries are installed.  There is a Maven plugin for CMake as well.

3. Different versions of autotools can have very different behaviors.  For example, the version installed under openSUSE defaults to putting libraries in /usr/local/lib64, whereas the version shipped with Ubuntu 11.04 defaults to installing the same libraries under /usr/local/lib.  (This is why the FUSE build is currently broken when using OpenSUSE.)  This is another source of build failures and complexity.  If things go wrong, you will often get an error message which is incomprehensible to normal humans (see point #2).

CMake allows you to specify the minimum_required_version of CMake that a particular CMakeLists.txt will accept.  In addition, CMake maintains strict backwards compatibility between different versions.  This prevents build bugs due to version skew.

4. autoconf, automake, and libtool are large and rather slow.  This adds to build time.

For all these reasons, I think we should switch to CMake for compiling native (C/C++) code in Hadoop."
HADOOP-8367,Improve documentation of declaringClassProtocolName in rpc headers ,
HADOOP-8366,Use ProtoBuf for RpcResponseHeader,
HADOOP-8365,Add flag to disable durable sync,Per HADOOP-8230 there's a request for a flag to disable the sync code paths that dfs.support.append used to enable. The sync method itself will still be available and have a broken implementation as that was the behavior before HADOOP-8230. This config flag should default to false as the primary motivation for HADOOP-8230 is so HBase works out-of-the-box with Hadoop 1.1.
HADOOP-8362,Improve exception message when Configuration.set() is called with a null key or value,"Currently, calling Configuration.set(...) with a null value results in a NullPointerException within Properties.setProperty. We should check for null key/value and throw a better exception."
HADOOP-8361,Avoid out-of-memory problems when deserializing strings,"In HDFS, we want to be able to read the edit log without crashing on an OOM condition.  Unfortunately, we currently cannot do this, because there are no limits on the length of certain data types we pull from the edit log.  We often read strings without setting any upper limit on the length we're prepared to accept.

It's not that we don't have limits on strings-- for example, HDFS limits the maximum path length to 8000 UCS-2 characters.  Linux limits the maximum user name length to either 64 or 128 bytes, depending on what version you are running.  It's just that we're not exposing these limits to the deserialization functions that need to be aware of them."
HADOOP-8359,Clear up javadoc warnings in hadoop-common-project,"Javadocs added in HADOOP-8172 has introduced two new javadoc warnings. Should be easy to fix these (just missing #s for method refs).

{code}
[WARNING] Javadoc Warnings
[WARNING] /Users/harshchouraria/Work/code/apache/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java:334: warning - Tag @link: missing '#': ""addDeprecation(String key, String newKey)""
[WARNING] /Users/harshchouraria/Work/code/apache/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java:285: warning - Tag @link: missing '#': ""addDeprecation(String key, String newKey,
[WARNING] String customMessage)""
{code}"
HADOOP-8358,Config-related WARN for dfs.web.ugi can be avoided.,"{code}
2012-05-04 11:55:13,367 WARN org.apache.hadoop.http.lib.StaticUserWebFilter: dfs.web.ugi should not be used. Instead, use hadoop.http.staticuser.user.
{code}

Looks easy to fix, and we should avoid using old config params that we ourselves deprecated."
HADOOP-8356,FileSystem service loading mechanism should print the FileSystem impl it is failing to load,"If by mistake somebody adds a FileSystem implementation to the service definition that is not serviceloader friendly (it does not override the getScheme() method) or adds to the classpath an older one defined in the service definition, the exception thrown should print out the FileSystem class failing to load."
HADOOP-8355,SPNEGO filter throws/logs exception when authentication fails,"if the auth-token is NULL means the authenticator has not authenticated the request and it has already issue an UNAUTHORIZED response, there is no need to throw an exception and then immediately catch it and log it. The 'else throw' can be removed."
HADOOP-8353,hadoop-daemon.sh and yarn-daemon.sh can be misleading on stop,"The way that stop actions is implemented is a simple SIGTERM sent to the JVM. There's a time delay between when the action is called and when the process actually exists. This can be misleading to the callers of the *-daemon.sh scripts since they expect stop action to return when process is actually stopped.

I suggest we augment the stop action with a time-delay check for the process status and a SIGKILL once the delay has expired.

I understand that sending SIGKILL is a measure of last resort and is generally frowned upon among init.d script writers, but the excuse we have for Hadoop is that it is engineered to be a fault tolerant system and thus there's not danger of putting system into an incontinent state by a violent SIGKILL. Of course, the time delay will be long enough to make SIGKILL event a rare condition.

Finally, there's always an option of an exponential back-off type of solution if we decide that SIGKILL timeout is short."
HADOOP-8352,We should always generate a new configure script for the c++ code,"If you are compiling c++, you should always generate a configure script."
HADOOP-8350,Improve NetUtils.getInputStream to return a stream which has a tunable timeout,"Currently, NetUtils.getInputStream will set the timeout on the new stream based on the socket's configured timeout at the time of construction. After that, the timeout cannot be changed. This causes a problem for cases like HDFS-3357. One approach used in some places in the code is to construct new streams when the timeout has to be changed, but this can cause bugs given that the streams are often wrapped by BufferedInputStreams."
HADOOP-8349,ViewFS doesn't work when the root of a file system is mounted,Viewing files under a ViewFS mount which mounts the root of a file system shows trimmed paths. Trying to perform operations on files or directories under the root-mounted file system doesn't work. More info in the first comment of this JIRA.
HADOOP-8347,Hadoop Common logs misspell 'successful',"'successfull' is a misspelling of 'successful.'  Trivial patch attached.  The constants are private, and there doesn't seem to be any serialized form of these comments except in log files, so this shouldn't have compatibility issues."
HADOOP-8346,Changes to support Kerberos with non Sun JVM (HADOOP-6941) broke SPNEGO,"before HADOOP-6941 hadoop-auth testcases with Kerberos ON pass, *mvn test -PtestKerberos*

after HADOOP-6941 the tests fail with the error below.

Doing some IDE debugging I've found out that the changes in HADOOP-6941 are making the JVM Kerberos libraries to append an extra element to the kerberos principal of the server (on the client side when creating the token) so *HTTP/localhost* ends up being *HTTP/localhost/localhost*. Then, when contacting the KDC to get the granting ticket, the server principal is unknown.

{code}
testAuthenticationPost(org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator)  Time elapsed: 0.053 sec  <<< ERROR!
org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:236)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:142)
	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:217)
	at org.apache.hadoop.security.authentication.client.AuthenticatorTestCase._testAuthentication(AuthenticatorTestCase.java:124)
	at org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$2.call(TestKerberosAuthenticator.java:77)
	at org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$2.call(TestKerberosAuthenticator.java:74)
	at org.apache.hadoop.security.authentication.KerberosTestUtils$1.run(KerberosTestUtils.java:111)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.authentication.KerberosTestUtils.doAs(KerberosTestUtils.java:108)
	at org.apache.hadoop.security.authentication.KerberosTestUtils.doAsClient(KerberosTestUtils.java:124)
	at org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator.testAuthenticationPost(TestKerberosAuthenticator.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
Caused by: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)
	at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:663)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:230)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:215)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:191)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:191)
	... 36 more
Caused by: KrbException: Server not found in Kerberos database (7) - UNKNOWN_SERVER
	at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:64)
	at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:185)
	at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:294)
	at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:106)
	at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:575)
	at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:594)
	... 43 more
Caused by: KrbException: Identifier doesn't match expected value (906)
	at sun.security.krb5.internal.KDCRep.init(KDCRep.java:133)
	at sun.security.krb5.internal.TGSRep.init(TGSRep.java:58)
	at sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:53)
	at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:46)
{code}"
HADOOP-8343,Allow configuration of authorization for JmxJsonServlet and MetricsServlet,"When using authorization for the daemons' web server, it would be useful to specifically control the authorization requirements for accessing /jmx and /metrics.  Currently, they require administrative access.  This JIRA would propose that whether or not they are available to administrators only or to all users be controlled by ""hadoop.instrumentation.requires.administrator"" (or similar).  The default would be that administrator access is required."
HADOOP-8342,HDFS command fails with exception following merge of HADOOP-8325,"We are seeing most hdfs commands in our nightly acceptance tests fail with an exception as shown below. This started with a few hours of the merge of HADOOP-8325 on 4/30/2012

hdfs --config conf/hadoop/ dfs -ls dirname
ls: `dirname': No such file or directory
12/05/01 16:57:52 WARN util.ShutdownHookManager: ShutdownHook 'ClientFinalizer' failed, java.lang.IllegalStateException: Shutdown in progress, cannot remove a shutdownHook
java.lang.IllegalStateException: Shutdown in progress, cannot remove a shutdownHook
	at org.apache.hadoop.util.ShutdownHookManager.removeShutdownHook(ShutdownHookManager.java:166)
	at org.apache.hadoop.fs.FileSystem$Cache.remove(FileSystem.java:2202)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2231)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2251)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
"
HADOOP-8341,Fix or filter findbugs issues in hadoop-tools,Now that the precommit build can test hadoop-tools we need to fix or filter the many findbugs warnings that are popping up in there.
HADOOP-8340,SNAPSHOT build versions should compare as less than their eventual final release,"We recently added a utility function to compare two version strings, based on splitting on '.'s and comparing each component. However, it considers a version like 2.0.0-SNAPSHOT as being greater than 2.0.0. This isn't right, since SNAPSHOT builds come before the final release."
HADOOP-8338,Can't renew or cancel HDFS delegation tokens over secure RPC,The fetchdt tool is failing for secure deployments when given --renew or --cancel on tokens fetched using RPC. (The tokens fetched over HTTP can be renewed and canceled fine.)
HADOOP-8335,Improve Configuration's address handling,There's a {{Configuration#getSocketAddr}} but no symmetrical {{setSocketAddr}}.  An {{updateSocketAddr}} would also be very handy for yarn's updating of wildcard addresses in the config.
HADOOP-8334,HttpServer sometimes returns incorrect port,{{HttpServer}} is not always returning the correct listening port.
HADOOP-8330,TestSequenceFile.testCreateUsesFsArg() is broken,It seems HADOOP-8305 broke TestSequenceFile.testCreateUsesFsArg(). Fix the tests if the test is broken or the source.
HADOOP-8329,Build fails with Java 7,"I am seeing the following message running IBM Java 7 running branch-1.0 code.
compile:
[echo] contrib: gridmix
[javac] Compiling 31 source files to /home/hadoop/branch-1.0_0427/build/contrib/gridmix/classes
[javac] /home/hadoop/branch-1.0_0427/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java:396: error: type argument ? extends T is not within bounds of type-variable E
[javac] private <T> String getEnumValues(Enum<? extends T>[] e) {
[javac] ^
[javac] where T,E are type-variables:
[javac] T extends Object declared in method <T>getEnumValues(Enum<? extends T>[])
[javac] E extends Enum<E> declared in class Enum
[javac] /home/hadoop/branch-1.0_0427/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java:399: error: type argument ? extends T is not within bounds of type-variable E
[javac] for (Enum<? extends T> v : e) {
[javac] ^
[javac] where T,E are type-variables:
[javac] T extends Object declared in method <T>getEnumValues(Enum<? extends T>[])
[javac] E extends Enum<E> declared in class Enum
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 2 errors

BUILD FAILED
/home/hadoop/branch-1.0_0427/build.xml:703: The following error occurred while executing this line:
/home/hadoop/branch-1.0_0427/src/contrib/build.xml:30: The following error occurred while executing this line:
/home/hadoop/branch-1.0_0427/src/contrib/build-contrib.xml:185: Compile failed; see the compiler error output for details.
"
HADOOP-8328,Duplicate FileSystem Statistics object for 'file' scheme,"Because of a change in HADOOP-8013, there are duplicate Statistics objects in FileSystem's statistics table: one for LocalFileSystem and one for RawLocalFileSystem. This causes MapReduce local file system counters to be incorrect some of the time. "
HADOOP-8327,distcpv2 and distcpv1 jars should not coexist,"Distcp v2 (hadoop-tools/hadoop-distcp/...)    and Distcp v1 (hadoop-tools/hadoop-extras/...) are currently both built, and the resulting hadoop-distcp-x.jar and hadoop-extras-x.jar end up in the same class path directory.   This causes some undeterministic problems, where v1 is launched when v2 is intended, or even v2 is launched, but may later fail on various nodes because of mismatch with v1.

According to
http://docs.oracle.com/javase/6/docs/technotes/tools/windows/classpath.html (""Understanding class path wildcards"")

""The order in which the JAR files in a directory are enumerated in the expanded class path is not specified and may vary from platform to platform and even from moment to moment on the same machine.""

Suggest distcpv1 be deprecated at this point, possibly by discontinuing build of distcpv1.
"
HADOOP-8325,Add a ShutdownHookManager to be used by different components instead of the JVM shutdownhook,"FileSystem adds a JVM shutdown hook when a filesystem instance is cached.

MRAppMaster also uses a JVM shutdown hook, among other things, the MRAppMaster JVM shutdown hook is used to ensure state are written to HDFS.

This creates a race condition because each JVM shutdown hook is a separate thread and if there are multiple JVM shutdown hooks there is not assurance of order of execution, they could even run in parallel.


"
HADOOP-8323,Revert HADOOP-7940 and improve javadocs and test for Text.clear(),"Per [~jdonofrio]'s comments on HADOOP-7940, we should revert it as it has caused a performance regression (for scenarios where Text is reused, popular in MR).

The clear() works as intended, as the API also offers a current length API."
HADOOP-8317,Update maven-assembly-plugin to 2.3 - fix build on FreeBSD,"There is bug in hadoop-assembly plugin which makes builds fail on FreeBSD because its chmod do not understand nonstgandard linux parameters. Unless you do mvn clean before every build it fails with:

[INFO] --- maven-assembly-plugin:2.2.1:single (dist) @ hadoop-common ---
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'org.apache.ant:*:jar'
o  'jdiff:jdiff:jar'

[INFO] Copying files to /usr/local/jboss/.jenkins/jobs/Hadoop-0.23/workspace/hadoop-common-project/hadoop-common/target/hadoop-common-0.23.3-SNAPSHOT
[WARNING] -------------------------------
[WARNING] Standard error:
[WARNING] -------------------------------
[WARNING] 
[WARNING] -------------------------------
[WARNING] Standard output:
[WARNING] -------------------------------
[WARNING] chmod: /usr/local/jboss/.jenkins/jobs/Hadoop-0.23/workspace/hadoop-common-project/hadoop-common/target/hadoop-common-0.23.3-SNAPSHOT/share/hadoop/common/lib/hadoop-auth-0.23.3-SNAPSHOT.jar: Inappropriate file type or format

[WARNING] -------------------------------
mojoFailed org.apache.maven.plugins:maven-assembly-plugin:2.2.1(dist)
projectFailed org.apache.hadoop:hadoop-common:0.23.3-SNAPSHOT
sessionEnded"
HADOOP-8316,Audit logging should be disabled by default,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent).

Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages."
HADOOP-8315,Support SASL-authenticated ZooKeeper in ActiveStandbyElector,"Currently, if you try to use SASL-authenticated ZK with the ActiveStandbyElector, you run into a couple issues:
1) We hit ZOOKEEPER-1437 - we need to wait until we see SaslAuthenticated before we can make any requests
2) We currently throw a fatalError when we see the SaslAuthenticated callback on the connection watcher

We need to wait for ZK-1437 upstream, and then upgrade to the fixed version for #1. For #2 we just need to add a case there and ignore it."
HADOOP-8314,HttpServer#hasAdminAccess should return false if authorization is enabled but user is not authenticated,"If the user is not authenticated (request.getRemoteUser() returns NULL) or there is not authentication filter configured (thus returning also NULL), hasAdminAccess should return false. Note that a filter could allow anonymous access, thus the first case.
"
HADOOP-8310,FileContext#checkPath should handle URIs with no port,"AbstractFileSystem#checkPath is used to verify that a given path is for the same file system as represented by the AbstractFileSystem instance.

The original intent of the code was to allow for no port to be provided in the checked path, if the default port was being used by the AbstractFileSystem instance. However, before performing port handling, AFS#checkPath compares the full URI authorities for equality. Since the URI authority includes the port, the port handling code is never reached, and thus valid paths may be erroneously considered invalid."
HADOOP-8309,Pseudo & Kerberos AuthenticationHandler should use getType() to create token,"Currently they use the constant TYPE, this means that if AuthenticationHandler are subclassed a new type cannot be used.

Instead they should use the getType() method."
HADOOP-8305,distcp over viewfs is broken,This is similar to MAPREDUCE-4133. distcp over viewfs is broken because getDefaultReplication/BlockSize are being requested with no arguments.
HADOOP-8304,DNSToSwitchMapping should add interface to resolve individual host besides a list of host,DNSToSwitchMapping now has only one API to resolve a host list: public List<String> resolve(List<String> names). But the two major caller: RackResolver.resolve() and DatanodeManager.resolveNetworkLocation() are taking single host name but have to wrapper it to an single entry ArrayList. This is not necessary especially the host has been cached before.
HADOOP-8296,hadoop/yarn daemonlog usage wrong ,"$ yarn daemonlog

USAGES:
java org.apache.hadoop.log.LogLevel -getlevel <host:port> <name>
java org.apache.hadoop.log.LogLevel -setlevel <host:port> <name> <level>


The usage shouldn't print java org.apache.hadoop.log.LogLevel"
HADOOP-8294,IPC Connection becomes unusable even if server address was temporarilly unresolvable,"This is same as HADOOP-7428, but was observed on 1.x data nodes. This can happen more frequently after HADOOP-7472, which allows IPC Connection to re-resolve the name. HADOOP-7428 needs to be back-ported."
HADOOP-8293,The native library's Makefile.am doesn't include JNI path,"When compiling on centos 6, I get the following error when compiling the native library:

{code}
 [exec] /usr/bin/ld: cannot find -ljvm
{code}

The problem is simply that the Makefile.am libhadoop_la_LDFLAGS doesn't include AM_LDFLAGS."
HADOOP-8288,Remove references of mapred.child.ulimit etc. since they are not being used any more,"Courtesy Philip Su, we found that (mapred.child.ulimit, mapreduce.map.ulimit, mapreduce.reduce.ulimit) were not being used at all. The configuration exists but is never used. Its also mentioned in mapred-default.xml and templates/../mapred-site.xml . Also the method getUlimitMemoryCommand in Shell.java is now useless and can be removed."
HADOOP-8287,etc/hadoop is missing hadoop-env.sh,"The etc/hadoop directory in the tarball is missing hadoop-env.sh. It should be copied over like the other files in share/hadoop/common/templates/conf. Noticed templates/conf also contains mapred-site.xml and taskcontroller.cfg, we should remove those while we're at it."
HADOOP-8286,Simplify getting a socket address from conf,"{{NetUtils.createSocketAddr(addr, port, confKey}} will throw an exception with a descriptive message of a malformed conf value.  A corresponding {{conf#getSocketAddr}} would make it easier to use, and ensure that {{NetUtils}} is used to parse the address."
HADOOP-8285,Use ProtoBuf for RpcPayLoadHeader,
HADOOP-8283,Allow tests to control token service value,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.
HADOOP-8282,start-all.sh refers incorrectly start-dfs.sh existence for starting start-yarn.sh,
HADOOP-8280, Move VersionUtil/TestVersionUtil and GenericTestUtils from HDFS into Common.," We need to use VersionUtil in MAPREDUCE-4150. Moving VersionUtil/TestVersionUtil from HDFS into common will help this, especially since test-patch doesn't seem to deal well with cross-sub-project patches."
HADOOP-8278,Make sure components declare correct set of dependencies,"As mentioned by Scott Carey in https://issues.apache.org/jira/browse/MAPREDUCE-3378?focusedCommentId=13173437&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13173437, we need to make sure that components are declaring the correct set of dependencies. In current trunk there are errors of omission and commission (as reported by 'mvn dependency:analyze'):
* ""Used undeclared dependencies"" - these are dependencies that are being met transitively. They should be added explicitly as ""compile"" or ""provided"" scope.
* ""Unused declared dependencies"" - these are dependencies that are not needed for compilation, although they may be needed at runtime. They certainly should not be ""compile"" scope - they should either be removed or marked as ""runtime"" or ""test"" scope.
"
HADOOP-8275,Range check DelegationKey length ,"Harden serialization logic against malformed or malicious input.

Add range checking to readVInt, to detect overflows, underflows, and larger-than-expected values."
HADOOP-8270,hadoop-daemon.sh stop action should return 0 for an already stopped service ,"The following bit of code from hadoop-daemon.sh is not LSB compliant, since
according to http://refspecs.linuxbase.org/LSB_3.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html
a stop action on an already stopped service should return 0.

{noformat}
 (stop)

    if [ -f $pid ]; then
      if kill -0 `cat $pid` > /dev/null 2>&1; then
        echo stopping $command
        kill `cat $pid`
      else
        echo no $command to stop
        exit 1
      fi
    else
      echo no $command to stop
      exit 1
    fi
    ;;
{noformat}"
HADOOP-8269,Fix some javadoc warnings on branch-1,"There are some javadoc warnings on branch-1, let's fix them."
HADOOP-8268,A few pom.xml across Hadoop project may fail XML validation,In a few pom files there are embedded ant commands which contains '>' - redirection. This makes XML file invalid and this POM file can not be deployed into validating Maven repository managers such as Artifactory.
HADOOP-8264,Remove irritating double double quotes in front of hostname ,"The attached patch fixes display of destination port from """"hostname"" to ""hostname"""
HADOOP-8263,Stringification of IPC calls not useful,"Since the Protobufification of Hadoop, the log messages on IPC exceptions on the server side now read like:

12/04/09 16:04:06 INFO ipc.Server: IPC Server handler 9 on 8021, call org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWritable@7087e9bf from 127.0.0.1:47989: error: org.apache.hadoop.ipc.StandbyException: Operation category READ is not supported in state standby

The call should instead stringify the method name and the request protobuf (perhaps abbreviated if it is longer than a few hundred chars)"
HADOOP-8261,Har file system doesn't deal with FS URIs with a host but no port,"If you try to run an MR job with a Hadoop Archive as the input, but the URI you give it has no port specified (e.g. ""hdfs://simon"") the job will fail with an error like the following:

{noformat}
java.io.IOException: Incomplete HDFS URI, no host: hdfs://simon:-1/user/atm/input.har/input
{noformat}"
HADOOP-8254,Log message is same for two different conditions in BlockTokenSecretManager#checkAccess(),"{code}
if (!id.getBlockPoolId().equals(block.getBlockPoolId())) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't apply to block "" + block);
    }
    if (id.getBlockId() != block.getBlockId()) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't apply to block "" + block);
    }
{code}
"
HADOOP-8251,SecurityUtil.fetchServiceTicket broken after HADOOP-6941,"HADOOP-6941 replaced direct references to some classes with reflective access so as to support other JDKs. Unfortunately there was a mistake in the name of the Krb5Util class, which broke fetchServiceTicket. This manifests itself as the inability to run checkpoints or other krb5-SSL HTTP-based transfers:

java.lang.ClassNotFoundException: sun.security.jgss.krb5"
HADOOP-8249,invalid hadoop-auth cookies should trigger authentication if info is avail before returning HTTP 401,"WebHdfs gives out cookies. But when the client passes them back, it'd sometimes reject them and return a HTTP 401 instead. (""Sometimes"" as in after a restart.) The interesting thing is that if the client doesn't pass the cookie back, WebHdfs will be totally happy.

The correct behaviour should be to ignore the cookie if it looks invalid, and attempt to proceed with the request handling.

I haven't tried HttpFs to see whether it handles restart better.

Reproducing it with curl:
{noformat}
####################################################
## Initial curl. Storing cookie to file.
####################################################

[root@vbox2 ~]# curl -c /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'
HTTP/1.1 200 OK
Content-Type: application/json
Expires: Thu, 01-Jan-1970 00:00:00 GMT
Set-Cookie: hadoop.auth=""u=bcwalrus&p=bcwalrus&t=simple&e=1333614686366&s=z2w5xpFlufnnEoOHxVRiXqxwtqM="";Path=/
Content-Length: 597
Server: Jetty(6.1.26)

{""FileStatuses"":{""FileStatus"":[
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577906198,""owner"":""mapred"",""pathSuffix"":""tmp"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577511848,""owner"":""hdfs"",""pathSuffix"":""user"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333428745116,""owner"":""mapred"",""pathSuffix"":""var"",""permission"":""755"",""replication"":0,""type"":""DIRECTORY""}
]}}

####################################################
## Another curl. Using the cookie jar.
####################################################

[root@vbox2 ~]# curl -b /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 597
Server: Jetty(6.1.26)

{""FileStatuses"":{""FileStatus"":[
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577906198,""owner"":""mapred"",""pathSuffix"":""tmp"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577511848,""owner"":""hdfs"",""pathSuffix"":""user"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333428745116,""owner"":""mapred"",""pathSuffix"":""var"",""permission"":""755"",""replication"":0,""type"":""DIRECTORY""}
]}}

####################################################
## Restart NN.
####################################################

[root@vbox2 ~]# /etc/init.d/hadoop-hdfs-namenode restartStopping Hadoop namenode:                                  [  OK  ]
stopping namenode
Starting Hadoop namenode:                                  [  OK  ]
starting namenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-namenode-vbox2.out

####################################################
## Curl using cookie jar gives error.
####################################################

[root@vbox2 ~]# curl -b /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'
HTTP/1.1 401 org.apache.hadoop.security.authentication.util.SignerException: Invalid signature
Content-Type: text/html; charset=iso-8859-1
Set-Cookie: hadoop.auth=;Path=/;Expires=Thu, 01-Jan-1970 00:00:00 GMT
Cache-Control: must-revalidate,no-cache,no-store
Content-Length: 1520
Server: Jetty(6.1.26)

<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html; charset=ISO-8859-1""/>
<title>Error 401 org.apache.hadoop.security.authentication.util.SignerException: Invalid signature</title>
</head>
<body><h2>HTTP ERROR 401</h2>
<p>Problem accessing /webhdfs/v1/. Reason:
<pre>    org.apache.hadoop.security.authentication.util.SignerException: Invalid signature</pre></p><hr /><i><small>Powered by Jetty://</small></i><br/>                                                
...

####################################################
## Curl without cookie jar is ok.
####################################################

[root@vbox2 ~]# curl -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'
HTTP/1.1 200 OK
Content-Type: application/json
Expires: Thu, 01-Jan-1970 00:00:00 GMT
Set-Cookie: hadoop.auth=""u=bcwalrus&p=bcwalrus&t=simple&e=1333614995947&s=IXSvPIDbNrqmZryivGeoey6Kjwo="";Path=/
Content-Length: 597
Server: Jetty(6.1.26)

{""FileStatuses"":{""FileStatus"":[
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577906198,""owner"":""mapred"",""pathSuffix"":""tmp"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577511848,""owner"":""hdfs"",""pathSuffix"":""user"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},
{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333428745116,""owner"":""mapred"",""pathSuffix"":""var"",""permission"":""755"",""replication"":0,""type"":""DIRECTORY""}
]}}
{noformat}"
HADOOP-8244,Improve comments on ByteBufferReadable.read,"There are a couple of ways in which the comment on ByteBufferReadable.read can be improved. Since this is a public-facing API with potentially many implementations, it's worth taking the time to get it right.

* We should describe what can become of the byte buffer state in the case of an exception (is the limit changed? where's the position?). For the DFSInputStream implementation, position and limit are unchanged if there is an error, but I don't think that's the right think to mandate for all implementations. 
* We should mention explicitly that 0-byte reads are legitimate - this is particularly important in light of HDFS-3110 which detects support for direct reads by issuing a 0-byte read from libhdfs. "
HADOOP-8243,Security support broken in CLI (manual) failover controller,"Some recent refactoring accidentally caused the proxies in some places to get created with a default Configuration, instead of using the Configuration set up by the DFSHAAdmin tool. This causes the HAServiceProtocol to be missing the configuration which specifies the NN principle -- and thus breaks the CLI HAAdmin tool in secure setups."
HADOOP-8242,AbstractDelegationTokenIdentifier: add getter methods for owner and realuser,AbstractDelegationTokenIdentifier: add getter methods for owner and realuser.   This is needed for another change in HDFS.
HADOOP-8240,Allow users to specify a checksum type on create(),"Per discussion in HADOOP-8060, a way for users to specify a checksum type on create() is needed. The way FileSystem cache works makes it impossible to use dfs.checksum.type to achieve this. Also checksum-related API is at Filesystem-level, so we prefer something at that level, not hdfs-specific one.  Current proposal is to use CreatFlag.
"
HADOOP-8239,Extend MD5MD5CRC32FileChecksum to show the actual checksum type being used,"In order to support HADOOP-8060, MD5MD5CRC32FileChecksum needs to be extended to carry the information on the actual checksum type being used. The interoperability between the extended version and branch-1 should be guaranteed when Filesystem.getFileChecksum() is called over hftp, webhdfs or httpfs."
HADOOP-8238,NetUtils#getHostNameOfIP blows up if given ip:port string w/o port,"NetUtils#getHostNameOfIP blows up if not given a string of form ip:port, because the regex matches the port optionally but the code requires a semicolon. It also doesn't check the given string for null."
HADOOP-8236,haadmin should have configurable timeouts for failover commands,"The HAAdmin failover could should time out reasonably aggressively and go onto the fencing strategies if it's dealing with a mostly dead active namenode.  Currently it uses what's probably the default, which is to say no timeout whatsoever.

{code}
  /**
   * Return a proxy to the specified target service.
   */
  protected HAServiceProtocol getProtocol(String serviceId)
      throws IOException {
    String serviceAddr = getServiceAddr(serviceId);
    InetSocketAddress addr = NetUtils.createSocketAddr(serviceAddr);
    return (HAServiceProtocol)RPC.getProxy(
          HAServiceProtocol.class, HAServiceProtocol.versionID,
          addr, getConf());
  }
{code}"
HADOOP-8230,Enable sync by default and disable append,"Per HDFS-3120 for 1.x let's:
- Always enable the sync path, which is currently only enabled if dfs.support.append is set
- Remove the dfs.support.append configuration option. We'll keep the code paths though in case we ever fix append on branch-1, in which case we can add the config option back"
HADOOP-8227,Allow RPC to limit ephemeral port range.,This is a sub task of MAPREDUCE-4079.  For security reasons we would like to limit the range of ports that are used when some RPC servers select a port.
HADOOP-8225,DistCp fails when invoked by Oozie,"When DistCp is invoked through a proxy-user (e.g. through Oozie), the delegation-token-store isn't picked up by DistCp correctly. One sees failures such as:

ERROR [main] org.apache.hadoop.tools.DistCp: Couldn't complete DistCp
operation: 
java.lang.SecurityException: Intercepted System.exit(-999)
    at
org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)
    at java.lang.Runtime.exit(Runtime.java:88)
    at java.lang.System.exit(System.java:904)
    at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at
org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)
    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)

Looking over the DistCp code, one sees that HADOOP_TOKEN_FILE_LOCATION isn't being copied to mapreduce.job.credentials.binary, in the job-conf. I'll post a patch for this shortly."
HADOOP-8224,Don't hardcode hdfs.audit.logger in the scripts,"The HADOOP_*OPTS defined for HDFS in hadoop-env.sh hard-code the hdfs.audit.logger (is explicitly set via ""-Dhdfs.audit.logger=INFO,RFAAUDIT"") so it's not overridable. Let's allow someone to override it as we do the other parameters by introducing HADOOP_AUDIT_LOGGER."
HADOOP-8218,RPC.closeProxy shouldn't throw error when closing a mock,"HADOOP-8202 changed the behavior of RPC.stopProxy() to throw an exception if called on an object which doesn't implement Closeable. Unfortunately, we use mock objects in many test cases, and those mocks don't implement Closeable. This is causing TestZKFailoverController to fail in trunk, for example."
HADOOP-8214,make hadoop script recognize a full set of deprecated commands,"bin/hadoop launcher script does a nice job of recognizing deprecated usage and vectoring users towards the proper command line tools (hdfs, mapred). It would be nice if we can take care of the following deprecated commands that don't get the same special treatment:

{noformat}
  oiv                  apply the offline fsimage viewer to an fsimage
  dfsgroups            get the groups which users belong to on the Name Node
  mrgroups             get the groups which users belong to on the Job Tracker
  mradmin              run a Map-Reduce admin client
  jobtracker           run the MapReduce job Tracker node
  tasktracker          run a MapReduce task Tracker node
{noformat}

Here's what I propos to do with them:
  # oiv        -- issue DEPRECATED warning and run hdfs oiv
  # dfsgroups  -- issue DEPRECATED warning and run hdfs groups
  # mrgroups   -- issue DEPRECATED warning and run mapred groups
  # mradmin    -- issue DEPRECATED warning and run yarn rmadmin
  # jobtracker -- issue DEPRECATED warning and do nothing
  # tasktracker-- issue DEPRECATED warning and do nothing

Thoughts?"
HADOOP-8211,Update commons-net version to 3.1,HADOOP-8210 requires the commons-net version be upgraded. Let's bump it to the latest stable version. The only other user is FtpFs.
HADOOP-8210,Common side of HDFS-3148,"Common side of HDFS-3148, add necessary DNS and NetUtils methods. Test coverage is in the HDFS-3148 patch. "
HADOOP-8209,Add option to relax build-version check for branch-1,"In 1.x DNs currently refuse to connect to NNs if their build *revision* (ie svn revision) do not match. TTs refuse to connect to JTs if their build *version* (version, revision, user, and source checksum) do not match.

This prevents rolling upgrades, which is intentional, see the discussion in HADOOP-5203. The primary motivation in that jira was (1) it's difficult to guarantee every build on a large cluster got deployed correctly, builds don't get rolled back to old versions by accident etc, and (2) mixed versions can lead to execution problems that are hard to debug.

However there are also cases when users know they two builds are compatible, eg when deploying a new build which contains the same contents as the previous one, plus a critical security patch that does not affect compatibility. Currently deploying a 1 line patch requires taking down the entire cluster (or trying to work around the issue by lying about the build revision or checksum, yuck). These users would like to be able to perform a rolling upgrade.

In order to support this, let's add an option that is off by default, but, when enabled, makes the DN and TT version check just check for an exact version match (eg ""1.0.2"") but ignore the build revision (DN) and the source checksum (TT). Two builds still need to match the major, minor, and point numbers, but nothing else."
HADOOP-8206,Common portion of ZK-based failover controller,"This JIRA is for the Common (generic) portion of HDFS-2185. It can't run on its own, but this JIRA will include unit tests using mock/dummy services."
HADOOP-8204,TestHealthMonitor fails occasionally ,See e.g. https://builds.apache.org/job/PreCommit-HADOOP-Build/756//testReport/org.apache.hadoop.ha/TestHealthMonitor/testMonitor/
HADOOP-8202,stopproxy() is not closing the proxies correctly,"I was running testbackupnode and noticed that NNprotocol proxy was not being closed. Talked with Suresh and he observed that most of the protocols do not implement ProtocolTranslator and hence the logic in stopproxy() does not work. Instead, since all of them are closeable, Suresh suggested that closeable property should be used at close.
"
HADOOP-8201,create the configure script for native compilation as part of the build,"configure script is checked into svn and its not regenerated during build. 
Ideally configure scritp should not be checked into svn and instead should be generated during build using autoreconf.
"
HADOOP-8200,Remove HADOOP_[JOBTRACKER|TASKTRACKER]_OPTS ,The HADOOP_[JOBTRACKER|TASKTRACKER]_OPTS env variables are no longer in trunk/23 since there's no MR1 implementation and the tests don't use them. This makes the patch for HADOOP-8149 easier.
HADOOP-8199,Fix issues in start-all.sh and stop-all.sh,"1. Warning message

Execute start-all.sh and stop-all.sh scripts it displays message like ""This script is Deprecated. Instead use start-dfs.sh/stop-dfs.sh and start-mapred.sh/stop-mapred.sh"". Instead of start-mapred.sh/stop-mapred.sh it should be start-yarn.sh/stop-yarn.sh.

2. start/stop hdfs

3. start/stop yarn

"
HADOOP-8197,Configuration logs WARNs on every use of a deprecated key,"The logic to do print a warning only once per deprecated key does not work:

{code}
2012-03-21 22:32:58,121  WARN Configuration:661 - user.name is deprecated. Instead, use mapreduce.job.user.name
....
2012-03-21 22:32:58,123  WARN Configuration:661 - fs.default.name is deprecated. Instead, use fs.defaultFS
...
2012-03-21 22:32:58,130  WARN Configuration:661 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2012-03-21 22:32:58,351  WARN Configuration:345 - fs.default.name is deprecated. Instead, use fs.defaultFS
...
2012-03-21 22:32:58,843  WARN Configuration:661 - user.name is deprecated. Instead, use mapreduce.job.user.name
2012-03-21 22:32:58,844  WARN Configuration:661 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
2012-03-21 22:32:58,844  WARN Configuration:661 - fs.default.name is deprecated. Instead, use fs.defaultFS
{code}"
HADOOP-8193,"Refactor FailoverController/HAAdmin code to add an abstract class for ""target"" services","In working at HADOOP-8077, HDFS-3084, and HDFS-3072, I ran into various difficulties which are an artifact of the current design. A few of these:
- the service name is ""resolved"" from the logical name (eg ns1.nn1) to an IP address at the outer layer of DFSHAAdmin
-- this means it's difficult to provide the logical name ""ns1.nn1"" to fence scripts (HDFS-3084)
-- this means it's difficult to configure fencing method per-namespace (since the FailoverController doesn't know what the namespace is) (HADOOP-8077)
- the configuration for HA HDFS is weirdly split between core-site and hdfs-site, even though most users see this as an HDFS feature. For example, users expect to configure NN fencing configurations in hdfs-site, and expect the keys to have a dfs.* prefix
- proxies are constructed at the outer layer of the admin commands. This means it's impossible for the inner layers (eg FailoverController.failover) to re-construct proxies with different timeouts (HDFS-3072)

The proposed refactor is to add a new interface (tentatively named HAServiceTarget) which refers to target for one of the admin commands. An instance of this class is responsible for creating proxies, creating fencers, mapping back to a logical name, etc. The HDFS implementation of this class can then provide different results based on the particular nameservice, can use HDFS-specific configuration prefixes, etc. Using this class as the argument for fencing methods also makes the API more evolvable in the future, since we can add new getters to HAServiceTarget (whereas the current InetSocketAddress is quite limiting)"
HADOOP-8191,SshFenceByTcpPort uses netcat incorrectly,"SshFencyByTcpPort currently assumes that the NN is listening on localhost.  Typical setups have the namenode listening just on the hostname of the namenode, which would lead ""nc -z"" to not catch it.

Here's an example in which the NN is running, listening on 8020, but doesn't respond to ""localhost 8020"".
{noformat}
[root@xxx ~]# lsof -P -p 5286 | grep -i listen
java    5286 root  110u  IPv4            1772357              TCP xxx:8020 (LISTEN)
java    5286 root  121u  IPv4            1772397              TCP xxx:50070 (LISTEN)
[root@xxx ~]# nc -z localhost 8020
[root@xxx ~]# nc -z xxx 8020
Connection to xxx 8020 port [tcp/intu-ec-svcdisc] succeeded!
{noformat}

Here's the likely offending code:
{code}
        LOG.info(
            ""Indeterminate response from trying to kill service. "" +
            ""Verifying whether it is running using nc..."");
        rc = execCommand(session, ""nc -z localhost 8020"");
{code}

Naively, we could rely on netcat to the correct hostname (since the NN ought to be listening on the hostname it's configured as), or just to use fuser.  Fuser catches ports independently of what IPs they're bound to:

{noformat}
[root@xxx ~]# fuser 1234/tcp
1234/tcp:             6766  6768
[root@xxx ~]# jobs
[1]-  Running                 nc -l localhost 1234 &
[2]+  Running                 nc -l rhel56-18.ent.cloudera.com 1234 &
[root@xxx ~]# sudo lsof -P | grep -i LISTEN | grep -i 1234
nc         6766      root    3u     IPv4            2563626                 TCP localhost:1234 (LISTEN)
nc         6768      root    3u     IPv4            2563671                 TCP xxx:1234 (LISTEN)
{noformat}"
HADOOP-8189,LdapGroupsMapping shouldn't throw away IOException,"When extracting a password from a file during the config setup, the LdapGroupsMapping throws a RuntimeException, but doesn't bubble up the IOException that caused it"
HADOOP-8188,"Fix the build process to do with jsvc, with IBM's JDK as the underlying jdk","When IBM JDK is used as the underlying JDK for the build process, the build of jsvc fails. I just needed to add an extra ""os arch"" expression in the condition that sets os-arch."
HADOOP-8185,Update namenode -format documentation and add -nonInteractive and -force,documentation changes related to HDFS-3094
HADOOP-8184,ProtoBuf RPC engine does not need it own reply packet - it can use the IPC layer reply packet.,
HADOOP-8183,"Stop using ""mapred.used.genericoptionsparser"" to avoid unnecessary warnings","Its about time we stopped the following from appearing in 0.23/trunk:

{code}
12/03/19 20:53:51 WARN conf.Configuration: mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used
{code}"
HADOOP-8180,Remove hsqldb since its not needed from pom.xml,Related to MAPREDUCE-3621
HADOOP-8179,risk of NPE in CopyCommands processArguments(),"My IDE is warning me that the {{is.close()}} method will NPE if the {{is = src.fs.open(src.path);}} call raises an exception, which could happen if the source path could not be opened. There should be an if (is!=null) wrapper"
HADOOP-8172,Configuration no longer sets all keys in a deprecated key list.,"I did not look at the patch for HADOOP-8167 previously, but I did in response to a recent test failure. The patch appears to have changed the following code (I am just paraphrasing the code)

{code}
if(!deprecated(key)) {
  set(key, value);
} else {
  for(String newKey: depricatedKeyMap.get(key)) {
    set(newKey, value);
  }
}
{code}

to be 

{code}
set(key, value);
if(depricatedKeyMap.contains(key)) {
   set(deprecatedKeyMap.get(key)[0], value);
} else if(reverseKeyMap.contains(key)) {
   set(reverseKeyMap.get(key), value);
}
{code}

If a key is deprecated and is mapped to more then one new key value only the first one in the list will be set, where as previously all of them would be set."
HADOOP-8169,javadoc generation fails with java.lang.OutOfMemoryError: Java heap space,"building the docs (mvn package -Pdocs -Dtar -DskipTests) on branch-0.23 results in a javadoc java.lang.OutOfMemoryError: Java heap space. Note this seems to only happen when building with 32 bit java, 64 bit works fine."
HADOOP-8168,empty-string owners or groups causes {{MissingFormatWidthException}} in o.a.h.fs.shell.Ls.ProcessPath(),"In {{adjustColumnWidths()}}, we set the member variable {{lineFormat}}, which is used by {{ProcessPath()}} to print directory entries. Owners and groups are formatted using the formatting conversion {{%-Xs}}, where X is the max length of the owner or group. However, when trying this with an S3 URL, I found that the owner and group were empty (""""). This caused X to be 0, which means that the formatting conversion is set to {{%-0s}}. This caused a {{MissingFormatWidthException}} to be thrown when the formatting string was used in {{ProcessPath()}}. 

Formatting conversions are described here: 

http://docs.oracle.com/javase/1.6.0/docs/api/java/util/Formatter.html#intFlags

The specific exception thrown (a subtype of {{IllegalFormatException}}) is described here:

http://docs.oracle.com/javase/1.6.0/docs/api/java/util/MissingFormatWidthException.html
"
HADOOP-8167,Configuration deprecation logic breaks backwards compatibility,"The deprecated Configuration logic works as follows:

For a dK deprecated key in favor of nK:

* on set(dK, V), it stores (nK,V)
* on get(dK) it does a reverseLookup of dK to nK and looks for get(nK)

While this works fine for single set/get operations, the iterator() method that returns an iterator of all config key/values, returns only the new keys.

This breaks applications that did a set(dK, V) and expect, when iterating over the configuration to find (dK, V).
"
HADOOP-8166,Remove JDK 1.5 dependency from building forrest docs,Currently Hadoop requires both JDK 1.6 and JDK 1.5. JDK 1.5 is a requirement of Forrest. It is easy to remove the latter requirement by turning off forrest.validate.sitemap and forrest.validate.skins.stylesheets.
HADOOP-8164,Handle paths using back slash as path separator for windows only,Please see the description in HADOOP-8139. Using escape character back slash as path separator could cause accidental deletion of data. This jira for now supports back slash only for windows. Eventually HADOOP-8139 will remove the support for back slash based paths.
HADOOP-8163,Improve ActiveStandbyElector to provide hooks for fencing old active,"When a new node becomes active in an HA setup, it may sometimes have to take fencing actions against the node that was formerly active. This JIRA extends the ActiveStandbyElector which adds an extra non-ephemeral node into the ZK directory, which acts as a second copy of the active node's information. Then, if the active loses its ZK session, the next active to be elected may easily locate the unfenced node to take the appropriate actions."
HADOOP-8160,"HardLink.getLinkCount() is getting stuck in eclipse ( Cygwin) for long file names, due to MS-Dos style Path.","HardLink.getLinkCount() is getting stuck in cygwin for long file names, due to MS-DOS style path."
HADOOP-8159,NetworkTopology: getLeaf should check for invalid topologies,"Currently, in NetworkTopology, getLeaf doesn't do too much validation on the InnerNode object itself. This results in us getting ClassCastException sometimes when the network topology is invalid. We should have a less confusing exception message for this case.
"
HADOOP-8158,Interrupting hadoop fs -put from the command line causes a LeaseExpiredException,"If you run ""hadoop fs -put - foo"", write a few lines, then ^C it from the shell, about half the time you will get a LeaseExpiredException. It seems like the shell is first calling {{delete()}} on the file, then calling {{close()}} on the stream. The {{close}} call fails since the {{delete}} call kills the lease. I saw this on trunk but my guess is that it affects 23 also."
HADOOP-8157,TestRPCCallBenchmark#testBenchmarkWithWritable fails with RTE,"Saw TestRPCCallBenchmark#testBenchmarkWithWritable fail with the following on jenkins:
Caused by: java.lang.RuntimeException: IPC server unable to read call parameters: readObject can't find class java.lang.String
"
HADOOP-8154,DNS#getIPs shouldn't silently return the local host IP for bogus interface names,"DNS#getIPs silently returns the local host IP for bogus interface names. In this case let's throw an UnknownHostException. This is technically an incompatbile change. I suspect the current behavior was origininally introduced so the interface name ""default"" works w/o explicitly checking for it. It may also be used in cases where someone is using a shared config file and an option like ""dfs.datanode.dns.interface"" or ""hbase.master.dns.interface"" and eg interface ""eth3"" that some hosts don't have, though I think silently ignorning this is the wrong behavior (those hosts should be configured to use a different interface)."
HADOOP-8152,Expand public APIs for security library classes,"Currently projects like Hive and HBase use UserGroupInformation and SecurityUtil methods. Both of these classes are marked LimitedPrivate(HDFS,MR) but should probably be marked more generally public."
HADOOP-8151,Error handling in snappy decompressor throws invalid exceptions,"SnappyDecompressor.c has the following code in a few places:
{code}
    THROW(env, ""Ljava/lang/InternalError"", ""Could not decompress data. Buffer length is too small."");
{code}
this is incorrect, though, since the THROW macro doesn't need the ""L"" before the class name. This results in a ClassNotFoundException for Ljava.lang.InternalError being thrown, instead of the intended exception."
HADOOP-8149,cap space usage of default log4j rolling policy ,"I've seen several critical production issues because logs are not automatically removed after some time and accumulate. Changes to Hadoop's default log4j file appender would help with this.

I recommend we move to an appender which:

1) caps the max file size (configurable)
2) caps the max number of files to keep (configurable)
3) uses rolling file appender rather than DRFA, see the warning here:
http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/DailyRollingFileAppender.html
Specifically: ""DailyRollingFileAppender has been observed to exhibit synchronization issues and data loss.""

We'd lose (based on the default log4j configuration) the daily rolling aspect, however increase reliability.
"
HADOOP-8144,pseudoSortByDistance in NetworkTopology doesn't work properly if no local node and first node is local rack node,"pseudoSortByDistance in NetworkTopology.java should sort nodes according to its distance with reader as local node, local rack node, ... 
But if there is no local node with reader in nodes and the first node is local rack node with reader, then it will put a random node at position 0."
HADOOP-8142,Update versions from 0.23.2 to 0.23.3,Some versions in the 0.23 branch are still 0.23.2.
HADOOP-8141,Add method to init krb5 cipher suites,We have duplicated the code in a lot of places to set https.cipherSuites. We should put a utility method in SecurityUtil for it.
HADOOP-8135,Add ByteBufferReadable interface to FSDataInputStream,"To prepare for HDFS-2834, it's useful to add an interface to FSDataInputStream (and others inside hdfs) that adds a read(ByteBuffer...) method as follows:

{code}
  /**
   * Reads up to buf.remaining() bytes into buf. Callers should use
   * buf.limit(..) to control the size of the desired read.
   * 
   * After the call, buf.position() should be unchanged, and therefore any data
   * can be immediately read from buf.
   * 
   * @param buf
   * @return - the number of bytes available to read from buf
   * @throws IOException
   */
  public int read(ByteBuffer buf) throws IOException;
{code}"
HADOOP-8132,64bit secure datanodes do not start as the jsvc path is wrong,64bit secure datanodes were looking for /usr/libexec/../libexec/jsvc. instead of /usr/libexec/../libexec/jsvc.amd64
HADOOP-8129,ViewFileSystemTestSetup setupForViewFileSystem is erring when the user's home directory is somewhere other than /home (eg. /User) etc.,"All TestFSMainOperationsLocalFileSystem tests (99 in all) fail saying:
{noformat}
java.io.FileNotFoundException: /home
	at org.apache.hadoop.fs.viewfs.InodeTree.resolve(InodeTree.java:403)
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.mkdirs(ViewFileSystem.java:373)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1684)
	at org.apache.hadoop.fs.FSMainOperationsBaseTest.setUp(FSMainOperationsBaseTest.java:90)
	at org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:42)
{noformat}"
HADOOP-8121,Active Directory Group Mapping Service,Planning on building a group mapping service that will go and talk directly to an Active Directory setup to get group memberships
HADOOP-8119,Fix javac warnings in TestAuthenticationFilter,There are 43 javac warnings in TestAuthenticationFilter.
HADOOP-8118,Print the stack trace of InstanceAlreadyExistsException in trace level,"There are many InstanceAlreadyExistsException stack traces in the unit tests output like below.
{noformat}
javax.management.InstanceAlreadyExistsException: Hadoop:service=NameNode,name=NameNodeInfo
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
	...
{noformat}"
HADOOP-8117,Upgrade test build to Surefire 2.12,"Surefire 2.9, which we're using currently, has a few annoying bugs. In particular, if a test exits with a non-zero exit code, it doesn't report the test as failed."
HADOOP-8113,"Correction to BUILDING.txt: HDFS needs ProtocolBuffer, too (not just MapReduce)","Currently BUILDING.txt states: 

{quote}
  ProtocolBuffer 2.4.1+ (for MapReduce)
{quote}

But HDFS needs ProtocolBuffer too: 

{code}
hadoop-common/hadoop-hdfs-project$ find . -name ""*.proto"" | wc -l
      11
{code}

"
HADOOP-8110,TestViewFsTrash occasionally fails,"{noformat}
junit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:283)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:195)
	at org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)
	at org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)
	...
{noformat}
There are quite a few TestViewFsTrash failures recently.  E.g. [build #624 for trunk|https://builds.apache.org/job/PreCommit-HADOOP-Build/624//testReport/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/] and [build #2 for 0.23-PB|https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Common-0.23-PB-Build/2/testReport/junit/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/].
"
HADOOP-8104,Inconsistent Jackson versions,"This is a maven build issue.

Jersey 1.8 is pulling in version 1.7.1 of Jackson.  Meanwhile, we are manually specifying that we want version 1.8 of Jackson in the POM files.  This causes a conflict where Jackson produces unexpected results when serializing Map objects.

How to reproduce: try this code:
{quote}
ObjectMapper mapper = new ObjectMapper();
 Map<String, Object> m = new HashMap<String, Object>();
mapper.writeValue(new File(""foo""), m);
{quote}

You will get an exception:
{quote}
Exception in thread ""main"" java.lang.NoSuchMethodError: org.codehaus.jackson.type.JavaType.isMapLikeType()Z
at org.codehaus.jackson.map.ser.BasicSerializerFactory.buildContainerSerializer(BasicSerializerFactory.java:396)
at org.codehaus.jackson.map.ser.BeanSerializerFactory.createSerializer(BeanSerializerFactory.java:267)
{quote}

Basically the inconsistent versions of various Jackson components are causing this NoSuchMethod error.

As far as I know, this only occurs when serializing maps-- that's why it hasn't been found and fixed yet."
HADOOP-8098,KerberosAuthenticatorHandler should use _HOST replacement to resolve principal name,"Currently the exact Kerberos principal name has to be set in the configuration of each node.

KerberosAuthenticatorHandler should do similar logic as the RPC ports to support HTTP/_HOST@REALM"
HADOOP-8090,rename hadoop 64 bit rpm/deb package name,change hadoop rpm/deb name from hadoop-<version>.amd64.rpm/deb hadoop-<version>.x86_64.rpm/deb   
HADOOP-8088,User-group mapping cache incorrectly does negative caching on transient failures,"We've seen a case where some getGroups() calls fail when the ldap server or the network is having transient failures. Looking at the code, the shell-based and the JNI-based implementations swallow exceptions and return an empty or partial list. The caller, Groups#getGroups() adds this likely empty list into the mapping cache for the user. This will function as negative caching until the cache expires. I don't think we want negative caching here, but even if we do, it should be intelligent enough to distinguish transient failures from ENOENT. The log message in the jni-based impl also needs an improvement. It should print what exception it encountered instead of just saying one happened."
HADOOP-8086,"KerberosName silently sets defaultRealm to """" if the Kerberos config is not found, it should log a WARN",
HADOOP-8085,Add RPC metrics to ProtobufRpcEngine,"ProtobufRpcEngine is missing the RPC metrics compared to WritableRpcEngine. It is important information for monitoring the cluster and understanding the server performance.
"
HADOOP-8084,Protobuf RPC engine can be optimized to not do copying for the RPC request/response,
HADOOP-8077,HA: fencing method should be able to be configured on a per-NN or per-NS basis,"Currently, the fencing method configuration is global. Given that different nameservices may use different underlying storage mechanisms or different types of PDUs, it would be preferable to allow the fencing method configuration to be scoped by namenode or nameservice."
HADOOP-8075,Lower native-hadoop library log from info to debug ,"The following log shows up in stderr all commands. We've already got a warning if the native library can't be loaded, don't need to log this every time at info level.

{noformat}
[eli@centos6 ~]$ hadoop fs -cat /user/eli/foo
12/02/12 20:10:20 INFO util.NativeCodeLoader: Loaded the native-hadoop library
{noformat}
"
HADOOP-8070,Add standalone benchmark of protobuf IPC,"To be more comfortable with the switch to protobuf IPC, I'd like to contribute a standalone benchmark which can start any number of client threads and server threads."
HADOOP-8069,Enable TCP_NODELAY by default for IPC,"I think we should switch the default for the IPC client and server NODELAY options to true. As wikipedia says:
{quote}
In general, since Nagle's algorithm is only a defense against careless applications, it will not benefit a carefully written application that takes proper care of buffering; the algorithm has either no effect, or negative effect on the application.
{quote}
Since our IPC layer is well contained and does its own buffering, we shouldn't be careless."
HADOOP-8060,Add a capability to discover and set checksum types per file.,"After the improved CRC32C checksum feature became default, some of use cases involving data movement are no longer supported.  For example, when running DistCp to copy from a file stored with the CRC32 checksum to a new cluster with the CRC32C set to default checksum, the final data integrity check fails because of mismatch in checksums."
HADOOP-8059,Add javadoc to InterfaceAudience and InterfaceStability,InterfaceAudience and InterfaceStability javadoc is incomplete. The details from HADOOP-5073.
HADOOP-8055,Distribution tar.gz does not contain etc/hadoop/core-site.xml,"A dist built from trunk (0.24.0-SNAPSHOT) does not contain a core-site.xml in $HADOOP_HOME/etc/hadoop/ folder.

$HADOOP_HOME/sbin/start-dfs.sh without that folder gives an exception
Exception in thread ""main"" java.lang.IllegalArgumentException: URI has an authority component
 at java.io.File.<init>(File.java:368)
 at org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageDirectory(NNStorage.java:310)
 at org.apache.hadoop.hdfs.server.namenode.FSEditLog.init(FSEditLog.java:178)
...

Manually creating $HADOOP_HOME/etc/hadoop/core-site.xml solves this problem and hadoop starts fine.

"
HADOOP-8054,NPE with FilterFileSystem,"While running Hive tests, I'm seeing the following exception with 0.23.1,
{noformat}
ava.lang.NullPointerException
        at org.apache.hadoop.fs.FileSystem.getDefaultBlockSize(FileSystem.java:1901)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:447)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:351)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:351)
        at org.apache.hadoop.fs.ProxyFileSystem.getFileStatus(ProxyFileSystem.java:247)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:351)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1165)
        at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:390)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:242)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:232)
{noformat}

Did not see this with 0.23.0, though."
HADOOP-8052,Hadoop Metrics2 should emit Float.MAX_VALUE (instead of Double.MAX_VALUE) to avoid making Ganglia's gmetad core,"Ganglia's gmetad converts the doubles emitted by Hadoop's Metrics2 system to strings, and the buffer it uses is 256 bytes wide.

When the SampleStat.MinMax class (in org.apache.hadoop.metrics2.util) emits its default min value (currently initialized to Double.MAX_VALUE), it ends up causing a buffer overflow in gmetad, which causes it to core, effectively rendering Ganglia useless (for some, the core is continuous; for others who are more fortunate, it's only a one-time Hadoop-startup-time thing).

The fix needed to Ganglia is simple - the buffer needs to be bumped up to be 512 bytes wide, and all will be well - but instead of requiring a minimum version of Ganglia to work with Hadoop's Metrics2 system, it might be more prudent to just use Float.MAX_VALUE.

An additional problem caused in librrd (which Ganglia uses beneath-the-covers) by the use of Double.MIN_VALUE (which functions as the default max value) is an underflow when librrd runs the received strings through libc's strtod(), but the librrd code is good enough to check for this, and only emits a warning - moving to Float.MIN_VALUE fixes that as well."
HADOOP-8050,Deadlock in metrics,"The metrics serving thread and the periodic snapshot thread can deadlock.
It happened a few times on one of namenodes we have. When it happens RPC works but the web ui and hftp stop working. I haven't look at the trunk too closely, but it might happen there too."
HADOOP-8040,Add symlink support to FileSystem,"HADOOP-6421 added symbolic links to FileContext. Resolving symlinks is done on the client-side, and therefore requires client support. An HDFS symlink (created by FileContext) when accessed by FileSystem will result in an unhandled UnresolvedLinkException. Because not all users will migrate from FileSystem to FileContext in lock step, and we want users of FileSystem to be able to access all paths created by FileContext, we need to support symlink resolution in FileSystem as well, to facilitate migration to FileContext."
HADOOP-8037,"Binary tarball does not preserve platform info for native builds, and RPMs fail to provide needed symlinks for libhadoop.so","The source tarball uses ""package"" ant target, which includes both sets of native builds (32 and 64 bit libraries), under subdirectories that are named for the supported platform, so you can tell what they are.

The binary tarball uses the ""bin-package"" ant target, which projects both sets of native builds into a single directory, stripping out the platform names from the directory paths.  Since the native built libraries have identical names, only one of each survives the process.  Afterward, there is no way to know whether they are intended for 32 or 64 bit environments.

It seems to be done this way as a step toward building the rpm and deb artifacts.  But the rpms and debs are self-identifying as to the platform they were built for, and contain only one set of libs each, while the binary tarball isn't.  The binary tarball should have the same platform-specific subdirectories that the full tarball does; but this means that the rpm and deb builds have to be more careful about include/exclude specs for what goes into those artifacts.
"
HADOOP-8031,Configuration class fails to find embedded .jar resources; should use URL.openStream(),"While running a hadoop client within RHQ (monitoring software) using its classloader, I see this:

2012-02-07 09:15:25,313 INFO  [ResourceContainer.invoker.daemon-2] (org.apache.hadoop.conf.Configuration)- parsing jar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml
2012-02-07 09:15:25,318 ERROR [InventoryManager.discovery-1] (rhq.core.pc.inventory.InventoryManager)- Failed to start component for Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com] from synchronized merge.
org.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].
Caused by: java.lang.RuntimeException: core-site.xml not found
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:438)

This is because the URL

jar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml

cannot be found by DocumentBuilder (doesn't understand it). (Note: the logs are for an old version of Configuration class, but the new version has the same code.)

The solution is to obtain the resource stream directly from the URL object itself.

That is to say:

{code}
         URL url = getResource((String)name);
-        if (url != null) {
-          if (!quiet) {
-            LOG.info(""parsing "" + url);
-          }
-          doc = builder.parse(url.toString());
-        }
+        doc = builder.parse(url.openStream());
{code}

Note: I have a full patch pending approval at Apple for this change, including some cleanup."
HADOOP-8027,Visiting /jmx on the daemon web interfaces may print unnecessary error in logs,"Logs that follow a {{/jmx}} servlet visit:

{code}
11/11/22 12:09:52 ERROR jmx.JMXJsonServlet: getting attribute UsageThreshold of java.lang:type=MemoryPool,name=Par Eden Space threw an exception
javax.management.RuntimeMBeanException: java.lang.UnsupportedOperationException: Usage threshold is not supported
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:856)
...
{code}"
HADOOP-8023,Add unset() method to Configuration,"HADOOP-7001 introduced the *Configuration.unset(String)* method.

MAPREDUCE-3727 requires that method in order to be back-ported.

This is required to fix an issue manifested when running MR/Hive/Sqoop jobs from Oozie, details are in MAPREDUCE-3727.
"
HADOOP-8018,Hudson auto test for HDFS has started throwing javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list,"Hudson automated testing has started failing with one javadoc warning message, consisting of
javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list

This may be due to Oracle's decommissioning of the sun.com domain.  If one tries to access it manually, it is redirected to 
http://download.oracle.com/javase/6/docs/api/package-list

So it looks like a build script needs to be updated."
HADOOP-8015,ChRootFileSystem should extend FilterFileSystem,"{{ChRootFileSystem}} simply extends {{FileSystem}}, and attempts to delegate some methods to the underlying mount point.  It is essentially the same as {{FilterFileSystem}} but it mangles the paths to include the chroot path.  Unfortunately {{ChRootFileSystem}} is not delegating some methods that should be delegated.  Changing the inheritance will prevent a copy-n-paste of code for HADOOP-8013 and HADOOP-8014 into both {{ChRootFileSystem}} and {{FilterFileSystem}}."
HADOOP-8014,"ViewFileSystem does not correctly implement getDefaultBlockSize, getDefaultReplication, getContentSummary",{{ViewFileSystem}} incorrectly returns the {{FileSystem}} default values for {{getDefaultBlockSize()}} and {{getDefaultReplication()}}.  This causes files to be created with incorrect values.  The problem is that the current apis are insufficient for viewfs because the defaults depend on the underlying mount point.  These methods need counterparts that accept a {{Path}} so viewfs can resolve the mount point for a path.
HADOOP-8013,ViewFileSystem does not honor setVerifyChecksum,{{ViewFileSystem#setVerifyChecksum}} is a no-op.  It should call {{setVerifyChecksum}} on the mount points.
HADOOP-8012,hadoop-daemon.sh and yarn-daemon.sh are trying to mkdir and chow log/pid dirs which can fail,"Here's what I see when using Hadoop in Bigtop:

{noformat}
$ sudo /sbin/service hadoop-hdfs-namenode start
Starting Hadoop namenode daemon (hadoop-namenode): chown: changing ownership of `/var/log/hadoop': Operation not permitted
starting namenode, logging to /var/log/hadoop/hadoop-hdfs-namenode-centos5.out
{noformat}

This is a cosmetic issue, but it would be nice to fix it."
HADOOP-8010,hadoop-config.sh spews error message when HADOOP_HOME_WARN_SUPPRESS is set to true and HADOOP_HOME is present,"Running hadoop daemon commands when HADOOP_HOME_WARN_SUPPRESS is set to true and HADOOP_HOME is present produces:
{noformat}
  [: 76: true: unexpected operator
{noformat}"
HADOOP-8009,Create hadoop-client and hadoop-minicluster artifacts for downstream projects ,"Using Hadoop from projects like Pig/Hive/Sqoop/Flume/Oozie or any in-house system that interacts with Hadoop is quite challenging for the following reasons:

* *Different versions of Hadoop produce different artifacts:* Before Hadoop 0.23 there was a single artifact hadoop-core, starting with Hadoop 0.23 there are several (common, hdfs, mapred*, yarn*)

* *There are no 'client' artifacts:* Current artifacts include all JARs needed to run the services, thus bringing into clients several JARs that are not used for job submission/monitoring (servlet, jsp, tomcat, jersey, etc.)

* *Doing testing on the client side is also quite challenging as more artifacts have to be included than the dependencies define:* for example, the history-server artifact has to be explicitly included. If using Hadoop 1 artifacts, jersey-server has to be explicitly included.

* *3rd party dependencies change in Hadoop from version to version:* This makes things complicated for projects that have to deal with multiple versions of Hadoop as their exclusions list become a huge mix & match of artifacts from different Hadoop versions and it may be break things when a particular version of Hadoop requires a dependency that other version of Hadoop does not require.

Because of this it would be quite convenient to have the following 'aggregator' artifacts:

* *org.apache.hadoop:hadoop-client* : it includes all required JARs to use Hadoop client APIs (excluding all JARs that are not needed for it)
* *org.apache.hadoop:hadoop-minicluster* : it includes all required JARs to run Hadoop Mini Clusters

These aggregator artifacts would be created for current branches under development (trunk, 0.22, 0.23, 1.0) and for released versions that are still in use.

For branches under development, these artifacts would be generated as part of the build.

For released versions we would have a a special branch used only as vehicle for publishing the corresponding 'aggregator' artifacts.
"
HADOOP-8007,HA: use substitution token for fencing argument,"Per HADOOP-7983 currently the fencer always passes the target host:port to fence as the first argument to the fence script, it would be better to use a substitution token. That is to say, the user would configure ""myfence.sh $TARGETHOST foo bar"" and Hadoop would substitute the target. This would allow use of pre-existing scripts that might have a different ordering of arguments without a wrapper."
HADOOP-8006,TestFSInputChecker is failing in trunk.,"Trunk build number 939 failed with TestFSInputChecker.
https://builds.apache.org/job/Hadoop-Hdfs-trunk/939/

junit.framework.AssertionFailedError: expected:<10> but was:<0>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:283)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:130)
	at junit.framework.Assert.assertEquals(Assert.java:136)
	at org.apache.hadoop.hdfs.TestFSInputChecker.checkSkip(TestFSInputChecker.java:194)
	at org.apache.hadoop.hdfs.TestFSInputChecker.testChecker(TestFSInputChecker.java:224)
"
HADOOP-8005,Multiple SLF4J binding message in .out file for all daemons,"When I start the NameNode or DataNode using sbin/hadoop-daemon.sh, I get a variant of the following error on stdout:

{noformat}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/joecrow/Code/hadoop-0.23.0/share/hadoop/common/lib/slf4j-log4j12-1.5.11.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/joecrow/Code/hadoop-0.23.0/share/hadoop/hdfs/lib/slf4j-log4j12-1.5.11.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
{noformat}
"
HADOOP-8002,SecurityUtil acquired token message should be a debug rather than info,
HADOOP-8001,ChecksumFileSystem's rename doesn't correctly handle checksum files,"Rename will move the src file and its crc *if present* to the destination.  If the src file has no crc, but the destination already exists with a crc, then src will be associated with the old file's crc.  Subsequent access to the file will fail with checksum errors."
HADOOP-8000,fetchdt command not available in bin/hadoop,fetchdt command needs to be added to bin/hadoop to allow for backwards compatibility.
HADOOP-7999,"""hadoop archive"" fails with ClassNotFoundException","Running ""hadoop archive"" from a command prompt results in this error:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/tools/HadoopArchives
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.tools.HadoopArchives
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.apache.hadoop.tools.HadoopArchives.  Program will exit.


The hadoop front-end script expects the TOOL_PATH environment variable to be set, but nothing provides a default value for it if it is not set.  Since $TOOL_PATH expands to nothing, the hadoop-archives jar under share/hadoop/tools/lib isn't found, and we end up with a ClassNotFound exception."
HADOOP-7998,CheckFileSystem does not correctly honor setVerifyChecksum,"Regardless of the verify checksum flag, {{ChecksumFileSystem#open}} will instantiate a {{ChecksumFSInputChecker}} instead of a normal stream."
HADOOP-7994,Remove getProtocolVersion and getProtocolSignature from the client side translator and server side implementation,HADOOP-7965 implements a different mechanism to obtain version information from the server for protocols supported. We can get rid of getProtocolVersion and getProtocolSignature from the client side translator and server side implementation.
HADOOP-7993,Hadoop ignores old-style config options for enabling compressed output,"Hadoop seems to ignore the config options even though they are printed as deprecation warnings in the log: mapred.output.compress and
mapred.output.compression.codec

- settings that work on 0.20 but not on 0.23
mapred.output.compress=true
mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec

- settings that work on 0.23
mapreduce.output.fileoutputformat.compress=true
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec

This breaks backwards compatibility and causes existing jobs to fail.

This was found to happen due to the JobSubmitter writing out the job.xml file with the old-style configs and can be fixed by handdling deprecation before the file is written out.
"
HADOOP-7988,Upper case in hostname part of the principals doesn't work with kerberos.,"Kerberos doesn't like upper case in the hostname part of the principals.
This issue has been seen in 23 as well as 1.0."
HADOOP-7987,Support setting the run-as user in unsecure mode,"Some applications need to be able to perform actions (such as launch MR jobs) from map or reduce tasks. In earlier unsecure versions of hadoop (20.x), it was possible to do this by setting user.name in the configuration. But in 20.205 and 1.0, when running in unsecure mode, this does not work. (In secure mode, you can do this using the kerberos credentials)."
HADOOP-7986,Add config for History Server protocol in hadoop-policy for service level authorization.,We need to add a property for History server protocol in hadoop-policy.xml for service level auth.
HADOOP-7984,Add hadoop --loglevel option to change log level,"It would be helpful if bin/hadoop had --loglevel option to change the log level. Currently users need to set an env variable or prefix the command (eg ""HADOOP_ROOT_LOGGER=DEBUG,console hadoop distcp"") which isn't very user friendly."
HADOOP-7982,UserGroupInformation fails to login if thread's context classloader can't load HadoopLoginModule,"In a few hard-to-reproduce situations, we've seen a problem where the UGI login call causes a failure to login exception with the following cause:

Caused by: javax.security.auth.login.LoginException: unable to find 
LoginModule class: org.apache.hadoop.security.UserGroupInformation 
$HadoopLoginModule

After a bunch of debugging, I determined that this happens when the login occurs in a thread whose Context ClassLoader has been set to null."
HADOOP-7981,Improve documentation for org.apache.hadoop.io.compress.Decompressor.getRemaining,
HADOOP-7975,Add entry to XML defaults for new LZ4 codec,"HADOOP-7657 added in a new LZ4 codec, but failed to extend the io.compression.codecs list which MR/etc. use up to load codecs.

We should add an entry to the core-default XML for this new codec, just as we did with Snappy."
HADOOP-7974,TestViewFsTrash incorrectly determines the user's home directory,"HADOOP-7284 added a test called TestViewFsTrash which contains the following code to determine the user's home directory. It only works if the user's directory is one level deep, and breaks if the home directory is more than one level deep (eg user hudson, who's home dir might be /usr/lib/hudson instead of /home/hudson).

{code}
    // create a link for home directory so that trash path works
    // set up viewfs's home dir root to point to home dir root on target
    // But home dir is different on linux, mac etc.
    // Figure it out by calling home dir on target
    
   String homeDir = fsTarget.getHomeDirectory().toUri().getPath();
   int indexOf2ndSlash = homeDir.indexOf('/', 1);
   String homeDirRoot = homeDir.substring(0, indexOf2ndSlash);
   ConfigUtil.addLink(conf, homeDirRoot,
       fsTarget.makeQualified(new Path(homeDirRoot)).toUri()); 
   ConfigUtil.setHomeDirConf(conf, homeDirRoot);
   Log.info(""Home dir base "" + homeDirRoot);
{code}

Seems like we should instead search from the end of the path for the last slash and use that as the base, ie ask the home directory for its parent.






"
HADOOP-7973,DistributedFileSystem close has severe consequences,"The way {{FileSystem#close}} works is very problematic.  Since the {{FileSystems}} are cached, any {{close}} by any caller will cause problems for every other reference to it.  Will add more detail in the comments."
HADOOP-7971,"hadoop <job/queue/pipes> removed - should be added back, but deprecated","The mapred subcommands (mradmin|jobtracker|tasktracker|pipes|job|queue)
 were removed from the /bin/hadoop command. I believe for backwards compatibility at least some of these should have stayed along with the deprecated warnings."
HADOOP-7968,Errant println left in RPC.getHighestSupportedProtocol,"hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java: System.out.println(""Size of protoMap for "" + rpcKind + "" ="" + getProtocolImplMap(rpcKind).size());
"
HADOOP-7967,Need generalized multi-token filesystem support,"Multi-token filesystem support and its interactions with the MR {{TokenCache}} is problematic.  The {{TokenCache}} tries to assume it has the knowledge to know if the tokens for a filesystem are available, which it can't possibly know for multi-token filesystems.  Filtered filesystems are also problematic, such as har on viewfs.  When mergeFs is implemented, it too will become a problem with the current implementation.  Currently {{FileSystem}} will leak tokens even when some tokens are already present.

The decision for token acquisition, and which tokens, should be pushed all the way down into the {{FileSystem}} level.  The {{TokenCache}} should be ignorant and simply request tokens from each {{FileSystem}}."
HADOOP-7965,Support for protocol version and signature in PB,VersionedProtocol methods are currently not supported in PB.
HADOOP-7964,Deadlock in class init.,"After HADOOP-7808, client-side commands hang occasionally. There are cyclic dependencies in NetUtils and SecurityUtil class initialization. Upon initial look at the stack trace, two threads deadlock when they hit the either of class init the same time."
HADOOP-7963,test failures: TestViewFileSystemWithAuthorityLocalFileSystem and TestViewFileSystemLocalFileSystem,"The following tests are failing:

 org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem.testGetDelegationTokensWithCredentials 	
 org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem.testGetDelegationTokensWithCredentials 

See hudson: 
https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Common-trunk/lastCompletedBuild/testReport/"
HADOOP-7960,"Port HADOOP-5203 to branch-1, build version comparison is too restrictive","hadoop services should not be using the build timestamp to verify version difference in the cluster installation. Instead it should use the source checksum as in HADOOP-5203.
  "
HADOOP-7957,Classes deriving GetGroupsBase should be able to override proxy creation.,"GetGroups in Hdfs needs to override getUgmProtocol because, due to PB implementations, it will return a translator instance instead of a proxy."
HADOOP-7949,Updated maxIdleTime default in the code to match core-default.xml,"HADOOP-2909 intended to set the server max idle time for a connection to twice the client value. (""The server-side max idle time should be greater than the client-side max idle time, for example, twice of the client-side max idle time."") This way when a server times out a connection it's due a crashed client and not an inactive client so we don't close client connections with outstanding requests (by setting 2x the client value on the server side the client should time out the connection first).

Looks like there was a typo in the patch and it set the default value to 1/5th the client value, instead of the intended 2x.

{noformat}
hadoop2 (pre-HADOOP-4687)$ git reset --hard 6fa4597e
hadoop2 (pre-HADOOP-4687)$ grep -r ipc.client.connection.maxidletime . 
./src/core/org/apache/hadoop/ipc/Client.java:      conf.getInt(""ipc.client.connection.maxidletime"", 10000); //10s
./src/core/org/apache/hadoop/ipc/Server.java:    this.maxIdleTime = 2*conf.getInt(""ipc.client.connection.maxidletime"", 1000);
{noformat}"
HADOOP-7948,Shell scripts created by hadoop-dist/pom.xml to build tar do not properly propagate failure,"The run() function, as defined in dist-layout-stitching.sh and dist-tar-stitching, created in hadoop-dist/pom.xml, does not properly propagate the error code of a failing command.  See the following:
{code}
    ...
    ""${@}""                 # call fails with non-zero exit code
    if [ $? != 0 ]; then   
        echo               
        echo ""Failed!""     
        echo               
        exit $?            # $?=result of echo above, likely 0, thus exit with code 0
    ...
{code}"
HADOOP-7942,enabling clover coverage reports fails hadoop unit test compilation,"enabling clover reports fails compiling the following junit tests.
link to the console output of jerkins :
https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-1-Code-Coverage/13/console



{noformat}
[javac] /tmp/clover50695626838999169.tmp/org/apache/hadoop/security/TestUserGroupInformation.java:224: cannot find symbol
......
    [javac] /tmp/clover50695626838999169.tmp/org/apache/hadoop/security/TestUserGroupInformation.java:225: cannot find symbol
......

 [javac] /tmp/clover50695626838999169.tmp/org/apache/hadoop/security/TestJobCredentials.java:67: cannot find symbol
    [javac] symbol  : class T 
......
[javac] /tmp/clover50695626838999169.tmp/org/apache/hadoop/security/TestJobCredentials.java:68: cannot find symbol
    [javac] symbol  : class T
.....
[javac] /tmp/clover50695626838999169.tmp/org/apache/hadoop/fs/TestFileSystem.java:653: cannot find symbol
    [javac] symbol  : class T
.....
[javac]         ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 5 errors
    [javac] 63 warnings

{noformat}

"
HADOOP-7940,method clear() in org.apache.hadoop.io.Text does not work,"LineReader reader = new LineReader(in, 4096);
...

Text text = new Text();
while((reader.readLine(text)) > 0) {
     ...
     text.clear();
}
}

Even the clear() method is called each time, some bytes are still not filled as zero.
So, when reader.readLine(text) is called in a loop, some bytes are dirty which was from last call."
HADOOP-7939,Improve Hadoop subcomponent integration in Hadoop 0.23,"h1. Introduction

For the rest of this proposal it is assumed that the current set
of Hadoop subcomponents is:
 * hadoop-common
 * hadoop-hdfs
 * hadoop-yarn
 * hadoop-mapreduce

It must be noted that this is an open ended list, though. For example,
implementations of additional frameworks on top of yarn (e.g. MPI) would
also be considered a subcomponent.

h1. Problem statement

Currently there's an unfortunate coupling and hard-coding present at the
level of launcher scripts, configuration scripts and Java implementation
code that prevents us from treating all subcomponents of Hadoop independently
of each other. In a lot of places it is assumed that bits and pieces
from individual subcomponents *must* be located at predefined places
and they can not be dynamically registered/discovered during the runtime.
This prevents a truly flexible deployment of Hadoop 0.23. 

h1. Proposal

NOTE: this is NOT a proposal for redefining the layout from HADOOP-6255. 
The goal here is to keep as much of that layout in place as possible,
while permitting different deployment layouts.

The aim of this proposal is to introduce the needed level of indirection and
flexibility in order to accommodate the current assumed layout of Hadoop tarball
deployments and all the other styles of deployments as well. To this end the
following set of environment variables needs to be uniformly used in all of
the subcomponent's launcher scripts, configuration scripts and Java code
(<SC> stands for a literal name of a subcomponent). These variables are
expected to be defined by <SC>-env.sh scripts and sourcing those files is
expected to have the desired effect of setting the environment up correctly.
  # HADOOP_<SC>_HOME
   ## root of the subtree in a filesystem where a subcomponent is expected to be installed 
   ## default value: $0/..
  # HADOOP_<SC>_JARS 
   ## a subdirectory with all of the jar files comprising subcomponent's implementation 
   ## default value: $(HADOOP_<SC>_HOME)/share/hadoop/$(<SC>)
  # HADOOP_<SC>_EXT_JARS
   ## a subdirectory with all of the jar files needed for extended functionality of the subcomponent (nonessential for correct work of the basic functionality)
   ## default value: $(HADOOP_<SC>_HOME)/share/hadoop/$(<SC>)/ext
  # HADOOP_<SC>_NATIVE_LIBS
   ## a subdirectory with all the native libraries that component requires
   ## default value: $(HADOOP_<SC>_HOME)/share/hadoop/$(<SC>)/native
  # HADOOP_<SC>_BIN
   ## a subdirectory with all of the launcher scripts specific to the client side of the component
   ## default value: $(HADOOP_<SC>_HOME)/bin
  # HADOOP_<SC>_SBIN
   ## a subdirectory with all of the launcher scripts specific to the server/system side of the component
   ## default value: $(HADOOP_<SC>_HOME)/sbin
  # HADOOP_<SC>_LIBEXEC
   ## a subdirectory with all of the launcher scripts that are internal to the implementation and should *not* be invoked directly
   ## default value: $(HADOOP_<SC>_HOME)/libexec
  # HADOOP_<SC>_CONF
   ## a subdirectory containing configuration files for a subcomponent
   ## default value: $(HADOOP_<SC>_HOME)/conf
  # HADOOP_<SC>_DATA
   ## a subtree in the local filesystem for storing component's persistent state
   ## default value: $(HADOOP_<SC>_HOME)/data
  # HADOOP_<SC>_LOG
   ## a subdirectory for subcomponents's log files to be stored
   ## default value: $(HADOOP_<SC>_HOME)/log
  # HADOOP_<SC>_RUN
   ## a subdirectory with runtime system specific information
   ## default value: $(HADOOP_<SC>_HOME)/run
  # HADOOP_<SC>_TMP
   ## a subdirectory with temprorary files
   ## default value: $(HADOOP_<SC>_HOME)/tmp"
HADOOP-7936,There's a Hoop README in the root dir of the tarball,"The Hoop README.txt is now in the root dir of the tarball.

{noformat}
hadoop-trunk1 $ tar xvzf hadoop-dist/target/hadoop-0.24.0-SNAPSHOT.tar.gz  -C /tmp/
..
hadoop-trunk1 $ head -n3 /tmp/hadoop-0.24.0-SNAPSHOT/README.txt 
-----------------------------------------------------------------------------
HttpFS - Hadoop HDFS over HTTP
{noformat}"
HADOOP-7934,Normalize dependencies versions across all modules,"Move all dependencies versions to the dependencyManagement section in the hadoop-project POM

Move all plugin versions to the dependencyManagement section in the hadoop-project POM"
HADOOP-7933,Viewfs changes for MAPREDUCE-3529,"ViewFs.getDelegationTokens returns a list of tokens for the associated namenodes. Credentials serializes these tokens using the service name for the actual namenodes. Effectively, tokens are not cached for viewfs (some more details in MR 3529). Affects any job which uses the TokenCache in tasks along with viewfs (some Pig jobs).

Talk to Jitendra about this, some options
1. Change Credentials.getAllTokens to return the key, instead of just a token list (associate the viewfs canonical name with a token in credentials)
2. Have viewfs issue a fake token.
Both of these would allow for a single viewfs configuration only.
3. An additional API in FileSystem - something like getDelegationTokens(String renewer, Credentials credentials) - which would check the credentials object before making token requests to the actual namenode.
4. An additional API in FileSystem - getCanonicalServiceNames - similar to getDelegationTokens, which would return service names for the actual namenodes. TokenCache/Credentials can work using this list.
5. have getDelegationTokens check the current UGI - and fetch tokens only if they don't exist.

Have a quick patch for 3, along with associated MR changes."
HADOOP-7931,o.a.h.ipc.WritableRpcEngine should have a way to force initialization,{{WritableRpcEngine}} currently relies on a static initializer to register itself with {{o.a.h.ipc.Server}} as a valid {{RpcKind}}. There should be a way to ensure this initialization has already occurred.
HADOOP-7920,Remove Avro RPC,"Please see the discussion in HDFS-2660 for more details. I have created a branch HADOOP-6659 to save the Avro work, if in the future some one wants to use the work that existed to add support for Avro RPC."
HADOOP-7919,[Doc] Remove hadoop.logfile.* properties.,"The following only resides in core-default.xml and doesn't look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all.

These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.

{code}
137 <!--- logging properties -->
138 
139 <property>
140   <name>hadoop.logfile.size</name>
141   <value>10000000</value>
142   <description>The max size of each log file</description>
143 </property>
144 
145 <property>
146   <name>hadoop.logfile.count</name>
147   <value>10</value>
148   <description>The max number of log files</description>
149 </property>
{code}"
HADOOP-7917,compilation of protobuf files fails in windows/cygwin,"HADOOP-7899 & HDFS-2511 introduced compilation of proto files as part of the build.

Such compilation is failing in windows/cygwin"
HADOOP-7914,duplicate declaration of hadoop-hdfs test-jar,"[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-common-project:pom:0.24.0-SNAPSHOT
[WARNING] 'dependencyManagement.dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hadoop:hadoop-hdfs:test-jar -> duplicate declaration of version ${project.version} @ org.apache.hadoop:hadoop-project:0.24.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-project/pom.xml, line 140, column 19
"
HADOOP-7913,Fix bug in ProtoBufRpcEngine - ,"The parent Jira moved the multiple protocol support to lower layer; it introduced a bug: the paramCLass parameter to
#server() constructor should be null so that it uses the registered rpc request deserializers."
HADOOP-7912,test-patch should run eclipse:eclipse to verify that it does not break again,Recently the eclipse:eclipse build was broken.  If we are going to document this on the wiki and have many developers use it we should verify that it always works.
HADOOP-7910,add configuration methods to handle human readable size values,"It's better to have a new configuration methods which handle human readable size values.
For example, see HDFS-1314.

"
HADOOP-7908,Fix three javadoc warnings on branch-1,"Fix 3 javadoc warnings on branch-1:

  [javadoc] /home/eli/src/hadoop-branch-1/src/core/org/apache/hadoop/io/Sequence
File.java:428: warning - @param argument ""progress"" is not a parameter name.

  [javadoc] /home/eli/src/hadoop-branch-1/src/core/org/apache/hadoop/util/ChecksumUtil.java:32: warning - @param argument ""chunkOff"" is not a parameter name.

  [javadoc] /home/eli/src/hadoop-branch-1/src/mapred/org/apache/hadoop/mapred/QueueAclsInfo.java:52: warning - @param argument ""queue"" is not a parameter name."
HADOOP-7907,hadoop-tools JARs are not part of the distro,"After mavenizing streaming, the hadoop-streaming JAR is not part of the final tar.

"
HADOOP-7903,hadoop artifacts do not contain 64 bit libhdfs native lib,the lidhfs native library is only being built for 32 bit and not for 64. We should add it to 64 bit as well.
HADOOP-7902,skipping name rules setting (if already set) should be done on UGI initialization only ,Both TestDelegationToken and TestOfflineEditsViewer are currently failing.
HADOOP-7900,LocalDirAllocator confChanged() accesses conf.get() twice,"LocalDirAllocator.AllocatorPerContext.confChanged() accesses conf.get() twice unnecessarily. The 2 calls can give 2 different values, which can lead to issues because the first call's return value is saved in savedLocalDirs and is used for comparison in the next call to confChanged() method --- So the comparison is wrong."
HADOOP-7899,Generate proto java files as part of the build,currently the generated java files are precompiled and checked in into the source.
HADOOP-7898,Fix javadoc warnings in AuthenticationToken.java,"Fix the following javadoc warning:
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest
"
HADOOP-7897,ProtobufRPCEngine client side exception mechanism is not consistent with WritableRpcEngine,"In ProtobufRpcEngine the client side exceptions are wrapped in RpcClientException. The RpcClientException is used to create RemoteException which is set as cause in the thrown ServiceException. However WritableRpcEngine throws the client encountered exception as is. This difference in behavior causes, many tests to fail in the existing unit tests, which expect the client invoker() to thrown the exception as is. This jira makes the ProtobufRpcEngine behavior consistent with that of WritableRpcEngine."
HADOOP-7892,"IPC logs too verbose after ""RpcKind"" introduction","Recently in trunk I started seeing the following log messages on every IPC connection:
11/12/07 15:56:49 INFO ipc.Server: rpcKind=RPC_WRITABLE, rpcRequestWrapperClass=class org.apache.hadoop.ipc.WritableRpcEngine$Invocation, rpcInvoker=org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker@320cf66b
which is probably more appropriately at DEBUG or TRACE level"
HADOOP-7890,Redirect hadoop script's deprecation message to stderr,"$ hadoop dfs -ls
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
...

If we're still letting the command run, I think we should redirect the deprecation message to stderr in case users have a script taking the output from stdout.
"
HADOOP-7888,TestFailoverProxy fails intermittently on trunk,"TestFailoverProxy can fail intermittently with the failures occurring in testConcurrentMethodFailures().  The test has a race condition where the two threads may be sequentially invoking the unreliable interface rather than concurrently.  Currently the proxy provider's getProxy() method contains the thread synchronization to enforce a concurrent invocation, but examining the source to RetryInvocationHandler.invoke() shows that the call to getProxy() during failover is too late to enforce a truly concurrent invocation.

For this particular test, one thread could race ahead and block on the CountDownLatch in getProxy() before the other thread even enters RetryInvocationHandler.invoke().  If that happens the second thread will cache the newly updated value for proxyProviderFailoverCount, since the failover has mostly been processed by the original thread.  Therefore the second thread ends up assuming no other thread is present, performs a failover, and the test fails because two failovers occurred instead of one."
HADOOP-7887,KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration,"While the KerberosAuthenticatorHandler defines the name rules property, it does not set it in KerberosName."
HADOOP-7886,Add toString to FileStatus,"It would be nice if FileStatus had a reasonable toString, for debugging purposes."
HADOOP-7879,DistributedFileSystem#createNonRecursive should also incrementWriteOps statistics.,"This method:

{code}
 public FSDataOutputStream createNonRecursive(Path f, FsPermission permission,
      boolean overwrite,
      int bufferSize, short replication, long blockSize, 
      Progressable progress) throws IOException {
    return new FSDataOutputStream
        (dfs.create(getPathName(f), permission, 
                    overwrite, false, replication, blockSize, progress, bufferSize), 
         statistics);
  }
{code}

Needs a statistics.incrementWriteOps(1);"
HADOOP-7878,Regression HADOOP-7777 switch changes break HDFS tests when the isSingleSwitch() predicate is used,"This doesn't show up until you apply the HDFS-2492 patch, but the attempt to make the {{StaticMapping}} topology clever by deciding if it is single rack or multi rack based on its rack->node mapping breaks the HDFS {{TestBlocksWithNotEnoughRacks}} test. Why? Because the racks go in after the switch topology is cached by the {{BlockManager}}, which assumes the system is always single-switch.

Fix: default to assuming multi-switch; remove the intelligence, add a setter for anyone who really wants to simulate single-switch racks. 

Test: verify that a newly created simple mapping is multi switch"
HADOOP-7877,Federation: update Balancer documentation,Update Balancer documentation for the new balancing policy and CLI.
HADOOP-7876,Allow access to BlockKey/DelegationKey encoded key for RPC over protobuf,"In order to support RPC over protobuf, the BlockKey needs to provide access to encoded key. The byte[] encoded key will be transported over protobuf as byte[], instead of SecretKey."
HADOOP-7875,Add helper class to unwrap RemoteException from ServiceException thrown on protobuf based RPC,
HADOOP-7874,native libs should be under lib/native/ dir,"Currently common and hdfs SO files end up under lib/ dir with all JARs, they should end up under lib/native.

In addition, the hadoop-config.sh script needs some cleanup when comes to native lib handling:

* it is using lib/native/${JAVA_PLATFORM} for the java.library.path, when it should use lib/native.
* it is looking for build/lib/native, this is from the old ant build, not applicable anymore.
* it is looking for the libhdfs.a and adding to the java.librar.path, this is not correct.
"
HADOOP-7870,fix SequenceFile#createWriter with boolean createParent arg to respect createParent.,"After HBASE-6840, one set of calls to createNonRecursive(...) seems fishy - the new boolean createParent variable from the signature isn't used at all.  

{code}
+  public static Writer
+    createWriter(FileSystem fs, Configuration conf, Path name,
+                 Class keyClass, Class valClass, int bufferSize,
+                 short replication, long blockSize, boolean createParent,
+                 CompressionType compressionType, CompressionCodec codec,
+                 Metadata metadata) throws IOException {
+    if ((codec instanceof GzipCodec) &&
+        !NativeCodeLoader.isNativeCodeLoaded() &&
+        !ZlibFactory.isNativeZlibLoaded(conf)) {
+      throw new IllegalArgumentException(""SequenceFile doesn't work with "" +
+                                         ""GzipCodec without native-hadoop code!"");
+    }
+
+    switch (compressionType) {
+    case NONE:
+      return new Writer(conf, 
+          fs.createNonRecursive(name, true, bufferSize, replication, blockSize, null),
+          keyClass, valClass, metadata).ownStream();
+    case RECORD:
+      return new RecordCompressWriter(conf, 
+          fs.createNonRecursive(name, true, bufferSize, replication, blockSize, null),
+          keyClass, valClass, codec, metadata).ownStream();
+    case BLOCK:
+      return new BlockCompressWriter(conf,
+          fs.createNonRecursive(name, true, bufferSize, replication, blockSize, null),
+          keyClass, valClass, codec, metadata).ownStream();
+    default:
+      return null;
+    }
+  } 
+
{code}

Nicolas Spiegelberg suggests changing it to
{code} 
if (createParent) { use fs.create() } 
else { use fs.createNonRecursive(); }
{code}"
HADOOP-7869,HADOOP_HOME warning happens all of the time,"With HADOOP-7816, the check for HADOOP_HOME has moved after it is set by hadoop-config so that it always happens unless HADOOP_HOME_WARN_SUPPRESS is set in hadoop-env or the environment."
HADOOP-7868,"Hadoop native fails to compile when default linker option is -Wl,--as-needed","Recent releases of Ubuntu and Debian have switched to using --as-needed as default when linking binaries.

As a result the AC_COMPUTE_NEEDED_DSO fails to find the required DSO names during execution of configure resulting in a build failure.

Explicitly using ""-Wl,--no-as-needed"" in this macro when required resolves this issue.

See http://wiki.debian.org/ToolChain/DSOLinking for a few more details"
HADOOP-7865,Test Failures in 1.0.0 hdfs/common,"Following tests in hdfs and common are failing
1. TestFileAppend2
2. TestFileConcurrentReader
3. TestDoAsEffectiveUser "
HADOOP-7864,Building mvn site with Maven < 3.0.2 causes OOM errors,"If you try to run mvn site with Maven 3.0.0 (and possibly 3.0.1 - haven't actually tested that), you get hit with unavoidable OOM errors. Switching to Maven 3.0.2 or later fixes this. The enforcer should require 3.0.2 for builds."
HADOOP-7862,"Move the support for multiple protocols to lower layer so that Writable, PB and Avro can all use it",
HADOOP-7861,"changes2html.pl should generate links to HADOOP, HDFS, and MAPREDUCE jiras",changes2html.pl correctly generates links to HADOOP jiras only. This hasn't been updated since projects split.
HADOOP-7859,TestViewFsHdfs.testgetFileLinkStatus is failing an assert,"Probably introduced by HADOOP-7783. I'll fix it.

{noformat}
java.lang.AssertionError
	at org.apache.hadoop.fs.FileContext.qualifySymlinkTarget(FileContext.java:1111)
	at org.apache.hadoop.fs.FileContext.access$000(FileContext.java:170)
	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1142)
	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1137)
	at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2327)
	at org.apache.hadoop.fs.FileContext.getFileLinkStatus(FileContext.java:1137)
	at org.apache.hadoop.fs.FileContextTestHelper.checkFileLinkStatus(FileContextTestHelper.java:233)
	at org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testgetFileLinkStatus(ViewFsBaseTest.java:448)
{noformat}"
HADOOP-7858,"Drop some info logging to DEBUG level in IPC, metrics, and HTTP","Our info level logs have gotten noisier and noisier over time, which is annoying both for users and when looking at unit tests. I'd like to drop a few of the less useful INFO level messages down to DEBUG."
HADOOP-7854,UGI getCurrentUser is not synchronized,Sporadic {{ConcurrentModificationExceptions}} are originating from {{UGI.getCurrentUser}} when it needs to create a new instance.  The problem was specifically observed in a JT under heavy load when a post-job cleanup is accessing the UGI while a new job is being processed.
HADOOP-7853,multiple javax security configurations cause conflicts,"Both UGI and the SPNEGO KerberosAuthenticator set the global javax security configuration.  SPNEGO stomps on UGI's security config which leads to kerberos/SASL authentication errors.
"
HADOOP-7851,Configuration.getClasses() never returns the default value.,Configuration.getClasses() never returns the default value.
HADOOP-7843,compilation failing because workDir not initialized in RunJar.java,Compilation is failing on 0.23 and trunk because workDir is not initialized in RunJar.java
HADOOP-7841,Run tests with non-secure random,Post-mavenization we lost the improvement made by HADOOP-7335 which set up a system property such that Random is seeded by {{urandom}}. This makes the tests run faster and prevents timeouts due to lack of entropy on the build boxes.
HADOOP-7837,no NullAppender in the log4j config,"running sbin/start-dfs.sh gives me a telling off about no null appender -should one be in the log4j config file.

Full trace (failure expected, but full output not as expected)
{code}
./start-dfs.sh 
log4j:ERROR Could not find value for key log4j.appender.NullAppender
log4j:ERROR Could not instantiate appender named ""NullAppender"".
Incorrect configuration: namenode address dfs.namenode.servicerpc-address or dfs.namenode.rpc-address is not configured.
Starting namenodes on []
cat: /Users/slo/Java/Hadoop/versions/hadoop-0.23.0/libexec/../etc/hadoop/slaves: No such file or directory
cat: /Users/slo/Java/Hadoop/versions/hadoop-0.23.0/libexec/../etc/hadoop/slaves: No such file or directory
Secondary namenodes are not configured.  Cannot start secondary namenodes.
{code}"
HADOOP-7836,TestSaslRPC#testDigestAuthMethodHostBasedToken fails with hostname localhost.localdomain,"TestSaslRPC#testDigestAuthMethodHostBasedToken fails on branch-1 on some hosts.

null expected:<localhost[]> but was:<localhost[.localdomain]>
junit.framework.ComparisonFailure: null expected:<localhost[]> but was:<localhost[.localdomain]>

null expected:<[localhost]> but was:<[eli-thinkpad]>
junit.framework.ComparisonFailure: null expected:<[localhost]> but was:<[eli-thinkpad]>
"
HADOOP-7833,Inner classes of org.apache.hadoop.ipc.protobuf.HadoopRpcProtos generates findbugs warnings which results in -1 for findbugs,"findbugs reports the following medium priority warnings for some inner class in the generated class ./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/protobuf/HadoopRpcProtos.java:

* SE_BAD_FIELD_STORE: Non-serializable value stored into instance field of a serializable class
* SE_BAD_FIELD: Non-transient non-serializable instance field in serializable class
* UCF_USELESS_CONTROL_FLOW: Useless control flow

This can be fixed by adding the following findbugs exclude filter:

+    <Match>
+      <!-- protobuf generated code -->
+      <Class name=""~org\.apache\.hadoop\.ipc\.protobuf\.HadoopRpcProtos.*""/>
+    </Match>

which will exclude all inner classes of org.apache.hadoop.ipc.protobuf.HadoopRpcProtos"
HADOOP-7831,ConcurrentModificationException in getCurrentUser(),"I've seen the following exception from a job tracker log. Security/kerberos enabled.

{noformat}
2011-11-17 01:54:00,288 WARN org.apache.hadoop.mapred.CleanupQueue: Error deleting path
hdfs://namenode:1234/xxxx/xxxx/xxxx/job_xxxxxxxxx_xxxx
java.util.ConcurrentModificationException
        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:761)
        at java.util.LinkedList$ListItr.next(LinkedList.java:696)
        at javax.security.auth.Subject$SecureSet$1.next(Subject.java:1014)
        at javax.security.auth.Subject$ClassSet$1.run(Subject.java:1345)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject$ClassSet.populateSet(Subject.java:1342)
        at javax.security.auth.Subject$ClassSet.<init>(Subject.java:1317)
        at javax.security.auth.Subject.getPrivateCredentials(Subject.java:731)
        at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:379)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:398)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1445)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1346)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
        at org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext$1.run(CleanupQueue.java:78)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1064)
        at org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext.deletePath(CleanupQueue.java:75)
        at org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread.run(CleanupQueue.java:131)
{noformat}
"
HADOOP-7827,jsp pages missing DOCTYPE,"The various jsp pages in the UI are all missing a DOCTYPE declaration.  This causes the pages to render incorrectly on some browsers, such as IE9.  Every UI page should have a valid tag, such as <!DOCTYPE HTML>, as their first line.  There are 31 files that need to be changed, all in the core\src\webapps tree."
HADOOP-7823,port HADOOP-4012 to branch-1 (splitting support for bzip2),"Please see HADOOP-4012 - Providing splitting support for bzip2 compressed files.
"
HADOOP-7818,DiskChecker#checkDir should fail if the directory is not executable,"DiskChecker#checkDir fails if a directory can't be created, read, or written but does not fail if the directory exists and is not executable. This causes subsequent code to think the directory is OK but later fail due to an inability to access the directory (eg see MAPREDUCE-2921). I propose checkDir fails if the directory is not executable. Looking at the uses, this should be fine, I think it was ignored because checkDir is often used to create directories and it creates executable directories."
HADOOP-7817,RawLocalFileSystem.append() should give FSDataOutputStream with accurate .getPos(),"When RawLocalFileSyste.append() is called it returns an FSDataOutputStream whose .getPos() returns 0.
getPos() should return position in the file where appends will start writing.

"
HADOOP-7816,Allow HADOOP_HOME deprecated warning suppression based on config specified in hadoop-env.sh,"Move suppression check for ""Warning: $HADOOP_HOME is deprecated""  to after sourcing of hadoop-env.sh so that people can set HADOOP_HOME_WARN_SUPPRESS inside the config."
HADOOP-7815,Map memory mb is being incorrectly set by hadoop-setup-conf.sh,"HADOOP-7728 enabled task memory management to be configurable in the hadoop-setup-conf.sh. However, the default value for mapred.job.map.memory.mb is being set incorrectly."
HADOOP-7813,test-patch +1 patches that introduce javadoc and findbugs warnings in some cases,"test-patch.sh uses string comparisons of numbers to decide whether to +1 for javadoc and findbugs warnings.

decisions are made using the following construct

[[ $A > $B ]] 

Brackets put the script into conditional expression mode 

Operator definition for  conditional expression mode
       string1 > string2
              True if string1 sorts after string2 lexicographically in the current locale.

Examples
$ sh -c 'if [[ 99 > 100 ]]; then echo true; fi'
true

$ sh -c 'if [[ -99 > -10 ]]; then echo true; fi'
true

Arithmetic operations in conditional expressions are defined below
       arg1 OP arg2
              OP is one of -eq, -ne, -lt, -le, -gt, or -ge.  These arithmetic binary operators return true if arg1 is equal to, not equal to, less than, less than or equal to, greater  than,  or  greater  than  or equal to arg2, respectively.  Arg1 and arg2 may be positive or negative integers.

Alternatively arithmetic evaluation mode can be entered using double parenthesis ""(( .. ))"""
HADOOP-7811,TestUserGroupInformation#testGetServerSideGroups test fails in chroot,"It is common when running in chroot to have root's group vector preserved when running as your self.

For example

# Enter chroot
$ sudo chroot /myroot

# still root
$ whoami
root

# switch to user preserving root's group vector
$ sudo -u user -P -s

# root's groups
$ groups root
a b c

# user's real groups
$ groups user
d e f

# user's effective groups
$ groups
a b c d e f
-------------------------------
"
HADOOP-7810,move hadoop archive to core from tools,"""The HadoopArchieves classes are included in the $HADOOP_HOME/hadoop_tools.jar, but this file is not found in `hadoop classpath`.

A Pig script using HCatalog's dynamic partitioning with HAR enabled will therefore fail if a jar with HAR is not included in the pig call's '-cp' and '-Dpig.additional.jars' arguments.""

I am not aware of any reason to not include hadoop-tools.jar in 'hadoop classpath'. Will attach a patch soon."
HADOOP-7808,Port token service changes from 205,Need to merge the 205 token bug fixes and the feature to enable hostname-based tokens.
HADOOP-7806,Support binding to sub-interfaces,"Right now, with the {{DNS}} class, we can look up IPs of provided interface names ({{eth0}}, {{vm1}}, etc.). However, it would be useful if the I/F -> IP lookup also took a look at subinterfaces ({{eth0:1}}, etc.) and allowed binding to only a specified subinterface / virtual interface.

This should be fairly easy to add, by matching against all available interfaces' subinterfaces via Java."
HADOOP-7804,enable hadoop config generator to set dfs.block.local-path-access.user to enable short circuit read,we have a new config that allows to select which user can have access for short circuit read. We should make that configurable through the config generator scripts.
HADOOP-7802,"Hadoop scripts unconditionally source ""$bin""/../libexec/hadoop-config.sh.",It would be nice to be able to specify some other location for hadoop-config.sh
HADOOP-7801,HADOOP_PREFIX cannot be overriden,"hadoop-config.sh forces HADOOP_prefix to a specific value:
export HADOOP_PREFIX=`dirname ""$this""`/..

It would be nice to make this overridable.
"
HADOOP-7798,Release artifacts need to be signed for Nexus,When I uploaded hadoop-0.23.0-rc1 artifacts to Nexus it complains that artifacts aren't signed. Hence I won't be able push the release jars after the vote.
HADOOP-7797,Fix the repository name to support pushing to the staging area of Nexus,"The repository name doesn't match the old one, leading to confusion."
HADOOP-7792,Common component for HDFS-2416: Add verifyToken method to AbstractDelegationTokenSecretManager,"This captures the common component of the fix required for HDFS-2416.
A verifyToken method in AbstractDelegationTokenSecretManager is useful to verify a delegation token without rpc connection. A use case is to verify tokens passed in URL for webhdfs."
HADOOP-7789,Minor edits to top-level site,Minor edits to top-level site
HADOOP-7788,HA: Simple HealthMonitor class to watch an HAService,"This is a utility class which will be part of the FailoverController. The class starts a daemon thread which periodically monitors an HAService, calling its monitorHealth function. It then generates callbacks into another class when the health status changes (eg the RPC fails or the service returns a HealthCheckFailedException)"
HADOOP-7787,Make source tarball use conventional name.,"When building binary and source tarballs, I get the following artifacts:
Binary tarball: hadoop-0.23.0-SNAPSHOT.tar.gz 
Source tarball: hadoop-dist-0.23.0-SNAPSHOT-src.tar.gz

Notice the ""-dist"" right between ""hadoop"" and the version in the source tarball name.
"
HADOOP-7786,Remove HDFS-specific configuration keys defined in FsConfig,"HADOOP-4952 added a couple HDFS-specific configuration values to common (the block size and the replication factor) that conflict with the HDFS values (eg have the wrong defaults, wrong key name), are not used by common or hdfs and should be removed. After removing these I noticed the rest of FsConfig is only used once outside a test, and isn't tagged as a public API, I think we can remove it entirely."
HADOOP-7785,"Add equals, hashcode, toString to DataChecksum",Simple patch to add these functions to the DataChecksum interface. This is handy for the sake of HDFS-2130.
HADOOP-7784,secure datanodes fail to come up stating jsvc not found ,"building 205.1 and trying to startup a secure dn leads to the following

/usr/libexec/../bin/hadoop: line 386: /usr/libexec/../libexec/jsvc.amd64: No such file or directory
/usr/libexec/../bin/hadoop: line 386: exec: /usr/libexec/../libexec/jsvc.amd64: cannot execute: No such file or directory"
HADOOP-7783,Add more symlink tests that cover intermediate links,This covers the tests for HDFS-2514.
HADOOP-7782,Aggregate project javadocs,"With 'mvn javadoc:javadoc' we now get docs spread over the maven modules. Is there a way to stich them all together?

Also, there are some differences in their generation: hadoop-auth and hadoop-yarn-* hadoop-mapreduce-* modules goto a top-level apidocs dir which isn't the case for hadoop-common and hadoop-hdfs - they go straight to target/site/api."
HADOOP-7778,FindBugs warning in Token.getKind(),"From https://builds.apache.org/job/PreCommit-HADOOP-Build/330//artifact/trunk/hadoop-common-project/patchprocess/newPatchFindbugsWarningshadoop-common.html

bq. org.apache.hadoop.security.token.Token.getKind() is unsynchronized, org.apache.hadoop.security.token.Token.setKind(Text) is synchronized

Looks like this was introduced by MAPREDUCE-2764."
HADOOP-7777,Implement a base class for DNSToSwitchMapping implementations that can offer extra topology information,"HDFS-2492 has identified a need for DNSToSwitchMapping implementations to provide a bit more topology information (e.g. whether or not there are multiple switches). This could be done by writing an extended interface, querying its methods if present and coming up with a default action if there is no extended interface. 

Alternatively, we have a base class that all the standard mappings implement, with a boolean isMultiRack() method; all the standard subclasses would extend this, as could any third party topology provider. The advantage of this approach is that it is easier to add new operations without going into a multi-interface mess."
HADOOP-7776,Make the Ipc-Header in a RPC-Payload an explicit header,
HADOOP-7773,Add support for protocol buffer based RPC engine,This jira adds support for protocol buffer RPC engine.
HADOOP-7772,javadoc the topology classes,"To help people understand and make changes to the Topology classes, their javadocs could be rounded off."
HADOOP-7771,"NPE when running hdfs dfs -copyToLocal, -get etc",NPE when running hdfs dfs -copyToLocal if the destination directory does not exist. The behavior in branch-0.20-security is to create the directory and copy/get the contents from source.
HADOOP-7770,ViewFS getFileChecksum throws FileNotFoundException for files in /tmp and /user,"Thanks to Rohini Palaniswamy for discovering this bug. To quote
bq. When doing getFileChecksum for path /user/hadoopqa/somefile, it is trying to fetch checksum for /user/user/hadoopqa/somefile. If /tmp/file, it is trying /tmp/tmp/file. Works fine for other FS operations."
HADOOP-7766,"The auth to local mappings are not being respected, with webhdfs and security enabled.",KerberosAuthenticationHandler reloads the KerberosName statically and overrides the auth to local mappings. 
HADOOP-7765,Debian package contain both system and tar ball layout,"When packaging is invoked as ""ant clean tar deb"".  The system creates both system layout and tarball layout in the same build directory.  Debian packaging target would pick up files for both layouts.  The end result of using produced debian package built this way, would end up README.txt LICENSE.txt, and jar files in /usr."
HADOOP-7764,Allow both ACL list and global path spec filters to HttpServer,HttpServer allows setting global path spec filters in one constructor and ACL list in another constructor. Having both set in HttpServer is not user settable either by public API or constructor.
HADOOP-7763,Add top-level navigation to APT docs,We need navigation menus for the APT docs that have been written so far.
HADOOP-7761,Improve performance of raw comparisons,Guava has a nice implementation of lexicographical byte-array comparison that uses sun.misc.Unsafe to compare unsigned byte arrays long-at-a-time. Their benchmarks show it as being 2x more CPU-efficient than the equivalent pure-Java implementation. We can easily integrate this into WritableComparator.compareBytes to improve CPU performance in the shuffle.
HADOOP-7758,Make GlobFilter class public,"Currently the GlobFilter class is package private.

As a generic filter it is quite useful (and I've found myself doing cut&paste of it a few times)"
HADOOP-7754,Expose file descriptors from Hadoop-wrapped local FileSystems,"In HADOOP-7714, we determined that using fadvise inside of the MapReduce shuffle can yield very good performance improvements. But many parts of the shuffle are FileSystem-agnostic and thus operate on FSDataInputStreams and RawLocalFileSystems. This JIRA is to figure out how to allow RawLocalFileSystem to expose its FileDescriptor object without unnecessarily polluting the public APIs."
HADOOP-7753,"Support fadvise and sync_data_range in NativeIO, add ReadaheadPool class",This JIRA adds JNI wrappers for sync_data_range and posix_fadvise. It also implements a ReadaheadPool class for future use from HDFS and MapReduce.
HADOOP-7749,Add NetUtils call which provides more help in exception messages,"In setting up MR2, I accidentally had a bad configuration value specified for one of the IP configs. I was getting a NumberFormatException parsing this config, but no indication as to what config value was being fetched. This JIRA is to add an API to NetUtils.createSocketAddr which takes the configuration name, so that any exceptions thrown will point back to where the user needs to fix it."
HADOOP-7745,I switched variable names in HADOOP-7509,"As Aaron pointed out on https://issues.apache.org/jira/browse/HADOOP-7509?focusedCommentId=13126725&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13126725 I stupidly swapped CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION with CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION.

"
HADOOP-7744,Incorrect exit code for hadoop-core-test tests when exception thrown,Please see MAPREDUCE-3179 for a full description.
HADOOP-7743,Add Maven profile to create a full source tarball,"Currently we are building binary distributions only.

We should also build a full source distribution from where Hadoop can be built."
HADOOP-7740,"security audit logger is not on by default, fix the log4j properties to enable the logger",
HADOOP-7737,normalize hadoop-mapreduce & hadoop-dist dist/tar build with common/hdfs,"Normalize the build fo hadoop-mapreduce and hadoop-dist with hadoop-common and hadoop-hdfs making the -Pdist and -Dtar maven options to be consistent.

* -Pdist should create the layout
* -Dtar should create the TAR

"
HADOOP-7736,Remove duplicate call of Path#normalizePath during initialization.,"Found during code reading on HADOOP-6490, there seems to be an unnecessary call of {{normalizePath(...)}} being made in the constructor {{Path(Path, Path)}}. Since {{initialize(...)}} normalizes its received path string already, its unnecessary to do it to the path parameter in the constructor's call of the same."
HADOOP-7731,Hadoop 0.20.2-4 Deb Install hangs on Ubuntu 11.04,"deb install hangs indefinitely, will not cancel."
HADOOP-7729,Send back valid HTTP response if user hits IPC port with HTTP GET,"Often, I've seen users get confused between the IPC ports and HTTP ports for a daemon. It would be easy for us to detect when an HTTP GET request hits an IPC port, and instead of sending back garbage, we can send back a valid HTTP response explaining their mistake."
HADOOP-7728,hadoop-setup-conf.sh should be modified to enable task memory manager,The hadoop-setup-conf.sh should be modified such that task memory management is enabled.
HADOOP-7727,fix some typos and tabs in CHANGES.TXT,This is a minor edit to CHANGES.txt; giving it a JIRA issue to have complete release notes (though I'm not going to add it to the CHANGES.txt file as that would be too recursive). There are a couple of tabs and mis-spelling of the word exception in the trunk CHANGES.TXT
HADOOP-7724,hadoop-setup-conf.sh should put proxy user info into the core-site.xml ,proxy user info should go to the core-site.xml instead of the hdfs-site.xml
HADOOP-7722,Support a stop command in ShellCommandExecutor ,"Add a stop function call to Shell so as to either stop the process from getting started if not already done so or destroy the process if it already has started. 

Context of usage: 
There can be race conditions when trying to kill an execution as the shell::execute call is blocking and using the getProcess call is not a  feasible option as there can be a delay between the execute call and when the process object is actually created ( after which it can be used to kill the running sub-process ). 


"
HADOOP-7721,dfs.web.authentication.kerberos.principal expects the full hostname and does not replace _HOST with the hostname,
HADOOP-7720,improve the hadoop-setup-conf.sh to read in the hbase user and setup the configs,Read in the hbase user in the script to set the auth to local mapping appropriately in hadoop so hbase can write. If the value is not sent default to 'hbase'
HADOOP-7717,Move handling of concurrent client fail-overs to RetryInvocationHandler,"Currently every implementation of a {{FailoverProxyProvider}} will need to perform its own synchronization to ensure that multiple concurrent failed calls to a single client proxy object don't result in multiple client fail over events. It would be better to put this logic in {{RetryInvocationHandler.invoke}}.

This is based on feedback provided by Todd in [this comment|https://issues.apache.org/jira/browse/HDFS-1973?focusedCommentId=13119567&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13119567]."
HADOOP-7716,RPC protocol registration on SS does not log the protocol name (only the class which may be different),
HADOOP-7709,Running a set of methods in a Single Test Class,"Instead of running every test method in a class, limit to specific testing methods as describe in the link below.

http://maven.apache.org/plugins/maven-surefire-plugin/examples/single-test.html

Upgrade to the latest version of maven-surefire-plugin that has this feature.
"
HADOOP-7708,config generator does not update the properties file if on exists already,We are copying configs to the hadoop conf dir but are not using the -f option. This leads to conf file not getting replaced in case the file exists and thus the user never gets the new conf.
HADOOP-7707,"improve config generator to allow users to specify proxy user, turn append on or off, turn webhdfs on or off",
HADOOP-7705,"Add a log4j back end that can push out JSON data, one per line","If we had a back end for Log4j that pushed out log events in single line JSON content, we'd have something that is fairly straightforward to machine parse. If: it may be harder to do than expected. Once working HADOOP-6244 could use it."
HADOOP-7704,JsonFactory can be created only once and used for every next request to create JsonGenerator inside JMXJsonServlet ,"1. Currently JMXJsonServlet creates JsonFactory for every http request.
Its not efficient. 
JsonFactory can be created only once and used for every next request to create JsonGenerator.

2. Also following null check is not required.
{code}
 if (mBeanServer == null) {
        jg.writeStringField(""result"", ""ERROR"");
        jg.writeStringField(""message"", ""No MBeanServer could be found"");
        jg.close();
        LOG.error(""No MBeanServer could be found."");
        response.setStatus(HttpServletResponse.SC_NOT_FOUND);
        return;
 }
{code}

Because ManagementFactory.getPlatformMBeanServer(); will not return null.

3. Move the following code to finally so that any exception should not cause skipping of close method on JsonGenerator

{code}
jg.close();
{code}"
HADOOP-7703,WebAppContext should also be stopped and cleared,"1. If listener stop method throws any exception then the webserver stop method will not be called

{code}
public void stop() throws Exception {
    listener.close();
    webServer.stop();
}
{code}

2. also, WebAppContext stores all the context attributes, which does not get cleared if only webServer is stopped.
so following calls are necessary to ensure clean and complete stop.
{code}
webAppContext.clearAttributes();
webAppContext.stop();
{code}

3. Also the WebAppContext display name can be the name passed to HttpServer instance.
{code}
webAppContext.setDisplayName(name);
{code}

instead of

{code}
webAppContext.setDisplayName(""WepAppsContext"");
{code}
"
HADOOP-7698,jsvc target fails on x86_64,"Recent changes to the build.xml determine with jsvc file to download based on the os.arch.  It maps various arch values to i386 or x86_64. However, it notably doesn't consider x86_64 to be x86_64.  The result is this the download fails because {{os-arch}} doesn't expand.

{code}
build.xml:2626: Can't get http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-${os-arch}.tar.gz
{code}

This breaks {{test-patch}}."
HADOOP-7697,Remove dependency on different version of slf4j in avro,"Avro upgrade led to a mixture of slf4j versions. Hadoop uses slf4j 1.5.11, and avro brings in 1.6.1"
HADOOP-7695,RPC.stopProxy can throw unintended exception while logging error,"{{RPC.stopProxy}} includes the following lines in case of error:

{code}
LOG.error(""Could not get invocation handler "" + invocationHandler +
          "" for proxy "" + proxy + "", or invocation handler is not closeable."");
{code}

Trouble is, the {{proxy}} object is usually backed by {{WritableRpcEngine}}, which will fail in the event {{toString}} is called on one of its proxy objects. See HADOOP-7694 for more details on that issue. Until that's addressed, we might as well change the log message in {{RPC.stopProxy}} to not call {{toString()}} on {{proxy}}."
HADOOP-7693,fix RPC.Server#addProtocol to work in AvroRpcEngine,"HADOOP-7524 introduced a new way of passing protocols to RPC servers, but it was not implemented correctly by AvroRpcEngine in that issue.  This is required to fix HDFS-2298."
HADOOP-7691,hadoop deb pkg should take a diff group id,"ubuntu - 11.04 is using group id 114 for gdm.
hadoop deb pkg should pickup a different groupid."
HADOOP-7688,"When a servlet filter throws an exception in init(..), the Jetty server failed silently. ","When a servlet filter throws a ServletException in init(..), the exception is logged by Jetty but not re-throws to the caller.  As a result, the Jetty server failed silently."
HADOOP-7687,Make getProtocolSignature public ,
HADOOP-7684,jobhistory server and secondarynamenode should have init.d script,"The current set of init.d scripts can start/stop process for:

namenode
datanode
jobtracker
tasktracker

It is missing init.d scripts for:

secondarynamenode
jobhistory"
HADOOP-7683,hdfs-site.xml template has properties that are not used in 20,properties dfs.namenode.http-address and dfs.namenode.https-address should be removed
HADOOP-7681,log4j.properties is missing properties for security audit and hdfs audit should be changed to info,log4j.properties defines the security audit and hdfs audit files but is missing properties for security audit which causes security audit logs to not be present and also updates the hdfs audit to log at a WARN level. hdfs-audit logs should be at the INFO level so admin's/users can track when the namespace got the appropriate change.
HADOOP-7679,log4j.properties templates does not define mapred.jobsummary.logger,"In templates/conf/hadoop-env.sh, HADOOP_JOBTRACKER_OPTS is defined as -Dsecurity.audit.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dmapred.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}
However, in templates/conf/hadoop-env.sh, instead of mapred.jobsummary.logger, hadoop.mapreduce.jobsummary.logger is defined as follows:
hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}
This is preventing collection of jobsummary logs.

We have to consistently use mapred.jobsummary.logger in the templates.
"
HADOOP-7674,TestKerberosName fails in 20 branch.,"TestKerberosName fails in 20 branch. In fact this test has got duplicated in 20, with a little change to the rules."
HADOOP-7672,TestKerberosAuthenticator should be disabled in 20 branch.,"TestKerberosAuthenticator is disabled in trunk. It should be disabled in 20 also. 
It is not expected to pass in unit tests because it tries real kerberos login and expects a valid keytab."
HADOOP-7671,Add license headers to hadoop-common/src/main/packages/templates/conf/,hadoop-common/src/main/packages/templates/conf/ not in the exclude list for apache-rat plugin . This causes 10 release audit warnings for missing license headers (in the properties and xml files like hdfs-site.xml)
HADOOP-7669,Fix newly introduced release audit warning.,"It looks to me that HADOOP-7633 introduced one release audit warning.
we need to include Apache header in that hadoop-common-project\hadoop-common\src\main\packages\templates\conf\log4j.properties."
HADOOP-7668,Add a NetUtils method that can tell if an InetAddress belongs to local host,"This utility will be useful for mechanism proposed in HDFS-2231 where based on matching configured address with local address, a node determines the related configuration parameters."
HADOOP-7666,branch-0.20-security doesn't include o.a.h.security.TestAuthenticationFilter,Looks like the back-port of HADOOP-7119 to branch-0.20-security missed {{o.a.h.security.TestAuthenticationFilter}}.
HADOOP-7665,branch-0.20-security doesn't include SPNEGO settings in core-default.xml,Looks like back-port of HADOOP-7119 to branch-0.20-security missed the changes to {{core-default.xml}}.
HADOOP-7664,o.a.h.conf.Configuration complains of overriding final parameter even if the value with which its attempting to override is the same. ,o.a.h.conf.Configuration complains of overriding final parameter even if the value with which its attempting to override is the same. 
HADOOP-7663,TestHDFSTrash failing on 22,"Seems to have started failing recently in many commit builds as well as the last two nightly builds of 22:
https://builds.apache.org/hudson/job/Hadoop-Hdfs-22-branch/51/testReport/org.apache.hadoop.hdfs/TestHDFSTrash/testTrashEmptier/

https://issues.apache.org/jira/browse/HDFS-1967"
HADOOP-7662,logs servlet should use pathspec of /*,"The logs servlet in HttpServer should use a pathspec of /* instead of /.
      logContext.addServlet(AdminAuthorizedServlet.class, ""/*"");

In making the changes for the yarn webapps (MAPREDUCE-2999), I registered a webapp to use ""/"".  This blocked the /logs servlet from working.  because both had a pathSpec of ""/"" and the guice filter seemed to take precendence.  Changing the pathspec of the logs servlet to /* fixes the issue."
HADOOP-7658,to fix hadoop config template,"hadoop rpm config template by default sets the HADOOP_SECURE_DN_USER, HADOOP_SECURE_DN_LOG_DIR & HADOOP_SECURE_DN_PID_DIR 
the above values should only be set for secured deployment ; 
# On secure datanodes, user to run the datanode as after dropping privileges
export HADOOP_SECURE_DN_USER=${HADOOP_HDFS_USER}

# Where log files are stored.  $HADOOP_HOME/logs by default.
export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}/$USER

# Where log files are stored in the secure data environment.
export HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}

# The directory where pid files are stored. /tmp by default.
export HADOOP_PID_DIR=${HADOOP_PID_DIR}
export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
"
HADOOP-7657,Add support for LZ4 compression,"According to several benchmark sites, LZ4 seems to overtake other fast compression algorithms, especially in the decompression speed area. The interface is also trivial to integrate (http://code.google.com/p/lz4/source/browse/trunk/lz4.h) and there is no license issue."
HADOOP-7655,provide a small validation script that smoke tests the installed cluster,"currently we have scripts that will setup a hadoop cluster, create users etc. We should add a script that will smoke test the installed cluster. The script could run 3 small mr jobs teragen, terasort and teravalidate and cleanup once its done."
HADOOP-7653,tarball doesn't include .eclipse.templates,"The hadoop tarball doesn't include .eclipse.templates. This results in a failure to successfully run ant eclipse-files:

eclipse-files:

BUILD FAILED
/home/natty/Downloads/hadoop-0.20.2/build.xml:1606: /home/natty/Downloads/hadoop-0.20.2/.eclipse.templates not found.

"
HADOOP-7649,TestMapredGroupMappingServiceRefresh and TestRefreshUserMappings  fail after HADOOP-7625,"TestMapredGroupMappingServiceRefresh and TestRefreshUserMappings  fail after HADOOP-7625.
The classpath has been changed, so they try to create the rsrc file in a jar and fail.
"
HADOOP-7646,Make hadoop-common use same version of avro as HBase,"HBase depends on avro 1.5.3 whereas hadoop-common depends on 1.3.2.
When building HBase on top of hadoop, this should be consistent."
HADOOP-7645,HTTP auth tests requiring Kerberos infrastructure are not disabled on branch-0.20-security,"The back-port of HADOOP-7119 to branch-0.20-security included tests which require Kerberos infrastructure in order to run. In trunk and 0.23, these are disabled unless one enables the {{testKerberos}} maven profile. In branch-0.20-security, these tests are always run regardless, and so fail most of the time.

See this Jenkins build for an example: https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-0.20-security/26/"
HADOOP-7644,Fix the delegation token tests to use the new style renewers,"Currently, TestDelegationTokenRenewal and TestDelegationTokenFetcher use the old style renewal and fail.

"
HADOOP-7642,create hadoop-dist module where TAR stitching would happen,"Instead having a post build script that stitches common&hdfs&mmr, this should be done as part of the build when running 'mvn package -Pdist -Dtar'

"
HADOOP-7639,yarn ui not properly filtered in HttpServer,"Currently httpserver only has .html"", "".jsp as user facing urls when you add a filter. For the new web framework in yarn, the pages no longer have the *.html or *.jsp and thus they are not properly being filtered.  The yarn ui just uses paths - for it would be serve:port/yarn/*"
HADOOP-7637,Fair scheduler configuration file is not bundled in RPM,"205 build of tar is fine, but rpm failed with:

{noformat}
      [rpm] Processing files: hadoop-0.20.205.0-1
      [rpm] warning: File listed twice: /usr/libexec
      [rpm] warning: File listed twice: /usr/libexec/hadoop-config.sh
      [rpm] warning: File listed twice: /usr/libexec/jsvc.i386
      [rpm] Checking for unpackaged file(s): /usr/lib/rpm/check-files /tmp/hadoop_package_build_hortonfo/BUILD
      [rpm] error: Installed (but unpackaged) file(s) found:
      [rpm]    /etc/hadoop/fair-scheduler.xml
      [rpm]     File listed twice: /usr/libexec
      [rpm]     File listed twice: /usr/libexec/hadoop-config.sh
      [rpm]     File listed twice: /usr/libexec/jsvc.i386
      [rpm]     Installed (but unpackaged) file(s) found:
      [rpm]    /etc/hadoop/fair-scheduler.xml
      [rpm] 
      [rpm] 
      [rpm] RPM build errors:

BUILD FAILED
/grid/0/dev/mfoley/hadoop-0.20-security-205/build.xml:1747: '/usr/bin/rpmbuild' failed with exit code 1
{noformat}"
HADOOP-7635,RetryInvocationHandler should release underlying resources on close,"It is often the case that RPC invocation handlers (e.g. {{o.a.h.ipc.WritableRpcEngine.Invoker}}) are wrapped in a {{RetryInvocationHandler}} instance to handle RPC retry logic. Since {{RetryInvocationHandler}} doesn't have any resources of its own, and is incapable of releasing the resources of the wrapped {{InvocationHandler}}, users of {{RetryInvocationHandler}} must keep around a reference to the underlying {{InvocationHandler}} only for the purpose of closing. For an example of this, see {{o.a.h.hdfs.DFSClient}}, in particular the member variables {{namenode}} and {{rpcNamenode}}."
HADOOP-7634,Cluster setup docs specify wrong owner for task-controller.cfg ,The cluster setup docs indicate task-controller.cfg must be owned by the user running TaskTracker but the code checks for root. We should update the docs to reflect the real requirement.
HADOOP-7633,log4j.properties should be added to the hadoop conf on deploy,currently the log4j properties are not present in the hadoop conf dir. We should add them so that log rotation happens appropriately and also define other logs that hadoop can generate for example the audit and the auth logs as well as the mapred summary logs etc.
HADOOP-7632,NPE in copyToLocal,"[todd@c0309 hadoop-trunk-home]$ ./bin/hadoop fs -copyToLocal /hbase/.META./1028785192/ /tmp/meta/
copyToLocal: Fatal internal error
java.lang.NullPointerException
        at org.apache.hadoop.fs.shell.PathData.getPathDataForChild(PathData.java:182)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPaths(CommandWithDestination.java:115)
        at org.apache.hadoop.fs.shell.Command.recursePath(Command.java:329)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:302)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPaths(CommandWithDestination.java:116)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:272)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:255)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:239)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:105)
        at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:185)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:149)
"
HADOOP-7631,"In mapred-site.xml, stream.tmpdir is mapped to ${mapred.temp.dir} which is undeclared.","Streaming jobs seem to fail with the following exception:

{noformat}
Exception in thread ""main"" java.io.IOException: No such file or directory
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.checkAndCreate(File.java:1704)
        at java.io.File.createTempFile(File.java:1792)
        at org.apache.hadoop.streaming.StreamJob.packageJobJar(StreamJob.java:603)
        at org.apache.hadoop.streaming.StreamJob.setJobConf(StreamJob.java:798)
        at org.apache.hadoop.streaming.StreamJob.run(StreamJob.java:117)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:32)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{noformat}

Eric pointed out that in RPM based installs, in /etc/hadoop/mapred-site.xml, stream.tmpdir is mapped to ${mapred.temp.dir}, but ${mapred.temp.dir} is not declared"
HADOOP-7630,hadoop-metrics2.properties should have a property *.period set to a default value foe metrics,"currently the hadoop-metrics2.properties file does not have a value set for *.period

This property is useful for metrics to determine when the property will refresh. We should set it to default of 60"
HADOOP-7629,regression with MAPREDUCE-2289 - setPermission passed immutable FsPermission (rpc failure),"MAPREDUCE-2289 introduced the following change:

{noformat}
+        fs.setPermission(stagingArea, JOB_DIR_PERMISSION);
{noformat}

JOB_DIR_PERMISSION is an immutable FsPermission which cannot be used in RPC calls, it results in the following exception:

{noformat}
2011-09-08 16:31:45,187 WARN org.apache.hadoop.ipc.Server: Unable to read call parameters for client 127.0.0.1
java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)
        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)
        at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)
        at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()
        at java.lang.Class.getConstructor0(Class.java:2706)
        at java.lang.Class.getDeclaredConstructor(Class.java:1985)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)
        ... 8 more
{noformat}
"
HADOOP-7627,Improve MetricsAsserts to give more understandable output on failure,"In developing a test case that uses MetricsAsserts, I had two issues:
1) the error output in the case that an assertion failed does not currently give any information as to the _actual_ value of the metric
2) there is no way to retrieve the metric variable (eg to assert that the sum of a metric over all DNs is equal to some value)

This JIRA is to improve this test class to fix the above issues."
HADOOP-7626,Allow overwrite of HADOOP_CLASSPATH and HADOOP_OPTS,"Quote email from Ashutosh Chauhan:

bq. There is a bug in hadoop-env.sh which prevents hcatalog server to start in secure settings. Instead of adding classpath, it overrides them. I was not able to verify where the bug belongs to, in HMS or in hadoop scripts. Looks like hadoop-env.sh is generated from hadoop-env.sh.template in installation process by HMS. Hand crafted patch follows:

bq. - export HADOOP_CLASSPATH=$f
bq. +export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:$f

bq. -export HADOOP_OPTS=""-Djava.net.preferIPv4Stack=true ""
bq. +export HADOOP_OPTS=""${HADOOP_OPTS} -Djava.net.preferIPv4Stack=true ""
"
HADOOP-7625,TestDelegationToken is failing in 205,"After the patches on Friday, org.apache.hadoop.hdfs.security.TestDelegationToken is failing."
HADOOP-7621,alfredo config should be in a file not readable by users,"[thxs ATM for point this one out]

Alfredo configuration currently is stored in the core-site.xml file, this file is readable by users (it must be as Configuration defaults must be loaded).

One of Alfredo config values is a secret which is used by all nodes to sign/verify the authentication cookie.

A user could get hold of this secret and forge authentication cookies for other users.

Because of this the Alfredo configuration, should be move to a user non-readable file."
HADOOP-7614,Reloading configuration when using imputstream resources results in org.xml.sax.SAXParseException,"When using an inputstream as a resource for configuration, reloading this configuration will throw the following exception:

Exception in thread ""main"" java.lang.RuntimeException: org.xml.sax.SAXParseException: Premature end of file.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1576)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1445)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1381)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:569)
...
Caused by: org.xml.sax.SAXParseException: Premature end of file.
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:249)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:124)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1504)
	... 4 more

To reproduce see following testcode:
    Configuration conf = new Configuration();
    ByteArrayInputStream bais = new ByteArrayInputStream(""<configuration></configuration>"".getBytes());
    conf.addResource(bais);
    System.out.println(conf.get(""blah""));
    conf.addResource(""core-site.xml""); //just add a named resource, doesn't matter which one
    System.out.println(conf.get(""blah""));

Allowing inputstream resources is flexible, but in cases such as this in can lead to difficult to debug problems.

What do you think is the best solution? We could:
A) reset the inputstream after it is read instead of closing it (but what to do when the stream does not support marking?)
B) leave it up to the client (for example make sure you implement close() so that it resets the steam)
C) when reading the inputstream for the first time, cache or wrap the contents somehow so that is can be read multiple times (let's at least document it)
D) remove inputstream method altogether
e) something else?

For now I have attached a patch for solution A."
HADOOP-7612,Change test-patch to run tests for all nested modules,"HADOOP-7561 changed the behaviour of test-patch to run tests for changed modules, however this was assuming a flat structure. Given the nested maven hierarchy we should always run all the common tests for any common change, all the HDFS tests for any HDFS change, and all the MapReduce tests for any MapReduce change.

In addition, we should do a top-level build to test compilation after any change."
HADOOP-7610,/etc/profile.d does not exist on Debian,"As part of post installation script, there is a symlink created in /etc/profile.d/hadoop-env.sh to source /etc/hadoop/hadoop-env.sh.  Therefore, users do not need to configure HADOOP_* environment.  Unfortunately, /etc/profile.d only exists in Ubuntu.  [Section 9.9 of the Debian Policy|http://www.debian.org/doc/debian-policy/ch-opersys.html#s9.9] states:

{quote}
A program must not depend on environment variables to get reasonable defaults. (That's because these environment variables would have to be set in a system-wide configuration file like /etc/profile, which is not supported by all shells.)

If a program usually depends on environment variables for its configuration, the program should be changed to fall back to a reasonable default configuration if these environment variables are not present. If this cannot be done easily (e.g., if the source code of a non-free program is not available), the program must be replaced by a small ""wrapper"" shell script which sets the environment variables if they are not already defined, and calls the original program.

Here is an example of a wrapper script for this purpose:

{noformat}
     #!/bin/sh
     BAR=${BAR:-/var/lib/fubar}
     export BAR
     exec /usr/lib/foo/foo ""$@""
{noformat}

Furthermore, as /etc/profile is a configuration file of the base-files package, other packages must not put any environment variables or other commands into that file.
{quote}

Hence the default environment setup should skip for Debian.
"
HADOOP-7608,SnappyCodec check for Hadoop native lib is wrong,"Currently SnappyCodec is doing:

{code}
  public static boolean isNativeSnappyLoaded(Configuration conf) {
    return LoadSnappy.isLoaded() && conf.getBoolean(
        CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY,
        CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_DEFAULT);
  }
{code}

But the conf check is wrong as it defaults to true. Instead it should use *NativeCodeLoader.isNativeCodeLoaded()*"
HADOOP-7607,Simplify the RPC proxy cleanup process,"The process to clean up an RPC proxy object is to call RPC.stopProxy, which looks up the RPCEngine previously associated with the interface which that proxy object provides and calls RPCEngine.stopProxy passing in the proxy object. Every concrete implementation of RPCEngine.stopProxy then looks up the invocation handler associated with the proxy object and calls close() on that invocation handler.

This process can be simplified by cutting out the steps of looking up the previously-registered RPCEngine, and instead just having RPC.stopProxy directly look up the invocation handler for the proxy object and call close() on it."
HADOOP-7606,Upgrade Jackson to version 1.7.1 to match the version required by Jersey,"As of 2 days ago, 13 tests started failing, all with errors in Avro-related tests."
HADOOP-7604,Hadoop Auth examples pom in 0.23 point to 0.24 versions.,"hadoop-auth-examples/pom.xml has references to 0.24 in the 0.23 branch.
"
HADOOP-7603,"Set default hdfs, mapred uid, and hadoop group gid for RPM packages","Hadoop rpm package creates hdfs, mapped users, and hadoop group for automatically setting up pid directory and log directory with proper permission.  The default headless users should have a fixed uid, and gid numbers defined.

Searched through the standard uid and gid on both Redhat and Debian distro.  It looks like:

{noformat}
uid: 201 for hdfs
uid: 202 for mapred
gid: 49 for hadoop
{noformat}

would be free for use."
HADOOP-7602,"wordcount, sort etc on har files fails with NPE","wordcount, sort etc on har files fails with NPE@createSocketAddr(NetUtils.java:137). "
HADOOP-7599,Improve hadoop setup conf script to setup secure Hadoop cluster,Setting up a secure Hadoop cluster requires a lot of manual setup.  The motivation of this jira is to provide setup scripts to automate setup secure Hadoop cluster.
HADOOP-7598,smart-apply-patch.sh does not handle patching from a sub directory correctly.,"smart-apply-patch.sh does not apply valid patches from trunk, or from git like it was designed to do in some situations."
HADOOP-7596,Enable jsvc to work with Hadoop RPM package,"For secure Hadoop 0.20.2xx cluster, datanode can only run with 32 bit jvm because Hadoop only packages 32 bit jsvc.  The build process should download proper jsvc versions base on the build architecture.  In addition, the shell script should be enhanced to locate hadoop jar files in the proper location."
HADOOP-7595,Upgrade dependency to Avro 1.5.3,Avro 1.5.3 depends on Snappy-Java 1.5.3 which enables the use of its SO file from the java.library.path
HADOOP-7594,Support HTTP REST in HttpServer,"Provide an API in HttpServer for supporting HTTP REST.

This is a part of HDFS-2284."
HADOOP-7593,AssertionError in TestHttpServer.testMaxThreads(),"TestHttpServer passed but there were AssertionError in the output.
{noformat}
11/08/30 03:35:56 INFO http.TestHttpServer: HTTP server started: http://localhost:52974/
Exception in thread ""pool-1-thread-61"" java.lang.AssertionError: 
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.hadoop.http.TestHttpServer$1.run(TestHttpServer.java:164)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}
	"
HADOOP-7590,Mavenize streaming and MR examples,"MR1 code is still available in MR2 for testing contribs.

While this is a temporary until contribs tests are ported to MR2.

As a follow up the contrib projects themselves should be mavenized."
HADOOP-7589,Prefer mvn test -DskipTests over mvn compile in test-patch.sh,"I got a failure running test-patch with a clean .m2 directory.

To quote Alejandro:
{quote}
The reason for this failure is because of how Maven reactor/dependency
resolution works (IMO a bug).

Maven reactor/dependency resolution is smart enough to create the classpath
using the classes from all modules being built.

However, this smartness falls short just a bit. The dependencies are
resolved using the deepest maven phase used by current mvn invocation. If
you are doing 'mvn compile' you don't get to the test compile phase.  This
means that the TEST classes are not resolved from the build but from the
cache/repo.

The solution is to run 'mvn test -DskipTests' instead 'mvn compile'. This
will include the TEST classes from the build.
{quote}

So this is to replace mvn compile in test-patch.sh with mvn test -DskipTests"
HADOOP-7585,hadoop-config.sh should be changed to not rely on java6 behavior for classpath expansion since it breaks jsvc,hadoop-config.sh should be changed to not rely on java6 behavior for classpath expansion since it breaks jsvc - we need to add back the for loops in hadoop-config.sh which were changed in HADOOP-7563
HADOOP-7582,mvn eclipse:eclipse doesn't add generated-test-sources to build path for eclipse,"mvn eclipse:eclipse doesn't add generated-test-sources to build path for eclipse, forces user to add it manually - nice to fix."
HADOOP-7580,Add a version of getLocalPathForWrite to LocalDirAllocator which doesn't create dirs,Required in MR where directories are created by ContainerExecutor (mrv2) / TaskController (0.20) as a specific user.
HADOOP-7579,Rename package names from alfredo to auth,
HADOOP-7578,Fix test-patch to be able to run on MR patches.,
HADOOP-7576,Fix findbugs warnings in Hadoop Auth (Alfredo),Found in HADOOP-7567: https://builds.apache.org/job/PreCommit-HADOOP-Build/65//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-alfredo.html
HADOOP-7575,Support fully qualified paths as part of LocalDirAllocator,Contexts with configuration path strings using fully qualified paths (e.g. file:///tmp instead of /tmp) mistakenly creates a directory named 'file:' and sub-directories in the current local file system working directory.
HADOOP-7574,Improvement for FSshell -stat,"Add two optional formats for FSshell -stat, one is %G for group name of owner and the other is %U for user name."
HADOOP-7568,SequenceFile should not print into stdout,"The following line in {{SequenceFile.Reader.initialize()}} should be removed:
{code}
System.out.println(""Setting end to "" + end);
{code}
"
HADOOP-7566,MR tests are failing  webapps/hdfs not found in CLASSPATH,
HADOOP-7564,Remove test-patch SVN externals,"With the new top-level test-patch script in dev-support, the SVN externals for the old test-patch scripts are no longer needed."
HADOOP-7563,"hadoop-config.sh setup CLASSPATH, HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME incorrectly",HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME was set to HADOOP_PREFIX/share/hadoop/hdfs and HADOOP_PREFIX/share/hadoop/mapreduce.  This setup confuses the location of hdfs and mapred scripts.  Instead the script should look for hdfs and mapred script in HADOOP_PREFIX/bin.
HADOOP-7561,Make test-patch only run tests for changed modules,By running test-patch from trunk we can check that a change in one project (e.g. common) doesn't cause compile errors in other projects (e.g. HDFS). To get this to work we only need to run tests for the modules that are affected by the patch.
HADOOP-7560,Make hadoop-common a POM module with sub-modules (common & alfredo),"Currently hadoop-common is a JAR module, thus it cannot aggregate sub-modules.

Changing it to POM module it makes it an aggregator module, all the code under hadoop-common must be moved to a sub-module.

I.e.:

mkdir hadoop-common-project

mv hadoop-common hadoop-common-project

mv hadoop-alfredo hadoop-common-project

hadoop-common-project/pom.xml is a POM module that aggregates common & alfredo
"
HADOOP-7557,Make  IPC  header be extensible,
HADOOP-7555,Add a eclipse-generated files to .gitignore,"The .gitignore file in the hadoop-mapreduce directory specifically excludes .classpath, .settings, and .project files/dirs. We should move these excludes to the top level .gitignore so that Common and HDFS have these files excluded as well."
HADOOP-7552,FileUtil#fullyDelete doesn't throw IOE but lists it in the throws clause,"FileUtil#fullyDelete doesn't throw IOException so it shouldn't have IOException in its throws clause. Having it listed makes it easy to think you'll get an IOException eg trying to delete a non-existant file or on an IO error accessing the local file, but you don't."
HADOOP-7551,LocalDirAllocator should incorporate LocalStorage,"The o.a.h.fs.LocalDirAllocator is not aware of o.a.h.m.t.LocalStorage (introduced in MAPREDUCE-2413) - it always considers the configured local dirs, not just the ones that happen to be good. Therefore if there's a disk failure then *every* call to get a local path will result in LocalDirAllocator#confChanged doing a disk check of *all* the configured local dirs. It seems like LocalStorage should be a private class to LocalAllocator so that all users of LocalDirAllocator benefit from the disk failure handling and all the various users of LocalDirAllocator don't have to be modified to handle disk failures. Note that LocalDirAllocator already handles faulty directories."
HADOOP-7549,Use JDK ServiceLoader mechanism to find FileSystem implementations,"Currently configuring FileSystem implementations must be done by declaring the FileSystem class in the Hadoop configuration files (core-default.xml, ...).

Using JDK ServiceLoader mechanism this configuration step can be avoided. Adding the JAR file with the additional FileSystem implementation would suffice. 

This is similar to what is being proposed for compression codecs (HADOOP-7350).
"
HADOOP-7547,Fix the warning in writable classes.[ WritableComparable is a raw type. References to generic type WritableComparable<T> should be parameterized  ],"WritableComparable is a raw type. References to generic type WritableComparable<T> should be parameterized.

Also address the same in example implementation in WritableComparable interface's javadoc."
HADOOP-7545,common -tests jar should not include properties and configs,"This is the cause of HDFS-2242. The -tests jar generated from the common build should only include the test classes, and not the test resources."
HADOOP-7539,merge hadoop archive goodness from trunk to .20,"hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.
"
HADOOP-7537,Add PowerMock for the development of better tests,"We already have Mockito, but PowerMock extends its capabilties so that we can mock constructors and static methods. I find that it is extremely difficult, if not impossible, to properly test some of the low-level features without this. "
HADOOP-7536,Correct the dependency version regressions introduced in HADOOP-6671,"I just noticed the versions specified for dependencies have gone backward with HADOOP-6671.
To name a few,
* commons-logging  was 1.1.1, now 1.0.4
* commons-logging-api  was 1.1, now 1.0.4
* slf4 was 1.5.11, now 1.5.8

There might be more."
HADOOP-7533,Allow test-patch to be run from any subproject directory ,Currently dev-support/test-patch.sh can only be run from the top-level (and only for hadoop-common).
HADOOP-7531,Add servlet util methods for handling paths in requests ,Common side of HDFS-2235.
HADOOP-7529,Possible deadlock in metrics2,Lock cycle detected by jcarder between MetricsSystemImpl and DefaultMetricsSystem
HADOOP-7528,Maven build fails in Windows,"Maven does not run in window for the following reasons:

* Enforcer plugin restricts build to Unix
* Ant run snippets to create TAR are not cygwin friendly"
HADOOP-7526,Add TestPath tests for URI conversion and reserved characters  ,TestPath needs tests that cover URI conversion (eg places where Paths and URIs differ) and handling of URI reserved characters in paths. 
HADOOP-7525,Make arguments to test-patch optional,"Currently you have to specify all the arguments to test-patch.sh, which makes it cumbersome to use. We should make all arguments except the patch file optional. "
HADOOP-7524,Change RPC to allow multiple protocols including multiple versions of the same protocol,
HADOOP-7523,Test org.apache.hadoop.fs.TestFilterFileSystem fails due to java.lang.NoSuchMethodException,"Test org.apache.hadoop.fs.TestFilterFileSystem fails due to java.lang.NoSuchMethodException. Here is the error message:

-------------------------------------------------------------------------------
Test set: org.apache.hadoop.fs.TestFilterFileSystem
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.232 sec <<< FAILURE!
testFilterFileSystem(org.apache.hadoop.fs.TestFilterFileSystem)  Time elapsed: 0.075 sec  <<< ERROR!
java.lang.NoSuchMethodException: org.apache.hadoop.fs.FilterFileSystem.copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)
	at java.lang.Class.getDeclaredMethod(Class.java:1937)
	at org.apache.hadoop.fs.TestFilterFileSystem.testFilterFileSystem(TestFilterFileSystem.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:145)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)

This prevents a clean build."
HADOOP-7520,hadoop-main fails to deploy,Doing a Maven deployment hadoop-main (trunk/pom.xml) fails to deploy because it does not have the distribution management information.
HADOOP-7516,Mavenized test-patch.sh must skip tests,test-patch.sh calls mvn install with -DskipTests. Tests needs to be skipped.
HADOOP-7515,test-patch reports the wrong number of javadoc warnings,
HADOOP-7513,mvn-deploy target fails,"When executing mvn-deploy target, the build fails.
hadoop-common and hadoop-common-sources deploy, but the test jar does not.

property staging is not set and/or set to false, meaning when you try to deploy a snapshot build.

The error reads:
Invalid reference: 'hadoop.core.test'.
"
HADOOP-7512,Fix example mistake in WritableComparable javadocs,"From IRC, via uberj:

{code}
[9:58pm] uberj: http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/WritableComparable.html
[9:58pm] uberj: In the example it says ""int thatValue = ((IntWritable)o).value;""
[9:59pm] uberj: should 'o' be replaced with 'w'?
[9:59pm] uberj: int thatValue = ((IntWritable)w).value;
{code}

Attaching patch for s/w/o."
HADOOP-7510,Tokens should use original hostname provided instead of ip,Tokens currently store the ip:port of the remote server.  This precludes tokens from being used after a host's ip is changed.  Tokens should store the hostname used to make the RPC connection.  This will enable new processes to use their existing tokens.
HADOOP-7509,Improve message when Authentication is required,"The message when security is enabled and authentication is configured to be simple is not explicit enough. It simply prints out ""Authentication is required"" and prints out a stack trace. The message should be ""Authorization (hadoop.security.authorization) is enabled but authentication (hadoop.security.authentication) is configured as simple. Please configure another method."""
HADOOP-7508,compiled nativelib is in wrong directory and it is not picked up by surefire setup,"The location of the compiled native libraries differs from the one surefire plugin (run testcases) is configured to use.

This makes testcases using nativelibs to fail loading them."
HADOOP-7507,jvm metrics all use the same namespace,"Ganglia jvm metrics don't make sense because it's not clear which java process the metrics refer to. In fact, all hadoop java processes running on a node report their jvm metrics to the same namespace.

The metrics are exposed by the ""jvm"" context in JvmMetrics.java. This leads to confusing and nonsensical graphs in ganglia and maybe other monitoring tools.

One way to fix this is to make sure the process name is reported in the jvm context, making it clear which process is associated with the context, and separating out the jvm metrics per process.

This is marked as an ""incompatible change"" because the fix provided removes the JVM metrics and replaces it with process-specific metrics."
HADOOP-7504,hadoop-metrics.properties missing some Ganglia31 options ,"The ""jvm"", ""rpc"", and ""ugi"" sections of hadoop-metrics.properties should have Ganglia31 options like ""dfs"" and ""mapred"""
HADOOP-7502,Use canonical (IDE friendly) generated-sources directory for generated sources,
HADOOP-7501,publish Hadoop Common artifacts (post HADOOP-6671) to Apache SNAPSHOTs repo,"A *distributionManagement* section must be added to the hadoop-project POM with the SNAPSHOTs section, then 'mvn deploy' will push the artifacts to it."
HADOOP-7499,Add method for doing a sanity check on hostnames in NetUtils,"As part of MAPREDUCE-2489, we need a method in NetUtils to do a sanity check on hostnames"
HADOOP-7498,Remove legacy TAR layout creation,"Currently the build creates 2 different tarball layouts.

One is the legacy one, the layout used until 0.22 (ant tar &  mvn package -Ptar)

The other is new new one, the layout used in trunk that mimics the Unix layout (ant binary & mvn package -Pbintar).

The legacy layout is of not use as all the scripts have been modified to work with the new layout only.

We should thus remove the legacy layout generation.

In addition we could rename the current 'bintar' to just 'tar'
"
HADOOP-7496,break Maven TAR & bintar profiles into just LAYOUT & TAR proper,"Currently the tar & bintar profile create the layout and create tarball.

For development it would be convenient to break them into layout and tar, thus not having to pay the overhead of TARing up."
HADOOP-7493,[HDFS-362] Provide ShortWritable class in hadoop.,"As part of HDFS-362, Provide the ShortWritable class.
"
HADOOP-7491,hadoop command should respect HADOOP_OPTS when given a class name ,"When using the hadoop command HADOOP_OPTS and HADOOP_CLIENT_OPTS options are not passeed through.
"
HADOOP-7487,DF should throw a more reasonable exception when mount cannot be determined,"Currently, when using the DF class to determine the mount corresponding to a given directory, it will throw the generic exception ""Expecting a line not the end of stream"" if it can't determine the mount (for example if the directory doesn't exist).

This error message should be improved in several ways:
# If the dir to check doesn't exist, we can see that before even execing df, and throw a better exception (or behave better by chopping path components until it exists)
# Rather than parsing the lines out of df's stdout, collect the whole output, and then parse. So, if df returns a non-zero exit code, we can avoid trying to parse the empty result
# If there's a success exit code, and we still can't parse it (eg incompatible OS), we should include the unparseable line in the exception message."
HADOOP-7484,Update HDFS dependency of Java for deb package,"Java dependency for Debian package is specified as open JDK, but it should depends on Sun version of Java."
HADOOP-7479,Separate data types,
HADOOP-7475,hadoop-setup-single-node.sh is broken,"When running hadoop-setup-single-node.sh, the system can not find the templates configuration directory:

{noformat}
cat: /usr/libexec/../templates/conf/core-site.xml: No such file or directory
cat: /usr/libexec/../templates/conf/hdfs-site.xml: No such file or directory
cat: /usr/libexec/../templates/conf/mapred-site.xml: No such file or directory
cat: /usr/libexec/../templates/conf/hadoop-env.sh: No such file or directory
chown: cannot access `hadoop-env.sh': No such file or directory
chmod: cannot access `hadoop-env.sh': No such file or directory
cp: cannot stat `*.xml': No such file or directory
cp: cannot stat `hadoop-env.sh': No such file or directory
{noformat}"
HADOOP-7474,Refactor ClientCache out of WritableRpcEngine.,"This jira captures the changes in common corresponding to MAPREDUCE-2707.
Moving ClientCache out into its own class makes sense because it can be used by other RpcEngine implementations as well."
HADOOP-7473,TestDU is too sensitive to underlying filesystem,"DU currently uses ""du -ks <path>"" to determine the size. The returned number can be substantially more than the apparent size of the file, which causes the TestDU to fail."
HADOOP-7472,RPC client should deal with the IP address changes,"The current RPC client implementation and the client-side callers assume that the hostname-address mappings of servers never change. The resolved address is stored in an immutable InetSocketAddress object above/outside RPC, and the reconnect logic in the RPC Connection implementation also trusts the resolved address that was passed down.

If the NN suffers a failure that requires migration, it may be started on a different node with a different IP address. In this case, even if the name-address mapping is updated in DNS, the cluster is stuck trying old address until the whole cluster is restarted.

The RPC client-side should detect this situation and exit or try to recover.

Updating ConnectionId within the Client implementation may get the system work for the moment, there always is a risk of the cached address:port become connectable again unintentionally. The real solution will be notifying upper layer of the address change so that they can re-resolve and retry or re-architecture the system as discussed in HDFS-34. 

For 0.20 lines, some type of compromise may be acceptable. For example, raise a custom exception for some well-defined high-impact upper layer to do re-resolve/retry, while other will have to restart.  For TRUNK, the HA work will most likely determine what needs to be done.  So this Jira won't cover the solutions for TRUNK.
"
HADOOP-7471,the saveVersion.sh script sometimes fails to extract SVN URL,"When using an SVN checkout of the source, sometime the {{svn info}} command outputs a 'Copied from URL: ###' line in addition to the 'URL: ###'.

This breaks the saveVersion.sh script that assume there is only one line in the output of {{svn info}} that contains the word URL."
HADOOP-7470,move up to Jackson 1.8.8,"I see that hadoop-core still depends on Jackson 1.0.1 -but that project is now up to 1.8.2 in releases. Upgrading will make it easier for other Jackson-using apps that are more up to date to keep their classpath consistent.

The patch would be updating the ivy file to pull in the later version; no test"
HADOOP-7469,add a standard handler for socket connection problems which improves diagnostics,"connection refused, connection timed out, no route to host, etc, are classic IOExceptions that can be raised in a lot of parts of the code. The standard JDK exceptions are useless for debugging as they 
# don't include the destination (host, port) that can be used in diagnosing service dead/blocked problems
# don't include any source hostname that can be used to handle routing issues
# assume the reader understands the TCP stack.
It's obvious from the -user lists that a lot of people hit these problems and don't know how to fix them. Sometimes the source has been patched to insert the diagnostics, but it may be convenient to have a single method to translate some
{code}
SocketException processIOException(SocketException e, String destHost, int destPort) {
  String localhost = getLocalHostname();
  String details = ""From ""+ localhost +"" to ""+ desthost + "":""+destPort;
  if (e instanceof ConnectException) {
    return new ConnectException(details 
            + "" -- see http://wiki.apache.org/hadoop/ConnectionRefused --"" + e, e);
  }
  if (e instanceof UnknownHostException) {
    return new UnknownHostException(details 
            + "" -- see http://wiki.apache.org/hadoop/UnknownHost --"" + e, e);
  }
  // + handlers for other common socket exceptions
  
//and a default that returns an unknown class unchanged
  return e;
}
  
{code}

Testing: try to connect to an unknown host, a local port that isn't live, etc. It's hard to replicate all failures consistently. It may be simpler just to verify that if you pass in a specific exception, the string is expanded and the class is unchanged.

This code could then be patched in to places where IO takes place. Note that Http Components and HttpClient libs already add some destination details on some operation failures, with their own HttpException tree: it's simplest to leave these alone.
"
HADOOP-7468,hadoop-core JAR contains a log4j.properties file,"the hadoop-core JAR in the distribution and in the Maven repositories has a log4j JAR. This can break the logging of any client programs which import that JAR to do things like DFSClient work. It should be stripped from future releases. This should not impact server-side deployments, as the properties file in conf/ should be picked up instead. "
HADOOP-7465,A several tiny improvements for the LOG format,"There are several fields in the log that the space characters are missed.
For instance:
src/java/org/apache/hadoop/ipc/Client.java(248): LOG.debug(""The ping interval is"" + this.pingInterval + ""ms."");
src/java/org/apache/hadoop/fs/LocalDirAllocator.java(235):  LOG.warn( localDirs[i] + ""is not writable¥n"", de);
"
HADOOP-7463,Adding a configuration parameter to SecurityInfo interface.,"HADOOP-6929 allowed to make implementations/providers of SecurityInfo to be configurable via service class loaders. For adding Security to TunnelProtocols, configuration is needed to figure out which particular interface getKerberosInfo is called for. Just the class name is not enough since its always TunnerProtocol for all the interfaces. I propose adding a config to getKerberosInfo, so that its easy for TunnerProtocols to get the information they need."
HADOOP-7461,Jackson Dependency Not Declared in Hadoop POM,"(COMMENT: This bug still affects 0.20.205.0, four months after the bug was filed.  This causes total failure, and the fix is trivial for whoever manages the POM -- just add the missing dependency! --ben)

This issue was identified and the fix & workaround was documented at 

https://issues.cloudera.org/browse/DISTRO-44

The issue affects use of Hadoop 0.20.203.0 from the Maven central repo. I built a job using that maven repo and ran it, resulting in this failure:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/codehaus/jackson/map/JsonMappingException
	at thinkbig.hadoop.inputformat.TestXmlInputFormat.run(TestXmlInputFormat.java:18)
	at thinkbig.hadoop.inputformat.TestXmlInputFormat.main(TestXmlInputFormat.java:23)
Caused by: java.lang.ClassNotFoundException: org.codehaus.jackson.map.JsonMappingException



"
HADOOP-7460,Support for pluggable Trash policies,It would be beneficial to make the Trash policy pluggable. One primary use-case for this is to archive files (in some remote store) when they get removed by Trash emptier.
HADOOP-7457,Remove out-of-date Chinese language documentation,"The Chinese language documentation haven't been updated (other than copyright years and svn moves) since their original contribution several years ago.  Worse than no docs is out-of-date, wrong docs.  We should delete them from the source tree."
HADOOP-7454,Common side of High Availability Framework (HDFS-1623),There will likely need to be a few changes to Hadoop Common (e.g. HDFS-7380) to complete HDFS-1623 (High Availability Framework for HDFS NN). This JIRA is an umbrella for those Common changes.
HADOOP-7451,merge for MR-279: Generalize StringUtils#join,"Fix incomplete merge from yahoo-merge branch to trunk: 
-r 1079167: Generalize StringUtils::join (Chris Douglas)"
HADOOP-7450,Bump jetty to 6.1.26,"Bump the jetty version, as previous version has an issue that can cause it to hang at startup.

6.1.14 jetty is also tends to hung on heavy datanode loads."
HADOOP-7449,"merge for MR-279: add Data(In,Out)putByteBuffer to work with ByteBuffer similar to Data(In,Out)putBuffer for byte[]","Fix incomplete merge from yahoo-merge branch to trunk: 
-r 1079163: Added Data(In,Out)putByteBuffer to work with ByteBuffer similar to Data(In,Out)putBuffer for byte[]. (Chris Douglas)"
HADOOP-7448,merge for MR-279: HttpServer /stacks servlet should use plain text content type,"Fix incomplete merge from yahoo-merge branch to trunk: 
-r 1079157: Fix content type for /stacks servlet (Luke Lu)
-r 1079164: No need to escape plain text (Luke Lu)
"
HADOOP-7446,Implement CRC32C native code using SSE4.2 instructions,"Once HADOOP-7445 is implemented, we can get further performance improvements by implementing CRC32C using the hardware support available in SSE4.2. This support should be dynamically enabled based on CPU feature flags, and of course should be ifdeffed properly so that it doesn't break the build on architectures/platforms where it's not available."
HADOOP-7445,Implement bulk checksum verification using efficient native code,"Once HADOOP-7444 is implemented (""bulk"" API for checksums), good performance gains can be had by implementing bulk checksum operations using JNI. This JIRA is to add checksum support to the native libraries. Of course if native libs are not available, it will still fall back to the pure-Java implementations."
HADOOP-7444,"Add Checksum API to verify and calculate checksums ""in bulk""","Currently, the various checksum types only provide the capability to calculate the checksum of a range of a byte array. For HDFS-2080, it's advantageous to provide an API that, given a buffer with some number of ""checksum chunks"", can either calculate or verify the checksums of all of the chunks. For example, given a 4KB buffer and a 512-byte chunk size, it would calculate or verify 8 CRC32s in one call.

This allows efficient JNI-based checksum implementations since the cost of crossing the JNI boundary is amortized across many computations."
HADOOP-7443,Add CRC32C as another DataChecksum implementation,"CRC32C is another checksum very similar to our existing CRC32, but with a different polynomial. The chief advantage of this other polynomial is that SSE4.2 includes hardware support for its calculation. HDFS-2080 is the umbrella JIRA which proposes using this new polynomial to save substantial amounts of CPU."
HADOOP-7442,"Docs in core-default.xml still reference deprecated config ""topology.script.file.name""","HADOOP-6233 renamed the config ""{{topology.script.file.name}}"" to ""{{net.topology.script.file.name}}"" but missed a few spots in the docs of core-default.xml."
HADOOP-7440,HttpServer.getParameterValues throws NPE for missing parameters,"If the requested parameter was not specified in the request, the raw request's getParameterValues function returns null. Thus, trying to access {{unquoteValue.length}} throws NPE."
HADOOP-7438,Using the hadoop-deamon.sh script to start nodes leads to a depricated warning ,hadoop-daemon.sh calls common/bin/hadoop for hdfs/bin/hdfs tasks and so common/bin/hadoop complains its deprecated for those uses.
HADOOP-7437,IOUtils.copybytes will suppress the stream closure exceptions. ,"{code}

public static void copyBytes(InputStream in, OutputStream out, long count,
      boolean close) throws IOException {
    byte buf[] = new byte[4096];
    long bytesRemaining = count;
    int bytesRead;

    try {
      .............
      .............
    } finally {
      if (close) {
        closeStream(out);
        closeStream(in);
      }
    }
  }

{code}

Here if any exception in closing the stream, it will get suppressed here.

So, better to follow the stream closure pattern as HADOOP-7194.

"
HADOOP-7434,"Display error when using ""daemonlog -setlevel"" with illegal level","While using the command with inexistent ""level"" like ""nomsg"", there is no error message displayed,and the level ""DEBUG"" is set by default."
HADOOP-7432,Back-port HADOOP-7110 to 0.20-security,HADOOP-7110 implemented chmod in the NativeIO library so we can have good performance (ie not fork) and still not be prone to races. This should fix build failures (and probably task failures too).
HADOOP-7431,Test DiskChecker's functionality in identifying bad directories (Part 2 of testing DiskChecker),Add a test for the DiskChecker#checkDir method used in other projects (HDFS).
HADOOP-7430,Improve error message when moving to trash fails due to quota issue,-rm command doesn't suggest -skipTrash on failure.
HADOOP-7428,IPC connection is orphaned with null 'out' member,"We had a situation a JT ended up in a state where a certain user could not submit a job, due to an NPE on the following line in {{sendParam}}:
{code}
synchronized (Connection.this.out) {
{code}
Looking at the code, my guess is that an RTE was thrown in setupIOstreams, which only catches IOE. This could leave the connection in a half-setup state which is never cleaned up and also cannot perform IPCs."
HADOOP-7424,Log an error if the topology script doesn't handle multiple args,"ScriptBasedMapping#resolve currently warns and returns null if it passes n arguments to the topology script and gets back a different number of resolutions. This indicates a bug in the topology script (or it's input) and therefore should be an error.

{code}
// invalid number of entries returned by the script
LOG.warn(""Script "" + scriptName + "" returned ""
   + Integer.toString(m.size()) + "" values when ""
   + Integer.toString(names.size()) + "" were expected."");
return null;
{code}

There's only one place in Hadoop (FSNamesystem init) where we pass multiple arguments to the topology script, and it only done for performance (to trigger resolution/caching of all the hosts in the includes file on startup). So currently a topology script that doesn't handle multiple arguments just means the initial cache population doesn't work."
HADOOP-7419,new hadoop-config.sh doesn't manage classpath for HADOOP_CONF_DIR correctly,"Since the introduction of the RPM packages, hadoop-config.sh incorrectly puts $HADOOP_HDFS_HOME/conf on the classpath regardless of whether HADOOP_CONF_DIR is already defined in the environment."
HADOOP-7417,Hadoop Management System (Umbrella),"The primary goal of Hadoop Management System is to build a component around management and deployment of Hadoop related projects. This includes software installation, configuration, application orchestration, deployment automation and monitoring Hadoop.

Prototype demo source code can be obtained from:

http://github.com/macroadster/hms"
HADOOP-7409,TestTFileByteArrays is failing on Hudson,"This test has failed in the last 4 nightly builds, as seen here: https://builds.apache.org/job/Hadoop-Common-trunk/

I can't reproduce this failure on my machine, running the test either in isolation or as part of the full suite."
HADOOP-7407,Snappy integration breaks HDFS build.,"The common/ivy/hadoop-common-template.xml submitted with 7206 has a typo which breaks anything that depends on the hadoop-common maven package.
Instead of <artifactId>java-snappy</artifactId>, you should have <artifactId>snappy-java</artifactId>

[ivy:resolve] downloading https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common/0.23.0-SNAPSHOT/hadoop-common-0.23.0-20110620.163810-177.jar ...
[ivy:resolve] .......................
[ivy:resolve] ..................................
[ivy:resolve] ...........................................
[ivy:resolve] ...............................................................
[ivy:resolve] ................................................ (1631kB)
[ivy:resolve] .. (0kB)
[ivy:resolve] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-common;0.23.0-SNAPSHOT!hadoop-common.jar (8441ms)
[ivy:resolve] 
[ivy:resolve] :: problems summary ::
[ivy:resolve] :::: WARNINGS
[ivy:resolve] 		module not found: org.xerial.snappy#java-snappy;1.0.3-rc2
[ivy:resolve] 	==== apache-snapshot: tried
[ivy:resolve] 	  https://repository.apache.org/content/repositories/snapshots/org/xerial/snappy/java-snappy/1.0.3-rc2/java-snappy-1.0.3-rc2.pom
[ivy:resolve] 	  -- artifact org.xerial.snappy#java-snappy;1.0.3-rc2!java-snappy.jar:
[ivy:resolve] 	  https://repository.apache.org/content/repositories/snapshots/org/xerial/snappy/java-snappy/1.0.3-rc2/java-snappy-1.0.3-rc2.jar
[ivy:resolve] 	==== maven2: tried
[ivy:resolve] 	  http://repo1.maven.org/maven2/org/xerial/snappy/java-snappy/1.0.3-rc2/java-snappy-1.0.3-rc2.pom
[ivy:resolve] 	  -- artifact org.xerial.snappy#java-snappy;1.0.3-rc2!java-snappy.jar:
[ivy:resolve] 	  http://repo1.maven.org/maven2/org/xerial/snappy/java-snappy/1.0.3-rc2/java-snappy-1.0.3-rc2.jar
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: org.xerial.snappy#java-snappy;1.0.3-rc2: not found
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 
"
HADOOP-7402,TestConfiguration doesn't clean up after itself,"{{testGetFile}} and {{testGetLocalPath}} both create directories a, b, and c in the working directory from where the tests are run. They should clean up after themselves."
HADOOP-7400,HdfsProxyTests fails when the -Dtest.build.dir and -Dbuild.test is set ,"HdfsProxyTests fails when the -Dtest.build.dir and -Dbuild.test is set a dir other than build dir

test-junit:
     [copy] Copying 1 file to /home/y/var/builds/thread2/workspace/Cloud-Hadoop-0.20.1xx-Secondary/src/contrib/hdfsproxy/src/test/resources/proxy-config
    [junit] Running org.apache.hadoop.hdfsproxy.TestHdfsProxy
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hdfsproxy.TestHdfsProxy FAILED"
HADOOP-7398,create a mechanism to suppress the HADOOP_HOME deprecated warning,"Create a new mechanism to suppress the warning about HADOOP_HOME deprecation.

I'll create a HADOOP_HOME_WARN_SUPPRESS environment variable that suppresses the warning."
HADOOP-7396,"The information returned by the wrong usage of the command ""hadoop job -events <job-id> <from-event-#> <#-of-events>"" is not appropriate","With wrong value of from-event-# and #-of-events, though the from-events-# after the #-of-events for example from 1000 to 1, the command always return 0.It is expected to show detailed information, like ""the start number should be less than the end number for range of events""."
HADOOP-7395,"The infomation returned by the wrong usage of the command ""job -counter<job-id> <group-name> <counter-name> ""is not appropriate","When use the command ""job -counter<job-id> <group-name> <counter-name> "" with wrong group-name or wrong counter-name(with correct job-id), the result is always 0. It is better to show the user the detail, like ""illegal group-name"", ""illegal counter-name"", etc. "
HADOOP-7392,Implement capability of querying individual property of a mbean using JMXProxyServlet ,"Hadoop-7144 provides the capability to query all the properties of a mbean using JMXProxyServlet.  In addition to this, we add the capability to query an individual property of a mbean.  Client will send http request,

http://hostname/jmx?get=meanName::property

to query from server.  
"
HADOOP-7391,Document Interface Classification from HADOOP-5073,"The documentation for interface classification in Jira Hadoop-5073 was not copied to the Javadoc
of the classification."
HADOOP-7389,Use of TestingGroups by tests causes subsequent tests to fail,"As mentioned in HADOOP-6671, {{UserGroupInformation.createUserForTesting(...)}} manipulates static state which can cause test cases which are run after a call to this function to fail."
HADOOP-7388,Remove definition of HADOOP_HOME and HADOOP_PREFIX from hadoop-env.sh.template,"The file structure layout proposed in HADOOP-6255 was designed to remove the need of using HADOOP_HOME environment to locate hadoop bits.  The file structure layout should be able to map to /usr or system directories, therefore HADOOP_HOME is renamed to HADOOP_PREFIX to be more concise.  HADOOP_PREFIX should not be exported to the user.  If the user use hadoop-setup-single-node.sh or hadoop-setup-conf.sh to configure hadoop, the current scripts put HADOOP_PREFIX/HADOOP_HOME in hadoop-env.sh.  The config template generation code should remove reference of HADOOP_PREFIX/HADOOP_HOME from hadoop-env.sh.
"
HADOOP-7385,Remove StringUtils.stringifyException(ie) in logger functions,"Apache logger api has an overloaded function which can take the message and exception. I am proposing to clean the logging code with this api.
ie.:
Change the code from LOG.warn(msg, StringUtils.stringifyException(exception)); to LOG.warn(msg, exception);
"
HADOOP-7384,Allow test-patch to be more flexible about patch format,"Right now the test-patch process only accepts patches that are generated as ""-p0"" relative to common/, hdfs/, or mapreduce/. This has always been annoying for git users where the default patch format is -p1. It's also now annoying for SVN users who may generate a patch relative to trunk/ instead of the subproject subdirectory. We should auto-detect the correct patch level."
HADOOP-7383,HDFS needs to export protobuf library dependency in pom,"MR builds are failing since the HDFS protobuf patch went in, since they aren't picking up protobuf as a transitive dependency. I think we just need to add it to the HDFS pom template."
HADOOP-7381,FindBugs OutOfMemoryError,"When running the findbugs target from Jenkins, I get an OutOfMemory error.
The ""effort"" in FindBugs is set to Max which ends up using a lot of memory to go through all the classes. The jvmargs passed to FindBugs is hardcoded to 512 MB max.

We can leave the default to 512M, as long as we pass this as an ant parameter which can be overwritten in individual cases through -D, or in the build.properties file (either basedir, or user's home directory).
"
HADOOP-7380,Add client failover functionality to o.a.h.io.(ipc|retry),Implementing client failover will likely require changes to {{o.a.h.io.ipc}} and/or {{o.a.h.io.retry}}. This JIRA is to track those changes.
HADOOP-7379,Add ability to include Protobufs in ObjectWritable,"Per HDFS-2060, it would make it easier to piecemeal switch to protocol buffer based data structures in the wire protocol if we could intermix the two. The IPC framework currently provides the concept of ""engines"" for RPC, but that doesn't easily allow mixed types within the same framework for ease of transition.

I'd like to add the cases to ObjectWritable to be handle subclasses of {{Message}}, the superclass of codegenned protobufs."
HADOOP-7377,Fix command name handling affecting DFSAdmin,"When an error occurs in the get/set quota commands in DFSAdmin, they are displaying the following:
setQuota: failed to get SetQuotaCommand.NAME

The {{Command}} class expects the {{NAME}} field to be accessible, but for DFSAdmin, it's not."
HADOOP-7375,Add resolvePath method to FileContext,
HADOOP-7374,Don't add tools.jar to the classpath when running Hadoop,"bin/hadoop-config.sh (and bin/rcc) add lib/tools.jar from JAVA_HOME to the classpath. This has been there since the initial commit of bin/hadoop, but I don't think it's needed. *Executing* Hadoop does not depend on tools.jar (or other libraries only available in the JDK, not the JRE) so let's not automatically add it. Marking this as an incompatible change since a job could potentially have relied on Hadoop adding tools.jar to the CLASSPATH automatically (though such a job would not have run on a system that did not have JAVA_HOME point to a jdk). The build of course still requires a JDK."
HADOOP-7373,"Tarball deployment doesn't work with {start,stop}-{dfs,mapred}","The hadoop-config.sh overrides the variable ""bin"", which makes the scripts use libexec for hadoop-daemon(s)."
HADOOP-7371,Improve tarball distributions,"Hadoop release tarball contains both raw source and binary.  This leads users to use the release tarball as base for applying patches, to build custom Hadoop.  This is not the recommended method to develop hadoop because it leads to mixed development system where processed files and raw source are hard to separate.  

To correct the problematic usage of the release tarball, the release build target should be defined as:

""ant source"" generates source release tarball.
""ant binary"" is binary release without source/javadoc jar files.
""ant tar"" is a mirror of binary release with source/javadoc jar files.

Does this sound reasonable?"
HADOOP-7364,TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test,TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test. 
HADOOP-7361,Provide overwrite option (-overwrite/-f) in put and copyFromLocal command line options,"FileSystem has the API 



*public void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst)*
     
                         
This API provides overwrite option. But the mapping command line doesn't have this option. To maintain the consistency and better usage  the command line option also can support the overwrite option like to put the files forcefully. ( put [-f] <srcpath> <dstPath>) and also for copyFromLocal command line option.
"
HADOOP-7360,FsShell does not preserve relative paths with globs,FsShell currently preserves relative paths that do not contain globs.  Unfortunately the method {{fs.globStatus()}} is fully qualifying all returned paths.  This is causing inconsistent display of paths.
HADOOP-7358,Improve log levels when exceptions caught in RPC handler,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level.

I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug."
HADOOP-7357,hadoop.io.compress.TestCodec#main() should exit with non-zero exit code if test failed,"It's convenient to run something like
{noformat}
HADOOP_CLASSPATH=hadoop-test-0.20.2.jar bin/hadoop org.apache.hadoop.io.compress.TestCodec  -count 3 -codec fo
{noformat}
but the error code it returns isn't interesting.

1-line patch attached fixes that."
HADOOP-7356,RPM packages broke bin/hadoop script for hadoop 0.20.205,"hadoop-config.sh has been moved to libexec for binary package, but developers prefers to have hadoop-config.sh in bin.  Hadoo shell scripts should be modified to support both scenarios."
HADOOP-7355,Add audience and stability annotations to HttpServer class,HttpServer has at least one subclasser in HBase.  Flag this class w/ annotations that make this plain so we avoid regressions like HADOOP-7351
HADOOP-7354,"NullPointerException in the job tracker UI, when we perform kill or change the priority of jobs without selecting the any job.","If we click on Kill Selected Jobs or Change button without selecting any job, it is giving the below exception in the UI.

{code}
java.lang.NullPointerException
at org.apache.hadoop.http.HttpServer$QuotingInputFilter$RequestQuoter.getParameterValues(HttpServer.java:798)
at org.apache.hadoop.mapred.JSPUtil.processButtons(JSPUtil.java:209)
at org.apache.hadoop.mapred.jobtracker_jsp._jspService(jobtracker_jsp.java:146)
at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1124)
at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:871)
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115)
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:361)
at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
at org.mortbay.jetty.Server.handle(Server.java:324)
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:879)
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:741)
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:213)
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)
{code}"
HADOOP-7353,Cleanup FsShell and prevent masking of RTE stacktraces,"{{FsShell}}'s top level exception handler catches and displays exceptions.  Unfortunately it displays only the first line of an exception, which means an unexpected {{RuntimeExceptions}} like {{NullPointerException}} only display ""{{cmd: NullPointerException}}"".  This user has no context to understand and/or accurately report the issue.

Found due to bugs such as {{HADOOP-7327}}."
HADOOP-7350,Use ServiceLoader to discover compression codec classes,"By using a ServiceLoader users wouldn't have to add codec classes to io.compression.codecs for codecs that aren't shipped with Hadoop (e.g. LZO), since they would be automatically picked up from the classpath."
HADOOP-7349,HADOOP-7121 accidentally disabled some tests,"When I converted TestIPC to JUnit 4, I missed a couple of tests towards the bottom of the file when adding the @Test annotation."
HADOOP-7348,Modify the option of FsShell getmerge from [addnl] to [-nl] for consistency,"The [addnl] option of FsShell getmerge should be either ""true"" or ""false"",but it is very hard to understand by users, especially  who`s never used this option before. 
So,the [addnl] option should be changed to [-nl] for consistency.
"
HADOOP-7344,globStatus doesn't grok groupings with a slash,"If a glob contains a grouping with a single item that contains a slash, ex. ""{a/b}"", then globStatus throws {{""Illegal file pattern: Unclosed group near index 2""}} -- regardless of whether the path exists.  However, if the glob set contains more than one item, ex. ""{a/b,c}"", then it throws a {{NullPointerException}} from {{FileSystem.java:1277}}.

{code}
1276: FileStatus[] files = globStatusInternal(new Path(filePattern), filter);
1277: for (FileStatus file : files) {
1278:   results.add(file);
1279: }
{code}

The method {{globStatusInternal}} can return null, so the iterator fails with the NPE."
HADOOP-7343,backport HADOOP-7008 and HADOOP-7042 to branch-0.20-security,backport HADOOP-7008 and HADOOP-7042 to branch-0.20-security so that we can enable test-patch.sh to have a configured number of acceptable findbugs and javadoc warnings
HADOOP-7342,Add an utility API in FileUtil for JDK File.list,Java File.list API can return null when disk is bad or directory is not a directory. This utility API in FileUtil will throw an exception when this happens rather than returning null. 
HADOOP-7341,Fix option parsing in CommandFormat,"CommandFormat currently allows options in any location within the args.  This is not the intended behavior for FsShell commands.  Prior to the redesign, the commands used to expect option processing to stop at the first non-option.

CommandFormat was an existing class prior the redesign, but it was only used by ""count"" to find the -q flag.  All commands were converted to using this class, thus inherited the unintended behavior."
HADOOP-7339,Introduce a buffered checksum for avoiding frequently calls on Checksum.update(),"We found that PureJavaCRC32/CRC32.update() is the TOP 1 of the methods consuming CPU in a map side, and in reduce side, it cost a lots of CPU too.

IFileOutputStream would frequently call Checksum.update() during writing a record. It's very common a MR key/value less than 512 bytes. Checksum.update() would be called every time writing a key/value. 

Test case: terasort 100MB. 
Checksum.update() calls has be reduced from 4030348 to 28069.  This method is not a hotspot anymore.


"
HADOOP-7337,Annotate PureJavaCrc32 as a public API,"The API of PureJavaCrc32 is stable.  It is incorrect to annotate it as private unstable.
"
HADOOP-7336,TestFileContextResolveAfs will fail with default test.build.data property.,"In TestFileContextResolveAfs if test.build.data property is not set and default is used, the test case will try to create that in the root directory and that will fail. /tmp should be used as default as in many other test cases. Normally, test.build.data will be set and this issue should not occur."
HADOOP-7333,Performance improvement in PureJavaCrc32,"I would like to propose a small patch to 

  org.apache.hadoop.util.PureJavaCrc32.update(byte[] b, int off, int len)

Currently the method stores the intermediate result back into the data member ""crc."" I noticed this method gets
inlined into DataChecksum.update() and that method appears as one of the hotter methods in a simple hprof profile collected while running terasort and gridmix.

If the code is modified to save the temporary result into a local and just once store the final result back into the data member, it results in slightly more efficient hotspot codegen.

I tested this change using the the ""org.apache.hadoop.util.TestPureJavaCrc32$PerformanceTest"" which is embedded in the existing unit test for this class, TestPureJavaCrc32 on a variety of linux x64 AMD and Intel multi-socket and multi-core systems I have available to test.

The patch removes several stores of the intermediate result to memory yielding a 0%-10% speedup in the ""org.apache.hadoop.util.TestPureJavaCrc32$PerformanceTest"" which is embedded in the existing unit test for this class, TestPureJavaCrc32.
 
If you use a debug hotspot JVM with -XX:+PrintOptoAssembly, you can see the intermediate stores such as:

414     movq    R9, [rsp + #24] # spill
419     movl    [R9 + #12 (8-bit)], RDX # int ! Field PureJavaCrc32.crc
41d     xorl    R10, RDX        # int

The patch results in just one final store of the fully computed value.
"
HADOOP-7331,Make hadoop-daemon.sh to return 1 if daemon processes did not get started,Makes hadoop-daemon.sh to return 1 if daemon processes did not get started.
HADOOP-7330,The metrics source mbean implementation should return the attribute value instead of the object,The MetricsSourceAdapter#getAttribute in 0.20.203 is returning the attribute object instead of the value.
HADOOP-7329,incomplete help message  is displayed for df -h option,"The help message for the command ""hdfs dfs -help df"" is displayed like this:
""-df [<path> ...]:    Shows the capacity, free and used space of the filesystem.
        If the filesystem has multiple partitions, and no path to a
        particular partition is specified, then the status of the root
        partitions will be shown.""
and the information about df -h option is missed,despite the fact that df -h option is implemented.

Therefore,the expected message should be displayed like this:
""-df [-h] [<path> ...]:    Shows the capacity, free and used space of the filesystem.
        If the filesystem has multiple partitions, and no path to a
        particular partition is specified, then the status of the root
        partitions will be shown.
          -h   Formats the sizes of files in a human-readable fashion
               rather than a number of bytes."""
HADOOP-7327,FileSystem.listStatus() throws NullPointerException instead of IOException upon access permission failure,"Many processes that call listStatus() expect to handle IOException, but instead are getting runtime error NullPointerException, if the directory being scanned is visible but no-access to the running user id.  For example, if directory foo is drwxr-xr-x, and subdirectory foo/bar is drwx------, then trying to do listStatus(Path(foo/bar)) will cause a NullPointerException."
HADOOP-7324,Ganglia plugins for metrics v2,"Although, all metrics in metrics v2 are exposed via the standard JMX mechanisms, most users are using Ganglia to collect metrics."
HADOOP-7322,Adding a util method in FileUtil for JDK File.listFiles,"While testing Disk Fail Inplace, we encountered lots of NPE from Dir.listFiles API. This API can return null when Dir is not directory or disk is bad. I am proposing to have a File Util which can be used consistently across to deal with disk issues. This util api will do the following:

1. When error happens it will throw IOException
2. Else it will return empty list or list of files.

Signature:
File[] FileUtil.listFiles(File dir) throws IOException {}

This way we no need to write wrapper code every where. Also, API is consistent with the signature.
 "
HADOOP-7320,Refactor FsShell's copy & move commands,Need to refactor the move and copy commands to conform to the FsCommand class.
HADOOP-7318,MD5Hash factory should reset the digester it returns,"Currently the getDigest() method in MD5Hash does not reset the digester it returns. Since it's a thread-local, this means that a previous aborted usage of the same digester could leave some state around. For example, if the secondary namenode receives an IOException while transfering the image, and does another image transfer with the same thread, it will think it has received an invalid digest."
HADOOP-7316,Add public javadocs to FSDataInputStream and FSDataOutputStream,"This is a method made public for testing.  In comments in HADOOP-7301 after commit, adding javadoc comments was requested.  This is a follow up jira to address it."
HADOOP-7314,Add support for throwing UnknownHostException when a host doesn't resolve,"As part of MAPREDUCE-2489, we need support for having the resolve methods (for DNS mapping) throw UnknownHostExceptions.  (Currently, they hide the exception).  Since the existing 'resolve' method is ultimately used by several other locations/components, I propose we add a new 'resolveValidHosts' method."
HADOOP-7311,Port remaining metrics v1 from trunk to branch-0.20-security,HADOOP-7190 added metrics packages/classes. This is a port from trunk to make them actually work for the security branch.
HADOOP-7309,improve trademark symbol usage and add trademark footer,"We only need to mention trademarks on the first usage, add a footer specifying the trademarks, and update the pdf files."
HADOOP-7306,Start metrics system even if config files are missing,"Per experience and discussion with HDFS-1922, it seems preferable to treat missing metrics config file as empty/default config, which is more compatible with metrics v1 behavior (the MBeans are always registered.)"
HADOOP-7305,Eclipse project files are incomplete,"After a fresh checkout of hadoop-common I do 'ant compile eclipse'.
I open eclipse, set ANT_HOME and build the project. 
At that point the following error appears:
{quote}
The type com.sun.javadoc.RootDoc cannot be resolved. It is indirectly referenced from required .class files	ExcludePrivateAnnotationsJDiffDoclet.java	/common/src/java/org/apache/hadoop/classification/tools	line 1	Java Problem
{quote}

The solution is to add the ""tools.jar"" from the JDK to the buildpath/classpath.
This should be fixed in the build.xml."
HADOOP-7301,FSDataInputStream should expose a getWrappedStream method,Ideally FSDataInputStream should expose a getWrappedStream method similarly to how FSDataOutputStream exposes a getWrappedStream method.  Exposing this is useful for verifying correctness in tests cases.  This FSDataInputStream type is the class that the o.a.h.fs.FileSystem.open call returns.
HADOOP-7298,Add test utility for writing multi-threaded tests,"A lot of our tests spawn off multiple threads in order to check various synchronization issues, etc. It's often tedious to write these kinds of tests because you have to manually propagate exceptions back to the main thread, etc.

In HBase we have developed a testing utility which makes writing these kinds of tests much easier. I'd like to copy that utility into Hadoop so we can use it here as well."
HADOOP-7297,Error in the documentation regarding Checkpoint/Backup Node,"On http://hadoop.apache.org/common/docs/r0.20.203.0/hdfs_user_guide.html#Checkpoint+Node: the command bin/hdfs namenode -checkpoint required to launch the backup/checkpoint node does not exist.
I have removed this from the docs."
HADOOP-7294,FileUtil uses wrong stat command for FreeBSD,"I get next exception when try to use append:

2011-05-16 17:07:54,648 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.112.0.207:50010, storageID=DS-1047171559-
10.112.0.207-50010-1302796304164, infoPort=50075, ipcPort=50020):DataXceiver
java.io.IOException: Failed to get link count on file /var/data/hdfs/data/current/finalized/subdir26/subdir17/subdir55/blk_-1266943884751786595:
 message=null; error=stat: illegal option -- c; exit value=1
        at org.apache.hadoop.fs.FileUtil.createIOException(FileUtil.java:709)
        at org.apache.hadoop.fs.FileUtil.access$000(FileUtil.java:42)
        at org.apache.hadoop.fs.FileUtil$HardLink.getLinkCount(FileUtil.java:682)
        at org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.unlinkBlock(ReplicaInfo.java:215)
        at org.apache.hadoop.hdfs.server.datanode.FSDataset.append(FSDataset.java:1116)

It seems that FreeBSD is treated like UNIX and so calls 'stat -c%h', while FreeBSD is much more like Mac (since they have same BSD roots):

$ stat --help
stat: illegal option -- -
usage: stat [-FlLnqrsx] [-f format] [-t timefmt] [file ...]

$ stat -f%l a_file
1
"
HADOOP-7292,Metrics 2 TestSinkQueue is racy,"The TestSinkQueue is racy (Thread.yield is not enough to guarantee other intended thread getting run), though it's the first time (from HADOOP-7289) I saw it manifested here."
HADOOP-7291,Update Hudson job not to run test-contrib,"The test-contrib target was removed in HADOOP-7137, which causes the Hudson job to fail. The build file doesn't execute test-contrib so I suspect the Hudson job needs to be updated to not call ant with the test-contrib target.  "
HADOOP-7289,ivy: test conf should not extend common conf,"Otherwise, the same jars will appear in both {{build/ivy/lib/Hadoop-Common/common/}} and {{build/ivy/lib/Hadoop-Common/test/}}."
HADOOP-7287,Configuration deprecation mechanism doesn't work properly for GenericOptionsParser/Tools,"For example, you can't use -D options on the ""hadoop fs"" command line in order to specify the deprecated names of configuration options. The issue is that the ordering is:
- JVM starts
- GenericOptionsParser creates a Configuration object and calls set() for each of the options specified on command line
- DistributedFileSystem or other class eventually instantiates HdfsConfiguration which adds the deprecations
- Some class calls conf.get(""new key"") and sees the default instead of the version set on the command line"
HADOOP-7286,Refactor FsShell's du/dus/df,Need to refactor to conform to FsCommand subclass.
HADOOP-7285,Refactor FsShell's test,Need to refactor to conform to FsCommand subclass.
HADOOP-7284,Trash and shell's rm does not work for viewfs,
HADOOP-7283,Include 32-bit and 64-bit native libraries in Jenkins tarball builds,"The job at https://builds.apache.org/hudson/view/G-L/view/Hadoop/job/Hadoop-22-Build/ is building tarballs, but they do not currently include both 32-bit and 64-bit native libraries. We should update/duplicate hadoop-nighly/hudsonBuildHadoopRelease.sh to support post-split builds."
HADOOP-7282,getRemoteIp could return null in cases where the call is ongoing but the ip went away.,"getRemoteIp gets the ip from socket instead of the stored ip in Connection object. Thus calls to this function could return null when a client disconnected, but the rpc call is still ongoing..."
HADOOP-7281,update site to include the 0.20.203.0 release,Update site to include the 0.20.203.0 release.
HADOOP-7277,Add Eclipse launch tasks for the 0.20-security branch,"This is to add the eclipse launchers from HADOOP-5911 to the 0.20 security branch.

Eclipse has a notion of ""run configuration"", which encapsulates what's needed to run or debug an application. I use this quite a bit to start various Hadoop daemons in debug mode, with breakpoints set, to inspect state and what not.

This is simply configuration, so no tests are provided. After running ""ant eclipse"" and refreshing your project, you should see entries in the Run Configurations and Debug Configurations for launching the various hadoop daemons from within eclipse. There's a template for testing a specific test, and also templates to run all the tests, the job tracker, and a task tracker. It's likely that some parameters need to be further tweaked to have the same behavior as ""ant test"", but for most tests, this works.

This also requires a small change to build.xml for the eclipse classpath."
HADOOP-7276,Hadoop native builds fail on ARM due to -m32,"The native build fails on machine targets where gcc does not support -m32. This is any target other than x86, SPARC, RS/6000, or PowerPC, such as ARM.

$ ant -Dcompile.native=true
...
     [exec] make  all-am
     [exec] make[1]: Entering directory
`/home/trobinson/dev/hadoop-common/build/native/Linux-arm-32'
     [exec] /bin/bash ./libtool  --tag=CC   --mode=compile gcc
-DHAVE_CONFIG_H -I. -I/home/trobinson/dev/hadoop-common/src/native
-I/usr/lib/jvm/java-6-openjdk/include
-I/usr/lib/jvm/java-6-openjdk/include/linux
-I/home/trobinson/dev/hadoop-common/src/native/src
-Isrc/org/apache/hadoop/io/compress/zlib
-Isrc/org/apache/hadoop/security -Isrc/org/apache/hadoop/io/nativeio/
-g -Wall -fPIC -O2 -m32 -g -O2 -MT ZlibCompressor.lo -MD -MP -MF
.deps/ZlibCompressor.Tpo -c -o ZlibCompressor.lo `test -f
'src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c' || echo
'/home/trobinson/dev/hadoop-common/src/native/'`src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c
     [exec] libtool: compile:  gcc -DHAVE_CONFIG_H -I.
-I/home/trobinson/dev/hadoop-common/src/native
-I/usr/lib/jvm/java-6-openjdk/include
-I/usr/lib/jvm/java-6-openjdk/include/linux
-I/home/trobinson/dev/hadoop-common/src/native/src
-Isrc/org/apache/hadoop/io/compress/zlib
-Isrc/org/apache/hadoop/security -Isrc/org/apache/hadoop/io/nativeio/
-g -Wall -fPIC -O2 -m32 -g -O2 -MT ZlibCompressor.lo -MD -MP -MF
.deps/ZlibCompressor.Tpo -c
/home/trobinson/dev/hadoop-common/src/native/src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c
 -fPIC -DPIC -o .libs/ZlibCompressor.o
     [exec] make[1]: Leaving directory
`/home/trobinson/dev/hadoop-common/build/native/Linux-arm-32'
     [exec] cc1: error: unrecognized command line option ""-m32""
     [exec] make[1]: *** [ZlibCompressor.lo] Error 1
     [exec] make: *** [all] Error 2
"
HADOOP-7275,Refactor FsShell's stat,Refactor to conform to the FsCommand class.
HADOOP-7274,CLONE - IOUtils.readFully and IOUtils.skipFully have typo in exception creation's message,"Same fix as for HADOOP-7057 for the Hadoop security branch

{noformat}
        throw new IOException( ""Premeture EOF from inputStream"");
{noformat}"
HADOOP-7272,Remove unnecessary security related info logs,"Two info logs are printed when connection to RPC server is established, is not necessary. On a production cluster, these log lines made up of close to 50% of lines in the namenode log. I propose changing them into debug logs.


"
HADOOP-7271,Standardize error messages,"The FsShell commands have no standard format for the same error message.  For instance, here is a snippet of the variations of just one of many error messages:

cmd: $path: No such file or directory
cmd: cannot stat `$path': No such file or directory
cmd: Can not find listing for $path
cmd: Cannot access $path: No such file or directory.
cmd: No such file or directory `$path'
cmd: File does not exist: $path
cmd: File $path does not exist
... etc ...

These need to be common."
HADOOP-7268,FileContext.getLocalFSFileContext() behavior needs to be fixed w.r.t tokens,"FileContext.getLocalFSFileContext() instantiates a FileContext object upon the first call to it, and for all subsequent calls returns back that instance (a static localFsSingleton object). With security turned on, this causes some hard-to-debug situations when that fileContext is used for doing HDFS operations. This is because the UserGroupInformation is stored when a FileContext is instantiated. If the process in question wishes to use different UserGroupInformation objects for different file system operations (where the corresponding FileContext objects are obtained via calls to FileContext.getLocalFSFileContext()), it doesn't work."
HADOOP-7267,Refactor FsShell's rm/rmr/expunge,Refactor to conform to the FsCommand class.
HADOOP-7265,Keep track of relative paths,"As part of the effort to standardize the display of paths, the PathData tracks the exact string used to create a path.  When obtaining a directory's contents, the relative nature of the original path should be preserved."
HADOOP-7264,Bump avro version to at least 1.4.1,Needed by mapreduce 2.0 avro support. Maybe we could jump to Avro 1.5. There is incompatible API changes from 1.3x to 1.4x (Utf8 to CharSequence in user facing APIs) not sure about 1.5x though.
HADOOP-7261,Disable IPV6 for junit tests,"IPV6 addresses not handles currently in the common library methods. IPV6 can return address as ""0:0:0:0:0:0:port"". Some utility methods such as NetUtils#createSocketAddress(), NetUtils#normalizeHostName(), NetUtils#getHostNameOfIp() to name a few, do not handle IPV6 address and expect address to be of format host:port.

Until IPV6 is formally supported, I propose disabling IPV6 for junit tests to avoid problems seen in HDFS-1891."
HADOOP-7260,update the tlp site with the bylaws,Include the bylaws in the tlp site.
HADOOP-7259,contrib modules should include build.properties from parent.,Current build.properties in the hadoop root directory is not included by the contrib modules.
HADOOP-7258,Gzip codec should not return null decompressors,"In HADOOP-6315, the gzip codec was changed to return a null codec with the intent to disallow pooling of the decompressors. Rather than break the interface, we can use an annotation to achieve the goal."
HADOOP-7257,A client side mount table to give per-application/per-job file system view,This jira proposes a client side mount table to allow application-centric (or job-centric) filesystem views. 
HADOOP-7254,Programmatically start  processes with JMX port open ,We propose a programmatic way to start processes with JMX enabled.  This is the counter part of HDFS-1874.
HADOOP-7253,Fix default config,"Currently, we get errors about the security audit log and warnings about the metrics2 configuration being missing."
HADOOP-7251,Refactor FsShell's getmerge,Need to refactor getmerge to conform to new FsCommand class.
HADOOP-7250,Refactor FsShell's setrep,Need to refactor setrep to conform to new FsCommand class.
HADOOP-7249,Refactor FsShell's chmod/chown/chgrp,Need to refactor permissions commands to conform to new FsCommand class.
HADOOP-7248,Have a way to automatically update Eclipse .classpath file when new libs are added to the classpath through Ivy for 0.20-* based sources,Backport HADOOP-6407 into 0.20 based source trees
HADOOP-7247,Fix documentation to reflect new jar names,"In several places, we have the old jar naming style of hadoop - * - examples.jar. With Ivy and Maven, we had to rename the jars to hadoop - examples - *.jar. Therefore, we need to update the documentation."
HADOOP-7246,The default log4j configuration causes warnings about EventCounter,The default log4j configuration causes warnings about the configuration.
HADOOP-7245,FsConfig should use constants in CommonConfigurationKeys,"In particular, FsConfig should use fs.defaultFS instead of the deprecated fs.default.name."
HADOOP-7244,Documentation change for updated configuration keys,Common counterpart of HDFS-671.
HADOOP-7243,"Fix contrib unit tests (fairshare, hdfsproxy, datajoin, streaming)","Currently the unit tests for fairshare, hdfsproxy, datajoin, and streaming fail on the 203 and 204 branches with class not found exceptions."
HADOOP-7241,fix typo of command 'hadoop fs -help tail',"Fix the typo of command 'hadoop fs -help tail'.

$ hadoop fs -help tail
-tail [-f] <file>:  Show the last 1KB of the file. 
		The -f option shows apended data as the file grows. 

The ""apended data"" should be ""appended data""."
HADOOP-7238,Refactor FsShell's cat & text,Need to refactor cat & text to conform to new FsCommand class.
HADOOP-7237,Refactor FsShell's touchz,Need to refactor touchz to conform to new FsCommand class.
HADOOP-7236,Refactor FsShell's mkdir,Need to refactor tail to conform to new FsCommand class.
HADOOP-7235,Refactor FsShell's tail,Need to refactor tail to conform to new FsCommand class.
HADOOP-7233,Refactor FsShell's ls,Need to refactor ls to conform to new FsCommand class.
HADOOP-7232,Fix javadoc warnings,The javadoc is currently generating 31 warnings.
HADOOP-7231,Fix synopsis for -count,"The synopsis for the count command is wrong.
1) missing a space in ""-count[-q]""
2) missing ellipsis for multiple path args"
HADOOP-7230,Move -fs usage tests from hdfs into common,"The -fs usage tests are in hdfs which causes an unnecessary synchronization of a common & hdfs bug when changing the text.  The usages have no ties to hdfs, so they should be moved into common."
HADOOP-7229,Absolute path to kinit in auto-renewal thread,"In the auto-renewal thread for Kerberos credentials in {{UserGroupInformation}}, the path to {{kinit}} is defaulted to {{/usr/kerberos/bin/kinit}}. This is the default path to {{kinit}} on RHEL/CentOS for MIT krb5, but not on Debian/Ubuntu (and perhaps others OSes.)"
HADOOP-7227,Remove protocol version check at proxy creation in Hadoop RPC.,"Currently when a proxy is created for a protocol, there is a round trip of messages to check the protocol version. The protocol version is not checked in any subsequent rpc which could be a problem if the server restarts with a new protocol version. This issue and also the additional round-trip at proxy creation can be avoided if we add the protocol version in every rpc, and server checks the protocol version for every call."
HADOOP-7223,FileContext createFlag combinations during create are not clearly defined,"During file creation with FileContext, the expected behavior is not clearly defined for combination of createFlag EnumSet.
"
HADOOP-7216,HADOOP-7202 broke TestDFSShell in HDFS,"The commit of HADOOP-7202 now requires that classes that extend {{FsCommand}} implement the {{void run(PathData)}} method. The {{Count}} class was changed to extend {{FsCommand}}, but renamed the {{run}} method and did not provide a replacement."
HADOOP-7215,RPC clients must connect over a network interface corresponding to the host name in the client's kerberos principal key,"HADOOP-7104 introduced a change where RPC server matches client's hostname with the hostname specified in the client's Kerberos principal name. RPC client binds the socket to a random local address, which might not match the hostname specified in the principal name. This results authorization failure of the client at the server."
HADOOP-7214,Hadoop /usr/bin/groups equivalent,"Since user -> groups resolution is done on the NN and JT machines, there should be a way for users to determine what groups they're a member of from the NN's and JT's perspective."
HADOOP-7210,Chown command is not working from FSShell.,chown command is not invoking the setOwner on FileSystem.
HADOOP-7208,equals() and hashCode() implementation need to change in StandardSocketFactory,"  In Hadoop IPC Client, we are using ClientCache which will maintain the HashMap to keep the Client references.
private Map<SocketFactory, Client> clients =
      new HashMap<SocketFactory, Client>();

 Now let us say, we want use two standard factories with Hadoop. MyStandardSocketFactory (which extends StandardSocketFactory), and StandardSocketFactory. In this case, because of equals and hashcode implementation, MyStandardSocketFactory client can be overridden by StandardSocketFactoryClient
"
HADOOP-7206,Integrate Snappy compression,"Google release Zippy as an open source (APLv2) project called Snappy (http://code.google.com/p/snappy). This tracks integrating it into Hadoop.

{quote}
Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.
{quote}"
HADOOP-7205,automatically determine JAVA_HOME on OS X,OS X provides a java_home command that will return the user's selected jvm.  The hadoop-env.sh should use this command if JAVA_HOME is not set.
HADOOP-7202,Improve Command base class,Need to extend the Command base class to allow all command to easily subclass from a code set of code that correctly handles globs and exit codes.
HADOOP-7194,Potential Resource leak in IOUtils.java,"{code:title=IOUtils.java|borderStyle=solid}

try {
      copyBytes(in, out, buffSize);
    } finally {
      if(close) {
        out.close();
        in.close();
      }
    }
{code} 
In the above code if any exception throws from the out.close() statement, in.close() statement will not execute and the input stream will not be closed.
"
HADOOP-7193,Help message is wrong for touchz command.,"Help message for touchz command is
 -touchz <path>: Write a timestamp in yyyy-MM-dd HH:mm:ss format
                in a file at <path>. An error is returned if the file exists with non-zero length.

 Actually current DFS behaviour is that it will not write any time stamp in created file. Just it is creating zero size file.

So better to change the help message to give exact meaning.
"
HADOOP-7190,Put metrics v1 back into the hadoop-20-security branch,The metrics v1 code was removed on the branch. It should be put back and deprecated.
HADOOP-7187,Socket Leak in org.apache.hadoop.metrics.ganglia.GangliaContext,"Init method is creating DatagramSocket. But this is not closed any where. 
"
HADOOP-7184,Remove deprecated local.cache.size from core-default.xml,MAPREDUCE-2379 documents the new name of this parameter (mapreduce.tasktracker.cache.local.size) in mapred-default.xml where it belongs.
HADOOP-7183,WritableComparator.get should not cache comparator objects,"HADOOP-6881 modified WritableComparator.get such that the constructed WritableComparator gets saved back into the static map. This is fine for stateless comparators, but some comparators have per-instance state, and thus this becomes thread-unsafe and causes errors in the shuffle where multiple threads are doing comparisons. An example of a Comparator with per-instance state is WritableComparator itself."
HADOOP-7180,Improve CommandFormat,"CommandFormat currently takes an array and offset for parsing and returns a list of arguments.  It'd be much more convenient to have it process a list too.  It would also be nice to differentiate between too few and too many args instead of the generic ""Illegal number of arguments"".  Finally, CommandFormat is completely devoid of tests."
HADOOP-7178,FileSystem should have an option to control the .crc file creations at Local.,"When we copy the files from DFS to local, it is creating the .crc files in local filesystem for the verification of checksum. When user dont want to do any check sum verifications, this files will not be useful. 

Command line already has an option ignoreCrc, to control this.
So, we should have the similar option on FileSystem also..... like fs.ignoreCrc().
 This should set the setVerifyChecksum to false and also should select the non CheckSumFileSystem as local fs."
HADOOP-7177,CodecPool should report which compressor it is using,Certain native compression libraries are overly verbose causing confusion while reading the task logs.  Let's actually say which compressor we got when we report it in the task logs.
HADOOP-7175,Add isEnabled() to Trash,"The moveToTrash method returns false in a number of cases.  It's not possible to discern if false means an error occurred. In particular, it's not possible to know if the trash is disabled vs. an error occurred."
HADOOP-7174,"null is displayed in the console,if the src path is invalid while doing copyToLocal operation from commandLine","When we perform copyToLocal operations from commandLine and if src Path is invalid 

srcFS.globStatus(srcpath) will return null. So, when we find the length of resulted value, it will *throw NullPointerException*.

 Since we are handling generic exception , it will display null as the message.
"
HADOOP-7172,SecureIO should not check owner on non-secure clusters that have no native support,"The SecureIOUtils.openForRead function currently uses a racy stat/open combo if security is disabled and the native libraries are not available. This ends up shelling out to ""ls -ld"" which is very very slow. We've seen this cause significant performance regressions on clusters that match this profile.

Since the racy permissions check doesn't buy us any security anyway, we should just fall back to a normal ""open"" without any stat() at all, if we can't use the native support to do it efficiently."
HADOOP-7171,Support UGI in FileContext API,The FileContext API needs to support UGI.
HADOOP-7170,Support UGI in FileContext API,The FileContext API needs to support UGI.
HADOOP-7167,Allow using a file to exclude certain tests from build,"It would be nice to be able to exclude certain tests when running builds. For example, when a test is ""known flaky"", you may want to exclude it from the main Hudson job, but not actually disable it in the codebase (so that it still runs as part of another Hudson job, for example)."
HADOOP-7164,rmr command is not displaying any error message when a path contains wildcard characters and does not exist.,"When we give invalid directory path then it will show error message on the console. But if we provide the wildcard expression in invalid directory path then it will not show any error message even there is no pattern match for that path.

linux-9j5v:/home/hadoop-hdfs-0.22.0-SNAPSHOT/bin # ./hdfs dfs -rmr /test/test
rmr: cannot remove /test/test: No such file or directory.

*linux-9j5v:/home/hadoop-hdfs-0.22.0-SNAPSHOT/bin # ./hdfs dfs -rmr /test* *
*linux-9j5v:/home/hadoop-hdfs-0.22.0-SNAPSHOT/bin #*
"
HADOOP-7163,"""java.net.SocketTimeoutException: 60000 millis timeout"" happens a lot","We don't have retries for the case where the secure SASL connection is getting created from the tasks. There is retry
for TCP connections, but once the TCP connection has been set up, communication at the RPC layer (and that includes
SASL handshake) happens without retries. So for example, a client's ""read"" can timeout."
HADOOP-7162,FsShell: call srcFs.listStatus(src) twice,"in file ./src/java/org/apache/hadoop/fs/FsShell.java line 555
call method twice:
1. for init variable
2. for getting data

"
HADOOP-7159,RPC server should log the client hostname when read exception happened,This makes find mismatched clients easier
HADOOP-7156,getpwuid_r is not thread-safe on RHEL6,"Due to the following bug in SSSD, functions like getpwuid_r are not thread-safe in RHEL 6.0 if sssd is specified in /etc/nsswitch.conf (as it is by default):

https://fedorahosted.org/sssd/ticket/640

This causes many fetch failures in the case that the native libraries are available, since the SecureIO functions call getpwuid_r as part of fstat. By enabling -Xcheck:jni I get the following trace on JVM crash:

*** glibc detected *** /mnt/toolchain/JDK6u20-64bit/bin/java: free(): invalid pointer: 0x0000003575741d23 ***
======= Backtrace: =========
/lib64/libc.so.6[0x3575675676]
/lib64/libnss_sss.so.2(_nss_sss_getpwuid_r+0x11b)[0x7fe716cb42cb]
/lib64/libc.so.6(getpwuid_r+0xdd)[0x35756a5dfd]
"
HADOOP-7154,Should set MALLOC_ARENA_MAX in hadoop-config.sh,"New versions of glibc present in RHEL6 include a new arena allocator design. In several clusters we've seen this new allocator cause huge amounts of virtual memory to be used, since when multiple threads perform allocations, they each get their own memory arena. On a 64-bit system, these arenas are 64M mappings, and the maximum number of arenas is 8 times the number of cores. We've observed a DN process using 14GB of vmem for only 300M of resident set. This causes all kinds of nasty issues for obvious reasons.

Setting MALLOC_ARENA_MAX to a low number will restrict the number of memory arenas and bound the virtual memory, with no noticeable downside in performance - we've been recommending MALLOC_ARENA_MAX=4. We should set this in hadoop-env.sh to avoid this issue as RHEL6 becomes more and more common."
HADOOP-7153,MapWritable violates contract of Map interface for equals() and hashCode(),"o.a.h.io.MapWritable implements the java.util.Map interface, however it does not define an implementation of the equals() or hashCode() methods; instead the default implementations in java.lang.Object are used.

This violates the contract of the Map interface which defines different behaviour for equals() and hashCode() than Object does. More information here: http://download.oracle.com/javase/6/docs/api/java/util/Map.html#equals(java.lang.Object)

The practical consequence is that MapWritables containing equal entries cannot be compared properly. We were bitten by this when trying to write an MRUnit test for a Mapper that outputs MapWritables; the MRUnit driver cannot test the equality of the expected and actual MapWritable objects."
HADOOP-7151,Document need for stable hashCode() in WritableComparable,"When a Writable is used as a key, HashPartitioner implicitly assumes that hashCode() will return the same value across different instances of the JVM. This is not a guaranteed behavior in Java, and Object's default hashCode() does not in fact do this, which can lead to subtle bugs. This requirement should be explicitly called out.

In addition the sample MyWritable in the javadoc for WritableComparable does not implement hashCode() and thus has a bug. That should be fixed."
HADOOP-7146,RPC server leaks file descriptors,"Both the Listener and Responder thread call Selector.open but don't have a matching .close(). This causes a leak of anonymous pipes. Not a big deal because people rarely close and re-open servers, but worth fixing."
HADOOP-7145,Configuration.getLocalPath should trim whitespace from the provided directories,"MR and HDFS use the Configuration.getTrimmedStrings API for local directory lists, but in a few places also use Configuration.getLocalPath. The former API trims whitespace around each entry in the list, but the latter doesn't. This can cause some subtle problems - the latter API should be fixed to also trim the directory names."
HADOOP-7144,Expose JMX with something like JMXProxyServlet ,"Much of the Hadoop metrics and status info is available via JMX, especially since 0.20.100, and 0.22+ (HDFS-1318, HADOOP-6728 etc.) For operations staff not familiar JMX setup, especially JMX with SSL and firewall tunnelling, the usage can be daunting. Using a JMXProxyServlet (a la Tomcat) to translate JMX attributes into JSON output would make a lot of non-Java admins happy.

We could probably use Tomcat's JMXProxyServlet code directly, if it's already output some standard format (JSON or XML etc.) The code is simple enough to port over and can probably integrate with the common HttpServer as one of the default servelet (maybe /jmx) for the pluggable security."
HADOOP-7143,Hive Hadoop20SShims depends on removed HadoopArchives,"Compiling (Hive 0.6 + HIVE-1264) or Hive-trunk against 0.20.100 fails compilation.
/hive/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java depends on o.a.h.tools.HadoopArchives which was removed from 0.20.100.

HadoopArchives in turn depends on src/core/o.a.h/fs/HarFileSystem.java which was also removed from 0.20.100."
HADOOP-7140,IPC Reader threads do not stop when server stops,"After HADOOP-6713, the new IPC ""Reader"" threads are not properly stopped when the server shuts down. One repercussion of this is that conditions that are supposed to shut down a daemon no longer work (eg the TT doesn't shut itself down if it detects an incompatible build version)"
HADOOP-7139,Allow appending to existing SequenceFiles,
HADOOP-7138,Remove contrib build and test support,"With the removal of hod and failmon, these files are no longer needed:
src/contrib/test/*
src/contrib/build.xml
src/contrib/build-contrib.xml
In addition, the contrib support in the top level build.xml should be removed."
HADOOP-7137,Remove hod contrib,"As per vote on general@ (http://mail-archives.apache.org/mod_mbox/hadoop-general/201102.mbox/%3CAC35A7EF-1D68-4055-8D47-EDA2FCF8C2F6@mac.com%3E) I will 
svn remove common/trunk/src/contrib/hod
using this Jira.
"
HADOOP-7136,Remove failmon contrib,"As per vote on general@ (http://mail-archives.apache.org/mod_mbox/hadoop-general/201102.mbox/%3CC98FD122-F11B-4A3F-8D34-3C547392F881@mac.com%3E) I will 
svn remove common/trunk/src/contrib/failmon
using this Jira."
HADOOP-7134,configure files that are generated as part of the released tarball need to have executable bit set,Currently the configure files that are packaged in a tarball are -rw-rw-r--
HADOOP-7133,"CLONE to COMMON - HDFS-1445 Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file","The fix for HDFS-1445 ""Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file"" requires coordinated change in COMMON and HDFS.  This is the COMMON portion, submitted here under a separate bug to activate the automated testing.

Warning: this patch to COMMON, by itself, will break HDFS.  It requires coordinated commit of the HDFS portion of the patch in HDFS-1445."
HADOOP-7131,"set() and toString Methods of the org.apache.hadoop.io.Text class does not include the root exception, in the wrapping RuntimeException."," In below code snippets, we can include e, instead of e.toString(), so that caller can get complete trace.

1) 
   /** Set to contain the contents of a string.
   */
  public void set(String string) {
    try {
      ByteBuffer bb = encode(string, true);
      bytes = bb.array();
      length = bb.limit();
    }catch(CharacterCodingException e) {
      throw new RuntimeException(""Should not have happened "",e.toString());
    }
  } 
2)
   public String toString() {
    try {
      return decode(bytes, 0, length);
    } catch (CharacterCodingException e) {
      throw new RuntimeException(""Should not have happened "",e.toString());
    }
  }
"
HADOOP-7126,TestDFSShell fails in trunk,"TestDFSShell.testFilePermissions() fails on an assert in Windows.  This originated from HDFS-1084 but the fix is in Common.

{noformat}
junit.framework.ComparisonFailure: null expected:<rwxr[w----]> but was:<rwxr[-xr-x]>
	at junit.framework.Assert.assertEquals(Assert.java:81)
	at junit.framework.Assert.assertEquals(Assert.java:87)
	at org.apache.hadoop.hdfs.TestDFSShell.confirmPermissionChange(TestDFSShell.java:836)
	at org.apache.hadoop.hdfs.TestDFSShell.testChmod(TestDFSShell.java:777)
	at org.apache.hadoop.hdfs.TestDFSShell.testFilePermissions(TestDFSShell.java:856)
{noformat}"
HADOOP-7122,Timed out shell commands leak Timer threads,"When a shell command times out, the TimerThread used to cause the timeout is leaked."
HADOOP-7121,Exceptions while serializing IPC call response are not handled well,"We had a situation where for some reason the serialization of an RPC call's response was throwing OOME. When this happens, the exception is not caught, and the call never gets a response - the client just hangs. Additionally, the OOME propagated all the way to the top of the IPC handler and caused the handler. Plus, the Handler upon exit only logged to stdout and not to the log4j logs."
HADOOP-7120,200 new Findbugs warnings,"ant test-patch on an empty patch over hdfs trunk.
{noformat}
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no new tests are needed for this patch.
     [exec]                         Also please list what manual steps were performed to verify this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     -1 findbugs.  The patch appears to introduce 200 new Findbugs (version 1.3.9) warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 1 release audit warnings (more than the trunk's current 0 warnings).
     [exec] 
     [exec]     +1 system test framework.  The patch passed system test framework compile.
{noformat}"
HADOOP-7119,add Kerberos HTTP SPNEGO authentication support to Hadoop JT/NN/DN/TT web-consoles,"Currently the JT/NN/DN/TT web-consoles don't support any form of authentication.

Hadoop RPC API already supports Kerberos authentication.

Kerberos enables single sign-on.

Popular browsers (Firefox and Internet Explorer) have support for Kerberos HTTP SPNEGO.

Adding support for Kerberos HTTP SPNEGO to Hadoop web consoles would provide a unified authentication mechanism and single sign-on for Hadoop web UI and Hadoop RPC."
HADOOP-7118,NPE in Configuration.writeXml,"In HADOOP-7082 I stupidly introduced a regression whereby Configuration.writeXml will throw an NPE if it is called before any .get() call is made, since the properties member is not initialized. This is causing a failure in TestCapacitySchedulerWithJobTracker on my box, but doesn't appear to trigger any failures in the non-contrib tests since .get() is usually called first.

This JIRA is to fix the bug and add a unit test for writeXml in common (apparently it never had a unit test)"
HADOOP-7117,Move secondary namenode checkpoint configs from core-default.xml to hdfs-default.xml,"
The following configs are in core-default.xml, but are really read by the Secondary Namenode. These should be moved to hdfs-default.xml for consistency.
<property>
<name>fs.checkpoint.dir</name>
<value>${hadoop.tmp.dir}/dfs/namesecondary</value>
<description>Determines where on the local filesystem the DFS secondary
name node should store the temporary images to merge.
If this is a comma-delimited list of directories then the image is
replicated in all of the directories for redundancy.
</description>
</property>

<property>
<name>fs.checkpoint.edits.dir</name>
<value>${fs.checkpoint.dir}</value>
<description>Determines where on the local filesystem the DFS secondary
name node should store the temporary edits to merge.
If this is a comma-delimited list of directoires then teh edits is
replicated in all of the directoires for redundancy.
Default value is same as fs.checkpoint.dir
</description>
</property>

<property>
<name>fs.checkpoint.period</name>
<value>3600</value>
<description>The number of seconds between two periodic checkpoints.
</description>
</property>

<property>
<name>fs.checkpoint.size</name>
<value>67108864</value>
<description>The size of the current edit log (in bytes) that triggers
a periodic checkpoint even if the fs.checkpoint.period hasn't expired.
</description>
</property>

"
HADOOP-7115,Add a cache for getpwuid_r and getpwgid_r calls,"As discussed in HADOOP-6978, a cache helps a lot."
HADOOP-7114,FsShell should dump all exceptions at DEBUG level,"Most of the FsShell commands catch exceptions and then just print out an error like ""foo: "" + e.getLocalizedMessage(). This is fine when the exception is ""user-facing"" (eg permissions errors) but in the case of a user hitting a bug you get a useless error message with no stack trace. For example, something ""chmod: null"" in the case of a NullPointerException bug.

It would help debug these cases for users and developers if we also logged the exception with full trace at DEBUG level."
HADOOP-7112,Issue a warning when GenericOptionsParser libjars are not on local filesystem,In GenericOptionsParser#getLibJars() any jars that are not local filesystem paths are silently ignored. We should issue a warning for users.
HADOOP-7111,Several TFile tests failing when native libraries are present,"When running tests with native libraries present, TestTFileByteArrays and TestTFileJClassComparatorByteArrays fail on trunk. They don't seem to fail in 0.20 with native libraries."
HADOOP-7110,Implement chmod with JNI,MapReduce is currently using a race-prone workaround to approximate chmod() because forking chmod is too expensive. This race is causing build failures (and probably task failures too). We should implement chmod in the NativeIO library so we can have good performance (ie not fork) and still not be prone to races.
HADOOP-7108,hadoop-0.20.100,Jira to track merging in Yahoo security patchset to Apache Hadoop 0.20
HADOOP-7106,Re-organize hadoop subversion layout,As discussed on general@ at http://tinyurl.com/4q6lhxm
HADOOP-7104,Remove unnecessary DNS reverse lookups from RPC layer,"RPC connection authorization needs to verify client's Kerberos principal name matches what specified for the protocol. For service clients like DN's, their Kerberos principal names can be specified in the form of  ""datanode/_HOST@DOMAIN.COM"". To get the expected
client principal name, the server needs to substitute ""_HOST"" with the client's fully qualified domain name, which requires a reverse DNS lookup from client IP address. However, for connections from clients whose principal name are either unspecified or specified not using the ""_HOST"" convention, the substitution is not required and the reverse DNS lookup should be avoided. Currently the reverse DNS lookup is done for all clients, which could slow services like NN down, when local named cache is not available."
HADOOP-7102,"Remove ""fs.ramfs.impl"" field from core-deafult.xml","""fs.ramfs.impl"" used to be configuration parameter for InMemoryFileSystem, which was deprecated in 0.18 (HADOOP-3501) and removed in 0.21 (HADOOP-4648). Configuration should have been cleaned up at the time."
HADOOP-7101,UserGroupInformation.getCurrentUser() fails when called from non-Hadoop JAAS context,"If a Hadoop client is run from inside a container like Tomcat, and the current AccessControlContext has a Subject associated with it that is not created by Hadoop, then UserGroupInformation.getCurrentUser() will throw NoSuchElementException, since it assumes that any Subject will have a hadoop User principal."
HADOOP-7100,Build broken by HADOOP-6811,"The commit of HADOOP-6811 removed the ec2 contrib but didn't update build.xml, which references some of these files from the packaging targets. So, the hudson build is currently broken."
HADOOP-7098,tasktracker property not set in conf/hadoop-env.sh,"For all cluster components, except TaskTracker the OPTS environment variable is set like this in hadoop-env.sh:
export HADOOP_<COMPONENT>_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_<COMPONENT>_OPTS""

The provided patch fixes this."
HADOOP-7097,java.library.path missing basedir,"My Hadoop installation is  having trouble loading the native code library. It appears from the log below that java.library.path is missing the basedir in its path. The libraries are built, and present in the directory shown below (relative to hadoop-common directory). Instead of seeing:

 /build/native/Linux-amd64-64/lib

I would expect to see:

 /path/to/hadoop-common/build/native/Linux-amd64-64/lib

I'm working in branch-0.22.

2011-01-10 17:09:27,695 DEBUG org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
2011-01-10 17:09:27,695 DEBUG org.apache.hadoop.util.NativeCodeLoader: java.library.path=/build/native/Linux-amd64-64/lib
2011-01-10 17:09:27,695 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
HADOOP-7096,Allow setting of end-of-record delimiter for TextInputFormat,"The patch for https://issues.apache.org/jira/browse/MAPREDUCE-2254 required minor changes to the LineReader class to allow extensions (see attached 2.patch). Description copied below:

It will be useful to allow setting the end-of-record delimiter for TextInputFormat. The current implementation hardcodes '\n', '\r' or '\r\n' as the only possible record delimiters. This is a problem if users have embedded newlines in their data fields (which is pretty common). This is also a problem for other tools using this TextInputFormat (See for example: https://issues.apache.org/jira/browse/PIG-836 and https://issues.cloudera.org/browse/SQOOP-136).
I have wrote a patch to address this issue. This patch allows users to specify any custom end-of-record delimiter using a new added configuration property. For backward compatibility, if this new configuration property is absent, then the same exact previous delimiters are used (i.e., '\n', '\r' or '\r\n').

"
HADOOP-7094,hadoop.css got lost during project split,"hadoop.css no longer exists in common or HDFS, so the web UIs look pretty ugly. The HTML still refers to this file, it's just gone."
HADOOP-7093,Servlets should default to text/plain,In trunk the servlets like /stacks and /metrics are returning text/html content-type instead of text/plain. Security wise it's much safer to default to text/plain and require servlets to explicitly set the content-type to text/html when required.
HADOOP-7091,reloginFromKeytab() should happen even if TGT can't be found,"HADOOP-6965 introduced a getTGT() method and prevents reloginFromKeytab() from happening when TGT is not found. This results in the RPC layer not being able to refresh TGT after TGT expires. The reason is RPC layer only does relogin when the expired TGT is used and an exception is thrown. However, when that happens, the expired TGT will be removed from Subject. Therefore, getTGT() will return null and relogin will not be performed. We observed, for example, JT will not be able to re-connect to NN after TGT expires."
HADOOP-7090,Possible resource leaks in hadoop core code,"It is always a good practice to close the IO streams in a finally block.. 

For example, look at the following piece of code in the Writer class of BloomMapFile 

{code:title=BloomMapFile .java|borderStyle=solid}
    public synchronized void close() throws IOException {
      super.close();
      DataOutputStream out = fs.create(new Path(dir, BLOOM_FILE_NAME), true);
      bloomFilter.write(out);
      out.flush();
      out.close();
    }
{code} 

If an exception occurs during fs.create or on any other line,  out.close() will not be executed..

The following can reduce the scope of resorce leaks..
{code:title=BloomMapFile .java|borderStyle=solid}
    public synchronized void close() throws IOException {
      super.close();
      DataOutputStream out = null;
      try{
          out = fs.create(new Path(dir, BLOOM_FILE_NAME), true);
          bloomFilter.write(out);
          out.flush();
      }finally{
	 IOUtils.closeStream(out);
    }
{code} 

"
HADOOP-7089,Fix link resolution logic in hadoop-config.sh,"The link resolution logic in bin/hadoop-config.sh fails when when executed via a symlink, from the root directory.  We can replace this logic with cd -P and pwd -P, which should be portable across Linux, Solaris, BSD, and OSX. "
HADOOP-7087,SequenceFile.createWriter ignores FileSystem parameter,The SequenceFile.createWriter methods that take a FileSystem ignore this parameter after HADOOP-6856. This is causing some MR tests to fail and is a breaking change when users pass unqualified paths to these calls.
HADOOP-7082,Configuration.writeXML should not hold lock while outputting,Common side of HDFS-1542
HADOOP-7078,Add better javadocs for RawComparator interface,The RawComparator interface is very important to understand for users implementing their own serialization classes. Right now the javadoc is woefully sparse. We should improve that.
HADOOP-7071,test-patch.sh has bad ps arg,
HADOOP-7070,JAAS configuration should delegate unknown application names to pre-existing configuration,"As reported here: https://issues.cloudera.org/browse/DISTRO-66 it is impossible to use secured Hadoop inside an application that relies on other JAAS configurations. This is because the static initializer of UserGroupInformation replaces the JAAS configuration, but we don't delegate unknown applications up to whatever Configuration was installed previously. The delegation technique seems to be used by JBoss's XMLLoginConfigImpl for example."
HADOOP-7069,Replace forrest with supported framework,"It's time to burn down the forrest.  Apache forrest, which is used to generate the documentation for all three subprojects, has not had a release in several years (0.8, the version we use was released April 18, 2007), and requires JDK5, which was EOL'ed in November 2009.  Since it doesn't seem likely Forrest will be developed any more, and JDK5 is not shipped with recent OSX versions, or included by default in most linux distros, we should look to find a new documentation system and convert the current docs to it."
HADOOP-7068,Ivy resolve force mode should be turned off by default,"The problem is introduced by  HADOOP-6486. Which have caused a lot of mysterious artifact issues (unable to downgrade or do parallel dev, without wiping out both m2 and ivy caches etc.) wasting countless hours of dev (many people's) time to track down the issue."
HADOOP-7061,unprecise javadoc for CompressionCodec,"In CompressionCodec.java, there is the following code:

  /**
   * Create a stream decompressor that will read from the given input stream.
   * 
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException
   */
  CompressionInputStream createInputStream(InputStream in) throws IOException;

""stream decompressor"" should be ""{@link CompressionInputStream}""."
HADOOP-7060,A more elegant FileSystem#listCorruptFileBlocks API,"I'd like to change the newly added listCorruptFileBlocks signature to be:
{code}
/**
* Get all files with corrupt blocks under the given path
*/
RemoteIterator<Path> listCorruptFileBlocks(Path src) throws IOException;
{code}
This new API does not expose ""cookie"" to user although underlying implementation may still need to invoke multiple RPCs to get the whole list."
HADOOP-7059,"Remove ""unused"" warning in native code","The file:
   src/native/src/org_apache_hadoop.h

declares the static function ""do_dlsym"" in the header as non-inline. Files including the header (e.g. for the THROW macro) receive a ""defined but unused"" warning during compilation.

This function should either a) be inlined or b) use GCC ""unused"" attribute."
HADOOP-7058,Expose number of bytes in FSOutputSummer buffer to implementatins,For HDFS-1497 it would be useful for an FSOutputSummer implementation to know how many bytes are in the FSOutputSummer buffer. This trivial patch adds a protected call to return this.
HADOOP-7057,IOUtils.readFully and IOUtils.skipFully have typo in exception creation's message,"{noformat}
        throw new IOException( ""Premeture EOF from inputStream"");
{noformat}"
HADOOP-7055,Update of commons logging libraries causes EventCounter to count logging events incorrectly,"Hadoop 0.20.2 uses commons logging 1.0.4. EventCounter works correctly with this version of commons logging. Hadoop 0.21.0 uses commons logging 1.1.1 which causes EventCounter to count logging events incorrectly. I have verified it with Hadoop 0.21.0. After start-up of hadoop, I checked jvmmetrics.log after several minutes. In every metrics record, ""logError=0, logFatal=0, logInfo=3, logWarn=0"" was shown. The following text is an example.

jvm.metrics: hostName=jingguolin, processName=DataNode, sessionId=, gcCount=3, gcTimeMillis=31, logError=0, logFatal=0, logInfo=3, logWarn=0, maxMemoryM=888.9375, memHeapCommittedM=38.0625, memHeapUsedM=3.6539612, memNonHeapCommittedM=18.25, memNonHeapUsedM=11.335686, threadsBlocked=0, threadsNew=0, threadsRunnable=8, threadsTerminated=0, threadsTimedWaiting=6, threadsWaiting=6

Then I stopped hadoop and replaced commons logging 1.1.1 with 1.0.4. After the re-start of hadoop, a lot of logging events showed up in jvmmetrics.log.

I have checked the source code of Log4JLogger for both 1.0.4 (http://svn.apache.org/viewvc/commons/proper/logging/tags/LOGGING_1_0_4/src/java/org/apache/commons/logging/impl/Log4JLogger.java?view=markup) and 1.1.1 (http://svn.apache.org/viewvc/commons/proper/logging/tags/commons-logging-1.1.1/src/java/org/apache/commons/logging/impl/Log4JLogger.java?view=markup). For 1.0.4, Level instances such as Level.INFO are used to construct LoggingEvent. But for 1.1.1, Priority instances such as Priority.INFO are used to construct LoggingEvent. So 1.1.1 version's event.getLevel() always returns Priority instances. EventCounter append method's ""=="" check always fails between a Level instance and a Priority instance. For ""logInfo=3"" metrics records produced by commons logging 1.1.1., I think that these 3 logging events are produced by log4j code directly instead of through commons logging API. The following code is EventCounter's append method.

    public void append(LoggingEvent event) {
        Level level = event.getLevel();
        if (level == Level.INFO) {
            counts.incr(INFO);
        }
        else if (level == Level.WARN) {
            counts.incr(WARN);
        }
        else if (level == Level.ERROR) {
            counts.incr(ERROR);
        }
        else if (level == Level.FATAL) {
            counts.incr(FATAL);
        }

    }"
HADOOP-7054,Change NN LoadGenerator to use the new FileContext api,
HADOOP-7053,wrong FSNamesystem Audit logging setting in conf/log4j.properties,"""log4j.logger.org.apache.hadoop.fs.FSNamesystem.audit=WARN"" should be ""log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=WARN""."
HADOOP-7052,misspelling of threshold in conf/log4j.properties,"In ""log4j.threshhold=ALL"", threshhold is a misspelling of threshold. So ""log4j.threshhold=ALL"" has no effect on the control of log4j logging.

"
HADOOP-7049,TestReconfiguration should be junit v4,TestReconfiguration should be a junit v4 unit test. I'll also add some messages to the assertions.
HADOOP-7048,Wrong description of Block-Compressed SequenceFile Format in SequenceFile's javadoc,"Here is the following description for Block-Compressed SequenceFile Format in SequenceFile's javadoc:

 * <li>
 * Record <i>Block</i>
 *   <ul>
 *     <li>Compressed key-lengths block-size</li>
 *     <li>Compressed key-lengths block</li>
 *     <li>Compressed keys block-size</li>
 *     <li>Compressed keys block</li>
 *     <li>Compressed value-lengths block-size</li>
 *     <li>Compressed value-lengths block</li>
 *     <li>Compressed values block-size</li>
 *     <li>Compressed values block</li>
 *   </ul>
 * </li>
 * <li>
 * A sync-marker every few <code>100</code> bytes or so.
 * </li>

This description misses ""Uncompressed record number in the block"". And ""A sync-marker every few <code>100</code> bytes or so"" is not the case for Block-Compressed SequenceFile Format. Correct description should be:

 * <li>
 * Record <i>Block</i>
 *   <ul>
 *     <li>Uncompressed record number in the block</li>
 *     <li>Compressed key-lengths block-size</li>
 *     <li>Compressed key-lengths block</li>
 *     <li>Compressed keys block-size</li>
 *     <li>Compressed keys block</li>
 *     <li>Compressed value-lengths block-size</li>
 *     <li>Compressed value-lengths block</li>
 *     <li>Compressed values block-size</li>
 *     <li>Compressed values block</li>
 *   </ul>
 * </li>
 * <li>
 * A sync-marker every block.
 * </li>

"
HADOOP-7046,1 Findbugs warning on trunk and branch-0.22,There is 1 findbugs warnings on trunk. See attached html file. This must be fixed or filtered out to get back to 0 warnings. The OK_FINDBUGS_WARNINGS property in src/test/test-patch.properties should also be set to 0 in the patch that fixes this issue.
HADOOP-7045,TestDU fails on systems with local file systems with extended attributes,"The test reports that the file takes an extra 4k on disk:

{noformat}
Testcase: testDU took 5.74 sec
        FAILED
expected:<32768> but was:<36864>
junit.framework.AssertionFailedError: expected:<32768> but was:<36864>
        at org.apache.hadoop.fs.TestDU.testDU(TestDU.java:79)
{noformat}

This is because du reports 32k for the file and 4k because the file system it lives on uses extended attributes.

{noformat}
common-branch-0.20 $ dd if=/dev/zero of=data bs=4096 count=8
8+0 records in
8+0 records out
32768 bytes (33 kB) copied, 9.6e-05 seconds, 341 MB/s
common-branch-0.20 $ du data
36	data
common-branch-0.20 $ du --apparent-size data
32	data
{noformat}

We should modify the test to allow for some extra on-disk slack. The on-disk usage could also be smaller if the file data is all zeros or compression is enabled. The test currently handles the former by writing random data, we're punting on the latter."
HADOOP-7042,Update test-patch.sh to include failed test names and move test-patch.properties,"As Jakob suggested, it would be helpful if the Jira messages left by Hudson included the list of failed tests.

Also, test-patch.properties must be moved out of the src/test/bin dir because it is project specific and the entire bin dir is svn included into other projects (hdfs and mapreduce)"
HADOOP-7040,DiskChecker:mkdirsWithExistsCheck swallows FileNotFoundException.,"As a result, DataNode.checkDir will miss the exception (it catches DiskErrorException, not FileNotFoundException), and fail instead of ignoring the non-existent directory."
HADOOP-7034,"Add TestPath tests to cover dot, dot dot, and slash normalization","Add tests for the current path normalization for dot, dot dot, and slash in TestPath (from HDFS-836)."
HADOOP-7032,Assert type constraints in the FileStatus constructor,"A FileStatus may represent a file, directory or symlink.  This is indicated using the isdir and symlink members, let's add an assert that validates the contstraints on these members (eg a directory may not have the symlink member set).  We could also verify this by having more than one constructor but we don't statically know the type of the file status when we create it."
HADOOP-7030,Add TableMapping topology implementation to read host to rack mapping from a file,"The default ScriptBasedMapping implementation of DNSToSwitchMapping for determining cluster topology has some drawbacks. Principally, it forks to an OS-specific script.

This issue proposes two new Java implementations of DNSToSwitchMapping. TableMapping reads a two column text file that maps an IP or hostname to a rack ID. Ip4RangeMapping reads a three column text file where each line represents a start and end IP range plus a rack ID.


"
HADOOP-7024,Create a test method for adding file systems during tests.,It allows a (mocked) filesystem object to be added to cache for testing purposes. This is used by HDFS-1187.
HADOOP-7023,Add listCorruptFileBlocks to FileSystem,Add listCorruptFileBlocks to FileSystem as discussed in HDFS-1482.
HADOOP-7015,RawLocalFileSystem#listStatus does not deal with a  directory whose entries are changing ( e.g. in a multi-thread or multi-process environment),
HADOOP-7014,Generalize CLITest structure and interfaces to facilitate upstream adoption (e.g. for web testing),There's at least one use case where TestCLI infrastructure is helpful for testing projects outside of core Hadoop (e.g. Owl web testing). In order to make this acceptance easier for upstream project TestCLI needs to be refactored.
HADOOP-7013,Add boolean field isCorrupt to BlockLocation,"This is needed to allow DFSClient.getBlockLocations to notify the calling application when returning a BlockLocation that corresponds to a corrupt block. Currently, this happens when there are no uncorrupted replicas of a requested block."
HADOOP-7011,KerberosName.main(...) throws NPE,The main method of KerberosName attempts to do short name translation before calling KerberosName.setConfiguration(...).
HADOOP-7009,MD5Hash provides a public factory method that creates an instance of MessageDigest,MD5Hash has a private way of creating a MessageDigest object that's thread local. I'd like to have such a method which is public so that checksuming fsimage (HDFS-903) could use it.
HADOOP-7008,Enable test-patch.sh to have a configured number of acceptable findbugs and javadoc warnings,test-patch.sh should be able to accept a properties file containing an acceptable number of findbugs and javadoc warnings.
HADOOP-7007,update the hudson-test-patch target to work with the latest test-patch script.,The hudson-test-patch target has to be updated to work with the current test-patch.sh script. Since the callback login in the test-patch.sh is removed. by hadoop-7005
HADOOP-7006,hadoop fs -getmerge does not work using codebase from trunk.,"Running the codebase from trunk, the hadoop fs -getmerge command does not work.  As implemented in prior versions (i.e. 0.20.2), I could run hadoop fs -getmerge pointed at a directory containing multiple files.  It would merge all files into a single file on the local file system.  Running the same command using the codebase from trunk, it looks like nothing happens.
"
HADOOP-7005,Update test-patch.sh to remove callback to Hudson master,
HADOOP-7001,Allow configuration changes without restarting configured nodes,"Currently, changing the configuration on a node (e.g., the name node) requires that we restart the node. We propose a change that would allow us to make configuration changes without restarting. Nodes that support configuration changes at run time should implement the following interface:

interface ChangeableConfigured extends Configured {
   void changeConfiguration(Configuration newConf) throws ConfigurationChangeException;
}

The contract of changeConfiguration is as follows:
The node will compare newConf to the existing configuration. For each configuration property that is set to a different value than in the current configuration, the node will either adjust its behaviour to conform to the new configuration or throw a ConfigurationChangeException if this change is not possible at run time. If a configuration property is set in the current configuration but is unset in newConf, the node should use its default value for this property. After a successful invocation of changeConfiguration, the behaviour of the configured node should be indistinguishable from the behaviour of a node that was configured with newConf at creation.

It should be easy to change existing nodes to implement this interface. We can start by throwing the exception for all changes and then gradually start supporting more and more changes at run time. (We might even consider replacing Configured with ChangeableConfigured entirely, but I think the proposal above afford greater flexibility). 


"
HADOOP-6996,Allow CodecFactory to return a codec object given a codec' class name,"CodecFactory specify the list of codec that are supported by Hadoop. However, it returns a codec only by a file's name. I would like to make getCodec method to alternatively take a codec's class name.

This is required by  HDFS-1435, where
1. it allows an HDFS admin to configure which codec to use to save an image. 
2. It stores the codec class name in its on-disk image instead of a file's suffix.

When saving and reading an image, I'd like to get an codec from CodecFactory by its class name. "
HADOOP-6995,Allow wildcards to be used in ProxyUsers configurations,"There are some cases where the full tightness of the ProxyUsers configuration is not required or available -- for example, not all users of oozie may share a common ""oozie-users"" group, and the operators would prefer to allow oozie on a given host to act proxy for any user. We should allow the operator to specify a wildcard for hosts or groups in the proxyuser configurations."
HADOOP-6994,Api to get delegation token in AbstractFileSystem,"APIs to get delegation tokens is required in AbstractFileSystem. AbstractFileSystems are accessed via file context therefore an API to get list of AbstractFileSystems accessed in a path is also needed. 
A path may refer to several file systems and delegation tokens could be needed for many of them for a client to be able to successfully access the path. "
HADOOP-6993,Broken link on cluster setup page of docs,"The link on http://hadoop.apache.org/common/docs/current/cluster_setup.html#Configuring+the+Hadoop+Daemons to core-default.xml is presently:

{quote}
http://hadoop.apache.org/common/docs/current/common-default.html
{quote}

but it should be:

{quote}
http://hadoop.apache.org/common/docs/current/core-default.html
{quote}"
HADOOP-6989,TestSetFile is failing on trunk,"Testsuite: org.apache.hadoop.io.TestSetFile
Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 1.015 sec
------------- Standard Output ---------------
2010-10-04 16:32:01,030 INFO  io.TestSetFile (TestSetFile.java:generate(56)) - generating 10000 records in memory
2010-10-04 16:32:01,249 INFO  io.TestSetFile (TestSetFile.java:generate(63)) - sorting 10000 records
2010-10-04 16:32:01,350 INFO  io.TestSetFile (TestSetFile.java:writeTest(72)) - creating with 10000 records
------------- ---------------- ---------------

Testcase: testSetFile took 0.964 sec
	Caused an ERROR
key class or comparator option must be set
java.lang.IllegalArgumentException: key class or comparator option must be set
	at org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)
	at org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)
	at org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)
	at org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)"
HADOOP-6987,Use JUnit Rule to optionally fail test cases that run more than 10 seconds,"Using JUnit Rules annotations we can fail tests cases that take longer than 10 seconds (for instance) to run.  This provides a regression check against test cases taking longer than they had previously due to unintended code changes, as well as provides a membership criteria for unit tests versus integration tests in HDFS and MR."
HADOOP-6978,Add JNI support for secure IO operations,"In support of MAPREDUCE-2096, we need to add some JNI functionality. In particular, we need the ability to use fstat() on an open file stream, and to use open() with O_EXCL, O_NOFOLLOW, and without O_CREAT."
HADOOP-6977,Herriot daemon clients should vend statistics,"The HDFS web user interface serves useful information through dfshealth.jsp and dfsnodelist.jsp.

The Herriot interface to Hadoop cluster daemons would benefit from the addition of some way to channel metics information.
"
HADOOP-6975,integer overflow in S3InputStream for blocks > 2GB,S3InputStream has the same integer overflow issue as DFSInputStream (fixed in HDFS-96).
HADOOP-6970,SecurityAuth.audit should be generated under /build,"SecurityAuth.audit is generated under currently root project directory whenever I run anything, and is not being cleaned up by the clean target. It should be created under build directory instead."
HADOOP-6965,Method in UGI to get Kerberos ticket. ,The getTGT method in AutoRenewal thread is moved to the outer UGI class. It is still a private method but can be used by reloginFromKeyTab to check for TGT expiry. This jira covers common changes for HDFS-1364
HADOOP-6963,Fix FileUtil.getDU. It should not include the size of the directory or follow symbolic links,"The getDU method should not include the size of the directory. The Java interface says that the value is undefined and in Linux/Sun it gets the 4096 for the inode. Clearly this isn't useful.
It also recursively calls itself. In case the directory has a symbolic link forming a cycle, getDU keeps spinning in the cycle. In our case, we saw this in the org.apache.hadoop.mapred.JobLocalizer.downloadPrivateCacheObjects call. This prevented other tasks on the same node from committing, causing the TT to become effectively useless (because the JT thinks it already has enough tasks running)"
HADOOP-6951,Distinct minicluster services (e.g. NN and JT) overwrite each other's service policies,"Because the protocol -> ACL mapping in ServiceAuthorizationManager is static, services which are run in the same JVM have the potential to clobber the other's service authorization ACLs whenever ServiceAuthorizationManager.refresh() is called. This causes authorization failures if one tries to launch a 2NN connected to a minicluster with hadoop.security.authorization enabled. Seems like each service should have its own instance of a ServiceAuthorizationManager, instead of using static methods."
HADOOP-6949,"Reduces RPC packet size for primitive arrays, especially long[], which is used at block reporting","Current implementation of oah.io.ObjectWritable marshals primitive array types as general object array ; array type string + array length + (element type string + value)*n

It would not be needed to specify each element types for primitive arrays."
HADOOP-6947,Kerberos relogin should set refreshKrb5Config to true,"In working on securing a daemon that uses two different principals from different threads, I found that I wasn't able to login from a second keytab after I'd logged in from the first. This is because we don't set the refreshKrb5Config in the Configuration for the Krb5LoginModule - hence it won't switch over to the correct keytab file if it's different than the first."
HADOOP-6943,The GroupMappingServiceProvider interface should be public,"The GroupMappingServiceProvider interface is presently package-protected. It seems likely that many organizations will be implementing their own versions of this to suit their particular setup. It would be nice if this interface were made public, and annotated with ""@InterfaceAudience.Private"" and ""@InterfaceStability.Evolving""."
HADOOP-6941,Support non-SUN JREs in UserGroupInformation,"Attempting to format the namenode or attempting to start Hadoop using Apache Harmony or the IBM Java JREs results in the following exception:

10/09/07 16:35:05 ERROR namenode.NameNode: java.lang.NoClassDefFoundError: com.sun.security.auth.UnixPrincipal
	at org.apache.hadoop.security.UserGroupInformation.<clinit>(UserGroupInformation.java:223)
	at java.lang.J9VMInternals.initializeImpl(Native Method)
	at java.lang.J9VMInternals.initialize(J9VMInternals.java:200)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setConfigurationParameters(FSNamesystem.java:420)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:391)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1240)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1348)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1368)
Caused by: java.lang.ClassNotFoundException: com.sun.security.auth.UnixPrincipal
	at java.net.URLClassLoader.findClass(URLClassLoader.java:421)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:652)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:346)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:618)
	... 8 more

This is a negative regression as previous versions of Hadoop worked with these JREs"
HADOOP-6939,Inconsistent lock ordering in AbstractDelegationTokenSecretManager,"AbstractDelegationTokenSecretManager.startThreads() is synchronized, which calls updateCurrentKey(), which calls logUpdateMasterKey. logUpdateMasterKey's implementation for HDFS's manager calls namesystem.logUpdateMasterKey() which is synchronized. Thus the lock order is ADTSM -> FSN. In FSN.saveNamespace, though, it calls DTSM.saveSecretManagerState(), so the lock order is FSN -> ADTSM.

I don't think this deadlock occurs in practice since saveNamespace won't occur until after the ADTSM has started its threads, but should be fixed anyway."
HADOOP-6938,ConnectionId.getRemotePrincipal() should check if security is enabled,"When security is not enabled, getRemotePrincipal() should return null, which means the Kerberos principal of the remote server is ignored. This bug was caught by TestCLI on Yahoo 20S branch."
HADOOP-6934,test for ByteWritable comparator,"The test for the ByteWritable comparator bug (see HADOOP-6928), which is already fixed in 0.21."
HADOOP-6932,"Namenode start (init) fails because of invalid kerberos key, even when security set to ""simple""",NameNode.initialize() calls login() method even when security set to simple
HADOOP-6930,AvroRpcEngine doesn't work with generated Avro code,AvroRpcEngine uses 'reflect' based java implementation. There should be a way to have it work with 'specific' (generated code from avro idl).
HADOOP-6929,RPC should have a way to pass Security information other than protocol annotations,Currently Hadoop RPC allows protocol annotations as the only way to pass security information. This becomes a problem if protocols are generated and not hand written. For example protocols generated via Avro and passed over Avro tunnel (AvroRpcEngine.java) can't pass the security information.
HADOOP-6925,BZip2Codec incorrectly implements read(),HADOOP-4012 added an implementation of read() in BZip2InputStream that doesn't work correctly when reading bytes > 0x80. This causes EOFExceptions when working with BZip2 compressed data inside of sequence files in some datasets.
HADOOP-6924,Build fails with non-Sun JREs due to different pathing to the operating system architecture shared libraries,"The src/native/configure script used to build the native libraries has an environment variable called JNI_LDFLAGS which is set as follows:

JNI_LDFLAGS=""-L$JAVA_HOME/jre/lib/$OS_ARCH/server""

This pathing convention to the shared libraries for the operating system architecture is unique to Oracle/Sun Java and thus on other flavors of Java the path will not exist and will result in a build failure with the following exception:

     [exec] gcc -shared  ../src/org/apache/hadoop/io/compress/zlib/.libs/ZlibCompressor.o ../src/org/apache/hadoop/io/compress/zlib/.libs/ZlibDecompressor.o  -L/home/hadoop/Java-Versions/ibm-java-i386-60/jre/lib/x86/server -ljvm -ldl  -m32 -m32 -Wl,-soname -Wl,libhadoop.so.1 -o .libs/libhadoop.so.1.0.0
     [exec] /usr/lib/gcc/i586-suse-linux/4.1.2/../../../../i586-suse-linux/bin/ld: cannot find -ljvm
     [exec] collect2: ld returned 1 exit status"
HADOOP-6922,COMMON part of MAPREDUCE-1664,MAPREDUCE-1664 changes the behavior of queue acls and job acls. This needs documentation changes to cluster_setup.xml and a small change in AccessControlList.java
HADOOP-6921,metrics2: metrics plugins,"This jira tracks the porting of builtin metrics sink plugins (file, ganglia) for the new metrics framework.

Whether or not ganglia 3.0/3.1 plugins will be ported depends on the outcome of the discussion (proposed in the parent issue: HADOOP-6728) on backward compatibility (at some cost/limitations of course.)"
HADOOP-6920,Metrics2: metrics instrumentation,"The jira tracks the metrics instrumentation for the new framework, i.e., porting jvm and rpc metrics to the new framework.

This issue (esp. the ""incompatible change flag"") depends on the outcome of the discussion (proposed in HADOOP-6728) on whether we should support backward compatibility (at some cost.)"
HADOOP-6919,Metrics2: metrics framework,"The jira tracks the new metrics framework only changes, i.e., it doesn't track the instrumentation changes (and compatibility issues) to existing code."
HADOOP-6913,Circular initialization between UserGroupInformation and KerberosName,"If the first call to UGI is UGI.setConfiguration(conf), it will try to initialize UGI class. During this initialization, the code calls KerberosName.setConfiguration(). KerberosName's static initializer will in turn call UGI.isSecurityEnabled(). Since UGI hasn't been completely initialized yet, isSecurityEnabled() will re-initialize UGI with a DEFAULT conf. As a result, the original conf used in UGI.setConfiguration(conf) will be overwritten by the DEFAULT conf."
HADOOP-6912,Guard against NPE when calling UGI.isLoginKeytabBased(),NPE can happen when isLoginKeytabBased() is called before a login is performed. See MAPREDUCE-1992 for an example.
HADOOP-6907,Rpc client doesn't use the per-connection conf to figure out server's Kerberos principal,"Currently, RPC client caches the conf that was passed in to its constructor and uses that same conf (or values obtained from it) for every connection it sets up. This is not sufficient for security since each connection needs to figure out server's Kerberos principal on a per-connection basis. It's not reasonable to expect the first conf used by a user to contain all the Kerberos principals that her future connections will ever need. Or worse, if her first conf contains an incorrect principal name, it will prevent the user from connecting to the server even if she later on passes in a correct conf on retry (by calling RPC.getProxy())."
HADOOP-6906,FileContext copy() utility doesn't work with recursive copying of directories.,
HADOOP-6905,Better logging messages when a delegation token is invalid,"From our production logs, we see some logging messages of ""token is expired or doesn't exist"". It would be helpful to know whose token it was."
HADOOP-6904,A baby step towards inter-version RPC communications,"Currently RPC communications in Hadoop is very strict. If a client has a different version from that of the server, a VersionMismatched exception is thrown and the client can not connect to the server. This force us to update both client and server all at once if a RPC protocol is changed. But sometime different versions do not mean the client & server are not compatible. It would be nice if we could relax this restriction and allows us to support inter-version communications.

My idea is that DfsClient catches VersionMismatched exception when it connects to NameNode. It then checks if the client & the server is compatible. If yes, it sets the NameNode version in the dfs client and allows the client to continue talking to NameNode. Otherwise, rethrow the VersionMismatch exception."
HADOOP-6903,Make AbstractFileSystem's methods public to allow filter-Fs like implementions in a differnt package than fs,Make AbstractFileSystem's methods public to allow filter-Fs like implementions in a differnt package than fs
HADOOP-6900,FileSystem#listLocatedStatus should not throw generic RuntimeException to indicate error conditions,"HDFS-6870 introduced FileSystem#listLocatedStatus(), that returns an Iterator to iterate through LocatedFileStatus for files under a directory or recursively under a sub-directory. Iterator currently throws generic RuntimeException to indicate error conditions. API needs to be changed to throw appropriate exceptions to indicate error conditions."
HADOOP-6899,RawLocalFileSystem#setWorkingDir() does not work for relative names,RawLocalFileSystem#setWorkingDir() does not work for relative names
HADOOP-6898,FileSystem.copyToLocal creates files with 777 permissions,"FileSystem.copyToLocal ends up calling through to FileUtil.copy, which calls create() on the target file system without passing any permission object. Therefore, the file ends up getting created locally with 777 permissions, which is dangerous -- even if the caller then fixes up permissions afterwards, it exposes a window in which an attacker can open the file."
HADOOP-6892,Common component of HDFS-1150 (Verify datanodes' identities to clients in secure clusters),HDFS-1150 will have changes to the start-up scripts and HttpServer.  These are handled here.
HADOOP-6890,Improve listFiles API introduced by HADOOP-6870,"This jira is mainly for addressing Suresh's review comments for HADOOP-6870:

   1. General comment: I have concerns about recursive listing. This could be abused by the applications, creating a lot of requests into HDFS.
   2. Any deletion of files/directories while reursing through directories results in RuntimeException and application has a partial result. Should we ignore if a directory was in stack and was not found later when iterating through it?
   3. FileSystem.java
          * listFile() - method javadoc could be better organized - first write about if path is directory and two cases recursive=true and false. Then if path is file and two cases recursive=true or false.
          * listFile() - document throwing RuntimeException, UnsupportedOperationException and the possible cause. IOException is no longer thrown.
   4. TestListFiles.java
          * testDirectory() - comments test empty directory and test directory with 1 file should be moved up to relevant sections of the test."
HADOOP-6889,Make RPC to have an option to timeout,"Currently Hadoop RPC does not timeout when the RPC server is alive. What it currently does is that a RPC client sends a ping to the server whenever a socket timeout happens. If the server is still alive, it continues to wait instead of throwing a SocketTimeoutException. This is to avoid a client to retry when a server is busy and thus making the server even busier. This works great if the RPC server is NameNode.

But Hadoop RPC is also used for some of client to DataNode communications, for example, for getting a replica's length. When a client comes across a problematic DataNode, it gets stuck and can not switch to a different DataNode. In this case, it would be better that the client receives a timeout exception.

I plan to add a new configuration ipc.client.max.pings that specifies the max number of pings that a client could try. If a response can not be received after the specified max number of pings, a SocketTimeoutException is thrown. If this configuration property is not set, a client maintains the current semantics, waiting forever."
HADOOP-6888,Being able to close all cached FileSystem objects for a given UGI,This is the Common part of MAPREDUCE-1900. It adds a utility method to FileSystem that closes all cached filesystems for a given UGI.
HADOOP-6887,Need a separate metrics per garbage collector,"In addition to current GC metrics which are the sum of all the collectors, Need separate metrics for monitoring young generation and old generation collections per collector w.r.t collection count and collection time. "
HADOOP-6886,LocalFileSystem Needs createNonRecursive API,"While running sanity check tests for HBASE-2312, I noticed that HDFS-617 did not include createNonRecursive() support for the LocalFileSystem.  This is a problem for HBase, which allows the user to run over the LocalFS instead of HDFS for local cluster testing.  I think this only affects 0.20-append, but may affect the trunk based upon how exactly FileContext handles non-recursive creates."
HADOOP-6885,Fix java doc warnings in Groups and RefreshUserMappingsProtocol,There are a couple java docs warnings in Groups and RefreshUserMappingsProtocol.
HADOOP-6884,"Add LOG.isDebugEnabled() guard for each LOG.debug(""..."")","Each LOG.debug(""..."") should be executed only if LOG.isDebugEnabled() is true, in some cases it's expensive to construct the string that is being printed to log. It's much easier to always use LOG.isDebugEnabled() because it's easier to check (rather than in each case reason whether it's necessary or not)."
HADOOP-6882,Update the patch level of Jetty,"I'd like to move to a newer patch level of Jetty. 6.1.23 (instead of our current 6.1.14) has been suggested. As seen in http://svn.codehaus.org/jetty/jetty/branches/jetty-6.1/VERSION.txt, that represents 18 months of bug fixes."
HADOOP-6881,The efficient comparators aren't always used except for BytesWritable and Text,"When we moved from Java 4 to Java 5 (and then 6), there was a change in the JVM semantics such that references to a class such as IntWritable.class no longer forces initialization. Since all of the Writables depend on their class static blocks to register their fast comparators, that can happen *after* we look up the comparator. In that case, the framework will fall back to the generic comparator that deserializes both keys and does the object compare, which may cause a huge slow down in the sort."
HADOOP-6879,Provide SSH based (Jsch) remote execution API for system tests,"http://mvnrepository.com/
com.jcraft » jsch 
0.1.42 version needs to be included in the build. This is  needed to facilitate implementation of some system (Herriot) testcases .

Please include this in ivy.

jsch is originally located in http://www.jcraft.com/jsch/"
HADOOP-6877,Common part of HDFS-1178,This is the Common part of HDFS-1178.
HADOOP-6875,[Herriot] Cleanup of temp. configurations is needed upon restart of a cluster,"1. New configuration directory is not cleaning up after resetting to default configuration directory in a pushconfig functionality. Because of this reason, it's giving  permission denied problem for a folder, if  other user tried running the tests in the same cluster with pushconfig functionality. I could see this issue while running the tests on a cluster with security enabled and different user.

I have added the functionality for above issue and attaching the patch

2.  Throwing IOException and it says token is expired while running  the tests. I could see this issue in a secure cluster.

This issue has been resolved by setting the following attribute in the configuration. 

mapreduce.job.complete.cancel.delegation.tokens=false
 
adding/updating this attribute in the push configuration functionality while creating the new configuration.
"
HADOOP-6873,using delegation token over hftp for long running clients (part of hdfs 1296),
HADOOP-6870,Add FileSystem#listLocatedStatus to list a directory's content together with each file's block locations,"This jira implements the new FileSystem API as proposed in HDFS-202. The new API aims to eliminate individual ""getFileBlockLocations"" calls to NN for each file in the input directory of a job. Instead, a file's block locations are returned together with FileStatus when listing a directory, thus improving getSplits performance."
HADOOP-6869,Functionality to create file or folder on a remote daemon side,"Functionality for creating either files or folders in task attempt folder while job is running. The functionality covers the following methods.

1. public void DaemonProtocol.createFile(String path, String fileName, boolean local) throws IOException; 
It uses to create a file with full permissions.

2.   public void DaemonProtocol.createFile(String path, String fileName, FsPermission permission, boolean local) throws IOException; 
It uses to create a file with given permissions.

3.   public void DaemonProtocol.createFolder(String path, String folderName, boolean local) throws IOException;
It uses to create a file with full permissions.

4.   public void DaemonProtocol.createFolder(String path, String folderName, FsPermission permission, boolean local) throws IOException;
It uses to create a folder with given permissions.
"
HADOOP-6864,Provide a JNI-based implementation of ShellBasedUnixGroupsNetgroupMapping (implementation of GroupMappingServiceProvider),"The netgroups implementation of GroupMappingServiceProvider (see ShellBasedUnixGroupsNetgroupMapping.java) does a fork of a unix command to get the netgroups of a user. Since the group resolution happens in the servers, this might be costly. This jira aims at providing a JNI-based implementation for GroupMappingServiceProvider.

Note that this is similar to what https://issues.apache.org/jira/browse/HADOOP-6818 does for implementation of GroupMappingServiceProvider that  supports only unix groups."
HADOOP-6862,Add api to add user/group to AccessControlList,Add api addUser(String user) and addGroup(String group) to add user/group to AccessControlList
HADOOP-6861,Method in Credentials to read and write a token storage file.,"The jira covers the changes in common corresponding to MAPREDUCE-1566. 

This jira adds  new non-static methods in Credentials to read and write token storage file. A method to copy tokens from another credential object is also added. Static method readTokensAndLoadInUGI is removed."
HADOOP-6860, 'compile-fault-inject' should never be called directly.,Similar to HDFS-1299 a  helper target  'compile-fault-inject' should never be called directly.
HADOOP-6859,Introduce additional statistics to FileSystem,Currently FileSystem#statistics tracks bytesRead and bytesWritten. Additional statistics that gives summary of operations performed will be useful for tracking file system use.
HADOOP-6857,FsShell should report raw disk usage including replication factor,"Currently FsShell report HDFS usage with ""hadoop fs -dus <path>"" command.  Since replication level is per file level, it would be nice to add raw disk usage including the replication factor (maybe ""hadoop fs -dus -raw <path>""?).  This will allow to assess resource usage more accurately.  -- Alex K
"
HADOOP-6856,SequenceFile and MapFile need cleanup to remove redundant constructors,"Currently there are 2 public SequenceFile.Reader constructors, 3 public SequenceFile.Writer constructors, 9 public SequenceFile.createWriter, 2 public MapFile.Reader constructors, and 8 public MapFile.Writer constructors. All of with various historical combinations of parameters that don't cover the entire space.

All of this makes it *very* difficult to add new optional parameters to SequenceFile and MapFile. 

I'd like change to the style of FileContext.create with option parameters. I'll implement one public SequenceFile.Reader constructor and one public SequenceFile.createWriter and implement all of the current variants based on those two. I'll do the same for MapFile.Reader and MapFile.Writer including passing parameters down to the underlying SequenceFile."
HADOOP-6853,Common component of HDFS-1045,"HDFS-1045 modified UGI, which is in Common on trunk.  This JIRA is for that change."
HADOOP-6847,Problem staging 0.21.0 artifacts to Apache Nexus Maven Repository,"I get this error:
{noformat}
-mvn-system-deploy:
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-http:jar:1.0-beta-2:runtime
[artifact:deploy] Deploying to https://repository.apache.org/content/repositories/snapshots
[artifact:deploy] Uploading: org/apache/hadoop/hadoop-common-instrumented/0.21.0/hadoop-common-instrumented-0.21.0.jar to apache.snapshots.https
[artifact:deploy] Uploaded 1331K
[artifact:deploy] An error has occurred while processing the Maven artifact tasks.
[artifact:deploy]  Diagnosis:
[artifact:deploy] 
[artifact:deploy] Error deploying artifact 'org.apache.hadoop:hadoop-common-instrumented:jar': Error deploying artifact: Failed to transfer file: https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common-instrumented/0.21.0/hadoop-common-instrumented-0.21.0.jar. Return code is: 400
{noformat}

Note that 400 is ""Bad Request"", not an authentication error (401)."
HADOOP-6846,Scripts for building Hadoop 0.22.0 release,
HADOOP-6845,TokenStorage renamed to Credentials.,This jira tracks common changes for MAPREDUCE-1528.
HADOOP-6840,Support non-recursive create() in FileSystem & SequenceFile.Writer,"The proposed solution for HBASE-2312 requires the sequence file to handle a non-recursive create.  This is already supported by HDFS, but needs to have an equivalent FileSystem & SequenceFile.Writer API."
HADOOP-6839,[Herriot] Implement a functionality for getting the user list for creating proxy users.,"Develop a new method for getting the user list. 

Method signature is 
public ArrayList<String> getHadoopMultiUsersList() throws IOException;
Add new attribute in system-test.xml  file for getting userlist path.

For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test."
HADOOP-6836,[Herriot]: Generic method for adding/modifying the attributes for new configuration.,"HADOP-6772 deals with 
Common utilities for system tests.
1. A method for restarting the daemon with new configuration.
c throws Exception; 

2. A method for restarting the daemon with default configuration.
public void restart() throws Exception;

3. A method for waiting until daemon is stop.
public void waitForClusterToStop() throws Exception;

In this  some variables are made of String, instead of Long. Those needs ot be changed too. So, can this method 
""public static void restartClusterWithNewConfig(Hashtable<String,Long> props, String confFile) throws Exception;""
be generalized to accepts string too. All otehr methods should work as usual.

"
HADOOP-6835,Support concatenated gzip files,"When running MapReduce with concatenated gzip files as input only the first part is read, which is confusing, to say the least. Concatenated gzip is described in http://www.gnu.org/software/gzip/manual/gzip.html#Advanced-usage and in http://www.ietf.org/rfc/rfc1952.txt. (See original report at http://www.nabble.com/Problem-with-Hadoop-and-concatenated-gzip-files-to21383097.html)
"
HADOOP-6834,TFile.append compares initial key against null lastKey  ,"The following code in TFile.KeyReigster.close: 


            byte[] lastKey = lastKeyBufferOS.getBuffer();
            int lastLen = lastKeyBufferOS.size();
            if (tfileMeta.getComparator().compare(key, 0, len, lastKey, 0,
                lastLen) < 0) {
              throw new IOException(""Keys are not added in sorted order"");
            }

compares the initial  key (passed in via  TFile.Writer.append) against a technically NULL lastKey. lastKey is not initialized until after the first call to TFile.Writer.append. The underlying RawComparator interface used for comparisons does not stipulate the proper behavior when either length 1  or length 2 is zero. In the case of LongWritable, its WritableComparator implementation does an unsafe read on the passed in byte arrays b1 and b2. Since TFile pre-allocates the buffer used for storing lastKey, this passes a valid buffer with zero count to LongWritable's comparator, which ignores length and thus produces incorrect results. 
"
HADOOP-6833,IPC leaks call parameters when exceptions thrown,"HADOOP-6498 moved the calls.remove() call lower into the SUCCESS clause of receiveResponse(), but didn't put a similar calls.remove into the ERROR clause. So, any RPC call that throws an exception ends up orphaning the Call object in the connection's ""calls"" hashtable. This prevents cleanup of the connection and is a memory leak for the call parameters."
HADOOP-6832,Provide a web server plugin that uses a static user for the web UI,We need a simple plugin that uses a static user for clusters with security that don't want to authenticate users on the web UI.
HADOOP-6828,Herrior uses old way of accessing logs directories,Between 0.20 and 0.21 the way of passing log directories location has been changed from using environment variables to passing system properties. Herrior code needs to address it as well.
HADOOP-6826,Revert FileSystem create method that takes CreateFlags,"As discussed in HDFS-609 and HADOOP-5438 we should back out the FileSystem create() method that takes a set of CreateFlag objects, until the interface has been agreed upon and fully tested."
HADOOP-6825,FileStatus needs unit tests,We need some unit tests for FileStatus to prevent problems like those we recently had on HADOOP-6796 and MAPREDUCE-1858.
HADOOP-6821,Document changes to memory monitoring,Modify the cluster_setup guide with information about memory monitoring and admin configuration.
HADOOP-6819,[Herriot] Shell command for getting the new exceptions in the logs returning exitcode 1 after executing successfully.,"I have found an corner case issue while executing the shell command for getting the new exceptions in the logs. The problem could see whenever the log files are empty and trying to get the new exceptions count, it returns the output is 0 ,however it returns the exitcode is 1 instead of 0. grep -vc in the command is not  giving the expected exitcode because its not handling the multiple in options.



"
HADOOP-6818,Provide a JNI-based implementation of GroupMappingServiceProvider,"The default implementation of GroupMappingServiceProvider does a fork of a unix command to get the groups of a user. Since the group resolution happens in the servers, this might be costly. This jira aims at providing a JNI-based implementation for GroupMappingServiceProvider."
HADOOP-6815,refreshSuperUserGroupsConfiguration should use server side configuration for the refresh,
HADOOP-6814,Method in UGI to get the authentication method of the real user.,UGI should have a method to return the authentication method of the real user for a proxy-user scenario.
HADOOP-6813,"Add a new newInstance method in FileSystem that takes a ""user"" as argument","In order to implement HDFS-1000 for trunk, I need to have a newInstance API in FileSystem that takes a ""user"" as an argument."
HADOOP-6812,fs.inmemory.size.mb not listed in conf. Cluster setup page gives wrong advice.,"http://hadoop.apache.org/common/docs/current/cluster_setup.html

fs.inmemory.size.mb does not appear in any xml file
{noformat}
grep ""fs.inmemory.size.mb"" ./mapred/mapred-default.xml 
[edward@ec src]$ grep ""fs.inmemory.size.mb"" ./hdfs/hdfs-default.xml 
[edward@ec src]$ grep ""fs.inmemory.size.mb"" ./core/core-default.xml 
{noformat}

http://hadoop.apache.org/common/docs/current/cluster_setup.html
Documentation error:
Real-World Cluster Configurations
{noformat}
conf/core-site.xml  	io.sort.factor  	100  	More streams merged at once while sorting files.
conf/core-site.xml 	io.sort.mb 	200 	Higher memory-limit while sorting data.
{noformat}

core --- io.sort.factor					-- should be mapred
core --- io.sort.mb					-- should be mapred
"
HADOOP-6811,Remove EC2 bash scripts,The bash scripts are deprecated in 0.21 (HADOOP-6403) in favour of scripts in Whirr (http://incubator.apache.org/projects/whirr.html). They should be removed in 0.22. 
HADOOP-6805,add buildDTServiceName method to SecurityUtil (as part of MAPREDUCE-1718),
HADOOP-6803,Add native gzip read/write coverage to TestCodec ,"Looking at ZlibCompressor I noticed that the finished member is never modified, and is therefore always false. This means ZlibCompressor#finished will always return false so CompressorStream#close loops indefinitely in finish:

{code} 
      while (!compressor.finished()) {
        compress();
      }
{code}

I modifed TestCodec#testGzipCodecWrite to also cover writing using the native lib and confirmed the hang with jstack. The fix is simple, ZlibCompressor should record when it's been finished."
HADOOP-6802,"Remove FS_CLIENT_BUFFER_DIR_KEY = ""fs.client.buffer.dir"" from CommonConfigurationKeys.java (not used, deprecated)","In CommonConfigurationKeys.java:

public static final String  FS_CLIENT_BUFFER_DIR_KEY = ""fs.client.buffer.dir"";

The variable FS_CLIENT_BUFFER_DIR_KEY and string ""fs.client.buffer.dir"" are not used anywhere (Checked Hadoop Common, Hdfs and Mapred projects), it seems they should be removed."
HADOOP-6800,Harmonize JAR library versions,"Currently, multiple versions of the same library JAR are being pulled in for various reasons.
* Within the same project, multiple versions of the same JAR are pulled in. E.g. Avro (used by Common) depends on Commons Lang 2.5 while Common depends on Commons Lang 2.4.
* Dependent subprojects use different versions. E.g. Common depends on Avro 1.3.2 while MapReduce depends on 1.3.0. Since MapReduce depends on Common, this has the potential to cause a problem at runtime."
HADOOP-6798,Align Ivy version for all Hadoop subprojects.,HDFS-1177 and MAPREDUCE-1830 are upgrading Ivy to 2.1.0. Common still has Ivy version at 2.1.0-rc1
HADOOP-6794,Move configuration and script files post split,
HADOOP-6791,Refresh for proxy superuser config  (common part for HDFS-1096),
HADOOP-6790,Instrumented (Herriot) build uses too wide mask to include aspect files.,"AspectJ compiler needs to use narrower mask to look for sources and aspect files. 
Otherwise, {{injectfaults}} might fail if called after {{inject-system-faults}}.
This is very minor issue and won't appear until we'll be having actually FI tests in the Common."
HADOOP-6788,[Herriot] Exception exclusion functionality is not working correctly.,"Exception exclusion functionality is not working correctly because of that tests are failing by not matching the error count.
I debugged the issue and found that the problem with shell command which is generating in the getNumberOfMatchesInLogFile function.

Currently building the shell command in the following way. 

if(list != null){
  for(int i =0; i < list.length; ++i)
  {
    filePattern.append("" | grep -v "" + list[i] );
  }
}
    String[] cmd =
        new String[] {
            ""bash"",
            ""-c"",
            ""grep -c ""
                + pattern + "" "" + filePattern
                + "" | awk -F: '{s+=$2} END {print s}'"" };    

However, The above commnad won't work correctly because you are counting the exceptions in the file before excluding the known exceptions.
In this case it gives the mismatch error counts everytime.The shell command should be in the following way to work correctly.

if (list != null) {
  int index = 0;
  for (String excludeExp : list) {
    filePattern.append((++index < list.length)? ""| grep -v "" : 
            ""| grep -vc "" + list[i] );  
  }
}
String[] cmd =
   new String[] {
       ""bash"",
       ""-c"",
       ""grep ""
           + pattern + "" "" + filePattern
           + "" | awk -F: '{s+=$2} END {print s}'"" };  "
HADOOP-6787,Factor out glob pattern code from FileContext and Filesystem,"Refactor the glob pattern code out of FileContext and FileSystem into a package private GlobFilter and the reusable GlobPattern class (InterfaceAudience.Private)

Also fix the handling of ^ outside character class ([...]) reported in HADOOP-6618 and make the glob pattern code less restrictive (not throwing on some valid glob patterns.) and more POSIX standard compliant (support [!...])."
HADOOP-6785,Fix references to 0.22 in 0.21 branch,Build files in the 0.21 branch incorrectly refer to 0.22.
HADOOP-6782,TestAvroRpc fails with avro-1.3.1 and avro-1.3.2,"I tried to upgrade Hadoop-common to use avro-1.3.1. TestAvroRpc fails with following exceptions.
Same exception is seen with avro-1.3.2 as well.

org.apache.avro.AvroRuntimeException: Not in union [""string""]: org.apache.avro.AvroRuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.ipc.TestAvroRpc$TestImpl.echo(java.lang.String)
        at org.apache.avro.generic.GenericData.resolveUnion(GenericData.java:340)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:67)
        at org.apache.avro.reflect.ReflectDatumWriter.write(ReflectDatumWriter.java:116)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:55)
        at org.apache.avro.specific.SpecificResponder.writeError(SpecificResponder.java:81)
        at org.apache.avro.ipc.Responder.respond(Responder.java:137)
        at org.apache.hadoop.ipc.AvroRpcEngine$TunnelResponder.call(AvroRpcEngine.java:183)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:342)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1350)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1346)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:738)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1344)
2010-05-20 13:50:39,227 INFO  ipc.Server (Server.java:run(1358)) - IPC Server handler 0 on 54672, call call(org.apache.hadoop.ipc.AvroRpcEngine$BufferListWritable@7bcd280b) from 127.0.0.1:54673: error: java.io.IOException: org.apache.avro.AvroRuntimeException: Not in union [""string""]: org.apache.avro.AvroRuntimeException: Not in union [""string""]: org.apache.avro.AvroRuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.ipc.TestAvroRpc$TestImpl.echo(java.lang.String)
java.io.IOException: org.apache.avro.AvroRuntimeException: Not in union [""string""]: org.apache.avro.AvroRuntimeException: Not in union [""string""]: org.apache.avro.AvroRuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.ipc.TestAvroRpc$TestImpl.echo(java.lang.String)
        at org.apache.avro.generic.GenericData.resolveUnion(GenericData.java:340)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:67)
        at org.apache.avro.reflect.ReflectDatumWriter.write(ReflectDatumWriter.java:116)
        at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:55)
        at org.apache.avro.specific.SpecificResponder.writeError(SpecificResponder.java:81)
        at org.apache.avro.ipc.Responder.respond(Responder.java:146)
        at org.apache.hadoop.ipc.AvroRpcEngine$TunnelResponder.call(AvroRpcEngine.java:183)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:342)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1350)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1346)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:738)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1344)
"
HADOOP-6781,security audit log shouldn't have exception in it.,security audit log in Server.java also prints the exception information. It shouldn't be there.
HADOOP-6778,add isRunning() method to AbstractDelegationTokenSecretManager (for  HDFS-1044),
HADOOP-6777,Implement a functionality for suspend and resume a process.,"Adding  two methods in DaemonProtocolAspect.aj for suspend and resume the process.

public int DaemonProtocol.resumeProcess(String pid) throws IOException;
public int DaemonProtocol.suspendProcess(String pid) throws IOException;

"
HADOOP-6772,Utilities for system tests specific.,"Common utilities for system tests.
1. A method for restarting the daemon with new configuration.
public static void restartClusterWithNewConfig(Hashtable<String,Long> props, String confFile) throws Exception;

2. A method for restarting the daemon with default configuration.
public void restart() throws Exception;

3. A method for waiting until daemon is stop.
public void waitForClusterToStop() throws Exception;

4. A method for waiting until daemon is start.
public void waitForClusterToStart() throws Exception;"
HADOOP-6771,Herriot's artifact id for Maven deployment should be set to hadoop-core-instrumented,
HADOOP-6769,Add an API in FileSystem to get FileSystem instances based on users,HDFS-1000 requires an API in FileSystem to get FileSystem instances based on users. 
HADOOP-6764,Add number of reader threads and queue length as configuration parameters in RPC.getServer,"In HDFS-599 we are introducing multiple RPC servers running inside of the same process on different ports. Since one might want to configure these servers differently we need a good abstraction to pass configuration values to servers as parameters, not through Configuration."
HADOOP-6763,Remove verbose logging from the Groups class,"{quote}
2010-02-25 08:30:52,269 INFO  security.Groups (Groups.java:<init>(60)) - Group m
apping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout
=300000
...
2010-02-25 08:30:57,872 INFO  security.Groups (Groups.java:getGroups(76)) - Retu
rning cached groups for 'oom'
{quote}

should both be demoted to debug level."
HADOOP-6762,exception while doing RPC I/O closes channel,"If a single process creates two unique fileSystems to the same NN using FileSystem.newInstance(), and one of them issues a close(), the leasechecker thread is interrupted.  This interrupt races with the rpc namenode.renew() and can cause a ClosedByInterruptException.  This closes the underlying channel and the other filesystem, sharing the connection will get errors."
HADOOP-6761,Improve Trash Emptier,"There are two inefficiencies in the Trash functionality right now that have caused some problems for us.
First if you configured your trash interval to be one day (24 hours) that means that you store 2 days worth of data eventually. The Current and the previous timestamp that will not be deleted until the end of the interval.
And another problem is accumulating a lot of data in Trash before the Emptier wakes up. If there are a couple of million files trashed and the Emptier does deletion on HDFS the NameNode will freeze until everything is removed. (this particular problem hopefully will be addressed with HDFS-1143).

My proposal is to have two configuration intervals. One for deleting the trashed data and another for checkpointing. This way for example for intervals of one day and one hour we will only store 25 hours of data instead of 48 right now and the deletions will be happening in smaller chunks every hour of the day instead of a huge deletion at the end of the day now.
"
HADOOP-6758,MapFile.fix does not allow index interval definition,"When using the static methond MapFile.fix() there is no way to override the default IndexInterval that is 128.

The IndexInterval should be taken from the configuration that is passed to the method.

{code}

int indexInterval = 128; 
indexInterval = conf.getInt(INDEX_INTERVAL, indexInterval); 

{code}"
HADOOP-6756,Clean up and add documentation for configuration keys in CommonConfigurationKeys.java,"Configuration keys in CommonConfigurationKeys.java should be cleaned up and documented (javadoc comments, appropriate *-default.xml descriptions)."
HADOOP-6754,DefaultCodec.createOutputStream() leaks memory,"DefaultCodec.createOutputStream() creates a new Compressor instance in each OutputStream. Even if the OutputStream is closed, this leaks memory."
HADOOP-6752,Remote cluster control functionality needs JavaDocs improvement,"Herriot has remote cluster control API. The functionality works fairly well, however, JavaDocs are missed here and there. This has to be fixed."
HADOOP-6750,UserGroupInformation incompatibility: getCurrentUGI() and setCurrentUser() missing,getCurrentUGI() and setCurrentUser() are missing from the new 0.21 branch
HADOOP-6748,Remove hadoop.cluster.administrators,"Remove hadoop.cluster.administrators in favor of having separate configuration property name for MapReduce and HDFS projects.
See more details on MAPREDUCE-1542."
HADOOP-6747,TestNetUtils fails on Mac OS X,"TestNetUtils fails consistently after HADOOP-6722 on Mac OS X Leopard 10.5.8:

{noformat}
------------- Standard Error -----------------
local address: /127.0.0.1
local port: 64991
------------- ---------------- ---------------

Testcase: testAvoidLoopbackTcpSockets took 0.421 sec
        Caused an ERROR
Invalid argument
java.net.SocketException: Invalid argument
        at sun.nio.ch.Net.connect(Native Method)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:369)
        at org.apache.hadoop.net.TestNetUtils.testAvoidLoopbackTcpSockets(TestNetUtils.java:46)
{noformat}

Although TCP spec seems to allow it, at least one implementation disallows this corner case. 
"
HADOOP-6742,Add methods HADOOP-6709 from to TestFilterFileSystem ,"HADOOP-6709 added back deprecated methods, these need to be added to the don't check list in TestFilterFileSystem so that test passes."
HADOOP-6740,Move commands_manual.xml from mapreduce into common,"Long term, we want to split the commands manual separately into the sub-projects (MAPEDUCE-1079). But for 0.21 atleast we can move it into common. Having it in mapreduce is definitely inconvenient."
HADOOP-6738,Move cluster_setup.xml from MapReduce to Common,Common half of MAPREDUCE-1404.
HADOOP-6730,Bug in FileContext#copy and provide base class for FileContext tests,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation. 

On further investigation with the help of Sanjay we found that there is bug in FileContext#checkDest.

{noformat}
  FileStatus dstFs = getFileStatus(dst);
    try {
      if (dstFs.isDir()) {
        if (null == srcNa
{noformat}


 *FileStatus dstFs = getFileStatus(dst);* should be in try...catch block.

{noformat}
    try {
       FileStatus dstFs = getFileStatus(dst);
       if (dstFs.isDir()) {
          if (null == srcNa
{noformat}


"
HADOOP-6727,Remove UnresolvedLinkException from public FileContext APIs,"HADOOP-6537 added UnresolvedLinkException to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, UnresolvedLinkException exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc."
HADOOP-6724,IPC doesn't properly handle IOEs thrown by socket factory,"If the socket factory throws an IOE inside setupIOStreams, then handleConnectionFailure will be called with socket still null, and thus generate an NPE on socket.close(). This ends up orphaning clients, etc."
HADOOP-6723,unchecked exceptions thrown in IPC Connection orphan clients,"If the server sends back some malformed data, for example,  receiveResponse() can end up with an incorrect call ID. Then, when it tries to find it in the calls map, it will end up with null and throw NPE in receiveResponse. This isn't caught anywhere, so the original IPC client ends up hanging forever instead of catching an exception. Another example is if the writable implementation itself throws an unchecked exception or OOME.

We should catch Throwable in Connection.run() and shut down the connection if we catch one."
HADOOP-6722,NetUtils.connect should check that it hasn't connected a socket to itself,"I had no idea this was possible, but it turns out that a TCP connection will be established in the rare case that the local side of the socket binds to the ephemeral port that you later try to connect to. This can present itself in very very rare occasion when an RPC client is trying to connect to a daemon running on the same node, but that daemon is down. To see what I'm talking about, run ""while true ; do telnet localhost 60020 ; done"" on a multicore box and wait several minutes.

This can be easily detected in NetUtils.connect by making sure the local address/port is not equal to the remote address/port."
HADOOP-6719,Missing methods on FilterFs,"The following methods are not declared on FilterFs(). Some of them are fine, but others like getUri(), getStatistics(), getHomeDirectory() look like they should be there.

Here is the complete list:

{code}
{
    public FSDataInputStream open(final Path f) { return null; }
    public void checkPath(Path path) { }
    public Statistics getStatistics() { return null; }
    public URI getUri() { return null; }
    public Path getHomeDirectory() { return null; }
    public void checkScheme(URI uri, String supportedScheme) { }
    public String getUriPath(final Path p) { return null; }
    public void renameInternal(final Path src, final Path dst, boolean overwrite) { }
    public FsStatus getFsStatus(final Path f) { return null; }
}
{code}"
HADOOP-6717,Log levels in o.a.h.security.Groups too high,The info logs in Groups.java for every getGroups call is causing my unrelated HDFS unit test to run out of memory since it logs so darn much.
HADOOP-6715,"AccessControlList.toString() returns empty string when we set acl to ""*""","AccessControlList.toString() returns empty string when we set the acl to ""\*"" and also when we set it to empty(i.e. "" ""). This is causing wrong values for ACLs shown on jobdetails.jsp and jobdetailshistory.jsp web pages when acls are set to ""\*"".

I think AccessControlList.toString() needs to be changed to return ""\*"" when we set the acl to ""\*""."
HADOOP-6714,FsShell 'hadoop fs -text' does not support compression codecs,"Currently, 'hadoop fs -text myfile' looks at the first few magic bytes of a file to determine whether it is gzip compressed or a sequence file. This means 'fs -text' cannot properly decode .deflate or .bz2 files (or other codecs specified via configuration).

It should be fairly straightforward to add support for other codecs by checking the file extension against the CompressionCodecFactory to retrieve an appropriate Codec."
HADOOP-6713,The RPC server Listener thread is a scalability bottleneck,The Hadoop RPC Server implementation has a single Listener thread that reads data from the socket and puts them into a call queue. This means that this single thread can pull RPC requests off the network only as fast as a single CPU can execute. This is a scalability bottlneck in our cluster.
HADOOP-6709,Re-instate deprecated FileSystem methods that were removed after 0.20,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.
"
HADOOP-6706,Relogin behavior for RPC clients could be improved,"Currently, the relogin in the RPC client happens on only a SaslException. But we have seen cases where other exceptions are thrown (like IllegalStateException when the client's ticket is invalid). This jira is to fix that behavior."
HADOOP-6705,jiracli fails to upload test-patch comments to jira,"     [exec] ======================================================================
     [exec]     Adding comment to Jira.
     [exec] ======================================================================
     [exec] ======================================================================
     [exec] 
     [exec] 
     [exec] Failed to connect to: http://issues.apache.org/jira/rpc/soap/jirasoapservice-v2?wsdl
     [exec] Failed to connect to: http://issues.apache.org/jira/rpc/soap/jirasoapservice-v2?wsdl
     [exec] Failed to connect to: http://issues.apache.org/jira/rpc/soap/jirasoapservice-v2?wsdl
     [exec]   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
"
HADOOP-6703,"Prevent renaming a file, symlink or directory to itself","Per HDFS-1088 let's throw a FileAlreadyExistsException if renaming a file, symlink or directory to itself, or a symlink to the file it links to. 
"
HADOOP-6702,"Incorrect exit codes for ""dfs -chown"", ""dfs -chgrp""  when input is given in wildcard format.","Currently incorrect exit codes  are given for ""dfs -chown"", ""dfs -chgrp""  when input is given in wildcard format.

This bug is due to missing update of errors count in {{FsShell.java}}.

{code:title=FsShell.java|borderStyle=solid}
int runCmdHandler(CmdHandler handler, String[] args,
                                   int startIndex, boolean recursive) 
                                   throws IOException {
    int errors = 0;
    
    for (int i=startIndex; i<args.length; i++) {
      Path srcPath = new Path(args[i]);
      FileSystem srcFs = srcPath.getFileSystem(getConf());
      Path[] paths = FileUtil.stat2Paths(srcFs.globStatus(srcPath), srcPath);
      for(Path path : paths) {
        try {
          FileStatus file = srcFs.getFileStatus(path);
          if (file == null) {
            System.err.println(handler.getName() + 
                               "": could not get status for '"" + path + ""'"");
            errors++;
          } else {
            errors += runCmdHandler(handler, file, srcFs, recursive);
          }
        } catch (IOException e) {
          String msg = (e.getMessage() != null ? e.getLocalizedMessage() :
            (e.getCause().getMessage() != null ? 
                e.getCause().getLocalizedMessage() : ""null""));
          System.err.println(handler.getName() + "": could not get status for '""
                                        + path + ""': "" + msg.split(""\n"")[0]);
          errors++;
        }
      }
    }
 {code}

If there are no files on HDFS matching to wildcard input then  {{srcFs.globStatus(srcpath)}} returns 0. 
{{ Path[] paths = FileUtil.stat2Paths(srcFs.globStatus(srcPath), srcPath);}}

Resulting no increment in {{errors}} and command exits with 0 even though file/directory does not exist."
HADOOP-6701," Incorrect exit codes for ""dfs -chown"", ""dfs -chgrp""","ravi@localhost:~$ hadoop dfs -chgrp abcd /; echo $?
chgrp: changing ownership of
'hdfs://localhost/':org.apache.hadoop.security.AccessControlException: Permission denied
0

ravi@localhost:~$ hadoop dfs -chown  abcd /; echo $?
chown: changing ownership of
'hdfs://localhost/':org.apache.hadoop.security.AccessControlException: Permission denied
0

ravi@localhost:~$ hadoop dfs -chmod 755 /DOESNTEXIST; echo $?
chmod: could not get status for '/DOESNTEXIST': File does not exist: /DOESNTEXIST
0

-

Exit codes for both of the above invocations should be non-zero to indicate that the command failed."
HADOOP-6698,Revert the io.serialization package to 0.20.2's api,"I have a lot of concern about the usability of the new generic serialization framework. Toward that end, I've filed a jira for improving it. There is resistance to pushing a new API into 0.21 at the last moment, so we should back out the changes rather than introducing a new api in 0.21 and deprecating it in 0.22."
HADOOP-6693,Add metrics to track kerberos login activity,Need metrics to track kerberos login activity such as login rate and the time taken for login.
HADOOP-6692,Add FileContext#listStatus that returns an iterator,"Add a method  Iterator<FileStatus> listStatus(Path), which allows HDFS client not to have the whole listing in the memory, benefit more from the iterative listing added in HDFS-985. Move the current FileStatus[] listStatus(Path) to be a utility method."
HADOOP-6691,TestFileSystemCaching sometimes hang,TestFileSystemCaching#testCacheEnabledWithInitializeForeverFS() sometimes hangs if InitializeForeverFileSystem initializes first.
HADOOP-6690,FilterFileSystem doesn't overwrite setTimes,"FilterFileSystem seems to be a little outdated and it doesn't implement a few methods (setTimes being the most important one):

- setTimes(Path, long, long)
- copyFromLocalFile(boolean,boolean, Path, Path)
- copyFromLocalFile(boolean, boolean, Path[], Path)
- getUsed()
- deleteOnExit()

I'm not sure if all of these methods should be wrapped in FilterFileSystem, but given its purpose, I would say the more the better. It would be great to have other people's opinions about this."
HADOOP-6689,Add directory renaming test to FileContextMainOperationsBaseTest,I noticed FileContextMainOperationsBaseTest does not have a test that renames an empty directory to an empty directory (and shows that this fails without the overwrite option). 
HADOOP-6686,Remove redundant exception class name in unwrapped exceptions thrown at the RPC client,
HADOOP-6683,the first optimization: ZlibCompressor does not fully utilize the buffer,"Thanks for Hong Tang's advice.

Sub task created for the first optimization. HADOOP-6662 closed. "
HADOOP-6682,NetUtils:normalizeHostName does not process hostnames starting with [a-f] correctly,"  public static String normalizeHostName(String name) {
    if (Character.digit(name.charAt(0), 16) != -1) {
      return name;

This code is attempting to short-circuit the hostname->ip resolution on the assumption that if name starts with a digit, it's already an ip address.  This is of questionable value, but because it checks for a hex digit, it will fail on names starting with [a-f].  Such names will not be converted to an ip address, but be returned unchanged."
HADOOP-6678,"Remove FileContext#isFile, isDirectory and exists","# Add a method  Iterator<FileStatus> listStatus(Path), which allows HDFS client not to have the whole listing in the memory, benefit more from the iterative listing added in HDFS-985. Move the current FileStatus[] listStatus(Path) to be a utility method.
# Remove methods isFile(Path), isDirectory(Path), and exists.
All these methods are implemented by calling getFileStatus(Path).But most users are not aware of this. They would write code as below: 
{code}
  FileContext fc = ..;
  if (fc.exists(path)) {
    if (fc.isFile(path)) {
     ...
    } else {
    ...
    }
  }
{code}
The above code adds unnecessary getFileInfo RPC to NameNode. In our production clusters, we often see that the number of getFileStatus calls is multiple times of the open calls. If we remove isFile, isDirectory, and exists from FileContext, users have to explicitly call getFileStatus first, it is more likely that they will write more efficient code as follow:
{code}
  FileContext fc = ...;
  FileStatus fstatus = fc.getFileStatus(path);
  if (fstatus.isFile() {
    ...
  } else {
    ...
  }
{code}"
HADOOP-6677,InterfaceAudience.LimitedPrivate should take a string not an enum,Trying to keep the list of all possible components sharing limited private interfaces up to date is painful.  As other subprojects beyond HDFS and MR use this interface it will only get worse (see PIG-1311).  If it is converted to a string other subprojects can use it without requiring patches to common.
HADOOP-6674,Performance Improvement in Secure RPC,"This jira introduces two performance improvements in Sasl RPC
1. Setting of Sasl 'quality of protection' to authentication only.
2. Addition of BufferedOutputStream underneath SaslOutputStream."
HADOOP-6671,To use maven for hadoop common builds,"We are now able to publish hadoop artifacts to the maven repo successfully [ Hadoop-6382]
Drawbacks with the current approach:
* Use ivy for dependency management with ivy.xml
* Use maven-ant-task for artifact publishing to the maven repository
* pom files are not generated dynamically 

To address this I propose we use maven to build hadoop-common, which would help us to manage dependencies, publish artifacts and have one single xml file(POM) for dependency management and artifact publishing.

I would like to have a branch created to work on mavenizing  hadoop common.
"
HADOOP-6670,UserGroupInformation doesn't support use in hash tables,"The UserGroupInformation objects are mutable, but they are used as keys in hash tables. This leads to serious problems in the FileSystem cache and RPC connection cache. We need to change the hashCode to be the identity hash code of the Subject and change equals to use == between the Subjects."
HADOOP-6668,Apply audience and stability annotations to classes in common,Mark private implementation classes with the InterfaceAudience.Private or InterfaceAudience.LimitedPrivate annotation to exclude them from user Javadoc and JDiff.
HADOOP-6665,DFSadmin commands setQuota and setSpaceQuota allowed when NameNode is in safemode.  ,"Currently DFSadmin commands setSpaceQuota and setQuota are allowed when Name node is in safe mode.
setQuota and setSpaceQuota operations causes changes in name node name system. These operations should be restricted when name node is in safe mode.

"
HADOOP-6663,BlockDecompressorStream get EOF exception when decompressing the file compressed from empty file,"An empty file can be compressed using BlockDecompressorStream, which is for block-based compressiong algorithm such as LZO. However, when decompressing the compressed file, BlockDecompressorStream get EOF exception.

Here is a typical exception stack:

java.io.EOFException
at org.apache.hadoop.io.compress.BlockDecompressorStream.rawReadInt(BlockDecompressorStream.java:125)
at org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:96)
at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:82)
at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:74)
at java.io.InputStream.read(InputStream.java:85)
at org.apache.hadoop.util.LineReader.readLine(LineReader.java:134)
at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:134)
at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:39)
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:186)
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:170)
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:18)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)
at org.apache.hadoop.mapred.Child.main(Child.java:196)
"
HADOOP-6658,Exclude  Public elements in  generated Javadoc,"Packages, classes and methods that are marked with the InterfaceAudience.Private or InterfaceAudience.LimitedPrivate annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements."
HADOOP-6657,Common portion of MAPREDUCE-1545,Common portion of the MAPREDUCE-1545. 
HADOOP-6656,Security framework needs to renew Kerberos tickets while the process is running,"While a client process is running, there should be a thread that periodically renews the Kerberos credentials to ensure they don't expire."
HADOOP-6654,Example in WritableComparable javadoc doesn't compile,See http://www.nabble.com/API-Documentation-question---WritableComparable-tt20967409.html.
HADOOP-6652,ShellBasedUnixGroupsMapping shouldn't have a cache,"Since the Groups class already has a time based cache, the cache from ShellBasedUnixGroupsMapping should be removed."
HADOOP-6648,Credentials should ignore null tokens,"When hftp goes to a non-secure cluster, getDelegationToken returns null. Credentials.addToken needs to ignore null tokens to avoid a NPE in serialization."
HADOOP-6646,Move HarfileSystem out of Hadoop Common.,"Move HarFileSystem out of common so that we can get out of the cumbersome task of making updates in 2 places and also HarFileSystem - a user level file system, belongs in tools."
HADOOP-6645,Bugs on listStatus for HarFileSystem,"Two bugs on listStatus for HarFileSystem:

1) consider the following directory tree inside a hadoop archive
/foo
/foo/bar1
/fooo
/fooo/bar2

In this case, listStatus(new Path(""/foo"")) will include /fooo/bar2 because fileStatusesInIndex is testing  a prefix.

2) HADOOP-6591 didn't take into consideration method fileStatusesInIndex(), and archives v2 return empty results for listStatus()
"
HADOOP-6642,"Fix javac, javadoc, findbugs warnings","Unfortunately we've missed some javac, javadoc, findbugs warnings in the recent security work."
HADOOP-6640,FileSystem.get() does RPC retries within a static synchronized block,"If using FileSystem.get() in a multithreaded environment, and one get() locks because the NN URI is too slow or not responding and retries are in progress, all other get() (for the diffferent users, NN) are blocked.

the synchronized block in in the static instance of Cache inner class.
"
HADOOP-6635,Install or deploy source jars to maven repo,Publishing source jars to the local m2 cache or a public maven repo is extremely handy for Hadoop users that want to be able to inspect the Hadoop source from within their IDE.
HADOOP-6634,AccessControlList uses full-principal names to verify acls causing queue-acls to fail,"ACL configuration so far was using short user-names. With the changed {{UserGroupInformation}}, short names are different from the long fully-qualified names. {{AccessControlList}} continues to use {{UserGroupInformation.getUserName()}} for verifying access control. Because of this, queue acls fail for a user ""user@domain.org"" even though the short name ""user"" is part of the acl configuration."
HADOOP-6632,Support for using different Kerberos keys for different instances of Hadoop services,"We tested using the same Kerberos key for all datanodes in a HDFS cluster or the same Kerberos key for all TaskTarckers in a MapRed cluster. But it doesn't work. The reason is that when datanodes try to authenticate to the namenode all at once, the Kerberos authenticators they send to the namenode may have the same timestamp and will be rejected as replay requests. This JIRA makes it possible to use a unique key for each service instance."
HADOOP-6631,FileUtil.fullyDelete() should continue to delete other files despite failure at any level.,"Ravi commented about this on HADOOP-6536. Paraphrasing...

Currently FileUtil.fullyDelete(myDir) comes out stopping deletion of other files/directories if it is unable to delete a file/dir(say because of not having permissions to delete that file/dir) anywhere under myDir. This is because we return from method if the recursive call ""if(!fullyDelete()) {return false;}"" fails at any level of recursion.

Shouldn't it continue with deletion of other files/dirs continuing in the for loop instead of returning false here ?

I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf')."
HADOOP-6630,hadoop-config.sh fails to get executed if hadoop wrapper scripts are in path,"If the hadoop/bin commands are in the path, hadoop-config.sh doesn't executed until much later in the stack because $0 lacks a path."
HADOOP-6622,Token should not print the password in toString.,The toString method in Token should not print out the password.
HADOOP-6620,NPE if renewer is passed as null in getDelegationToken,"If renewer is passed as null in getDelegationToken, an NPE is thrown. We should handle null renewer and the token must not be allowed to be renewed if the renewer is null or empty string."
HADOOP-6614,RunJar should provide more diags when it can't create a temp file,"When you see a stack trace about permissions, it is better if the trace included the file/directory at fault:
{code}
Exception in thread ""main"" java.io.IOException: Permission denied
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.checkAndCreate(File.java:1704)
	at java.io.File.createTempFile(File.java:1792)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:147)
{code}

As it is, you need to go into the code, discover that it's {{${hadoop.tmp.dir}/hadoop-unjar}}, but you need to know the value of hadoop.tmp.dir to really find out what the problem is."
HADOOP-6613,RPC server should check for version mismatch first,Currently AuthMethod is checked before Version.
HADOOP-6612,Protocols RefreshUserToGroupMappingsProtocol and RefreshAuthorizationPolicyProtocol will fail with security enabled,
HADOOP-6609,Deadlock in DFSClient#getBlockLocations even with the security disabled,"Here is the stack trace:
""IPC Client (47) connection to XX"" daemon
prio=10 tid=0x00002aaae0369c00 nid=0x655b waiting for monitor entry [0x000000004181f000..0x000000004181fb80]
   java.lang.Thread.State: BLOCKED (on object monitor)
at org.apache.hadoop.io.UTF8.readChars(UTF8.java:210)
- waiting to lock <0x00002aaab3eaee50> (a org.apache.hadoop.io.DataOutputBuffer)
at org.apache.hadoop.io.UTF8.readString(UTF8.java:203)
at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:179)
at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)
at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:638)
at org.apache.hadoop.ipc.Client$Connection.run(Client.java:573)

""IPC Client (47) connection to /0.0.0.0:50030 from job_201002262308_0007""
daemon prio=10 tid=0x00002aaae0272800 nid=0x6556 waiting for monitor entry [0x000000004131a000..0x000000004131ad00]
   java.lang.Thread.State: BLOCKED (on object monitor)
at org.apache.hadoop.io.UTF8.readChars(UTF8.java:210) 
- waiting to lock <0x00002aaab3eaee50> (a org.apache.hadoop.io.DataOutputBuffer)
at org.apache.hadoop.io.UTF8.readString(UTF8.java:203)
at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:179)
at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)
at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:638)
at org.apache.hadoop.ipc.Client$Connection.run(Client.java:573)

""main"" prio=10 tid=0x0000000046c17800 nid=0x6544 in Object.wait() [0x0000000040207000..0x0000000040209ec0]
   java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method) 
- waiting on <0x00002aaacee6bc38> (a org.apache.hadoop.ipc.Client$Call)
at java.lang.Object.wait(Object.java:485)
at org.apache.hadoop.ipc.Client.call(Client.java:854) - locked <0x00002aaacee6bc38> (a org.apache.hadoop.ipc.Client$Call)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:223)
at $Proxy2.getBlockLocations(Unknown Source)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
at $Proxy2.getBlockLocations(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:333)
at org.apache.hadoop.hdfs.DFSClient.access$2(DFSClient.java:330)
at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.getBlockAt(DFSClient.java:1606) 
- locked <0x00002aaacecb8258> (a org.apache.hadoop.hdfs.DFSClient$DFSInputStream)
at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1704)
- locked <0x00002aaacecb8258> (a org.apache.hadoop.hdfs.DFSClient$DFSInputStream)
at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1856)
- locked <0x00002aaacecb8258> (a org.apache.hadoop.hdfs.DFSClient$DFSInputStream)
at java.io.DataInputStream.readFully(DataInputStream.java:178)
at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
at org.apache.hadoop.io.UTF8.readChars(UTF8.java:211)
- locked <0x00002aaab3eaee50> (a org.apache.hadoop.io.DataOutputBuffer)
at org.apache.hadoop.io.UTF8.readString(UTF8.java:203)
at org.apache.hadoop.mapred.FileSplit.readFields(FileSplit.java:90)
at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:1)
at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:341)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:357)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)
at org.apache.hadoop.mapred.Child$4.run(Child.java:211)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:700)
at org.apache.hadoop.mapred.Child.main(Child.java:205)"
HADOOP-6607,Add different variants of non caching HTTP headers,"I'm suffering from proxy servers that are caching some of the HttpResponses that Hadoop generates in servlets/JSP pages. While the web ui is up to date, some of my build files are failing to pull stuff down because that is going via proxy -it sees an error page rather than the data

# Every servlet should set a short expires header and disable caching, especially in proxies. 
# JSP pages should do it to
# It's essential that error responses do it.

Maybe this could be done in a filter. Otherwise something like
{code}
    /**
     * Turn off caching and say that the response expires now
     * @param response the response
     */
    protected void disableCaching(HttpServletResponse response) {
        response.addDateHeader(""Expires"", System.currentTimeMillis());
        response.addHeader(""Cache-Control"", ""no-cache"");
        response.addHeader(""Pragma"", ""no-cache"");
    }
{code}

Before anyone rushes to do this, we should consult some HTTP experts in Yahoo! or Facebook to get the options right. It may be best to have, say, a 1s lifespan on everything."
HADOOP-6600,mechanism for authorization check for inter-server protocols,allow configuration of authorization for based on protocol (inter-server).
HADOOP-6599,Split RPC metrics into summary and detailed metrics,"Currently RPC metrics tracks items that provides summary of RPC usage along with more detailed per method statistics that tracks number of method calls made, time spent on that call. Combining both summary and detailed together results in large metrics report which metrics collection systems may not handle. Proposal is to split the metrics into summary and detailed metrics."
HADOOP-6594,Update hdfs script to provide fetchdt tool,"Since the bin/hdfs script is still in common, this JIRA is needed to provide access to the new delegation token fetcher done in HDFS-994"
HADOOP-6593,TextRecordInputStream doesn't close SequenceFile.Reader,"Using hadoop fs -text on a glob with many sequence files can fail with too many open file handles.

The cause seems to be that TextRecordInputStream doesn't override close(), so printToStdout's call to close doesn't release the SequenceFile.Reader."
HADOOP-6591,HarFileSystem cannot handle paths with the space character,"Since HarFileSystem is using "" "" as a separator in the index files, it won't work if there are "" "" in the path."
HADOOP-6589,Better error messages for RPC clients when authentication fails,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security."
HADOOP-6586,Log authentication and authorization failures and successes,"This jira will cover RPC authentication and SL authorizations logging.
"
HADOOP-6585,Add FileStatus#isDirectory and isFile,"Per Sanjay's suggestion in HADOOP-6421 let's deprecate FileStatus#isDir() and add isDirectory() and isFile() to compliment isSymlink. Currently clients assume !isDir() implies a file, which is no longer true with symlinks. I'll file a separate jira to change the various uses of !isDir() to be isFile() or isFile() or isSymlink() as appropriate."
HADOOP-6584,Provide Kerberized SSL encryption for webservices,"Some web services should be authenticated via SSL backed by Kerberos, both to provide cross-cluster secure communication and to provide intra-cluster server authentication for services such as the {Name,SecondaryName,Backup,Checkpoint}node's image transfer and balancer."
HADOOP-6583,Capture metrics for authentication/authorization at the RPC layer,Define metrics for authentication/authorization for the RPC layer.
HADOOP-6582,"Token class should have a toString, equals and hashcode method","The Token.toString would be helpful in logging. The equals/hashcode would be useful in UserGroupInformation.equals (currently the reference equality checks are done), and in other places."
HADOOP-6579,A utility for reading and writing tokens into a URL safe string.,"We need to include HDFS delegation tokens in the URLs while browsing the file system. Therefore, we need a url-safe way to encode and decode them."
HADOOP-6578,Configuration should trim whitespace around a lot of value types,"I've seen multiple users make an error where they've listed some whitespace around a class name (eg for configuring a scheduler). This results in a ClassNotFoundException which is very hard to debug, as you don't notice the whitespace in the exception! We should simply trim the whitespace in Configuration.getClass and Configuration.getClasses to avoid this class of user error.

Similarly, we should trim in getInt, getLong, etc - anywhere that whitespace doesn't have semantic meaning we should be a little less strict on input."
HADOOP-6577,IPC server response buffer reset threshold should be configurable,"In HDFS-6460, the response buffers in o.a.h.ipc.Server.Handler was reset when the buffer grows beyond max size of 1MB. This frees heap from large responses occupying it. This max response size limit should be configurable. Details in subsequent comment.  "
HADOOP-6573,Delegation Tokens should be persisted.,The Delegation tokens should be persisted in the FsImage and EditLogs so that they are valid to be used after namenode shutdown and restart. This jira tracks changes in common to support persistence of delegation tokens.
HADOOP-6572,RPC responses may be out-of-order with respect to SASL,"SASL enforces its own message ordering. When RPC server sends its responses back, response A may be wrapped by SASL before response B but is put on response queue after response B. This results in RPC client receiving wrapped response B ahead of A. When the received messages are unwrapped by SASL, SASL complaints the messages are out of order."
HADOOP-6570,RPC#stopProxy throws NullPointerExcption if getProxyEngine(proxy) returns null,"{noformat}
  public static void stopProxy(Object proxy) {
    if (proxy!=null) {
      getProxyEngine(proxy).stopProxy(proxy);
    }
  }
{noformat}
The method should check if getProxyEngine(proxy) returns null or not before stopProxy is called."
HADOOP-6569,FsShell#cat should avoid calling unecessary getFileStatus before opening a file to read,"Since FileSystem#open throws a FileNotFoundException when the file to be read does not exist, there is no need to check if the file is a directory or not before open. In case of HDFS, this could reduce one getFileInfo RPC to NameNode."
HADOOP-6568,Authorization for default servlets,"We have the following default servlets: /logs, /static, /stacks, /logLevel, /metrics, /conf. Barring ""/static"", rest of the servlets provide information that is only for administrators. In the context of security for the web-servlets, we need protected access to these pages."
HADOOP-6566,Hadoop daemons should not start up if the ownership/permissions on the directories used at runtime are misconfigured,"The Hadoop daemons (like datanode, namenode) should refuse to start up if the ownership/permissions on directories they use at runtime are misconfigured or they are not as expected. For example, the local directory where the filesystem image is stored should be owned by the user running the namenode process and should be only readable by that user. We can provide this feature in common and HDFS and MapReduce can use the same."
HADOOP-6563,Add more tests to FileContextSymlinkBaseTest that cover intermediate symlinks in paths,"Intermediate symlinks in paths are covered by the tests that use /linkToDir/file, /linkToDir/subDir, etc  eg testCreateVia* in FileContextSymlinkBaseTest. I'll add additional tests to cover other basic operations on paths like /dir/linkToSomeDir/file beyond create() and open()."
HADOOP-6560,HarFileSystem throws NPE for har://hdfs-/foo,"{noformat}
-bash-3.1$ hadoop distcp -Dmapred.job.queue.name=${JOBQ}  har://hdfs-/user/tsz/t10_4.har/t10_4 t10_4_distcp t10_4_distcp
10/01/28 23:20:45 INFO tools.DistCp: srcPaths=[har://hdfs-/user/tsz/t10_4.har/t10_4]
10/01/28 23:20:45 INFO tools.DistCp: destPath=t10_4_distcp
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.lang.NullPointerException
        at org.apache.hadoop.fs.HarFileSystem.decodeHarURI(HarFileSystem.java:184)
        at org.apache.hadoop.fs.HarFileSystem.initialize(HarFileSystem.java:95)
        ...
{noformat}
"
HADOOP-6559,The RPC client should try to re-login when it detects that the TGT expired,"Currently while making RPC calls, the client will throw an exception if the client is not able to use the TGT (expired or timedout). This could be improved - it could catch the exception and try doing a re-login."
HADOOP-6558,archive does not work with distcp -update,"The following distcp command  works.
{noformat}
hadoop distcp -Dmapred.job.queue.name=q har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101 t101_distcp
{noformat}
However, it does not work for -update.
{noformat}
-bash-3.1$ hadoop distcp -Dmapred.job.queue.name=q -update har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101 t101_distcp
10/01/29 20:06:53 INFO tools.DistCp: srcPaths=[har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101]
10/01/29 20:06:53 INFO tools.DistCp: destPath=t101
java.lang.IllegalArgumentException: Wrong FS: har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101/text-00000000, expected: hdfs://nn_hostname
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:310)
        at org.apache.hadoop.hdfs.DistributedFileSystem.checkPath(DistributedFileSystem.java:99)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:155)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:463)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:46)
        at org.apache.hadoop.fs.FilterFileSystem.getFileChecksum(FilterFileSystem.java:250)
        at org.apache.hadoop.tools.DistCp.sameFile(DistCp.java:1204)
        at org.apache.hadoop.tools.DistCp.setup(DistCp.java:1084)
        ...
{noformat}

"
HADOOP-6552,KEYTAB_KERBEROS_OPTIONS in UserGroupInformation should have options for automatic renewal of keytab based tickets,"KEYTAB_KERBEROS_OPTIONS in UserGroupInformation should have options for automatic renewal of keytab based tickets. Specifically, renewTGT=true (and useTicketCache=true), as documented in http://java.sun.com/javase/6/docs/jre/api/security/jaas/spec/com/sun/security/auth/module/Krb5LoginModule.html"
HADOOP-6551,Delegation tokens when renewed or cancelled should throw an exception that explains what went wrong,"Currently the renew and cancel delegation token method will return false if something goes wrong, but we need an error message and an exception."
HADOOP-6549,TestDoAsEffectiveUser should use ip address of the host for superuser ip check,TestDoAsEffectiveUser should configure ip address of the machine it is run on along with the localhost and loopback address in the configuration for allowed IP addresses from where a superuser can connect.
HADOOP-6548,Replace org.mortbay.log.Log imports with commons logging,"Some IDEs auto-import {{org.mortbay.log.Log}}, contrary to the standard pattern using commons logging."
HADOOP-6547,Move the Delegation Token feature to common since both HDFS and MapReduce needs it,Move the Delegation Token feature to common since both HDFS and MapReduce needs it.
HADOOP-6546,BloomMapFile can return false negatives,"BloomMapFile can return false negatives when using keys of varying sizes.  If the amount of data written by the write() method of your key class differs between instance of your key, your BloomMapFile may return false negatives.
"
HADOOP-6545,Cached FileSystem objects can lead to wrong token being used in setting up connections,"The FileSystem class caches the filesystem objects that it creates for users. For some cases, e.g., if the filesystem object is actually a DistributedFileSystem, it also has an associated RPC client and hence an UGI for the respective user. This could lead to issues to do with using the right credentials when connecting with the namenode. The credentials in the UGI is never updated (even if the user in question now has new credentials) and in case the cached UGI's credentials have expired, this would lead to authentication error whenever there is a re-authentication (in the process of re-establishing connection to the namenode)."
HADOOP-6543,Allow authentication-enabled RPC clients to connect to authentication-disabled RPC servers ,"This is useful when one has multiple clusters (of the same release version), some have authentication turned on and some off, and one needs to move data between them."
HADOOP-6540,"Contrib unit tests have invalid XML for core-site, etc.",The *-site.xml files in src/contrib/test are not valid XML. the <?xml .. ?> declaration must appear above the license header.
HADOOP-6538,"Set hadoop.security.authentication to ""simple"" by default","The default value of ""hadoop.security.authentication"" is ""kerberos"". It makes sense for that to be ""simple"" since not all users have kerberos infrastructure set up, and it is inconvenient to have it set it to ""simple"" manually."
HADOOP-6537,Proposal for exceptions thrown by FileContext and Abstract File System,"Currently the APIs in FileContext throw only IOException. Going forward these APIs will throw more specific exceptions.
This jira proposes following hierarchy of exceptions to be thrown by FileContext and AFS (Abstract File System) classes.


InterruptedException  (java.lang.InterruptedException)

IOException
                /* Following exceptions extend IOException */
                FileNotFoundException
                FileAlreadyExistsException
                DirectoryNotEmptyException
                NotDirectoryException
                AccessDeniedException
                IsDirectoryException
                InvalidPathNameException
                
                FileSystemException
                                     /* Following exceptions extend FileSystemException */
                                     FileSystemNotReadyException
                                     ReadOnlyFileSystemException
                                     QuotaExceededException
                                     OutOfSpaceException

                RemoteException   (java.rmi.RemoteException)

Most of the IOExceptions above are caused by invalid user input, while FileSystemException is thrown when FS is in such a state that the requested operation cannot proceed.
Please note that the proposed RemoteException is from standard java rmi package, which also extends IOException.
                            
HDFS throws many exceptions which are not in the above list. The DFSClient will unwrap the exceptions thrown by HDFS, and any exception not in the above list will be thrown as IOException or FileSystemException.
"
HADOOP-6536,FileUtil.fullyDelete(dir) behavior is not defined when we pass a symlink as the argument,"FileUtil.fullyDelete(dir) deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so. 
Or it should be changed not to delete the contents of the sym-linked directory."
HADOOP-6534,LocalDirAllocator should use whitespace trimming configuration getters,"This is the other half of MAPREDUCE-1441. If a user specifies mapred.local.dir with whitespace around directory names, it should be trimmed. Same goes for any LocalDirAllocator-based configuration."
HADOOP-6531,add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory,"Add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory, not directory itself. This will be useful if we want to clear the contents of cwd."
HADOOP-6527,UserGroupInformation::createUserForTesting clobbers already defined group mappings,"In UserGroupInformation::createUserForTesting the follow code creates a new groups instance, obliterating any groups that have been previously defined in the static groups field.
{code}    if (!(groups instanceof TestingGroups)) {
      groups = new TestingGroups();
    }
{code}
This becomes a problem in tests that start a Mini{DFS,MR}Cluster and then create a testing user.  The user that started the user (generally the real user running the test) immediately has their groups wiped out and is prevented from accessing files/folders/queues they should be able to.  Before the UserGroupInformation.createRemoteUserForTesting, calls to userA.getGroups may return {""a"", ""b"", ""c""} and immediately after the new fake user is created, the same call will return an empty array."
HADOOP-6522,TestUTF8 fails,TestUTF8 is actually flaky. It generates 10 random strings to run the test on. If you change this number to 100000 it fails every time.
HADOOP-6521,FsPermission:SetUMask not updated to use new-style umask setting.,"FsPermission:
{code}
221   /** Set the user file creation mask (umask) */
222   public static void setUMask(Configuration conf, FsPermission umask) {                                    
223     conf.setInt(UMASK_LABEL, umask.toShort());
224   }
{code}
Needs to be updated to not use a decimal value. This is a bug introduced by HADOOP-6234.  "
HADOOP-6520,UGI should load tokens from the environment,"For MapReduce tasks, we need not only the task itself, but child processes to get the delegation tokens, therefore we need an environment variable that if set loads a token cache into the login UGI. The task tracker can set this property so that tasks (and child processes of the tasks, such as streaming and pipes) have the job's tokens automatically."
HADOOP-6518,Kerberos login in UGI should honor KRB5CCNAME,UGI should honor the environment variable KRB5CCNAME as the location of the ticket cache.
HADOOP-6517,Ability to add/get tokens from UserGroupInformation,We need to be able to get and set tokens in the user's UGI object.
HADOOP-6515,Make maximum number of http threads configurable,"We found that http server threads may use considerable amount of resource in NameNode and JobTracker.
It would be good if the number of threads is allowed to be configured to a smaller number."
HADOOP-6510,doAs for proxy user,This jira will add support for a superuser authenticating on behalf of a proxy user.
HADOOP-6508,Incorrect values for metrics with CompositeContext,"In our clusters, when we use CompositeContext with two contexts, second context gets wrong values.
This problem is consistent on 500 (and above) node cluster."
HADOOP-6507,Hadoop Common Docs - delete 3 doc files that do not belong under Common,"Delete these 3 files from Hadoop Common TRUNK

\src\docs\src\documentation\content\xdocs\streaming.xml  (this file already in (and belongs in) Hadoop MAPREDUCE TRUNK)
\src\docs\src\documentation\content\xdocs\libhdfs.xml        (this file already in (and belongs in) Hadoop HDFS TRUNK)
\src\docs\src\documentation\content\xdocs\hdfs_permissions_guide.xml  (this file already in (and belongs in) Hadoop HDFS TRUNK)"
HADOOP-6505,sed in build.xml fails,"I'm not sure whether this is a Solaris thing or an ant 1.7.1 thing, but it definitely doesn't do what it is supposed to.  Instead of getting SunOS-x86-32 (or whatever) I get -x86-32.

This patch replaces the sed call with tr. "
HADOOP-6504,Invalid example in the documentation of org.apache.hadoop.util.Tool,"The example with the class {{MyApp}} doesn't work.
The {{run}} method needs to return an int ({{return 0;}} needs to be added at the end of the method) and {{main}} does a {{new Sort()}} instead of {{new MyApp()}}"
HADOOP-6497,Introduce wrapper around FSDataInputStream providing Avro SeekableInput interface,Reading data from avro files requires using Avro's SeekableInput interface; we need to be able to wrap FSDataInputStream in this interface.
HADOOP-6496,HttpServer sends wrong content-type for CSS files (and others),"CSS files are send as text/html causing problems if the HTML page is rendered in standards mode. The HDFS interface for example still works because it is rendered in quirks mode, the HBase interface doesn't work because it is rendered in standards mode. See HBASE-2110 for more details.

I've had a quick look at HttpServer but I'm too unfamiliar with it to see the problem. I think this started happening with HADOOP-6441 which would lead me to believe that the filter is called for every request and not only *.jsp and *.html. I'd consider this a bug but I don't know enough about this to provide a fix."
HADOOP-6492,Make avro serialization APIs public,Some avro-specific serialization methods need to be public for MAPREDUCE-815
HADOOP-6490,Path.normalize should use StringUtils.replace in favor of String.replace,"in our environment, we are seeing that the JobClient is going out of memory because Path.normalizePath(String) is called several tens of thousands of times, and each time it calls ""String.replace"" twice.

java.lang.String.replace compiles a regex to do the job which is very costly.
We should use org.apache.commons.lang.StringUtils.replace which is much faster and consumes almost no extra memory.
"
HADOOP-6489,"Findbug report: LI_LAZY_INIT_STATIC, OBL_UNSATISFIED_OBLIGATION","From findbug report:

Method org.apache.hadoop.io.compress.CompressionCodecFactory.main(String[]) may fail to clean up java.io.OutputStream

Incorrect lazy initialization of static field org.apache.hadoop.fs.FileContext.localFsSingleton in org.apache.hadoop.fs.FileContext.getLocalFSFileContext()

Incorrect lazy initialization of static field org.apache.hadoop.util.ReflectionUtils.serialFactory in org.apache.hadoop.util.ReflectionUtils.getFactory(Configuration)

Given that these have simple fixes I think one bug is enough.

This is from findbug 1.3.9 running on current trunk:

Path: .
URL: http://svn.apache.org/repos/asf/hadoop/common/trunk
Repository Root: http://svn.apache.org/repos/asf
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 898558
Node Kind: directory
Schedule: normal
Last Changed Author: tomwhite
Last Changed Rev: 897023
Last Changed Date: 2010-01-07 13:43:38 -0800 (Thu, 07 Jan 2010)
"
HADOOP-6486,fix common classes to work with Avro 1.3 reflection,A few minor changes are required to get some common classes to work correctly with Avro 1.3 reflection.
HADOOP-6479,TestUTF8 assertions could fail with better text ,"My HADOOP-6220 patch failed on Hudson with an error TestUTF8, but not one that is in any way useful:
{code}
Error Message: null

Stacktrace:

junit.framework.AssertionFailedError: null
	at org.apache.hadoop.io.TestUTF8.testIO(TestUTF8.java:80)
{code}
The tests should use better assertions for easier diagnostics"
HADOOP-6478,0.21 -   .eclipse-templates/.classpath out of sync with file system ,"some of the jars  in .classpath of  branch-0.21 out of sync with the file system retrieved by ivy . 

"
HADOOP-6477,0.21.0 - upload of the latest snapshot to apache snapshot repository,"Can you help upload the snapshot from the source control to hadoop-core for branch-0.21 - 

http://repository.apache.org/snapshots/org/apache/hadoop/hadoop-core/0.21.0-SNAPSHOT/  . 

HBASE-1433 , about enabling dependency management for ivy , would need this . "
HADOOP-6472,add tokenCache option to GenericOptionsParser for passing file with secret keys to a map reduce job,"for MAPRED-1338 - we need an option to pass a file with secreteKeys (tokens) to a mapreduce Job.
Name of the file is set into the config and will be picked up by JobSubmiter.
"
HADOOP-6471,StringBuffer -> StringBuilder - conversion of references as necessary,"Across hadoop-common codebase,  a good number of StringBuffer-s being used are actually candidates for StringBuilders , since the reference does not escape the scope of the method and no concurrency is needed. "
HADOOP-6467,Performance improvement for liststatus on directories in hadoop archives.,A liststatus call on a directory in hadoop archives leads to ( 2* number of files in directory) open calls to the namenode. This is very sub optimal and needs to be fixed to make it performant enough to be used on a daily basis. 
HADOOP-6462,"contrib/cloud failing, target ""compile"" does not exist","I'm not seeing this mentioned in hudson or other bugreports, which confuses me. With the addition of a src/contrib/cloud/build.xml from HADOOP-6426, contrib/build.xml won't build no more: 
hadoop-common/src/contrib/build.xml:30: The following error occurred while executing this line:
Target ""compile"" does not exist in the project ""hadoop-cloud"". 

What is odd is this: the final patch of HADOOP-6426 does include the stub <target> files needed, yet they aren't in SVN_HEAD. Which implies that a different version may have gone in than intended. "
HADOOP-6461,webapps aren't located correctly post-split,"Post-split, when one builds common it creates an empty build/webapps dir. If you then try to launch the NN for example using HADOOP_HDFS_HOME=/path/to/hdfs hdfs namenode, HttpServer.getWebAppsPath locates the empty common webapps dir, and the NN web UI fails to load."
HADOOP-6453,Hadoop wrapper script shouldn't ignore an existing JAVA_LIBRARY_PATH,"Currently the hadoop wrapper script assumes its the only place that uses JAVA_LIBRARY_PATH and initializes it to a blank line.

JAVA_LIBRARY_PATH=''

This prevents anyone from setting this outside of the hadoop wrapper (say hadoop-config.sh) for their own native libraries.

The fix is pretty simple. Don't initialize it to '' and append the native libs like normal. "
HADOOP-6452,Hadoop JSP pages don't work under a security manager,"When you run Hadoop under a security manager that says ""yes"" to all security checks, you get stack traces when Jetty tries to initialise the JSP engine. Which implies you can't use Jasper under a security manager"
HADOOP-6451,Contrib tests are not being run,The test target in src/contrib/build.xml references contrib modules that are no longer there post project split. This was discovered in HADOOP-6426.
HADOOP-6443,Serialization classes accept invalid metadata,The {{SerializationBase.accept()}} methods of several serialization implementations use incorrect metadata when determining whether they are the correct serializer for the user's metadata.
HADOOP-6441,Prevent remote CSS attacks in Hostname and UTF-7.,There are currently vulnerabilities for CSS in Hadoop's Web UI that allow CSS attacks.
HADOOP-6439,Shuffle deadlocks on wrong number of maps,"The new shuffle assumes that the number of maps is correct. The new JobSubmitter sets the old value. Something misfires in the middle causing:

09/12/01 00:00:15 WARN conf.Configuration: mapred.job.split.file is deprecated. Instead, use mapreduce.job.splitfile
09/12/01 00:00:15 WARN conf.Configuration: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps

But my reduces got stuck at 2 maps / 12 when there were only 2 maps in the job.
"
HADOOP-6436,Remove auto-generated native build files ,The repo currently includes the automake and autoconf generated files for the native build. Per discussion on HADOOP-6421 let's remove them and use the host's automake and autoconf. We should also do this for libhdfs and fuse-dfs. 
HADOOP-6435,Make RPC.waitForProxy with timeout public,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive.

The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch"
HADOOP-6434,Make HttpServer slightly easier to manage/diagnose faults with,"It would be easier to work with HttpServer if
# webServer.isStarted() was exported
# the toString() method included the (hostname,port) in use
# Bind Exceptions raised in startup included the (hostname, port) requested"
HADOOP-6433,Add AsyncDiskService that is used in both hdfs and mapreduce,"Both MAPREDUCE-1213 and HDFS-611 are using a class called AsyncDiskService.
The idea is to create a thread pool per disk volume, and use that for scheduling async disk operations.
"
HADOOP-6432,Statistics support in FileContext,FileContext should have API to get statistics from underlying file systems.
HADOOP-6422,permit RPC protocols to be implemented by Avro,"To more easily permit Hadoop to evolve to use Avro RPC, I propose to change RPC to use different implementations for clients and servers based on the configuration.  This is not intended as an end-user configuration: only a single RPC implementation will be supported in a given release, but rather a tool to permit us to more easily develop and test new RPC implementations.  As such, the configuration parameters used would not be documented."
HADOOP-6420,String-to-String Maps should be embeddable in Configuration,"Per MAPREDUCE-1126, we need to be able to take a map of (key, value) pairs and embed that inside a Configuration object."
HADOOP-6419,Change RPC layer to support SASL based mutual authentication,"The authentication mechanism to use will be SASL DIGEST-MD5 (see RFC-2222 and RFC-2831) or SASL GSSAPI/Kerberos. Since J2SE 5, Sun provides a SASL implementation by default. Both our delegation token and job token can be used as credentials for SASL DIGEST-MD5 authentication."
HADOOP-6415,Adding a common token interface for both job token and delegation token,Both job token and delegation token will be used by RPC.
HADOOP-6414,Add command line help for -expunge command.,Command line help for *hadoop fs -expunge* command is missing. 
HADOOP-6413,Move TestReflectionUtils to Common,The common half of MAPREDUCE-1209
HADOOP-6411,Remove deprecated file src/test/hadoop-site.xml,"hadoop-site.xml is deprecated. core-site.xml has to be used instead. However, hadoop-site.xml has been left in src/test folder which causes a lot of confusion and warning messages in the test runs."
HADOOP-6410,Rename TestCLI class to prevent JUnit from trying to run this class as a test,"TestCLI is a helper class which implements some common functionality for CLI based test (e.g {{TestDHFSCLI}}.
It doesn't include any tests by itself. However, JUnit tries to run it as a test because it has Test prefix in its name."
HADOOP-6409,TestHDFSCLI has to check if it's running any testcases at all,"There's a number of occasions when TestHDFSCLI reports a successful execution however doesn't run any tests at all.
For a typical case please take a [look here|http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/162/testReport/org.apache.hadoop.cli/TestCLI/testAll/]"
HADOOP-6408,Add a /conf servlet to dump running configuration,"HADOOP-6184 added a command line flag to dump the running configuration. It would be great for cluster troubleshooting to provide access to this as a servlet, preferably in both JSON and XML formats. But really, any format would be better than nothing. This should/could go into all of the daemons."
HADOOP-6407,Have a way to automatically update Eclipse .classpath file when new libs are added to the classpath through Ivy,"Currently Eclipse configuration (namely .classpath) isn't synchronized automatically when lib versions are changed. This causes great inconvenience so people have to change their project settings manually, etc. 

It'd be great if these configs could be updated automatically every time such a change takes place, e.g. whenever ivy is pulling in new version of a jar. "
HADOOP-6405,Update Eclipse configuration to match changes to Ivy configuration,"The .eclipse_templates/.classpath file doesn't match the Ivy configuration, so I've updated it to use the right version of commons-logging"
HADOOP-6404,Rename the generated artifacts to common instead of core,"In the project split we renamed Core to Common, but failed to change the artifact names. We should do it before 0.21.0 is released."
HADOOP-6403,Deprecate EC2 bash scripts,"With the addition of python-based EC2 scripts introduced in HADOOP-6108, the bash scripts in src/contrib/ec2 should be deprecated."
HADOOP-6402,testConf.xsl is not well-formed XML,"File {{/org/apache/hadoop/cli/testConf.xsl}} is not valid XML, as the <?xml?> directive comes after the comment. XML requires this to be the first thing in a file, so it can be used to determine the encoding."
HADOOP-6400,Log errors getting Unix UGI,"For various reasons, the calls out to `whoami` and `id` can fail when trying to get the unix UGI information. Currently it silently ignores failures and uses the default DrWho/Tardis ugi. This is extremely confusing for users - we should log the exception at warn level when the shell execs fail."
HADOOP-6398,Build is broken after HADOOP-6395 patch has been applied,"After new version of AspectJ jar files were introduces by HADOOP-6395 patch Common and HDFS builds are broken
{noformat}
compile-fault-inject:
  [taskdef] Could not load definitions from resource org/aspectj/tools/ant/taskdefs/aspectjTaskdefs.properties. It could not be found.
     [echo] Start weaving aspects in place

BUILD FAILED
/Users/xxx/work/H0.22/common/src/test/aop/build/aop.xml:78: The following error occurred while executing this line:
/Users/xxx/work/H0.22/common/src/test/aop/build/aop.xml:56: Problem: failed to create task or type iajc
{noformat}
"
HADOOP-6396,Provide a description in the exception when an error is encountered parsing umask,"Currently when there is a problem parsing a umask, the exception text is just the offending umask with no other clue, which can be quite confusing as demonstrated in HDFS-763.  This message should include the nature of the problem and whether or not the umask parsing attempt was using old-style or new-style values."
HADOOP-6395,Inconsistent versions of libraries are being included,"We currently include inconsistent versions of libraries. In particular,

aspectjtools and aspectjrt: 1.6.4 -> 1.6.5
avro: 1.0.0 -> 1.2.0
commons-logging: 1.0.4 -> 1.1.1
commons-logging-api: 1.0.4 -> 1.1"
HADOOP-6394,Helper class for FileContext tests,"    A helper class for FileContext tests should contain common methods which can be used in many unit tests, so that every unit test doesn't have to re-implement these functionality.
Examples of such methods:
  createFile(FileContext fc, Path path, int numBlocks, int blockSize)  //To create a file with number of blocks and block-size passed.
  getTestRootPath(FileContext fc)"
HADOOP-6391,Classpath should not be part of command line arguments,"Because bin/hadoop and bin/hdfs put the entire CLASSPATH in the command line arguments, it exceeds 4096 bytes, which is the maximum size that ps (or /proc) can work with. This makes looking for the processes difficult, since the output gets truncated for all components at the same point (e.g. NameNode, SecondaryNameNode, DataNode). 

The mapred sub-project does not have this problem, because it calls ""export CLASSPATH"" before the final exec. bin/hadoop and bin/hdfs should do the same thing"
HADOOP-6390,Block slf4j-simple from avro's pom,"Currently, we end up with two implementations of slf4j in Common via Avro's pom file.

Until AVRO-238 is released, we need to exclude slf4j-simple."
HADOOP-6386,NameNode's HttpServer can't instantiate InetSocketAddress: IllegalArgumentException is thrown,"In an execution of a tests the following exception has been thrown:
{noformat}
Error Message

port out of range:-1

Stacktrace
java.lang.IllegalArgumentException: port out of range:-1
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:118)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:371)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.activate(NameNode.java:313)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:304)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:410)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:404)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1211)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:287)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:131)
	at org.apache.hadoop.hdfs.server.namenode.TestEditLog.testEditLog(TestEditLog.java:92)
{noformat}"
HADOOP-6385,dfs does not support -rmdir (was HDFS-639),"From HDFS-639
> Given we have a mkdir, we should have a rmdir. Using rmr is not
> a reasonable substitute when you only want to delete empty
> directories."
HADOOP-6376,slaves file to have a header specifying the format of conf/slaves file ,"When we open the file conf/slaves - it is not immediately obvious what the format of the file is ( a comma-separated list or one per each line). The docs confirm it is 1 per line. 

Specifying the information by means of a comment in the template so that it is easy to modify the same, and self-explanatory. "
HADOOP-6375,Update documentation for FsShell du command,"HADOOP-4861 added new syntax for the FsShell du command, but neglected to update documentation. This JIRA is to add documentation for it to both branch-21 and trunk."
HADOOP-6374,JUnit tests should never depend on anything in conf,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete conf/mapred-queues.xml out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.
HADOOP-6367,Move Access Token implementation from Common to HDFS,"Access Token is HDFS specific and should be part of HDFS code base. Also, rename AccessToken to BlockAccessToken (and AccessKey to BlockAccessKey) to be more precise."
HADOOP-6366,Reduce ivy console output to ovservable level,It is very hard to see what's going in the build because ivy is literally flood the console with nonsensical messages...
HADOOP-6353,Create Apache Wiki page for JSure and FlashLight tools,"Need a page describing how to download, install, apply the license, and use concurrency and dynamic analysis tools from SureLogic"
HADOOP-6350,Documenting Hadoop metrics,"Metrics should be part of public API, and should be clearly documented similar to HADOOP-5073, so that we can reliably build tools on top of them."
HADOOP-6347,run-test-core-fault-inject runs a test case twice if -Dtestcase is set,"When a testcase is specified through {{-Dtestcase=}} and {{run-test-core-fault-inject}} is executed then the same test case is ran twice. Apparently it has to be executed only one time.

Also, it seems to be excessive to search for a fault injection test when {{run-fault-inject-with-testcaseonly}} is executed."
HADOOP-6346,Add support for specifying unpack pattern regex to RunJar.unJar,"The changes in Common necessary for MAPREDUCE-967:
- Adds support for Pattern types to Configuration (plus unit test)
- Adds support to specify a Pattern to RunJar.unJar to decide which files get unpacked"
HADOOP-6344,rm and rmr fail to correctly move the user's files to the trash prior to deleting when they are over quota.  ,"With trash turned on, if a user is over his quota and does a rm (or rmr), the file is deleted without a copy being placed in the trash.

"
HADOOP-6343,Stack trace of any runtime exceptions should be recorded in the server logs. ,Hadoop RPC catches any server side exception and throws an IOException. Runtime excpetions should be recorded in the server logs before being thrown as IOException.
HADOOP-6341,Hudson giving a +1 though no tests are included.,"An example [here|https://issues.apache.org/jira/browse/MAPREDUCE-1160?focusedCommentId=12771147&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12771147]. We should revert to -1 overall, otherwise, there's a real chance developers will miss giving a justification."
HADOOP-6337,Update FilterInitializer class to be more visible and take a conf for further development,"Currently the FilterInitializer class, used to filter access to the Hadoop web interfaces, has its main method, initFilter, set to package private.  This means that any classes wishing to implement this type must be in the same package.  This should be public so FilterInitializers can reside in other packages.  

Also, currently all parameters to the FilterInitializer must be provided at compile time (or via a different configuration method).  It would be better if the FilterInitalizer::initFilter received a Configuration parameter so it can pull conf values out at run-time.  Alternatively, the class could implement Configured, but that seems heavier than is needed at this point.
"
HADOOP-6334,GenericOptionsParser does not understand uri for -files -libjars and -archives option,"If we give an uri for -files, -libjars and -archives option , in GenericOptionsParser. It gives FileNotFoundException."
HADOOP-6332,Large-scale Automated Test Framework,"Hadoop would benefit from having a large-scale, automated, test-framework. This jira is meant to be a master-jira to track relevant work.

----

The proposal is a junit-based, large-scale test framework which would run against _real_ clusters.

There are several pieces we need to achieve this goal:

# A set of utilities we can use in junit-based tests to work with real, large-scale hadoop clusters. E.g. utilities to bring up to deploy, start & stop clusters, bring down tasktrackers, datanodes, entire racks of both etc.
# Enhanced control-ability and inspect-ability of the various components in the system e.g. daemons such as namenode, jobtracker should expose their data-structures for query/manipulation etc. Tests would be much more relevant if we could for e.g. query for specific states of the jobtracker, scheduler etc. Clearly these apis should _not_ be part of the production clusters - hence the proposal is to use aspectj to weave these new apis to debug-deployments.

----

Related note: we should break up our tests into at least 3 categories:
# src/test/unit -> Real unit tests using mock objects (e.g. HDFS-669 & MAPREDUCE-1050).
# src/test/integration -> Current junit tests with Mini* clusters etc.
# src/test/system -> HADOOP-6332 and it's children"
HADOOP-6329,Add build-fi directory to the ignore list,The build-fi directory should be added to the git and svn ignore lists.
HADOOP-6327,Fix build error for one of the FileContext Tests,"The build fails in Hudson
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations.testWorkingDirectory  (from TestLocalFSFileContextMainOperations)
Failing for the past 5 builds (Since Failed#272 )
Took 88 ms.
add description
Error Message

chmod: changing permissions of `/tmp/existingDir': Operation not permitted 

Stacktrace

org.apache.hadoop.util.Shell$ExitCodeException: chmod: changing permissions of `/tmp/existingDir': Operation not permitted

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:243)
	at org.apache.hadoop.util.Shell.run(Shell.java:170)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:363)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:449)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:432)
	at org.apache.hadoop.fs.RawLocalFileSystem.execCommand(RawLocalFileSystem.java:545)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:537)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:347)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:184)
	at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:769)
	at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:539)
	at org.apache.hadoop.fs.FileContextMainOperationsBaseTest.testWorkingDirectory(FileContextMainOperationsBaseTest.java:170)
"
HADOOP-6326,Hundson runs should check for AspectJ warnings and report failure if any is present,"When a code modifications break AspectJ bindings the following warning appears in the ant's build output:
{noformat}
     [echo] Start weaving aspects in place
     [iajc] warning at /Users/cos/work/branch-0.21/src/test/aop/org/apache/hadoop/hdfs/server/datanode/FSDatasetAspects.aj:55::0 advice defined in org.apache.hadoop.hdfs.server.datanode.FSDatasetAspects has not been applied [Xlint:adviceDidNotMatch]
     [echo] Weaving of aspects is finished
{noformat}

Build process (in particular under Hudson) needs to check the output for such warnings and fail the build if any is present"
HADOOP-6323,Serialization should provide comparators,The Serialization interface should permit one to create raw comparators.
HADOOP-6321,Hadoop Common - Site logo,"Hadoop Common - Site Logo

Update the logo (see attached jpg).
Image has elephant + common.

With this update, Site logo and Documentation logo will be the same."
HADOOP-6318,Upgrade to Avro 1.2.0,"Avro 1.2 has been released.  The API's Hadoop Common uses have been simplified, and it should be upgraded."
HADOOP-6314,"""bin/hadoop fs -help count""  fails to show help about only ""count"" command. ","Currently ""hadoop fs -help count"" fails to show help about only count command. 
Instead it displays following output

{noformat}
[rphulari@statepick-lm]> bin/hadoop fs -help count
hadoop fs is the command to execute fs commands. The full syntax is: 

hadoop fs [-fs <local | file system URI>] [-conf <configuration file>]
	[-D <property=value>] [-ls <path>] [-lsr <path>] [-df [<path>]] [-du <path>]
	[-dus <path>] [-mv <src> <dst>] [-cp <src> <dst>] [-rm [-skipTrash] <src>]
	[-rmr [-skipTrash] <src>] [-put <localsrc> ... <dst>] [-copyFromLocal <localsrc> ... <dst>]
	[-moveFromLocal <localsrc> ... <dst>] [-get [-ignoreCrc] [-crc] <src> <localdst>
	[-getmerge <src> <localdst> [addnl]] [-cat <src>]
	[-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>] [-moveToLocal <src> <localdst>]
	[-mkdir <path>] [-report] [-setrep [-R] [-w] <rep> <path/file>]
	[-touchz <path>] [-test -[ezd] <path>] [-stat [format] <path>]
	[-tail [-f] <path>] [-text <path>]

..
..
..

{noformat}

Expected output of ""bin/hadoop fs -help count "" should be 
{noformat}
[rphulari@statepick-lm]> bin/hadoop  fs -help count
-count[-q] <path>: Count the number of directories, files and bytes under the paths
		that match the specified file pattern.  The output columns are:
		DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or
		QUOTA REMAINING_QUATA SPACE_QUOTA REMAINING_SPACE_QUOTA 
		      DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME
{noformat}
"
HADOOP-6313,Expose flush APIs to application users,"Earlier this year, Yahoo, Facebook, and Hbase developers had a roundtable discussion where we agreed to support three types of flush in HDFS (API1, 2, and 3) and the append project aims to implement API2. Here is a proposal to expose these APIs to application users.
1. Three flush APIs
* API1: flushes out from the address space of client into the socket to the data nodes.   On the return of the call there is no guarantee that that data is out of the underlying node and no guarantee of having reached a DN.  New readers will eventually see this data if there are no failures.
* API2: flushes out to all replicas of the block. The data is in the buffers of the DNs but not on the DN's OS buffers.  New readers will see the data after the call has returned. 
* API3: flushes out to all replicas and all replicas have done posix fsync equivalent - ie the OS has flushed it to the disk device (but the disk may have it in its cache).

2. Support flush APIs in FS
* FSDataOutputStream#flush supports API1
* FSDataOutputStream implements Syncable interface defined below. If its wrapped output stream (i.e. each file system's stream) is Syncable, FSDataOutputStream#hflush() and hsync() call its wrapped output stream's hflush & hsync.
{noformat}
  public interface Syncable {
    public void hflush() throws IOException;  // support API2
    public void hsync() throws IOException;   // support API3
  }
{noformat}
* In each file system, if only hflush() is implemented, hsync() by default calls hflush().  If only hsync() is implemented, hflush() by default calls flush()."
HADOOP-6309,Enable asserts for tests by default,What do people think of making the tests run with java asserts enabled by default?
HADOOP-6307,Support reading on un-closed SequenceFile,"When a SequenceFile.Reader is constructed, it calls fs.getFileStatus(file).getLen().  However, fs.getFileStatus(file).getLen() does not return the hflushed length for un-closed file since the Namenode does not know the hflushed length.  DFSClient have to ask a datanode for the length last block which is being written; see also HDFS-570."
HADOOP-6305,Unify build property names to facilitate cross-projects modifications,"Current build files of {{common}}, {{mapreduce}}, {{hdfs}} have their own unique ways of naming properties which otherwise have very same meaning. I.e. the locations of java source code are called {{core.src.dir}}, {{mapred.src.dir}}, and {{hdfs.src.dir}} respectively.

It makes modifications needed for all three projects to be very unique thus increasing the number of difference between builds of the projects."
HADOOP-6303,Eclipse .classpath template has outdated jar files and is missing some new ones.,Vanilla checkout of common project has a number of problems with the eclipse templates. Avro jar listed in {{.classpath}} is very an one and it also misses some jackson jar files.
HADOOP-6301,Need to post Injection HowTo to Apache Hadoop's Wiki ,The HowTo page on how to use Hadoop injection framework for testing and development has to be posted on the wiki
HADOOP-6299,Use JAAS LoginContext for our login,Currently we use a custom login module in UnixUserGroupInformation for acquiring user-credentials (via config or exec'ing 'whoami'). We should switch to using standard JAAS components such as LoginContext and possibly implement a custom UnixLoginContext for our current requirements. In future we can use this for Kerberos etc. 
HADOOP-6298,BytesWritable#getBytes is a bad name that leads to programming mistakes,"Pretty much everyone at Rapleaf who has worked with Hadoop has misused BytesWritable#getBytes at some point, not expecting the byte array to be padded. I think we can completely alleviate these programming mistakes by deprecating and renaming this method (again) to be more descriptive. I propose ""getPaddedBytes()"" or ""getPaddedValue()"". It would also be helpful to have a helper method ""getNonPaddedValue()"" that makes a copy into a non-padded byte array. "
HADOOP-6293,FsShell -text should work on filesystems other than the default,FsShell currently recognizes only the default FileSystem for the \-text command. It should take arbitrary paths
HADOOP-6292,Native Libraries Guide - Update ,"Native Libraries Guide - Update

Updated/edited guide for Hadoop 0.21 release. 
Some passages/statements were not clear (user confusion).

Note: Setting priority to Blocker to make sure these changes get into Hadoop 0.21.0 release (and to pave the way for future updates/additions to this guide)"
HADOOP-6289,Add interface classification stable & scope to common,
HADOOP-6286,The Glob methods in FileContext doe not deal with URIs correctly,"The glob methods in FileContext were copied from FileSystem where they dealt with the a single filesystem.
While they were extended to fit in FileContext they don't seem to deal with URI pathnames correctly.

For example the following two lines in FileContext may be the source of the problem - the scheme and authority seem to be 
ignored beyond these points. 

line 1013: String filename = pathPattern.toUri().getPath();
line 1035:  String filename = pathPattern.toUri().getPath();

"
HADOOP-6285,HttpServer.QuotingInputFilter has the wrong signature for getParameterMap,"In the HDFS tests, we see:
{noformat}
java.lang.ClassCastException: [Ljava.lang.String; cannot be cast to java.lang.String
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter$RequestQuoter.getParameterMap(HttpServer.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:44)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1124)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:613)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:361)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)
{noformat}

"
HADOOP-6283,The exception meessage in FileUtil$HardLink.getLinkCount(..) is not clear,"When a file is not found, FileUtil$HardLink.getLinkCount(..) shows the following error message.
{noformat}
java.lang.NumberFormatException: For input string: """"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:470)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.hadoop.fs.FileUtil$HardLink.getLinkCount(FileUtil.java:663)
	at org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.detachBlock(ReplicaInfo.java:209)
	...
stat: cannot stat `/home/tsz/hadoop/hdfs/h265/build/test/data/dfs/data/data3/current/finalized/blk_-94418387820168072_1001.meta':
 No such file or directory on file:/home/tsz/hadoop/hdfs/h265/build/test/data/dfs/data/data3/current/finalized/blk_-94418387820168072_1001.meta
{noformat}
It looks like that there was a uncaught NumberFormatException."
HADOOP-6281,HtmlQuoting throws NullPointerException,"{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.http.HtmlQuoting.quoteHtmlChars(HtmlQuoting.java:95)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter$RequestQuoter.getParameter(HttpServer.java:570)
	at org.apache.hadoop.hdfs.server.namenode.NamenodeJspHelper$HealthJsp.generateHealthReport(NamenodeJspHelper.java:168)
	at org.apache.hadoop.hdfs.server.namenode.dfshealth_jsp._jspService(dfshealth_jsp.java:96)
	...
{noformat}"
HADOOP-6279,Add JVM memory usage to JvmMetrics,"The JvmMetrics currently publish memory usage from the MemoryMXBean. This is useful, but doesn't include the total heap size (eg as displayed in the JT Web UI).

It would be nice to expose Runtime.getRuntime().maxMemory() as part of JvmMetrics.

It seems that Runtime.getRuntime().totalMemory() (used by the JT for ""memory used"") is the same as the 'memHeapCommittedM' which already exists."
HADOOP-6274,TestLocalFSFileContextMainOperations tests wrongly expect a certain order to be returned.,The test to be more forgiving on the return values of list status calls.
HADOOP-6271,Fix FileContext to allow both recursive and non recursive create and mkdir,Modify FileContext to allow recursive and non-recursive create and mkdir (see HADOOP-4952)
HADOOP-6270,FileContext needs to provide deleteOnExit functionality,FileSystem provided an API to the applications {{deleteOnExit(Path f)}} used for registering a path to be deleted on JVM shutdown or when calling {{FileSystem.close()}} methods. Equivalent functionality is required in FileContext.
HADOOP-6268,Add ivy jar to .gitignore,"The ivy/ivy-2.0.0-rc2.jar is fetched automatically by ant, so it should be ignored by version control. Since many people use git, we should add it to the .gitignore file"
HADOOP-6267,build-contrib.xml unnecessarily enforces that contrib projects be located in contrib/ dir,build-contrib.xml currently sets hadoop.root to ${basedir}/../../../. This path is relative to the contrib project which is assumed to be inside src/contrib/. We occasionally work on contrib projects in other repositories until they're ready to contribute. We can use the <dirname> ant task to do this more correctly.
HADOOP-6261,Junit tests for FileContextURI,FileContextURI unit tests. 
HADOOP-6260,Unit tests for FileSystemContextUtil.,This Jira is to add the unit tests associated with HADOOP-4952.
HADOOP-6257,Two TestFileSystem classes are confusing hadoop-hdfs-hdfwithmr,"I propose to rename hadoop-common/src/test/core/org/apache/hadoop/fs/TestFileSystem.java -> src/test/core/org/apache/hadoop/fs/TestFileSystemCaching.java.  Otherwise, it conflicts with hadoop-hdfs/src/test/hdfs-with-mr/org/apache/hadoop/fs/TestFileSystem.java."
HADOOP-6255,Create an rpm integration project,We should be able to create RPMs for Hadoop releases.
HADOOP-6254,s3n fails with SocketTimeoutException,"If a user's map function is CPU intensive and doesn't read from the input very quickly, compounded by the buffering of input, then S3 might think the connection has been lost and will close the connection. Then when the user attempts to read from the input again, they'll receive a SocketTimeoutException and the task will fail."
HADOOP-6252,Provide method to determine if a deprecated key was set in the config file,"HADOOP-6105 provided a method to deprecate config keys and transparently refer to the new key. However, it didn't provide a method to see if the deprecated key had been used in the config file.  This is useful when, if the deprecated key had been used, its value needs to be converted before use, for instance when we changed the umask format.  A method like ""boolean wasDeprecatedKeySet()"" would be great.  Patch shortly."
HADOOP-6250,test-patch.sh doesn't clean up conf/*.xml files after the trunk run.,"test-patch.sh doesn't clean up conf/*.xml files after trunk run. This is a problem as after applying patch , the *.xml files are not updated by the template files."
HADOOP-6246,Update umask code to use key deprecation facilities from HADOOP-6105,"When the new octal/symbolic umask JIRA (HADOOP-6234) went through, the config key deprecation patch (HADOOP-6105) wasn't ready.  Now that both are committed, the home-brewed key deprecation system from 6234 should be updated to use the new system."
HADOOP-6243,NPE in handling deprecated configuration keys.,"I run TestFileCreation in Eclipse and get a NullPointerException. Debugging shows that {{Configuration.populateDeprecationMapping()}} sets {{Configuration.properties}} to null.
This code was introduced in HADOOP-6105."
HADOOP-6240,Rename operation is not consistent between different implementations of FileSystem,"The rename operation has many scenarios that are not consistently implemented across file systems.
"
HADOOP-6235,Adding a new method for getting server default values from a FileSystem,This is the changes made to Common to support file creation using server default values for a number of configuration params. See HDFS-578 for details.
HADOOP-6234,Permission configuration files should use octal and symbolic,"Currently, the settings for the default umask in Hadoop configuration files require the input format be in decimal.  Considering that every admin in the world thinks of permissions in octal and/or symbolic format, the config files should really use those two formats and drop decimal.

[... and, yes, I'm aware this breaks backwards compatibility.  But in this case, I think that is perfectly acceptable.]"
HADOOP-6233,Changes in common to rename the config keys as detailed in HDFS-531.,This jira tracks the code changes required in common to rename the config keys. The list of changed keys is attached to HDFS-531.
HADOOP-6230,"Move process tree, and memory calculator classes out of Common into Map/Reduce.",
HADOOP-6229,Atempt to make a directory under an existing file on LocalFileSystem should throw an Exception.,"This task is sub task of HDFS-303

{quote}
 ""1. When trying to make a directory under an existing file, HDFS throws an IOException while LocalFileSystem doesn't.
 An IOException seems good here. ""
{quote}
Actually HDFS throws FileNotFoundException in this case (in FSDirectory.mkdirs() ). So I guess we should do the same."
HADOOP-6227,Configuration does not lock parameters marked final if they have no value.,"A use-case: Recently, we stumbled on a bug that wanted us to disable the feature to run a debug script in map/reduce on tasks that fail, specified by mapred.{map|reduce}.task.debug.script. The best way of disabling it seemed to be to set it with no value in the cluster configuration and mark it final. This did not work, however, because the code in configuration explicitly checks if the value is not null before adding it to the list of final parameters.

Without an ability to do this, there might be a need to explicitly have a special way of disabling optional features."
HADOOP-6226,Create a LimitedByteArrayOutputStream that does not expand its buffer on write,"For MAPREDUCE-318, we would benefit from a ByteArrayOutputStream that would support a fixed size buffer that will not expand dynamically on writes. "
HADOOP-6224,Add a method to WritableUtils performing a bounded read of a String,MAPREDUCE-318 needs to sanity check vint-length-encoded Strings read from a byte stream. It would be appropriate to add this as a general utility method.
HADOOP-6223,New improved FileSystem interface for those implementing new files systems.,"The FileContext API (HADOOP-4952) provides an improved interface for the application writer.
This lets us simplify the FileSystem API since it will no longer need to deal with notions of default filesystem [ / ],  wd, and config
defaults for blocksize, replication factor etc. Further it will not need the many overloaded methods for create() and open() since
the FileContext API provides that convenience.
The FileSystem API can be simplified and can now be restricted to those implementing new file systems.


This jira proposes that we create new file system API,  and deprecate FileSystem API after a few releases."
HADOOP-6222,Core doesn't have TestCommonCLI facility,"TestCLI is a base class, which cannot run FS type of commands.
We need a ""copy"" of TestHDFSCLI as TestCommonCLI to be able to test CLI stuff in common.

I suggest we create TestCommonCLI.java in hadoop-common"
HADOOP-6221,RPC Client operations cannot be interrupted,"RPC.waitForProxy swallows any attempts to interrupt it while waiting for a proxy; this makes it hard to shutdown a service that you are starting; you have to wait for the timeouts. 

There are only 4-5 places in the code that use either of the two overloaded methods, removing the catch and changing the signature should not be too painful, unless anyone is using the method outside the hadoop codebase. "
HADOOP-6218,Split TFile by Record Sequence Number,"It would be nice if TFile can be split by Record Sequence Number. This way, columnar storage like PIG-833 can align fields that belong to the same row but in different columns."
HADOOP-6217,Hadoop Doc Split: Common Docs,"Hadoop Doc Split: Common Docs

Please note that I am unable to directly check all of the new links. Some links may break and will need to be updated."
HADOOP-6216,HDFS Web UI displays comments from dfs.exclude file and counts them as dead nodes,"I am putting comments in dfs.exclude file such as:

{noformat}
 # 32 GB memory upgrades
{noformat}

HDFS Web UI counts each word in the commented line as a dead node.
"
HADOOP-6204,Implementing aspects development and fault injeciton framework for Hadoop,"Fault injection framework implementation in HDFS (HDFS-435) turns out to be a very useful feature both for error handling testing and for various simulations.

There's certain demand for this framework, thus it need to be pulled up from HDFS and brought into Common, so other sub-projects will be able to share it if needed."
HADOOP-6203,Improve error message when moving to trash fails due to quota issue,"HADOOP-6080 provided an option for deleting files even when overquota, but the error message that's returned in this situation is unhelpful and doesn't suggest skipTrash as a remediation:
{noformat}$ hdfs -rmr /foo/bar/bat/boo

rmr: Failed to move to trash:
hdfs://cluster/foo/bar/bat/boo{noformat}
In this situation, the error message should say there was a quote problem and suggest -skipTrash.
"
HADOOP-6201,FileSystem::ListStatus should throw FileNotFoundException,"As discussed in HDFS-538, it would be better for FileSystem and its implementations to throw FileNotFoundException when the file is not found, rather than returning null.  This will bring it in line with getFileStatus and, I expect, default user expectations."
HADOOP-6199,Add the documentation for io.map.index.skip in core-default,io.map.index.skip is used in common. But is documented in MapReduce project. It should be documented in core-default.xml
HADOOP-6196,sync(0); next() breaks SequenceFile,"Currently, the end of the SequenceFile header is a sync block that isn't prefaced with SYNC_ESCAPE.  This means that sync(0) followed by next() fails."
HADOOP-6192,Shell.getUlimitMemoryCommand is tied to Map-Reduce,Currently org.apache.hadoop.util.Shell.getUlimitMemoryCommand relies on a MAPREDUCE specific configuration property for the memory limit. We should break the link.
HADOOP-6188,TestHDFSTrash fails because of TestTrash in common,
HADOOP-6185,Replace FSDataOutputStream#sync() by hflush(),"This jira aims add hflush() API to FSDataOutputStream and deprecates sync() API.

Note that the change should not commit to the trunk before hdfs append branch is merged to hdfs trunk."
HADOOP-6184,Provide a configuration dump in json format.,Configuration dump in json format.
HADOOP-6182,Adding Apache License Headers and reduce releaseaudit warnings to zero,"As of now rats tool shows 111 RA warnings 

[rat:report] Summary
[rat:report] -------
[rat:report] Notes: 18
[rat:report] Binaries: 118
[rat:report] Archives: 33
[rat:report] Standards: 942
[rat:report] 
[rat:report] Apache Licensed: 820
[rat:report] Generated Documents: 11
[rat:report] 
[rat:report] JavaDocs are generated and so license header is optional
[rat:report] Generated files do not required license headers
[rat:report] 
[rat:report] 111 Unknown Licenses
[rat:report] 
[rat:report] *******************************"
HADOOP-6181,Fixes for Eclipse template,"The Avro jar file is missing, and the entry for jets3t does not match the version downloaded by Ivy"
HADOOP-6180,Namenode slowed down when many files with same filename were moved to Trash,"This fix changes the naming of files moved to Trash.
Before files with the same name were appended with a running number 1,2,3.. and so on. Now it will have the current time in milliseconds attached to the name."
HADOOP-6177,FSInputChecker.getPos() would return position greater than the file size,"When using a small buffer (< 512 bytes) to read through the whole file, the final file position is 1+ the file size."
HADOOP-6176,Adding a couple private methods to AccessTokenHandler for testing purposes,"To support some test cases being added as part of HDFS-409, a couple private methods need to be added to AccessTokenHandler. One is for setting token lifetime and another is checking if a token has expired."
HADOOP-6175,Incorret version compilation with es_ES.ISO8859-15 locale on Solaris 10,"When you compile _hadoop_ on Solaris 10 with locale _es_ES.ISO8859-15_ the _src/saveVersion.sh_ script generates incorrect date on _build/src/org/apache/hadoop/package-info.java_

The ploblem is that the _saveVersion.sh_ script unsets the *LC_CTYPE* to avoid the problem, but on Solaris the _date_ command uses the *LC_TIME* enviroment variable as you can see at the _man page_

{noformat}
Specifications of native language translations of month  and
weekday  names  are  supported.  The month and weekday names
used for a language are based on the locale specified by the
environment variable LC_TIME. See environ(5).
{noformat}

Here's an example about *date* on Solaris

{quote}
$ echo $LC_CTYPE
es_ES.ISO8859-15
$ echo $LC_TIME
es_ES.ISO8859-15
$ date
lunes  3 de agosto de 2009 11H10'25"" CEST
$ unset LC_TYPE
$ date
lunes  3 de agosto de 2009 11H10'31"" CEST
$ unset LC_TIME
$ date
Mon Aug  3 11:10:35 CEST 2009
{quote}

So the _saveVersion.sh_ script creates the _package-info.java_ file as 

{code}
/*
 * Generated by src/saveVersion.sh
 */
@HadoopVersionAnnotation(version=""0.20.1-dev"", revision="""",
                         user=""itily"", date=""lunes  3 de agosto de 2009 11H16'01"" CEST"", url=""http://svn.apache.org/repos/asf/hadoop/common/tags/release-0.20.0"")
package org.apache.hadoop;
{code}

And if you run hadoop with _version_ argument it's says ""Unknow"", here's an example

{quote}
$ hadoop version
Hadoop Unknown
Subversion Unknown -r Unknown
Compiled by Unknown on Unknown
{quote}


To solve this issue is as simple as adding 

*unset LC_TIME*

to _saveVersion.sh_ script, and the output is as _C_ locale as
{code}
/*
 * Generated by src/saveVersion.sh
 */
@HadoopVersionAnnotation(version=""0.20.1-dev"", revision="""",
                         user=""itily"", date=""Mon Aug  3 11:19:41 CEST 2009"", url=""http://svn.apache.org/repos/asf/hadoop/common/tags/release-0.20.0"")
package org.apache.hadoop;
{code}


"
HADOOP-6173,"src/native/packageNativeHadoop.sh only packages files with ""hadoop"" in the name","src/native/packageNativeHadoop.sh only packages files with ""hadoop"" in the name. This becomes too restrictive when a user wants to inject third-party native libraries into his/her own tar build."
HADOOP-6172,bin/hadoop version not working,"Two problems found:
- ${build.src} not included in ant target ""compile-core-classes"", thus o.a.h.package-info.java is not compiled, which contains the version annotation.
- bin/hadoop-config.sh attempts to include jar files matching pattern hadoop-*-core.jar rather than hadoop-core-*.jar."
HADOOP-6170,add Avro-based RPC serialization,"Permit RPC protocols to use Avro to serialize requests and responses, so that protocols may better evolve without breaking compatibility."
HADOOP-6169,Removing deprecated method calls in TFile,We should remove the use of deprecated APIs in TFile.
HADOOP-6166,Improve PureJavaCrc32,Got some ideas to improve CRC32 calculation.
HADOOP-6165,Add metadata to Serializations,"The Serialization framework only allows a class to be passed as metadata. This assumes there is a one-to-one mapping between types and Serializations, which is overly restrictive. By permitting applications to pass arbitrary metadata to Serializations, they can get more control over which Serialization is used, and would also allow, for example, one to pass an Avro schema to an Avro Serialization."
HADOOP-6163,Progress class should provide an api if phases exist,"Progress class needs to provide an api for client to know if there are phases.
This is needed for Task.setProgress() to decide whether to update progress of task or progress of current phase in the task."
HADOOP-6161,Add get/setEnum to Configuration,It would be useful if Configuration had helper get/set methods for enumerated types.
HADOOP-6160,releaseaudit (rats) should not be run againt the entire release binary,
HADOOP-6158,Move CyclicIteration to HDFS,I think we should move CyclicIteration from Common utils to HDFS.
HADOOP-6155,deprecate Record IO,"With the advent of Avro, I think we should deprecate Record IO."
HADOOP-6152,Hadoop scripts do not correctly put jars on the classpath,"The various Hadoop scripts (bin/hadoop, bin/hdfs, bin/mapred) do not properly identify the jars needed to run Hadoop. They try to include hadoop-*-hdfs.jar, etc, rather than the hadoop-hdfs-*.jar that is actually created by the 'ant jar' and 'ant package' targets."
HADOOP-6151,The servlets should quote html characters,"We need to quote html characters that come from user generated data. Otherwise, all of the web ui's have cross site scripting attack, etc."
HADOOP-6150,Need to be able to instantiate a comparator instance from a comparator string without creating a TFile.Reader object,"Occasionally, we want have the same instance of comparator object represented by the TFile comparator string. We should be able to do that without requiring to first open up a tfile that was previously written use the same comparator."
HADOOP-6148,Implement a pure Java CRC32 calculator,"We've seen a reducer writing 200MB to HDFS with replication = 1 spending a long time in crc calculation. In particular, it was spending 5 seconds in crc calculation out of a total of 6 for the write. I suspect that it is the java-jni border that is causing us grief."
HADOOP-6146,Upgrade to JetS3t version 0.7.1,The JetS3t library is used for the S3 filesystems. We should upgrade to the latest version (0.7.1) which has support for Requester Pays buckets. 
HADOOP-6142,archives relative path changes in common.,HADOOP-3663 was moved to mapreduce accidentally. Opening this jira to upload my chanegs to common --- changes are rleated to MAPREDUCE-739.
HADOOP-6138,eliminate the depracate warnings introduced by H-5438,Eliminate the deprecated warnings introduced by HADOOP-5438.
HADOOP-6137,to fix project specific test-patch requirements ,only mapreduce project needs create-c++-configure target which needs to be executed as part to the test-core ant target.
HADOOP-6133,ReflectionUtils performance regression,"HADOOP-4187 introduced extra calls to Class.forName in ReflectionUtils.setConf. This caused a fairly large performance regression. Attached is a microbenchmark that shows the following timings (ms) for 100M constructions of new instances:

Explicit construction (new Test): around ~1.6sec
Using Test.class.newInstance: around ~2.6sec
ReflectionUtils on 0.18.3: ~8.0sec
ReflectionUtils on 0.20.0: ~200sec

This illustrates the ~80x slowdown caused by HADOOP-4187."
HADOOP-6132,RPC client opens an extra connection for VersionedProtocol,"RPC client caches connections per protocol. However, since all of our real protocols are subclasses of VersionedProtocol, a bug in the implementation makes the client opens an extra connection just for the VersionedProtocol, which is not needed."
HADOOP-6131,A sysproperty should not be set unless the property is set on the ant command line in build.xml.,"Patch for HADOOP-3315 contains an improper usage of setting a sysproperty. What it does now:
{code}
      <sysproperty key=""io.compression.codec.lzo.class""
          value=""${io.compression.codec.lzo.class}""/>
{code}
What should be:
{code} 
     <syspropertyset dynamic=""no"">
         <propertyref name=""io.compression.codec.lzo.class""/>
      </syspropertyset>
{code}"
HADOOP-6124,patchJavacWarnings and trunkJavacWarnings are not consistent.,The values of patchJavacWarnings and trunkJavacWarnings are not consistent when running test-patch.sh with an empty patch over Common.  HDFS and MapReduce seem not having this problem.
HADOOP-6123,hdfs script does not work after project split.,"There are problems running hdfs script from common.
# Usage message for hdfs does not have ""fs"" option anymore.
# There seem to be an undocumented option ""dfs"", but using it throws NoClassDefFoundError or ClassNotFoundException.
# Same with other options e.g. name-node.

May I am missing something here. How do we call ls these days?
Do we need to move hdfs script to hdfs project.
"
HADOOP-6122,64 javac compiler warnings,"Ran ""ant test-patch"" with a empty patch.  Got 
{noformat}
     [exec]     -1 javac.  The applied patch generated 64 javac compiler warnings (more than the trunk's current 124 warnings).
 {noformat}"
HADOOP-6120,Add support for Avro types in hadoop,Support to serialize and deserialize Avro types in Hadoop.
HADOOP-6114,bug in documentation: org.apache.hadoop.fs.FileStatus.getLen() ,"In javadoc method  org.apache.hadoop.fs.FileStatus.getLen()  writtend that this method ""return the length of this file, in blocks""
But method return size in bytes."
HADOOP-6112,to fix hudsonPatchQueueAdmin for different projects,"To fix hudsonPatchQueueAdmin process for different hadoop projects.

"
HADOOP-6109,Handle large (several MB) text input lines in a reasonable amount of time,"problem:
=======
hadoop was timing out on a simple pass-through job (with the default 10 min timeout)

cause:
=====
i hunted this down to how Text lines are being processed inside org.apache.hadoop.util.LineReader.
i have a fix, a task that took more than 20 minutes and still failed to complete, completes with this fix in under 30 s.
i attach the patch (for trunk)

the problem traces:
================

hadoop version: 0.19.0
userlogs on slave node:

2009-05-29 13:57:33,551 WARN org.apache.hadoop.mapred.TaskRunner: Parent died.  Exiting attempt_200905281652_0013_m_000006_1
[root@domU-12-31-38-01-7C-92 attempt_200905281652_0013_m_000006_1]#

tellingly, the last input line processed right before this WARN is 19K. (i log the full input line in the map function for debugging)

output on map-reduce task:

Task attempt_200905281652_0013_m_000006_2 failed to report status for 600 seconds. Killing!
09/05/29 14:08:01 INFO mapred.JobClient:  map 99% reduce 32%
09/05/29 14:18:05 INFO mapred.JobClient:  map 98% reduce 32%
java.io.IOException: Job failed!
    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1217)
    at com.adxpose.data.mr.DailyHeatmapAggregator.run(DailyHeatmapAggregator.java:547)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
    at com.adxpose.data.mr.DailyHeatmapAggregator.main(DailyHeatmapAggregator.java:553)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:165)
    at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
    at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)"
HADOOP-6106,Provide an option in ShellCommandExecutor to timeout commands that do not complete within a certain amount of time.,In MAPREDUCE-211 we came across a need to provide an option to timeout commands launched via the ShellCommandExecutor. The use case is for the health check script being developed in MAPREDUCE-211. We would like the TaskTracker thread to not be blocked by a problematic script or in instances where fork()+exec() has hung (which apparently has been observed in large clusters).
HADOOP-6105,Provide a way to automatically handle backward compatibility of deprecated keys,"There are cases when we have had to deprecate configuration keys. Use cases include, changing the names of variables to better match intent, splitting a single parameter into two - for maps, reduces etc.

In such cases, we typically provide a backwards compatible option for the old keys. The handling of such cases might typically be common enough to actually add support for it in a generic fashion in the Configuration class. Some initial discussion around this started in HADOOP-5919, but since the project split happened in between we decided to open this issue to fix it in common."
HADOOP-6103,Configuration clone constructor does not clone all the members.,"Currently, Configuration(Configuration other) constructor does not clone all the members.
It clones only resources, properties, overlay and finalParameters. It needs to clone loadDefaults, classLoader, defaultResources, quietmode.
This resulted in bugs like HADOOP-4975"
HADOOP-6099,Allow configuring the IPC module to send pings,"The IPC Client sets a socketTimeout for the time period specified by the pingInterval and then sends a ping every pingInterval. This means that if a RPC server does not respond to a RPC client, then the RPC client blocks forever. This is a problem for applications that wants to switch quickly from one un-responsive HDFS cluster  to a good one. "
HADOOP-6096,Fix Eclipse project and classpath files following project split,The Eclipse files need updating to use new paths following the split.
HADOOP-6090,GridMix is broke after upgrading random(text)writer to newer mapreduce apis,GridMix data generation scripts need to use the newer mapreduce api.
HADOOP-6079,"In DataTransferProtocol, the serialization of proxySource is not consistent","In DataTransferProtocol.OP_REPLACE_BLOCK, proxySource is written as BalancerDatanode but then the serialized value is read as a DatanodeInfo."
HADOOP-6076,Forrest documentation compilation is broken because of HADOOP-5913,"Forrest documentation is broken because of the commit of HADOOP-5913. The is causing test-patch to break with this error:

{noformat}
     [exec] validate-xdocs:
     [exec] ../src/docs/src/documentation/content/xdocs/commands_manual.xml:593:11: The element type ""td"" must be terminated by the matching end-tag ""</td>"".
{noformat}"
HADOOP-6074,TestDFSIO does not use configuration properly.,A part of TestDFSIO uses default conf instead of configuration passed in by MR framework. 
HADOOP-6056,Use java.net.preferIPv4Stack to force IPv4,"This was mentioned on HADOOP-3427, there is a property,  java.net.preferIPv4Stack, which you set to true for the java net process to switch to IPv4 everywhere. 

As Hadoop doesn't work on IPv6, this should be set to true in the startup scripts. Hopefully this will ensure that Jetty will also pick it up."
HADOOP-6031,Remove @author tags from Java source files,"Excluding first 2 results there are total 28 @author tags in java source files .

{code}
hostname:Hadoop rphulari$ grep -r ""@author""  .
./.svn/text-base/CHANGES.txt.svn-base: 69. HADOOP-1147.  Remove @author tags from Java source files.
./CHANGES.txt: 69. HADOOP-1147.  Remove @author tags from Java source files.
./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/IFSImpl.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSImpl.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSInputStream.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSOutputStream.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KosmosFileSystem.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/IFSImpl.java: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KFSImpl.java: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KFSInputStream.java: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KFSOutputStream.java: * @author: Sriram Rao (Kosmix Corp.)
./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java: * @author: Sriram Rao (Kosmix Corp.)
./src/test/bin/.svn/text-base/test-patch.sh.svn-base:### Check for @author tags in the patch
./src/test/bin/.svn/text-base/test-patch.sh.svn-base:  echo ""    Checking there are no @author tags in the patch.""
./src/test/bin/.svn/text-base/test-patch.sh.svn-base:  authorTags=`$GREP -c -i '@author' $PATCH_DIR/patch`
./src/test/bin/.svn/text-base/test-patch.sh.svn-base:  echo ""There appear to be $authorTags @author tags in the patch.""
./src/test/bin/.svn/text-base/test-patch.sh.svn-base:    -1 @author.  The patch appears to contain $authorTags @author tags which the Hadoop community has agreed to not allow in code contributions.""
./src/test/bin/.svn/text-base/test-patch.sh.svn-base:    +1 @author.  The patch does not contain any @author tags.""
./src/test/bin/test-patch.sh:### Check for @author tags in the patch
./src/test/bin/test-patch.sh:  echo ""    Checking there are no @author tags in the patch.""
./src/test/bin/test-patch.sh:  authorTags=`$GREP -c -i '@author' $PATCH_DIR/patch`
./src/test/bin/test-patch.sh:  echo ""There appear to be $authorTags @author tags in the patch.""
./src/test/bin/test-patch.sh:    -1 @author.  The patch appears to contain $authorTags @author tags which the Hadoop community has agreed to not allow in code contributions.""
./src/test/bin/test-patch.sh:    +1 @author.  The patch does not contain any @author tags.""
./src/test/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSEmulationImpl.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)
./src/test/core/org/apache/hadoop/fs/kfs/.svn/text-base/TestKosmosFileSystem.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)
./src/test/core/org/apache/hadoop/fs/kfs/KFSEmulationImpl.java: * @author: Sriram Rao (Kosmix Corp.)
./src/test/core/org/apache/hadoop/fs/kfs/TestKosmosFileSystem.java: * @author: Sriram Rao (Kosmix Corp.)
./src/test/hdfs/org/apache/hadoop/hdfs/.svn/text-base/TestModTime.java.svn-base: * @author Dhruba Borthakur
./src/test/hdfs/org/apache/hadoop/hdfs/TestModTime.java: * @author Dhruba Borthakur
hostname:Hadoop rphulari$ grep -r ""@author""  . | wc -l
      30    

{code}"
HADOOP-6017,NameNode and SecondaryNameNode fail to restart because of abnormal filenames.,"SecondaryNameNode (and NameNode) fail to load the edits log.  I will include stack trace in next comment.

This is traced to the fact that LeaseManager uses {{String.relaceFirst()}} to replace front of a sting with another string. Unfortunately {{replaceFirst()}} uses regex, though the first argument is {{quoted}} by the code, the second argument is not. (the second arg is not really treated as regex but still gets processed for back references (as in '{{sed s/first/second/g}}')

As Nicholas suggested, it is just simpler to use {{substring()}} to replace part of the string."
HADOOP-6009,S3N listStatus incorrectly returns null instead of empty array when called on empty root,"Null means the directory does not exist, which is obviously not the case."
HADOOP-6004,BlockLocation deserialization is incorrect,The {{readFields}} method of {{BlockLocation}} does not correctly allocate new arrays for topologyPaths and hosts. Patch shortly.
HADOOP-5992,Add ivy/ivy*.jar to .gitignore,"Git mirrors have been available at apache.org[1] for a while now, and judging from previous issues, it looks like there is a number of Hadoop developers using Git too.

The .gitignore file is already available in the repo, but it still doesn't include ivy/ivy*.jar.
Note that that entry has been added to svn:ignore[2], so it should be safe to add it to .gitignore too.

 1. http://git.apache.org/
 2. svn propget svn:ignore http://svn.apache.org/repos/asf/hadoop/core/trunk/ivy

Thanks."
HADOOP-5989,streaming tests fails trunk builds,"Recent check-in's to the trunk had introduced a dependency(run-time dependency for test's) on commons-cli for streaming. 

"
HADOOP-5981,HADOOP-2838 doesnt work as expected,"The substitution feature i.e X=$X:/tmp doesnt work as expected.

This issue completes the feature mentioned in HADOOP-2838. HADOOP-2838 provided a way to set env variables in child process. This issue provides a way to inherit tt's env variables and append or reset it. So now 
X=$X:y will inherit X (if  there) and append y to it. "
HADOOP-5980,LD_LIBRARY_PATH not passed to tasks spawned off by LinuxTaskController,"Currently, task spawned off by {{LinuxTaskController}} don't get LD_LIBRARY_PATH in their environment. The tasks should get same LD_LIBRARY_PATH value as when spawned off by {{DefaultTaskController}}"
HADOOP-5976,create script to provide classpath for external tools,It would be useful for tools building on top of Hadoop to have a script that returns the class path that is needed to get all of the Hadoop jars and the needed libraries.
HADOOP-5968,Sqoop should only print a warning about mysql import speed once,"After HADOOP-5844, Sqoop can use mysqldump as an alternative to JDBC for importing from MySQL. If you use the JDBC mechanism, it prints a warning if you could have enabled the mysqldump path instead. But the warning is printed multiple times (every time the LocalMySQLManager is instantiated), and also when the MySQL manager is used for informational queries (e.g., listing tables) rather than true imports.

It should only emit the warning once per session, and only then if it's actually doing an import."
HADOOP-5967,Sqoop should only use a single map task,"The current DBInputFormat implementation uses SELECT ... LIMIT ... OFFSET statements to read from a database table. This actually results in several queries all accessing the same table at the same time. Most database implementations will actually use a full table scan for each such query, starting at row 1 and scanning down until the OFFSET is reached before emitting data to the client. The upshot of this is that we see O(n^2) performance in the size of the table when using a large number of mappers, when a single mapper would read through the table in O(n) time in the number of rows.

This patch sets the number of map tasks to 1 in the MapReduce job sqoop launches."
HADOOP-5963,unnecessary exception catch in NNBench,"NNBench.createControlFiles - catches Exception (even though only IOException is thrown) and then throws a new IOException with message from original. This seems unnecessary and lossfull, because we loose stack info form the original exception.

Suggestion - remove the catch part of try block."
HADOOP-5961,DataNode should understand generic hadoop options,"DataNode should use ToolRunner to parse generic hadoop options (like -Dconfig.var=value). Apart from being more consistent with rest of the hadoop programs, this is necessary to write a simple script to run multiple datanodes on the same node.

I had proposed a simple shell script to running multiple datanodes on a the user mailing list http://www.nabble.com/Contributing-to-hadoop-td22198897.html, but it requires that DataNode parses -D option.

I will submit a patch and a shell script that works.

"
HADOOP-5958,Use JDK 1.6 File APIs in DF.java wherever possible,JDK 1.6 has File APIs like File.getFreeSpace() which should be used instead of spawning a command process for getting the various disk/partition related attributes. This would avoid spikes in memory consumption by tasks when things like LocalDirAllocator is used for creating paths on the filesystem.
HADOOP-5956,org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface test fails on trunk,
HADOOP-5955,TestFileOuputFormat can use LOCAL_MR instead of CLUSTER_MR,TestFileOutputFormat can use local MR instead of MiniMR. This brings down the execution time from 32 seconds to 3 seconds
HADOOP-5954,Fix javac warnings in HDFS tests,"There are javac warnings in the following tests:
{noformat}
src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java
src/test/hdfs/org/apache/hadoop/hdfs/TestSmallBlock.java
src/test/hdfs/org/apache/hadoop/hdfs/TestFileStatus.java
src/test/hdfs/org/apache/hadoop/hdfs/TestDFSShellGenericOptions.java
src/test/hdfs/org/apache/hadoop/hdfs/TestSeekBug.java
src/test/hdfs/org/apache/hadoop/hdfs/TestDFSStartupVersions.java
{noformat}"
HADOOP-5953,KosmosFileSystem.isDirectory() should not be deprecated.,FileSystem.isDirectory() was un-deprecated by HADOOP-5045.  We should do the same for KosmosFileSystem.
HADOOP-5952,Hudson -1 wording change,The wording should be changed when Hudson -1 a patch for no unit test updates.  New wording to be be added in comments.
HADOOP-5951,StorageInfo needs Apache license header.,The license headers are missing in this two files as reported by Flavio in HADOOP-5188.
HADOOP-5948,Modify TestJavaSerialization to use LocalJobRunner instead of MiniMR/DFS cluster,TestJavaSerialization currently uses MiniMR/DFS cluster. This test can be done with local job runner also. This reduces the run time of the test from 61 seconds to 6 seconds.
HADOOP-5947,org.apache.hadoop.mapred.lib.TestCombineFileInputFormat fails trunk builds,
HADOOP-5944,BlockManager needs Apache license header.,The license headers are missing in this two files as reported by Flavio in HADOOP-5188.
HADOOP-5940,trunk eclipse-plugin build fails while trying to copy commons-cli jar from the lib dir,
HADOOP-5935,Hudson's release audit warnings link is broken,"For example, on HADOOP-5170 the link http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/392/artifact/trunk/current/releaseAuditDiffWarnings.txt gives a 404. This makes it hard to work out which file or files are causing a problem."
HADOOP-5925,EC2 scripts should exit on error,"For example, if an ec2-authorize command fails the script should stop so that the problem is easier to debug."
HADOOP-5913,Allow administrators to be able to start and stop queues ,  This feature would provide functionality to stop and start queues in Hadoop at runtime. 
HADOOP-5902,4 contrib test cases are failing for the svn committed code,"Following 4 unit test cases are failing for the committed svn code.

    * org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles
    * org.apache.hadoop.streaming.TestStreamingBadRecords.testSkip
    * org.apache.hadoop.streaming.TestStreamingBadRecords.testNarrowDown
    * org.apache.hadoop.streaming.TestSymLink.testSymLink


This is also evident at the Hudson http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/ 
that every patch is facing this problem.  Can someone please see into this issue."
HADOOP-5900,Minor correction in HDFS Documentation ,"Remove incorrect statement from HDFS Document ( hdfs_design.html )  which says -

* HDFS does not yet implement user quotas or access permissions.*
"
HADOOP-5899,Minor - move info log to the right place to avoid printing unnecessary log,"Log below should only be printed in {{FSEditLog.processIOErros()}} encounters error condition. Currently this log is printed unnecessarily even when there is no error. 
{{INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: current list of storage dirs:....}}
"
HADOOP-5897,Add more Metrics to Namenode to capture heap usage,"Recently we had GC issues, where Namenode used more heap than usual. There was no growth indicated by the data in current Metrics to justify the heap usage. Adding more stats such as:
- Counter to track blocks that are pending deletion
- BlocksMap hashmap capacity
- Counter to track excess number of blocks 
"
HADOOP-5896,Remove the dependency of GenericOptionsParser on Option.withArgPattern,commons-cli released version does not have Option.withArgPattern API
HADOOP-5895,Log message shows -ve number of bytes to be merged in the final merge pass when there are no intermediate merges and merge factor is > number of segments,Log message shows -ve number of bytes to be merged in the final merge pass when there are no intermediate merges and the mergeFactor is >  total number of segments to be merged. This issue is because of code changes done in HADOOP-5572.
HADOOP-5891,"If dfs.http.address is default, SecondaryNameNode can't find NameNode","As detailed in this blog post:
http://www.cloudera.com/blog/2009/02/10/multi-host-secondarynamenode-configuration/
if dfs.http.address is not configured, and the 2NN is a different machine from the NN, the 2NN fails to connect.

In SecondaryNameNode.getInfoServer, the 2NN should notice a ""0.0.0.0"" dfs.http.address and, in that case, pull the hostname out of fs.default.name. This would fix the default configuration to work properly for most users."
HADOOP-5890,Use exponential backoff on Thread.sleep during DN shutdown,"Tests waste a lot of time in DataNode.shutdown. Typical logs look like:

{code}
2009-05-21 17:13:20,177 INFO  datanode.DataNode (DataNode.java:shutdown(637)) - Waiting for threadgroup to exit, active threads is 0
2009-05-21 17:13:20,177 INFO  datanode.DataBlockScanner (DataBlockScanner.java:run(620)) - Exiting DataBlockScanner thread.
2009-05-21 17:13:21,117 INFO  datanode.DataNode (DataNode.java:shutdown(637)) - Waiting for threadgroup to exit, active threads is 0
{code}

In this example (and very commonly) the DataBlockScanner thread exits within 5-10ms after the first wait. The DN then sleeps an entire second before succeeding in shutting down.

Using exponential backoff from a short value like 2ms up to a maximum of 1000ms would solve this."
HADOOP-5887,Sqoop should create tables in Hive metastore after importing to HDFS,"Sqoop (HADOOP-5815) imports tables into HDFS; it is a straightforward enhancement to then generate a Hive DDL statement to recreate the table definition in the Hive metastore and move the imported table into the Hive warehouse directory from its upload target.

This feature enhancement makes this process automatic. An import is performed with sqoop in the usual way; providing the argument ""--hive-import"" will cause it to then issue a CREATE TABLE .. LOAD DATA INTO statement to a Hive shell. It generates a script file and then attempts to run ""$HIVE_HOME/bin/hive"" on it, or failing that, any ""hive"" on the $PATH; $HIVE_HOME can be overridden with --hive-home. As a result, no direct linking against Hive is necessary.

The unit tests provided with this enhancement use a mock implementation of 'bin/hive' that compares the script it's fed with one from a directory full of ""expected"" scripts. The exact script file referenced is controlled via an environment variable. It doesn't actually load into a proper Hive metastore, but manual testing has shown that this process works in practice, so the mock implementation is a reasonable unit testing tool.
"
HADOOP-5879,GzipCodec should read compression level etc from configuration,"GzipCodec currently uses the default compression level. We should allow overriding the default value from Configuration.

{code}
  static final class GzipZlibCompressor extends ZlibCompressor {
    public GzipZlibCompressor() {
      super(ZlibCompressor.CompressionLevel.DEFAULT_COMPRESSION,
          ZlibCompressor.CompressionStrategy.DEFAULT_STRATEGY,
          ZlibCompressor.CompressionHeader.GZIP_FORMAT, 64*1024);
    }
  }
{code}
"
HADOOP-5878,Fix hdfs jsp import and Serializable javac warnings,"Fix javac warnings for
- unused import
- missing serialVersionUID"
HADOOP-5877,"Fix javac warnings in TestHDFSServerPorts, TestCheckpoint, TestNameEditsConfig, TestStartup and TestStorageRestore","All of these warnings relate to use of the deprecated SecondaryNameNode.  Since the 2ndNN isn't going to be excised anytime soon, these should all be suppressed with as narrow of a suppression scope as possible."
HADOOP-5873,Remove deprecated methods randomDataNode() and getDatanodeByIndex(..) in FSNamesystem,FSNamesystem.randomDataNode() and getDatanodeByIndex(..) can be replaced by getRandomDatanode().
HADOOP-5867,Cleaning NNBench* off javac warnings,"These files have a number of javac 'class depricated' warnings 
  src/test/hdfs-with-mr/org/apache/hadoop/hdfs/NNBench.java
  src/test/hdfs-with-mr/org/apache/hadoop/hdfs/NNBenchWithoutMR.java
It is possible to fix most of them plus make some readability improvements on the code."
HADOOP-5866,Move DeprecatedUTF8 to o.a.h.hdfs,"HADOOP-5823 added {{DeprecatedUTF8}} class as a wrapper for UTF8. Though UTF8 is deprecated, it is used in many places and most likely will continue to be used for quite sometime. My initial thought was that other packages might want to use the wrapper {{DeprecatedUTF8}}. 

But the current suggestion (discussed in HADOOP-5823) is to move the class to o.a.h.hdfs.

Alternately we could have just use ""@SuppressWarnings"" rather than introducing a new class."
HADOOP-5864,Fix DMI and OBL findbugs in packages hdfs and metrics,"This issue is to fix the following findbugs:
DMI in org.apache.hadoop.hdfs.server.namenode.INodeDirectory.getExistingPathINodes(byte[][], INode[]) 
OBL in org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(File, File, int)
OBL in org.apache.hadoop.hdfs.server.datanode.FSDataset.createBlockWriteStreams(File, File)
OBL in org.apache.hadoop.hdfs.server.datanode.FSDataset.getTmpInputStreams(Block, long, long)
OBL in org.apache.hadoop.metrics.ContextFactory.setAttributes() "
HADOOP-5861,s3n files are not getting split by default ,"running with stock ec2 scripts against hadoop-19 - i tried to run a job against a directory with 4 text files - each about 2G in size. These were not split (only 4 mappers were run).

The reason seems to have two parts - primarily that S3N files report a block size of 5G. This causes FileInputFormat.getSplits to fall back on goal size (which is totalsize/conf.get(""mapred.map.tasks"")).Goal Size in this case was 4G - hence the files were not split. This is not an issue with other file systems since the block size reported is much smaller and the splits get based on block size (not goal size).

can we make the S3N files report a more reasonable block size?"
HADOOP-5859,"FindBugs : fix ""wait() or sleep() with locks held"" warnings in hdfs","This JIRA fixes the following warnings:

SWL	org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal() calls Thread.sleep() with a lock held
TLW	wait() with two locks held in org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal()
TLW	wait() with two locks held in org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal()
TLW	wait() with two locks held in org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.writeChunk(byte[], int, int, byte[])"
HADOOP-5858,Eliminate UTF8 and fix warnings in test/hdfs-with-mr package,"Replace UTF8 with Text and fix java warnings not related to deprecated mapred api.
Warnings related to the deprecated map reduce api should probably be targeted in a separate unified approach."
HADOOP-5857,Refactor hdfs jsp codes,There are some normal java methods defiend in .jsp files.  It would be easier if these methods are moved to normal .java files.
HADOOP-5856,"FindBugs : fix ""unsafe multithreaded use of DateFormat"" warning in hdfs","This JIRA fixes the following warning:

STCAL	Call to method of static java.text.DateFormat in org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogEntry.newEnry(Block, long)
Bug type STCAL_INVOKE_ON_STATIC_DATE_FORMAT_INSTANCE (click for details) 
In class org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogEntry
In method org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogEntry.newEnry(Block, long)
Called method java.text.DateFormat.format(Date)
Field org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.dateFormat
At DataBlockScanner.java:[line 385]"
HADOOP-5855,Fix javac warnings for DisallowedDatanodeException and UnsupportedActionException,
HADOOP-5854,"findbugs : fix ""Inconsistent Synchronization"" warnings in hdfs ","This jira fixes the following findbugs warnings :

* 	Inconsistent synchronization of org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closed; locked 75% of time
* 	Inconsistent synchronization of org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.persistBlocks; locked 66% of time
* 	Inconsistent synchronization of org.apache.hadoop.hdfs.server.common.UpgradeManager.currentUpgrades; locked 61% of time
* 	Inconsistent synchronization of org.apache.hadoop.hdfs.server.common.UpgradeManager.upgradeState; locked 75% of time
*	Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.FSDirectory.ready; locked 50% of time
* 	Inconsistent synchronization of org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verificationLog; locked 45% of time
*      Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.FSNamesystem.safeMode; locked 48% of time
*	Inconsistent synchronization of org.apache.hadoop.hdfs.server.namenode.FSNamesystem$SafeModeInfo.extension; locked 80% of time
*      Inconsistent synchronization of org.apache.hadoop.io.SequenceFile$Reader.sync; locked 85% of time"
HADOOP-5853,Undeprecate HttpServer.addInternalServlet method to fix javac warnings,Deprecated method {{HttpServer.addInternalServlet()}} causes many java warnings. Next comment covers the reason for undeprecating the method.
HADOOP-5847,Streaming unit tests failing for a while on trunk,"http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/337/testReport/ shows some failed tests:

org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles
org.apache.hadoop.streaming.TestStreamingBadRecords.testSkip
org.apache.hadoop.streaming.TestStreamingBadRecords.testNarrowDown
org.apache.hadoop.streaming.TestSymLink.testSymLink"
HADOOP-5845,Build successful despite test failure on test-core target,"{{ant -Dtestcase=TestFoo test-core}} will succeed, even if TestFoo fails. Running the same test with the run-test-mapred target failed the build, as expected."
HADOOP-5844,Use mysqldump when connecting to local mysql instance in Sqoop,"Sqoop uses MapReduce + DBInputFormat to read the contents of a table into HDFS. On many databases, this implementation is O(N^2) in the number of rows. Also, the use of multiple mappers has low value in terms of throughput, because the database itself is inherently singlethreaded. While DBInputFormat/JDBC provides a useful fallback mechanism for importing from databases, db-specific dump utilities will nearly always provide faster throughput, and should be selected when available. This patch allows users to use mysqldump to read from local mysql instances instead of the MapReduce-based input.

If you provide sqoop with arguments of the form "" --connect jdbc:mysql://localhost/somedatabase --local"", it will use the mysqldump fast path to perform the import.

This patch, naturally, requires that MySQL be installed on a machine to test it. Thus the test that this adds is called LocalMySQLTest (instead of the Hadoop-preferred file naming, TestLocalMySQL) so that Hudson doesn't automatically run it. You can run this test yourself by using ""ant -Dtestcase=LocalMySQLTest test"". See the notes in the javadoc for the LocalMySQLTest class in how to set up the MySQL test environment for this."
HADOOP-5842,Fix a few javac warnings under packages fs and util,"Fix javac warnings in the following files:
fs/FileSystem.java
fs/ChecksumException.java 
fs/FSError.java
fs/kfs/KFSInputStream.java
fs/s3/S3FileSystemException.java
fs/s3/S3Exception.java
fs/s3/VersionMismatchException.java
util/ProcfsBasedProcessTree.java"
HADOOP-5841,"Resolve findbugs warnings in DistributedFileSystem.java, DatanodeInfo.java, BlocksMap.java, DataNodeDescriptor.java",Omnibus JIRA to address the findbugs issues raised in these files.
HADOOP-5839,fixes to ec2 scripts to allow remote job submission,"i would very much like the option of submitting jobs from a workstation outside ec2 to a hadoop cluster in ec2. This has been explored here:

http://www.nabble.com/public-IP-for-datanode-on-EC2-tt19336240.html

the net result of this is that we can make this work (along with using a socks proxy) with a couple of changes in the ec2 scripts:
a) use public 'hostname' for fs.default.name setting (instead of the private hostname being used currently)
b) mark hadoop.rpc.socket.factory.class.default as final variable in the generated hadoop-site.xml (that applies to server side)

#a has no downside as far as i can tell since public hostnames resolve to internal/private IP addresses within ec2 (so traffic is optimally routed)."
HADOOP-5838,Remove a few javac warnings under hdfs,"Fix a few javac warnings under hdfs.

I will list the files affected in jira i the following comment."
HADOOP-5836,"Bug in S3N handling of directory markers using an object with a trailing ""/"" causes jobs to fail","Some tools which upload to S3 and use a object terminated with a ""/"" as a directory marker, for instance ""s3n://mybucket/mydir/"". If asked to iterate that ""directory"" via listStatus(), then the current code will return an empty file """", which the InputFormatter happily assigns to a split, and which later causes a task to fail, and probably the job to fail. "
HADOOP-5835,Fix findbugs warnings,Cleaning up findbugs warnings for some of the files
HADOOP-5829,Fix javac warnings,Fixing javac warnings found in some of the java files
HADOOP-5827,Remove unwanted file that got checked in by accident,src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java.orig shouldn't have been checked in.
HADOOP-5824,remove OP_READ_METADATA functionality from Datanode,Operation OP_READ_METADATA exists on the datanode streaming interface. But it is not used by any client code and is currently not protected by access token. Should it be removed?  
HADOOP-5823,"Handling javac ""deprecated"" warning for using UTF8","
o.a.h.io.UTF8 is deprecated but is still used in multiple places. FSEditLog.java has 40 UTF8 related warnings. I don't think it is feasible to avoid using UTF8 in FSEditLog.java. 

Two options to get rid of these warnings :
  1. use @SupressWarnings at each use of UTF or for enclosing class.
  2. define a wrapper class {{DeprecatedUTF8}} that is not {{@deprecated}}. 

I prefer the second option in this case since it keeps FSEditLog.java and other places clean and still makes it explicit that a deprecated class is used.

This is part of spring cleaning effort to remove warnings in javac. I will attach a patch for the second option."
HADOOP-5822,Fix javac warnings in several dfs tests related to unncessary casts,"There are quite a few unnecessary casts as reported in javac in the following files:
/src/test/OAH/hdfs/TestDataTransferProtocol.java
/src/test/OAH/hdfs/TestFSInputChecker.java
/src/test/OAH/hdfs/TestFileAppend.java
/src/test/OAH/hdfs/TestPread.java
/src/test/OAH/hdfs/server/namenode/TestNodeCount.java
"
HADOOP-5820,Fix findbugs warnings for http related codes in hdfs,"There are a few findbugs warnings:
- HRS  HTTP parameter directly written to HTTP header output in org.apache.hadoop.hdfs.server.namenode.StreamFile.doGet(HttpServletRequest, HttpServletResponse)
- XSS  HTTP parameter directly written to JSP output, giving reflected XSS vulnerability in org.apache.hadoop.hdfs.server.datanode.browseBlock_jsp
- XSS  HTTP parameter directly written to JSP output, giving reflected XSS vulnerability in org.apache.hadoop.hdfs.server.datanode.browseBlock_jsp
- XSS  HTTP parameter directly written to JSP output, giving reflected XSS vulnerability in org.apache.hadoop.hdfs.server.datanode.browseDirectory_jsp
- XSS  HTTP parameter directly written to JSP output, giving reflected XSS vulnerability in org.apache.hadoop.hdfs.server.datanode.tail_jsp
- XSS  HTTP parameter directly written to JSP output, giving reflected XSS vulnerability in org.apache.hadoop.hdfs.server.datanode.tail_jsp"
HADOOP-5818,Revert the renaming from checkSuperuserPrivilege to checkAccess by HADOOP-5643,"HADOOP-5643 renamed FSNamesystem.checkSuperuserPrivilege to checkAccess.  However, ""checkAccess"" is confusing in FSNamesystem since there are other methods, checkPathAccess(..), checkParentAccess(..) and checkAncestorAccess(..) which have different meanings."
HADOOP-5816,ArrayIndexOutOfBoundsException when using KeyFieldBasedComparator,"{code:java}  
   if (!key.numeric) {
      compareResult = compareBytes(first, start1, end1, second, start2, end2);
    }
{code}
those lines above, compare two byte arrays in a wrong way, it will cause an ArrayIndexOutOfBoundsException,  that should be 
{code:java}  
   if (!key.numeric) {
      compareResult = compareBytes(first, start1, end1-start1, second, start2, end2-start2);
    }
{code}"
HADOOP-5815,Sqoop: A database import tool for Hadoop,"Overview:

Sqoop is a tool designed to help users import existing relational databases into their Hadoop clusters. Sqoop uses JDBC to connect to a database, examine the schema for tables, and auto-generate the necessary classes to import data into HDFS. It then instantiates a MapReduce job to read the table from the database via the DBInputFormat (JDBC-based InputFormat). The table is read into a set of files loaded into HDFS. Both SequenceFile and text-based targets are supported.

Longer term, Sqoop will support automatic connectivity to Hive, with the ability to load data files directly into the Hive warehouse directory, and also to inject the appropriate table definition into the metastore.

Some more specifics:

Sqoop is a program implemented as a contrib module. Its frontend is invoked through ""bin/hadoop jar sqoop.jar ..."" and allows you to connect to arbitrary JDBC databases and extract their tables into files in HDFS. The underlying implementation utilizes the JDBC interface of HADOOP-2536 (DBInputFormat). The DBWritable implementation needed to extract a table is generated by this tool, based on the types of the columns seen in the table. Sqoop uses JDBC to examine the table specification and translate this to the appropriate Java types.

The generated classes are provided as .java files for the user to reuse. They are also compiled into a jar and used to run a MapReduce task to perform the data import. This either results in text files or SequenceFiles in HDFS. In the latter case, these Java classes are embedded into the SequenceFiles as well.

The program will extract a specific table from a database, or optionally, all tables. For a table, it can read all columns, or just a subset. Since HADOOP-2536 requires that a sorting key be specified for the import task, Sqoop will auto-detect the presence of a primary key on a table and automatically use it as the sort order; the user can also manually specify a sorting column.

Example invocations:

To import an entire database:

hadoop jar sqoop.jar org.apache.hadoop.sqoop.Sqoop --connect jdbc:mysql://db.example.com/company --all-tables

(Requires that all tables have primary keys)

To select a single table:

hadoop jar sqoop.jar org.apache.hadoop.sqoop.Sqoop --connect jdbc:mysql://db.example.com/company --table employees

To select a subset of columns from a table:

hadoop jar sqoop.jar org.apache.hadoop.sqoop.Sqoop --connect jdbc:mysql://db.example.com/company --table employees --columns ""employee_id,first_name,last_name,salary,start_date""

To explicitly set the sort column, import format, and import destination (the table will go to /shared/imported_databases/employees):

hadoop jar sqoop.jar org.apache.hadoop.sqoop.Sqoop --connect jdbc:mysql://db.example.com/company --table employees --order-by employee_id --warehouse-dir /shared/imported_databases --as-sequencefile

Sqoop will automatically select the correct JDBC driver class name for HSQLdb and MySQL; this can also be explicitly set, e.g.:

hadoop jar sqoop.jar org.apache.hadoop.sqoop.Sqoop --connect jdbc:postgresql://db.example.com/company --driver org.postgresql.Driver --all-tables


Testing has been conducted with HSQLDB and MySQL. A set of unit tests covers a great deal of Sqoop's functionality, and this tool has been used in practice at Cloudera and with a few other early test users on ""real"" databases.

A readme file is included in the patch which contains documentation on how to use the tool.

"
HADOOP-5809,Job submission fails if hadoop.tmp.dir exists,"Currently in trunk, the job submission fails, stating ""Mkdirs failed to create ${hadoop.tmp.dir}"" "
HADOOP-5808,Fix hdfs un-used import warnings,There are quite few un-used import compiler warnings in the hdfs package.
HADOOP-5805,problem using top level s3 buckets as input/output directories,"When I specify top level s3 buckets as input or output directories, I get the following exception.

hadoop jar subject-map-reduce.jar s3n://infocloud-input s3n://infocloud-output

java.lang.IllegalArgumentException: Path must be absolute: s3n://infocloud-output
        at org.apache.hadoop.fs.s3native.NativeS3FileSystem.pathToKey(NativeS3FileSystem.java:246)
        at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:319)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:667)
        at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:109)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:738)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)
        at com.evri.infocloud.prototype.subjectmapreduce.SubjectMRDriver.run(SubjectMRDriver.java:63)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at com.evri.infocloud.prototype.subjectmapreduce.SubjectMRDriver.main(SubjectMRDriver.java:25)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)

The workaround is to specify input/output buckets with sub-directories:

 
hadoop jar subject-map-reduce.jar s3n://infocloud-input/input-subdir  s3n://infocloud-output/output-subdir

"
HADOOP-5804,neither s3.block.size not fs.s3.block.size are honoured,"S3FileSystem does not override FileSystem.getDefaultBlockSize(), so the s3 default block size is actualy controlled by fs.local.block.size

As far as I can see, the s3 block size specific parameters (either with or without the fs. prefix) are read nowhere."
HADOOP-5801,JobTracker should refresh the hosts list upon recovery,"If the hosts file is changes across restart then it should be refreshed upon recovery so that the excluded hosts are lost and the maps are re-executed.
The mapred-hosts list can change across restarts. Once the jobtracker recovers, it detects all the tasktracker there are there in the history. If the hosts list is changed, then the jobtracker will still have the tasktracker (data) internally but will disallow the tracker when it contacts. As a result, the jobtracker will have to wait for the tracker to timeout in order to re-execute the tasks. This patch simply refreshes the node list upon recovery so that the invalid trackers are lost immediately. "
HADOOP-5792,to resolve jsp-2.1 jars through IVY,
HADOOP-5790,Allow shuffle read and connection timeouts to be configurable,It would be good for latency-sensitive applications to tune the shuffle read/connection timeouts...
HADOOP-5784,The length of the heartbeat cycle should be configurable.,"Currently, the hearbeat cycle is set to (# nodes / 100) in seconds. This can be too long for clusters that need to run low latency jobs. We should make the number of heartbeats that should arrive a second configurable."
HADOOP-5782,Make formatting of BlockManager.java similar to FSNamesystem.java to simplify porting patch,"When block management code was refactored from FSNamesystem.java to BlockManager.java, some formatting changes are made. These changes make porting patches to BlockManager.java difficult."
HADOOP-5780,"Fix slightly confusing log from ""-metaSave"" on NameNode","After HADOOP-4103, ""-metaSave"" on NameNode prints ""MISSING"" in the line for a block that does not have any good replicas left. 

e.g. : {noformat}
Metasave: Blocks waiting for replication: 1
blk_6735500019364591152_0 (replicas: l: 0 d: 0 c: 2 e: 0 MISSING) 74.6.132.200:50010 :  192.168.0.107:50010 :
{noformat}

""0 MISSING"" in the log might imply to the user that there is no missing block, though it implies ""e(xcess replicas) : 0"".

We should fix that. Also as part of this I would like to indicate which datanodes are decommissioned or contain the corrupt replicas."
HADOOP-5775,HdfsProxy Unit Test should not depend on HDFSPROXY_CONF_DIR environment,"as war target read user-certs.xml and user-permissions.xml from $HDFSPROXY_CONF_DIR. If a user set this environment and have some files in it, it could potentially cause the unit test to fail if the conf files does not match what the unit test needs."
HADOOP-5771,Create unit test for LinuxTaskController,Add unit tests to test {{LinuxTaskController}} functionality introduced by HADOOP-4490
HADOOP-5764,Hadoop Vaidya test rule (ReadingHDFSFilesAsSideEffect) fails w/ exception if number of map input bytes for a job is zero.,Tool does not handle the case where number of map input bytes can be zero. Currently hadoop does not log the counter if its value is zero. So tool also needs to handle such situation and set zero value for the test rules.
HADOOP-5762,distcp does not copy empty directories,"If I have an empty directory /testdir1 and then I run the command bin/hadoop distcp /testdir1 /testdir2, the command completes successfully, but does not create the empty directory /testdir2.

"
HADOOP-5752,Provide examples of using offline image viewer (oiv) to analyze hadoop file systems,"The offline image viewer provides the ability to generate large amounts of data about an hdfs namespace. It would be good to provide tools, examples, etc. on how to analyze this data to find useful information."
HADOOP-5745,Allow setting the default value of maxRunningJobs for all pools,"The <pool> element allows setting the maxRunningJobs for that pool. It wold be nice to be able to set a default value for all pools.

In out configuration, pools are autocreated.. every new uesre gets his own pool. We would like to allow each user to be able to run a max of 5 jobs at a time. For the etl pool, this limit will be set to a greater value,"
HADOOP-5738,Split waiting tasks field in JobTracker metrics to individual tasks,"Currently, job tracker metrics reports waiting tasks as a single field in metrics. It would be better if we can split waiting tasks into maps and reduces. "
HADOOP-5737,UGI checks in testcases are broken,"While running {{TestMiniMRWithDFSWithDistinctUsers}}, I used this patch to test the ugi checks 
{code}
Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java
===================================================================
--- src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java	(revision 768189)
+++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java	(working copy)
@@ -40,6 +40,7 @@
     if (LOG.isDebugEnabled()) {
       LOG.debug(""ugi="" + ugi);
     }
+    LOG.info(""ugi="" + ugi);
 
     if (ugi != null) {
       user = ugi.getUserName();
{code}
While initializing a job, the ugi information should point to jobtracker as jobtracker does a dfs read. But today we will see that the log shows _pi_ as the caller instead of the jobtracker."
HADOOP-5734,HDFS architecture documentation describes outdated placement policy,"The ""Replica Placement: The First Baby Steps"" section of HDFS architecture document states:

""...
For the common case, when the replication factor is three, HDFS's placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. This policy cuts the inter-rack write traffic which generally improves write performance.
...""

However, according to the ReplicationTargetChooser.chooseTarger()'s code the actual logic is to put the second replica on a different rack as well as the third replica. So you have two replicas located on a different nodes of remote rack and one (initial replica) on the local rack's node. Thus, the sentence should say something like this:

""For the common case, when the replication factor is three, HDFS's placement policy is to put one replica on one node in the local rack, another on a node in a different (remote) rack, and the last on a different node in the same remote rack. This policy cuts the inter-rack write traffic which generally improves write performance.""
"
HADOOP-5733,Add map/reduce slot capacity and lost map/reduce slot capacity to JobTracker metrics,It would be nice to have the actual map/reduce slot capacity and the lost map/reduce slot capacity (# of blacklisted nodes * map-slot-per-node or reduce-slot-per-node). This information can be used to calculate a JT view of slot utilization.
HADOOP-5728,FSEditLog.printStatistics may cause IndexOutOfBoundsException,"We met IndexOutOfBoundsException exception when did logSync or rollEditlog,
the direct reason is the printStatistics of FSEditLog used the first element of
 StringArray editStreams, but does not check whether editStreams is empty when 
it is not null.  As below:

    if (editStreams == null ) {
      return;
    }
    .....[snipped]....
    buf.append("" Number of syncs: "" + editStreams.get(0).getNumSync());

the simple solution is to check whether it is empty.


"
HADOOP-5727,"Faster, simpler id.hashCode() which does not allocate memory","Integer.valueOf allocates memory if the integer is not in the object-cache, which is the vast majority of cases for the task id. It is possible to compute the hash code of an integer without going via the integer cache, and hence avoiding allocating memory."
HADOOP-5723,Changelog is inconsistent with commits,Recent branching has confused some of the entries in the changelog. A quick audit of recent releases would be useful
HADOOP-5721,Provide EditLogFileInputStream and EditLogFileOutputStream as independent classes ,EditLogFileInputStream and EditLogFileOutputStream are currently part of FSEditLog. With this patch we want to extract them and provide as independent classes. 
HADOOP-5717,Create public enum class for the Framework counters in org.apache.hadoop.mapreduce,"We need to make the counters from the framework visible, but they should be in the new package (org.apache.hadoop.mapreduce).

For compatibility, we need the old class of counters to recognize the old class and package names and convert them over to the new names. "
HADOOP-5715,Should conf/mapred-queue-acls.xml be added to the ignore list?,"conf/mapred-queue-acls.xml is auto-generated by ant but it becomes an unknown file to svn.
{noformat}
$ svn status
$ ant
...
BUILD SUCCESSFUL
Total time: 10 seconds
$ svn status
?      conf/mapred-queue-acls.xml
{noformat}"
HADOOP-5710,Counter MAP_INPUT_BYTES missing from new mapreduce api.,"MapTask, with current new mapreduce api, does not maintain MAP_INPUT_BYTES counter. Since RecordReader doesnot have getPos() api, it is not possible for the map task to maintain. Individual record readers (LineRecordReader, SequenceFileRecordReader) could be modified to maintain the same."
HADOOP-5709,Remove the additional synchronization in MapTask.MapOutputBuffer.Buffer.write,"As part of HADOOP-5661, a synchronization was introduced to address a false positive warning. While this does not have any impact on functionality and possibly on performance either, it would be better to remove this synchronization and just suppress the warning from a code maintenance point of view. "
HADOOP-5708,Configuration should provide a way to write only properties that have been set,"The Configuration.write and .writeXml methods always output all properties, whether they came from a default source, a loaded resource file, or an ""overlay"" set call.  There should be a way to write only the properties that were set, leaving out the properties that came from a default source.

Why?  Suppose I build a configuration on a machine that is not associated with a grid, write it out to XML, then try to load it on a grid gateway.  The configuration would contain all of the defaults picked up from my non-grid machine, and would completely overwrite all the defaults on that grid.

I propose to add methods to write out only the overlay values in Object and XML formats.

I see two options for implementing this:
1) Either completely new methods could be crafted (writeOverlay(DataOutput) and writeOverlayXml(OutputStream), or 
2) The existing write() and writeXml() methods could be adjusted to take an additional parameter indicating whether the full properties or overlay properties should be written.  (Of course, the existing write() and writeXml() methods would remain, defaulting to the current behavior.)

Option 1 has less impact to existing code.  Option 2 is a cleaner implementation with less code-duplication involved.  I would much prefer to do option 2.

Oh, and in case it's not clear, I'm offering to make this change and submit it.

Thoughts?

.  Topher"
HADOOP-5705,Improved tries in TotalOrderPartitioner to eliminate large leaf nodes.,"With the old technology, if a particular node in the trie has many children that contain no split points, TotalOrderPartitioner creates a separate empty leaf node for each one.  This takes a lot of space, which in turn limits the depth to which we can grow these trees to avoid making too many sparse nodes, so there is a parameter that defaults to 2 to control the depth.  With this patch, I can guarantee that each split point will create only three trie nodes in the trie: an empty one, a leaf that contains only one split point, and the internal nodes as needed, for a total space of perhaps 50-200 bytes per split point, even if the entire trie is elaborated.  There are pathological cases that can cause a recursion overflow during creation, so i have set the default tree max depth to 200, but I expect almost all tries to be fully elaborated, which means in turn that each byte of the sought key gets touched at most twice."
HADOOP-5704,Scheduler test code does not compile,HADOOP-5661 removed a deprecated constructor in ClusterStatus without updating callers in {{TestFairScheduler}} or {{TestCapacityScheduler}}
HADOOP-5702,hadoop-config.sh blows away CLASSPATH env,"The bin/hadoop-config.sh script blows away the user's CLASSPATH environment variable when setting up the hadoop-specific classpath. It's possible that this is intentional, but it would make developing contrib service plugins (eg HADOOP-4707) easier if hadoop-config appended rather than replaced CLASSPATH.

Attaching trivial patch to change this behavior."
HADOOP-5687,Hadoop NameNode throws NPE if fs.default.name is the default value,"Throwing NPE is confusing; instead, an exception with a useful string description could be thrown instead."
HADOOP-5679,Resolve findbugs warnings in core/streaming/pipes/examples,"Towards a solution for HADOOP-5628, we need to resolve all findbugs warnings. This jira will try to resolve the findbugs warnings where ever possible and suppress them where resolution is not possible."
HADOOP-5675,DistCp should not launch a job if it is not necessary,"Currently, DistCp launch a job even if it is not necessary.  In such case, the job does nothing."
HADOOP-5671,DistCp.sameFile(..) should return true if src fs does not support checksum,"In DistCp.sameFile(..), if src FileSystem does not support file checksum, it should return true."
HADOOP-5664,Use of ReentrantLock.lock() in MapOutputBuffer takes up too much cpu time,"In examining a profile of one of my mappers today, I noticed that the method ReentrantLock.lock() in MapTask$MapOutputBuffer seems to be taking up ~11 seconds out of around 100 seconds total. It seems like 10% is an awfully large amount of time to spend in this lock. "
HADOOP-5662,3 of the scheduler test's failing ,"Trunk builds are failing with this test case failrure's
http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/801/

http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/801/testReport/
org.apache.hadoop.mapred.TestQueueCapacities.testSingleQueue
org.apache.hadoop.mapred.TestQueueCapacities.testSingleQueueMultipleJobs
org.apache.hadoop.mapred.TestQueueCapacities.testMultipleQueues


"
HADOOP-5661,Resolve findbugs warnings in mapred,"Towards a solution for HADOOP-5628, we need to resolve all findbugs warnings. This jira will try to resolve the findbugs warnings where ever possible and suppress them where resolution is not possible."
HADOOP-5658,Eclipse templates fail out of the box; need updating,"The Hadoop templates, when run ""out of the box"" have Eclipse build ""problems"".  I've produced a small patch that fixes it."
HADOOP-5657,Validate data passed through TestReduceFetch,"While TestReduceFetch verifies the reduce semantics for reducing from in-memory segments, it does not validate the data it reads. Data corrupted during the merge will not be detected."
HADOOP-5656,Counter for S3N Read Bytes does not work,Counter for S3N Read Bytes does not work on trunk. On 0.18 branch neither read nor write byte counters work. 
HADOOP-5653,seems that some component is still looking for hadoop-site.xml,"I created hdfs_site.xml. I can see hdfs files from browser. But I cannot list hdfs files from command line. I got local file list instead. If I create a hadoop-site.xml, it gives warning below but the command ""./bin/hadoop dfs -ls"" works.

09/04/10 17:09:29 WARN conf.Configuration: DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively

It seems that some component is still looking for hadoop-site.xml"
HADOOP-5652,Reduce does not respect in-memory segment memory limit when number of on disk segments == io.sort.factor,"If the number of on-disk segments is exactly {{io.sort.factor}}, then map output segments may be left in memory for the reduce contrary to the specification in {{mapred.job.reduce.input.buffer.percent}}."
HADOOP-5650,Namenode log that indicates why it is not leaving safemode may be confusing,"A namenode with a large number of datablocks is setup with dfs.safemode.threshold.pct set to 1.0. With a small number of unreported blocks, namenode prints the following as the reason for not leaving safe mode:
{{The ratio of reported blocks 1.0000 has not reached the threshold 1.0000}}

With a large number of blocks, precision used for printing the log may not indicate the difference between the actual ratio of safe blocks to total blocks and the configured threshold. Printing number of blocks instead of ratio will improve the clarity."
HADOOP-5647,TestJobHistory fails if /tmp/_logs is not writable to. Testcase should not depend on /tmp,"TestJobHistory sets /tmp as hadoop.job.history.user.location to check if the history file is created in that directory or not. If /tmp/_logs is already created by some other user, this test will fail because of not having write permission."
HADOOP-5645,After HADOOP-4920 we need a place to checkin releasenotes.html,"Since HADOOP-4920 we no longer checkin our built documentation.  This means there is no longer a place to put releasenotes.html since HowToRelease wiki dictated that releasenotes.html be checked in to docs directory as part of the release process.

I'll create a build.xml patch that assumes release notes are in src/docs/releasenotes.html and copies them to build/docs/releasenotes.html"
HADOOP-5644,Namnode is stuck in safe mode,Restarting datanodes while a client is writing to it can cause namenode to get stuck in safe mode.
HADOOP-5643,Ability to blacklist tasktracker,"Its not always possible to shutdown the tasktracker to stop scheduling tasks on the node. (eg you can't login to the node but the TT is up). 

This can be via 
  * mapred.exclude and should be refreshed with out restarting the tasktracker
  * hadoop job -fail-tracker <tracker id>
"
HADOOP-5639,"Remove cyclic calls between JobTracker, JobInProgress and TaskInProgress","Today the JobTracker, upon a heartbeat request, calls JobInProgress which calls TaskInProgress which internally calls JobTracker. It will be nice if we can get rid of these cyclic calls. Ideally JobInProgress should not know about JobTracker and TaskInProgress should not know about JobInProgress and JobTracker. "
HADOOP-5638,More improvement on block placement performance,"Block placement algorithm currently has an excluded node list, which contains all datanodes that have been visited. This list is implemented as an array list, whose cost of inserting is O(1) but the cost of query ""contains"" is O( n ), where n is the number of datanodes. This makes the cost of block placement to be O(n*n) when a cluster is full.

I propose to change the data structure of the excluded node list as a HashMap. So in average, the cost of insertion is O(1) and the cost of query is O(1). This makes the block placement algorithm to be O( n ) in average."
HADOOP-5637,Update junit eclipse classpath,"Since the junit library is changed, the corresponding eclipse classpath should be updated."
HADOOP-5635,distributed cache doesn't work with other distributed file systems,"Currently the DistributedCache does a check to see if the file to be included is an HDFS URI. If the URI isn't in HDFS, it returns the default filesystem. This prevents using other distributed file systems -- such as s3, s3n, or kfs  -- with distributed cache. When a user tries to use one of those filesystems, it reports an error that it can't find the path in HDFS."
HADOOP-5625,Add I/O duration time in client trace ,"Add I/O duration information into client trace log for analyzing performance.
 "
HADOOP-5620,discp can preserve modification times of files,It will be helpful if distcp can preserve the modification time and access time of files. This helps to archive/unarchive hdfs files.
HADOOP-5618,Convert Storage.storageDirs into a map.,{{Storage.storageDirs}} is currently declared as an {{ArrayList}}. Recent changes made {{storageDirs}} a searchable collection. In order to reflect this changes the {{storageDirs}} type should be changed to a searchable collection. This will simplify and optimize current code.
HADOOP-5614,SIGKILL should be sent only after a while after SIGTERM is sent in sigkillInCurrentThread() similar to what sigKillThread is doing,sigkillInCurrentThread() doesn't give time for graceful exit of taskJvm after SIGTERM is sent. Currently mapred.tasktracker.tasks.sleeptime-before-sigkill is used by SigKillThread only. It should be used by sigKillInCurrentThread() also.
HADOOP-5613,change S3Exception to checked exception,"Currently the S3 filesystems can throw unchecked exceptions (S3Exception) which are not declared in the interface of FileSystem. These aren't caught by the various callers and can cause unpredictable behavior. IOExceptions are caught by most users of FileSystem since it is declared in the interface and hence is handled better.

I propose we modify S3Exception to extend IOException."
HADOOP-5609,Reducers continue to run even if a job failed.,"When a job failed, in rare case, some reducer slots are not freed (continue to try collecting data even if the job is not running). These occupied reducer slots are not usable by following mapred jobs unless the task trackers are restarted.
These reducers continue to report the progress to job tracker, which results the following logs:
2009-03-31 23:59:59,992 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid attempt_200903241312_0074_r_000001_0
2009-03-31 23:59:59,992 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid attempt_200903241312_0074_r_000958_0
2009-03-31 23:59:59,997 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid attempt_200903241312_0074_r_000440_0
2009-03-31 23:59:59,998 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid attempt_200903241312_0074_r_000463_0
"
HADOOP-5607,TestCapacityScheduler fails with NPE,"Observed on Hudson:
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobInProgress.terminateJob(JobInProgress.java:2117)
	at org.apache.hadoop.mapred.JobInProgress.terminate(JobInProgress.java:2153)
	at org.apache.hadoop.mapred.JobInProgress.kill(JobInProgress.java:2221)
	at org.apache.hadoop.mapred.TestCapacityScheduler$FakeTaskTrackerManager.killJob(TestCapacityScheduler.java:359)
	at org.apache.hadoop.mapred.CapacityTaskScheduler.killJobIfInvalidRequirements(CapacityTaskScheduler.java:1431)
	at org.apache.hadoop.mapred.CapacityTaskScheduler.jobAdded(CapacityTaskScheduler.java:1463)
	at org.apache.hadoop.mapred.JobQueuesManager.jobAdded(JobQueuesManager.java:183)
	at org.apache.hadoop.mapred.TestCapacityScheduler$FakeTaskTrackerManager.submitJob(TestCapacityScheduler.java:387)
	at org.apache.hadoop.mapred.TestCapacityScheduler.submitJob(TestCapacityScheduler.java:625)
	at org.apache.hadoop.mapred.TestCapacityScheduler.testHighMemoryJobWithInvalidRequirements(TestCapacityScheduler.java:1992)
{noformat}

This was introduced by HADOOP-5565. FakeJobInProgress doesn't pass a JobTracker reference to the subtype cstr, so calling kill() derefs the null JT field."
HADOOP-5605,All the replicas incorrectly got marked as corrupt.,"NameNode does not handle {{reportBadBlocks()}} properly. As a result, when DataNode reports the corruption (only in the case of block transfer between two datanodes), further attempts to replicate the block end up marking all the replicas as corrupt!

From the implementation, it looks like NN incorrectly uses the block object used in RPC to queue to neededReplication queue instead of using internal block object. 

will include an actual example in the next comment.
"
HADOOP-5604,TestBinaryPartitioner javac warnings.,"This is introduced by HADOOP-5528.
All of them about using unparametrized types."
HADOOP-5603,Improve block placement performance,"ReplicationTargetChooser chooses targets by iteratively selecting random nodes first and then filtering good targets until the required number of targets are chosen. This code may require selecting random nodes multiple times, thus introducing multiple traversals of the given portion of the cluster map.  This code can be improved by traversing the portion of the cluster map only once."
HADOOP-5599,Unable to run jobs when all the nodes in rack are down,"Jobs such as randomwriter, sort, validator fail when all the datanodes in a rack are down.
"
HADOOP-5596,Make ObjectWritable support EnumSet,"This is a demand for Hadoop-5438. 
Also another small improvement is that i saw that in the beginning of readObject, it tries to get the class from PRIMITIVE_NAMES and then conf. Maybe it is better to add a direct load after them if the delaredClass is still null. Like this:
{code}
String className = UTF8.readString(in);
    Class<?> declaredClass = PRIMITIVE_NAMES.get(className);
    if (declaredClass == null) {
      try {
        declaredClass = conf.getClassByName(className);
      } catch (Exception e) {
      }
    }
    
    if(declaredClass == null) {
      try {
        declaredClass = Class.forName(className);
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(""readObject can't find class "" + className, e);
      }
    }
{code}"
HADOOP-5595,NameNode does not need to run a replicator to choose a random DataNode,"FSNamesystem#getRandomDatanode uses a replicator to choose a random DataNode. It's an overkill. Instead, Topology#chooseRandom is a much light-weight way to choose a random DataNode from a cluster map."
HADOOP-5592,Hadoop Streaming - GzipCodec,"Hadoop Streaming - How do I generate output files with gzip format

This:
mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCode

Should be this:
mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec

"
HADOOP-5591,mapred.jobtracker.retirejob.interval killing long running reduce task,"I have long running jobs that run 30-50 hours I run from time to time . I noticed the reduce jobs getting a WARN child error and failing every 24 hours while in the Shuffle stage.
I modify the setting per suggestion on the user-list of setting mapred.jobtracker.retirejob.interval and changed it from 24 hours to 72 and the problem went away on the next 30 hour job.

I seen a reduce task run for longer then the 24 hours but only if it does not stay in the Shuffle stage or the Sort stage for longer then 24 hours.
I have seen the same error from faild task that reamin in the Shuffle or Sort Stage for longer then 24 hours.

the error I get form the jobtracker gui is this
java.io.IOException: Task process exit with nonzero status of 255.
 at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:418)

the error I get on the tasktracker logs is this:
2009-03-25 18:37:54,372 WARN org.apache.hadoop.mapred.TaskRunner: 
attempt_200903212204_0005_r_000001_1 Child Error

Then clean up happens and a reduce task is launched again to try again.

I am not 100% sure what the setting mapred.jobtracker.retirejob.interval does but I would not thank any setting would kill a actively NOT idle Sorting or Shuffle task
also someone on the list ask about my maps if they where long running also they are not long running average 4 mins completion time a map.

Also mapred.jobtracker.retirejob.interval is not in the default config but the code looks for it there when setting it in the code.
"
HADOOP-5590,testHighMemoryJobWithInvalidRequirements in TestCapacityScheduler fails with NullPointerException,testHighMemoryJobWithInvalidRequirements fails consistently with NullPointerException when TestCapacityScheduler is run.
HADOOP-5589,TupleWritable: Lift implicit limit on the number of values that can be stored,"TupleWritable uses an instance field of the primitive type, long, which I presume is so that it can quickly determine if a position has been written to in its array of Writables (by using bit-shifting operations on the long field). The problem with this is that it implies that there is a maximum limit of 64 values you can store in a TupleWritable.

An example of a use-case where I think this would be a problem is if you had two MR jobs with over 64 reduces tasks and you wanted to join the outputs with CompositeInputFormat  - this will probably cause unexpected results in the current scheme.

At the very least, the 64-value limit should be documented in TupleWritable.

"
HADOOP-5588,hadoop commands seem extremely slow in 0.20 branch,"hadoop dfs get, rm, -mkdir- ,cp, mv, ls, etc   mydir/fileA mydir/fileB mydir/fileC ...

seem to be very slow in 0.20 branch. 
 "
HADOOP-5585,FileSystem statistic counters are too high when JVM reuse is enabled.,"When JVM reuse is enabled, the FileSystem.Statistics are not cleared between tasks. That means that the second task gets credit for its own reads and writes as well as the first. The third gets credit for all 3 tasks reads and writes."
HADOOP-5583,"method fromEscapedCompactString()  from the depricated class, org.apache.hadoop.mapred.Counters is not supported in org.apache.hadoop.mapreduce.Counters ",Method fromEscapedCompactString()  is not supported in org.apache.hadoop.mapreduce.Counters class. The deprecated class  org.apache.hadoop.mapred.Counters provides it. This method is required for contrib/vaidya tool for parsing the COUNTERS string in compact escaped format from the job history logs. 
HADOOP-5582,Hadoop Vaidya throws number format exception due to changes in the job history counters string format (escaped compact representation).,Hadoop Vaidya (contrib/vaidya) tool throws number format exception while parsing the job history log files due to change in the format of counters string in 0.20.
HADOOP-5581,libhdfs does not get FileNotFoundException,"When trying to open a file that does not exist for read, libhdfs prints an error out along the line of ""Cannot open file <filename>"".

I believe it should be throwing the FileNotFoundException instead.  This would allow us to correctly set the errno and more naturally give the errors to the C-based clients."
HADOOP-5580,QuotaExceededException does not get raised via libhdfs,"When using libhdfs, if a quota exception occurs, we don't get a QuotaExceptionExceededException - the DFSClient appears to receive it, but not give it to libhdfs.

I want to get this correctly raised so I can in turn give a better error message to our users."
HADOOP-5579,libhdfs does not set errno correctly,"Due to a change in the package name of the AccessControlException, libhdfs no longer sets errno=EACCES when a permission occurs."
HADOOP-5577,The Job.waitForCompletion doesn't print status as the job runs,"Unlike the old JobClient.runJob, the context object api doesn't print status as the job runs. This is very unfriendly to users."
HADOOP-5576,LocalJobRunner does not run jobs using new MapReduce API,"{noformat}
java.lang.ClassCastException: org.apache.hadoop.mapred.FileSplit cannot be cast to org.apache.hadoop.mapreduce.lib.input.FileSplit
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:55)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:412)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:510)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:303)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:140)
{noformat}"
HADOOP-5575,regarding output dir usage,"I want to do following:

1. A Sequence of map-reduce operations - Found no relevant link / template of how it is done.
2. An an alternative I thought of passing output/part-00000 file to next map-reduce as input file, but then I got the exception for wrong FS while accessing output dir. This bug is already fixed in 0.20, but this version I am not able to find on apache core - download release page.

Can anyone help me out with 0.20 distribution?
Also, if possible can anyone give me an idea of how I can do a sequence of map-reduce iterations?

One more point is - I want my map task to access a common file which ENTIRELY should be accessible to it (i.e. not a split one, but the whole file a map task should be able to READ) and the same file I want my reducer task to write into.
2 things:
1. how to set such file which won't be split but will be given to each map task entirely?
2. how can i make all my reducer operating in parallel to modify such file, which will be used for next iteration?

I am not sure whether the things I mentioned above are indeed possible. I am new to hadoop..
Please help,
Thanks,

-- 
Girija"
HADOOP-5572,The map progress value should have a separate phase for doing the final sort.,"Currently, the final spill and sort doesn't record any progress while it runs, leading to the perception that the map is done, but ""stuck"".

This patch reserves 33.3% of map task's progress to sort by dividing map task in to 2 phases. Also makes the progress of sort phase(merges) work, both in map side and reduce side."
HADOOP-5571,TupleWritable can return incorrect results if it contains more than 32 values,"When attempting to do an outer join on 45 files with the CompositeInputFormat, I've been encountering unexpected results in the TupleWritable returned by the record reader. On closer inspection, it seems to be because TupleWritable.setWritten(int) is incorrectly setting some tuple positions as written, i.e when you set setWritten(42), it also sets position 10.

The following Junit test demonstrates the problem:
{code}
  public void testWideTuple() throws Exception {
    Text emptyText = new Text(""Should be empty"");
    Writable[] values = new Writable[64];
    Arrays.fill(values,emptyText);
    values[42] = new Text(""Number 42"");
                                     
    TupleWritable tuple = new TupleWritable(values);
    tuple.setWritten(42);
    
    for (int pos=0; pos<tuple.size();pos++) {
      boolean has = tuple.has(pos);
      if (pos == 42) {
        assertTrue(has);
      }
      else {
        assertFalse(""Tuple position is incorrectly labelled as set: "" + pos, has);
      }
    }
}
{code}

Similarly, TupleWritable.setWritten(9) also causes TupleWritable.has(41) to incorrectly return true.
"
HADOOP-5565,"The job instrumentation API needs to have a method for finalizeJob,","The job instrumentation API needs to have a method for finalizeJob, and this function should be called in JobTracker
finalizeJob. 
Currently the jobComplete's method on the job instrumentation class is called only for Job that succeed. 
 "
HADOOP-5564,hadoop command uses large JVM heap size,"Command used to determine JAVA_PLATFORM in bin/hadoop command does not set the heap size. The command uses default 1GB heap size. The tasks invoking hadoop command end up  using large heap size in streaming jobs. If the maximum memory that can be used by a task is restricted, this could result in map/reduce job failures.
"
HADOOP-5561,Javadoc-dev ant target runs out of heap space,"The default configuration for the ant task javadoc-dev does not specify a maxmemory and, after churning for a while, fails with an OOM exception:
{noformat}
[javadoc] Constructing Javadoc information...
[javadoc] Standard Doclet version 1.6.0_07
[javadoc] Building tree for all the packages and classes...
[javadoc] java.lang.OutOfMemoryError: Java heap space
[javadoc] 	at java.util.LinkedHashMap.createEntry(LinkedHashMap.java:424)
[javadoc] 	at java.util.LinkedHashMap.addEntry(LinkedHashMap.java:406)
[javadoc] 	at java.util.HashMap.put(HashMap.java:385)
[javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookup(OpenListResourceBundle.java:118)
[javadoc] 	at sun.util.resources.OpenListResourceBundle.loadLookupTablesIfNecessary(OpenListResourceBundle.java:97)
[javadoc] 	at sun.util.resources.OpenListResourceBundle.handleGetObject(OpenListResourceBundle.java:58)
[javadoc] 	at sun.util.resources.TimeZoneNamesBundle.handleGetObject(TimeZoneNamesBundle.java:59)
[javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:378)
[javadoc] 	at java.util.ResourceBundle.getObject(ResourceBundle.java:381)
[javadoc] 	at java.util.ResourceBundle.getStringArray(ResourceBundle.java:361)
[javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:100)
[javadoc] 	at sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:81)
[javadoc] 	at java.util.TimeZone.getDisplayNames(TimeZone.java:399)
[javadoc] 	at java.util.TimeZone.getDisplayName(TimeZone.java:350)
[javadoc] 	at java.util.Date.toString(Date.java:1025)
[javadoc] 	at com.sun.tools.doclets.formats.html.markup.HtmlDocWriter.today(HtmlDocWriter.java:337)
[javadoc] 	at com.sun.tools.doclets.formats.html.HtmlDocletWriter.printHtmlHeader(HtmlDocletWriter.java:281)
[javadoc] 	at com.sun.tools.doclets.formats.html.ClassWriterImpl.writeHeader(ClassWriterImpl.java:122)
[javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassHeader(ClassBuilder.java:164)
[javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
[javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
[javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
[javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)
[javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[javadoc] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[javadoc] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[javadoc] 	at java.lang.reflect.Method.invoke(Method.java:597)
[javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)
[javadoc] 	at com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)
{noformat}"
HADOOP-5559,BackupStorage should not use FSNamesystem.getFSNamesystem(),"In BackupStorage.java,
{code}
  private FSNamesystem getFSNamesystem() {
    // HADOOP-5119 should get rid of this.
    return FSNamesystem.getFSNamesystem();
  }
{code}
HADOOP-5119 already involves a lot of codes.  It is better to fix this problem in a separated issue."
HADOOP-5558,BackupStorage should not use FSNamesystem.getFSNamesystem(),"In BackupStorage.java,
{code}
  private FSNamesystem getFSNamesystem() {
    // HADOOP-5119 should get rid of this.
    return FSNamesystem.getFSNamesystem();
  }
{code}
HADOOP-5119 already involves a lot of codes.  It is better to fix this problem in a separated issue."
HADOOP-5557,Two minor problems in TestOverReplicatedBlocks,"- There is no apache license header.
- It uses a deprecated API, FSNamesystem.getFSNamesystem()."
HADOOP-5556,A few improvements to DataNodeCluster,"DataNodeCluster is a great tool to simulate a large scale DFS cluster using a small set of machines. A few suggestions to improve this tool:
# DataNodeCluster uses MiniDFSCluster#startDataNode to start multiple instances of DataNode on one machine. MiniDFSCluster sets DataNode's address to be 127.0.0.1. We should allow to set its address to 0.0.0.0 so DataNodes in different machines could communicate.
# Currently the size of the blocks injected to DataNode and created in CreatedEditsLog is hardcoded as 10. It would be more convenient if this could be configurable. Also we need to make sure that both use the same block size.
# If the replication factor of blocks is larger than 1, currently a DataNode in DataNodeCluster will be injected blocks multiple times and therefore it sends block reports to NameNode multiple times. Initial block reports contain only a portion of its blocks and therefore may cause unnecessary block replications. It would be cleaner if only one block report with all its blocks is sent. "
HADOOP-5554,DataNodeCluster should create blocks with the same generation stamp as the blocks created in CreateEditsLog,"HADOOP-5384 makes DataNodeCluster to create blocks with generation stamp Block#GRANDFATHER_GENERATION_STAMP(0) so simuated datanodes do not crash NameNode any more. But there is still a problem. CreateEditLogs creates blocks with generation stamp GenerationStamp#FIRST_VALID_STAMP (1000). Because of the generation stamp mismatch, all injected blocks are marked as invalid when NameNode processes block reports."
HADOOP-5553,Change modifier of SequenceFile.CompressedBytes and SequenceFile.UncompressedBytes from private to public,"SequenceFile.rawValue() provides the only interface to navigate the underlying bytes. And with some little work on implementing a customized ValueBytes can avoid reading all bytes into memory. Unfortunately, the current nextRawValue will cast the passing ValueBytes to either private class CompressedBytes or private class UnCompressedBytes, this will disallow user further extension.
I can not see any reason that CompressedBytes and UnCompressedBytes should be set to private. And since the ValueBytes is public and nextValue() casts it to either CompressedBytes or UnCompressedBytes, i think it would be better if they are public.

I am stuck now by this issue, really appracited if this got resolved as soon as possible."
HADOOP-5551,Namenode permits directory destruction on overwrite,"The FSNamesystem's startFileInternal allows overwriting of directories.  That is, if you have a directory named /foo/bar and you try to write a file named /foo/bar, the file is written and the directory disappears.

This is most apparent for folks using libhdfs directly, as overwriting is always turned on.  Therefore, if libhdfs applications do not check the existence of a directory first, then they will permit new files to destroy directories."
HADOOP-5550,jobhistory.jsp should ignore non-job files in the history folder and continue with the rest,"As of today, if there are stray files in the job history folder then jobhistory.jsp fails with some exception, mostly ArrayOutOfBound exception. Ideally it should filter out job files and display that."
HADOOP-5549,ReplicationMonitor should schedule both replication and deletion work in one iteration,The fix to HADOOP-5034 should make ReplicationMonitor to schedule both replication and deletion work in one iteration. The change was in the first submitted patch but got lost in the committed patch.
HADOOP-5548,Observed negative running maps on the job tracker,"We saw in both the web/ui and cli tools:

{noformat}
Cluster Summary (Heap Size is 11.7 GB/13.37 GB)

Maps  Reduces Total       Nodes  Map Task  Reduce Task  Avg.     Blacklisted 
              Submissions        Capacity   Capacity   Tasks/Node Nodes
-971  0       133         434     1736        1736      8.00        0
{noformat}

"
HADOOP-5541,Place holder for job level diagnostic information,"In some scenarios, when framework itself fails/kills the job, there is no clear way for the user to know what happened to the job. A job level diagnostic string, perhaps displayed in JobStatus when queried from command line or view from the web ui would help. Two cases I can think of:
 - When a job is failed when it transgresses mapred.jobtracker.maxtasks.per.job.
 - When CapacityScheduler kills a job when it detects that the job's memory requirements can never be meant."
HADOOP-5540,"On a fresh check out of 0.20 release branch, not able to run TestMapReduceLocal unit testcase, using ant","On a fresh check out of 0.20 release branch, 
I did a run of TestMapReduceLocal with command ""ant test -Dtestcase=TestMapReduceLocal""
It failed. The logs under build/test/TEST-org.apache.hadoop.mapreduce.TestMapReduceLocal.txt says  that ~/build/test/data/out exists.

I removed the data directory itself and then tried again. it says the same error.

I tried by doing a ant clean first and then running this unit testcase. It gives the same error.
"
HADOOP-5538,Implement -setSpaceQuota and -clrSpaceQuota tests on directory using globbing  ,"TestCLI.java  for testing Command Line Interface is missing  tests for -setSpaceQuota on directory using globbing , and -clrSpaceQuota on directory using globbing . "
HADOOP-5534,Deadlock triggered by FairScheduler scheduler's servlet due to changes from HADOOP-5214.,
HADOOP-5531,Remove Chukwa on branch-0.20,"Chukwa is now a Hadoop sub project.  The Chukwa code in the 0.20 branch is more than 3 months old and hasn't received many critical bug fixes.  One problem we're running into is that one or more Chukwa unit tests frequently fail on 0.20 branch.  This makes builds and releases of the branch more difficult.

I propose we disable the Chukwa unit tests on 0.20 branch before we release Hadoop 0.20.0.

Thoughts?"
HADOOP-5528,Binary partitioner,It would be useful to have a {{BinaryPartitioner}} that partitions {{BinaryComparable}} keys by hashing a configurable part of the bytes array corresponding to each key.
HADOOP-5522,Document job setup/cleaup tasks and task cleanup tasks in mapred tutorial,"Document the fact that job setup/cleanup and task cleanup happens as seperate tasks and they occupy map/reduce slots, in OutputCommitter section"
HADOOP-5521,Remove dependency of testcases on RESTART_COUNT,"There are 2 usecases for this :
# RESTART_COUNT is not guaranteed to be flushed to fs.
# HADOOP-5394 plans to discontinue logging restart-count to job history file."
HADOOP-5520,Typo in diskQuota help  documentation ,"Minor typo in setSpaceQuota help documentation displayed on CLI. 
disk is misspelled .

-clrQuota <dirname>...<dirname>: Clear the quota for each directory <dirName>.
		Best effort for the directory. with fault reported if
		1. the directory does not exist or is a file, or
		2. user is not an administrator.
		It does not fault if the directory has no quota.
-setSpaceQuota <quota> <dirname>...<dirname>: Set the dik space quota <quota> for each directory <dirName>.
		The directory quota is a long integer that puts a hard limit
		on the number of names in the directory tree.
		Quota can also be speciefied with a binary prefix for terabytes,
		petabytes etc (e.g. 50t is 50TB, 5m is 5MB, 3p is 3PB).
		Best effort for the directory, with faults reported if
		1. N is not a positive integer, or
		2. user is not an administrator, or
		3. the directory does not exist or is a file, or
		4. the directory would immediately exceed the new space quota.

Assigning to self."
HADOOP-5519,Remove claims from mapred-default.xml that prime numbers of tasks are helpful.,"There is no value in using prime numbers for numbers of tasks. In fact, most jobs don't even need the number of maps set, since it will increase to the number of blocks."
HADOOP-5518,MRUnit unit test library,"MRUnit is a tool to help authors of MapReduce programs write unit tests.

Testing map() and reduce() methods requires some repeated work to mock the inputs and outputs of a Mapper or Reducer class, and ensure that the correct values are emitted to the OutputCollector based on inputs. Also, testing a mapper and reducer together requires running them with the sorted ordering guarantees made by the shuffle process.

This library provides the above functionality to authors of maps and reduces; it allows you to test maps, reduces, and map-reduce pairs without needing to perform all the setup and teardown work associated with running a job.

I believe this tool may be useful to the broader Hadoop community, so I have cleaned it up and would like to see it become a ""contrib"" module. My current environment is based on Hadoop 0.18, so this is the format it expects to use. It does not have support for the new Context-based interfaces for mappers/reducers.

I have attached the overview.html file for its javadoc, which provides more synopsis and an example of usage; I am also providing the current source code so that you can evaluate its structure.

Ideally with some feedback from the community this will move toward supporting the current trunk interface soon.

This currently works with JUnit 4; the supplied patch changes Ivy's libraries.properties file to use JUnit 4.5. I'm marking HADOOP-4901 as a dependency for this reason.
"
HADOOP-5516,TaskMemoryManagerThread crashes in a corner case,"TT's stdout says.
{code}
Exception in thread ""org.apache.hadoop.mapred.TaskMemoryManagerThread"" java.lang.NullPointerException
        at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:126)
        at org.apache.hadoop.mapred.TaskMemoryManagerThread.run(TaskMemoryManagerThread.java:200)
{code}

TaskMemoryManager crashes and no further memory management is done."
HADOOP-5514,Add waiting/failed tasks to JobTracker metrics,"In addition to launched and completed tasks, it would be helpful if the number of waiting tasks and failed tasks were accumulated as metrics."
HADOOP-5513,Balancer throws NullPointerException,"Balancer command seems to be throwing NullPointerException when run on 0.20 using fairshare scheduler

"
HADOOP-5511,Add Apache License to EditLogBackupOutputStream,Apache License is missing in EditLogBackupOutputStream.java
HADOOP-5509,PendingReplicationBlocks should not start monitor in constructor.,"{{PendingReplicationBlocks}} starts {{PendingReplicationMonitor}} inside the constructor.
Monitor should be started by a separate method, which should be called in {{NameNode.activate()}}"
HADOOP-5507,javadoc warning in JMXGet,"{noformat}
  [javadoc] .\src\hdfs\org\apache\hadoop\hdfs\tools\JMXGet.java:45: warning:
 sun.management.ConnectorAddressLink is Sun proprietary API and may be removed in a future release
  [javadoc] import sun.management.ConnectorAddressLink;
  [javadoc]                      ^
  [javadoc] Standard Doclet version 1.6.0_07
  [javadoc] Building tree for all the packages and classes...
  [javadoc] .\src\hdfs\org\apache\hadoop\hdfs\tools\JMXGet.java:139: warning
 - @param argument ""args"" is not a parameter name.
{noformat}"
HADOOP-5505,TestMissingBlocksAlert fails on 0.20.,"TestMissingBlocksAlert fetches NameNode front page to verify that a an expected warning exists on the page. 

The namenode here is part of a MiniDFSCluster and looks like JspHelper is not initialized properly when JVM has both a datanode and a NameNode.

Trunk is not affected."
HADOOP-5502,Backup and checkpoint nodes should be documented,HDFS user guide should include description of backup and checkpoint nodes.
HADOOP-5500,Allow number of fields to be supplied when field names are not known in DBOutputFormat#setOutput(),"DBOutputFormat expects the column names to be included in the query. However, if the field names are not available, only the number of fields is sufficient for preparing the SELECT query. 

This issue will add API support for supplying the number of fields in DBOutputFormat#setOutput(). 

Please see HADOOP-4955 and HADOOP-5307 for related context. "
HADOOP-5499,JobTracker metrics do not match job counters,"After one run of a 10 map, 3 reduce sort job, the mapred metrics report:
{noformat}
mapred.tasktracker: hostName=snip, sessionId=, mapTaskSlots=3, maps_running=0, \
  reduceTaskSlots=3, reduces_running=0, tasks_completed=15, tasks_failed_ping=0, tasks_failed_timeout=0
mapred.jobtracker: hostName=snip, sessionId=, jobs_completed=1, jobs_submitted=1, \
  maps_completed=10, maps_launched=12, reduces_completed=3, reduces_launched=3
{noformat}

After the second (w/ one manually killed reduce)
{noformat}
mapred.tasktracker: hostName=snip, sessionId=, mapTaskSlots=3, maps_running=0, \
   reduceTaskSlots=3, reduces_running=0, tasks_completed=32, tasks_failed_ping=0, tasks_failed_timeout=0
mapred.jobtracker: hostName=snip, sessionId=, jobs_completed=2, jobs_submitted=2, \
  maps_completed=20, maps_launched=24, reduces_completed=6, reduces_launched=8
{noformat}

The counters report the expected 10/3 map/reduce completions, the second job reporting a single failure.

# The {{maps_launched}} and {{reduces_launched}} counts are likely recording setup and cleanup tasks as well
# After being recorded among launched tasks, it looks like setup and cleanup are _not_ included among completed tasks
# {{tasks_completed}} on the TaskTracker should include only user tasks
# The {{reduces_launched}} count makes little sense to me. With three reduces launched per job and one failed, what's the other launched reduce counting?"
HADOOP-5494,IFile.Reader should have a nextRawKey/nextRawValue,"Merger.Segment has only the next() method defined which internally calls next(key,value) on the underlying IFile stream. This would read both the key and the value bytes. It would be good to have Merger.Segment.nextRawKey(), that would read only the key and delay reading the value until needed (in Merger.MergeQueue.next()) via a new method Merger.Segment.nextRawValue(). 
This would mean that we load only one value bytes at a time, and hence would incur potentially much less (depending on how big the values are) on the memory footprint."
HADOOP-5493,Shuffle copiers do not return Codecs back to the pool at the end of shuffling,"At the end of shuffle, the copiers should return back the codecs to the pool. This doesn't happen and can potentially lead to a lot of memory leak on the reduce task (depending on how many shuffle copiers there are)."
HADOOP-5491,Better control memory usage in contrib/index,"The combiner was originally designed to work only on the map side. When used on the reduce side, it may use too much memory."
HADOOP-5490,TestParallelInitialization failed on NoSuchElementException,"java.util.NoSuchElementException
	at java.util.AbstractList$Itr.next(AbstractList.java:350)
	at java.util.Collections.sort(Collections.java:162)
	at org.apache.hadoop.mapred.EagerTaskInitializationListener.resortInitQueue(EagerTaskInitializationListener.java:162)
	at org.apache.hadoop.mapred.EagerTaskInitializationListener.jobAdded(EagerTaskInitializationListener.java:137)
	at org.apache.hadoop.mapred.TestParallelInitialization$FakeTaskTrackerManager.submitJob(TestParallelInitialization.java:142)
	at org.apache.hadoop.mapred.TestParallelInitialization.testParallelInitJobs(TestParallelInitialization.java:185)
"
HADOOP-5489,hadoop-env.sh still refers to java1.5,"The example JAVA_HOME in conf/hadoop-env.sh still points to /usr/lib/j2sdk1.5-sun

better to have it set to point to wherever the sun java 6 RPM sticks Java "
HADOOP-5488,HADOOP-2721 doesn't clean up descendant processes of a jvm that exits cleanly after running a task successfully,
HADOOP-5486,"ReliabilityTest does not test lostTrackers, some times.","ReliabilityTest does not lose trackers, if tasktracker pid could not be obtained.  If command for the TaskTracker process is large, doing 'ps' and 'grep' does not return the pid of the process, thus it could not be suspended.
"
HADOOP-5485,Authorisation machanism required for acceesing jobtracker url :- jobtracker.com:port/scheduler,"FS scheduler should have some mechanism to authorize people who can access the advanced scheduler url http://jobtracker.com:port/scheduler .  In large clusters , which has hundreds of users, any user can access the url now and change the priority of his/her runing job. We don't want the users to change the job priority. So we should restrcit users accessing the link and only admins should have access to the link. "
HADOOP-5484,TestRecoveryManager fails wtih FileAlreadyExistsException,"TestRecoveryManager always fails when I run core tests in a linux redhat machine. It does not fail on a Mac machine.

Testcase: testRecoveryManager took 55.842 sec
        Caused an ERROR
Output directory file:XX/build/test/data/test-recovery-manager/output1 already exists
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:XX/build/test/data/test-recovery-manager/output1 already exists
        at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:111)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:772)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)
        at org.apache.hadoop.mapred.TestRecoveryManager.testRecoveryManager(TestRecoveryManager.java:196)
"
HADOOP-5483,Directory/file cleanup thread throws IllegalStateException,"TestMiniMRWithDFS prints the following error message on screen:
    [junit] Exception in thread ""Directory/File cleanup thread"" java.lang.IllegalStateException: Shutdown in progress
    [junit]     at java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:39)
    [junit]     at java.lang.Runtime.addShutdownHook(Runtime.java:192)
    [junit]     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1361)
    [junit]     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:185)
    [junit]     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
    [junit]     at org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread.run(CleanupQueue.java"
HADOOP-5479,NameNode should not send empty block replication request to DataNode,"On our production clusters, we occasionally see that NameNode sends an empty block replication request to DataNode on  every heartbeat, thus blocking this DataNode from replicating or deleting any block.

This is partly caused by DataNode sending a wrong number of replications in progress which will be fixed by HADOOP-5465. There is also a flaw at the NameNode side. NameNode should not interpret the number of replications in progress as the number of targets since replication is done through a pipeline. It also should make sure that no empty replication request is sent to DataNode."
HADOOP-5477,TestCLI fails,
HADOOP-5476,"calling new SequenceFile.Reader(...) leaves an InputStream open, if the given sequence file is broken",
HADOOP-5474,All reduce tasks should be re-executed when tasktracker with a completed map task failed,"When a tasktracker with a completed map task failed, the map task will be re-exectuted, and all reduce tasks that haven't read the data from that tasktracker should be re-executed. But the reduce task that have read the data from that tasktracker will not be re-executed. 

In this situation, if the outputs of multi map tasks on the same dataset are different, for example outputting a random number, the outputs of maptask and the re-executed maptask will probably are different. Then the re-executed reduce tasks will read the new output of the re-executed maptask, but reduce tasks that have read the data from the failed tasktracker have read the old output. This probably will cause correctness of the result.

A recommended solution is that all reduce tasks should be re-executed if one tasktracker with a completed map task failed.

Any comments? thanks!


"
HADOOP-5473,Race condition in command-line kill for a task,"The race condition occurs in following sequence of events:
1. User issues a command-line kill for a RUNNING map-task. JT stores the task in tasksToKill mapping.
2. TT reports the task status as SUCCEEDED.
3. JT creates a TaskCompletionEvent as SUCCEEDED. Also sends a killTaskAction.
4. Reducers fail fetching the map output.
5. finally, the task would fail with Fetch failures. After HADOOP-4759, the task is left as FAILED_UNCLEAN task, since the task is present in tasksToKill mapping."
HADOOP-5472,Distcp does not support globbing of input paths,The current version of distcp does not support globbing of input paths. 
HADOOP-5471,"SyncLogs thread in Child.java would update wrong file for a cleanup attempt, in some cases.","This happens in the following scenario:
If jvm is launched for a cleanup attempt and getTask has not returned yet. thus, isCleanup value is not obtained yet. But, the SyncLogs thread would do a syncLogs with wrong isCleanup value (i.e. with wrong index file). "
HADOOP-5469,Exposing Hadoop metrics via HTTP,"Implement a ""/metrics"" URL on the HTTP server of Hadoop daemons, to expose metrics data to users via their web browsers, in plain-text and JSON."
HADOOP-5468,Change Hadoop doc menu to sub-menus,"In directory Trunk\src\docs\src\documentation\content\xdocs ...

1. site.xml - changed doc menu to submenus

2. *.xml - for a few docs, changed titles to match submenus and made some minor edits "
HADOOP-5467,Create an offline fsimage image viewer,"It would be useful to have a tool to examine/dump the contents of the fsimage file to human-readable form.  This would allow analysis of the namespace (file usage, block sizes, etc) without impacting the operation of the namenode.  XML would be reasonable output format, as it can be easily viewed, compressed and manipulated via either XSLT or XQuery.  

I've started work on this and will have an initial version soon."
HADOOP-5466,CSS Style Changes for Hadoop Doc Headers and Code,skinconf.xml file in /trunk/src/docs/src/documentation folder updated to include new CSS definitions for headers and code. Changes will apply to all hadoop docs. Improves readability.
HADOOP-5465,Blocks remain under-replicated,"Occasionally we see some blocks remain to be under-replicated in our production clusters. This is what we obeserved:
1. Sometimes when increasing the replication factor of a file, some blocks belonged to this file do not get to increase to the new replication factor.
2. When taking meta save in two different days, some blocks remain in under-replication queue. "
HADOOP-5464,DFSClient does not treat write timeout of 0 properly,"{{dfs.datanode.socket.write.timeout}} is used for sockets to and from datanodes. It is 8 minutes by default. Some users set this to 0, effectively disabling the write timeout (for some specific reasons). 

When this is set to 0, DFSClient sets the timeout to 5 seconds by mistake while writing to DataNodes. This is exactly the opposite of real intention of setting it to 0 since 5 seconds is too short.


"
HADOOP-5463,"Balancer throws ""Not a host:port pair"" unless port is specified in fs.default.name","If fs.default.name is specified as only a hostname (with no port), balancer throws ""Not a host:port pair"" and will not run.

Workaround is to add default port 8020 to fs.default.name.

According to Doug: ""That's the work-around, but it's a bug.  One should not need to specify the default port number (8020).""

According to Raghu Angadi: ""Balancer should use NameNode.getAddress(conf) to get NameNode address.""

See http://www.nabble.com/Not-a-host%3Aport-pair-when-running-balancer-td22459259.html for discussion.
"
HADOOP-5462,Glibc double free exception thrown when chown syscall fails.,"When setuid script's chown call fails, a glibc double free exception is thrown. The reason for this is that file_handle which was opened to write the pid file is already closed and the cleanup: label tries to close it once again."
HADOOP-5459,CRC errors not detected reading intermediate output into memory with problematic length,"It's possible that the expected, uncompressed length of the segment is less than the available/decompressed data. This can happen in some worst-cases for compression, but it is exceedingly rare. It is also possible (though also fantastically unlikely) for the data to deflate to a size greater than that reported by the map. CRC errors will remain undetected because IFileInputStream does not validate the checksum until the end of the stream, and close() does not advance the stream to the end of the segment. The (abbreviated) read loop fetching data in shuffleInMemory:

{code}
int n = input.read(shuffleData, 0, shuffleData.length);
while (n > 0) { 
  bytesRead += n;
  n = input.read(shuffleData, bytesRead, 
                 (shuffleData.length-bytesRead));
} 
{code}

Will read only up to the expected length. Without reading the whole segment, the checksum is not validated. Even if IFileInputStream instances are closed, they should always validate checksums."
HADOOP-5458,Remove Chukwa from .gitignore,"Since Chukwa has moved to a subproject, its entries in .gitignore can be removed."
HADOOP-5457,Failing contrib tests should not stop the build,"If one or more of the unit tests in HdfsProxy fail, none of the subsequent test suites are run. All of the unit tests should run, regardless of previous projects' status."
HADOOP-5456,javadoc warning: can't find restoreFailedStorage() in ClientProtocol,"ant javadoc-dev
{noformat}
  [javadoc] /home/tsz/hadoop/latest/src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:399: warning - Tag @see: can't find restoreFailedStorage() in org.apache.hadoop.hdfs.protocol.ClientProtocol
  [javadoc] /home/tsz/hadoop/latest/src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:412: warning - Tag @see: can't find restoreFailedStorage() in org.apache.hadoop.hdfs.protocol.ClientProtocol
{noformat}"
HADOOP-5455,"default ""hadoop-metrics.properties"" doesn't mention ""rpc"" context","The ""hadoop-metrics.properties"" file that's shipped in conf/ has configuration settings for the metrics contexts ""dfs"", ""mapred"", and ""jvm"".  The (trivial) patch I'm proposing is to include default configuration for the ""rpc"" context as well.  RPC metrics may be useful, and it's difficult for a user to intuit otherwise that rpc metrics even exist.  (I stumbled upon them after exploring with JConsole and JMX.)"
HADOOP-5451,Unresolved xmlenc dependency,"On trunk, ant fails with the following error message:

{code}
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: xmlenc#xmlenc;0.52: configuration not found in xmlenc#xmlenc;0.52: 'master'. It was required from org.apache.hadoop#hdfsproxy;working@WE02C022 common
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
{code}"
HADOOP-5450,Add support for application-specific typecodes to typed bytes,"For serializing objects of types that are not supported by typed bytes serialization, applications might want to use a custom serialization format. Right now, typecode 0 has to be used for the bytes resulting from this custom serialization, which could lead to problems when deserializing the objects because the application cannot know if a byte sequence following typecode 0 is a customly serialized object or just a raw sequence of bytes. Therefore, a range of typecodes that are treated as aliases for 0 should be added, such that different typecodes can be used for application-specific purposes."
HADOOP-5449,Verify if JobHistory.HistoryCleaner works as expected,"Here is the piece of code I doubt
{code}
public void run(){
      if (isRunning){
        return;
      }
      now = System.currentTimeMillis();
      // clean history only once a day at max
      if (lastRan ==0 || (now - lastRan) < ONE_DAY_IN_MS){
        return;
      }
      lastRan = now;
.....
// main code for cleaning
}
{code}
{{lastRun}} is initialized to 0 and hence HistoryCleaner will never execute the main code. Also a testcase should be written for JobHistory.HistoryCleaner to check if it works as expected. "
HADOOP-5446,TaskTracker metrics are disabled,"HADOOP-3772 changed TaskTracker to use an instrumentation class, but did not update the default metrics class to the new API. TT metrics are currently discarded."
HADOOP-5442,The job history display needs to be paged ,Currently the list of job history will try to render the entire list of jobs that have run. That doesn't scale up as more and more jobs run on a job tracker.
HADOOP-5440,Successful taskid are not removed from TaskMemoryManager,"Successfully completed task-attempt-ids are not removed from TaskMemoryManager. This is after refactoring the code in tracker.reportTaskFinished into tip.reportTaskFinished, in HADOOP-4759"
HADOOP-5438,Merge FileSystem.create and FileSystem.append,"Currently, when a user wants to modify a file, the user first calls exists() to know if this file is already there. And then uses create() or append() according to whether the file exists or not.
the code looks like:
{code}
FSDataOutputStream out_1 = null;
if (fs.exists(path_1))
   out_1 = fs.append(path_1);
else
   out_1 = fs.create(path_1);
{code}
. On the performace side,It involes two RPCs. On the easy-of-use side, it is not very convient in contrast to the traditional open interface.

It will more complicate if there is a overwrite parameter specified. I donot know whether there is a bug about 'overwrite' in 0.19, some times it takes a long time for overwrite creates to reture. So i make the write file code with overwrite param works like:
{code}
boolean exists = fs.exists(name);
if (overwrite) {
    if (exists)
       fs.delete(name, true);
     this.out = fs.create(name, overwrite, bufferSize, replication,
				    blockSize, progress);
     this.currentRowID = 0;
 } else {
   if (!exists)
	this.out = fs.create(name, overwrite, bufferSize,
					replication, blockSize, progress);
   else
	this.out = fs.append(name, bufferSize, progress);
{code}

Some code statements there are really redundant and not needed, especialy with the delete(). But without deleting first, the overwrite takes a long time to reture.

BTW, i will create another issue about the overwrite problem. If it is not a bug at all or a duplicate, someone please close it.

"
HADOOP-5437,Unit test for jvm -reuse,"TestMiniMRDFSSort.testMapReduceSortWithJvmReuse actually does not test jvm running multiple tasks. In the test, each jvm runs only one task."
HADOOP-5432,TestHdfsProxy fails on 0.20,"TestHdfsProxy fails with the following exception:
{noformat}
09/03/06 18:28:05 ERROR mortbay.log: EXCEPTION 
java.lang.NullPointerException
        at org.mortbay.jetty.security.SslSocketConnector.createFactory(SslSocketConnector.java:215)
        at org.mortbay.jetty.security.SslSocketConnector.newServerSocket(SslSocketConnector.java:423)
        at org.mortbay.jetty.bio.SocketConnector.open(SocketConnector.java:73)
        at org.apache.hadoop.http.HttpServer.start(HttpServer.java:420)
        at org.apache.hadoop.hdfsproxy.HdfsProxy.start(HdfsProxy.java:96)
        at org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface(TestHdfsProxy.java:232)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:154)
        at junit.framework.TestCase.runBare(TestCase.java:127)
        at junit.framework.TestResult$1.protect(TestResult.java:106)
        at junit.framework.TestResult.runProtected(TestResult.java:124)
        at junit.framework.TestResult.run(TestResult.java:109)
        at junit.framework.TestCase.run(TestCase.java:118)
        at junit.framework.TestSuite.runTest(TestSuite.java:208)
        at junit.framework.TestSuite.run(TestSuite.java:203)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
{noformat}"
HADOOP-5430,jobtracker.jsp showing bogus map and reduce counts,"My cluster is currently idle, but it shows the following statistics on the jobtracker status page:

Maps	Reduces	Total Submissions	Nodes	Map Task Capacity	Reduce Task Capacity	Avg. Tasks/Node
-86	4	118	26	130	78	8.00

I would expect the Maps and Reduces columns to both be 0."
HADOOP-5423,It should be posible to specify metadata for the output file produced by SequenceFile.Sorter.sort,
HADOOP-5422,In hdfs /*/* globbing does not work,"With reference to Jira issue : 3497, we tried globbing /*/*. But it does not work. Output is given below. 

 bin/hadoop --config /tmp/cluster/ fs -lsr /
log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration).
log4j:WARN Please initialize the log4j system properly.
drwxr-xr-x   - karthv supergroup          0 2009-03-06 08:00 /a
drwxr-xr-x   - karthv supergroup          0 2009-03-06 08:00 /a/b
drwxr-xr-x   - karthv supergroup          0 2009-03-06 08:00 /a/b/c



bin/hadoop --config /tmp/cluster/ fs -lsr /*/*

Actually it is ""bin/hadoop --config /tmp/cluster/ fs -lsr Forward slash followed by star followed by forward slash followed by star""

output:
lsr: Cannot access /bin/arch: No such file or directory.
lsr: Cannot access /bin/ash: No such file or directory.
lsr: Cannot access /bin/ash.static: No such file or directory.
lsr: Cannot access /bin/awk: No such file or directory.
lsr: Cannot access /bin/basename: No such file or directory.
lsr: Cannot access /bin/bash: No such file or directory.
lsr: Cannot access /bin/bsh: No such file or directory.
lsr: Cannot access /bin/cat: No such file or directory.
....
....
...

It keeps giving a long list like this without showing the results."
HADOOP-5421,HADOOP-4638 has broken 0.19 compilation,UtilsForTest is missing in 0.19 and {{TestRecoveryManager}} uses it.
HADOOP-5420,Support killing of process groups in LinuxTaskController binary,Support setsid based kill in LinuxTaskController.
HADOOP-5419,Provide a way for users to find out what operations they can do on which M/R queues,"This issue is to provide an improvement on the existing M/R framework to let users know which queues they have access to, and for what operations. One use case for this would that currently there is no easy way to know if the user has access to submit jobs to a queue, until it fails with an access control exception."
HADOOP-5417,IPC client drops interrupted exceptions,The IPC client needless drops InterruptedException.
HADOOP-5416,"Wrong description of "" hadoop fs -test "" in FS Shell guide . ","Hadoop FS Shell Guide  documentation for -test command option -d  current description is.
 
  "" -d check return 1 if the path is directory else return 0. "" 

Where as it should be .
   
   "" -d check to see if the path is Directory . Return 0 if true ""
"
HADOOP-5414,IO exception while executing hadoop fs -touchz  fileName  ,"Stack trace while executing hadoop fs -touchz command .

[user@xyzhostname ~]$ hadoop fs -touchz test/new0LenFile2
09/03/05 23:31:21 WARN hdfs.DFSClient: Problem renewing lease for DFSClient_-661919204
java.io.IOException: Call to xxxxx-xxx.xxx.com/xxx.xxx.xxx.xxx:xxxx failed on local exception: java.nio.channels.ClosedByInterruptException
	"
HADOOP-5406,Misnamed function in ZlibCompressor.c,"All methods in ZlibCompressor.c are named like ""Java_org_apache_hadoop_io_compress_zlib_ZlibCompressor_METHODNAME""

There is one that is called: Java_org_apache_hadoop_io_compress_ZlibCompressor_setDictionary 
Should be: Java_org_apache_hadoop_io_compress_zlib_ZlibCompressor_setDictionary"
HADOOP-5400,JT restart recovery:  Exclude jobs which failed during SUBMIT_JOB (due to acl) ,"mapred.jobtracker.restart.recover is set to true in mapred-site.xml

This is a job that failed  during Job submit due to invalid ACL 

2009-03-04 18:31:25,970 INFO org.apache.hadoop.ipc.Server: IPC Server handler 14 on 50300, call submitJob(job_200903041223_0259) from 192.168.10.1:41306: error: org.apache.hadoop.security.AccessControlException: User rajive cannot perform operation SUBMIT_JOB on queue default

When the  JobTracker was restarted after some time, the failed job was being recovered/restarted

2009-03-04 19:13:30,544 INFO org.apache.hadoop.mapred.JobTracker: Found an incomplete job directory job_200903041852_0040. Deleting it!!
2009-03-04 19:13:30,613 INFO org.apache.hadoop.mapred.FairScheduler: Successfully configured FairScheduler
2009-03-04 19:13:30,614 INFO org.apache.hadoop.mapred.JobTracker: Trying to recover job job_200903041223_0259


2009-03-04 18:53:17,147 INFO org.apache.hadoop.mapred.JobTracker: JobTracker failed to recover job job_200903041223_0259. Ignoring it.
java.io.FileNotFoundException: File file:/var/log/hadoop//history/jobtracker1.foo.com_1236192735577_job_200903041223_0259_rajive_word+count does not exist.
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:360)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:125)
        at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:283)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:336)
        at org.apache.hadoop.mapred.JobHistory.parseHistoryFromFS(JobHistory.java:245)
        at org.apache.hadoop.mapred.JobTracker$RecoveryManager.recover(JobTracker.java:1144)
        at org.apache.hadoop.mapred.JobTracker.offerService(JobTracker.java:1603)
        at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:3326)
2009-03-04 18:53:17,147 INFO org.apache.hadoop.mapred.JobTracker: Restart count for job job_200903041223_0259 is 0
2009-03-04 18:53:18,626 INFO org.apache.hadoop.mapred.JobInProgress: Input size for job job_200903041223_0259 = 4664646202464
2009-03-04 18:53:18,626 INFO org.apache.hadoop.mapred.JobInProgress: Split info for job:job_200903041223_0259 with 34640 splits:

These jobs failed during job submit shouldn't be considered for recovery. 
"
HADOOP-5396,Queue ACLs should be refreshed without requiring a restart of the job tracker,"In large shared deployments of the M/R clusters, it is normal that new users will periodically want to get access to some queues on the M/R framework. Requiring a JT restart for each such change is operationally inconvenient and seems an overkill. There should be a way for updating ACLs with new users without requiring a JT restart."
HADOOP-5395,"When queue ACLs are enabled, the error message shown when a job is submitted to a non-existent queue is misleading","When acls are enabled on the job tracker using the property mapred.acls.enabled, and a job is submitted to a queue name that does not exist in mapred.queue.names property, the following exception is thrown:

org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: User user-name cannot perform operation SUBMIT_JOB on queue queue-name
       at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
       at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
       at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
       at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
       at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:96)
       at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:58) 

The message makes it appear like the queue exists, but the user does not have access to it, where the actual problem is that the queue does not exist at all."
HADOOP-5394,JobTracker might schedule 2 attempts of the same task with the same attempt id across restarts,"This can happen when the jobtracker gets restarted more than once. In such cases, the jobtracker depends on the jobhistory file for the next restart count. If the new restart-count is not flushed to the file then there is a fair chance that upon next restart, the jobtracker might schedule a new attempt with an existing id. This can cause problems not only with the side-effect files but also can cause the jobtracker to be in an inconsistent state."
HADOOP-5392,JobTracker crashes during recovery if job files are garbled,Jobtracker crashed in the recovery stage for a job with 0 byte job.xml. Ideally one would expect the jobtracker to try and recover as many jobs as possible.
HADOOP-5391,TestJobHistory fails intermittently.,"TestJobHistory fails intermittently with following error message:
User log file file:/home/hudson/hudson-slave/workspace/Hadoop-Patch-vesta.apache.org/trunk/build/test/data/succeed/output1/_logs/history/localhost_1236132901323_job_200903040215_0001_hudson_test-job-succeed.recover does not exist

One of the hudson builds with failure :
http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/41/testReport/org.apache.hadoop.mapred/TestJobHistory/testJobHistoryUserLogLocation/

I saw the failure in local machine also some times."
HADOOP-5390,"hdfsproxy includes duplicate jars in tarball, source in binary tarball","The binary tarball should not include hdfsproxy source. Similarly, hdfsproxy should not include its own copy of jars already in the distribution, particularly hadoop-\* jars."
HADOOP-5386,To Probe free ports dynamically for Unit test to replace fixed ports,"Currently hdfsproxy unit test uses Cactus in-container test. It uses three fixed ports. one for tomcat start-up/shut-down, another for tomcat http-port and the third for tomcat https-port. 
If theses ports are already in use, ant build will fail. To fix this, we decided to use a java program to probe the free ports dynamically and update the tomcat conf with these free ports. "
HADOOP-5384,DataNodeCluster should not create blocks with generationStamp == 1,"In DataNodeCluster.main(..), injected blocks are created with generationStamp == 1, which is a reserved value but not a valid generation stamp.  As a consequence, NameNode may die when those blocks are reported."
HADOOP-5382,The new map/reduce api doesn't support combiners,"Currently, combiners are only called if they are defined using the old deprecated api. "
HADOOP-5379,Throw exception instead of writing to System.err when there is a CRC error on CBZip2InputStream,"From org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.java:


{code}
private static void reportCRCError() throws IOException {
   // The clean way would be to throw an exception.
   // throw new IOException(""crc error"");

   // Just print a message, like the previous versions of this class did
   System.err.println(""BZip2 CRC error"");
}
{code}"
HADOOP-5378,Error in displaying job history,"
The job history page is broken for some reason. When clicked, the page return the following:

HTTP ERROR: 500

1

RequestURI=/jobhistory.jsp

Powered by Jetty://



When I check the JobTracker log, I saw the following exception:


2009-03-02 10:29:00,916 WARN /: /jobhistory.jsp: 
java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.mapred.jobhistory_jsp$2.compare(jobhistory_jsp.java:109)
	at org.apache.hadoop.mapred.jobhistory_jsp$2.compare(jobhistory_jsp.java:93)
	at java.util.Arrays.mergeSort(Arrays.java:1270)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.sort(Arrays.java:1210)
	at org.apache.hadoop.mapred.jobhistory_jsp._jspService(jobhistory_jsp.java:93)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-5377,Inefficient jobtracker history file layout,"Storing too many files in a single directory slows things down tremendously and in this case, makes the grid just a bit more difficult to manage.  On our jobtrackers, even with a 45 day purge cycle, we see hundreds of thousands of files in logs/hadoop/history.  The following is an example:

pchdm01.ypost.re1: logs/hadoop/history - 1,176,927 files!

This is the time(1) output of the `ls | wc -l`

real    0m56.042s
user    0m28.702s
sys     0m1.794s

Note that this was the second time I ran this filecount. The first run took more than 4 minutes of real time.

===========================================

My recommended solution is that the Hadoop team store these files in the following structure:
    history/2008/08/19
    history/2008/08/20
    history/2008/08/21

Using this structure gives us 2 important things: consistently good performance and the ability to easily delete or archive old files.  

If we expect a Hadoop cluster to process hundreds of thousands of jobs per day, then we may want to break it down by
hour like this:
    history/2008/08/19/00
    history/2008/08/19/01
     ...
    history/2008/08/19/22
    history/2008/08/19/23
"
HADOOP-5376,JobInProgress.obtainTaskCleanupTask() throws an ArrayIndexOutOfBoundsException,
HADOOP-5374,NPE in JobTracker.getTasksToSave() method,
HADOOP-5369,Small tweaks to reduce MapFile index size,"Two minor tweaks can help reduce the memory overhead of the MapFile index a bit:

1) Because the index file is a sequence file, it's length is not known. That means the index is built using the standard ""mulitply the size of the buffer on overflow"" with a factor of 3/2. With small keys, the slack in the index can be substantial. This patch has a constant upper bound on the amount of slack allowed.

2) In block compressed map files the index file often has entries with the same offset (because the compressed block had more than index interval keys). The entries with identical offsets do not help MapFile do random access any faster. This patch eliminates these types of entries from new map files, and ignores them while reading old map files. This patch greatly helped with memory usage on a compressed hbase table."
HADOOP-5366,Support for retrieving files using standard HTTP clients like curl,Currently hdfsproxy only supports HFTP protocol and users have to use HFTP clients. Need to add a simple HTTP GET interface so that users can use standard HTTP clients like curl to retrieve files.
HADOOP-5365,hdfsprxoy should log every access,"Currently, only failed accesses are logged. Need to log successful accesses as well."
HADOOP-5364,Adding SSL certificate expiration warning to hdfsproxy,SSL certificate warning should be provided on both client and proxy server side.
HADOOP-5363,Proxying for multiple HDFS clusters of different versions,"A single hdfsproxy server should be able to proxy for multiple HDFS clusters, whose Hadoop versions may be different from each other."
HADOOP-5351,Child.java and Task.java use wrong class names for creating a Log instance,Child.java uses TaskTracker's class name and Task.java uses TaskRunner's class name. This causes confusion during debugging.
HADOOP-5347,bbp example cannot be run.,FileAlreadyExistsException: Output directory already exists.
HADOOP-5346," ""quota commands"" shown as lower case","http://hadoop.apache.org/core/docs/r0.19.0/hdfs_quota_admin_guide.html

The command is shown as:
dfsadmin -setquota
dfsadmin -setspacequota

But it should read as in the CLI
dfsadmin -setQuota
dfsadmin -setSpaceQuota

I'm assuming the other quota commands should also be mixed case."
HADOOP-5344,MapReduceBase has been deprecated but there are no references to classes that are replacing it,MapReduceBase has been deprecated but the JavaDoc has no references to classes that are replacements.
HADOOP-5341,hadoop-daemon isn't compatible after HADOOP-4868,"The CLI changed for hadoop-daemon.sh in an incompatible way. It now requires the sub-system name in the CLI.
"
HADOOP-5338,Reduce tasks are stuck waiting for map outputs when none are in progress,"When JT is restarted several times, a situation is encountered when the reduce tasks are stuck forever waiting for map outputs. However 100%map is complete and none of the map tasks are in progress. The reduce tasks wait infinitely."
HADOOP-5337,JobTracker greedily schedules tasks without running tasks to join,"This issue was observed when JobTracker was restarted 3 times and observed that 4 instances of each reduce task were running. This issue is observed when cluster is not fully occupied.

In testcase: Map/reduces capacity is 200/200 slots respectively and Job profile is 11000 maps, 10 reduces and speculative execution is off.  JobTracker was restarted 3 times in small intervals of about 5 mins and after recovery, 40 reduce tasks were running. Task details web page (taskdetails.jsp) was  showing 4 running attempts of each reduce task.
"
HADOOP-5333,The libhdfs append API is not coded correctly,The hdfsOpenFile() API does not handle the APPEND bit correctly.
HADOOP-5332,Make support for file append API configurable,FSNamesystem already has a private dfs.support.append config option.  Make this public in src/hdfs/hdfs-default.xml and set the default to false.  
HADOOP-5331,KFS: Add support for append,KFS has support for file appends.  THis jira is to get the append support for changes included in the KFS+Hadoop glue layer
HADOOP-5329,TestLocalMRNotification.testMR failed in Hudson,"TestLocalMRNotification.testMR failed in Hudson, from [build #3911|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3911/testReport/] to the latest, [build #3917|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3917/testReport/]."
HADOOP-5328,Renaming of Job histroy file is incorrect if Jobtracker is restarted multimple times,"After 1st JT restart job history file name was jobfilename.recover
After 2and JT restart  job history file name was jobfilename.recover.recover
After 3rd JT restart  ob history file name was jobfilename.recover.recover.recover"
HADOOP-5327,Job files for a job failing because of  ACLs are not clean from the system directory,Jobs which failed because of ACLs gets added during JT restart recovery 
HADOOP-5326,bzip2 codec (CBZip2OutputStream) creates corrupted output file for some inputs,"Bzip2 codec generated corrupted output files in some test executions I performed. This bug is probably related to https://issues.apache.org/bugzilla/show_bug.cgi?id=41596.

* In my case, the problem seems to be at the BWT (Burrows-Wheeler Transform) implementation."
HADOOP-5325,Jobs are getting  stuck  on JobTracker  restart,"Recovery manager failed to recover the job properly and threw ""
2009-02-25 09:24:09,944 INFO org.apache.hadoop.fs.FSInputChecker: Found checksum error: b[7168, 7168]=
org.apache.hadoop.fs.ChecksumException: Checksum error: file filename "".
. As part of recovery, one of the attempts got added to the expiry launching tasks list. But. another attempt of the same tip was relaunched with the same attempt id and the job was stuck. Looks like the attempt which got expired was marked as failed and newly launched attempt (with same attempt id) was successful and Jobtracker tried marking a failed tip as successful hence the tip was considered as failed but tip.isComplete returns true.
"
HADOOP-5323,Trash documentation should describe its directory structure and configurations,"Trash documentation should mention the significance of ""Current"" and ""<time-stamp>"" directories which get generated inside Trash directory. The documentation should also incorporate modifications done in HADOOP: 4970."
HADOOP-5322,comments in JobInProgress related to TaskCommitThread are not valid,"There are some comments in JobInProgress referring to TaskCommitThread. Since TaskCommitThread is  no more present, the comments should be deleted/modified. "
HADOOP-5321,BZip2CompressionOutputStream sometimes corrupts data,"We are using a BZip2CompressionOutputStream.java version with bugs.

See the following 2 issues for details:
https://issues.apache.org/bugzilla/show_bug.cgi?id=24798
https://issues.apache.org/bugzilla/show_bug.cgi?id=41596
"
HADOOP-5320,TestMapReduceLocal is missing a close() that is causing it to fail while running the test on NFS,The readFile method in this test is not calling a close of the file after it is done reading. This causes some lingering .nfs* files that is preventing the directory from getting deleted properly causing the second program in this test to fail.
HADOOP-5317,Provide documentation for LazyOutput Feature,"HADOOP-4927 introduced support for the ""LazyOutput"" feature. Documentation needs to be updated for this."
HADOOP-5316,Unable to detect running process from pid file,"Linux ps command output produces white space in front of the pid number, if the pid is lesser than 5 digits number.  The current shell script does not parse this correctly for watchdog.  In addition, CHUKWA_PID_DIR needs to be better supported by chukwa's shell script.  In some of the shell scripts, the reference to pid file is in CHUKWA_HOME/var/run, and this should be changed to CHUKWA_PID_DIR."
HADOOP-5314,needToSave incorrectly calculated in loadFSImage(),"{{FSImage.loadFSImage()}} incorrectly calculates the value of {{needToSave}}, which is always true and results in saving image at startup even if that is not necessary."
HADOOP-5307,Fix null value handling in StringUtils#arrayToString() and #getStrings(),"StringUtils#arrayToString() converts String array to a String of comma separated elements. If the String array includes null values, these are recovered as ""null"" (literal) from getStrings() method, which eventually causes configuration issues. "
HADOOP-5306,"Job History file can have empty string as http port after JobTracker Restart in case of lost TT, which can result in NumberFormatException when JT is restarted 2nd time","HTTP_PORT=""""  is seen in job history file after JT recovery in case of lost TT. The .recover file of TestJobTrackerRestartWithLostTracker has empty string as HTTP_PORT. If another time JT is restarted and then JT tries to read the history line and tries to createTaskAttempt, it would get NumberFormatException because of Integer.parseInt(httpPort). We somehow need to log a legal value as HTTP_PORT in the history file OR the exception needs to be caught and proper action is to be taken."
HADOOP-5305,Unit test org.apache.hadoop.fs.TestCopyFiles.testMapCount fails on trunk,"org.apache.hadoop.fs.TestCopyFiles.testMapCount fails on trunk quite often with ""Unexpected map count"" error message. See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/762/testReport/org.apache.hadoop.fs/TestCopyFiles/testMapCount/] for detailed output."
HADOOP-5300,"""ant javadoc-dev"" does not work","{noformat}
bash-3.2$ ant javadoc-dev
Buildfile: build.xml

javadoc-dev:
    [mkdir] Created dir: d:\@sze\hadoop\latest\build\docs\dev-api
  [javadoc] Generating Javadoc

BUILD FAILED
d:\@sze\hadoop\latest\build.xml:936: Reference ivy-common.classpath not found.

Total time: 0 seconds
{noformat}"
HADOOP-5298,Unit test fails out on trunk org.apache.hadoop.http.TestServletFilter.testServletFilter,"From: http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/760/

Regression

org.apache.hadoop.http.TestServletFilter.testServletFilter

Failing for the past 1 build (Since #760 )
Took 1 min 10 sec.
Error Message

url[4]=/static/hadoop-logo.jpg expected:<8> but was:<9>"
HADOOP-5292,KFS: calling getFileBlockLocations() on 0-length file causes a NPE,"When getFileBlockLocations() in KosmosFileSystem.java is called on a file with 0-length, there is a NPE."
HADOOP-5286,DFS client blocked for a long time reading blocks of a file on the JobTracker,"On a large cluster, we've observed that DFS client was blocked on reading a block of a file for almost 1 and half hours. The file was being read by the JobTracker of the cluster, and was a split file of a job. On the NameNode logs, we observed that the block had a message as follows:

Inconsistent size for block blk_2044238107768440002_840946 reported from <ip>:<port> current size is 195072 reported size is 1318567

Details follow.

 "
HADOOP-5285,JobTracker hangs for long periods of time,"On one of the larger clusters of 2000 nodes, JT hanged quite often, sometimes for times in the order of 10-15 minutes and once for one and a half hours(!). The stack trace shows that JobInProgress.obtainTaskCleanupTask() is waiting for lock on JobInProgress object which JobInProgress.initTasks() is holding for a long time waiting for DFS operations."
HADOOP-5282,Running tasks on lost trackers are incorrectly logging the attempt and task failures,"If a running attempt is lost on a tracker, a attempt-level cleanup with the same attempt-id is launched for it. In such cases the events are not logged correctly and instead of logging
{code}
Task Reduce Start
Attempt Reduce Start
Attempt Reduce Killed 
Task reduce Killed
{code}
it logs
{code}
Task Reduce Start
Task Reduce Failed
Attempt Reduce Start
Attempt Reduce Killed 
{code}"
HADOOP-5281,GzipCodec fails second time it is used in a process,"The attached code (GZt.java) raises:
{noformat}
java.io.IOException: incorrect header check
	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.inflateBytesDirect(Native Method)
	at org.apache.hadoop.io.compress.zlib.ZlibDecompressor.decompress(ZlibDecompressor.java:221)
	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:80)
	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:74)
	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:62)
	at java.io.DataInputStream.readByte(DataInputStream.java:248)
	at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:325)
	at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:346)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1853)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1876)
	at org.apache.hadoop.io.MapFile$Reader.readIndex(MapFile.java:319)
	at org.apache.hadoop.io.MapFile$Reader.seekInternal(MapFile.java:435)
	at org.apache.hadoop.io.MapFile$Reader.seekInternal(MapFile.java:417)
	at org.apache.hadoop.io.MapFile$Reader.seek(MapFile.java:404)
	at org.apache.hadoop.io.MapFile$Reader.get(MapFile.java:523)
{noformat}"
HADOOP-5280,"When expiring a lost launched task, JT doesn't remove the attempt from the taskidToTIPMap.",
HADOOP-5279,test-patch.sh scirpt should just call the test-core target as part of runtestcore function.,"checkReleaseAuditWarnings function just calls the releaseaudit target.
checkFindbugsWarnings function just calls findbugs. 

runCoreTests function is calling docs tar and test-core.

If we just want to execute the test-core target then we should skip calling docs and tar target ..
"
HADOOP-5278,Finish time of a TIP is incorrectly logged to the jobhistory upon jobtracker restart,"Upon recovery, the jobtracker replays the attempt information to the JIP. Upon seeing the attempt-end info, the JIP _completes_ the TIP (which sets the finish time to current time) and logs it to the new jobhistory file (the _.recover_ file). After seeing the task(tip) finish line, the recovery manager changes the finish time to correct finish time but the new jobhistory file still contains the wrong value for the finish time seen while completing the TIP."
HADOOP-5276,"Upon a lost tracker, the task's start time is reset to 0","Upon lost tracker, the JIP (via JobTracker.lostTracker() -> JobInProgress.failedTask()) hand crafts task status (marking the attempt as KILLED) and updates the JobInProgress (via JobInProgress.updateTaskStatus()). This status contains the attempt start time as 0. One major impact would be that the listener will overwrite the last start time value with the new one and hence the start tiem will get garbled."
HADOOP-5275,ivy directory should be there in hadoop tar ball,Ivy directory should also be in hadoop tar ball. It is required when we are using hadoop (untarred from hadoop tar ball) and need to compile any of the compnent individually (e.g. gridmix2). 
HADOOP-5274,gridmix2 is not getting compiled to generate gridmix.jar,"Not able to compile gridmix2 to generate gridmix.jar. Compilation gets failed giving build failed message.
It seems that problem is with mapper class and reduce class specified in CombinerJobCreator.java. Changed mapper class from ""MapClass.class"" to ""Mapper.class"" and reduce class  from ""Reduce.class"" to ""Reducer.class"" then it started working and gridmix.jar was generated."
HADOOP-5273,License header missing in TestJobInProgress.java,
HADOOP-5272,JobTracker does not log TIP start information after restart,"In case of JobTracker restart, attempt_0123456789_0001_m_000000_0 might not be the first attempt to get scheduled as the attempt id offset changes after every restart. For example, 
upon first restart the new attempt id will be attempt_0123456789_0001_m_000000_1000. Hence TIP start line never gets logged to the job history after the restart, TaskInProgress.isFirstAttempt() always returns *false*."
HADOOP-5269,TaskTracker.runningTasks holding FAILED_UNCLEAN and KILLED_UNCLEAN taskStatuses forever in some cases.,"Tasktracker is holdingup TaskStatus objects in runningTasks forever in somecases. This happens in the following scenario.
-> Task got an exception
-> Sets the phase to CLEANUP
-> The task tries to do cleanup. and it doesn't respond after that.
-> TaskTracker marks the task unresponsive and makes the task FAILED_UNCLEAN
-> TaskTracker doesn't remove it from runningTasks data structure, since phase is CLEANUP and state is FAILED_UNCLEAN (it treats this as cleanupAttempt).

I would propose that once the task goes to CLEANUP phase, kill on the task should mark it a clean failure i.e. The task state should be FAILED/KILLED."
HADOOP-5266,"Values Iterator should support ""mark"" and ""reset""",Some users have expressed interest in having a mark-reset functionality on values iterator. Users can call mark() at any point during the iteration process and a subsequent reset() should move the iterator to the last value emitted when mark() was called. 
HADOOP-5264,TaskTracker should have single conf reference,Tasktracker has 2 conf member variables - originalConf and fConf. This is not required. It should only have one.
HADOOP-5259,Job with output hdfs:/user/<username>/outputpath (no authority) fails with Wrong FS,"Using namenode with default port of 8020.

When starting a job with output hdfs:/user/knoguchi/outputpath, my job fails with 

Wrong FS: hdfs:/user/knoguchi/outputpath, expected: hdfs://aaa.bbb.cc
"
HADOOP-5258,Provide dfsadmin functionality to report on namenode's view of network topology,"As discussed in HADOOP-4954, it would be useful to be able to query the namenode to its current view on the network topology of racks and datanodes.  This would allow ops to compare what the namenode sees with what they expect it to see.  "
HADOOP-5257,Export namenode/datanode functionality through a pluggable RPC layer,"Adding support for pluggable components would allow exporting DFS functionallity using arbitrary protocols, like Thirft or Protocol Buffers. I'm opening this issue on Dhruba's suggestion in HADOOP-4707.

Plug-in implementations would extend this base class:

{code}abstract class Plugin {

    public abstract datanodeStarted(DataNode datanode);

    public abstract datanodeStopping();

    public abstract namenodeStarted(NameNode namenode);

    public abstract namenodeStopping();
}{code}

Name node instances would then start the plug-ins according to a configuration object, and would also shut them down when the node goes down:

{code}public class NameNode {

    // [..]

    private void initialize(Configuration conf)
        // [...]
        for (Plugin p: PluginManager.loadPlugins(conf))
          p.namenodeStarted(this);
    }

    // [..]

    public void stop() {
        if (stopRequested)
            return;
        stopRequested = true;
        for (Plugin p: plugins) 
            p.namenodeStopping();
        // [..]
    }

    // [..]
}{code}

Data nodes would do a similar thing in {{DataNode.startDatanode()}} and {{DataNode.shutdown}}"
HADOOP-5255,Fix for HADOOP-5079 HashFunction inadvertently destroys some randomness,"HADOOP-5079 did this ""HashFunction.hash restricts initval for the next hash to the [0, maxValue) range of the hash indexes returned. This is suboptimal, particularly for larger nbHash and smaller maxValue. Rather we should first set initval, then restrict the range for the result assignment.""  The patch committed on that issue introduced a new bug: ""My first patch contained a regression: you have to take the remainder before calling Math.abs, since Math.abs(Integer.MIN_VALUE) == Integer.MIN_VALUE still"" (Jonathan Ellis)."
HADOOP-5253,to remove duplicate calls to the cn-docs target.,"package target depends on docs, cn-docs and ....etc...
and doc target intern calls cn-docs which results in call calling cn-docs target twice when ant package executed.

"
HADOOP-5252,Streaming overrides -inputformat option,"For some reason, streaming currently falls back to {{StreamInputFormat}} when it is asked to use {{SequenceFileInputFormat}} or {{KeyValueTextInputFormat}} via the {{-inputformat}} option, even when no {{-inputreader}} option is specified. In case of {{KeyValueTextInputFormat}} this is not really a problem and for {{SequenceFileInputFormat}} the {{AutoInputFormat}} added by HADOOP-1722 provides a way around this, but it would be better to get this fixed..."
HADOOP-5251,TestHdfsProxy and TestProxyUgiManager frequently fail,"Had a look at the Hudson nightly builds for Hadoop and all of them have failed lately.
Two of the tests that fail frequently are TestHdfsProxy and TestProxyUgiManager.

Example:
http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/743/testReport/
http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/744/testReport/
http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/750/testReport/

It seems to be related to Clover, perhaps also related to HADOOP-3921?"
HADOOP-5248,Job directories could remain undeleted in some scenarios after job completes.,"I observed a couple of times that when a job has completed, its job directories were not cleaned up. In discussion, it seems like there is a condition when only reduces from a job are run on a machine and no maps, the TT does not get a signal from the JT to delete the files and could be left behind. FYI, JVM reuse was enabled at the time. I can confirm that 'KillJobAction' was not received by the TT."
HADOOP-5247,NPEs in JobTracker and JobClient when mapred.jobtracker.completeuserjobs.maximum is set to zero.,
HADOOP-5244,Distributed cache spends a lot of time runing du -s,"When running a MapReduce job that has a large jar on the class path (eg, jruby from hbase), the task tracker takes a large amount of CPU time during startup. Using jstack, I got the following stack trace:

""Thread-8941"" daemon prio=10 tid=0x00002aab08005c00 nid=0x2807 waiting on condition [0x0000000043eca000..0x0000000043ecbc90]
   java.lang.Thread.State: RUNNABLE
	at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:232)
	at java.lang.StringCoding.encode(StringCoding.java:272)
	at java.lang.String.getBytes(String.java:947)
	at java.io.UnixFileSystem.getLength(Native Method)
	at java.io.File.length(File.java:848)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:428)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:210)
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:178)

Running the system ""du"" returns very quickly on the directory. 
"
HADOOP-5241,Reduce tasks get stuck because of over-estimated task size (regression from 0.18),"I have a simple MR benchmark job that computes PageRank on about 600 GB of HTML files using a 100 node cluster. For some reason, my reduce tasks get caught in a pending state. The JobTracker's log gets filled with the following messages:

2009-02-12 15:47:29,839 WARN org.apache.hadoop.mapred.JobInProgress: No room for reduce task. Node tracker_d-59.cs.wisc.edu:localhost/127.0.0.1:33227 has 110125027328 bytes free; but we expect reduce input to take 399642198235
2009-02-12 15:47:29,852 WARN org.apache.hadoop.mapred.JobInProgress: No room for reduce task. Node tracker_d-67.cs.wisc.edu:localhost/127.0.0.1:48626 has 107537776640 bytes free; but we expect reduce input to take 399642198235
2009-02-12 15:47:29,885 WARN org.apache.hadoop.mapred.JobInProgress: No room for reduce task. Node tracker_d-73.cs.wisc.edu:localhost/127.0.0.1:58849 has 113631690752 bytes free; but we expect reduce input to take 399642198235
<SNIP>

The weird thing is that I get through about 70 reduce tasks completing before it hangs. If I reduce the amount of the input data on 100 nodes down to 200GB, then it seems to work. As I scale the amount of input to the number of nodes, I can get it work some of the times on 50 nodes and without any problems on 25 nodes and less.

Note that it worked without any problems on Hadoop 0.18 late last year without changing any of the input data or the actual MR code."
HADOOP-5240,'ant javadoc' does not check whether outputs are up to date and always rebuilds,Running 'ant javadoc' twice in a row calls the javadoc program both times; it doesn't check to see whether this is redundant work.
HADOOP-5235,possible NPE in tip.kill(),"There is possibility of NPE with runner.kill() code in TaskTracker.TaskInProgress.kill().
If a killTaskAction is issued for cleanup attempt of a task, forwhich runner is not created, the existing code would through NPE.
"
HADOOP-5234,NPE in TaskTracker reinit action,"I have seen an NPE in TT reinit action.
TT logs :
{noformat}
2009-02-12 12:14:02,859 INFO org.apache.hadoop.mapred.TaskTracker: Error cleaning up task runner: java.lang.NullPointerException
	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.cleanup(TaskTracker.java:2515)
	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:2381)
	at org.apache.hadoop.mapred.TaskTracker.close(TaskTracker.java:925)
	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1830)
	at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:2901)
{noformat}

corresponding code is 
{noformat}
2515            if (localJobConf.getNumTasksToExecutePerJvm() == 1) {
{noformat}
"
HADOOP-5233,Reducer not Succeded after 100%,"I have seen a reducer in RUNNING state, when it was waiting for the commitResponse.
Task logs show :
2009-02-12 08:35:36,414 INFO org.apache.hadoop.mapred.TaskRunner: Task:attempt_200902120746_0297_r_000033_0 is done. And is in the process of commiting
and no logs after that.
TT logs say:
2009-02-12 08:35:36,417 INFO org.apache.hadoop.mapred.TaskTracker: Task attempt_200902120746_0297_r_000033_0 is in COMMIT_PENDING
2009-02-12 08:35:36,417 INFO org.apache.hadoop.mapred.TaskTracker: attempt_200902120746_0297_r_000033_0 0.33333334% reduce > sort

This looks like, the task progress went from COMMIT_PENDING to RUNNING again."
HADOOP-5232,"preparing HadoopPatchQueueAdmin.sh,test-patch.sh scripts to run builds on hudson slaves.",To modify hadoopPatchQueueAdmin.sh and test-patch.sh script to run patch builds on hudson slaves.
HADOOP-5231,Negative number of maps in cluster summary,"I observed -ve number of maps in cluster summary, when running MRReliability test. (job with large number of failures)"
HADOOP-5229,duplicate variables in build.xml hadoop.version vs version let build fails at assert-hadoop-jar-exists,"<property name=""hadoop.jar"" location=""${build.dir}/hadoop-${hadoop.version}-core.jar"" /> where hadoop.version is defined in ${ivy.dir}/libraries.properties as hadoop.version=0.20.0. 
Though <jar jarfile=""${build.dir}/${final.name}-core.jar"" where <property name=""final.name"" value=""${name}-${version}""/>.
Means there is a hadoop-0.21.0-dev-core.jar builded though the assert-hadoop-jar-exists target looks for a  hadoop-0.2.0-core.jar.

"
HADOOP-5227,distcp -delete option deletes all files from the destination directory,distcp -delete option deletes all the files from the destination even though they are present on the source.
HADOOP-5226,Add license headers to html and jsp files,License headers are missing in some html and jsp files.
HADOOP-5225,workaround for tmp file handling on DataNodes in 0.19.1 (HADOOP-4663),"As discussed on core-dev@ (http://www.nabble.com/Hadoop-0.19.1-td21739202.html) we will reduce the semantics of sync in Hadoop 0.19.1.  This requires the same ""fix"" as HADOOP-4997."
HADOOP-5224,Disable append,"As discussed on core-dev@ (http://www.nabble.com/Hadoop-0.19.1-td21739202.html) we will disable append API in Hadoop 0.19.1 by throwing UnsupportedOperationException from FileSystem.append(...) methods.

Does append in libhdfs (HADOOP-4494) need attention too?"
HADOOP-5222,Add offset in client trace,"By adding offset in client trace, the client trace information can provide more accurately information about I/O.
It is useful for performance analyzing.

Since there is  no random write now, the offset of writing is always zero.
"
HADOOP-5219,SequenceFile is using mapred property,"SequenceFile is using ""mapred.local.dir"". It should not depend on mapred as it is part of the core."
HADOOP-5218,libhdfs unit test failed because it was unable to start namenode/datanode,The libhdfs unit tests fail because the start-daemon.sh script has changed. The unit tests fail to start/stop namenpde/datanode.
HADOOP-5217,"Split the AllTestDriver for core, hdfs and mapred",The sub projects would have individual test jar. This would require separate driver class for each.
HADOOP-5214,ConcurrentModificationException in FairScheduler.getTotalSlots,
HADOOP-5213,BZip2CompressionOutputStream NullPointerException,BZip2CompressionOutputStream will throw a NullPointerException if the user creates a BZip2CompressionOutputStream and close it without writing out any data.
HADOOP-5212,cygwin path translation not happening correctly after Hadoop-4868,
HADOOP-5211,TestSetupAndCleanupFailure fails with timeout,TestSetupAndCleanupFailure fails with timeout on my machine. The cause might be the while statements hanging the CPU at 100% for checking job completion. 
HADOOP-5209,Update year to 2009 for javadoc,The year is still 2008 in the generated javadoc.
HADOOP-5208,"SAXParseException: ""id"" must not contain the '<' character","Found the following exception in recent Hudson builds (e.g. see [build #3821|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3821/console]):
{noformat}
     [exec]   [javadoc] JDiff: reading the comments in from file
'/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/docs/jdiff/user_comments_for_hadoop_0.19.0_to_hadoop_742698_HADOOP-5205_PATCH-12399851.xml'...
     [exec]   [javadoc] Fatal Error (102): parsing XML comments file:org.xml.sax.SAXParseException: The value of attribute ""id"" must not contain the '<' character.
     [exec]   [javadoc] org.xml.sax.SAXParseException: The value of attribute ""id"" must not contain the '<' character.
{noformat}"
HADOOP-5207,Some core tests not executed by Hudson,"Since Hudson [build #3816|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3816/testReport/], there are 670 tests not executed by Hudson compared with [build #3815|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3815/testReport/]."
HADOOP-5206,"All ""unprotected*"" methods of FSDirectory should synchronize on the root.","Synchronization on {{rootDir}} is missing for two (relatively new) methods:
- {{unprotectedSetQuota()}}
- {{unprotectedSetTimes()}}"
HADOOP-5205,"Change CHUKWA_IDENT_STRING from ""demo"" to ""TODO-AGENTS-INSTANCE-NAME""",
HADOOP-5204,hudson trunk build failure due to autoheader failure in create-c++-configure-libhdfs task,"create-c++-configure-libhdfs:
     [exec] autoheader: warning: missing template: HADOOP_CONF_DIR
     [exec] autoheader: Use AC_DEFINE([HADOOP_CONF_DIR], [], [Description])
     [exec] autoreconf: /usr/bin/autoheader failed with exit status: 1

See output at: http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/746/"
HADOOP-5203,TT's version build is too restrictive,"
At start time, TT checks whether its version is compatible with JT.
The condition is too restrictive. 
It will shut down itself if one of the following conditions fail:
* the version numbers must match
* the revision numbers must match
* the user ids who build the jar must match
* the build times must match

I think it should check the major part of the version numbers only (thus any version like 0.19.xxxx should be compatible).
"
HADOOP-5200,NPE when the namenode comes up but the filesystem is set to file://,"If you bring up a namenode and the conf file points to file:/// as the URI, then the authority is null, breaking code that follows
"
HADOOP-5198,NPE in Shell.runCommand(),"I have seen one of the task failures with following exception:
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:441)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:149)
	at org.apache.hadoop.util.Shell.run(Shell.java:134)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:286)
	at org.apache.hadoop.util.ProcessTree.isAlive(ProcessTree.java:244)
	at org.apache.hadoop.util.ProcessTree.sigKillInCurrentThread(ProcessTree.java:67)
	at org.apache.hadoop.util.ProcessTree.sigKill(ProcessTree.java:115)
	at org.apache.hadoop.util.ProcessTree.destroyProcessGroup(ProcessTree.java:164)
	at org.apache.hadoop.util.ProcessTree.destroy(ProcessTree.java:180)
	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner.kill(JvmManager.java:377)
	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:249)
	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:113)
	at org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:76)
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:411)

"
HADOOP-5196,avoiding unnecessary byte[] allocation in SequenceFile.CompressedBytes and SequenceFile.UncompressedBytes,SequenceFile.CompressedBytes and SequenceFile.UncompressedBytes are used by the SequenceFile's raw bytes reading/writing API. The current implementation does not reuse the internal byte[] and causes unnecessary buffer allocation and initializaiton (zeroing the buffer).
HADOOP-5194,DiskErrorException in TaskTracker when running a job," In particular, this can be reproduced in Windows by running a hadoop example such as PiEstimator.
{noformat}
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/pids/attempt_200902021632_0001_m_000002_0 in any of the configured local directories
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:381)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)
	at org.apache.hadoop.mapred.TaskTracker.getPidFilePath(TaskTracker.java:430)
	at org.apache.hadoop.mapred.TaskTracker.removePidFile(TaskTracker.java:440)
	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner.runChild(JvmManager.java:370)
	at org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner.run(JvmManager.java:338)
{noformat}
(Have changed TaskTracker.java to print out the trace.)

This patch disables usage of setsid and pidfiles on Windows.
"
HADOOP-5193,SecondaryNameNode does not rollImage because of incorrect calculation of edits modification time.,"Secondary name-node cannot complete the second phase of the checkpoint because getFsEditsTime() returns the mod time of {{edits.new}} rather than {{edits}} file.
The difference is that {{edits}} remains unchanged during the whole checkpoint process an therefore can serve as an invariant. On the contrary {{edits.new}} is changing all the time since it is the target of the edits log during checkpoint. So comparison of the mod time of {{edits.new}} before and after checkpoint fail and name-node does not upload new image file from the secondary node and does not truncate edits files."
HADOOP-5192,Block reciever should not remove a finalized block when block replication fails,HADOOP-4702 makes block receivers to remove the received block in case of a block replication failure. But the block should not be removed if the cause of the failure is that the block to be received already exists. The key is that a block receiver should allow to remove only partial blocks in case of block replication failures. 
HADOOP-5191,"After creation and startup of the hadoop namenode on AIX or Solaris, you will only be allowed to connect to the namenode via hostname but not IP.","After creation and startup of the hadoop namenode on AIX or Solaris, you will only be allowed to connect to the namenode via hostname but not IP.

fs.default.name=hdfs://p520aix61.mydomain.com:9000
Hostname for box is p520aix and the IP is 10.120.16.68

If you use the following url, ""hdfs://10.120.16.68"", to connect to the namenode, the exception that appears below occurs. You can only connect successfully if ""hdfs://p520aix61.mydomain.com:9000"" is used. 

Exception in thread ""Thread-0"" java.lang.IllegalArgumentException: Wrong FS: hdfs://10.120.16.68:9000/testdata, expected: hdfs://p520aix61.mydomain.com:9000
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:320)
	at org.apache.hadoop.dfs.DistributedFileSystem.checkPath(DistributedFileSystem.java:84)
	at org.apache.hadoop.dfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:122)
	at org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:390)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:667)
	at TestHadoopHDFS.run(TestHadoopHDFS.java:116)"
HADOOP-5176,TestDFSIO reports itself as TestFDSIO,"When TestDFSIO starts up, it reports itself as ""TestFSDIO"", which would seem to be a typo. "
HADOOP-5175,Option to prohibit jars unpacking,"I've noticed that task tracker moves all unpacked jars into 
${hadoop.tmp.dir}/mapred/local/taskTracker.

We are using a lot of external libraries, that are deployed via ""-libjars"" 
option. The total number of files after unpacking is about 20 thousands.

After running a number of jobs, tasks start to be killed with timeout reason 
(""Task attempt_200901281518_0011_m_000173_2 failed to report status for 601 
seconds. Killing!""). All killed tasks are in ""initializing"" state. I've 
watched the tasktracker logs and found such messages:

{quote}
Thread 20926 (Thread-10368):
  State: BLOCKED
  Blocked count: 3611
  Waited count: 24
  Blocked on java.lang.ref.Reference$Lock@e48ed6
  Blocked by 20882 (Thread-10341)
  Stack:
    java.lang.StringCoding$StringEncoder.encode(StringCoding.java:232)
    java.lang.StringCoding.encode(StringCoding.java:272)
    java.lang.String.getBytes(String.java:947)
    java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
    java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:228)
    java.io.File.isDirectory(File.java:754)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:427)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
    org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:433)
{quote}

HADOOP-4780 patch brings the code which stores map of directories along 
with their DU's, thus reducing the number of calls to DU. However, the delete operation takes too long. I've manually deleted archive after 10 jobs had run and it took over 30 minutes on XFS.

I suppose that an option to prohibit jars unpacking would be helpfull in my situation."
HADOOP-5174,Pipes example throws NullPointerException on trunk when run with LocalJobRunner,"
Running the wordcount example (http://wiki.apache.org/hadoop/C++WordCount) fails with the following exception when run with the LocalJobRunner:

{code}
java.lang.NullPointerException
        at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:91)
        at org.apache.hadoop.mapred.pipes.PipesMapRunner.run(PipesMapRunner.java:68)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:344)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:295)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:140)
Exception in thread ""main"" java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1356)
        at org.apache.hadoop.mapred.pipes.Submitter.runJob(Submitter.java:244)
        at org.apache.hadoop.mapred.pipes.Submitter.run(Submitter.java:480)
        at org.apache.hadoop.mapred.pipes.Submitter.main(Submitter.java:494)
{code}"
HADOOP-5172,Chukwa : TestAgentConfig.testInitAdaptors_vs_Checkpoint regularly fails ,"
org.apache.hadoop.chukwa.datacollection.agent.TestAgentConfig.testInitAdaptors_vs_Checkpoint regularly fails in Hudson builds. I am not sure which branches it affects. I will attach one of the failure logs.
"
HADOOP-5170,"Set max map/reduce tasks on a per-job basis, either per-node or cluster-wide","There are a number of use cases for being able to do this.  The focus of this jira should be on finding what would be the simplest to implement that would satisfy the most use cases.

This could be implemented as either a per-node maximum or a cluster-wide maximum.  It seems that for most uses, the former is preferable however either would fulfill the requirements of this jira.

Some of the reasons for allowing this feature (mine and from others on list):
- I have some very large CPU-bound jobs.  I am forced to keep the max map/node limit at 2 or 3 (on a 4 core node) so that I do not starve the Datanode and Regionserver.  I have other jobs that are network latency bound and would like to be able to run high numbers of them concurrently on each node.  Though I can thread some jobs, there are some use cases that are difficult to thread (scanning from hbase) and there's significant complexity added to the job rather than letting hadoop handle the concurrency.
- Poor assignment of tasks to nodes creates some situations where you have multiple reducers on a single node but other nodes that received none.  A limit of 1 reducer per node for that job would prevent that from happening. (only works with per-node limit)
- Poor mans MR job virtualization.  Since we can limit a jobs resources, this gives much more control in allocating and dividing up resources of a large cluster.  (makes most sense w/ cluster-wide limit)"
HADOOP-5166,JobTracker fails to restart if recovery and ACLs are enabled,"JobTracker fails to restart and throw NullPointerException when mapred.jobtracker.restart.recover and mapred.acls.enabled are set to true

2009-02-04 12:28:19,834 FATAL org.apache.hadoop.mapred.JobTracker: java.lang.NullPointerException
        at org.apache.hadoop.mapred.QueueManager.hasAccess(QueueManager.java:185)
        at org.apache.hadoop.mapred.JobTracker.checkAccess(JobTracker.java:2694)
        at org.apache.hadoop.mapred.JobTracker.addJob(JobTracker.java:2663)
        at org.apache.hadoop.mapred.JobTracker.access$2300(JobTracker.java:86)
        at org.apache.hadoop.mapred.JobTracker$RecoveryManager.recover(JobTracker.java:1099)
        at org.apache.hadoop.mapred.JobTracker.offerS
"
HADOOP-5163,FSNamesystem#getRandomDatanode() should not use Replicator to choose a random datanode,"Below is the code:
  public DatanodeDescriptor getRandomDatanode() {
    return replicator.chooseTarget(1, null, null, 0)[0];
  }
Using Replicator to choose a random datanode is an overkill. It is very expensive and unnecessary. "
HADOOP-5161,Accepted sockets do not get placed in DataXceiverServer#childSockets,DAtaXceiver#childSockets is a map that keeps track of all open sockets that are accepted by DataXceiverServer but no socket does get added to this map.
HADOOP-5156,TestHeartbeatHandling uses MiniDFSCluster.getNamesystem() which does not exist in branch 0.20,"This breaks branch 0.20 build, which currently does not compile.
This will probably require promoting HADOOP-5017 to branch 0.20 or simply using {{cluster.getNameNode().getNamesystem()}} in this test."
HADOOP-5154,4-way deadlock in FairShare scheduler,This happened while trying to change the priority of a job from the scheduler servlet.
HADOOP-5151,"hdfs_quota_admin_guide.html 'q' in setQuota, etc not properly capitalized","This is trivial, but unfortunately significant. The quota admin guide has entries like, ""dfsadmin -setquota <N> <directory>...<directory>"" and ""dfsadmin -clrquota <directory>...<director>"" (oh hey that's missing a 'y').

The dfsadmin command is case-sensitive:
{quote}
$ hadoop dfsadmin -setquota 200 /user/marco
setquota: Unknown command
{quote}

but setQuota works:
{quote}
$ hadoop dfsadmin -setQuota 200 /user/marco
$ 
{quote}
"
HADOOP-5150,"""ant binary"" wastes time building documentations that are not needed in the packaging",
HADOOP-5149,HistoryViewer throws IndexOutOfBoundsException when there are files or directories not confrming to log file name convention,"When running history viewer in local mode (specifying file:///<path/to/hodlogs> as path to logs), it throws IndexOutOfBoundsException due to the following code:

{code}
      String[] jobDetails = 
          JobInfo.decodeJobHistoryFileName(jobFiles[0].getName()).split(""_"");
      trackerHostName = jobDetails[0];
      trackerStartTime = jobDetails[1];
{code}

The reason is because there are some directories under the log directories that do not conform to the log file naming convention, and the length of the jobDetails array is 1.

History viewer should be more defensive and ignore (possibly with warning) files or directories that it does not recognize."
HADOOP-5148,make watchdog disable-able,A scripting bug leaves watchdogging always on.  
HADOOP-5147,remove refs to slaves file,"The Chukwa bin scripts refer to conf/slaves.  This should be conf/chukwa-agents.
Further, stop-agents should not touch cron, since start-agents doesn't."
HADOOP-5146,LocalDirAllocator misses files on the local filesystem,"For some reason the LocalDirAllocator.getLocaPathToRead doesn't find files which are present, extra logging shows:

{noformat}
2009-01-30 06:43:32,312 INFO org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: in ifExists, /grid/2/arunc/mapred-local/taskTracker/archive/xxx.yyy.com/tera/in/_partition.lst exists
2009-01-30 06:43:32,389 WARN org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext: in getLocalPathToRead, taskTracker/archive/xxx.yyy.com/tera/in/_partition.lst doesn't exist
2009-01-30 06:43:32,390 WARN org.apache.hadoop.mapred.TaskRunner: attempt_200901300512_0007_m_000055_0 Child Error
 org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/archive/xx.yyy.com/tera/in/_partition.lst in any of the configured local directories
         at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:388)
         at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)
         at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:172)
{noformat}"
HADOOP-5145,Balancer sometimes runs out of memory after days or weeks running,"The culprit is a HashMap called MovedBlocks. By design this map does not get cleaned up between iterations. This is because the deletion of source replicas is done by NN. When next iteration starts, source replicas may not have been deleted, Balancer does not want to schedule them to move again. To prevent running out of memory, Balancer should expire/clean the movedBlocks from some iterations back.
"
HADOOP-5144,manual way of turning on restore of failed storage replicas for namenode,"when HADOOP-4885 is implemented and committed we will automatic facility to restore failed storage replicas for namenode. Currently it is controlled by configuration file.
But since it is quite a ""sensitive"" feature we might want to have a manual way of turning it on. (i.e. starting with the feature turned off, and turn it on when needed).

we can use hadoop dfsadmin script to do this.

"
HADOOP-5142,MapWritable#putAll does not store classes,"MapWritable's putAll method does not call addToMap for keys and values as MapWritable#put does. So new classes will not be stored
in class-id maps and will lead to problems during readFields."
HADOOP-5139,RPC call throws IllegalArgumentException complaining duplicate metrics registration,"Here is the error log:
    INFO  ipc.Server (Server.java:run(968)) - IPC Server handler 7 on 51017, call addBlock(/file7, DFSClient_-2132593831) from 127.0.0.1:51030: error: java.io.IOException: java.lang.IllegalArgumentException: Duplicate metricsName:addBlock
    java.io.IOException: java.lang.IllegalArgumentException: DuplicatemetricsName:addBlock
         at org.apache.hadoop.metrics.util.MetricsRegistry.add(MetricsRegistry.java:56)
         at org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.<init>(MetricsTimeVaryingRate.java:89)
         at org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.<init>(MetricsTimeVaryingRate.java:99)
         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:522)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:396)
         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)"
HADOOP-5138,Current Chukwa Trunk failed contrib unit tests.,"junit.framework.AssertionFailedError: org.apache.hadoop.chukwa.datacollection.agent.ChukwaAgent$AlreadyRunningException: Agent already running; aborting
	at org.apache.hadoop.chukwa.datacollection.agent.TestAgentConfig.testInitAdaptors_vs_Checkpoint(TestAgentConfig.java:73)

Test org.apache.hadoop.chukwa.datacollection.adaptor.filetailer.TestStartAtOffset FAILED (timeout)"
HADOOP-5135,"Separate the core, hdfs and mapred junit tests","To support splitting of projects, the tests should be separated into different directories."
HADOOP-5134,FSNamesystem#commitBlockSynchronization adds under-construction block locations to blocksMap,"From my understanding of sync/append design, an under construction block should not have any block locations associated with it in the blocksMap. So an under construction block will not be managed by ReplicationMonitor.

However, if there is an error in the write pipeline, a lease recovery will trigger a call, commitBlockSynchronization, to NN. This call will add the successfully-recovered datanodes to blocksMap. This seems to violate the design. It should update the targets of the last block at INode instead. "
HADOOP-5130,TaskTracker seems to hold onto the assigned task for a long while before launching it,"I saw atleast a couple of instances where the task assigned to the TaskTracker is launched several minutes after the receipt of the LaunchTaskAction:

{noformat}
2009-01-27 13:55:53,402 INFO org.apache.hadoop.mapred.TaskTracker: LaunchTaskAction (registerTask): attempt_200901270818_0006_m_000602_0
2009-01-27 13:55:55,129 INFO org.apache.hadoop.mapred.TaskTracker: Trying to launch : attempt_200901270818_0006_m_000602_0
2009-01-27 13:55:55,129 INFO org.apache.hadoop.mapred.TaskTracker: In TaskLauncher, current free slots : 2 and trying to launch attempt_200901270818_0006_m_000602_0
2009-01-27 14:04:17,744 INFO org.apache.hadoop.mapred.TaskTracker: JVM with ID: jvm_200901270818_0006_m_176495965 given task: attempt_200901270818_0006_m_000602_0
2009-01-27 14:04:24,020 INFO org.apache.hadoop.mapred.TaskTracker: attempt_200901270818_0006_m_000602_0 1.0% 
2009-01-27 14:04:27,023 INFO org.apache.hadoop.mapred.TaskTracker: attempt_200901270818_0006_m_000602_0 1.0% 
2009-01-27 14:04:30,026 INFO org.apache.hadoop.mapred.TaskTracker: attempt_200901270818_0006_m_000602_0 1.0% 
2009-01-27 14:04:30,362 INFO org.apache.hadoop.mapred.TaskTracker: attempt_200901270818_0006_m_000602_0 1.0% 
2009-01-27 14:04:30,362 INFO org.apache.hadoop.mapred.TaskTracker: Task attempt_200901270818_0006_m_000602_0 is done.
2009-01-27 14:04:30,362 INFO org.apache.hadoop.mapred.TaskTracker: reported output size for attempt_200901270818_0006_m_000602_0  was 0
{noformat}
"
HADOOP-5127,FSDirectory should not have public methods.,{{FSDirectory}} class contains public constructors and methods. All of them except for one {{close()}} can be converted into package private.
HADOOP-5126,Empty file BlocksWithLocations.java should be removed,File org.apache.hadoop.hdfs.protocol.BlocksWithLocations.java is empty and should be removed.
HADOOP-5124,A few optimizations to FsNamesystem#RecentInvalidateSets,"This jira proposes a few optimization to FsNamesystem#RecentInvalidateSets:
1. when removing all replicas of a block, it does not traverse all nodes in the map. Instead it traverse only the nodes that the block is located.
2. When dispatching blocks to datanodes in ReplicationMonitor. It randomly chooses a predefined number of datanodes and dispatches blocks to those datanodes. This strategy provides fairness to all datanodes. The current strategy always starts from the first datanode."
HADOOP-5122,libhdfs test conf uses deprecated fs.default.name value,"src/c++/libhdfs/tests/conf/core-site.xml contains a deprecated layout for fs.default.name:
{noformat}
  <name>fs.default.name</name>
  <value>localhost:23000</value>
{noformat}"
HADOOP-5120,UpgradeManagerNamenode and UpgradeObjectNamenode should not use FSNamesystem.getFSNamesystem(),UpgradeManagerNamenode and UpgradeObjectNamenode should not access the namespace by the static method FSNamesystem.getFSNamesystem()
HADOOP-5119,FSEditLog should not use FSNamesystem.getFSNamesystem(),FSEditLog should not access the namespace by the static method FSNamesystem getFSNamesystem().
HADOOP-5113,"logcondense should delete hod logs for a user , whose username has any of the characters in the value passed to ""-l"" options ","Logcondense script is not able to delete the completed job directories of hadoop logs in hod-logs inside HDFS  for the the users , whose username has any of the characters , in the value passed to ""-l"" options  or in '/user' as set default . This happened because logcondense script use python 'lstrip' method , which returns copy of the string after removing leading characters in the value passed to ""-l"" options or in ""/user"" instead of just stripping value from the given string ."
HADOOP-5111,Generic mapreduce classes cannot be used with Job::set* methods,"The set\* methods on a Job take {{Class}} instances whose parameterized type is an unbounded type (e.g. {{Class<? extends RawComparator<?>>}}). Attempting to pass a {{Class}} whose parameterized types are not explicit will cause compile-time errors, as in HADOOP-5065."
HADOOP-5107,"split the core, hdfs, and mapred jars from each other and publish them independently to the Maven repository","I think to support splitting the projects, we should publish the jars for 0.20.0 as independent jars to the Maven repository "
HADOOP-5106,hdfs-default.xml option names are not consistent with DataNode ,Some of the options that are named in hdfs-default.xml do not match the options which are read in DataNode. One of the two files needs to be changed to be consistent -ideally whichever breaks the least installations.
HADOOP-5103,"Too many logs saying ""Adding new node"" on JobClient console","JobClient's console has logs saying ""Adding a new node <rackname>/<node-ip-addr>:<port>"" for all the hosts, where each split resides.
For jobs with more #maps, these logs just fill up client's space.

This is introduced by HADOOP-3293"
HADOOP-5101,optimizing build.xml target dependencies,"Need to optimize build.xml

For ex: findbugs target depends on package target and package target depends on doc, jar, cn-docs , etc...
Though findbugs is run on three of the jar files for which we have three different targets, jar, tools-jar , examples

Likewise different targets could be optimized. 

Thanks,
Giri"
HADOOP-5100,Chukwa Log4JMetricsContext class should append new log to current log file,Log4JMetricsContext is setting is own appender dynamically and by doing so it's truncating the current log file. It should append to it if the file exist. 
HADOOP-5097,Remove static variable JspHelper.fsn,"There is another static FSNamesystem variable, fsn, declared in JspHelper.  We should remove it."
HADOOP-5095,chukwa watchdog does not monitor the system correctly,"watchdog depends on environment variables to locate pid files correctly.  Due to the recent path enhancement for build.xml, watchdog needs to be updated with the correct CHUKWA_LOG_DIR and CHUKWA_CONF_DIR at build time.

"
HADOOP-5094,Show dead nodes information in dfsadmin -report,"As part of operations responsibility to bring back dead nodes, it will be good to have a quick way to obtain a list of dead data nodes.  
The current way is to scrape the namenode web UI page and parse that information, but this creates load on the namenode.   
In search of a less costly way, I noticed dfsadmin -report only reports data nodes with State: ""In Service"" and ""Decommission in progress"" get listed.
Asking for a cheap way to obtain a list of dead nodes.  

In addition, can the following requests be reviewed for additional enhancement and changes to dfsadmin -report.

- Consistent formatting output in ""Remaining raw bytes:"" for the data nodes should have a space between the exact value and the parenthesized value.
Sample:
Total raw bytes: 3842232975360 (3.49 TB)
Remaining raw bytes: 146090593065(136.06 GB)
Used raw bytes: 3240864964620 (2.95 TB)

- Include the running version of Hadoop.  

- What is the meaning of ""Total effective bytes""?

- Display the hostname instead of the IP address for the data node (toggle option?)
"
HADOOP-5093,Configuration default resource handling needs to be able to remove default resources ,"There's a way to add default resources, but not remove them. This allows someone to push an invalid resource into the default list, and for the rest of the JVM's life, any Conf file loaded with quietMode set will fail."
HADOOP-5092,Branch 0.18 doesn't display version info,"""hadoop version"" doesn't display the version of hadoop for branch 0.18 rather it shows version unknown. Same behaviour is observed in svn as well as in git."
HADOOP-5088,include releaseaudit as part of  test-patch.sh script ,"Existing test-patch.sh script doesn't seem to execute releaseaudit target as part of patch testing.
We need to call releaseaudit target from test-patch.sh script

Thanks,
Giri"
HADOOP-5086,Trash URI semantics can be relaxed,"When using fully qualified URIs with FsShell, the authority element of the URI must match the default filesystem exactly, or else one may get an error message when the trash is enabled:
{noformat}
$ hadoop fs -rmr hdfs://namenode1/user/foo/bar
rmr: Wrong FS: hdfs://namenode1/user/foo/bar, expected: hdfs://namenode1.foobar.com
Usage: java FsShell [-rmr <path>]
$ hadoop fs -rmr hdfs://namenode1.foobar.com/user/foo/bar
$
{noformat}

It should be possible to use the FileSystem for the Path provided rather than the default FileSystem. 0.17 was less particular about this."
HADOOP-5085,Copying a file to local with Crc throws an exception,"$ hadoop dfs -get -crc /user/aa/test.txt test.txt
get: org.apache.hadoop.dfs.DistributedFileSystem cannot be cast to
org.apache.hadoop.fs.ChecksumFileSystem

The problem seems to be caused by the line 251 in FsShell#copyToLocal:
{noformat}
250;      if (copyCrc) {
251:        ChecksumFileSystem csfs = (ChecksumFileSystem) srcFS;
               ...
             }
{noformat}

Copying crc files to local should not require the source file system to be ChecksumFileSystem. 
"
HADOOP-5081,"Split TestCLI into HDFS, Mapred and Core tests","At present, TestCLI contains command line tests for both hdfs and mapred. Going forward, this test has to be broken up into separate hdfs, mapred and core tests."
HADOOP-5080,Update TestCLI with additional test cases.,"Currently TestCLI contains few of the dfs commands and verifies some of the error messages for quota and refreshServiceAcl.. Here is a proposal to add additional test cases to TestCLI to cover an exhaustive list of Hadoop commands. Here is a list of action items for the same:
1) Complete the test cases for dfs commands which are not yet automated such as count, chmod, chown, chgrp etc
2) Verify help messages in fs, dfsadmin, mradmin
3) Add other Hadoop commands such as archives, dfsadmin, balancer, job, queue, version, jar, distcp, daemonlog etc to the command line test.
"
HADOOP-5079, HashFunction inadvertently destroys some randomness,"HashFunction.hash restricts initval for the next hash to the [0, maxValue) range of the hash indexes returned. This is suboptimal, particularly for larger nbHash and smaller maxValue.  Rather we should first set initval, then restrict the range for the result assignment."
HADOOP-5078,Broken AMI/AKI for ec2 on hadoop,"c1.xlarge and m1.large instances fail to boot. 

ec2-get-console-output show them stuck at ""Creating /dev"" step."
HADOOP-5077,JavaDoc errors in 0.18.3,"There are JavaDoc errors in 0.18.3. These are not present in 0.19 and above thus went undetected by Hudson and others.
"
HADOOP-5076,chukwa metrics file get overwritten when process launch,"In Log4jMetricsContext, the log4j appender always rewrite the file instead of append to the log file.  This should be
changed to append to ensure the metrics log file is streamed correctly."
HADOOP-5075,Potential infinite loop in updateMinSlots,"We ran into a problem at Facebook where the updateMinSlots loop in the scheduler was repeating infinitely. This might happen if, due to rounding, we are unable to assign the last few slots in a pool. This patch adds a break statement to ensure that the loop exists if it hasn't managed to assign any slots."
HADOOP-5073,Hadoop 1.0 Interface Classification - scope (visibility - public/private) and stability,"This jira proposes an interface classification for hadoop interfaces.
The discussion was started in email alias core-dev@hadoop.apache.org in Nov 2008.

"
HADOOP-5072,testSequenceFileGzipCodec won't pass without native gzip codec,"Somehow, SequenceFile requires native gzip codec. We should remove it from the test cases since that may not pass on all platforms.
"
HADOOP-5070,Update the year for the copyright to 2009,The year should be updated to 2009 before any new release comes out.
HADOOP-5068,testClusterBlockingForLackOfMemory in TestCapacityScheduler fails randomly,testClusterBlockingForLackOfMemory fails randomly when TestCapacityScheduler is run.
HADOOP-5067,Failed/Killed attempts column in jobdetails.jsp does not show the number of failed/killed attempts correctly,"I see one of the task failures when i see it from the taskdetails.jsp page, but  failed/killed attempts column show it as zero."
HADOOP-5066,ant binary should not compile docs,"ant binary now compiles docs. The compilation of binary itself takes around 6 minutes. Since the tar ball does not include docs, they need not be compiled.

The size of binary is 17MB on my system. I could see duplicate library copies in the tar contents.

For example :
-rw-rw-r-- /             26202 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/commons-logging-api-1.0.4.jar
-rw-rw-r-- /           2532573 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/hadoop-0.21.0-dev-core.jar
-rw-rw-r-- /             69850 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/hadoop-0.21.0-dev-tools.jar
-rw-rw-r-- /            516429 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/jetty-6.1.14.jar
-rw-rw-r-- /            121070 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/junit-3.8.1.jar
-rw-rw-r-- /            391834 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/log4j-1.2.15.jar
-rw-rw-r-- /             15345 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/slf4j-api-1.4.3.jar
-rw-rw-r-- /             15010 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/xmlenc-0.52.jar

----------------------------------------------

-rw-rw-r-- /           2532573 2009-01-16 11:53:51 hadoop-0.21.0-dev/hadoop-0.21.0-dev-core.jar
-rw-rw-r-- /             69850 2009-01-16 11:53:51 hadoop-0.21.0-dev/hadoop-0.21.0-dev-tools.jar
-rw-rw-r-- /            516429 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/jetty-6.1.14.jar
-rw-rw-r-- /             26202 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/commons-logging-api-1.0.4.jar
-rw-rw-r-- /            121070 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/junit-3.8.1.jar
-rw-rw-r-- /            391834 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/log4j-1.2.15.jar
-rw-rw-r-- /             15345 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/slf4j-api-1.4.3.jar
-rw-rw-r-- /             15010 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/xmlenc-0.52.jar
"
HADOOP-5065,setOutputFormatClass in mapreduce.Job fails for SequenceFileOutputFormat,"{noformat}
    [javac] /.../snip.java:201: setOutputFormatClass(java.lang.Class<? extends org.apache.hadoop.mapreduce.OutputFormat<?,?>>) in \
        org.apache.hadoop.mapreduce.Job cannot be applied to (java.lang.Class<org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat>)
    [javac]     job.setOutputFormatClass(SequenceFileOutputFormat.class);
    [javac]        ^
{noformat}

The signatures for get\{Input,Output\}FormatClass are probably too restrictive:
{code}
  public void setOutputFormatClass(Class<? extends OutputFormat<?,?>> cls) throws IllegalStateException
{code}"
HADOOP-5056,chukwa init.d script can't run over pdsh ,"On Redhat 5.1, the sudo script does not allow commands to be executed as another user over ssh.  This means chukwa
init.d script can not be executed via pdsh.  The error message was: sudo: sorry, you must have a tty to run sudo"
HADOOP-5053,Collector does not shutdown properly,"Using ""ps ax"" to determine process status is more reliable than jps, and chukwa has changed to use ps ax as part of shutdown.  However, the shell script does not handle the case where there are blank space in front of pid number, though running ""jettyCollector.sh stop"" does not shutdown the collector process."
HADOOP-5052,Add an example for computing exact digits of Pi,It would be useful to add an example showing how to use Hadoop to do scientific computing.  We should add an example for computing exact digits of Pi.
HADOOP-5050,TestDFSShell fails intermittently,"TestDFSShell.testFilePermissions fails intermittently with following assertion failure :

Testcase: testFilePermissions took 0.299 sec
	FAILED
expected:<...-...> but was:<...w...>
junit.framework.ComparisonFailure: expected:<...-...> but was:<...w...>
	at org.apache.hadoop.hdfs.TestDFSShell.testChmod(TestDFSShell.java:781)
	at org.apache.hadoop.hdfs.TestDFSShell.testFilePermissions(TestDFSShell.java:832)
"
HADOOP-5048,Sometimes job is still displayed in jobqueue_details page for long time after job was killed.,"When I tried kill all running job, I noticed that were two jobs were listed on jobqueue_details.jsp page page as well as they were also listed under failed job on jobtracker.jsp page.
When I checked status of each that was displayed ""killed"" and Cleanup task status as ""Successful"", but both jobs were also being on jobqueue_details.jsp page for longtime e.g up to 10 -15 mins after I restarted JobTracker.

Before killing the jobs, status of both jobs was running and no task of from them was scheduled.
I noticed this behavior on 3 different occasions. But is this random, not always reproducible."
HADOOP-5045,FileSystem.isDirectory() should not be deprecated.,We should remove FileSystem.isDirectory().
HADOOP-5042, Add expiration handling to the chukwa log4j appender,"Chukwa log4j appender is not doing any sort of cleanup. 
The idea here is to keep only n rotate files and delete the older ones. 
This way we don't have to worry about manually cleaning old files"
HADOOP-5039,Hourly&daily rolling are not using the right path,"The path for Rolling is build like this: chukwaMainRepository + ""/"" + cluster + ""/"" + dataSource + ""/"" + workingDay + ""/"" + workingHour + ""/*/*.evt"";
If there's a spill file in ""/"" + cluster + ""/"" + dataSource + ""/"" + workingDay + ""/"" + workingHour it will be part of the rolling but it shouldn't. Only data from subDirectories should be part of the rolling.
"
HADOOP-5038,remove System.out.println statement,Starting the agent using the chuka-daemon.sh script redirect the standard output to a file. Only critical information should be outputted to stdout
HADOOP-5037,Deprecate FSNamesystem.getFSNamesystem() and change fsNamesystemObject to private,HADOOP-2413 involves quite many codes.  This is the first step to fix it.
HADOOP-5036,chukwa agent controller remove file does not work ,"Test case: 
Start the agent
Add a fileTailing adaptor to a file & validate that this adaptor is written to the checkpoint
remove the adaptor using the shutdown command on port 9093 (default)
wait for the next checkpoint file to be written
The adaptor should no longer be there but it is"
HADOOP-5035,Support non Time Series charting and handle data gap more gracefully for Chukwa charts,"Chukwa charting only support time series data because the data structure is a HashMap of Long, Double.
Long is the timestamp (xaxis), and Double is the yaxis value.  The data structure will be changed to HashMap of String, Double.
This will add capability to plot xaxis with any data.

If the yaxis value is not a number, the charting system should skip plotting for this data point."
HADOOP-5034,NameNode should send both replication and deletion requests to DataNode in one reply to a heartbeat,"Currently NameNode favors block replication requests over deletion requests. On reply to a heartbeat, NameNode does not send a block deletion request unless there is no block replication request. 

This brings a problem when a near-full cluster loses a bunch of DataNodes. In react to the DataNode loss, NameNode starts to replicate blocks. However, replication takes a lot of cpu and a lot of replications fail because of the lack of disk space. So the administrator tries to delete some DFS files to free up space. However, block deletion requests get delayed for very long time because it takes a long time to drain the block replication requests for most DataNodes.

I'd like to propose to let NameNode to send both replication requests and deletion requests to DataNodes in one reply to a heartbeat. This also implies that the replication monitor should schedule both replication and deletion work in one iteration."
HADOOP-5033,chukwa writer API is confusing,The ChukwaWriter interface has both add(Chunk) and add(List<Chunk>).   The code doesn't actually use the former.  I'd like to remove it.  Thoughts?
HADOOP-5032,CHUKWA_CONF_DIR environment variable needs to be exported to shell script,"CHUKWA_CONF_DIR is used by chukwa command line scripts, like agent.sh, jettyCollector.sh  When the command line scripts are called through chukwa-daemon.sh wrapper, the CHUKWA_CONF_DIR environment is not passed to the command line script.  This variable should be exported.
"
HADOOP-5031,metrics aggregation is incorrect in database,"A few problem with the aggregation SQL statements:

hdfs throughput should be calculated by doing two level aggregation:

First, calculate the rate for hadoop datanode metrics with accumulated vales.
Second, sum up all datanode rate to provide a single number to represent the current cluster performance.

Disable hod jobs utilization measurement - The data provide a rough view of the cluster performance but mostly inaccurate.
Disable user utilization measurement generated from hod job - The data is generated from hod job metrics, and it's mostly inaccurate."
HADOOP-5030,Chukwa RPM build improvements,"When user defines location of Chukwa RPM as /usr/local/chukwa, the RPM should install into the defined directory.
The rpm build currently creates /usr/local/chukwa/chukwa.  This should be changed."
HADOOP-5026,Startup scripts should be svn-executable,"When you check out Chukwa, not all of the shell scripts in bin that need to be executable are so. In particular, I believe that all files ending in .sh that exist in bin should be executable. This will simply require svn:executable properties to be set."
HADOOP-5024,DFS chmod does not correctly parse some multiple-mode permission specifications,"The current implementation of chmod attempts to combine multiple permission specifications into one omnibus specification that represents the end result of all the specifications.  However, this fails for some specifications, essentially allowing the latest specified mode to override the prior ones.  For example the following chmod in unix results in:
{noformat}
     [auser@machine test]$ mkdir foo   
     [auser@machine test]$ chmod 000 foo
     [auser@machine test]$ ls -l foo 
     d---------  2 auser users 4.0K Jan 10 02:09 foo
     [auser@machine test]$ chmod u+rw,+x foo
     [auser@machine test]$ ls -l foo
     drwx--x--x  2 auser users 4.0K Jan 10 02:09 foo
{noformat}
while the current implementation results in: (adapted from TestDFSShell.java)
{noformat}
     runCmd(shell, ""-chmod"", ""000"", file.toString());
     assertEquals(""---------"", fs.getFileStatus(file).getPermission().toString());
     
     runCmd(shell, ""-chmod"", ""u+rw,+x"", file.toString());
     assertEquals(""drwx--x--x"", fs.getFileStatus(file).getPermission().toString());
     // fails, with the actual result of
     // result = --x--x--x
{noformat}
Unix appears to apply each specified mode sequentially, and this approach would correct the problem in DFSShell as well at the cost of a separate rpc call for each mode specification.  


"
HADOOP-5023,Add Tomcat support to hdfsproxy,We plan to add Tomcat support to hdfsproxy since Tomcat has good production support at Yahoo.
HADOOP-5022,"[HOD] logcondense should delete all hod logs for a user, including jobtracker logs","Currently, logcondense.py does not delete jobtracker logs that it uploads to the DFS when the HOD cluster is deallocated. This will result in the hod-logs directory to slowly accumulate a whole bunch of jobtracker logs. Particularly for users who run a lot of user jobs, this could fill up the namespace.  Further these directories will cause the logcondense program to keep repeatedly looking at these directories stressing out the namenode. So, logcondense.py should optionally also delete the jobtracker logs."
HADOOP-5018,Chukwa should support pipelined writers,We ought to support chaining together writers; this will radically increase flexibility and make it practical to add new features without major surgery by putting them in pass-through or filter classes.
HADOOP-5017,NameNode.namesystem should be private,"As stated in the comment:
{code}
  public FSNamesystem namesystem; // TODO: This should private. Use getNamesystem() instead. 
{code}"
HADOOP-5015,Separate block/replica management code from FSNamesystem,"Currently FSNamesystem contains a big amount of code that manages blocks and replicas. The code scatters in FSNamesystem and it is hard to read and maintain. It would be nice to move the code to a separate class called, for example, BlockManager. "
HADOOP-5012,addStoredBlock should take into account corrupted blocks when determining excesses,"I found another source of corruption on our cluster.

0) Three replicas of a block exist
1) One is recognized as corrupt (3 reps total)
2) Namenode decides to create a new replica.  Replication done and addStoredBlock is called (4 reps total)
3) There are too many replicas, so processOverReplicatedBlock is called by addStoredBlock.
4) processOverReplicatedBlock is called, and it decides do invalidate the newly created replica.  [Oddly enough, it decides to invalidate the newly created one instead of the one in the corrupted replicas map!]
5) We are in the same state as (1) -- 3 replicas total, 1 of which is still bad.

I believe we can fix this easily -- change numCurrentReplica variable to take into account the number of corrupt replicas."
HADOOP-5011,Scanner setup takes too long...,posix4? and dr_ryan are trying to figure why setup of a scanner takes so long.  Use case is fetch of a hundred or a thousand or so rows at a time.
HADOOP-5009,DataNode#shutdown sometimes leaves data block scanner verification log unclosed,"When datanode gets shutdown by calling DataNode#shutdown, it occasionally leaves the data block scanner verification log unclosed. There are two possible causes:
1. DataNode does not wait until block scanner thread to exit.
2. DataBlockScanner does not guarantee that the verification log is closed if the scanner is interrupted. "
HADOOP-5008,TestReplication#testPendingReplicationRetry leaves an opened fd unclosed,The unit test opens a block file to overwrite but does not close it; So subsequent test would fail because the data directory is not able to be removed.
HADOOP-5007,can't hard-stop Chukwa adaptors,"There are two ways to stop a Chukwa adaptor -- gracefully, via Adaptor.shutdown(), and abruptly, via Adaptor.hardStop(). Agent.stopAdaptor() should call hardStop() when the user passes in the appropriate flag, instead of just leaving the adaptor running.

This got broken by HADOOP-4709.  I assume this was inadvertent; if not, could someone explain the rationale for this change?"
HADOOP-5003,"When computing absoluet guaranteed capacity (GC) from a percent value, Capacity Scheduler should round up floats, rather than truncate them.","The Capacity Scheduler calculates a queue's absolute GC value by getting its percent of the total cluster capacity (which is a float, since the configured GC% is a float) and casting it to an int. Casting a float to an int always rounds down. For very small clusters, this can result in the GC of a queue being one lower than what it should be. For example, if Q1 has a GC of 50%, Q2 has a GC of 40%, and Q3 has a GC of 10%, and if the cluster capacity is 4 (as we have, in our test cases), Q1's GC works out to 2, Q2's to 1, and Q3's to 0 with today's code. Q2's capacity should really be 2, as 40% of 4, rounded up, should be 2. 
Simple fix is to use Math.round() rather than cast to an int. "
HADOOP-5002,2 core tests TestFileOutputFormat and TestHarFileSystem are failing in branch 19,"I see 2 of the core tests(TestHarFileSystem and TestFileOutputFormat) failing in branch 19. These are passing in branch 20 and in trunk.
I see nullpointer exceptions for the attempts of reducde tasks in the logs(for example, see the following):

2009-01-09 13:53:34,112 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(484)) - Error from attempt_200901091353_0001_r_000000_0: java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:61)
	at org.apache.hadoop.fs.Path.<init>(Path.java:50)
	at org.apache.hadoop.tools.HadoopArchives$HArchivesReducer.configure(HadoopArchives.java:552)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:58)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:83)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:337)
	at org.apache.hadoop.mapred.Child.main(Child.java:155)"
HADOOP-5001,Junit tests that time out don't write any test progress related logs,"Some junit tests time out frequently possibly because of a bug. When such tests time out, the log4j appender isn't writing anything to the log files. It seems that all the log statements  are buffered in the memory till test completion. The logs get written to the log file only after the test goes to completion.

This is seriously limiting debugging in presence of a test time out. *If* possible, we should try to flush logs regularly so that we can find out the extent to which a test has progressed before timing out."
HADOOP-4999,IndexOutOfBoundsException in FSEditLog,"when we go over a collection of editStreams in FSEditLog::logEdit we pre-calculate number of iterations for the ""for loop"":
int numEditStreams = editStreams.size();
for (int idx = 0; idx < numEditStreams(); idx++) {
...
processIOError(idx);
...
}

but there is a possibility of an IOError that will call processIOError(idx) which will remove an editStream from editStreams inside the loop, and that will cause IndexOutOfBoundsException when end of collection is reached.

proposed fix: recalculate size of the collection on every iteration (it is very cheap, cause it just returns an integer)."
HADOOP-4997,workaround for tmp file handling on DataNodes in 0.18 (HADOOP-4663),"
This is a temporary work around issues discussed in HADOOP-4663. 

The proposal is to remove all the files under tmp directory, thus bringing the behavior back to 0.17. The main cost is that sync() will not be supported. This is incompatible with 0.18.x, but not with 0.17 because of this reason.

"
HADOOP-4996,JobControl does not report killed jobs,"After speaking with Arun and Owen, my understanding of the situation is that separate killed job tracking was added in hadoop 18: http://issues.apache.org/jira/browse/HADOOP-3924.

However, it does not look like this change was integrated into JobControl class. While I have not verified this yet, it looks like, applications that use JobControl would no way of knowing if one of the jobs was killed.

This would be blocker for Pig to move to Hadoop 19."
HADOOP-4993,chukwa agent startup should be more modular,"Right now, the Chukwa Agent configuration and startup code is complex and tangled. "
HADOOP-4992,TestCustomOutputCommitter fails on hadoop-0.19,"Test compilation fails on 0.19 with the following error
{noformat}
compile-core-test:
    [javac] Compiling 7 source files to /path/to/classes
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 295 source files to /path/to/classes
    [javac] /path/to/test/TestCustomOutputCommitter.java:57: cannot find symbol
    [javac] symbol  : class WordCount
    [javac] location: class org.apache.hadoop.mapred.TestCustomOutputCommitter
    [javac]       int ret = ToolRunner.run(jobConf, new WordCount(), args);
    [javac]                                             ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 1 error

{noformat}"
HADOOP-4988,"An earlier fix, for HADOOP-4373, results in a problem with reclaiming capacity when one or more queues have a capacity equal to zero","HADOOP-4373 introduced a fix for queues with guaranteed capacity (gc) equal to zero. Part of the fix was in the queue comparator used to sort queues. Queues with gc=0 were placed at the end. This causes a problem with the code for reclaiming capacity, which assumes that queues are sorted based on free space available and that a queue with gc=0 is no different than a queue which is running at capacity. Because of this, the following problem can arise: if we have a system with at least one queue whose gc=0, we may fail to reclaim capacity for some queues. "
HADOOP-4987,One more illegal map task been launched when the job almost complete,"When the job nearly complete, the JobTracker launched one more  map task and quickly complete.
The Last map task is useless and illegal actually. In the following examples, the job should have 101 map tasks and the taskID begin from ""attempt_200901071109_0001_m_000000"". So the TaskID ""attempt_200901071109_0001_m_000101"" is out of range.



2009-01-07 11:16:04,435 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'attempt_200901071109_0001_m_000101_0' to tip task_200901071109_0001_m_000101, for tracker 'tracker_ict.vega2010.org:127.0.0.1/127.0.0.1:40668'
2009-01-07 11:16:05,230 INFO org.apache.hadoop.mapred.JobInProgress: Task 'attempt_200901071109_0001_m_000101_0' has completed task_200901071109_0001_m_000101 successfully.
2009-01-07 11:16:05,231 INFO org.apache.hadoop.mapred.JobInProgress: Job job_200901071109_0001 has completed successfully.
"
HADOOP-4986,FSNamesystem.getBlockLocations sets access time without holding the namespace locks,"After the access time feature is added to HDFS, FSNamesystem.getBlockLocations(..) is no longer a read only namespace operation.  It changes the namespace by updating the access time.  However, the thread does not own namespace locks.  This may lead to namespace inconsistency."
HADOOP-4985,IOException is abused in FSDirectory,"In FSDirectory, quite a few methods are unnecessary declared with ""throws IOException"".  In some cases, it can just be removed without causing any compilation problem.  In some other cases, it can be replaced with a specific subclass like QuotaExceededException."
HADOOP-4983,Job counters sometimes go down as tasks run without task failures,"As tasks run, the counters seem to back up and move forward again. They always seem to be right when the task completes. I suspect this may have been introduced in HADOOP-2208."
HADOOP-4982,TestFsck does not run in Eclipse.,"{{TestFsck.testFsckMove()}} falls into infinite loop if run from Eclipse because it uses incorrect default settings for {{""test.build.data""}} configuration variable."
HADOOP-4980,Cleanup the Capacity Scheduler code,"Given the number of changes that have been made by different folks to the Capacity Scheduler code, the code needs to be cleaned up. Some comments and variable names are misleading, and the core logic is not in a central place, making it harder to understand. "
HADOOP-4979,Capacity Scheduler does not always return no task to a TT if a job's memry requirements are not met,"As per HADOOP-4035, the Capacity Scheduler should return no task to a TT if a job's high mem requirements are not met. This doesn't always happen. In the Scheduler's assignTasks() method, if a job's map task does not enough memory to run, the Scheduler looks at reduce tasks, and vice-versa. This can result in a case where a reduce task from another job is returned to the TT (if the high-mem job does not have a reduce task to run, for example), thus starving the high-mem job. "
HADOOP-4977,Deadlock between reclaimCapacity and assignTasks,"I was running the latest trunk with the capacity scheduler and saw the JobTracker lock up with the following deadlock reported in jstack:

Found one Java-level deadlock:
=============================
""18107298@qtp0-4"":
  waiting to lock monitor 0x08085b40 (object 0x56605100, a org.apache.hadoop.mapred.JobTracker),
  which is held by ""IPC Server handler 4 on 54311""
""IPC Server handler 4 on 54311"":
  waiting to lock monitor 0x0808594c (object 0x5660e518, a org.apache.hadoop.mapred.CapacityTaskScheduler$MapSchedulingMgr),
  which is held by ""reclaimCapacity""
""reclaimCapacity"":
  waiting to lock monitor 0x08085b40 (object 0x56605100, a org.apache.hadoop.mapred.JobTracker),
  which is held by ""IPC Server handler 4 on 54311""

Java stack information for the threads listed above:
===================================================
""18107298@qtp0-4"":
	at org.apache.hadoop.mapred.JobTracker.getClusterStatus(JobTracker.java:2695)
	- waiting to lock <0x56605100> (a org.apache.hadoop.mapred.JobTracker)
	at org.apache.hadoop.mapred.jobtracker_jsp._jspService(jobtracker_jsp.java:93)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)
""IPC Server handler 4 on 54311"":
	at org.apache.hadoop.mapred.CapacityTaskScheduler$TaskSchedulingMgr.updateQSIObjects(CapacityTaskScheduler.java:564)
	- waiting to lock <0x5660e518> (a org.apache.hadoop.mapred.CapacityTaskScheduler$MapSchedulingMgr)
	at org.apache.hadoop.mapred.CapacityTaskScheduler$TaskSchedulingMgr.assignTasks(CapacityTaskScheduler.java:855)
	at org.apache.hadoop.mapred.CapacityTaskScheduler$TaskSchedulingMgr.access$1000(CapacityTaskScheduler.java:294)
	at org.apache.hadoop.mapred.CapacityTaskScheduler.assignTasks(CapacityTaskScheduler.java:1336)
	- locked <0x5660dd20> (a org.apache.hadoop.mapred.CapacityTaskScheduler)
	at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:2288)
	- locked <0x56605100> (a org.apache.hadoop.mapred.JobTracker)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)

Unfortunately I didn't manage to select all of the output by mistake, so some is missing, but it appears that reclaimCapacity locks the MapSchedulingMgr and then tries to lock the JobTracker, whereas the updateQSIObjects called in assignTasks holds a lock on the JobTracker (the JobTracker grabs this lock when it calls assignTasks) and then tries to lock the MapSchedulingMgr. The other thread listed there is a Jetty thread for the web interface and isn't part of the circular locking. The solution to this would be to lock the JobTracker in reclaimCapacity before locking anything else."
HADOOP-4975,CompositeRecordReader: ClassLoader set in JobConf is not passed onto WrappedRecordReaders,"I am using a custom ClassLoader which I set in my JobConf via setClassLoader(). The ClassLoader is loaded key and value classes which are required to read records from SequenceFiles that were written out in a previous MapReduce job. 

However, I am getting a ClassNotFoundException when using the CompositeInputFormat to create a RecordReader to read these SequenceFiles from HDFS. It occurs when the SequenceFile.Reader tries to create an instance of the Key/Value classes, presumably because the class loader SequenceFile.Reader is using is not the one I set with JobConf.setClassLoader. Below is an example of the stack trace I get:

{code}
Caused by: java.io.IOException: WritableName can't load class
	at org.apache.hadoop.io.WritableName.getClass(WritableName.java:73)
	at org.apache.hadoop.io.SequenceFile$Reader.getKeyClass(SequenceFile.java:1596)
	... 33 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.MyWritableClass
	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:252)
	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:247)
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:673)
	at org.apache.hadoop.io.WritableName.getClass(WritableName.java:71)
	... 34 more
{code}

I'll attach a unit test that can demonstrate this more clearly....

"
HADOOP-4972,FSNamesystem.countNodes(Block b) should be synchronized.,"(Copied a [comment|https://issues.apache.org/jira/browse/HADOOP-4840?focusedCommentId=12657242#action_12657242] from HADOOP-4840.)

FSNamesystem.countNodes(..) is called in many places including:

    * FSNamesystem.addStoredBlock(Block, DatanodeDescriptor, DatanodeDescriptor)
    * FSNamesystem.checkReplicationFactor(INodeFile)
    * FSNamesystem.decrementSafeBlockCount(Block)
    * FSNamesystem.getBlockLocationsInternal(String, INodeFile, long, long, int, boolean)
    * FSNamesystem.invalidateBlock(Block, DatanodeInfo)
    * FSNamesystem.isReplicationInProgress(DatanodeDescriptor)
    * FSNamesystem.markBlockAsCorrupt(Block, DatanodeInfo)
    * FSNamesystem.processMisReplicatedBlocks()
    * FSNamesystem.processPendingReplications()
    * FSNamesystem.updateNeededReplications(Block, int, int)

However, some of them, e.g. getBlockLocationsInternal, call countNodes(..) without owning the fsnamesystem lock before calling . It may causes NPE in runtime."
HADOOP-4971,Block report times from datanodes could converge to same time.   ,"Datanode block reports take quite a bit of memory to process at the namenode. After the inital report, DNs pick a random time to spread this load across at the NN. This normally works fine. 

Block reports are sent inside ""offerService()"" thread in DN. If for some reason this thread was stuck for long time (comparable to block report interval), and same thing happens on many DNs, all of them get back to the loop at the same time and start sending block report then and every hour at the same time. 

RPC server and clients in 0.18 can handle this situation fine. But since this is a memory intensive RPC it lead to large GC delays at the NN. We don't know yet why offerService therads seemed to be stuck, but DN should re-randomize it block report time in such cases."
HADOOP-4970,Use the full path when move files to .Trash/Current,"When a directory or file is deleted using rm or rmr, the subtree is moved directly to .Trash/Current. If there are two deleted objects with the same name but with different original paths it is not feasible to undelete them without storing additional information regarding the original paths. Instead if the full path of the deleting object is used while moving (replicating the complete path in .Trash/Current) it will be easier to undelete objects to its original paths.
"
HADOOP-4967,Inconsistent state in JVM manager,"The following is the scenario:
-> TT got a LaunchTaskAction. And It found the slot free. It started the TaskRunner
-> TT received KillJobActtion. It did a runner.kill().
But, runner.kill() didn't remove JVM Runner from jvmIdToRunner , as it is not polulated yet. 
It released the slots.
-> TaskRunner thread spawned jvm and populated jvmIdToRunner.
-> The next task getting launched on the slot finds it as busy.

"
HADOOP-4966,Setup tasks are not removed from JobTracker's taskIdToTIPMap even after the job completes,"When a task is launched, a {{TaskID}} to {{TaskInProgress}} mapping is added to {{taskidToTIPMap}} in {{JobTracker}}. Upon task completion the mapping is removed. Mapping for all the tasks are removed (including cleanup tasks) except setup tasks."
HADOOP-4965,DFSClient should log instead of printing into std err.,{{DFSClient.LeaseChecker.close()}} uses {{System.err.println()}} and {{Exception.printStackTrace()}} to output the exception.{{LOG.error()}} should be used instead.
HADOOP-4963,Logs saying org.apache.hadoop.util.DiskChecker$DiskErrorException in TaskTracker are not relevant,"TaskTracker logs has not much relevant  logs at info level for
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_200812311029_0001/attempt_200812311029_0001_m_000000_0/output/file.out in any of the configured local directories.
This happens when the map has not created output file.
These logs make it hard to debug."
HADOOP-4962,HADOOP-4679 to be fixed for branches >= 0.19,"HADOOP-4679 applies to 0.18.3 only. I'm able to consistently reproduce the same issue in 0.19.0.

For reference, our Datanodes use 4 independent drives. When we pull any of them, the Datanode begins logging hundreds of the following message per second:
{quote}
2008-12-31 03:33:48,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Waiting for threadgroup to exit, active threads is 2
{quote}"
HADOOP-4961,ConcurrentModificationException in lease recovery of empty files.,"The problem is that internalReleaseLease() finalizes empty files, which removes these file path names from the lease. So this modifies the Collection of file names which {{LeaseManager.checkLeases()}} is iterating on."
HADOOP-4960,Hadoop metrics are showing in irregular intervals,"dfs datanode metrics are not parsing correctly in demux.  There was a bug introduced for rounding the timestamp of the metrics.  Instead of log file timestamp, system current timestamp was used."
HADOOP-4959,System metrics does not output correctly for Redhat 5.1.,"Top output is different between Redhat 4.5 and Redhat 5.1.  The demux parser needs to be updated to parse information correctly.

From Redhat EL 4.5:

008-12-23 00:00:33,983 INFO org.apache.hadoop.chukwa.inputtools.plugin.metrics.Exec: top - 00:00:33 up 291 days,  3:34,
 0 users,  load average: 1.42, 4.47, 4.97^D
Tasks: 184 total,   1 running, 183 sleeping,   0 stopped,   0 zombie^D
Cpu(s):  6.9% us,  0.8% sy,  0.5% ni, 90.8% id,  0.9% wa,  0.0% hi,  0.2% si^D
Mem:   3088508k total,  1364972k used,  1723536k free,    68008k buffers^D
Swap: 16386160k total,   501196k used, 15884964k free,   406344k cached^D

From Redhat EL 5.1:

2008-12-23 00:55:00,242 INFO org.apache.hadoop.chukwa.inputtools.plugin.metrics.Exec: top - 00:55:00 up 33 days, 22:07,
 2 users,  load average: 1.14, 1.17, 1.07^D
Tasks: 177 total,   1 running, 176 sleeping,   0 stopped,   0 zombie^D
Cpu(s):  0.3%us,  0.0%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st^D
Mem:   8175232k total,  2027628k used,  6147604k free,   344052k buffers^D
Swap: 25157664k total,        0k used, 25157664k free,  1248400k cached^D

Notice in 5.1, there is a % for st.  

The parser needs to be improved."
HADOOP-4957,Lease monitor should not fail on a live name-node.,"If a runtime exception is thrown inside the lease monitor, as it happened in HADOOP-4951, the monitor will die although the name-node will continue to run. The problem is that abandoned files will not be garbage collected."
HADOOP-4956,NPE when jobdetails.jsp ,"I see exceptions like the one pasted below on the job UI. This happens when tasks fail.
java.lang.NullPointerException
	at org.apache.hadoop.mapred.Counters.incrAllCounters(Counters.java:440)
	at org.apache.hadoop.mapred.JobInProgress.incrementTaskCounters(JobInProgress.java:910)
	at org.apache.hadoop.mapred.JobInProgress.getMapCounters(JobInProgress.java:880)
	at org.apache.hadoop.mapred.jobdetails_jsp._jspService(jobdetails_jsp.java:313)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)
"
HADOOP-4955,Make DBOutputFormat us column names from setOutput(...),"In org.apache.hadoop.mapred.lib.db.DBOutputFormat, the supplied names of the columns are not used for inserting values. The column names supplied to DBOutputFormat.setOutput(JobConf, String, String...) are used to determine the number of values to insert, but the order is dictated by the table definition of the underlying database. This affects the correct indices for DBWritable.write(PreparedStatement).

I will attach a patch that correctly maps these values.

I am characterizing this as a bug rather than an improvement because there may be existing code which implicitly relied on DBOutputFormat ignoring the supplied table names."
HADOOP-4952,Improved files system interface for the application writer.,"Currently the FIleSystem interface serves two purposes:
- an application writer's interface for using the Hadoop file system
- a file system implementer's interface (e.g. hdfs, local file system, kfs, etc)

This Jira proposes that we provide a simpler interfaces for the application writer and leave the FilsSystem  interface for the implementer of a filesystem.

- Filesystem interface  has a  confusing set of methods for the application writer
- We could make it easier to take advantage of the URI file naming
** Current approach is to get FileSystem instance by supplying the URI and then access that name space. It is consistent for the FileSystem instance to not accept URIs for other schemes, but we can do better.
** The special copyFromLocalFIle can be generalized as a  copyFile where the src or target can be generalized to any URI, including the local one.
** The proposed scheme (below) simplifies this.

-	The client side config can be simplified. 
** New config() by default uses the default config. Since this is the common usage pattern, one should not need to always pass the config as a parameter when accessing the file system.  
-	
** It does not handle multiple file systems too well. Today a site.xml is derived from a single Hadoop cluster. This does not make sense for multiple Hadoop clusters which may have different defaults.
** Further one should need very little to configure the client side:
*** Default files system.
*** Block size 
*** Replication factor
*** Scheme to class mapping
** It should be possible to take Blocksize and replication factors defaults from the target file system, rather then the client size config.  I am not suggesting we don't allow setting client side defaults, but most clients do not care and would find it simpler to take the defaults for their systems  from the target file system. 
"
HADOOP-4951,Lease monitor does not own the LeaseManager lock in changing leases.,"In Monitor.checkLeases(), the monitor thread does not own the LeaseManager lock but it may modify the leases."
HADOOP-4950,CompressorStream and BlockCompressorStream should be public,"To simplify writing codecs, CompressionStream and BlockCompressionStream provide helper base classes. They should be made public so they can be used outside of Hadoop's package structure."
HADOOP-4949,Native compilation is broken,"Compilation of the native libs is broken:
{noformat}
compile-core-native:
    [javah] [Search path = /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/resources.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/rt.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/sunrsasign.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/jsse.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/jce.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/charsets.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/classes: \
                           /hadoophome/build/classes]
    [javah] [Loaded /hadoophome/build/classes/org/apache/hadoop/io/compress/zlib/ZlibCompressor.class]
    [javah] [Loaded /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/rt.jar(java/lang/Object.class)]
    [javah] [Forcefully writing file /hadoophome/build/native/Linux-i386-32/src/org/apache/hadoop/io/compress/zlib/org_apache_hadoop_io_compress_zlib_ZlibCompressor.h]
    [javah] [Loaded /hadoophome/build/classes/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.class]
    [javah] [Forcefully writing file /hadoophome/build/native/Linux-i386-32/src/org/apache/hadoop/io/compress/zlib/org_apache_hadoop_io_compress_zlib_ZlibDecompressor.h]
    [javah] [Search path = /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/resources.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/rt.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/sunrsasign.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/jsse.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/jce.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/lib/charsets.jar: \
                           /toolshome/build/Linux_2.6_rh4_x86_64/tools/java/jdk1.6.0_i586/jre/classes: \
                           /hadoophome/build/classes]
    [javah] Error: Class org.apache.hadoop.io.compress.lzo.LzoCompressor could not be found.
{noformat}
"
HADOOP-4948,ant test-patch does not work,"ant test-patch is reporting ""Trunk compilation is broken?"" for any patch.
{noformat}
...
     [exec] /home/tsz/apache-ant-1.7.1/bin/ant -Dversion=PATCH-a.patch -Djavac.args=-Xlint -Xmaxwarns 1000  -DHadoopPatchProcess= clean tar > /home/tsz/tmp/trunkJavacWarnings.txt 2>&1
     [exec] Trunk compilation is broken?
{noformat}"
HADOOP-4947,use regex to parse chukwa commands,"In chukwa agent, we should do a better regex match; this will allow more arbitrary whitespace.
"
HADOOP-4945,Add BlockTool to query file and its block info,"The fsck can get the file's block detail,but when you want see which file or datanode the block belongs to ,it will be helpless.
The BlockTool will be  helpfull in developing,for example when you happened to these message :

2008-12-25 12:12:10,049 WARN  dfs.DataNode (DataNode.java:readBlock(901)) - Got exception while serving blk_28622148 to /10.7
3.4.101:
java.io.IOException: Block blk_28622148 is not valid.
        at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:541)
        at org.apache.hadoop.dfs.DataNode$BlockSender.<init>(DataNode.java:1090)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:882)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:840)
        at java.lang.Thread.run(Thread.java:595)


the Blocktool may help you to get the location,it can get the file name and which datanodes hold the block. 
Also it can get the file or directory 's block details too.


"
HADOOP-4944,Allow Xinclude in hadoop config file,"It would be easier to mange the configuration of hadoop by allowing include files in configuration file (file: hadoop-site.xml)
"
HADOOP-4943,fair share scheduler does not utilize all slots if the task trackers are configured heterogeneously,"There is some code in the fairshare scheduler that tries to make the load across the whole cluster the same.
That piece of code will break if the task trackers are configured differently. Basically, we will stop assigning more tasks to tasks trackers that have tasks above the cluster average, but we may still want to do that because other task trackers may have less slots.

We should change the code to maintain a cluster-wide slot usage percentage (instead of absolute number of slot usage) to make sure the load is evenly distributed.
"
HADOOP-4942,"Remove getName() and getNamed(String name, Configuration conf)","Remove these two methods:
- public String getName()
- public static FileSystem getNamed(String name, Configuration conf)
"
HADOOP-4941,"Remove getBlockSize(Path f), getLength(Path f) and getReplication(Path src)","Remove the following
- public long getBlockSize(Path f) throws IOException
- public long getLength(Path f) throws IOException
- public short getReplication(Path src) throws IOException"
HADOOP-4940,Remove delete(Path f),"Remove the following:
{code}
  /** Delete a file. */
  /** @deprecated Use delete(Path, boolean) instead */ @Deprecated 
  public abstract boolean delete(Path f) throws IOException;
{code}"
HADOOP-4937,[HOD] Include ringmaster RPC port information in the notes attribute,"In large cluster deployments, due to node failures, it sometimes happens that HOD clusters get allocated, but not deallocated even after the idleness limit of the cluster (the time for which no jobs are run) exceeds. One of the main reasons for this is the ringmaster process which is responsible for tracking and cleaning an idle cluster (of which it is a part) itself goes down. To handle such scenarios it makes sense to centrally track the ringmaster nodes for suspicious clusters. But since the information about which port the ringmaster is bound to is not centrally available, this becomes impossible to monitor.

This issue is an enhancement request to include ringmaster RPC port information along with the JT and NN info as part of the resource manager's notes attribute so that it can be used by any monitoring processes built around it."
HADOOP-4936,Improvements to TestSafeMode,"TestSafeMode 
- needs a detailed description of the test case
- should not use direct calls to the name-node rather call {{DistributedFileSystem}} methods.
"
HADOOP-4935,Manual leaving of safe mode may lead to data lost,"Due to HADOOP-4610, NameNode calculates mis-replicated blocks when leaving safe mode manually, where it clears the pending deletion queue before it does the calculation. This works fine when NameNode just starts but introduced a bug when NameNode is running for a while. Clearing the pending deletion queue makes NameNode not able to distinguish valid replicas from invalid ones, ie, the ones that have scheduled or dispatched for deletion. Therefore, NameNode may mistakenly decide the block is over-replicated and choose all valid ones to delete.    "
HADOOP-4934,Distinguish running/successful/failed/killed jobs in jobtracker's history,It would be nice to group jobs in jobtracker's history based on their completion status.
HADOOP-4933,ConcurrentModificationException in JobHistory.java,{{JobHistory.java}} throws {{ConcurrentModificationException}} while finding out the job history version.
HADOOP-4930,Implement setuid executable for Linux to assist in launching tasks as job owners,"HADOOP-4490 tracks the design and implementation of launching tasks as job owners. As per discussion there, the proposal is to implement a setuid executable for Linux that would be launched by the tasktracker to run tasks as the job owners. In this task we will track the implementation of the setuid executable. HADOOP-4490 will be used to track changes in the tasktracker."
HADOOP-4927,Part files on the output filesystem are created irrespective of whether the corresponding task has anything to write there,"When OutputFormat.getRecordWriter is invoked, a part file is created on the output filesystem. But the created RecordWriter is not used until the OutputCollector.collect call is made by the task (user's code). This results in empty part files even if the OutputCollector.collect is never invoked by the corresponding tasks."
HADOOP-4925,Chukwa HTTP connection policies are too hard to configure,"We should make the HttpSender configuration parameters configurable, instead of compile-time constants."
HADOOP-4924,Race condition in re-init of TaskTracker,"The taskReportServer is stopped in the TaskTracker.close() method in a thread. The race condition is:
1) TaskTracker.close() is invoked - this starts a thread to stop the taskReportServer
2) TaskTracker.initialize is invoked - this tries to create a new taskReportServer
Assume that the thread started to stop the taskReportServer gets to start its work after (2) above. The thread will end up stopping the newly created taskReportServer."
HADOOP-4920,do not keep forrest output in subversion,"We currently re-generate PDF and HTML documentation whenever we commit a 
documentation patch, which creates huge commit messages that few read. This was 
originally done so that folks who check out the sources from subversion did not 
need to install forrest in order to read the documentation.

Note, this issue only concerns the versioned documentation included in trunk and releases, not the website, whose forrest output should continue to be kept in subversion."
HADOOP-4919,[HOD] Provide execute access to JT history directory path for group,"HADOOP-4782 was opened to discuss the right level of access required for Chukwa to be able to read JT history logs under HOD provisioned directories. This was in turn required because HADOOP-4705, which provided world readable rights for all directories provisioned by HOD was found to be very unsecure for shared clusters. As per discussions on these two jiras, we decided to NOT change HOD's default behavior (of not granting access) for Hadoop 0.20, and providing very restricted access for Hadoop 0.18.3 (only execute permissions for group on the directory path until the history directory on the JT node).

HADOOP-4782 tracked the reversal of the changes in HADOOP-4705 for Hadoop 0.20. This issue is being opened to make the restricted change in Hadoop 0.18.3. The patch submitted on HADOOP-4782 for Hadoop 0.18.3 can just be uploaded here. I am filing a new jira only because the nature of the fixes is different in spirit for the two versions."
HADOOP-4918,Fix bzip2 work with SequenceFile,"Somehow bzip2 does not work with SequenceFile:

{code}
    String codec = ""org.apache.hadoop.io.compress.BZip2Codec"";
    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, new Path(output), 
        reader.getKeyClass(), reader.getValueClass(), CompressionType.BLOCK, 
        (CompressionCodec)Class.forName(codec).newInstance());
{code}

The stack trace is here:
{noformat}
java.lang.UnsupportedOperationException
        at org.apache.hadoop.io.compress.BZip2Codec.getCompressorType(BZip2Codec.java:80)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:98)
        at org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:914)
        at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1198)
        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:401)
        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:329)
        at org.apache.hadoop.mapred.TestSequenceFileBZip.main(TestSequenceFileBZip.java:43)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:165)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)
{noformat}
"
HADOOP-4916,"adjust build.xml to reference external properties when building the ""init.d"" for the rpm","Instead of having the properties embedded in the build.xml directly, allow them to be configured in a properties file that
can override the defaults in build.xml that way we can check in a local build.properties file and have the defaults.properties pulls
from Apache SVN.
"
HADOOP-4915,Out of Memory error in reduce shuffling phase when compression is turned on,"mapred.compress.map.output is set to true, and the job has 6860 mappers and 300 reducers.

Several reducers failed because:out of memory error in the shuffling phase.
"
HADOOP-4914,Support chkconfig for chukwa start up scripts,"Chukwa init.d scripts does not have description field.  By running:

chkconfig --add chukwa-agent

This fails with message:  service chukwa-agent does not support chkconfig

The fix is to add description: field in the init.d scripts."
HADOOP-4910,NameNode should exclude corrupt replicas when choosing excessive replicas to delete,"Currently, when NameNode handles an over-replicated block in FSNamesystem#processOverReplicatedBlock, it excludes ones already in excessReplicateMap and decommissed ones, but it treats a corrupt replica as a valid one. This may lead to unnecessary deletion of more replicas and thus cause data lose. It should exclude corrupt replicas as well."
HADOOP-4909,Clean up map/reduce api to take JobContext in the static methods,Several of the static methods in the HADOOP-1230 api aren't consistent in that the setters take Job objects and the getters take Configuration objects. It would be better if the getters took JobContext objects.
HADOOP-4907,TestMapReduceLocal fails,"{noformat}
Testcase: testWithLocal took 33.276 sec
	Caused an ERROR
Output directory /home/tsz/hadoop/latest/build/test/data/out already exists
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory /home/tsz/hadoop/latest/build/test/data/out already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:124)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:770)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:437)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:450)
	at org.apache.hadoop.mapreduce.TestMapReduceLocal.runSecondarySort(TestMapReduceLocal.java:144)
	at org.apache.hadoop.mapreduce.TestMapReduceLocal.testWithLocal(TestMapReduceLocal.java:89)
{noformat}"
HADOOP-4906,TaskTracker running out of memory after running several tasks,"Looks like the TaskTracker isn't cleaning up correctly after completed/failed tasks, I suspect that the JobConfs aren't being deallocated. Eventually the TaskTracker runs out of memory after running several tasks."
HADOOP-4904,Deadlock while leaving safe mode.,"{{SafeModeInfo.leave()}} acquires locks in an incorrect order, which causes the deadlock.
It first acquires the {{SafeModeInfo}} lock, then calls {{FSNamesystem.processMisReplicatedBlocks()}}, which requires the global {{FSNamesystem}} lock.
It should be the other way around: first {{FSNamesystem}} lock, then {{SafeModeInfo}}."
HADOOP-4899,Mutliple times wrong class provided for logger instantiation,"Working in PermissionChecker.java I noticed the wrong class was being provided for the logger instantiation (LOG = LogFactory.getLog(UserGroupInformation.class, should be LogFactory.getLog(PermissionChecker.class););.  Probably a copy-and-paste mistake.  I searched through the source files and found several other instances across the project of this error.  This could cause problems when trying to examine the logs, as entries would appear to be coming from the incorrect class."
HADOOP-4896,hdfs fsck does not load hdfs configuration.,"DFsck does not load new hdfs configuration files. So it does not work if you have the recommended configuration.
"
HADOOP-4895,Remove deprecated methods in DFSClient,"The following two deprecated methods in DFSClient should be removed:
- getHints(String src, long start, long length) throws IOException
- isDirectory(String src) throws IOException"
HADOOP-4894,fix Chukwa's jettyCollector.sh,"Two related issues with jettyCollector.sh
1) We should pass through shell arguments to process
2) When using the script to stop a running process, the script will fail silently if the PID is too few characters to use up the whole line: the first field extracted by ""cut"" will be a space and then nothing will get stopped.  

"
HADOOP-4892,File descriptor leak in Chukwa's ExecPlugin,"Chukwa's ExecPlugin doesn't properly close pipes after running a child process.  For ExecPlugin, this is benign, since it runs in a separate process.  For ExecAdaptor, this is catastrophic, since file descriptors leak. "
HADOOP-4891,Remove lzo from chukwa,Lzo is encumbered by the GPL and thus the Lzo-based codec needs to be removed from Hadoop.
HADOOP-4889,Chukwa RPM needs to chown user/group in build stage instead of post install stage.,"When running rpm -V the rpm shows:

bash-3.1# rpm -V chukwa-0.1.1-1
.....UG.   /grid/0/chukwa/LICENSE.txt
.....UG.   /grid/0/chukwa/bin
.....UG.   /grid/0/chukwa/bin/VERSION
.....UG.   /grid/0/chukwa/bin/agent.sh

This means the User and Group permission are mismatched with what RPM provided.  The build script needs to change to chown file ownership during build time instead of chown in the post installation script."
HADOOP-4888,Use Apache HttpClient for fetching map outputs,It's worth experimenting with the [HttpClient|http://hc.apache.org/httpclient-3.x/] library to speed up the shuffle.
HADOOP-4885,Try to restore failed replicas of Name Node storage (at checkpoint time),
HADOOP-4884,Change Date format pattern for Time Series graph,"The tool tip of Time series chart for Chukwa is formatting date as:

day/month/year hour:minute:second

The date format should change to:

year/month/day hour:minute:second

"
HADOOP-4880,Improvements to TestJobTrackerRestart,"TestJobTrackerRestart could use the following improvements:
# Remove the one minute 'wait' - this is really bad for test-cases
# It assumes that a particular ordering of job-scheduling based on current behaviour of JobQueueTaskScheduler assigning a single task per heartbeat (pre HADOOP-3136) and thus a particular job completion order, which is incorrect and won't work with all schedulers."
HADOOP-4879,TestJobTrackerRestart fails on trunk,"HADOOP-1230 changed the definition of TaskReport.equals:

{noformat}
@@ -172,7 +172,7 @@
       return false;
     if(o.getClass().equals(TaskReport.class)) {
       TaskReport report = (TaskReport) o;
-      return counters.contentEquals(report.getCounters())
+      return counters.equals(report.getCounters())
{noformat}

This results in:
{noformat}
Testcase: testJobTrackerRestart took 473.926 sec
  FAILED
Task reports for same attempt has changed
junit.framework.AssertionFailedError: Task reports for same attempt has changed
  at org.apache.hadoop.mapred.TestJobTrackerRestart.testTaskReports(TestJobTrackerRestart.java:514)
  at org.apache.hadoop.mapred.TestJobTrackerRestart.testTaskEventsAndReportsWithRecovery(TestJobTrackerRestart.java:447)
  at org.apache.hadoop.mapred.TestJobTrackerRestart.testJobTrackerRestart(TestJobTrackerRestart.java:599)
{noformat}

"
HADOOP-4878,After introduction of ivy ant test-patch always returns -1 score,"After dependency management is moved to ivy : ant test-patch always returns a -1 score. The reason reported by the target is modification of the Eclipse classpath.
"
HADOOP-4876,Capacity reclaimation for queue would not work as intended,Capacity schedulers reclaim logic would not work as intended after [HADOOP-4513|https://issues.apache.org/jira/browse/HADOOP-4513] went in.
HADOOP-4874,Remove bindings to lzo,It looks like the lzo bindings are infected by lzo's GPL and must be removed from Hadoop.
HADOOP-4873,display minMaps/Reduces on advanced scheduler page,the deficit and minMap are the most important variables for purposes of scheduling. showing the minMaps would help in debugging any scheduling issues
HADOOP-4871,SE,"I got 3 hosts in the cluster : one for master 172.18.30.149 and the others as slaves (172.18.30.137 e 172.18.30.153). When i try to start, i got the following error. When i configure the masters file, and try 172.18.30.137, the secondary namenode starts. 
I think the problem is at master (172.18.30.149), cause this host doesn´t start neither Namenode nor secondary NameNode.
Cloud anyone give me a hand ????

hadoop@cloud-cos001:/tmp/hadoop$ bin/start-dfs.sh 
starting namenode, logging to /tmp/hadoop/bin/../logs/hadoop-hadoop-namenode-cloud-cos001.out
172.18.30.137: datanode running as process 23598. Stop it first.
172.18.30.153: datanode running as process 19379. Stop it first.
172.18.30.149: starting datanode, logging to /tmp/hadoop/bin/../logs/hadoop-hadoop-datanode-cloud-cos001.out
172.18.30.149: starting secondarynamenode, logging to /tmp/hadoop/bin/../logs/hadoop-hadoop-secondarynamenode-cloud-cos001.out
172.18.30.149: Exception in thread ""main"" java.lang.NullPointerException
172.18.30.149:  at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:119)
172.18.30.149:  at org.apache.hadoop.dfs.SecondaryNameNode.<init>(SecondaryNameNode.java:118)
172.18.30.149:  at org.apache.hadoop.dfs.SecondaryNameNode.main(SecondaryNameNode.java:495)

"
HADOOP-4869,Lost Trackers may not be able to join back,"There is a bug in the heartbeat processing which shows up when TaskTrackers are lost. Due to the bug, lost TTs may not be able to join back the JT after reinitializing (and binding to a RPC port different from the previous one). This bug got introduced in HADOOP-4305."
HADOOP-4868,Split the hadoop script into 3 parts,"We need to split the bin/hadoop into 3 parts for core, mapred and hdfs. This will enable us to distribute the individual scripts with each component."
HADOOP-4866,NameNode error in commitBlockSynchronization,"The NameNode continuously has an error in the commitBlockSynchronization.  This happens for ~5 blocks at a rate of 5-10Hz.  I have no idea when this started happening because this has been going on for days, well past the start of our current logs.

This appears to be a new symptom in 0.19.0, but I have no idea what could be causing it."
HADOOP-4864,-libjars with multiple jars broken when client and cluster reside on different OSs,"When submitting a hadoop job from Windows (Cygwin) to a Linux hadoop cluster (or vice versa), and when you specify multiple additional jar files via the -libjars flag, hadoop throws a ClassNotFoundException for any classes located in the additional jars specified via the -libjars flag.

This is caused by the fact that hadoop uses system.getProperty(""path.separator"") as the delimiter in the list of jar files passed via -libjars.

My suggested solution is to use a comma as the delimiter, rather than the path.separator.

I realize comma is, perhaps, a poor choice for a delimiter because it is valid in filenames on both Windows and Linux, but the -libjars flag uses it as the delimiter when listing the additional required jars.  So, I figured if it's already being used as a delimiter, then it's reasonable to use it internally as well.
"
HADOOP-4862,A spurious IOException log on DataNode is not completely removed,"HADOOP-3678 fixes a spurious warning log in DataNode, in most cases. This log is still possible in some cases.

For e.g. The exception is still logged if the client closes the connection after DataNode writes all the block data but before it writes final 4 bytes to indicate end of data.
"
HADOOP-4861,Add disk usage with human-readable size (-duh),"I think it'd be super useful to have a form of ""hadoop dfs -du"" that showed the sizes in human-readable format, as in the standard command ""du -h"". "
HADOOP-4860,File Tailing Adaptor test cases causes random failure,"TestFileTailingAdaptors contain code to startup and shut down Chukwa Agent 3 times.  In each of the startup, ChukwaAgent will attempt to restore to its state to where it was previously sending data.  This means the data in transit may not be the data that the test case is expecting, and test case fails.  The test cases should be divided into 3 different Test classes, the automated test framework will remove chukwa check point file to prevent the previous  state to interfere with the current test cases."
HADOOP-4859,Make the M/R Job output dir unique for Daily rolling,
HADOOP-4858,to add appropriate reference to the dependent library files in the chukwa/build.xml file ,"While going through the chukwa/build.xml 's package-hadoop target found that we are trying to copy jsp-api.jar from the hadoop/lib dir but actually the jsp-api. jar resides in chukwa/lib directory. 

For more details see comment 

https://issues.apache.org/jira/browse/HADOOP-4709?focusedCommentId=12654489#action_12654489

-Giri"
HADOOP-4857,TestUlimit is failing after Hadoop-4620,"TestUlimit launches 3 map tasks and generates output for only 1 as input to other 2 map tasks are empty. With Hadoop-4620, even the map tasks with empty input are generating the output.
The fix could be just setting the no of mapper to 1"
HADOOP-4855,Fix help message in MRAdmin,MRAdmin's help message wrongly uses 'refresh-auth-policy' instead of '-refreshServiceAcl' for the command specific help message.
HADOOP-4854,Capacity Scheduler should read the reclaim-capacity-interval config value from its own config file,"The Capacity Scheduler needs to know the time interval for checking whether capacity needs to be reclaimed. This is represented by the static variable RECLAIM_CAPACITY_INTERVAL. This value is currently read as follows: 
{code}
    RECLAIM_CAPACITY_INTERVAL = conf.getLong(""mapred.capacity-scheduler.reclaimCapacity.interval"", 5);
{code}

This is incorrect for the following reasons: 
* """"mapred.capacity-scheduler.reclaimCapacity.interval"" is not present in any of the config files: hadoop-site.xml or capacity-scheduler.xml.
* ""mapred.capacity-scheduler.reclaimCapacity.interval"" should be specified in the scheduler's config file, capacity-scheduler.xml. It should be read through org.apache.hadoop.mapred.CapacitySchedulerConf. 
* The Forrest documentation for the Capacity Scheduler should describe this setting. We should probably create a new sub-section under 'Configuration', titled 'Configuring properties for the Scheduler'. 
"
HADOOP-4849,Document service level authorization - HADOOP-4348,Document service level authorization (HADOOP-4348) via forrest.
HADOOP-4847,OutputCommitter is loaded in the TaskTracker in localizeConfiguration,"The TaskTracker localizes the JobConf for the task. Currently, this involves creating an OutputCommitter to set the work path. Since the TaskTracker will not have the user's OutputCommitter, this will get a ClassNotFoundException, unless they use FileOutputCommitter.

There should also be a test case for this case."
HADOOP-4846,HDFS arch doc outdated,"Just noticed that nobody's updated the Hadoop Distributed File System: Architecture and Design doc in a while. For instance, it still says:
bq. HDFS does not yet implement user quotas or access permissions.
"
HADOOP-4845,Shuffle counter issues,HADOOP-4749 added a new counter tracking the bytes shuffled into the reduce. It adds an accumulator to ReduceCopier instead of simply incrementing the new counter and did not define a human-readable value in src/mapred/org/apache/hadoop/mapred/Task_Counter.properties.
HADOOP-4844,Document deprecation of o.a.h.fs.permission.AccessControlException better,HADOOP-4348 deprecated org.apache.hadoop.fs.permission.AccessControlException - it would be good to document the deprecation better to point users to org.apache.hadoop.security.AccessControlException.
HADOOP-4843,Collect Job History log file and Job Conf file into Chukwa,"Chukwa monitoring framework should collect job history log file, and job configuration files for analysis of the job performance and utilization.
The current design is to tap into Job Tracker Instrumentation API.  When a job is submitted, or completed, the JobTracker Instrumentation API handles the communication with Chukwa Agent to start or shutdown job log file streaming."
HADOOP-4842,"Streaming combiner should allow command, not just JavaClass","Streaming jobs are way slower than Java jobs for many reasons, but certainly stopping the shell-only programmer from using the combiner feature won't help. Right now, the streaming usage says:

{quote}
  -mapper   <cmd|JavaClassName>      The streaming command to run
  -combiner <JavaClassName> Combiner has to be a Java class
  -reducer  <cmd|JavaClassName>      The streaming command to run
{quote}
"
HADOOP-4841,TestFileTailingAdaptors fails sometimes,"Hit this a few times:
{noformat}
Testsuite: org.apache.hadoop.chukwa.datacollection.adaptor.filetailer.TestFileTailingAdaptors
Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec

Testcase: unknown took 0.004 sec
        Caused an ERROR
Timeout occurred
junit.framework.AssertionFailedError: Timeout occurred
{noformat}"
HADOOP-4840,TestNodeCount sometimes fails with NullPointerException,"Testcase: testNodeCount took 9.628 sec
        Caused an ERROR

java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.countNodes(FSNamesystem.java:3523)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.countNodes(FSNamesystem.java:3543)
        at org.apache.hadoop.hdfs.server.namenode.TestNodeCount.testNodeCount(TestNodeCount.java:64)
"
HADOOP-4838,Cleanup Metrics and the MBeans,"This patch cleans up the metrics to make it easier to write metrics and the mbeans.
Added a registry to the metrics so that the pushing of metrics can be done automatically.
Also this registry is used to create dynamic mbeans from metrics.
Added the dynamic mbeans base and changed the existing activity beans to use them.
Also cleanup the names of the mbeans.

Hence each time a new metrics is added to an existing set of metrics,  one needs to simply do
1) add the metrics to a holder class (such as NameNodeMetrics
2) add code to update the metrics.
The metrics will be pushed as per metrics config and it will also be published  via mbeans.
(Previous to this patch also has to add a line of code to push and add new mbean entries).

If one is adding a new category of metrics (ie for which a holder class does *not* exist) then
1) create metrics holder (see NameNodeMetrics as an example)
2) create mbean  (see NameNodeActivityMBean as an example)"
HADOOP-4837,Move HADOOP_CONF_DIR to chukwa-env.sh,
HADOOP-4836,Minor typos in documentation and comments,Found a few minor typos in the documentation and source code comments.
HADOOP-4830,Have end to end tests based on MiniMRCluster to verify that queue capacities are honoured.,"At present, we only have unit tests that make use of FakeTaskManager and that only test the proper functionality of capacity scheduler in isolation. Many issues unearthed recently proved that this is not enough and that it is required to have end-to-end tests so that real JT is brought into the picture and with that the interaction of the scheduler with JT. This issue along with few other related jiras should automate and replace the end-to-end tests that are now manually done by QA, using MiniMRCluster."
HADOOP-4829,Allow FileSystem shutdown hook to be disabled,"FileSystem sets a JVM shutdown hook so that it can clean up the FileSystem cache. This is great behavior when you are writing a client application, but when you're writing a server application, like the Collector or an HBase RegionServer, you need to control the shutdown of the application and HDFS much more closely. If you set your own shutdown hook, there's no guarantee that your hook will run before the HDFS one, preventing you from taking some shutdown actions.

The current workaround I've used is to snag the FileSystem shutdown hook via Java reflection, disable it, and then run it on my own schedule. I'd really appreciate not having to do take this hacky approach. It seems like the right way to go about this is to just to add a method to disable the hook directly on FileSystem. That way, server applications can elect to disable the automatic cleanup and just call FileSystem.closeAll themselves when the time is right."
HADOOP-4828,Update documentation for default configuration,Documentation needs to be updated as per the configuration changes in HADOOP-4631
HADOOP-4827,Improve data aggregation in database,"Chukwa is running data aggregation in database, and it has the ability to generate SQL statements base on macros.  On the other hand, there is also Consolidator program which Chukwa uses to down sample data in the database.  Consolidator program can be replaced with the generic Aggregator program plus some macros.  This enhancement will make the database aggregation framework cleaner."
HADOOP-4826,Admin command saveNamespace.,"It would be useful to have an admin command that saves current namespace.
This command can be used before regular (planned) cluster shutdown.
The command will save the namespace into storage directory(s) and reset the name-node journal (edits file).
It will also reduce name-node startup time, because edits do not need to be digest."
HADOOP-4825,Chukwa - change jps to ps for process status detection,"jps is not reliable on Redhat EL 5.1, and chukwa startup/shutdown scripts depends on jps for process status list.  The shell scripts need to change to use ps instead of jps to improve reliability."
HADOOP-4824,Should not use File.setWritable(..) in 0.18,"In TestDiskError, there are calls to File.setWritable(..)."
HADOOP-4823,Should not use java.util.NavigableMap in 0.18,
HADOOP-4822,0.18 cannot be compiled in Java 5.,"Currently, 0.18 cannot be compiled in Java 5 since some codes are using Java 6 API.  Will create sub-tasks for individual components."
HADOOP-4821,Usage description in the Quotas guide documentations are incorrect,"Qutas guide shows the following usage:
dfsadmin -setquota <N> <directory>...<directory>
dfsadmin -clrquota <directory>...<director>
dfsadmin -setspacequota <N> <directory>...<directory> 
dfsadmin -clrspacequota <directory>...<director>

the correct commands are:
-setQuota
-clrQuota
-setSpaceQuota
-clrSpaceQuota"
HADOOP-4820,Remove unused method FSNamesystem.deleteInSafeMode,Method FSNamesystem.deleteInSafeMode is not used. This method and the code related to it can be removed.
HADOOP-4818,Enable JobTracker Instrumentation API with user job configuration,"The JobTracker Instrumentation API is currently sending JobTracker configuration to submitJob(conf, JobID) call.  Instead, this should be changed to submitJob(job.getJobConf(), JobID) to obtain job related configuration.  i.e., Chukwa could use this API to find out the job history file location.  In addition, the API should be public for general public to implement useful utilities to interface with this API."
HADOOP-4811,Import style sheet images for Chukwa,"Chukwa UI (Hadoop Infrastructure Care Center) HICC for short, is missing the Chukwa Logo in the source code, and some background images."
HADOOP-4810,Data lost at cluster startup time,"hadoop dfs -cat file1 returns
dfs.DFSClient: Could not obtain block blk_XX_0 from any node: java.io.IOException: No live nodes contain current block

Tracing the history of the block from NN log, we found
 WARN org.apache.hadoop.fs.FSNamesystem: Inconsistent size for block blk_-6160940519231606858_0 reported from A1.A2.A3.A4:50010 current size is 9303872 reported size is 262144
 WARN org.apache.hadoop.fs.FSNamesystem: Deleting block blk_-6160940519231606858_0 from A1.A2.A3.A4:50010
INFO org.apache.hadoop.dfs.StateChange: DIR* NameSystem.invalidateBlock: blk_-6160940519231606858_0 on A1.A2.A3.A4:50010 
WARN org.apache.hadoop.fs.FSNamesystem: Error in deleting bad block blk_-6160940519231606858_0 org.apache.hadoop.dfs.SafeModeException: Cannot invalidate block blk_-6160940519231606858_0. Name node is in safe mode. 
WARN org.apache.hadoop.fs.FSNamesystem: Inconsistent size for block blk_-6160940519231606858_0 reported from B1.B2.B3.B4:50010 current size is 9303872 reported size is 306688 
WARN org.apache.hadoop.fs.FSNamesystem: Deleting block blk_-6160940519231606858_0 from B1.B2.B3.B4:50010 
INFO org.apache.hadoop.dfs.StateChange: DIR* NameSystem.invalidateBlock: blk_-6160940519231606858_0 on B1.B2.B3.B4:50010 
WARN org.apache.hadoop.fs.FSNamesystem: Error in deleting bad block blk_-6160940519231606858_0 org.apache.hadoop.dfs.SafeModeException: Cannot invalidate block blk_-6160940519231606858_0. Name node is in safe mode. 
INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.chooseExcessReplicates: (C1.C2.C3.C4:50010, blk_-6160940519231606858_0) is added to recentInvalidateSets 
INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.chooseExcessReplicates: (D1.D2.D3.D4:50010, blk_-6160940519231606858_0) is added to recentInvalidateSets
INFO org.apache.hadoop.dfs.StateChange: BLOCK* ask C1.C2.C3.C4:50010 to delete blk_-6160940519231606858_0
INFO org.apache.hadoop.dfs.StateChange: BLOCK* ask D1.D2.D3.D4:50010 to delete blk_-6160940519231606858_0
"
HADOOP-4806,HDFS rename does not work correctly if src contains Java regular expression special characters,"For examples,
# create a file /a+b/foo but not close it
# rename /a+b/foo to /a+b/bar/foo
Then, the corresponding INodeFileUnderConstruction will be moved from /a+b/foo to /a+b/bar/foo but the lease path remains unchanged."
HADOOP-4805,Remove black list feature from Chukwa Agent to Chukwa Collector communication,"Recently, new load balance algorithm was added to improve chukwa agent to chukwa collector communication.  The design was to send one HTTP POST per collector, and rotate through the list of collector to load balance the collectors.  When a collector fail to respond, the collector is black listed for 5 minutes.  If all collectors are not responding, sleep for random 1-5 minutes.  Unfortunately, this algorithm produced problem for slower machines.  The slower machines end up black list all collectors and sleep indefinitely.  This ticket is to restore the algorithm to the original design.  The agent will shuffle the collector list. The agent will try it's best effort to make HTTP POST to the same collector until error occurs, then it will iterate through the list of random collectors.  
"
HADOOP-4804,Create Forrest documentation for the fair scheduler,Title says it all.. take the documentation from the README into Forrest docs.
HADOOP-4797,RPC Server can leave a lot of direct buffers ,"RPC server unwittingly can soft-leak direct buffers. One observed case is that one of the namenodes at Yahoo took 40GB of virtual memory though it was configured for 24GB memory. Most of the memory outside Java heap expected to be direct buffers. This shown to be because of how RPC server reads and writes serialized data. The cause and proposed fix are in following comment.

  "
HADOOP-4796,Test target for chukwa build.xml needs to comply to hadoop build.xml test suites,"Chukwa build.xml file is not honoring the hadoop test target.  When calling ant -Dtest.include=mapred/Test* test, this command also executes chukwa test cases.  The build.xml needs to be polished to avoid testing the wrong test cases."
HADOOP-4795,Lease monitor may get into an infinite loop,"If a lease is not found in the namespace for some reasons (e.g. bugs), lease monitor may get into an infinite loop."
HADOOP-4794,separate branch for HadoopVersionAnnotation,"I think we should pull out the ""branch"" in the HadoopVersionAnnotation and display it on the Web/UI.
"
HADOOP-4793,Remove ant.jar from chukwa,"Bad things could happen if Chukwa's version of ant.jar is in the class path while compiling.  One purposed solution is to remove ant.jar from Chukwa home directory, and copy ant.jar from ANT_HOME/lib.  Chukwa uses ant tar task for grouping files together at run time.  I suspect this part of code has been deprecated and removed.  If this is the case, we can remove ant.jar from chukwa/lib completely."
HADOOP-4792,Chukwa build process generates files not tracked by svn,"After building chukwa, there are a couple artifacts:
{noformat}
?      src/contrib/chukwa/conf/mdl.xml
?      src/contrib/chukwa/conf/chukwa-agents
{noformat}
These should be ignored, as in HADOOP-4571"
HADOOP-4791,Improve rpm build process for Chukwa,"Chukwa has a ant target which builds RPM file.  The RPM includes /etc/init.d script which enable admin to start chukwa processes for the local machine.  For making the RPM installation path more flexible, chukwa build.xml should look for a property file to decide the installation location for the RPM and modify the init.d script accordingly in the build process."
HADOOP-4789,"Change fair scheduler to share between pools by default, not between invidual jobs","The fair scheduler currently treats jobs as equal entities in sharing by default, so that a user who submits 2 jobs gets 2x the share of a user who submits only 1 job. We found that it makes more sense to support equal shares between individual pools instead, and have one pool per user, because users can otherwise game the system by submitting multiple small jobs. This patch will set the scheduler to share between pools by default and set the default pool assignment process to one pool per user. it will also be possible to give weights to pools so that some users/groups/applications get a larger share of the cluster if they really do need to run more jobs."
HADOOP-4788,Set mapred.fairscheduler.assignmultiple to true by default,"Pretty much all users of the fair scheduler are using the ""assignmultiple"" feature which lets it launch reduce tasks at the same time as map tasks (on the same heartbeats), leading to reduces starting faster. This is also going to happen in Hadoop by default in HADOOP-3136. So we should set it to true by default and perhaps even remove the config parameter."
HADOOP-4787,TestTrackerBlacklistAcrossJobs fails randomly,"While fixing HADOOP-4786, I found that TestTrackerBlacklistAcrossJobs is failing randomly."
HADOOP-4786,TestTrackerBlacklistAcrossJobs compilation is broken,TestTrackerBlacklistAcrossJobs compilation is broken in trunk. I guess this is because of the commit of HADOOP-4623 which was committed about the same time as HADOOP-4305.
HADOOP-4785,avoid two calls to System.currentTimeMillis() in heartbeat,"After HADOOP-4305, shouldAssignTasksToTracker and processHeartbeat two methods make a call to System.currentTimeMillis(). 
This can be avoided by calling it once in heartbeat and passing to both of them."
HADOOP-4783,History files are given world readable permissions.,"It is found that history files are being created with permissions 0777. On shared clusters this is opening up too much. However, there is a requirement to allow Chukwa to read history files. (See HADOOP-4705). This issue is to set up appropriate permissions for the files to be as restrictive as required, while still fixing the problem for Chukwa."
HADOOP-4782,[HOD] HOD opens up directory permissions more than required,"In HADOOP-4705, we fixed hod to grant read access to all directories and files that HOD creates, and also set the umask such that files created by Hadoop also get read permissions to world. In shared clusters, this is opening up too much to the users. This issue is for resetting the permissions to be as constrained as possible, while still addressing the requirement raised in HADOOP-4705."
HADOOP-4780,Task Tracker  burns a lot of cpu in calling getLocalCache,"I noticed that many times, a task tracker max up to 6 cpus.
During that time, iostat shows majority of that was  system cpu.
That situation can last for quite long.
During that time, I saw a number of threads were in the following state:

  java.lang.Thread.State: RUNNABLE
        at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
        at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:228)
        at java.io.File.exists(File.java:733)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:399)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.fs.FileUtil.getDU(FileUtil.java:407)
        at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:176)
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:140)

I suspect that getLocalCache is too expensive.
And calling it for every task initialization seems too much waste.

"
HADOOP-4779,Remove deprecated FileSystem methods,"Deprecated FileSystem methods like getReplication(Path src), delete(Path f), etc. should be removed."
HADOOP-4778,Check for zero size block meta file when updating a block.,"In case of datanode running out of disk space, block meta files may be created with zero length.  We should detect this obvious problem when updating a block."
HADOOP-4775,FUSE crashes reliably on 0.19.0,"Every morning I come in and find many nodes which have developed the dreaded ""Transport endpoint not connected"" error overnight.  This has only started after the 0.19.0 upgrade."
HADOOP-4774,CapacityScheduler should work even if capacity-scheduler.xml is not in the classpath i.e with the code defaults,"If I try running capacity-scheduler without having capacity-scheduler.xml in my class path, the jobtracker fails with the following error
{code}
Invalid initializater poller interval -1
{code}
I feel the problem lies with the default value assigned to it in the code. The capacity-scheduler conf says the default is 5000. "
HADOOP-4771,FSImage saveFSImage() will have problem.,"When you format the namenode , hadoop will call FSImage.saveFsImage(). saveFsImage includes the following code:

out.writeLong(fsDir.rootDir.numItemsInTree());

When format, the fsDir.rootDir.numItemsInTree() should be 1 (it include the rootdir). But now fsDir.rootDir.numItemsInTree() is 0. 

The reason why the bug is not simply discovered or triggered is the code in  FSImage.saveFsImage().->saveINode2Image().

    } else {   // write directory inode
      out.writeShort(0);  // replication

Because  the directory doesn't have replication factor, so  here is 0. This will cause loadFilesUnderConstruction() will not load any files when hadoop fisrt starts up after format."
HADOOP-4770,gridmix2 run script doesn't work on trunk,The order of arguments in the rungridmix_2 script is invalid after HADOOP-3986. 
HADOOP-4769,DataNode should validate the data integrity of a block before updating it,DataNode currently only validate the existence of the block and meta files before update.  The data integrity of a block should also be validated.
HADOOP-4768,Dynamic Priority Scheduler that allows queue shares to be controlled dynamically by a currency,"Dynamic (economic) priority scheduler based on work presented at the Hadoop User Group meeting in Santa Clara in September and the HadoopCamp in New Orleans in November 2008.

"
HADOOP-4761,Tool to give the block location and to check if the block at a given datanode is corrupt or not,"It would be useful if we could have a command line tool which would list out the location of all the replicas of a block given a block-id/filename. Also, an utility to check if the block at a given datanode is corrupt or not would be of great help in managing the cluster."
HADOOP-4760,HDFS streams should not throw exceptions when closed twice,"When adding an {{InputStream}} via {{addResource(InputStream)}} to a {{Configuration}} instance, if the stream is a HDFS stream the {{loadResource(..)}} method fails with {{IOException}} indicating that the stream has already been closed.
"
HADOOP-4759,HADOOP-4654 to be fixed for branches >= 0.19,Since HADOOP-4654 is fixed only for branch 18.3. This jira looks at the issue reported for 0.19 and above branches 
HADOOP-4758,Add a splitter for metrics contexts,It is currently not possible to configure multiple metrics contexts. It would be helpful if one could support more than one type of collector.
HADOOP-4756,Create a command line tool to access JMX exported properties from a NameNode server,Create a command line tool that will easy script access to JMX exported properties of the NameNode.
HADOOP-4753,gridmix2 code can be condensed,"The gridmix2 benchmark code, particularly GridMixRunner, contains a lot of duplication that can be condensed into a more manageable state."
HADOOP-4749,reducer should output input data size when shuffling is done,"Sometimes we see a single slow reducer because of the load balancing problem. This information will be very useful to understand how imbalanced the load is.

Should be easy to fix I guess, since reducer should have all information needed at the end of the shuffling phase.
"
HADOOP-4747,Reuse FileStatus in FsShell where possible,"FsShell should reuse FileStatus objects instead of converting to a Path and making extra calls to the backend FS (which can be slow and expensive).
"
HADOOP-4746,Job output directory should be normalized,JobClient should normalize the output directory by calling FileSystem#makeQualified() in order to avoid the problem described in HADOOP-4717.
HADOOP-4744,Wrong resolution of hostname and port ,"I noticed the following for one of the hosts in a cluster:

1. machines.jsp page resolves the http address as just ""http://hostname"" (which doesn't work). It doesnt put the port number for the host. Even if I add the port number manually in the URI, the  task tracker page does not come up. 
2. All the tasks(both maps and reduces) which ran on the machine ran successfully. But tasklogs cannot be viewed, because port-number is not resolved. ( same problem as in (1)).
3. The reducers waiting for maps ran on that machine fail with connection failed errors saying the hostname is 'null'.

"
HADOOP-4742,Mistake delete replica in hadoop 0.18.1,"We recently deployed a 0.18.1 cluster and did some test. And we found
if we corrupt a block, the namenode will find it and replicate it as soon as
a client read that block. However, the namenode will delete a health block
(the source of the above replication operation) at the same time, (I think this
issue may affect all 0.18 tree.)

Having did some trace, I find in FSNamesystem.addStoredBlock(), it will
check the number of replications after add the block to blocksMap:

 |   NumberReplicas num = countNodes(storedBlock);
 |    int numLiveReplicas = num.liveReplicas();
 |    int numCurrentReplica = numLiveReplicas
 |      + pendingReplications.getNumReplicas(block);

which means all the live replicas and pending replications will be
counted. But in the end of FSNamesystem.blockReceived(), which
calls the addStoredBlock(), it will call addStoredBlock() first, then
reduce the pendingReplications count.

 |    //
 |    // Modify the blocks->datanode map and node's map.
 |    //
 |    addStoredBlock(block, node, delHintNode );
 |    pendingReplications.remove(block);

Hence, the newly replicated replica will be counted twice, and then
will be marked as excess and lead to a mistake deletion.

I think change the counting lines in   blockReceived(), may solve this
issue:

--- FSNamesystem.java-orig      2008-11-28 13:34:40.000000000 +0800
+++ FSNamesystem.java   2008-11-28 13:54:12.000000000 +0800
@@ -3152,8 +3152,8 @@
    //
    // Modify the blocks->datanode map and node's map.
    //
-    addStoredBlock(block, node, delHintNode );
    pendingReplications.remove(block);
+    addStoredBlock(block, node, delHintNode );
  }

  long[] getStats() throws IOException {

The following is the logs for the mistake deletion, with additional
logging info inserted by me.

2008-11-28 11:22:08,866 INFO org.apache.hadoop.dfs.StateChange: *DIR*
NameNode.reportBadBlocks
2008-11-28 11:22:08,866 INFO org.apache.hadoop.dfs.StateChange: BLOCK
NameSystem.addToCorruptReplicasMap: blk_3828935579548953768 added as
corrupt on 192.168.33.51:50010 by /192.168.33.51
2008-11-28 11:22:10,179 INFO org.apache.hadoop.dfs.StateChange: BLOCK*
ask 192.168.33.50:50010 to replicate blk_3828935579548953768_1184 to
datanode(s) 192.168.33.45:50010
2008-11-28 11:22:12,629 INFO org.apache.hadoop.dfs.StateChange: BLOCK*
NameSystem.addStoredBlock: blockMap updated: 192.168.33.45:50010 is
added to blk_3828935579548953768_1184 size 67108864
2008-11-28 11:22:12,629 INFO org.apache.hadoop.dfs.StateChange: Wang
Xu* NameSystem.addStoredBlock: current replicas 4 in which has 1
pendings
2008-11-28 11:22:12,630 INFO org.apache.hadoop.dfs.StateChange: DIR*
NameSystem.invalidateBlock: blk_3828935579548953768_1184 on
192.168.33.51:50010
2008-11-28 11:22:12,630 INFO org.apache.hadoop.dfs.StateChange: BLOCK*
NameSystem.delete: blk_3828935579548953768 is added to invalidSet of
192.168.33.51:50010
2008-11-28 11:22:13,180 INFO org.apache.hadoop.dfs.StateChange: BLOCK*
ask 192.168.33.44:50010 to delete  blk_3828935579548953768_1184
2008-11-28 11:22:13,181 INFO org.apache.hadoop.dfs.StateChange: BLOCK*
ask 192.168.33.51:50010 to delete  blk_3828935579548953768_1184

"
HADOOP-4739,Minor enhancements to some sections of the Map/Reduce tutorial,"I found some grammatical/spelling mistakes in the following sections of the Hadoop Map/Reduce tutorial: 'Submitting Jobs to a Queue', 'Debugging', and 'Skipping Bad Records'. The writeups for these sections can also be enhanced a bit. "
HADOOP-4738,saveVersion.sh could write a package-info.java that cannot be compiled when used with git,"When git is being used, the saveVersion.sh script generates a revision based on git log as follows:
{{revision=`git log -1 --pretty=oneline`}}
This revision is used to generate the HadoopVersionAnnotation in the package-info file. If the revision string contains quotes this will result in a compile time error."
HADOOP-4737,"Job end notification should give status as ""KILLED"" when job gets killed.","Currently in Job End notification, there are only 2 statuses : SUCCEEDED or FAILED. But as we now have job end status as Killed also, job end notification should give status as ""KILLED"" when job gets killed."
HADOOP-4735,NameNode reporting 0 size for originally non-empty files,"NameNode reports 0 size for a handful of files that were non-empty originally.
The corresponding blocks on the DataNodes are non-empty.
NameNode must have reported correct size at some time, because applications that would have failed with 0 size files, executed successfully."
HADOOP-4734,Some lease recovery codes in 0.19 or trunk should also be committed in 0.18.,"In HADOOP-1700, there are codes for detecting corrupted blocks.

In HADOOP-4257, there are bug fixes for lease recovery."
HADOOP-4732, connectionTimeout and readTimeout are passed in wrong order in ReduceTask.java,"In ReduceTask.java, The parameters for method calls for getInputStream(URLConnection connection,  int connectionTimeout,  int readTimeout) should be in that order. Now it is
{code}
        InputStream input = getInputStream(connection, DEFAULT_READ_TIMEOUT, 
                                           STALLED_COPY_TIMEOUT);
{code}
It works now because both DEFAULT_READ_TIMEOUT and STALLED_COPY_TIMEOUT have the same value."
HADOOP-4731,Job is not removed from the waiting jobs queue upon completion.,"When a job completes, the {{JobQueueManager}} removes this job from it's internal structures, which is a queue of the running jobs and waiting jobs. While the removal from the running jobs happens correctly, the removal from the waiting jobs does not happen, if the job is still in the PREP state. This was found while implementing HADOOP-4035, as described [by Vinod here|https://issues.apache.org/jira/browse/HADOOP-4035?focusedCommentId=12650879#action_12650879]. The change was introduced in HADOOP-4471, which I should have caught in the review. Sorry about that !"
HADOOP-4729,Adding pig jobs to the becnchmark suit,"
Currently, we are using gridmix for benchmarking Hadoop.
One problem with the gridmix is that it dos not include pig jobs.

Since pig jobs has to work with a specific pig release which also is also tied to a specific Hadoop release,
we decide it does not make sense to add pig jobs to the existing gridmix.
It is better to create a separate package for pig jobs.
"
HADOOP-4728,Tests for NameNode -checkpoint option with different configurations ,"here are the tests:

id               :   secnn-4
Desc         :   Start SecondaryNameNode first before starting NameNode  	 
Expected 
results      :    Secondary namenode should fail to start

---------------------------------
id               :  secnn-6
Desc         :  Configure NameNode with dfs.name.dir and dfs.name.edits.dir pointing to different directories. Make sure the directories are empty.
                     Use default configuration for fs.checkpoint.dir and fs.checkpoint.edits.dir pointing to same directory.
                     Start NameNode using -importCheckpoint.
Expected
results      : 	Name-node should import image and edits into different directories. 

---------------------------------
id               :  secnn-7  	
Desc         :  Start NameNode.
                      Configure SecondaryNameNode with fs.checkpoint.dir and fs.checkpoint.edits.dir pointing to different directories.
                      Start SecondaryNameNode. 	
Expected
results       :  Secondary name-node should download edits and create the image in different directories. 
----------------------------

id                :  secnn-8
Desc          : Configure NameNode with dfs.name.dir and dfs.name.edits.dir pointing to different directories. Make sure the directories are empty.
                      Configure fs.checkpoint.dir and fs.checkpoint.edits.dir pointing to different (same as in secnn-7) directories.
                      Start NameNode using -importCheckpoint. 	
Expected
Results      : Name-node should import image and edits into different directories. "
HADOOP-4727,Groups do not work for fuse-dfs out of the box on 0.19.0,The groups functionality of fuse-dfs did not work for me in Hadoop 0.19.0.  Everything shows up as group nobody.
HADOOP-4726,"documentation typos: ""the the""","Try searching ""the the"" in the documentation files.  See http://issues.apache.org/jira/browse/HADOOP-4704?focusedCommentId=12649589#action_12649589 for a list."
HADOOP-4722,adding tests for quotas command line error  messages,"adding tests for quotas command line error messages.
Will add it to the TestCLI xml configuration"
HADOOP-4721,OOM in .TestSetupAndCleanupFailure,"The root cause may be my lifecycle changes, but I'm seeing an OOM in TestSetupAndCleanupFailure
"
HADOOP-4720,docs/api does not contain the hdfs directory after building,"When ant package is run, the build/docs/api folder is not reflecting the ""hdfs"" hierarchy of packages and classes.  org.apache.hadoop.hdfs is not getting created in the api folder. Instead, the build creates org.apache.hadoop.fs. 

The same can be observed here as well : http://hadoop.apache.org/core/docs/r0.19.0/api/index.html

"
HADOOP-4719,The ls shell command documentation is out-dated,"Current ls output is
{noformat}
bash-3.2$ ./bin/hadoop fs -ls  
Found 1 items
-rw-r--r--   3 tsz supergroup       1366 2008-11-24 16:58 /user/tsz/r.txt
{noformat}
but the doc says ""dirname <dir> modification_time modification_time permissions userid groupid"".  See http://hadoop.apache.org/core/docs/r0.18.2/hdfs_shell.html#ls"
HADOOP-4717,Removal of default port# in NameNode.getUri() cause a map/reduce job failed to prompt temporay output,"Problem reported here is that when the default port number (8020) is specified in the output, job succeeds but no output is created. The cause of the problem is that ""listStatus"" call drops the port number because NameNode.getUri removes the default port#.

Assuming that a map/reduce output directory is set to be ""hdfs://localhost:8020/out"", A call ""listStatus"" on any of its sub directory, for example, ""hdfs://localhost:8020/out/tempXX"", returns results like below: 

hdfs://localhost/out/tempXX/part-00005

Because of this, Task.java
    574   private Path getFinalPath(Path jobOutputDir, Path taskOutput) {
    575     URI relativePath = taskOutputPath.toUri().relativize(taskOutput.toUri());

does not get the correct relativePath because TaskOutputPath contain ports, but taskOutput doesn't.

It seems to me that the problem could be fixed if we make Path.makeQualified() to return the same path not matter the input path contains the default port or not. 
"
HADOOP-4716,testRestartWithLostTracker frequently times out,"This test frequently times out: org.apache.hadoop.mapred.TestJobTrackerRestartWithLostTracker.testRestartWithLostTracker
Example: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3637/testReport/org.apache.hadoop.mapred/TestJobTrackerRestartWithLostTracker/testRestartWithLostTracker/"
HADOOP-4715,Fix quickstart.html to reflect that Hadoop works with Java 1.6.x now,We should fix http://hadoop.apache.org/core/docs/current/quickstart.html to reflect that Hadoop requires 1.6.x now.
HADOOP-4714,map tasks timing out during merge phase,"With compression of transient data turned on some parts of the merge phase seem to not report progress enough.

We see a lot of task failures during the merge phase, most of them timing out (even with a 20 min timeout)"
HADOOP-4713,librecordio does not scale to large records,librecordio does not deserialize records containing strings larger than 64k bytes. 
HADOOP-4710,"Chukwa - Add duplicate detection, and implement virtual offset of the log file to checkpoint file","Each data stream has been sent to Chukwa with sequence id, and this sequence id is used as the guide line for tracking duplicate chunk data in Chukwa.  However, the check point file does not include the virtual offset.  This means when collector crashed, sequence id is reset to zero.  Chukwa Agent needs to keep track of the sequence id in the check point file in order to recover from a crash."
HADOOP-4708,Add support for dfsadmin commands for test TestCLI unit test,"Curretly TestCLI assumes dfs command when describing tests in testConf.xml
we need to add ability to run dfsadmin commands too."
HADOOP-4706,IFileOutputStream.close should call close of the underlying stream,"Currently the IFileOutputStream.close just creates the checksum of the data written, but does not call the close of the underlying stream. Ideally the checksum calculation should be handled in a different finish() function and the close should close the underlying stream"
HADOOP-4705,[HOD] Grant read permissions for files/directories created by hod as part of provisioning hadoop clusters,"When HOD creates the log, work and temp directories for Hadoop, it creates them with permissions 0700. This is too restrictive for applications like Chukwa which need access to files in this directory for generating Hadoop metrics. This is a request to allow read permissions to other applications."
HADOOP-4704,"javadoc: ""the the"" => ""the""","Try searching ""the the"" in .java files."
HADOOP-4703,DataNode.createInterDataNodeProtocolProxy should not wait for proxy forever while recovering lease,The problem is that the list of DataNodes may contain DataNode that has already shutdown as the case in TestFileCreationClient.testClientTriggeredLeaseRecovery(). Using waitForProxy causes the DataNode to wait forever for the dead DataNode.
HADOOP-4702,Failed block replication leaves an incomplete block in receiver's tmp data directory,"When a failure occurs while replicating a block from a source DataNode to a target DataNode, the target node keeps an incomplete on-disk copy of the block in its temp data directory and an in-memory copy of the block in ongoingCreates queue. This causes two problems:
1. Since this block is not (should not) be finalized, NameNode is not aware of the existence of this incomplete block. It may schedule replicating the same block to this node again, which will fail with a message: ""Block XX has already been started (though not completed), and thus cannot be created.""
2. Restarting the datanode moves the blocks under the temp data directory to be valid blocks, thus introduces corrupted blocks into HDFS. Sometimes those corrupted blocks stay in the system undetected if it happens that the partial block and its checksums match.

A failed block replication should clean up both the in-memory & on-disk copies of the incomplete block."
HADOOP-4699,"Change TaskTracker.MapOutputServlet to send only the IFile segment, validate checksum in Reduce","Instead of validating the checksum of the IFile segment in MapOutputServlet, validation may be left to the reduce. While failures may not be detected until late in the reduce, the throughput and CPU improvements should make up for it in the average case."
HADOOP-4698,TestMapRed fails with 64bit JDK,2 test cases in TestMapRed fail when run with 64bit JDK.  They both run out of memory.  This is due to the default value of io.sort.mb being used (100mb).
HADOOP-4697,KFS::getBlockLocations() fails with files having multiple blocks,"getBlockLocations() on KFS fail with the following stack trace for large files (with multiple blocks).
{noformat}
 java.lang.IllegalArgumentException: Offset 67108864 is outside of file
 (0..67108863)
        at
 org.apache.hadoop.mapred.FileInputFormat.getBlockIndex(FileInputFormat.java:336)
        at
 org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:248)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)
        at org.apache.hadoop.examples.WordCount.run(WordCount.java:149)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.examples.WordCount.main(WordCount.java:155)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
 sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at
 sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at
 org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:141)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:54)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

blkStart was not updated properly."
HADOOP-4695,TestGlobalFilter.testServletFilter fails,"{noformat}
junit.framework.AssertionFailedError: expected:<14> but was:<15>
	at org.apache.hadoop.http.TestGlobalFilter.testServletFilter(TestGlobalFilter.java:150)
{noformat}

For more details, see http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/666/ ."
HADOOP-4692, Namenode in infinite loop for replicating/deleting corrupted block,"Our cluster has an under-replicated block with only one replica, assuming its block id is B. NameNode log shows that NameNode is in an infinite loop replicating/deleting the block.

INFO org.apache.hadoop.dfs.StateChange: BLOCK* ask DN1 to replicate blk_B to datanode(s) DN2, DN3
WARN org.apache.hadoop.fs.FSNamesystem: Inconsistent size for block blk_B reported from DN2  current size is 134217728 reported size is 134205440
WARN org.apache.hadoop.fs.FSNamesystem: Deleting block blk_B from DN2
INFO org.apache.hadoop.dfs.StateChange: DIR* NameSystem.invalidateBlock: blk_B on DN2
INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.delete: blk_B is added to invalidSet of DN2
INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: DN2 is added to blk_B size 134217728
WARN org.apache.hadoop.fs.FSNamesystem: Inconsistent size for block blk_-B reported from DN3 current size is 134217728 reported size is 134205440
WARN org.apache.hadoop.fs.FSNamesystem: Deleting block blk_B from DN3
INFO org.apache.hadoop.dfs.StateChange: DIR* NameSystem.invalidateBlock: blk_B on DN3
INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.delete: blk_B is added to invalidSet of DN3
INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: DN3 is added to blk_B size 134217728
INFO org.apache.hadoop.dfs.StateChange: BLOCK* ask DN1 to replicate blk_B  to datanode(s) DN4, DN5
...
"
HADOOP-4691,"In the javadoc of IndexedSortable.compare(...), the link is wrong.",Class java.util.Comparable does not exist.
HADOOP-4690,fuse-dfs - create source file/function + utils + config + main source files,"this will make development easier as all patches don't have to be completely serialized and generally better organizationally. 

"
HADOOP-4688,multiple spills/reducers test case for mergeParts() needed,"The first  patch version (incorrect) for http://issues.apache.org/jira/browse/HADOOP-4614 exposed a problem that existing tests do not exercise some code paths of MapOutputBuffer::mergeParts().  In particular, we need a test that would validate output when numSpills >1 and partitions >1.

This might be a test that has more than 1 reducer and forces multiple first level (map) spills by setting *io.sort.mb* (and possibly *io.sort.spill.percent*) to small values."
HADOOP-4684,"NativeS3FileSystem always tries to create a bucket, even when used for read-only workflows.","I'm running two types of workflow. The first is a set of map jobs that read from S3 using s3n://ID:SECRET@bucket/ as input and HDFS as output. The other is using distcp as a backup tool to copy the outputs of other jobs using distcp.


bq. hadoop distcp hdfs://host:50001/user/root/from/ s3n://ID:SECRET@backup-bucket/

Unfortunately, I'm getting too many failures of this kind:

{code}
08/11/19 16:08:27 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 PUT failed for '/' XML Error Message: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>OperationAborted</Code><Message>A conflicting conditional operation is currently in progress against this resource. Please try again.</Message><RequestId>324E696A4BCA8731</RequestId><HostId>{REMOVED}</HostId></Error>
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.createBucket(Jets3tNativeFileSystemStore.java:74)
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.initialize(Jets3tNativeFileSystemStore.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at org.apache.hadoop.fs.s3native.$Proxy2.initialize(Unknown Source)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:215)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1339)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:56)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1351)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:213)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:158)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:210)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)
	at org.apache.hadoop.streaming.StreamJob.submitAndMonitorJob(StreamJob.java:925)
	at org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:115)
	at org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
	at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)
Caused by: org.jets3t.service.S3ServiceException: S3 PUT failed for '/' XML Error Message: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>OperationAborted</Code><Message>A conflicting conditional operation is currently in progress against this resource. Please try again.</Message><RequestId>{REMOVED}</RequestId><HostId>{REMOVED}</HostId></Error>
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:424)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestPut(RestS3Service.java:734)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.createObjectImpl(RestS3Service.java:1357)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.createBucketImpl(RestS3Service.java:1234)
	at org.jets3t.service.S3Service.createBucket(S3Service.java:1390)
	at org.jets3t.service.S3Service.createBucket(S3Service.java:1158)
	at org.jets3t.service.S3Service.createBucket(S3Service.java:1177)
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.createBucket(Jets3tNativeFileSystemStore.java:69)
	... 29 more
{code}

The issue is that Jets3tNativeFileSystemStore always tries to create a bucket during initialization. I'm sure that's a good thing for when s3n is used to write output, but for read operations, it'd be best if it checked for existence first. IMHO."
HADOOP-4683,Move the call to getMapCompletionEvents in ReduceTask.ReduceCopier.fetchOutputs to a separate thread,The method ReduceTask.ReduceCopier.fetchOutputs makes a call to getMapCompletionEvents every iteration of the loop. This should be moved out to a separate thread. This might slow down the shuffle scheduler in some cases since there is a sleep inside the getMapCompletionEvents method.
HADOOP-4679,"Datanode prints tons of log messages: Waiting for threadgroup to exit, active theads is XX","When a data receiver thread sees a disk error, it immediately calls shutdown to shutdown DataNode. But the shutdown method does not return before all data receiver threads exit, which will never happen. Therefore the DataNode gets into a dead/live lock state, emitting tons of log messages: Waiting for threadgroup to exit, active threads is XX."
HADOOP-4677,FileSystem.getFileBlockLocations() (aka default implementation for Local FileSystem) incorrect.,"The default implementation of FileSystem.getFileBlockLocations(FileStatus file, long start, long len) seems to be wrong.
"
HADOOP-4676,Go back to Job has a wrong URL on blacklisted trackers page,Go back Job link has wrong URL on jobblacklistedtrackers.jsp page on web UI
HADOOP-4675,Current Ganglia metrics implementation is incompatible with Ganglia 3.1,Ganglia changed its wire protocol in the 3.1.x series; the current implementation only works for 3.0.x.
HADOOP-4673, IFile.Writer close() uses compressor after returning it to CodecPool.,"The problem is of the same nature as HADOOP-4195.

The compression codec is returned to the CodecPool, and later is finished in ""out.close()"".

{code:title=IFile.java|borderStyle=solid}

    public void close() throws IOException {
      // Close the serializers
      keySerializer.close();
      valueSerializer.close();

      // Write EOF_MARKER for key/value length
      WritableUtils.writeVInt(out, EOF_MARKER);
      WritableUtils.writeVInt(out, EOF_MARKER);
      decompressedBytesWritten += 2 * WritableUtils.getVIntSize(EOF_MARKER);
      
      if (compressOutput) {
        // Flush data from buffers into the compressor
        out.flush();
        
        // Flush & return the compressor
        compressedOut.finish();
        compressedOut.resetState();
        CodecPool.returnCompressor(compressor);
        compressor = null;
      }

      // Close the stream
      rawOut.flush();
      compressedBytesWritten = rawOut.getPos() - start;

      // Close the underlying stream iff we own it...
      if (ownOutputStream) {
        out.close();
      }
      out = null;
    }

{code}
"
HADOOP-4671,"Make some ""flag"" booleans volatile because they are set/read in different threads","Peter Veentjer	has identified the following variables as being tracked in a loop without assignments or synchronization.

AgentControllerSocketListener.closing
HttpConnector.stopMe
IndexUpdateReducer.closed
SecundaryNodeName.shouldRun
voTask.hasNext
"
HADOOP-4669,A file-open sometimes may not set the access time of a file,"The access time of a file may not be set even when the file is opened for read. This is because FileSystem.getBlockLocationsInternal may not correctly evaluate the condition that checks whether ""access time"" is enabled or not."
HADOOP-4668,Improve JavaDoc on JobConf.setCombinerClass to better document restrictions on combiners,"It is often not clear to developers that Combiners may be called 0, 1, or many times and that combiners input and output types must be the same and the combiner must not have side effects. Since there isn't a Combiner class, the best place to put this documentation is on JobConf.setCombinerClass."
HADOOP-4666,Launch reduces only after a few maps have run in the Fair Scheduler,"It makes no sense to schedule reduces for a job before its maps have started running. As an initial fix, we will wait until a certain percent have run (likely 10%). In the future it would be good to choose the time to wait based on amount of map output data as well - launching reducers that will mostly be idle is not helpful. Average amount of map output bytes per mapper is easy to compute using counters in JobInProgress."
HADOOP-4664,Parallelize job initialization,"The job init thread currently initializes one job at a time. However, this is a lengthy and partly IO-bound process because all of the job's block locations need to be resolved through the namenode and a map of them needs to be built. It can take tens of seconds. As a result, the cluster sometimes initializes jobs too slowly for full utilization to be achieved, if there are many small jobs queued up. It would be better to have a pool of threads that initialize multiple jobs in parallel. One thing to be careful of, however, is not causing deadlocks or holding locks for too long in these threads."
HADOOP-4661,"distch: a tool for distributed ch{mod,own}","As suggested in HADOOP-3052, a tool to do distributed ch{mod,own} is very useful for large clusters."
HADOOP-4660,JobTracker wont compile on Java5 as you cant use @Override with an interface implementation in Java5,"Unless there is a java6+ only policy, the bits of JobTracker that declare they @Override the JobSubmissionProtocol need to have those @Override attributes commented out, as Java5 wont let you override interface methods, only class methods (strange but true)"
HADOOP-4659,Root cause of connection failure is being lost to code that uses it for delaying startup,"ipc.Client the root cause of a connection failure is being lost as the exception is wrapped, hence the outside code, the one that looks for that root cause, isn't working as expected. The results is you can't bring up a task tracker before job tracker, and probably the same for a datanode before a  namenode. The change that triggered this is not yet located, I had thought it was HADOOP-3844 but I no longer believe this is the case."
HADOOP-4656,Add a user to groups mapping service ,Currently the IPC client sends the UGI which contains the user/group information for the Server. However this represents the groups for the user on the client-end. The more pertinent mapping from user to groups is actually the one seen by the Server. Hence the client should only send the user and we should add a 'group mapping service' so that the Server can query it for the mapping.
HADOOP-4655,FileSystem.CACHE should be ref-counted,"FileSystem.CACHE is not ref-counted, and could lead to resource leakage."
HADOOP-4654,remove temporary output directory of failed tasks,"When dfs is getting full (80+% of reserved space), the rate of write failures increases, such that more map-reduce tasks can fail. By not cleaning up the temporary output directory of tasks the situation worsens over the lifetime of a job, increasing the probability of the whole job failing."
HADOOP-4650,"local.cache.size is set to 10 GB, while DEFAULT_CACHE_SIZE is set to 1 MB","In filecache.DistributedCache.java, the constant DEFAULT_CACHE_SIZE is set to 1 MB, while the value of local.cache.size in conf/hadoop-default.xml is 10 GB. I'm assuming these two values should be consistent, and that 10 GB is the appropriate value.
"
HADOOP-4649,Improve abstraction for spill indices,"In support of changing checksum handling as part of the migration to Jetty6, some of the spill code would be easier to reason about with a different abstraction."
HADOOP-4648,Remove ChecksumDistriubtedFileSystem and InMemoryFileSystem,We should remove the obsolete Checksum FileSystems.
HADOOP-4647,NamenodeFsck creates a new DFSClient but never closes it,"In NamenodeFsck.lostFoundMove(FileStatus file, LocatedBlocks blocks), a new DFSClient is created but never closed."
HADOOP-4645,hdfs proxy doesn't package itself right.,"Currently ""ant clean package"" creates a directory structure like:

ls build/hadoop-0.20-dev/contrib/hdfsproxy:

classes/
examples/
hdfsproxy-1.0/
hdfsproxy-1.0.jar
test/

and the better structure would copy just the hdfsproxy-1.0 into the hadoop package.
"
HADOOP-4643,NameNode should exclude excessive replicas when counting live replicas for a block,"Currently NameNode include excessive replicas in blockMap and count them as live replicas. Although excessive replicas have marked as invalid, scheduling deletion may be delayed and also datanode does not send deletion confirmation until the next block report. As a result, excessive replicas may stay in blocksMap for quite a while. This may cause underReplicated blocks undetected in NameNode. 

For example, assume that block b is at datanode d1, d2, and d3. We have the following scenario
1. d1 loses heartbeat, NN will replicate b to another datanode, assuming it is d4. 
2. d1 comes back. NN finds out b is over-replicated therefore choosing one replica, assuming d4, as a excessive replica and marking it as invalid. Now b has 3 valid replicas d1, d2, d3 and 1 excessive (invalid) replica d4, all in blocksMap.
3. d2 loses heartbeat. d2 gets removed from blocksMap. Block b has 2 valid replicas d1 and d3, and 1 excessive invalid replica d4. Block b is under-replicated; But NN still counts block b has 3 live replicas so does not take any action to replicate block b."
HADOOP-4642,Send JobConf and JobHistory data to MetricsContext,"JobConf and JobHistory data contain rich information of the mapreduce job.  However, the job information are only available temporarily until they are expired by JobTracker.  This ticket is to request the information to be cloned to MetricsContext.  This information can be archived by external system that interface with Hadoop, such as Chukwa and Ganglia."
HADOOP-4641,Send JobConf and JobHistory data to MetricsContext,"JobConf and JobHistory data contain rich information of the mapreduce job.  However, the job information are only available temporarily until they are expired by JobTracker.  This ticket is to request the information to be cloned to MetricsContext.  This information can be archived by external system that interface with Hadoop, such as Chukwa and Ganglia."
HADOOP-4640,Add ability to split text files compressed with lzo,"Right now any file compressed with lzop will be processed by one mapper. This is a shame since the lzo algorithm would be very suitable for large log files and similar common hadoop data sets. The compression rate is not the best out there but the decompression speed is amazing.  Since lzo writes compressed data in blocks it would be possible to make an input format that can split the files.
"
HADOOP-4638,Exception thrown in/from RecoveryManager.recover() should be caught and handled,"{{RecoveryManager.recover()}} can throw an exception while recovering a job. Since the {{JobTracker}} calls {{RecoveryManager.recover()}} from {{offerService()}}, any failure in recovery will cause {{JobTracker}} to crash. Ideally the {{RecoveryManager}} should log the failure encountered while recovering the job and continue."
HADOOP-4635,Memory leak ?,"I am running a process that needs to crawl a tree structure containing ~10K images, copy the images to the local disk, process these images, and copy them back to HDFS.

My problem is the following : after about 10h of processing, the processes crash, complaining about a std::bad_alloc exception (I use hadoop pipes to run existing software). When running fuse_dfs in debug mode, I get an outOfMemoryError, telling that there is no more room in the heap.

While the process is running, using top or ps, I notice that fuse is using up an increasing amount of memory, until some limit is reached. At that point , the memory used is oscillating. I suppose that this is due to the use of the virtual memory.

This leads me to the conclusion that there is some memory leak in fuse_dfs, since the only other programs running are Hadoop and the existing software, both thoroughly tested in the past.

My problem is that my knowledge concerning memory leak tracking is rather limited, so I will need some instructions to get more insight concerning this issue.

Thank you"
HADOOP-4634,220 javac compiler warnings,"""ant test-patch"" on trunk for a zero size patch file.  Then, it said,
{noformat}
    [exec]     -1 javac.  The applied patch generated 220 javac compiler warnings (more than the trunk's current 1011 warnings).
{noformat} "
HADOOP-4633,Streaming exits with zero-status in cases even on failure,"If a streaming job fails in the map/reduce phase, streaming exits with a non-zero status. However, if it fails before the map-reduce started (such as output path already exists), streaming still exits with a zero status. 
"
HADOOP-4632,TestJobHistoryVersion should not create directory in current dir.,"TestJobHistoryVersion creates a directory,  test-history-version, in the current dierctory. It should be created in test.build.data directory."
HADOOP-4631,Split the default configurations into 3 parts,"We need to split hadoop-default.xml into core-default.xml, hdfs-default.xml and mapreduce-default.xml. That will enable us to split the project into 3 parts that have the defaults distributed with each component."
HADOOP-4629,libhdfs sets read return code to 0 when hadoop returns < 0 ,"libhdfs is changing the return code to 0 whenever hadoop returns < 0 on a read.  This means EOF cannot be determined correctly since it should have a return code of -1.
"
HADOOP-4628,Move Hive out of Core,Make Hive a standalone subproject.
HADOOP-4627,gridmix version 2,"The new gridmix differs from the original gridmix in the following ways:

1. Use an xml config file to specify the types and sizes mix of a mix load. This provides better granularity control.
2. Use JobControl to submit gridmix load, instead of shell script.
3. Include Pig jobs.
"
HADOOP-4623,Running tasks are not maintained by JobInProgress if speculation is off,"{{JobInProgress}} doesnt maintain any structure for running tasks if speculation is turned _off_.  {{getRunningMapCache()}} in {{JobInProgress}} exposes the running map cache. This api returns an empty {{Map}} if speculation turned off. 

_Usage_ :
{{CapicityScheduler}} requires a list of running tasks for both speculated and non-speculated jobs. See HADOOP-4558 to see how this issue affects {{CapacityScheduler}}."
HADOOP-4622,Explicitly specify interpretor for non-native binaries,"I'm using pipes from Mono, and while one could dick around with the kernel (binfmt) to make .exe's executable, it would be nice to be able to specify an interpretor (and maybe even other flags for it) for one's executables."
HADOOP-4621,javadoc: warning - Multiple sources of package comments found for some packages,"ant javadoc
{noformat}
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging""
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging.impl""
{noformat}
"
HADOOP-4620,Streaming mapper never completes if the mapper does not write to stdout,"A mapper of a streaming job has empty input data and thus it produces no output.
The task never completes.

The following are the last two lines from the task log:
2008-11-07 21:59:48,254 INFO org.apache.hadoop.streaming.PipeMapRed: PipeMapRed exec [/usr/bin/perl, xxx]
2008-11-07 21:59:48,330 INFO org.apache.hadoop.streaming.PipeMapRed: mapRedFinished
 "
HADOOP-4618,Move http server from FSNamesystem into NameNode.,"NameNode is responsible now for starting its RPC server. The (web UI) http server is started inside FSNamesystem class.
I think it should be the NameNode's responsibility to deal with all issues intended for exposing information it holds to the outside world. This should include the RCP server as well as the http server.
And FSNamesystem class should be a logical container of internal namespace data-structures and in general should not be accessed directly from the outside."
HADOOP-4616,assertion makes fuse-dfs exit when reading incomplete data,"When trying to read a file that is corrupt on HDFS (registered by the namenode, but part of the data is missing on the datanodes), some of the assertions in dfs_read fail, causing the program to abort. This makes it  impossible to access the mounted partition until it is mounted again.

A simple way to reproduce this bug is to remove enough datanodes to have part of the data missing, and to read each file listed in HDFS.

this is the assertion that fails (fuse_dfs.c:903) : assert(bufferReadIndex >= 0 && bufferReadIndex < fh->bufferSize);

The expected behaviour would be to return either no file or a corrupt file, but continue working afterward.

removing the assertion seems to work for now, but a special behaviour is probably needed to handle this particular problem correctly."
HADOOP-4615,Job submitted to an invalid queue still remains with the jobtracker,"If a job is submitted to an invalid queue, {{JobQueuesManager}} ignores it but {{Jobtracker}} still considers the job as waiting."
HADOOP-4614,"""Too many open files"" error while processing a large gzip file","I am running a simple word count program on a gzip compressed data of size 4 GB (Uncompressed size is about 7 GB).  I have setup of 17 nodes in my Hadoop cluster.  After some time, I get the following exception:

java.io.FileNotFoundException: /usr/local/hadoop/hadoop-hadoop/mapred/local/taskTracker/jobcache/job_200811041109_0003/attempt_200811041109_0003_m_000000_0/output/spill4055.out.index
(Too many open files)
       at java.io.FileInputStream.open(Native Method)
       at java.io.FileInputStream.(FileInputStream.java:137)
       at org.apache.hadoop.fs.RawLocalFileSystem$TrackingFileInputStream.(RawLocalFileSystem.java:62)
       at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.(RawLocalFileSystem.java:98)
       at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:168)
       at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:359)
       at org.apache.hadoop.mapred.IndexRecord.readIndexFile(IndexRecord.java:47)
       at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.getIndexInformation(MapTask.java:1339)
       at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1237)
       at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:857)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:333)
       at org.apache.hadoop.mapred.Child.main(Child.java:155)

From a user's perspective I know that Hadoop will use only one mapper for a gzipped file.  The above exception suggests that probably Hadoop puts the intermediate data into many files.  But the question is that ""exactly how many open files are required in the worst case for any data size and cluster size?""  Currently it looks as if Hadoop needs more number of open files as the size of input or the cluster size (in terms of nodes, mapper, reducers) increases.  This is not plausible as far as scalability is concerned.  A user needs to write some number in the /etc/security/limits.conf file that how many open files are allowed by hadoop node.  The question is what that ""magical number"" should be?

So probably the best solution to this problem is to change Hadoop such a way that it can work with some moderate number of allowed open files (e.g. 4 K) or any other number should be suggested as an upper limit such that a user is sure that for any data size and cluster size, hadoop will not run into this ""too many open files"" issue."
HADOOP-4613,"browseBlock.jsp does not generate ""genstamp"" property.","A link to a block replica display page on the web UI breaks with the following message: ""Invalid input (genstamp absent)"".
The reason is that browsBlock.jsp sets ""genstamp="" in the code two times out of three. 
One spot in line #148 was missed by HADOOP-2656."
HADOOP-4612,Remove RunJar's dependency on JobClient,
HADOOP-4610,Always calculate mis-replicated blocks when safe-mode is turned off.,"After HADOOP-4597 there is one last case when mis-replicated blocks are not processed. It is when the safe-mode {{extension = 0}}.
We should regenerate {{neededReplications}} and {{excessReplicateMap}} whenever the safe mode is turned off.
"
HADOOP-4609,Some log message of INFO level seems unuseful.,"When I tried to run a PIEstimator, I recieved a below log message of INFO level.
Should we throw it to user?

----
08/11/07 10:12:05 INFO fs.FSInputChecker: java.io.IOException: Checksum ok was sent and should not be sent again
        at org.apache.hadoop.dfs.DFSClient$BlockReader.read(DFSClient.java:863)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1392)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1428)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:64)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:102)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1933)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1833)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1879)
        at org.apache.hadoop.examples.PiEstimator.launch(PiEstimator.java:220)
        at org.apache.hadoop.examples.PiEstimator.run(PiEstimator.java:245)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.examples.PiEstimator.main(PiEstimator.java:252)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:141)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:54)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)

Estimated value of PI is 3.0
hadoop@vldb ~/hadoop $ 
"
HADOOP-4608,Examples -Driver does not check first argument.,"hadoop@vldb ~/hadoop $ bin/hadoop jar hadoop-0.18.2-examples.jar 
An example program must be given as the first argument.
Valid program names are:
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
  multifilewc: A job that counts words from several files.
  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
  pi: A map/reduce program that estimates Pi using monte-carlo method.
  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
  randomwriter: A map/reduce program that writes 10GB of random data per node.
  sleep: A job that sleeps at each map and reduce task.
  sort: A map/reduce program that sorts the data written by the random writer.
  sudoku: A sudoku solver.
  wordcount: A map/reduce program that counts the words in the input files.
java.lang.IllegalArgumentException: An example program must be given as the first argument.
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:123)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:54)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)
"
HADOOP-4606,There is a cygpath error if log directory does not exist,"{noformat}
//d:\@sze\hadoop\latest\logs does not exist.

bash-3.2$ ./bin/hadoop fs -lsr /
cygpath: cannot create short name of d:\@sze\hadoop\latest\logs
...
{noformat}"
HADOOP-4599,BlocksMap and BlockInfo should be package private.,"BlocksMap and BlockInfo should be changed from public to package private.
This is possible now since HADOOP-4572 moved the test class CreateEditsLog under the namenode package."
HADOOP-4598,'-setrep' command skips under-replicated blocks,"When blocks are under-replicated due to some hadoop bugs, like HADOOP-4597, calling 

hadoop dfs  -setrep <higher_replication>  <filename>

would not schedule the replication for the under-replicated blocks (unless it goes to a different priority queue).

"
HADOOP-4597,Under-replicated blocks are not calculated if the name-node is forced out of safe-mode.,"Currently during name-node startup under-replicated blocks are not added to the neededReplications queue until the name-node leaves safe mode. This is an optimization since otherwise all blocks will first go into the under-replicated queue and then most of them will be removed from it.
When the name-node leaves safe-mode automatically it checks all blocks to have a correct number of replicas ({{processMisReplicatedBlocks()}}). 
When the name-node leaves safe-mode manually it does not perform the checkup.
In the latter case all under-replicated blocks remain not replicated forever because there is no alternative mechanism to trigger replications.
The proposal is to call {{processMisReplicatedBlocks()}} any time the name-node leaves safe mode - automatically or manually.
In addition to solving that problem this could be an alternative mechanism for refreshing {{neededReplications}} and {{excessReplicateMap}} sets."
HADOOP-4595,"JVM Reuse triggers RuntimeException(""Invalid state"")","A Reducer triggers the following exception:

08/11/05 08:58:50 INFO mapred.JobClient: Task Id : attempt_200811040110_0230_r_000008_1, Status : FAILED
java.lang.RuntimeException: Inconsistent state!!! JVM Manager reached an unstable state while reaping a JVM for task: attempt_200811040110_0230_r_000008_1 Number of active JVMs:2
 JVMId jvm_200811040110_0230_r_-735233075 #Tasks ran: 0 Currently busy? true Currently running: attempt_200811040110_0230_r_000012_0
 JVMId jvm_200811040110_0230_r_-1716942642 #Tasks ran: 0 Currently busy? true Currently running: attempt_200811040110_0230_r_000040_0
   at java.lang.Throwable.<init>(Throwable.java:67)
   at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:245)
   at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:113)
   at org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:78)
   at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:410) 

Other clues:

In the three reduce task attempts where this was observed, this was attempt _1. Attempt _0 had started and eventually switches to ""SUCCEEDED."" So I think this is happening only on speculatively-executed reduce task attempts. The reduce output (part-XXXXX) gets lost when this attempt fails, even though the other (earlier) attempt succeeded.
"
HADOOP-4589,Fix the PiEstimator output messages and code comments,There are bugs in the PiEstimator output messages and the code comments are insufficient for beginners learning map/reduce programs.
HADOOP-4587,typo in javadoc for map.input.file,"One user pinged me that inside the Mapper javadoc

{noformat}
 83  *       public void configure(JobConf job) {
 84  *         mapTaskId = job.get(""mapred.task.id"");
 85  *         inputFile = job.get(""mapred.input.file"");
 86  *       }
{noformat}

inputFile should be *map*.input.file and not *mapred*.input.file."
HADOOP-4585,unused and misleading configuration in hadoop-init,"src/contrib/ec2/bin/image/hadoop-init is appended to rc.local on all
ec2 cluster boxes.  This shell script generates the hadoop-site.xml
configuration file.  It starts with some default settings, which are
used to populate the file.  These defaults are then overwritten by the
user data (from hadoop-ec2-env.sh) passed to the EC2 instance by
launch-hadoop-master and launch-hadoop-slaves.

This isn't a bug; setting variables in hadoop-ec2-env.sh does the
right thing.  However, it's dead and misleading code (well, it misled
me) and running a test Hadoop job to figure out what's going on takes
a little effort.

Suggested change to hadoop-init:

Remove these lines:

{noformat}
# set defaults
MAX_TASKS=3
[ ""$INSTANCE_TYPE"" == ""m1.large"" ] && MAX_TASKS=6
[ ""$INSTANCE_TYPE"" == ""m1.xlarge"" ] && MAX_TASKS=12

MAX_MAP_TASKS=$MAX_TASKS
MAX_REDUCE_TASKS=$MAX_TASKS
{noformat}

Add a comment before the lines which access the user data:

{noformat}
# get user data passed in by the ec2 instance launch
wget -q -O - http://169.254.169.254/latest/user-data | tr ',' '\n' > /tmp/user-data
source /tmp/user-data
{noformat}
"
HADOOP-4584,Slow generation of blockReport at DataNode causes delay of sending heartbeat to NameNode,"sometimes due to disk or some other problems, datanode takes minutes or tens of minutes to generate a block report. It causes the datanode not able to send heartbeat to NameNode every 3 seconds. In the worst case, it makes NameNode to detect a lost heartbeat and wrongly decide that the datanode is dead.

It would be nice to have two threads instead. One thread is for scanning data directories and generating block report, and executes the requests sent by NameNode; Another thread is for sending heartbeats, block reports, and picking up the requests from NameNode. By having these two threads, the sending of heartbeats will not get delayed by any slow block report or slow execution of NameNode requests."
HADOOP-4583,Code optimization/cleanup in HDFS,"Some of the changes needed:
- FSNameSystem.allocateBlock creates a lot of object unnecessarily to find a blockID that is unused. It should reuse the Block object created by changing the blockID and reusing the GenerationStamp.
- InodeFile.addBlock should use System.arrayCopy instead of copying 1 at a time.
- Some of the for loops in FSEditLog.java can be optimized. Instead of getting the edit stream length in termination condition, a local variable should be used to do the same.
"
HADOOP-4579,StorageDirectory is not removed from the storage directories list when writing to an edits fails,"If there is an IO Error on any log operations the directory should be removed from the list of active storage directories.
when it happens function FSEditLog:processIOError() is called.

in this function:
....
File parentStorageDir = ((EditLogFileOutputStream)editStreams.get(index)).getFile().getParentFile().getParentFile();           --  which returns root 
....
fsimage.processIOErrors(parentStorageDir);
...

which calles FSImage:ProcessIOErrors(dir).
but when dir is compared against list of storage dirs getParent is used instead of getPath() 
so the match is never found.
....
 if (sd.getRoot().getPath().equals(dirName.getParent())) {                       - should be dirName.getPath()
.....
"
HADOOP-4576,Modify pending tasks count in the UI to pending jobs count in the UI,"The UI for capacity scheduler displays 'pending tasks' counts. However the capacity scheduler does not update these counts to be the actual values for optimization purposes, for e.g. to avoid walking all pending jobs on all heartbeats. Hence this information is not very accurate.

Also, while 'running tasks' counts are useful to compare against capacities and limits, 'pending tasks' counts do not add too much user value. A better count to display would be the number of running and pending jobs."
HADOOP-4575,An independent HTTPS proxy for HDFS,"Currently, an HDFS cluster itself exposes an HTTP/HTTPS interface for reading files (i.e., HFTP/HSFTP). However, there is a need for an HTTPS proxy server running independently from the cluster and serving data to users who otherwise can't access the cluster directly. This proxy will authenticate its users via SSL certificates and implements the HSFTP interface for reading files.  "
HADOOP-4572,INode and its sub-classes should be package private,"INode, INodeFile, INodeDirectory and INodeFileUnderConstruction should be changed from public to package private.  As a consequence, some test classes like CreateEditsLog have to be moved to org.apache.hadoop.hdfs.server.namenode."
HADOOP-4571,chukwa conf files are not cleaned up,"# ant
# svn status
{noformat}
?      src/contrib/chukwa/conf/initial_adaptors
?      src/contrib/chukwa/conf/alert.conf
?      src/contrib/chukwa/conf/chukwa-slaves
{noformat}
# ant clean
# svn status
{noformat}
?      src/contrib/chukwa/conf/initial_adaptors
?      src/contrib/chukwa/conf/alert.conf
?      src/contrib/chukwa/conf/chukwa-slaves
{noformat}

Should these files/directories be removed by ""ant clean""?  Or add these items to the svn:ignore list?"
HADOOP-4567,GetFileBlockLocations should return the NetworkTopology information of the machines that hosts those blocks,MultiFileInputFormat and FileInputFormat should use block locality information to construct splits. 
HADOOP-4565,MultiFileInputSplit can use data locality information to create splits,The MultiFileInputFormat takes a set of paths and creates splits based on file sizes. Each splits contains a few files an each split are roughly equal in size. It would be efficient if we can extend this InputFormat to create splits such each all the blocks in one split and either node-local or rack-local.
HADOOP-4562,"Logs filled with ""IOException: Checksum ok was sent and should not be sent again""","Updating hbase to use 0.19.0RC0 or latest from branch-0.19, I see reams of this in logs:

{code}
2008-10-31 18:33:41,296 INFO org.apache.hadoop.fs.FSInputChecker: java.io.IOException: Checksum ok was sent and should not be sent again
        at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1064)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1613)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1663)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1590)
        at java.io.DataInputStream.readByte(DataInputStream.java:248)
        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:325)
        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:346)
        at org.apache.hadoop.io.Text.readString(Text.java:400)
        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1471)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1428)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1417)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1412)
        at org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:293)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$HbaseMapFile$HbaseReader.<init>(HStoreFile.java:632)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader.<init>(HStoreFile.java:714)
        at org.apache.hadoop.hbase.regionserver.HStoreFile.getReader(HStoreFile.java:413)
        at org.apache.hadoop.hbase.regionserver.HStore.<init>(HStore.java:262)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:1729)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:469)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateRegion(HRegionServer.java:1004)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:976)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:901)
        at java.lang.Thread.run(Thread.java:619)

2008-10-31 18:33:41,272 DEBUG org.apache.hadoop.hbase.regionserver.HStore: loaded /hbasetrunk/-ROOT-/70236052/info/info/1689673398714621203, isReference=false, sequence id=1
2008-10-31 18:33:41,274 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Loaded 1 file(s) in hstore 70236052/info, max sequence id 1
2008-10-31 18:33:41,296 INFO org.apache.hadoop.fs.FSInputChecker: java.io.IOException: Checksum ok was sent and should not be sent again
        at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1064)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1613)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1663)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1590)
        at java.io.DataInputStream.readByte(DataInputStream.java:248)
        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:325)
        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:346)
        at org.apache.hadoop.io.Text.readString(Text.java:400)
        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1471)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1428)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1417)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1412)
        at org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:293)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$HbaseMapFile$HbaseReader.<init>(HStoreFile.java:632)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader.<init>(HStoreFile.java:714)
        at org.apache.hadoop.hbase.regionserver.HStoreFile.getReader(HStoreFile.java:413)
        at org.apache.hadoop.hbase.regionserver.HStore.<init>(HStore.java:262)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:1729)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:469)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateRegion(HRegionServer.java:1004)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:976)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:901)
        at java.lang.Thread.run(Thread.java:619)

2008-10-31 18:33:41,298 INFO org.apache.hadoop.fs.FSInputChecker: java.io.IOException: Checksum ok was sent and should not be sent again
        at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1064)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1613)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1663)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.io.Text.readString(Text.java:402)
        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1471)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1428)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1417)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1412)
        at org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:293)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$HbaseMapFile$HbaseReader.<init>(HStoreFile.java:632)
        at org.apache.hadoop.hbase.regionserver.HStoreFile$BloomFilterMapFile$Reader.<init>(HStoreFile.java:714)
        at org.apache.hadoop.hbase.regionserver.HStoreFile.getReader(HStoreFile.java:413)
        at org.apache.hadoop.hbase.regionserver.HStore.<init>(HStore.java:262)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:1729)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:469)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateRegion(HRegionServer.java:1004)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:976)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:901)
        at java.lang.Thread.run(Thread.java:619)
{code}

HBase is just opening a mapfile.

Here is from svn blame and history:

{code}
706798    hairong         if (sentChecksumOk) {
706798    hairong            // this should not happen; log the error for the debugging purpose
706798    hairong            LOG.info(StringUtils.stringifyException(new IOException(
708724    rangadi              ""Checksum ok was sent and should not be sent again"")));


r708724 | rangadi | 2008-10-28 16:33:40 -0700 (Tue, 28 Oct 2008) | 1 line

HADOOP-4499. DFSClient should invoke checksumOk only once. (Raghu Angadi)
------------------------------------------------------------------------
r706798 | hairong | 2008-10-21 15:19:07 -0700 (Tue, 21 Oct 2008) | 1 line

Merge -r 706795:706796 from trunk to main to move the change log of HADOOP-3914.
{code}

Code comment says this condition should never happen.

Looking at code, IIUC, we get this exception if we reread inside a block.

For now, I've marked it a blocker.  HBase can't use 0.19.0 if this is the carry-on.

I'll dig in some more."
HADOOP-4558,Scheduler fails to reclaim capacity if Jobs are submitted to queue one after the other,"Scheduler fails to reclaim capacity if Jobs are submitted to queue one after the other.
First job submitted with tasks equal to cluster's M/R Capacity
Second is submitted to different queue when all tasks of First Job are running, scheduler fails to reclaim capacity for second job.

"
HADOOP-4556,Block went missing,"Suspicion that all replicas of a block were marked for deletion. (Don't panic, investigation underway.)"
HADOOP-4552,Deadlock in RPC Server,"RPC server could get into a deadlock especially when clients or server are network starved. This is a deadlock between RPC responder thread trying to check if there are any connection to be purged and RPC handler trying to queue a response to be written by the responder.

This was first observed [this thread|http://www.nabble.com/TaskTrackers-disengaging-from-JobTracker-to20234317.html]. "
HADOOP-4546,Minor fix in dfs to make hadoop work in AIX,HDFS uses df command to get the disk space. The output format of df command is different in AIX compared to Linux and Solaris. Checking the OS type and then reading the df output as per the format will fix this issue and thus allows hadoop to be fully functional in AIX environment.
HADOOP-4545,Add an example of a secondary sort,We should have an example of using secondary sort.
HADOOP-4542,Fault in TestDistributedUpgrade,"A TestDistributedUpgrade subtest checks that the Name Node _does not_ start when a distributed upgrade is required. In 0.18, the subtest fails when the Name Node _does_ start. The fault is with the test, not HDFS. Not a problem in 0.19."
HADOOP-4541,Infinite loop in error handler for libhdfs,"If there is a problem writing out to HDFS, libhdfs gets put in an infinite loop.

Unfortunately, my program reroutes stderr/stdout to /dev/null, so I am attaching the strace output.  You can see the java stack traces which are written over and over."
HADOOP-4539,Streaming Edits to a Backup Node.,"Currently Secondary name-node acts as mere checkpointer.
Secondary name-node should be transformed into a standby name-node (SNN). 
The long term goal is to make it a warm standby. 
The purpose of this issue is to provide real time streaming of edits to SNN so that it contained the up-to-date namespace state."
HADOOP-4533,HDFS client of hadoop 0.18.1 and HDFS server 0.18.2 (0.18 branch) not compatible,"Not sure whether this is considered as a bug or is an expected case.
But here are the details.

I have a cluster using a build from hadoop 0.18 branch.
When I tried to use hadoop 0.18.1 dfs client to load files to it, I got the following exceptions:

hadoop --config ~/test dfs -copyFromLocal gridmix-env /tmp/.
08/10/28 16:23:00 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/10/28 16:23:00 INFO dfs.DFSClient: Abandoning block blk_-439926292663595928_1002
08/10/28 16:23:06 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/10/28 16:23:06 INFO dfs.DFSClient: Abandoning block blk_5160335053668168134_1002
08/10/28 16:23:12 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/10/28 16:23:12 INFO dfs.DFSClient: Abandoning block blk_4168253465442802441_1002
08/10/28 16:23:18 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/10/28 16:23:18 INFO dfs.DFSClient: Abandoning block blk_-2631672044886706846_1002
08/10/28 16:23:24 WARN dfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2349)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1800(DFSClient.java:1735)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1912)

08/10/28 16:23:24 WARN dfs.DFSClient: Error Recovery for block blk_-2631672044886706846_1002 bad datanode[0]
copyFromLocal: Could not get block locations. Aborting...
Exception closing file /tmp/gridmix-env
java.io.IOException: Could not get block locations. Aborting...
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2143)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1400(DFSClient.java:1735)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1889)

This problem has a severe impact on Pig 2.0, since it is pre-packaged with hadoop 0.18.1 and will use 
Hadoop 0.18.1 dfs client in its interaction with hadoop cluster.
That means that Pig 2.0 will not work with the to be released hadoop 0.18.2


"
HADOOP-4530,"In fsck, HttpServletResponse sendError fails with IllegalStateException","When looking at HADOOP-4526, I tried using sendError at line:53.

{code:title=FsckServlet.java|borderStyle=solid}                                _
     49     } catch (IOException ie) {
     50       StringUtils.stringifyException(ie);
     51       LOG.warn(ie);
     52       String errMsg = ""Fsck on path "" + pmap.get(""path"") + "" failed."";
     53       response.sendError(HttpServletResponse.SC_GONE, errMsg);
     54       throw ie;
     55     }
{code}

However, it always failed with 
{noformat}
2008-10-27 22:39:07,359 WARN /: /fsck?path=%2Fuser:
java.lang.IllegalStateException: Committed
  at org.mortbay.jetty.servlet.ServletHttpResponse.resetBuffer(ServletHttpResponse.java:212)
  at org.mortbay.jetty.servlet.ServletHttpResponse.sendError(ServletHttpResponse.java:375)
  at org.apache.hadoop.dfs.FsckServlet.doGet(FsckServlet.java:54)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
  at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
  at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
  at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
  at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
  at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
  at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
  at org.mortbay.http.HttpServer.service(HttpServer.java:954)
  at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
  at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
  at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
  at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
  at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
  at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
{noformat}

And client wasn't seeing the error."
HADOOP-4529,Support CHUKWA_CONF_DIR environment property for configuration storage,"Chukwa depends on existence of $CHUKWA_HOME/conf as configuration storage.  For consistency with Hadoop, it should have the ability to support CHUKWA_CONF_DIR as the configuration directory.  This jira is to keep track of enhancement required for this feature."
HADOOP-4528,Create ability to push a file to chukwa storage in one operation,"The current design of chukwa requires Chukwa Agent to monitor the file growth in order to stream data to Chukwa cluster.  When there are a lot of small files, it does not make sense to ask Chukwa Agent to monitor the file for long period of time.  This request is to add a special File adaptor to allow Chukwa Agent to stream small file as one time operation.  One example of this type of files is JobConf xml file.
"
HADOOP-4527,Add detection to duplicated data stream,"Chukwa Agent needs to be able to detect the duplicate data stream.  If a data stream is already in progress, discard the new request.
"
HADOOP-4526,fsck failing with NullPointerException  (return value 0),"fsck is siliently dying with return value of 0.

Namenode log showing 
2008-10-27 07:57:00,514 WARN /: /fsck?path=%2F:
java.lang.NullPointerException

No stack trace.
"
HADOOP-4525,config ipc.server.tcpnodelay is no loger being respected,"I was troubleshooting some slow IPC from hbase, and though it may be a Naggles algorithm issue. So I turned on tcpNoDelay on the client and server and this had no affect. Turns out that the ""ipc.server.tcpnodelay"" setting was no longer being read."
HADOOP-4523,Enhance how memory-intensive user tasks are handled,"HADOOP-3581 monitors each Hadoop task to see if its memory usage (which includes usage of any tasks spawned by it and so on) is within a per-task limit. If the task's memory usage goes over its limit, the task is killed. This, by itself, is not enough to prevent badly behaving jobs from bringing down nodes. What is also needed is the ability to make sure that the sum total of VM usage of all Hadoop tasks does not exceed a certain limit."
HADOOP-4520,javax.management.MalformedObjectNameException: Invalid character ':' in value part of property,"javax.management.MalformedObjectNameException: Invalid character ':' in value part of property

This can be reproduced by running any test using MiniDFSCluster."
HADOOP-4517,unstable dfs when running jobs on 0.18.1,"2 attempts of a job using 6000 maps, 1900 reduces

1.st attempt: failed during reduce phase after 22 hours with 31 dead datanodes most of which became unresponsive due to an exception; dfs lost blocks
2nd attempt: failed during map phase after 5 hours with 5 dead datanodes due to exception; dfs lost blocks responsible for job failure.

I will post typical datanode exception and attach thread dump."
HADOOP-4515,conf.getBoolean must be case insensitive,"Currently, if xx is set to ""TRUE"", conf.getBoolean(""xx"", false) would return false. 

conf.getBoolean should do an equalsIgnoreCase() instead of equals()

I am marking the change as incompatible because it does change semantics as pointed by Steve in HADOOP-4416"
HADOOP-4513,Capacity scheduler should initialize tasks asynchronously,"Currently, the capacity scheduler initializes tasks on demand, as opposed to the eager initialization technique used by the default scheduler. This is done in order to save JT memory footprint. However, the initialization is done in the {{assignTasks}} API which is not a good idea as task initialization could be a time consuming operation. This JIRA is to move out the initialization outside the {{assignTasks}} API and do it asynchronously."
HADOOP-4510,FileOutputFormat protects getTaskOutputPath,"o.a.h.m.FileOutputFormat#getTaskOutputPath() is protected. 

Having access to a task output directory as used internally by RecordWriters is quite handy. This is especially true if the user is attempting to serialize out data in a similar fashion as the output collector."
HADOOP-4508,FSDataOutputStream.getPos() == 0when appending to existing file and should be file length,"does not seem getPos is set correctly when opening an existing file with non-zero length in append mode.
"
HADOOP-4506,Exception during sort validation,"During sort validation, after validation job is completed, following exception is observed: 
2008-10-23 11:15:32,686 INFO org.apache.hadoop.fs.FSInputChecker: java.io.IOException: Checksum ok was sent and should not be sent again
	at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1064)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1613)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1590)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at org.apache.hadoop.io.BytesWritable.readFields(BytesWritable.java:149)
	at org.apache.hadoop.mapred.JobClient$RawSplit.readFields(JobClient.java:900)
	at org.apache.hadoop.mapred.JobClient.readSplitFile(JobClient.java:976)
	at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:369)
	at org.apache.hadoop.mapred.EagerTaskInitializationListener$JobInitThread.run(EagerTaskInitializationListener.java:55)
	at java.lang.Thread.run(Thread.java:619)

After that, ""job successfully completed"" message comes."
HADOOP-4505,Add a unit test to test faulty setup task and cleanup task killing the job,We should have a unit test which tests faulty setup task or cleanup task of the job eventually kills the job.
HADOOP-4502,Hive: DDL should allow users to add retention policy to the table,
HADOOP-4500,multifilesplit is using job default filesystem incorrectly,"getLocations() is using default filesystem based on jobconf instead of obtaining the right filesystem from the path
"
HADOOP-4499,DFSClient should invoke checksumOk only once.,This is a follow up for HADOOP-3914. HADOOP-3914 added a log when DFSClient tries to invoke checksumOk more than once. This jira is mainly for removing the debug log since we know why it happens.
HADOOP-4498,JobHistory does not escape literal jobName when used in a regex pattern,"JobHistory#getJobHistoryFileName throws a PatternSyntaxException if the jobName includes characters that could be regex metacharacters or metasequences and are subsequently considered malformed.

Trace:
Exception in thread ""initJobs"" java.util.regex.PatternSyntaxException: Unclosed character class near index 97
....
	at java.util.regex.Pattern.error(Pattern.java:1713)
	at java.util.regex.Pattern.clazz(Pattern.java:2254)
	at java.util.regex.Pattern.clazz(Pattern.java:2210)
	at java.util.regex.Pattern.sequence(Pattern.java:1818)
	at java.util.regex.Pattern.expr(Pattern.java:1752)
	at java.util.regex.Pattern.compile(Pattern.java:1460)
	at java.util.regex.Pattern.<init>(Pattern.java:1133)
	at java.util.regex.Pattern.compile(Pattern.java:823)
	at org.apache.hadoop.mapred.JobHistory$JobInfo.getJobHistoryFileName(JobHistory.java:632)
	at org.apache.hadoop.mapred.JobHistory$JobInfo.finalizeRecovery(JobHistory.java:740)
	at org.apache.hadoop.mapred.JobTracker.finalizeJob(JobTracker.java:1532)
	at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:2232)
	at org.apache.hadoop.mapred.JobInProgress.terminateJob(JobInProgress.java:1938)
	at org.apache.hadoop.mapred.JobInProgress.terminate(JobInProgress.java:1953)
	at org.apache.hadoop.mapred.JobInProgress.fail(JobInProgress.java:2012)
	at org.apache.hadoop.mapred.EagerTaskInitializationListener$JobInitThread.run(EagerTaskInitializationListener.java:62)
	at java.lang.Thread.run(Thread.java:637)"
HADOOP-4494,libhdfs does not call FileSystem.append when O_APPEND passed to hdfsOpenFile,libhdfs currently calls FileSystem.create when O_APPEND passed in - ie it ignores this flag
HADOOP-4490,Map and Reduce tasks should run as the user who submitted the job,"Currently the TaskTracker spawns the map/reduce tasks, resulting in them running as the user who started the TaskTracker.

For security and accounting purposes the tasks should be run as the job-owner."
HADOOP-4489,Blocks have been scheduled to delete by NameNode but are not deleted on the DataNode,"NameNode sometimes scheduled a replica to be deleted as shown in the NameNode log:

INFO org.apache.hadoop.dfs.StateChange: BLOCK* ask DataNode XX to delete blk_YY

But block YY did not get deleted on the DataNode XX. The datanode log had no indication of where the deletion was successful or failed. The block YY remains on the DataNode XX's disk with no timestamp change."
HADOOP-4487,Security features for Hadoop,"This is a top-level tracking JIRA for security work we are doing in Hadoop. Please add reference to this when opening new security related JIRAs.
"
HADOOP-4485,ant compile-native shorthand,"This issue introduces a shorthand for 
{code}
ant compile-core-native -Dcompile.native=true 
{code}

"
HADOOP-4483,getBlockArray in DatanodeDescriptor does not honor passed in maxblocks value,"The getBlockArray method in DatanodeDescriptor.java should honor the passed in maxblocks parameter. In its current form it passed in an array sized to min(maxblocks,blocks.size()) into the Collections.toArray method. As the javadoc for Collections.toArray indicates, the toArray method may discard the passed in array (and allocate a new array) if the number of elements returned by the iterator exceeds the size of the passed in array. As a result, the flawed implementation of this method would return all the invalid blocks for a data node in one go, and thus trigger the NameNode to send a DNA_INVALIDATE command to the DataNode with an excessively large number of blocks. This INVALIDATE command, in turn, could potentially take a very long time to process at the DataNode, and since DatanodeCommand(s) are processed in between heartbeats at the DataNode, this would trigger the NameNode to consider the DataNode to be offline / unresponsive (due to a lack of heartbeats). 

In our use-case at CommonCrawl.org, we regularly do large scale hdfs file deletions after certain stages of our map-reduce pipeline. These deletes would make certain DataNode(s) unresponsive, and thus impact the cluster's capability to properly balance file-system reads / writes across the whole available cluster. This problem only surfaced once we migrated from our 16.2 deployment to the current 18.1 release. 
"
HADOOP-4475,The (deprecated) method NetUtils.getServerAddress() doesn't validate configuration values enough,"Although HADOPP-2827 wants to kill this method, it is handy, but it doesn't have enough address validation, and can return a null value, which triggers NPEs a bit later on in the code"
HADOOP-4473,I Can't run bin/hive ,"When I run bin/hive
Then I get the message


java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/session/SessionState
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:194)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:220)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.session.SessionState
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
        ... 7 more
"
HADOOP-4471,Capacity Scheduler should maintain the right ordering of jobs in its running queue,"Currently, the Capacity Scheduler maintains a simple linked list of jobs which are running. This implies that running jobs are sorted by when they started running (i.e., when they were added to the queue). The Scheduler should maintain the same ordering among running jobs that it does for waiting jobs. Jobs should be sorted by priority (if the queue supports priorities) and by their submit time. 

This sorting would be more fair in deciding which running jobs get access to a free TT. It also does not penalize jobs that have a longer setup task, which affects when they enter the run queue. "
HADOOP-4469,ant jar file not being included in tar distribution,The ant-<version>.jar is not being included in the distribution.
HADOOP-4467,SerializationFactory should use current context ClassLoader,"SerializationFactory#add calls Class.forName without a ClassLoader instance. This prevents user-space Serialization classes from being loaded.

The resulting ClassNotFoundException is eaten after being logged, and a NPE is thrown further down the line when a Serializer is requested."
HADOOP-4466,SequenceFileOutputFormat is coupled to WritableComparable and Writable,For some reason SequenceFileOutputFormat calls asSubclass() for OutputKeyClass and OutputValueClass. This throws a ClassCastException when using non-Writable types with the new Serialization framework.
HADOOP-4464,Separate testClientTriggeredLeaseRecovery() out from TestFileCreation,TestFileCreation.testClientTriggeredLeaseRecovery() failed in some recent Hudson build.  We should separate testClientTriggeredLeaseRecovery() out from TestFileCreation for easily debugging.
HADOOP-4459,- Improve Chukwa Collector,"- Flush to disk before sending the acknowledge back to the client
- Prevent the collector from creating too many files on disk in case of HDFS errors
- fix stat request
- add simple ping request"
HADOOP-4458,Add a unit test for applications creating symlinks in wokring  directory,There should be a unit test for applications creating symlinks. The test should give the configuration for creating symlinks and verify the symlinks are created during the task's executation. 
HADOOP-4457,Input split logging in history is broken in 0.19,"HADOOP-3245 broke input split logging for map tasks in 0.19.The method JobInProgress.addRunningTaskToTip is always logging empty string ("""") for split locations now.
{code}
    if (tip.isFirstAttempt(id)) {
      JobHistory.Task.logStarted(tip.getTIPId(), name,
                                 tip.getExecStartTime(), """");
    }
{code}"
HADOOP-4456,distributed filecache: mapred.create.symlink=yes not effective,"I wanted to run a pipes job with an executable that has to load libhdfs.so.1 dynamically and I tried to achieve this by using mapred.create.symlink=yes.

But I did not see any symbolic links created.

I might do something wrong, but on the other hand, when looking at the code, it seems that in mapred/TaskRunner.java the workDir is created after the attempts to create symbolic links."
HADOOP-4455,Upload the derby.jar and TestSeDe.jar needed for fixes to 0.19 bugs,We have a new derby.jar which is needed to close out the 0.19 bugs for which we will be submitting the patch shortly.
HADOOP-4454,Support comments in 'slaves'  file,"Support comments in 'slaves'  file.
Comments  start with '#' (start of line or inline )
"
HADOOP-4453,Improve ssl handling for distcp,"HsftpFileSystem is an ad hoc way to read from HDFS over ssl, targeting distcp. Its organization can be improved and its support of ssl features expanded."
HADOOP-4451,Resource Manager to assume identity of user to execute task,"In the coming resource manager, the current model is that all tasks will run as the same generic user (task.java is called directly by the TaskTracker).

Instead of calling ""java task.java [arguments]"" allow for calling a program which will assume the UID (euid, gid, etc) of the calling user and call task.java on behalf of the TaskTracker.
"
HADOOP-4449,Minor formatting changes to quota related commands,'dfsadmin -help -setSpaceQuota' has very wide lines in the text help. 
HADOOP-4448,"""hadoop  jar"" runs wrong jar file in case of name clash","Imagine I run a command as follows:

    hadoop jar test.jar

Let's imagine that my main module in test.jar is named Sort.

Let's also imagine that some jar file in my CLASSPATH contains a module also named Sort, which also defines main.

Hadoop will run THAT jar file, not the one I specified."
HADOOP-4446,Update  Scheduling Information display in Web UI,"Under Scheduling Information display, change label ""Current Capacity"" to ""Guaranteed Capacity"", and display only one ""Guaranteed Capacity (%)"" instead of displaying it separately for maps and reduces"
HADOOP-4445,Wrong number of running map/reduce tasks are displayed in queue information.,Wrong number of running map/reduce tasks are displayed in queue information.
HADOOP-4444,Scheduling Information for queues shows one map/reduce task less then actual running map/reduce tasks.,Scheduling Information for queues shows one map/reduce task less then actual running map/reduce tasks.
HADOOP-4440,TestJobInProgressListener should also test for jobs killed in queued state,{{TestJobInProgressListener}} checks  if the listeners are informed as expected. This test case is missing one test where the job is killing in the queued state. 
HADOOP-4439,Cleanup memory related resource management,"HADOOP-3759 and HADOOP-3581 introduced memory based resource management. This JIRA is to cleanup certain aspects of the two issues that came up while doing HADOOP-4035, which is filed to support memory based scheduling "
HADOOP-4438,Add new/missing dfs commands in forrest,"The following DFS commands are missing Forrest documentation, as per HADOOP-4427. Splitting that as there are different teams working on the issues.

commands_manual:
- dfsadmin [-setSpaceQuota <quota> <dirname>...<dirname>]
- [-clrSpaceQuota <dirname>...<dirname>]

hdfs_shell:
- fs [-count[-q] <path>]
- [-moveToLocal [-crc] <src> <localdst>]"
HADOOP-4437,Use qMC sequence to improve the accuracy of PiEstimator,"Currently, PiEstimator uses java.util.Random to generate random 2d-points for estimating pi. The numbers generated by java.util.Random are uniformly distributed.  The 2d-points generated tense to have clump and gap. So the accuracy of the estimated pi is low.  The accuracy can be improved by using a quasi-Monte Carlo (qMC) sequence."
HADOOP-4436,S3 object names with arbitrary slashes confuse NativeS3FileSystem,"Consider a bucket with the following object names:

* /
* /foo
* foo//bar

NativeS3FileSystem treats an object named ""/"" as a directory.  Doing an ""fs -lsr"" causes an infinite loop.

I suggest we change NativeS3FileSystem to handle these by ignoring any such ""invalid"" names.  Thoughts?"
HADOOP-4435,The JobTracker should display the amount of heap memory used,"It would be nice to make the JobTracker UI display the amount of heap memory currently in use. This is similar to the DFS UI that already displays the amount if heap memory used by the NameNode.
"
HADOOP-4434,"Improve log rotation and reduce communication between agent and logging process, make agent more fault tolerant","- Chukwa Agent does not handle rotated log correctly.  The current design requires the logging process to pause the agent, rotate the log, then resume the log streaming.  The three communication between the running process and chukwa agent could become a problem when either or one of the process is down.  The implementation should be able to detect log rotation in the chukwa agent.
- Chukwa Agent should detect and prevent the same log being streamed more than once.
- Chukwa Agent should synchronize data streaming addition when picking up the check point file.
"
HADOOP-4433,Improve data loader for collecting metrics and log files from hadoop and system,"Enhancement should include:

- Ability to stream /var/log/messages
- Better handling of startup and shutdown chukwa processes
- Ability to monitor torque/hod allocated clusters
- Add serialization and deserialization of hadoop metrics to JSON log files."
HADOOP-4432,Provide metrics data mining interface in Chukwa,Chukwa in the contribute project creates interface to collect metrics data from large cluster and post process the data in Map reduce job.  The raw data and processed are stored in  HDFS.  This enhancement request is to create a storage pool for fast query of small sets of metrics data for reporting.
HADOOP-4431,Add versionning/tags to Chukwa Chunk,"add tags field
add version number to chunkImpl (contribute by Eric Y.)"
HADOOP-4430,Namenode Web UI capacity report is inconsistent with Balancer,"Solution to 2816 changed
- Total Capacity definition from (the disk space of all data directories) to (the disk space of all the data directories - the reserved space)
- We added a new element Present Capacity to the report. It is set to (Used Capacity + Remaining Capacity)
- We changed the Used Percentage reported from (Used Capacity)/(Total Capacity) to (Used Capacity)/(Present Capacity)
- All these changes are displayed on Namenode Web UI.

Balancer functionality
Balancer script is started with a threshold parameter. It tries to move the blocks from the nodes that have Used % that is more than (Cluster average + threshold) to the nodes that have less than (Cluster average - threshold). Essentially balancer gets all the datanodes used % to with in (the Cluster average +/- threshold).

Inconsistencies due to the change in 2816
When MapReduce jobs are run, temporary files are generated. This eats away a lot of space from Present Capacity. The difference between the Total Capacity and the Present Capacity can be huge. Currently balancer computes Used Percentage based (Used Capacity)/(Total Capacity). The Used % the balancer uses could be significantly different from Used % displayed on the Namenode Web UI. When balancer is done balancing, the Namenode Used % might still appear unbalanced.

"
HADOOP-4429,Misconfigured UNIX Groups Break Hadoop,"If a UNIX user has misconfigured groups, either locally or through LDAP, then Hadoop will not be able to start."
HADOOP-4428,Job Priorities are not handled properly ,Job Priorities are not handled properly 
HADOOP-4427,Add new/missing commands in forrest,"Following commands needs to be added to:
- commands_manual:
queue
job  [-set-priority <job-id> <priority>]
dfsadmin  [-setSpaceQuota <quota> <dirname>...<dirname>]
                   [-clrSpaceQuota <dirname>...<dirname>]

- hdfs_shell:
fs [-count[-q] <path>]
    [-moveToLocal [-crc] <src> <localdst>]"
HADOOP-4426,TestCapacityScheduler is broken,"The commits for HADOOP-4053 and HADOOP-4373 caused each other's test cases to break. The patches were being worked on in parallel, and hence the break wasn't caught earlier."
HADOOP-4425,Edits log takes much longer to load,The edits log takes significantly longer (+52% to +377% in tests simulating 200k to 20M files) to load in 0.19 than it did in 0.18.
HADOOP-4424,menu layout change for Hadoop documentation,"http://hadoop.apache.org/core/docs/r0.18.1/

changing menu layout for the Hadoop documentation + some minor content changes.
New menu will include:
Overview
Quick Start
==========================
Cluster Setup
Hadoop Quick Start
Hadoop Cluster Setup
Hadoop Map/Reduce Tutorial
Hadoop Command Guide
Hadoop FS Shell Guide
Hadoop DistCp Guide
Hadoop Native Libraries
Hadoop Streaming
Hadoop Archive Files
==========================
HDFS User Guide
HDFS Architecture
HDFS Permissions Guide
HDFS Quotas Admin Guide
==========================
HOD User Guide
HOD Config Guide
HOD Admin Guide
======================
Scheduling
New 0.19 topic ...
======================
API Docs
API Changes
Wiki
FAQ
Release Notes
Change Log"
HADOOP-4423,FSDataset.getStoredBlock(id) should not return corrupted information,"If a block file is corrupted so that the length is changed, FSDataset.getStoredBlock(id) will return a Block object with the corrupted length."
HADOOP-4422,S3 file systems should not create bucket,"Both S3 file systems (s3 and s3n) try to create the bucket at every initialization.  This is bad because

* Every S3 operation costs money.  These unnecessary calls are an unnecessary expense.
* These calls can fail when called concurrently.  This makes the file system unusable in large jobs.
* Any operation, such as a ""fs -ls"", creates a bucket.  This is counter-intuitive and undesirable.

The initialization code should assume the bucket exists:

* Creating a bucket is a very rare operation.  Accounts are limited to 100 buckets.
* Any check at initialization for bucket existence is a waste of money.

Per Amazon: ""Because bucket operations work against a centralized, global resource space, it is not appropriate to make bucket create or delete calls on the high availability code path of your application. It is better to create or delete buckets in a separate initialization or setup routine that you run less often.""
"
HADOOP-4420,JobTracker.killJob() doesn't check for the JobID being valid,"As with HADOOP-4419, JobTracker.killJob() assumes that the supplied JobID always matches to a valid job."
HADOOP-4419,JobTracker.setJobPriority() doesn't check for a jobID lookup failing,"Looking at the entry points of the JobTracker API, it seems that JobTracker.setJobPriority()  doesnt expect the JobID lookup ever to return null

It goes straight from lookup to checking access, an operation that assumes that job!=null
    JobInProgress job = jobs.get(jobid);
    checkAccess(job, QueueManager.QueueOperation.ADMINISTER_JOBS);

Recommend: add a test that calls this operaton with an invalid jobID, then fix the code as appropriate"
HADOOP-4418,"Update documentation in forrest for Mapred, streaming and pipes","Update forrest documentation for jvm reuse and job tracker restart in mapred_tutorial.
Also update forrest documentation for streaming for HADOOP-2302 and HADOOP-3341.
Update the package summary for pipes for HADOOP-1480"
HADOOP-4410,TestMiniMRDebugScript fails on trunk,"TestMiniMRDebugScript fails with following assertion error:
junit.framework.ComparisonFailure: expected:<Test Script
Bailing out> but was:<bash: /zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/testscript: No such file or directory
bash: line 0: exec: /zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/testscript: cannot execute: No such file or directory>
	at org.apache.hadoop.mapred.TestMiniMRMapRedDebugScript.testMapDebugScript(TestMiniMRMapRedDebugScript.java:207)"
HADOOP-4408,FsAction operations shouldn't create new Objects,"FsAction::and and related operations use {{values()[<expr>]}}, referencing a member var {{INDEX}}. This creates a new array object and uses member indices identical to {{ordinal()}}. It should either keep a reference to {{values()}} and use {{ordinal()}} explicitly or observe semantics consistent with {{INDEX}}."
HADOOP-4407,HADOOP-4395 should use a Java 1.5 API for 0.18,String.isEmpty() is not available in Java 1.5.
HADOOP-4406,"Make TCTLSeparatedProtocol configurable and have DynamicSerDe initialize, initialize the SerDe","Cannot currently pass in custom control separators - only can use the defaults.

this adds an interface for TProtocols to implement if they want to be configured and dynamic ser de checks its protocols to see if they implement this and if so calls their initialize method.
"
HADOOP-4405,all creation of hadoop dfs queries from with in hive shell,as the title suggests. this reduces the inconvenience of switching between hive and unix shell to do regular hadoop tasks. hive cli already supports shell commands (preppend ! to shell commands). 
HADOOP-4404,saveFSImage() should remove files from a storage directory that do not correspond to its type.,"NameNode's image and edits directories can be separated (HADOOP-3948).
So the name-node can have 3 types of storage directories. 
And each directory should contain only those files that correspond to its type:
# combined image and edits directory: should contain {{fsimage}} and {{edits}}
# image only directory should contain only {{fsimage}}
# edits only directory should contain only {{edits}}"
HADOOP-4403,TestLeaseRecovery.testBlockSynchronization failed on trunk,TestLeaseRecovery.testBlockSynchronization failed on a hadoop patch build. Please refer to http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3450/testReport/org.apache.hadoop.hdfs/TestLeaseRecovery/testBlockSynchronization/ for details. I don't know if it is a random issue.
HADOOP-4402,Namenode is unaware of FS corruption,"I think the name node is not told when there is block corruption.

I found a huge number of files corrupted when I restarted my namenode.  Digging through the datanode logs, I saw the following:

2008-10-13 03:30:44,266 INFO org.apache.hadoop.dfs.DataBlockScanner: Reporting bad block blk_-54103619973430645_3038 to namenode.
2008-10-13 03:57:11,447 WARN org.apache.hadoop.dfs.DataBlockScanner: First Verification failed for blk_7657563767222456337_3165. Exception : java.io.IOException: Block blk_7657563767222456337_3165 is not valid.
	at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:716)
	at org.apache.hadoop.dfs.FSDataset.getLength(FSDataset.java:704)
	at org.apache.hadoop.dfs.DataNode$BlockSender.<init>(DataNode.java:1678)
	at org.apache.hadoop.dfs.DataBlockScanner.verifyBlock(DataBlockScanner.java:408)
	at org.apache.hadoop.dfs.DataBlockScanner.verifyFirstBlock(DataBlockScanner.java:474)
	at org.apache.hadoop.dfs.DataBlockScanner.run(DataBlockScanner.java:565)
	at java.lang.Thread.run(Thread.java:595)

2008-10-13 03:57:11,448 WARN org.apache.hadoop.dfs.DataBlockScanner: Second Verification failed for blk_7657563767222456337_3165. Exception : java.io.IOException: Block blk_7657563767222456337_3165 is not valid.
	at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:716)
	at org.apache.hadoop.dfs.FSDataset.getLength(FSDataset.java:704)
	at org.apache.hadoop.dfs.DataNode$BlockSender.<init>(DataNode.java:1678)
	at org.apache.hadoop.dfs.DataBlockScanner.verifyBlock(DataBlockScanner.java:408)
	at org.apache.hadoop.dfs.DataBlockScanner.verifyFirstBlock(DataBlockScanner.java:474)
	at org.apache.hadoop.dfs.DataBlockScanner.run(DataBlockScanner.java:565)
	at java.lang.Thread.run(Thread.java:595)

2008-10-13 03:57:11,448 INFO org.apache.hadoop.dfs.DataBlockScanner: Reporting bad block blk_7657563767222456337_3165 to namenode.

So, node099 found a bad block.  However, if I grep the namenode information for that block:

/scratch/hadoop/logs/hadoop-root-namenode-hadoop-name.log.2008-10-10:2008-10-10 20:21:20,002 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/uscms01/LoadTestDownload/LoadTest07_FNAL_01_MyQXiu5a22TJQlcB_508. blk_7657563767222456337_3165
/scratch/hadoop/logs/hadoop-root-namenode-hadoop-name.log.2008-10-10:2008-10-10 20:21:32,150 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 172.16.1.110:50010 is added to blk_7657563767222456337_3165 size 67108864
/scratch/hadoop/logs/hadoop-root-namenode-hadoop-name.log.2008-10-10:2008-10-10 20:21:32,151 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 172.16.1.99:50010 is added to blk_7657563767222456337_3165 size 67108864
/scratch/hadoop/logs/hadoop-root-namenode-hadoop-name.log.2008-10-12:2008-10-12 05:05:26,898 INFO org.apache.hadoop.dfs.StateChange: BLOCK* ask 172.16.1.99:50010 to replicate blk_7657563767222456337_3165 to datanode(s) 172.16.1.18:50010
/scratch/hadoop/logs/hadoop-root-namenode-hadoop-name.log.2008-10-12:2008-10-12 05:05:40,742 INFO org.apache.hadoop.dfs.NameNode: Error report from 172.16.1.99:50010: Can't send invalid block blk_7657563767222456337_3165
/scratch/hadoop/logs/hadoop-root-namenode-hadoop-name.log.2008-10-12:2008-10-12 05:12:43,759 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_7657563767222456337_3165

To summarize:
- Block is allocated and written successfully to node100, then replicated to node099.
- Name node asks node099 to replicate block to node018
- Name node is told it can't send invalid block to node018!  A few minutes later, the PendingReplicationMonitor times out
- No new replications are launched!!!
- Block is found to be corrupted on node099 a few days later.  Data node claims to inform the namenode of this, but nothing is listed in the namenode logs.
- Block is suspiciously missing on node110 as well

Perhaps there are a few bugs here?
1) Name node doesn't get notified of the corrupted blocks - even though the datanode claims to!
2) On replication failure, no new replicas are created.
3) Corruption events are much, much too common.  We have a specific dataset which the namenode now claims is mostly-corrupted (100/167 files); before I restarted the namenode, we had jobs run against it continuously for the entire weekend.  The jobs were all successful, and the binary data format does internal integrity checks as it reads the files.  If the corruption was real, jobs would have failed.

I'm concerned that the corruption-detection systems of Hadoop are seriously busted for me. 


"
HADOOP-4401,ORDER BY clause is ignored for SELECT statements,"The ""ORDER BY"" clause seems to be ignored (see the truncated output below). I'd really like to use this in conjunction with the forthcoming LIMIT clause to get TOP K functionality.

hive> SELECT t.* FROM t ORDER BY t.numfriends;
3	Whatever Dude	25
4	Sweet Guy	20
1	Jeff Hammerbacher	15
2	Teddy Roosevelt	20
"
HADOOP-4400,"Add ""hdfs://"" to fs.default.name on quickstart.html","Trivial change, but if you use the suggested hadoop-site.xml from the current quickstart guide, you'll get an error like the below:

2008-10-12 23:32:42,560 WARN  fs.FileSystem (FileSystem.java:fixName(165)) - ""localhost:9000"" is a deprecated filesystem name. Use ""hdfs://localhost:9000/"" instead.

"
HADOOP-4399, fuse-dfs per FD context is not thread safe and can cause segfaults and corruptions,"for reads, optimal solution would be to have a per thread (per FD) context - including the buffer. Otherwise, protect the FD context with a mutex as in hadoop-4397 and hadoop-4398.

for writes, should just protect the context with a mutex as in hadoop-4398.

"
HADOOP-4398,fuse-dfs per FD context is not thread safe and can cause segfaults and corruptions,"this affects dfs_read and dfs_write and dfs_open dfs_release.

people have seen corruptions in 0.18.1 on reads - hadoop-4397.

In 0.19 since there is no global DFS handle, it is slightly different, but the same principles need to be applied and make the per FD context data (ie ptr to DFS, read buffer, ...) thread safe.

I can't think of a way to add a reliable test case for this in the current framework, but should be sure to assert pre/post conditions wherever possible.
"
HADOOP-4397,fuse-dfs causes corruptions on multi-threaded access,"If multiple threads in the same process perform file system reads, then fuse-dfs causes various problems due to the per-context buffer.  I've seen this reflected in segmentation violations and corruptions.

I'll attach a proposed patch which takes the ""easy way"" out - I surround all calls to dfs_read with a mutex.  You will obviously get performance degradations through thrashing if the threads are reading different parts of the file (but for our application, the multi-threaded reads are very, very infrequent.

If we want to have fuse-dfs writes/reads in 0.19 or 0.20, we'll probably need to do the same thing with writes.

This patch could be easily integrated as stands, or a more elaborate approach could be taken - per-thread buffers maybe?

Thanks as always for looking into this,

Brian"
HADOOP-4396,sort on 400 nodes is now slower than in 18,"Sort on 400 nodes on  hadoop release 18 takes about 29 minutes, but with the 19 branch takes about 32 minutes. This behavior is consistent."
HADOOP-4395,Reloading FSImage and FSEditLog may erase user and group information,"The bug can be reproduced as following:
{noformat}
bash-3.2$ ./bin/hadoop fs -lsr                 
-rw-r--r--   3 tsz supergroup       1366 2008-10-10 17:52 /user/tsz/a.txt
-rw-r--r--   3 tsz supergroup       1366 2008-10-10 17:52 /user/tsz/b.txt
bash-3.2$ ./bin/hadoop fs -chown sze a.txt
bash-3.2$ ./bin/hadoop fs -lsr
-rw-r--r--   3 sze supergroup       1366 2008-10-10 17:52 /user/tsz/a.txt
-rw-r--r--   3 tsz supergroup       1366 2008-10-10 17:52 /user/tsz/b.txt
{noformat}
Then, restart cluster ...
{noformat}
bash-3.2$ ./bin/hadoop fs -lsr
-rw-r--r--   3 sze                  1366 2008-10-10 17:52 /user/tsz/a.txt
-rw-r--r--   3 tsz supergroup       1366 2008-10-10 17:52 /user/tsz/b.txt
{noformat}
The group information for a.txt is missing."
HADOOP-4393,Merge AccessControlException and AccessControlIOException into one exception class,Merge org.apache.hadoop.fs.permission.AccessControlException and org.apache.hadoop.security.AccessControlIOException into a single class since they are cut & paste from each other.
HADOOP-4392,Various TestMiniMR tests failing on Windows,"On the windows nightly build many {{TestMiniMR*}} tests fail with a timeout. I will attach output from some of the tests. 

The problem might actually be some mis configuration on the machine.. but it is easy to see that from the test output. Someone more familiar with the test needs look into I think."
HADOOP-4391,"When killing a streaming task, it should kill the sub processes (subtree) spawned by the task ","Many times we saw sub processes left still running after a streaming task is killed.
This is one kind of resource leakage. 
The framework should kill the whole subprocess tree when killing a task.
"
HADOOP-4390,Hive: test for case sensitivity in serde2 thrift serde,
HADOOP-4388,Bug in Datanode transferBlocks code,"In the following code, when the blocks to be transferred has an invalid block, all the blocks that follow it are also not transferred. That might not be the intended behavior. Instead of breaking out of the loop, perhaps the right thing to do is to ignore the invalid block and continue with transferring the rest.

{noformat}
  private void transferBlocks( Block blocks[], 
                               DatanodeInfo xferTargets[][] 
                               ) throws IOException {
    for (int i = 0; i < blocks.length; i++) {
      if (!data.isValidBlock(blocks[i])) {
        String errStr = ""Can't send invalid block "" + blocks[i];
        LOG.info(errStr);
        namenode.errorReport(dnRegistration, 
                             DatanodeProtocol.INVALID_BLOCK, 
                             errStr);
        //
        // ******** This should be continue instead of break?
        //
        break;
      }
      int numTargets = xferTargets[i].length;
      if (numTargets > 0) {
        if (LOG.isInfoEnabled()) {
          StringBuilder xfersBuilder = new StringBuilder();
          for (int j = 0; j < numTargets; j++) {
            DatanodeInfo nodeInfo = xferTargets[i][j];
            xfersBuilder.append(nodeInfo.getName());
            if (j < (numTargets - 1)) {
              xfersBuilder.append("", "");
            }
          }
          String xfersTo = xfersBuilder.toString();
          LOG.info(dnRegistration + "" Starting thread to transfer block "" + 
                   blocks[i] + "" to "" + xfersTo);                       
        }
        new Daemon(new DataTransfer(xferTargets[i], blocks[i], this)).start();
      }
    }
  }

{noformat}"
HADOOP-4387,TestHDFSFileSystemContract fails on windows,"'ant -Dtestcase=TestHDFSFileSystemContract test-core' fails on Windows nightly build machine. 

Not sure if this is Hadoop error or a configuration error particular to this machine. Basically the following assert fails :

{noformat}
    Path workDir = path(getDefaultWorkingDirectory());
    assertEquals(workDir, fs.getWorkingDirectory());
{noformat}

This is essentially testing that {{System.getProperty(""user.name"")}} is same as string returned by Cygwin's 'whoami'. But in this case, these are ""SYSTEM"" and ""hadoopqa"" respectively."
HADOOP-4384,Build fails for Mac OSX,Automatic build fails on Mac OSX due to problems linking with JVM.
HADOOP-4380,Make new classes in mapred package private instead of public,"I think we should make the following new mapred classes package private:

Child
JVMId
JobTrackerInstrumentation
QueueManager
ResourceEstimator
SkipBadRecords
TaskTrackerInstrumentation
TaskTrackerMetricsInst"
HADOOP-4378,TestJobQueueInformation fails regularly,"Got the same result from Linux and Windows:
{noformat}
Testcase: testJobQueues took 40.806 sec
        FAILED
task tracker dir /home/tsz/hadoop/latest/build/test/mapred/local/1_0/taskTrackerr does not exist.
junit.framework.AssertionFailedError: task tracker dir /home/tsz/hadoop/latest/build/test/mapred/local/1_0/taskTracker does not exist.
        at org.apache.hadoop.mapred.TestMiniMRWithDFS.checkTaskDirectories(TestMMiniMRWithDFS.java:140)
        at org.apache.hadoop.mapred.TestMiniMRWithDFS.runWordCount(TestMiniMRWitthDFS.java:196)
        at org.apache.hadoop.mapred.TestJobQueueInformation.testJobQueues(TestJoobQueueInformation.java:90)
{noformat}"
HADOOP-4377,Race condition creating S3 buffer directory for NativeS3FileSystem,"The buffer directory is checked for existence, then created if it doesn't exist.  But the create can fail if the another process creates it in between.  We can fix this by checking for existence again if the create fails.  I've seen ""Cannot create S3 buffer directory"" occur, and this race is the most plausible explanation."
HADOOP-4376,Fix line formatting in hadoop-default.xml for hadoop.http.filter.initializers,"Line formatting in hadoop-default.xml for property hadoop.htttp.filter.initializers is not elegant, we should fix it before 0.19. "
HADOOP-4375,Accesses to CompletedJobStore should not lock the JobTracker.,"
When CompletedJobStatusStore is configured to use dfs, it can cause lockup of JT due to inline dfs access.
For e.g.
{code}
  public synchronized JobStatus getJobStatus(JobID jobid) {
    JobInProgress job = jobs.get(jobid);
    if (job != null) {
      return job.getStatus();
    } else {
      return completedJobStatusStore.readJobStatus(jobid);
    }
  }
{code}

Instead, JT should be locked only when accessing its internal memory structures. The other such methods are JobTracker.getJobCounters, JobTracker.getJobProfile and JobTracker.finalizeJob."
HADOOP-4374,JVM should not be killed but given an opportunity to exit gracefully,"When the tasktracker picks an idle JVM for purging, it should signal the JVM to exit gracefully, rather than forcefully killing it. This might have the unfortunate side effect of logs not fully flushed yet in some cases."
HADOOP-4373,Guaranteed Capacity calculation is not calculated correctly,"Guaranteed Capacity calculation is not calculated correctly.

"
HADOOP-4372,Improve the way the job history files are managed during job recovery,Today we use the _.recover_ technique to handle the job history files when the jobtracker restarts. The comment [here|https://issues.apache.org/jira/browse/HADOOP-3245?focusedCommentId=12629080#action_12629080] proposes a better way to handle the files. 
HADOOP-4369,Metric Averages are not averages,"Metrics averages are not averages; instead of updating the metric with the average number periodically, Hadoop metrics *increments* the metric.

I.e., each update we have value = old_value + current average.  Instead, we want each update to have value = current_average.

Patch will be attached momentarily."
HADOOP-4368,"Superuser privileges required to do ""df""","super user privileges are required in DFS in order to get the file system statistics (FSNamesystem.java, getStats method).  This means that when HDFS is mounted via fuse-dfs as a non-root user, ""df"" is going to return 16exabytes total and 0 free instead of the correct amount.

As far as I can tell, there's no need to require super user privileges to see the file system size (and historically in Unix, this is not required).

To fix this, simply comment out the privilege check in the getStats method."
HADOOP-4367,Hive: UDAF functions cannot handle NULL values,"The UDAF functions cannot handle null values. It just throws a NullPointerException. We should properly handle null values in UDAF.
For all UDAF functions that we have now: sum, count, ave, we should ignore the null values."
HADOOP-4366,Provide way to replace existing column names for columnSet tables,"all syntax like 'alter table xyz replace columns (a int, b string)'"
HADOOP-4365,Configuration.getProps() should be made protected for ease of overriding,"HADOOP-4293 does make Configuration cleaner and easier to work with, but it does contain assumptions about how configurations are represented (in a private Properties instance) that subclasses may wish to diverge from.

By making getProps() protected, people who subclass JobConf or Configuration can do their own binding from configuration data to Properties. 

"
HADOOP-4362,Hadoop Streaming failed with large number of input files,"Simple job failed with ""java.lang.ArrayIndexOutOfBoundsException"" when the mapper is /bin/cat and the number of input files is large.

$  hadoop jar $HADOOP_HOME/hadoop-streaming.jar -input in_data -output op_data -mapper /bin/cat -reducer NONE
additionalConfSpec_:null
null=@@@userJobConfProps_.get(stream.shipped.hadoopstreaming
packageJobJar: [/tmp/hadoop-unjar49637/] []
/tmp/streamjob49638.jar tmpDir=/tmp
08/10/07 07:03:09 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should
implement Tool for the same.
08/10/07 07:03:11 INFO mapred.FileInputFormat: Total input paths to process : 16365
08/10/07 07:03:12 INFO mapred.FileInputFormat: Total input paths to process : 16365
08/10/07 07:03:15 ERROR streaming.StreamJob: Error Launching job : java.io.IOException:
java.lang.ArrayIndexOutOfBoundsException

Streaming Job Failed!


But when the input number of files are less job does not fail . 

$ hadoop  jar $HADOOP_HOME/hadoop-streaming.jar -input inp_data1 -output op_data1 -mapper /bin/cat -reducer NONE
additionalConfSpec_:null
null=@@@userJobConfProps_.get(stream.shipped.hadoopstreaming
packageJobJar: [/tmp/hadoop-unjar3725/] []
/tmp/streamjob3726.jar tmpDir=/tmp
08/10/07 07:06:37 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should
implement Tool for the same.
08/10/07 07:06:39 INFO mapred.FileInputFormat: Total input paths to process : 16
08/10/07 07:06:39 INFO mapred.FileInputFormat: Total input paths to process : 16
08/10/07 07:06:42 INFO streaming.StreamJob: getLocalDirs():
[/var/mapred/local]
08/10/07 07:06:42 INFO streaming.StreamJob: Running job: job_200810070645_0006
08/10/07 07:06:42 INFO streaming.StreamJob: To kill this job, run:
08/10/07 07:06:42 INFO streaming.StreamJob: hadoop job -Dmapred.job.tracker=login1:51981 -kill job_200810070645_0006
08/10/07 07:06:42 INFO streaming.StreamJob: Tracking URL: http://login1:52941/jobdetails.jsp?jobid=job_200810070645_0006
08/10/07 07:06:43 INFO streaming.StreamJob:  map 0%  reduce 0%
08/10/07 07:06:46 INFO streaming.StreamJob:  map 44%  reduce 0%
08/10/07 07:06:47 INFO streaming.StreamJob:  map 75%  reduce 0%
08/10/07 07:06:48 INFO streaming.StreamJob:  map 88%  reduce 0%
08/10/07 07:06:49 INFO streaming.StreamJob:  map 100%  reduce 100%
08/10/07 07:06:49 INFO streaming.StreamJob: Job complete: job_200810070645_0006
08/10/07 07:06:49 INFO streaming.StreamJob: Output: op_data1


"
HADOOP-4361,Corner cases in killJob from command line,"If the job has occupied all the slots (maps and reduces), and there is kill from command line to kill the job, we have to make sure the cleanup task is eventually launched. Now there is no slot free until one of the attempts completes/failed.
If there is kill from commandline during job setup, setup task has to be killed, before launching cleanup task."
HADOOP-4360,Reducers hang in SHUFFLING phase due to duplicate completed tasks in TaskTracker.FetchStatus.allMapEvents,"On our cluster we have seen JobTracker went to a weird state that a lot of TaskTrackers are getting duplicate entries in TaskTracker.FetchStatus.allMapEvents.

Since TaskTracker fetches new completed map tasks using the size of the allMapEvents as starting index, this prohibits the tasktracker from getting all completed map tasks. And as a result, reducer just hangs in the shuffling status.

The problem does not get fixed by killing and restarting TaskTracker, and when it happens a lot of TaskTrackers will show the same problem.


It seems some problems happen to the communication between JobTracker and TaskTracker.

An easy preventive fix will be to include the starting index of the list of completed map tasks from JobTracker to TaskTracker, so that TaskTracker can just throw away the data if the starting index does not match the current size of the array.
"
HADOOP-4359,Access Token: Support for data access authorization checking on DataNodes,"Currently, DataNodes do not enforce any access control on accesses to its data blocks. This makes it possible for an unauthorized client to read a data block as long as she can supply its block ID. It's also possible for anyone to write arbitrary data blocks to DataNodes. 

When users request file accesses on the NameNode, file permission checking takes place. Authorization decisions are made with regard to whether the requested accesses to those files (and implicitly, to their corresponding data blocks) are permitted. However, when it comes to subsequent data block accesses on the DataNodes, those authorization decisions are not made available to the DataNodes and consequently, such accesses are not verified. Datanodes are not capable of reaching those decisions independently since they don't have concepts of files, let alone file permissions.

In order to implement data access policies consistently across HDFS services, there is a need for a mechanism by which authorization decisions made on the NameNode can be faithfully enforced on the DataNodes and any unauthorized access is declined."
HADOOP-4358,NPE from CreateEditsLog,"HADOOP-1869 added a call to setAccessTime(long) from the INode cstr, which relies on a non-null value from FSNamesystem::getFSNamesystem.
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.INode.setAccessTime(INode.java:301)
        at org.apache.hadoop.hdfs.server.namenode.INode.<init>(INode.java:99)
        at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.<init>(INodeDirectory.java:45)
        at org.apache.hadoop.hdfs.CreateEditsLog.addFiles(CreateEditsLog.java:68)
        at org.apache.hadoop.hdfs.CreateEditsLog.main(CreateEditsLog.java:214)
{noformat}"
HADOOP-4356,"[Hive] for a 2-stage map-reduce job, number of reducers not set correctly","the number of reducers are always set to the default. In a group-by statement, this leads to so many redundant reducers for the 2nd map-reduce job"
HADOOP-4355,hive 2 case sensitivity issues,
HADOOP-4354,Separate TestDatanodeDeath.testDatanodeDeath() into 4 tests,TestDatanodeDeath fails occasionally as reported in HADOOP-4278.  TestDatanodeDeath.testDatanodeDeath() indeed contains 4 tests.  It would be easily for debugging to separate the tests.
HADOOP-4353,enable multi-line query from Hive CLI,currently no line breaks are allowed when writing a query from interactive interface. this can lead to execution of partial queries and cumbersomely long queries. this JIRA is to allow for multi-line query. Each query must end with a semi-colon.
HADOOP-4351,ArrayIndexOutOfBoundsException during fsck,"After observing a lot of corrupted blocks, I suddenly started to get a lot of ArrayIndexOutOfBoundsException.

It appears to be an issue very similar to HADOOP-3649, which is supposed to be fixed in 0.18.1.

2008-10-06 08:48:43,241 WARN /: /fsck?path=%2F:
java.lang.ArrayIndexOutOfBoundsException: 2
   at org.apache.hadoop.dfs.FSNamesystem.getBlockLocationsInternal(FSNamesystem.java:789)
   at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:727)
   at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:167)
   at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:162)
   at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:162)
   at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:162)
   at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:162)
   at org.apache.hadoop.dfs.NamenodeFsck.fsck(NamenodeFsck.java:128)
   at org.apache.hadoop.dfs.FsckServlet.doGet(FsckServlet.java:48)
   at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
   at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
   at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
   at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
   at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
   at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
   at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
   at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
   at org.mortbay.http.HttpServer.service(HttpServer.java:954)
   at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
   at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
   at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
   at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
   at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
   at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-4348,Adding service-level authorization to Hadoop,"Service-level authorization is the initial checking done by a Hadoop service to find out if a connecting client is a pre-defined user of that service. If not, the connection or service request will be declined. This feature allows services to limit access to a clearly defined group of users. For example, service-level authorization allows ""world-readable"" files on a HDFS cluster to be readable only by the pre-defined users of that cluster, not by anyone who can connect to the cluster. It also allows a M/R cluster to define its group of users so that only those users can submit jobs to it.

Here is an initial list of requirements I came up with.

    1. Users of a cluster is defined by a flat list of usernames and groups. A client is a user of the cluster if and only if her username is listed in the flat list or one of her groups is explicitly listed in the flat list. Nested groups are not supported.

    2. The flat list is stored in a conf file and pushed to every cluster node so that services can access them.

    3. Services will monitor the modification of the conf file periodically (5 mins interval by default) and reload the list if needed.

    4. Checking against the flat list is done as early as possible and before any other authorization checking. Both HDFS and M/R clusters will implement this feature.

    5. This feature can be switched off and is off by default.

I'm aware of interests in pulling user data from LDAP. For this JIRA, I suggest we implement it using a conf file. Additional data sources may be supported via new JIRA's."
HADOOP-4347,[mapred] Job client cannot kill jobs that are not initialized.,"Job client cannot kill a job that is not yet initialized, JobTracker silently ignores the kill request (JobTracker.killJob(JobID)). Though the client reports that the job is killed, the job still resides with JT and in fact, it goes to running state and eventually finishes."
HADOOP-4346,"Hadoop triggers a ""soft"" fd leak. ","
Starting with Hadoop-0.17, most of the network I/O uses non-blocking NIO channels. Normal blocking reads and writes are handled by Hadoop and use our own cache of selectors. This cache suites well for Hadoop where I/O often occurs on many short lived threads. Number of fds consumed is proportional to number of threads currently blocked. 

If blocking I/O is done using java.*, Sun's implementation uses internal per-thread selectors. These selectors are closed using {{sun.misc.Cleaner}}. Looks like this cleaning is kind of like finalizers and tied to GC. This is pretty ill suited if we have many threads that are short lived. Until GC happens, number of these selectors keeps growing. Each selector consumes 3 fds.

Though blocking read and write are handled by Hadoop, {{connect()}} is still the default implementation that uses per-thread selector. 

Koji helped a lot in tracking this. Some sections from 'jmap' output and other info  Koji collected led to this suspicion and will include that in the next comment.

One solution might be to handle connect() also in Hadoop using our selectors.
"
HADOOP-4345,Hive: Check that partitioning predicate is present when hive.partition.pruning = strict,"Add a new parameter 

hive.partition.pruning

when this is set to strict

then do not allow queries that do not have a predicate on the partitioning columns of partitioned tables. This helps to gate users from submitting huge queries to the cluster."
HADOOP-4344,Hive: Partition pruning causes semantic exception with joins,Partition pruning code throws a semantic exception with joins as it tries to match a partition column with the serde.
HADOOP-4343,Adding user and service-to-service authentication to Hadoop,"Currently, Hadoop services do not authenticate users or other services. As a result, Hadoop is subject to the following security risks.

1. A user can access an HDFS or M/R cluster as any other user. This makes it impossible to enforce access control in an uncooperative environment. For example, file permission checking on HDFS can be easily circumvented.

2. An attacker can masquerade as Hadoop services. For example, user code running on a M/R cluster can register itself as a new TaskTracker.

This JIRA is intended to be a tracking JIRA, where we discuss requirements, agree on a general approach and identify subtasks. Detailed design and implementation are the subject of those subtasks."
HADOOP-4342,[hive] bug in partition pruning,"The following test is not working:

FROM srcpart
INSERT OVERWRITE TABLE dest1 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-08' and srcpart.hr = '12'
INSERT OVERWRITE TABLE dest2 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-09' and srcpart.hr = '12';


if the partitions are different. It seems that all the partitions are pruned if they are different."
HADOOP-4340,"""hadoop jar"" always returns exit code 0 (success) to the shell when jar throws a fatal exception ","Running ""hadoop jar"" always returns 0 (success) when the jar dies with a stack trace.  As an example, run these commands:

/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/hadoop-0.18.1-examples.jar pi 10 10 2>&1; echo $?
exits with 0

/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/hadoop-0.18.1-examples.jar pi 2>&1; echo $?
exits with 255

/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/hadoop-0.18.1-examples.jar  2>&1; echo $?
exits with 0 

This seems to be expected behavior.  However, running:

/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/hadoop-0.18.1-examples.jar pi 10 badparam 2>&1; echo $?
java.lang.NumberFormatException: For input string: ""badparam""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Long.parseLong(Long.java:403)
        at java.lang.Long.parseLong(Long.java:461)
        at org.apache.hadoop.examples.PiEstimator.run(PiEstimator.java:241)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.examples.PiEstimator.main(PiEstimator.java:252)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:53)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)
exits with 0.

In my opinion, if a jar throws an exception that kills the program being run, and the developer doesn't catch the exception and do a sane exit with a exit code, hadoop should at least exit with a non-zero exit code.

As another example, while running a main class that exits with an exit code of 201, Hadoop will preserve the correct exit code:

    public static void main(String[] args) throws Exception {
    System.exit(201);
  }

But when deliberately creating a null pointer exception, Hadoop exits with 0.

  public static void main(String[] args) throws Exception {
	Object o = null;
	o.toString();
    System.exit(201);
  }

This behaviour makes it very difficult, if not impossible, to use Hadoop programatically with tools such as HOD or non-Java data processing frameworks, since if a jar crashes with an unhandled exception, Hadoop doesn't inform the calling program in a well-bahaved way (polling stderr for output is not a very good way to detect application failure).     

I'm not a Java programmer, so I don't know what the best code to signal failure would be.

Please let me know what other information I can include about my setup

Thanks.


"
HADOOP-4339,Improve FsShell -du/-dus and FileSystem.getContentSummary efficiency,"FsShell.du has two inefficiencies:

* calling getContentSummary twice for each top-level item rather than calling it once and saving the result
* calling getContentSummary for files rather than using the size it already has in FileStatus

getContentSummary has one:

* calling itself for files rather than using the length it already has in FileStatus

Every call to getContentSummary results in a call to getFileStatus, which may be expensive (e.g. NativeS3FileSystem has both network latency and actual monetary cost).

The simple solution:

* FsShell.du calls once per item and saves the ContentSummary
* FsShell.du uses FileStatus.getLen for files
* getContentSummary only calls itself for directories

Another solution, rather than adding special casing to callers, is to add a getContentSummary that takes a FileStatus."
HADOOP-4336,fix sampling bug in fractional bucket case,"if there are 32 buckets and sampling clause is 1/64, no rows are selected."
HADOOP-4335,FsShell -ls fails for file systems without owners or groups,"FsShell.ls() fails (uses printf ""%-0d"") when all owners or groups are empty.  This happens with NativeS3FileSystem.
"
HADOOP-4333,add ability to drop partitions through DDL,currently there is no way to delete a table partition from warehouse throgh Hive DDL. this jira is add that ability.
HADOOP-4330,Hive: AS clause with subqueries having group bys is not propogated to the outer query block,"The query of the following form:

select a.c1, a.c2 from (select b.c1 as c1, count(b.c2) as c2 from b group by b.c1) where a.c1 > 100

does not work as the c1 and c2 aliases are not recognized by the outer query block."
HADOOP-4329,Hive: [] operator with maps does not work,[] operator on map types is broken.
HADOOP-4328,Some convenient methods in the FileSystem API should be final.,"In FileSystem, some methods are overloaded with different parameter list.  We should mark the convenient methods final.

For example, the create(...) method has 10 different signatures.  Subclasses of FileSystem should only override the one with the most number or parameters but not the others.  We should mark all the other 9 create(...) final."
HADOOP-4327,Create table hive does not set delimeters,"When creating a table the syntax to change delimiter does not work
ROW FORMAT DELIMITED
FIELDS TERMINATED BY \001
COLLECTION ITEMS TERMINATED BY \002
MAP KEYS TERMINATED BY \003
LINES TERMINATED BY \012

Hive accepts other delimiters but \001 'SOH' is still used as the delimiter."
HADOOP-4326,ChecksumFileSystem does not override all create(...) methods,"ChecksumFileSystem does not override the following method
{code}
public FSDataOutputStream create(Path f, FsPermission permission,
      boolean overwrite, int bufferSize, short replication, long blockSize,
      Progressable progress) throws IOException 
{code}
As a consequence, when creating a file with LocalFileSystem and the create method above, no checksum will be created."
HADOOP-4325,Hadoop SocketInputStream.read() should return -1 in case of EOF.,"{{org.apache.hadoop.net.SocketInputStream.read()}} should return -1 when underlying read returns -1.
"
HADOOP-4324,Need a way to get the inforserver port of a name node,"To test that services shut down cleanly, I need to know the port that the namenode brings up an info server on, which means that its assigned port value needs to be exported from the namesystem.

I can see two ways to do this, and wish some recommendations of the best approach

1.  extract the port value when the FSNameSystem comes up, and add it to that classes Conf; add a method to get that Conf so that its state can be read.
2.  save the port value to a member variable in FSNameSystem and provide a public method to get at it.

My preference is for #1, as it is consistent with how the NameNode saves its port value, and doesn't change any APIs."
HADOOP-4321,Document the capacity scheduler in Forrest,"Document the capacity scheduler and related issues, some of which have touched the framework as well. The issues are all linked in HADOOP-3444."
HADOOP-4320,[Hive] TCTLSeparatedProtocol implement maps/lists/sets read/writes,"implement maps/lists/sets by doing the split for their values in read/write Map/set/list begin,  For maps, the added problem of a different key/value separator means we have to know when we're on a key or a value when doing a readString by having some sort of elementIndex. Hacky, but effective and somewhat like TDenseProtocol - there's nothing to do but to have state in the protocol and this is preferable to having state in DynamicSerDe since there may be all sorts of protocols.
"
HADOOP-4319,fuse-dfs dfs_read function may return less than the requested #of bytes even if EOF not reached,"in this situation, fuse will pad the remainder of the file with zeros resulting in corrupt reads.
"
HADOOP-4316,[Hive] extra new lines at output,"extra new lines at output.

If you have a empty table, a empty line is printed still"
HADOOP-4315,Hive: Cleanup temporary files once the job is done,We leave temporary files around once the job is finished. We need to clean these up after the job is done.
HADOOP-4314,TestReplication fails quite often,TestReplication times out once every 5-6 attempts. I will attach log from a failed test.
HADOOP-4311,Change job's runstate from int to enum,Job's runstate is stored in {{JobStatus}} as _static final int_. I think it would be better to change it to enum as done in other parts of the framework.
HADOOP-4309,eclipse-plugin no longer compiles on trunk,"{noformat}
compile:
     [echo] contrib: eclipse-plugin
    [javac] Compiling 2 source files to /Users/chrisdo/work/commit/build/contrib/eclipse-plugin/classes
    [javac] /Users/chrisdo/work/commit/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopServer.java:423: write(java.io.DataOutput) in org.apache.hadoop.conf.Configuration cannot be applied to (java.io.FileOutputStream)
    [javac]     this.conf.write(fos);
    [javac]              ^
    [javac] /Users/chrisdo/work/commit/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/servers/RunOnHadoopWizard.java:166: write(java.io.DataOutput) in org.apache.hadoop.conf.Configuration cannot be applied to (java.io.FileOutputStream)
    [javac]       conf.write(fos);
    [javac]           ^
    [javac] Note: /Users/chrisdo/work/commit/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/servers/RunOnHadoopWizard.java uses unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors
{noformat}"
HADOOP-4308,Hive: DynamicSerDe based on thrift DDL,"This is the DynamicSerDe code in hive/serde that supports serialization/deserialization in thrift format. It supports Thrift DDL so users could read the data outside Hive.
"
HADOOP-4307,add an option to  describe table to show extended properties of the table such as serialization/deserialization properties ,"support statement like below
describe extended table_name_abc

this should print all the properties of the table instead of just the columns."
HADOOP-4305,repeatedly blacklisted tasktrackers should get declared dead,"When running a batch of jobs it often happens that the same tasktrackers are blacklisted again and again. This can slow job execution considerably, in particular, when tasks fail because of timeout.
It would make sense to no longer assign any tasks to such tasktrackers and to declare them dead."
HADOOP-4303,Hive: trim and rtrim UDFs behaviors are reversed,Currently trim removes trailing spaces where as rtrim removes spaces from both ends of the string.
HADOOP-4302,TestReduceFetch fails intermittently,"I see TestReduceFetch failing once in a while. Here is one such failure 
http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3350/testReport/org.apache.hadoop.mapred/TestReduceFetch/testReduceFromPartialMem/

Marking this as a blocker until we get to the root cause."
HADOOP-4301,Forrest doc for skip bad records feature,Forrest documentation is required for skip records feature HADOOP-153
HADOOP-4300,"When fs.trash.interval is set to non-zero value, the deleted files and directory which are in .Trash are not getting removed from there after <fs.trash.interval>","Set fs.trash.interval to non zero value(say 1), touch a file (say file.txt) and delete it. The expected behavior would be that file.txt is moved to .Trash and also file.txt is removed from .Trash after 1min. But the observed behavior is that, even though file.txt is being moved to .Trash, it is not removed from .Trash after 1min."
HADOOP-4299,Unable to access a file by a different user in the same group when permissions is set to 770 or when permissions is turned OFF,"Consider the following two scenarios
1)   With permissions ON, start cluster as user A, create a file as user A, chmod 770 file as user A, access file as user B(using 'get' command) 
The expected behavior would be that user B should be able to access the file since user A and user B are in the same group. But this is not happening. Instead the following error message is displayed ""get: Permission denied""

2)  With permissions ON, start cluster as user A, create a file as user A, chmod 700 file as user A, shut down dfs and turn permissions OFF and start up dfs, access file as user B(using 'get')
The expected behavior would be that user B should be able to access the file since permissions is turned OFF. But this is not happening. Instead the following error message is displayed ""get: Permission denied"""
HADOOP-4298,File corruption when reading with fuse-dfs,"I pulled a 5GB data file into Hadoop using the following command:
hadoop fs -put /scratch/886B9B3D-6A85-DD11-A9AB-000423D6CA6E.root /user/brian/testfile
I have HDFS mounted in /mnt/hadoop using fuse-dfs.

However, when I try to md5sum the file in place (md5sum /mnt/hadoop) or copy the file back to local disk using ""cp"" then md5sum it, the checksum is incorrect.

When I pull the file using normal hadoop means (hadoop fs -get /user/brian/testfile /scratch), the md5sum is correct.

When I repeat the test with a smaller file (512MB, on the theory that there is a problem with some 2GB limit somewhere), the problem remains.
When I repeat the test, the md5sum is consistently wrong - i.e., some part of the corruption is deterministic, and not the apparent fault of a bad disk.

CentOs 4.6 is, unfortunately, not the apparent culprit.  When checking on CentOs 5.x, I could recreate the corruption issue.  The second node was also a 64-bit compile and CentOs 5.2 (`uname -r` returns 2.6.18-92.1.10.el5).

Thanks for looking into this,
Brian"
HADOOP-4297,Enable Java assertions when running tests,"A suggestion to enable Java assertions in the project's build xml when running tests. I think this would improve the build quality.
To enable assertions add the following snippets to the JUnit tasks in build.xml:

<assertions>
     <enable />
</assertions>

--
For example:

<junit ... >
     ...
    <assertions>
        <enable />
    </assertions>
</junit>

 "
HADOOP-4296,Spasm of JobClient failures on successful jobs every once in a while,"At very busy times - we get a wave of job client failures all at the same time. the failures come when the job is about to complete. when we look at the job history files - the jobs are actually complete. Here's the stack:

08/09/27 02:18:00 INFO mapred.JobClient:  map 100% reduce 98%
08/09/27 02:18:41 INFO mapred.JobClient:  map 100% reduce 99% 
java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:993)
	at com.facebook.hive.common.columnSetLoader.main(columnSetLoader.java:535)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
"
HADOOP-4295,job-level configurable mapred.map.tasks.maximum and mapred.reduce.tasks.maximum ,"Right now mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum are set on the tasktracker level.

In absense of a smart tasktracker monitoring resources and deciding in an adaptive manner how many tasks can be run simultaneously, it would be nice to move these two configuration options to the job level. This would make it easier to optimize the performance of a batch of jobs."
HADOOP-4294,Hive: Parser should pass field schema to SerDe,
HADOOP-4293,Remove WritableJobConf ,"The new class WritableJobConf doesn't add value over just making JobConf Writable, and I propose that we remove it before it is released."
HADOOP-4292,append() does not work for LocalFileSystem,"append is supported by LocalFileSystem but it does not update crc when a file is appended. 

When you enable checksum verification {{TestLocalFileSystem.testAppend}} fails. Since HADOOP-4277 is a blocker for 0.17  I am planning to disable this test in HADOOP-4277."
HADOOP-4290,In jobdetails.jsp page the values of Pending and Complete tasks for map keeps on interchanging with each other.,In jobdetails.jsp page the values of Pending and Complete tasks for map keeps on interchanging with each other.
HADOOP-4289,Running maps show negative valiue in JobTracker log.,While running jobs it has been observerd that in the JobTracker log running maps show negative value.
HADOOP-4288,java.lang.NullPointerException is observed in Jobtracker log while   call heartbeat,"2008-09-26 09:33:08,190 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 34441, call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@177a878, false, true, 100) from <ipaddress:port>: error: java.io.IOException: java.lang.NullPointerException  
is observed in Jobtracker log."
HADOOP-4287,[mapred] jobqueue_details.jsp shows negative count of running and waiting reduces with CapacityTaskScheduler.,This I observed while running a job that always fails because of reduce failures. Need to investigate this.
HADOOP-4284,Support for user configurable global filters on HttpServer,"HADOOP-3854 introduced a framework for adding filters to filter browser facing urls. Sometimes, there is a need to filter all urls. For example, at Yahoo, we need to open an SSL port on the HttpServer and only accept hsftp requests from clients who can authenticate themselves using client certificate and is authorized according to certain policy file. For this to happen, we need a method to add a user configurable ""global"" filter, which filters on all client requests. For our purposes, such a global filter will block all https requests except those accessing the hsftp interface (it will let all http requests go through, so accesses through the normal http ports are unaffected). Moreover, those hsftp requests will be subject to further authorization checking according to the policy file."
HADOOP-4283,[Hive] user should be able to write their own serde,"[Hive] user should be able to write their own serde. Their is no test currently and the DDL also does not allow that::

For example: the following syntax should be allowed:

CREATE TABLE INPUT16_CC(KEY STRING, VALUE STRING) ROW FORMAT SERIALIZER 'org.apache.hadoop.hive.serde2.TestSerDe'  with properties ('testserde.default.serialization.format'='\003', 'dummy.prop.not.used'='dummyy.val');
LOAD DATA LOCAL INPATH '../data/files/kv1_cc.txt' INTO TABLE INPUT16_CC;
SELECT INPUT16_CC.VALUE, INPUT16_CC.KEY FROM INPUT16_CC;


Here the user is specifying its own serde and input file and should be able to read using that"
HADOOP-4282,User configurable filter fails to filter accesses to certain directories,"Hadoop-3854 introduced a framework where users can add filters to the HTTP server to filter part of the HTTP interface, i.e., those user facing URLs. Directories ""/logs/*"" and ""/static/*"" are supposed to be filtered. However, files in those directories can be retrieved without triggering my configured filter. The corresponding junit test didn't catch the bug for 2 reasons. 1) it didn't try to access files in those 2 directories. 2) Even if it did, it might still fail to catch the bug, since according to my observation, only when accessing existent files my filter is bypassed. When accessing non-existent files (which is what the junit test does), my filter is triggered as expected."
HADOOP-4281,Capacity reported in some of the commands is not consistent with the Web UI reported data,Changes submitted for 2816 changed the Web UI to report the capacity differently. The same changes need to be applied to commands that show namenode and datanode capacities
HADOOP-4280,test-libhdfs consistently fails on trunk,{{ant -Dlibhdfs=yes test-libhdfs}} fails on 0.19 and trunk. It is ok on 0.18. I will attach output from a run.
HADOOP-4279,write the random number generator seed to log in the append-related tests,"There are quite a few append-related tests (see HADOOP-2658 for a list of related issues) failed occasionally.  Some of the tests use a random number generator.  We should write the random number generator seed to log and, hopefully, we could reproduce the failure."
HADOOP-4278,TestDatanodeDeath failed occasionally,"TestDatanodeDeath keeps failing occasionally.  For example, see
http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3365/testReport/"
HADOOP-4277,Checksum verification is disabled for LocalFS,"Since HADOOP-2063 checksum verification is disabled on ChecksumFileSystem used by LocalFS and others. HDFS is not affected. 

We should add test case that corrupts LocalFS file in {{TestFSInputChecker.java}}."
HADOOP-4276,The mapred.*ID classes are inefficient for hashCode and serialization,"Currently the ID classes call toString and hash the resulting string rather than computing a hash directly.

The ID classes also create new instances of the higher level object in readFields (via read) rather than re-using the object via readFields."
HADOOP-4275,New public methods added to the *ID classes,The mapred.*ID classes have had their interfaces expanded with inappropriate public methods that should be removed before they are released.
HADOOP-4274,Capacity scheduler's implementation of getJobs modifies the list of running jobs inadvertently,"The implementation of the {{getJobs}} API gets the running jobs queue from the {{JobQueueManager}}. If it is not null, it appends waiting jobs to the list and returns that. This modifies the original list of running jobs and can affect the scheduler drastically."
HADOOP-4273,[Hive] job submission exception if input is null,"If a table is empty, and the following query is issued:

select a, count(1) from x group by a;

the second map-reduce job dies (job submission fails since the input is empty)"
HADOOP-4272,Hive: metadataTypedColumnsetSerDe should check if SERIALIZATION.LIB is old columnsetSerDe,"If it is SERIALIZATION.LIB, then we should always use ReflectionObjectInspector on ColumnSet to make sure we can successfully access the field ""col"".
Prasad has a fix for this already.
"
HADOOP-4271,Bug in FSInputChecker makes it possible to read from an invalid buffer,"Bug in FSInputChecker makes it possible to read from an invalid buffer. The buffer in FSInputChecker becomes invalid when readChecksumChunk is used to read a chunk to a user buffer directly. Currently, it's not marked as invalid in this case and may be read subsequently."
HADOOP-4269,LineRecordReader.LineReader should use util.LineReader,"LineRecordReader.LineReader was moved to util and removed from the mapreduce package, but its implementation remains in mapred.LineRecordReader. It would be better if this used the implementation in util."
HADOOP-4268,Permission checking in fsck,"Quoting from HADOOP-3222 (""fsck should require superuser privilege""), 

bq. I agree that it makes sense to make fsck do permission checking for the nodes that it traverses. If a user does a fsck on files/directories that he/she has access to (using permissions) then that invocation of fsck should be allowed. Since ""/"" is usually owned by super-user, only super-user should be allowed to run fsck on ""/""."
HADOOP-4267,TestDBJob failed on Linux,"TestDBJob failed on nightly Linux build. 

I will attach output for the test."
HADOOP-4266,"Hive: Support ""IS NULL"", ""IS NOT NULL"", and size(x) for map and list","We should add UDFs to tell whether a map/list is null or not, and the size of a map/list.
"
HADOOP-4265,[Hive] error when user specifies the delimiter,"When the user  specifies the delimiter (which is different from the default delimiter), the data is not processed correctly. Consider the test case:

CREATE TABLE INPUT4_CB(KEY STRING, VALUE STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002' LINES TERMINATED BY '\012';
LOAD DATA LOCAL INPATH '../data/files/kv1_cb.txt' INTO TABLE INPUT4_CB;
SELECT INPUT4_CB.VALUE, INPUT4_CB.KEY FROM INPUT4_CB;
DROP TABLE INPUT4_CB



where kv1_cb.txt contains data seperated by Ctrl-B (\002) - the results are not as expected"
HADOOP-4262,message generated when the client exception has a null message is not useful,"This was created by HADOOP-3844; if the exception doesn't have a meaningful message, the output isn't that informative :
java.io.IOException: Call to localhost/127.0.0.1:8012 failed on local exception: null"
HADOOP-4261,Jobs failing in the init stage will never cleanup,"Pre HADOOP-3150, if the job fails in the init stage, {{job.kill()}} was called. This used to make sure that the job was cleaned up w.r.t 
- staus set to KILLED/FAILED
- job files from the system dir are deleted
- closing of job history files
- making jobtracker aware of this through {{jobTracker.finalizeJob()}}
- cleaning up the data structures via {{JobInProgress.garbageCollect()}}

Now if the job fails in the init stage, {{job.fail()}} is called which doesnt do the cleanup. HADOOP-3150 introduces cleanup tasks which are launched once the job completes i.e killed/failed/succeeded.  Jobtracker will never consider this job for scheduling as the job will be in the {{PREP}} state forever."
HADOOP-4260,support show partitions in hive,"support for

SHOW PARTITIONS <tablename>

in order to list the partitions of a table."
HADOOP-4259,findbugs should run over the tools.jar also,"Currently the findbugs target doesn't include the tools.jar, as I discovered when I moved a class from tools.jar to hadoop.jar and got hit by a findbugs warning."
HADOOP-4257,TestLeaseRecovery2.testBlockSynchronization failing.,"Found this while running HADOOP-4173 through Hudson.

HadoopQA output:
{code}
org.apache.hadoop.hdfs.TestLeaseRecovery2.testBlockSynchronization
Failing for the past 2 builds (Since Failed#3352 )
Took 0 seconds.
Error Message

Timeout occurred. Please note the time in the report does not reflect the time until the timeout.

Stacktrace

junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
{code}
See http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3353/testReport/org.apache.hadoop.hdfs/TestLeaseRecovery2/testBlockSynchronization/ and http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3352/testReport/org.apache.hadoop.hdfs/TestLeaseRecovery2/testBlockSynchronization/"
HADOOP-4256,Remove Completed and Failed Job tables from jobqueue_details.jsp,Remove completed and failed Job tables from _jobqueue_details.jsp_.
HADOOP-4255,Allow streaming to have a libjars option,StreamingJob does not currently allow a libjars option although it does have comparable options for file -> cacheFile and archive -> cacheArchive.  This improvement adds a libjars option to the StreamJob which takes a comma separated list of jars to include in the streaming job.  Jars are unjarred and added to the classes of the streaming job.
HADOOP-4254,Cannot setSpaceQuota to 1TB,"When setting space quota to 1TB, it shows ""/ by zero"".
{noformat}
bash-3.2$ ./bin/hadoop dfsadmin -setSpaceQuota 1TB /user
setSpaceQuota: / by zero
{noformat}"
HADOOP-4253,Fix warnings generated by FindBugs,"Findbugs generates several errors related to unused return values, thread synchronization and ambiguous method calls"
HADOOP-4252,Catch Ctrl-C in Hive CLI so that corresponding hadoop jobs can be killed,"Right now, Ctrl-C on hive cli will leave the current running jobs running. This can leave unnecessary jobs running on cluster wasting resources. Couple of alternatives here, kill the job or give a warning before quitting."
HADOOP-4250,"Remove short names of serdes from Deserializer, Serializer & SerDe interface and relevant code.","having short names necessitates the need for registering of classes that implement these interfaces and the maintenance of short name to class name map. there is no clear need for this so removing the short name feature altogether.

"
HADOOP-4249,Declare hsqldb.jar in eclipse plugin,lib/hsqldb.jar is not declared in eclipse plugin.
HADOOP-4248,Remove HADOOP-1230 API from 0.19,I think we should remove the HADOOP-1230 interface (org.apache.hadoop.mapreduce) from the 0.19 branch to avoid confusing users before it is live.
HADOOP-4247,hadoop jar throwing exception when running examples,"With trunk, running examples like wordcount or sleep is giving an exception. "
HADOOP-4246,Reduce task copy errors may not kill it eventually,maxFetchRetriesPerMap in reduce task can be zero some times (when maxMapRunTime is less than 4 seconds or mapred.reduce.copy.backoff is less than 4). This will not count reduce task copy errors to kill it eventually.
HADOOP-4245,KFS: Update the kfs jar file,Please update the kfs jar file in hadoop/lib to the one in this jira (kfs-0.2.2.jar).
HADOOP-4244,"In ""ant test-patch"", runContribTestOnEclipseFiles should not be run on Hudson only","runContribTestOnEclipseFiles is ONLY executed on Hudson but it won't be executed by default.  So it won't be run when developers run ""ant test-patch""."
HADOOP-4242,"Remove an extra "";"" in FSDirectory","In FSDirectory,
{code}
import org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption;;
{code}
Eclipse complains that 
{noformat}
Syntax error on token "";"", delete this token.
{noformat}
but javac seems okay about it."
HADOOP-4241,-hiveconf config parameters in hive cli should override all config variables,currently -hiveconf variables are only overriding hive specific config variables and a subset of hadoop config variables that are registered in hiveconf. this needs to be fixed since otherwise it's difficult to run queries with all manner of hadoop setting specified in cmdline.
HADOOP-4238,"[mapred] Unavailable schedulingInfo in ""hadoop list [all]"" output should be marked ""N/A""","When schedulingInfo is not available for a job, ""hadoop list [all]"" doesn't print anything. This, combined with the fact that the columns are not aligned properly, makes it difficult to comprehend the output."
HADOOP-4237,TestStreamingBadRecords.testNarrowDown fails intermittently,TestStreamingBadRecords.testNarrowDown is failing sometimes because of the delay in the processed counter updation (processed counter updation happens by the separate thread). Due to this there is a broader skipped range to be narrow down. The no of attempts provided in the test case get exhausted before completing the task successfully. 
HADOOP-4236,JobTracker.killJob() fails to kill a job if the job is not yet initialized,"HADOOP-3864 made the following changes to {{JobTracker.killJob()}}
{code}
   public synchronized void killJob(JobID jobid) {
     JobInProgress job = jobs.get(jobid);
-    job.kill();
+    if (job.inited()) {
+      job.kill();
+    }
   }
{code}
This is a bug as a job will not get killed if its not yet initialized."
HADOOP-4234,KFS: Allow KFS layer to interface with multiple KFS namenodes,"The KFS ""glue"" layer code in Hadoop, currently, only allows an application to interface with a single KFS namenode.  The KFS client side library has been modified to allow applications to interface with multiple KFS namenodes.  This jira issue is for incorporating the change into the KFS code.  "
HADOOP-4233,Streaming scripts need more information,"Streaming scripts are lack of useful information.
For example we need job.get(""map.input.file"")) in script.

Information can be provide by environment variable.
It's easy to implement by adding in PipeMapRed.configure() something like:
// add HADOOP_INPUTFILE environment variable with the value of input filename
envPut(childEnv, ""HADOOP_INPUTFILE"", job_.get(""map.input.file""));"
HADOOP-4232,Race condition in JVM reuse when more than one slot becomes free,"A race condition exists where there are two or more slots free and there are two or more tasks waiting to run. As an example, consider a case where there are two free slots and there are two tasks waiting to run. JVM_job1 and JVM_job2 are the two idle jvms in memory. A waiting task, task job1_t1, kills the JVM_job2 and spawns a new one, JVM_1_job1. While JVM_1_job1 is initializing (it is marked busy during initialization), JVM_job1 picks this task up and hence this becomes busy as well. Another waiting task, job3_t1 finds both the JVMs busy and doesn't spawn a new JVM."
HADOOP-4231,Hive: converting complex objects to JSON failed.,"java.lang.RuntimeException: java.lang.IllegalArgumentException: Can not set int field xxx to java.lang.String at 
org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.getStructFieldData(ReflectionStructObjectInspector.java:123)
        at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:231)
        at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:163)
        at org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.serialize(MetadataTypedColumnsetSerDe.java:181)
"
HADOOP-4230,Hive: GroupBy should not pass the whole row from mapper to reducer,"We only need to pass fields that are needed.
"
HADOOP-4228,"dfs datanode metrics, bytes_read, bytes_written overflows due to incorrect type used.","bytes_read, and bytes_written metrics are using int (MetricsTimeVaryingInt) as counter.  This type is too small to store the bytes_read and bytes_written metrics.  Recommend to change this to long (metricsLongValue)."
HADOOP-4227,"Remove the deprecated, unused class ShellCommand.",org.apache.hadoop.fs.ShellCommand was deprecated in 0.16 and is unused in current trunk.
HADOOP-4226,LineReader::readLine cleanup,"I've been looking at HADOOP-4010 and realized that readLine is pretty convoluted.  I changed the implementation which made it hopefully a little easier to read/validate/understand.  

I've had some problems testing it locally, so I'll submit it for Hudson to test."
HADOOP-4225,FSEditLog logs modification time instead of access time.,"{{FSEditLog.logOpenFile()}} does not persist accessTime but modificationTime instead. Should be {{getAccessTime()}}.
Introduced in HADOOP-1869."
HADOOP-4220,"Job Restart tests take 10 minutes, can time out very easily","HADOOP-3245 added job restart and tests for it, but the tests take a long time

TestJobTrackerRestart	667.682	
TestJobTrackerRestartWithLostTracker	322.223

Something needs to be done to speed them up to keep the test cycle viable."
HADOOP-4217,TestLimitTasksPerJobTaskSchedule test is failing on linux,"TestLimitTasksPerJobTaskSchedule test is failing on linux giving the following message:

   [junit] Running org.apache.hadoop.mapred.TestLimitTasksPerJobTaskScheduler
    [junit] Tests run: 5, Failures: 0, Errors: 5, Time elapsed: 0.363 sec
    [junit] Test org.apache.hadoop.mapred.TestLimitTasksPerJobTaskScheduler FAILED"
HADOOP-4213,NPE in TestLimitTasksPerJobTaskScheduler,"The test is failed consistently.  It is also failed on Hudson.
{noformat}
Testsuite: org.apache.hadoop.mapred.TestLimitTasksPerJobTaskScheduler
Tests run: 5, Failures: 0, Errors: 5, Time elapsed: 0.391 sec

Testcase: testMaxRunningTasksPerJob took 0.328 sec
	Caused an ERROR
null
java.lang.NullPointerException
	at org.apache.hadoop.mapred.LimitTasksPerJobTaskScheduler.start(LimitTasksPerJobTaskScheduler.java:53)
	at org.apache.hadoop.mapred.TestJobQueueTaskScheduler.setUp(TestJobQueueTaskScheduler.java:197)

...
{noformat}"
HADOOP-4211,"Capacity Scheduler does not divide queue resources properly among users, when jobs are submitted one after other.","Capacity Scheduler does not divide queue resources  properly among users, when job are submitted one after other. E.g. user limit =25. Say User1's job is running. Then user2 submits a job. Then user1's job uses 75% and user2's job 25%=user limit."
HADOOP-4210,Findbugs warnings are printed related to equals implementation of several classes,"During compilation - findbugs generates several warnings that indicates bugs in the implementation of equals method. One of the example of this report is:

Bug type EQ_GETCLASS_AND_CLASS_CONSTANT (click for details)
In class org.apache.hadoop.mapred.ID
In method org.apache.hadoop.mapred.ID.equals(Object)
At ID.java:[line 66]
Value doesn't work for subtypes

This class has an equals method that will be broken if it is inherited by subclasses. It compares a class literal with the class of the argument (e.g., in class Foo it might check if Foo.class == o.getClass()). It is better to check if this.getClass() == o.getClass().


"
HADOOP-4209,The TaskAttemptID should not have the JobTracker start time,The TaskAttemptID now includes the redundant copy of the JobTracker's start time as milliseconds. We should instead change the JobID to have the longer unique string.
HADOOP-4207,update derby.jar to eliminate the one time wait of 2-3 minutes while running junit tests,current derby.jar (version 10.4.1) has a bug where getting metadata of tables can hang upto few minutes. Hive metastore validates the existing tables & metadata while connecting and it can encounter this bug sometimes. The new version of derby.jar has a fix for this bug which would eliminate this hang.
HADOOP-4206,fuse-dfs support using fuse's default_permission checking,"fuse-dfs can get posix permission checking by using this flag. fuse will assume 'root' is the superuser of the FS. Since HDFS does not do anything like a ""root_squash"" in exporting the FS,  this is consistent with the current security model.

"
HADOOP-4205,[Hive] metastore and ql to use the refactored SerDe library,"Metastore and ql should use the refactored SerDe library.
"
HADOOP-4204,Fix warnings generated by FindBugs,There are several warnings currently printed by findbugs. These need to be addressed.
HADOOP-4203,implement LineReader (ie Text) Serialization/Serializer/Deserializer,"Implement a wrapper around LineRecordReader.LineReader so as to support Text in the Serialization framework.
"
HADOOP-4202,"TaskTracker never stops cleanup threads, MiniMRCluster becomes unstable","If many unit tests start/stop unique MiniMRCluster instances, over time the number of threads in the test vm grow to large causing tests to hang and/or slow down."
HADOOP-4200,Hadoop-Patch build is failing ,"Hadoop-Patch build is failing with the following error:

test-patch.sh: line 165: -Xmaxwarns: command not found
[exec] BUILD FAILED
     [exec] Target ""&>"" does not exist in the project ""Hadoop"". 
     [exec]
     [exec] Trunk findbugs is broken?
"
HADOOP-4198,DFSClient should do lease recovery using data transfer port.,"HADOOP-3283 introduced {{dfs.datanode.ipc.address}} which defines where the data-node rpc server runs.
The rpc server on a data-node is used only for lease recovery (HADOOP-3310).
Lease recovery can be initialized by a name-node or by a client.
The problem was reported if lease recovery is initialized by a client running on an untrusted host.
The port that the http server runs on is closed for the outside use and therefore lease recovery fails.
Production level security model assumes that data-nodes are run on trusted nodes and therefore it is safe to have ports open for inter data-node communication.
HDFS clients can run on arbitrary nodes and according to the security model can access only the ports that are externally open.
We propose to use the standard data node port for lease recovery, which means that lease recovery will use {{DataXceiver}} and data transfer protocol rather than {{ClientDatanodeProtocol}}.
"
HADOOP-4197,Need to update DATA_TRANSFER_VERSION,DATA_TRANSFER_VERSION was not updated when the protocol was changed in HADOOP-3981.
HADOOP-4195,SequenceFile.Writer close() uses compressor after returning it to CodecPool.,"In function SequenceFile.Writer.close() (line 946): The first marked line returns the compressor while the second marked line will use the compressor again. This will lead to a race condition if another thread checks out the compressor between these two marked statements.

{code:title=SequenceFile.java|borderStyle=solid}
    public synchronized void close() throws IOException {
      CodecPool.returnCompressor(compressor); // <==== compressor returned
      
      keySerializer.close();
      uncompressedValSerializer.close();
      if (compressedValSerializer != null) {
        compressedValSerializer.close(); // <===== compressor used
      }

      if (out != null) {
        
        // Close the underlying stream iff we own it...
        if (ownOutputStream) {
          out.close();
        } else {
          out.flush();
        }
        out = null;
      }
    }
{code} 
"
HADOOP-4194,Add JobConf and JobID to job related methods in JobTrackerInstrumentation,"For example, would like to change from,
    public void submitJob()
to
    public void submitJob(JobConf conf, JobID id)

So the instrumentation class could retrieve / store job specific information."
HADOOP-4193,Tweak the presentation of Forrest-generated pages,New appearances.
HADOOP-4192,Class <? extends T> Deserializer.getRealClass() method to return the actual class of the objects from a deserializer,"Note: this use case is completely for non-self describing files with Serialization framework records.  If the Serialization Class and the actual type of records to be deserialized is configured higher up through the JobConf.  

It is motivated by the need to create a generic FlatFileDeserializerRecordReader that can be configued to use any Serialization implementation through the JobConf.

Since A deserializer can return a subtype of the type it is instantiated to return, we can create generic Deserializers for a base type - e.g., Writable, Record, Thrift.Tbase where the RecordReader need not be specific to any of them.  

In which case,to implement RecordReader.getValueClass();, the generic RecordReader really needs to query that from the Deserializer.

And since this RecordReader is generic even ithe Serialization Implementation it is going to use should come from the JobConf as should the actual specific class being Deserialized. e.g., Record/MyUserIDRecord, Writable/LongWritable.

The RecordReader would need to know how the Serialization and Deserializer get their configuration info to implement getValueClass().

A much cleaner way is to implement getRealClass I think.
 "
HADOOP-4191,Add a testcase for jobhistory,"Changes in job history might break the history parser which in turn might break some features like jobtracker-recovery, history-viewer etc. There should be a testcase that catches these incompatible changes early and informs about the expected change.

This patch validates job history files so that issues related to jobHistory are caught when changes to JobHistory  are made.
Validates history file name, file location, parsability. Validates user log location for varoius configuration settings. Validates job status for jobs that are succeeded, failed and killed.
Validates format of history file contents and also validates contents by comparing with actual values obtained from job tracker."
HADOOP-4190,Changes to JobHistory makes it backward incompatible,"With HADOOP-3245, the job history format has changed. Every log line now ends with a '.' (dot). JobHistory's parser will consider a history line valid only if it ends in a dot
Hence the new parser wont be able to parse old history files."
HADOOP-4189,HADOOP-3245 is incomplete,"There are 2 issues with HADOOP-3245
- The default block size for the history files in hadoop-default.conf is set to 0. This file will be empty throughout. It should be made null.
- Same goes for the buffer size. 
- InterTrackerProtocol version needs to be bumped."
HADOOP-4188,Remove Task's dependency on concrete file systems,
HADOOP-4187,Create a MapReduce-specific ReflectionUtils that handles JobConf and JobConfigurable,
HADOOP-4186,Move LineRecordReader.LineReader class to util package,
HADOOP-4185,Add setVerifyChecksum() method to FileSystem,
HADOOP-4184,"Fix simple module dependencies between core, hdfs and mapred",
HADOOP-4183,select * to console issues in Hive,"the biggest problem is that select * is assuming that the output is a regular text file. it can't read out from compressed text files. (which is what happens when we start using the output compression options).

the core issue is that select * is special code. and it should not be. it should go through the same 'fileinputformat' and serde code that any map-side task goes through.

the second issue is that a select * without any data transformations/filters goes through a map-reduce task unnecessarily. we need to fix this. this seems related to the limit N jira that ashish opened. (see hadoop-4086)"
HADOOP-4182,Streaming Documentation Update,"When Text input data is used with streaming, every line is expected to end with a newline.  Hadoop results are undefined if input files do not end in a newline.  (The results will depend on how files are assigned to mappers.)

Example:

In streaming if

mapper = xargs cat
reducer = cat

and the input is a two line, where each line is symbolic link in HDFS

link1\n
link2\n
EOF

link1 points to a file which contains

This is line1EOF

link2 points to a file which  contains

This is line2EOF

Now running a streaming job such that, there is only one split, will produce results:

This is line1This is line2\t\n

But if there were two splits, the result will be

This is line1\t\n
This is line2\t\n

So in summary, the output depends on the factor that how many mappers were invoked.  As a caution, it should be recorded in Streaming wiki that users always put a new line at the end of each line to get away with such problems."
HADOOP-4181,some minor things to make Hadoop friendlier to git,It would be nice to have a .gitignore file and a saveVersion that didn't fail under git.
HADOOP-4179,Hadoop-Vaidya : Rule based performance diagnostic tool for Map/Reduce jobs,Hadoop-Vaidya is a rule based performance diagnostic tool for Map/Reduce jobs. It performs a post execution analysis of map/reduce job by parsing and collecting execution statistics through job history log and job configuration files. It runs a set of predefined tests/rules against job execution statistics to diagnose various performance problems. Each test rule detects a specific performance problem with job and provides a targeted advice to the user. This tool generates an XML report based on the evaluation results of individual test rules.  
HADOOP-4178,The capacity scheduler's defaults for queues should be configurable.,The default values for the queue attributes should be configurable rather than hard coded.
HADOOP-4176,Implement getFileChecksum(Path) in HftpFileSystem,"In order to use FileChecksum in DistCp, we should implement getFileChecksum(Path) in HftpFileSystem"
HADOOP-4175,Incorporate metastore server review comments,As the title says. Will list detailed fixes in the comments.
HADOOP-4174,Move non-client methods ou of ClientProtocol,"3 ClientProtocol methods {{getEditLogSize()}}, {{rollEditLog()}}, {{rollFSImage()}} are never called and should not be by DFSClient.
These methods should be moved into NamenodeProtocol."
HADOOP-4173,TestProcfsBasedProcessTree failing on Windows machine,"TestProcfsBasedProcessTree unit test is failing on Windows. Here is the output:
    [junit] Running org.apache.hadoop.util.TestProcfsBasedProcessTree
    [junit] 2008-09-14 21:39:24,354 INFO  util.TestProcfsBasedProcessTree (TestProcfsBasedProcessTree.java:testProcessTree(121)) - Root process pid: 5512
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.609 sec
    [junit] Test org.apache.hadoop.util.TestProcfsBasedProcessTree FAILED"
HADOOP-4172,Unit tests (mapred) failing on Windows machine,"The following unit tests are failing on Windows machine:
1) TestMiniMRClasspath
2) TestMiniMRDFSSort
3) TestMiniMRLocalFS
4) TestMiniMRWithDFSWithDistinctUsers
5) TestTaskTrackerMemoryManager
"
HADOOP-4171,Unit tests failing on Windows,"The following unit tests are failing on Windows machine:
TestFileCreation
TestHDFSFileSystemContract
TestInjectionForSimulatedStorage"
HADOOP-4169,'compressed' keyword in DDL syntax misleading and does not compress,"Hive produces two types of data files - flat files and sequencefiles. Syntax should reflect this. Currently the 'compressed' keyword is used to choose sequencefile format - but does not actually compress the files. this is misleading. In addition - flat files can also be compressed.

Proposal is to replace 'compressed' with 'sequencefile'. And compression options should be applied from standard hadoop way of specifying whether output should be compressed (''mapred.output.compress') - ie. session options. (session options will also define codec etc.). default file format and compression options can be specified in conf file."
HADOOP-4163,"If a reducer failed at shuffling stage, the task should fail, not just logging an exception","
I saw a reducer stuck at the shuffling stage, with the following exception logged in the log file:

2008-08-30 00:16:23,265 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:140)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:59)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:79)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.close(ChecksumFileSystem.java:332)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:59)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:79)
	at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:185)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:815)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:764)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)
	... 11 more

2008-08-30 00:16:23,320 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.io.IOException: task_200808291851_0001_r_000023_0The reduce copier failed
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:329)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2122)

The task should have died.

"
HADOOP-4161,[HOD] Uncaught exceptions can potentially hang hod-client.,"In hod-client, we have
{code}
sys.exit(hod.operation())
sys.exit(hod.script())
{code}
sys.exit(opCode) makes sure that the client is truly cleaned up, killing unjoined threads etc. So, exceptions not caught by hodRunner.operation() or hodRunner.script(), will by-pass sys.exit method and thus can potentially hang hod-client.

For e.g., when hod allocate fails after allocation and before service-registry thread is cleaned up, hod client will hang.
"
HADOOP-4155,JobHisotry::JOBTRACKER_START_TIME is not initialized properly,"JobHistory::JOBTRACKER_START_TIME seems to be initalize to current time. I think it should be initialized to current jobtracker's start time.
"
HADOOP-4154,Fix javac warning in WritableUtils,There a few javac warning in WritableUtils.
HADOOP-4151,Add a memcmp-compatible interface for key types,"Sometimes it's beneficial to treat types (like BytesWritable and Text) as byte arrays, agnostic to their type. Adding an interface to tag key types as amenable to this treatment can permit optimizations and reuse for that subset of types in tools and libraries."
HADOOP-4150,Include librecordio as part of the release,
HADOOP-4149,JobQueueJobInProgressListener.jobUpdated() might not work as expected,"{{JobQueueJobInProgressListener}} uses a {{TreeSet}} to store the sorted collection of {{JobInProgress}} objects. The comparator used to sort the JIPs follow the following order
- priority (>=)
- start time (<=)
- job id [jt-identifier, job-index] (<=)

If any JIP object is changed w.r.t priority or start-time, then the TreeSet will be inconsistent. Hence doing  a delete might not work. Consider the following
1) jobs are submitted in the following order 
||number||jobid||priority||
|1|j1|NORMAL|
|2|j2|LOW|
|3|j3|NORMAL|

2) The sorted collection will be in the order : {{j1,j3,j2}}

3) If job3's priority is changed to LOW then the collection wont change but delete will bail out on j1 itself as the comparator will return a -ve number. TreeSet uses the comparator both for sorting and deleting. If  i indicates the index in the collection and obj represents the object under consideration, then looks like TreeSet.remove(obj) follows something like  :
- continue to search if the compare(i, obj) is -ve
- bail out if the compare(i, obj) is +ve
- delete the obj of compare(i,obj) == 0"
HADOOP-4148,DiskChecker$DiskErrorException,"hi

1- redhat - master( jobtracker + namenode+ tasktracker + datanode)
1- ubuntu - slave ( tasktracker + datanode)

when i execute
 bin/hadoop jar word/word.jar org.myorg.WordCount in mn2

08/09/10 15:12:56 INFO mapred.FileInputFormat: Total input paths to process : 5
08/09/10 15:12:56 INFO mapred.JobClient: Running job: job_200809101511_0003
08/09/10 15:12:57 INFO mapred.JobClient:  map 0% reduce 0%
08/09/10 15:13:00 INFO mapred.JobClient:  map 20% reduce 0%
08/09/10 15:13:01 INFO mapred.JobClient:  map 80% reduce 0%
08/09/10 15:13:02 INFO mapred.JobClient:  map 100% reduce 0%
08/09/10 15:13:11 INFO mapred.JobClient:  map 100% reduce 13%
08/09/10 15:30:41 INFO mapred.JobClient:  map 80% reduce 13%
08/09/10 15:30:41 INFO mapred.JobClient: Task Id : task_200809101511_0003_m_000000_0, Status : FAILED
Too many fetch-failures
08/09/10 15:30:42 WARN mapred.JobClient: Error reading task outputhttp://localhost:50060/tasklog?plaintext=true&taskid=task_200809101511_0003_m_000000_0&filter=stdout
08/09/10 15:30:42 WARN mapred.JobClient: Error reading task outputhttp://localhost:50060/tasklog?plaintext=true&taskid=task_200809101511_0003_m_000000_0&filter=stderr
08/09/10 15:30:44 INFO mapred.JobClient:  map 100% reduce 13%
08/09/10 15:30:49 INFO mapred.JobClient:  map 100% reduce 20%
08/09/10 15:40:52 INFO mapred.JobClient: Task Id : task_200809101511_0003_m_000004_0, Status : FAILED
Too many fetch-failures
08/09/10 15:40:52 WARN mapred.JobClient: Error reading task outputhttp://localhost:50060/tasklog?plaintext=true&taskid=task_200809101511_0003_m_000004_0&filter=stdout
08/09/10 15:40:52 WARN mapred.JobClient: Error reading task outputhttp://localhost:50060/tasklog?plaintext=true&taskid=task_200809101511_0003_m_000004_0&filter=stderr
08/09/10 15:41:03 INFO mapred.JobClient:  map 100% reduce 26%

it halts

when i saw the tasktracker's log, i found

 getMapOutput(task_200809101511_0003_m_000004_0,0) failed :
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_200809101511_0003/task_200809101511_0003_m_000004_0/output/file.out.index in any of the configured local directories
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:359)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)
	at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2315)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

2008-09-10 15:33:12,915 WARN org.apache.hadoop.mapred.TaskTracker: Unknown child with bad map output: task_200809101511_0003_m_000004_0. Ignored.
2008-09-10 15:33:17,425 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:23,431 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:29,437 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:32,439 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:38,445 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:44,451 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:47,454 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:53,460 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:33:59,465 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:34:02,469 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:34:08,475 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:34:14,480 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:34:17,484 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:34:23,490 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:34:29,495 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 
2008-09-10 15:34:32,498 INFO org.apache.hadoop.mapred.TaskTracker: task_200809101511_0003_r_000000_0 0.20000002% reduce > copy (3 of 5 at 0.00 MB/s) > 

reducer task runs on master(redhat)
the task_200809101511_0003_m_000004_0/ specified in the log was done in slave(ubuntu)

in jobtracker's log, i found

2008-09-10 15:35:46,977 INFO org.apache.hadoop.mapred.JobInProgress: Failed fetch notification #2 for task task_200809101511_0003_m_000004_0

hadoop-site.xml

<configuration>
<property>
    <name>fs.default.name</name>
    <value>hdfs://master:54310/</value>
<final>true</final>
  </property>

  <property>
    <name>mapred.job.tracker</name>
    <value>master:54311</value>
<final>true</final>
  </property>

  <property>
    <name>dfs.replication</name>
    <value>2</value>
<final>true</final>
  </property> 

<property>
  <name>hadoop.tmp.dir</name>
  <value>absolute path</value>
  <final>true</final>
</property>

<property>
  <name>mapred.child.java.opts</name>
  <value>-Xmx512M</value>
<final>true</final>
</property>

<property>
<name>mapred.speculative.execution</name>
<value>false</value>
<final>true</final>
</property>
</configuration>

i dont know where i went wrong ..
kindly help me solving this
thanks 

Chandravadana
"
HADOOP-4147,Remove JobWithTaskContext from JobInProgress,{{JobWithTaskContext}} was used by the task commit thread. HADOOP-3150 changed the task commit process and {{JobWithTaskContext}} is no longer required.
HADOOP-4146,[Hive] null pointer exception on a join,"create table xx(aa int);
select a.* from xx a join xx b on a.aa = b.aa;


gets a nullpointer exception in join operator. close()

The testcase works fine when it is issued from the command line - but it fails from ant"
HADOOP-4145,[HOD] Support an accounting plugin script for HOD,Production environments have accounting packages to track usage of a cluster. HOD should provide a mechanism to plug-in an accounting related script that can be used to verify if the user is using a valid account or not.
HADOOP-4143,"Support for a ""raw"" Partitioner that partitions based on the serialized key and not record objects","For some partitioners (particularly those using comparators to classify keys), it would be helpful if one could specify a ""raw"" partitioner that would receive the serialized version of the key rather than the object emitted from the map."
HADOOP-4141,ScriptBasedMapping has infinite loop with invalid jobconf parameter,"A few bugs in ScriptBasedTopology:

1) topology.script.number.args is not validated; if this is <= 0, the runResolveCommand() method will run its outer loop an infinite number of times. 

2) CachedDNSToSwitchMapping assumes that if a List is returned, it has the same cardinality as the number of names. ScriptBasedMapping should return 'null' instead of a mis-sized list on error"
HADOOP-4139,[Hive] multi group by statement is not optimized,"A simple multi-group by statement is not optimized. A simple statement like:

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, count(distinct  SUBSTR(SRC.value,4)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, count(distinct  SUBSTR(SRC.value,4)) GROUP BY SRC.key;


results in making 2 copies of the data (SRC). Instead, the data can be first partially aggregated on the distinct value and then aggregated. 
The first step can be common to all group bys."
HADOOP-4138,[Hive] refactor the SerDe library,"Hive uses the library from src/contrib/hive/serde to do serialization/deserialization.
We want to do a refactoring of the library to:

1. Split Serializer and Deserializer interface
2. Split Serializer/Deserializer and ObjectInspector interface
3. Change hive/metaserver and hive/ql to use the new SerDe framework
"
HADOOP-4137,NPE in GangliaContext.xdr_string (GangliaContext.java:195),"Exception in thread ""Timer thread for monitoring dfs"" java.lang.NullPointerException
        at org.apache.hadoop.metrics.ganglia.GangliaContext.xdr_string(GangliaContext.java:195)
        at org.apache.hadoop.metrics.ganglia.GangliaContext.emitMetric(GangliaContext.java:138)
        at org.apache.hadoop.metrics.ganglia.GangliaContext.emitRecord(GangliaContext.java:123)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.emitRecords(AbstractMetricsContext.java:304)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:290)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:50)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:249)
        at java.util.TimerThread.mainLoop(Unknown Source)
        at java.util.TimerThread.run(Unknown Source)

It looks like this caused the datanode to hang, though I accidentally killed the datanode before I could dump its stack."
HADOOP-4135,change max length of database columns for metastore to 767 ,for utf8 characters the max is 767. package.jdo should be changed accordingly.
HADOOP-4133,remove derby.log files form repository and also change the location where these files get created,running unit tests generate derby.log files in the current working directory. these files were mistakenly checked into the repository as well. this jira is to move them from repo and change the location of the derby.log file.
HADOOP-4129,Memory limits of TaskTracker and Tasks should be in kiloBytes.,"HADOOP-3759 uses memory limits in kilo-bytes and HADOOP-3581 changed it to be in bytes. Because of this, TestHighRAMJobs is failing on Linux. We should change this behaviour so that all memory limits are considered to be in kilo-bytes."
HADOOP-4127,TestHighRAMJobs fails on linux,"TestHighRAMJobs fails on linux box with the following errors:
{noformat}
------------- ---------------- ---------------
------------- Standard Error -----------------
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start...
TaskTree [pid=25566,tipID=attempt_200809091523_0001_m_000000_0] is running beyond memory-limits. Current usage : 358998016. Limit : 268435456. Killing task.
TaskTree [pid=25815,tipID=attempt_200809091523_0001_m_000000_1] is running beyond memory-limits. Current usage : 359047168. Limit : 268435456. Killing task.
TaskTree [pid=26110,tipID=attempt_200809091523_0001_m_000000_2] is running beyond memory-limits. Current usage : 360296448. Limit : 268435456. Killing task.
TaskTree [pid=26165,tipID=attempt_200809091523_0001_r_000000_0] is running beyond memory-limits. Current usage : 357711872. Limit : 268435456. Killing task.
TaskTree [pid=26251,tipID=attempt_200809091523_0001_m_000001_0] is running beyond memory-limits. Current usage : 352362496. Limit : 268435456. Killing task.
Waiting for the Mini HDFS Cluster to start...
TaskTree [pid=26473,tipID=attempt_200809091524_0001_m_000000_0] is running beyond memory-limits. Current usage : 358699008. Limit : 357913941. Killing task.
TaskTree [pid=26499,tipID=attempt_200809091524_0001_m_000000_1] is running beyond memory-limits. Current usage : 360230912. Limit : 357913941. Killing task.
TaskTree [pid=26528,tipID=attempt_200809091524_0001_m_000001_0] is running beyond memory-limits. Current usage : 358006784. Limit : 357913941. Killing task.
TaskTree [pid=26584,tipID=attempt_200809091524_0001_m_000000_2] is running beyond memory-limits. Current usage : 358866944. Limit : 357913941. Killing task.
TaskTree [pid=26638,tipID=attempt_200809091524_0001_m_000001_1] is running beyond memory-limits. Current usage : 358649856. Limit : 357913941. Killing task.
------------- ---------------- ---------------

Testcase: testDefaultValuesForHighRAMJobs took 22.564 sec
Testcase: testDefaultMemoryPerTask took 21.33 sec
Testcase: testConfiguredValueForFreeMemory took 52.748 sec
	Caused an ERROR
Job failed!
java.io.IOException: Job failed!
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1156)
	at org.apache.hadoop.examples.SleepJob.run(SleepJob.java:174)
	at org.apache.hadoop.examples.SleepJob.run(SleepJob.java:237)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.mapred.TestHighRAMJobs.launchSleepJob(TestHighRAMJobs.java:247)
	at org.apache.hadoop.mapred.TestHighRAMJobs.runJob(TestHighRAMJobs.java:238)
	at org.apache.hadoop.mapred.TestHighRAMJobs.testConfiguredValueForFreeMemory(TestHighRAMJobs.java:174)

Testcase: testHighRAMJob took 48.952 sec
	Caused an ERROR
Job failed!
java.io.IOException: Job failed!
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1156)
	at org.apache.hadoop.examples.SleepJob.run(SleepJob.java:174)
	at org.apache.hadoop.examples.SleepJob.run(SleepJob.java:237)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.mapred.TestHighRAMJobs.launchSleepJob(TestHighRAMJobs.java:247)
	at org.apache.hadoop.mapred.TestHighRAMJobs.runJob(TestHighRAMJobs.java:238)
	at org.apache.hadoop.mapred.TestHighRAMJobs.testHighRAMJob(TestHighRAMJobs.java:198)

{noformat}
"
HADOOP-4126,Allow access to HDFS web UI on EC2,Need to open access to ports 50070 and 50075 in the Hadoop EC2 security groups.
HADOOP-4125,Reduce cleanup tip web ui is does not show attempts,"Reduce cleanup tip web ui is does not show attempts, because cleanup tip is not returned correctly from JobInProgress.getTaskInProgress(tipid). And also the status for the reduce tip is shown as cleanup > copy, this is because the reduce task adds all the phases statically. "
HADOOP-4124,Changing priority of a job should be available in CLI and available on the web UI only along with the Kill Job actions,"Currently, a job's priority can only be changed from the job tracker web UI. However, it is really similar (if not as destructive) as killing a job, in the sense that is should not be exposed publicly. The use case for this kind of restriction is where Hadoop is shared in production environments by different users, and they should not be allowed to change another user's priority. At the same time, it should be possible to still change a job's priority in some manner, if a user is authorized to do so, and the CLI can provide a way to do this, along with mechanisms introduced for ACLs in HADOOP-3698."
HADOOP-4121,HistoryViewer initialization failure should log exception trace ,"If there is any exception in HistoryViewer's constructor, it throws an IOException. But the exception message should contain the exception trace."
HADOOP-4120,[Hive] print time taken by query in interactive shell,As title.
HADOOP-4117,Improve configurability of Hadoop EC2 instances,"Currently hadoop-site.xml for EC2 instances is stored as a part of the image and only a few properties can be controlled from the user scripts (compression, number of map/reduce tasks). Furthermore, it is not possible to rsync the configuration around the EC2 cluster with the current image, so the only way to customize the hadoop-site.xml file is to rebuild the image, which is time-consuming.

It would be much better to pass the initialization script for nodes at boot time, so that it is easy to edit the configuration before starting a cluster."
HADOOP-4116,Balancer should provide better resource management,"The number of threads are currently limited on datanodes. Once these threads are occupied, DataNode does not accept any more requests (DOS). Recently we saw a case where most of the 256 threads were waiting in {{DataXceiver.replaceBlock()}} trying to acquire  {{balancingSem}}.  Since rebalancing  is (heavily) throttled, I would think this would be the common case. 

These operations waiting  for active rebalancing threads to finish need not take up a thread. "
HADOOP-4115,Reducer gets stuck in shuffle when local disk out of space,"2008-08-29 23:53:12,357 WARN org.apache.hadoop.mapred.ReduceTask: task_200808291851_0001_r_000245_0 Merging of the local FS files threw an exception: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:47)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:339)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:155)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:47)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.io.SequenceFile$UncompressedBytes.writeUncompressedBytes(SequenceFile.java:617)
	at org.apache.hadoop.io.SequenceFile$Writer.appendRaw(SequenceFile.java:1038)
	at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2626)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$LocalFSMerger.run(ReduceTask.java:1564)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)
	... 16 more

2008-08-29 23:53:14,013 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.io.IOException: task_200808291851_0001_r_000245_0The reduce copier failed
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:329)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2122)"
HADOOP-4113, libhdfs should never exit on its own but rather return errors to the calling application - missing diff files,"Hadoop-3963 second patch did not include the small changes to hdfsJniHelper.c. sorry.

this definitely concludes removing exits from libhdfs."
HADOOP-4112,Got ArrayOutOfBound exception while analyzing the job history,"HADOOP-3150 introduced 2 new type of tasks called the cleanup tasks. These are logged to history either as map tasks or reduce tasks. The number of maps/reducers is also logged to history. Since the number of maps will be less than the total number of map tasks logged to history (actual num maps + cleanup tasks), I think thats the reason for this exception. The important question is to investigate the effect of HADOOP-3150 on job history and code related to it."
HADOOP-4106,"add time, permission and user attribute support to fuse-dfs","add:

dfs_chown
dfs_utime
dfs_chmod

Change open to have its own FS on writes (should we do this on reads too??) and use it for writes and disconnect when closing the file
Chane mkdir to open the FS itself and then close it
also added comments for dfs_access (which needs FileSystem support/libhdfs support) and I added the dfs_symlink and truncate since these 3 are the only 3 things left as far as functionality.

"
HADOOP-4105,libhdfs wiki is very out-of-date and contains mostly broken links,"This thing is way out of date and seems incorrect in some places. Also note it is not linked from the wiki front page.

http://wiki.apache.org/hadoop/LibHDFS
"
HADOOP-4104,"add time, permission and user attribute support to libhdfs","1. add mtime, accesstime, owner, permissions to HDFSFileInfo
2. create hdfsChown, hdfsChmod, hdfsUtime functions
3. create hdfsConnectAsUser function



"
HADOOP-4103,Alert for missing blocks,"A whole bunch of datanodes became dead because of some network problems resulting in  heartbeat timeouts although datanodes were fine.

Many processes started to fail because of the corrupted filesystem.

In order to catch and diagnose such problems faster the namenode should detect the corruption automatically and provide a way to alert operations. At the minimum it should show the fact of corruption on the GUI."
HADOOP-4100,Scheduler.assignTasks should not be dealing with cleanupTask,HADOOP-3150 introduced the notion of a cleanupTask. The implementation of the scheduling of the cleanupTask is such that the schedulers have to be aware of the cleanupTask. It would be very nice if schedulers didn't have to be aware of this special task. There is a discussion thread - https://issues.apache.org/jira/browse/HADOOP-3150?focusedCommentId=12628635#action_12628635
HADOOP-4099,HFTP interface compatibility with older releases broken,"Using current trunk (Revision 692556) build and trying to distcp from a 0.18.0 cluster via HFTP, got the following NullPointerException. Seems like on line 165 in HftpFileSystem.java, a potential null return value isn't checked before using.

coursemud-lm:bin kan$ ./hadoop distcp hftp://ucdev18.inktomisearch.com:50070/output/part-00000 /copied/part-00001
08/09/05 19:00:10 INFO tools.DistCp: srcPaths=[hftp://ucdev18.inktomisearch.com:50070/output/part-00000]
08/09/05 19:00:10 INFO tools.DistCp: destPath=/copied/part-00001
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.lang.NullPointerException
	at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1215)
	at java.text.DateFormat.parse(DateFormat.java:335)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.startElement(HftpFileSystem.java:165)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:501)
	at com.sun.org.apache.xerces.internal.parsers.AbstractXMLDocumentParser.emptyElement(AbstractXMLDocumentParser.java:179)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:377)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2740)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:647)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:508)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.fetchList(HftpFileSystem.java:194)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.getFileStatus(HftpFileSystem.java:205)
	at org.apache.hadoop.hdfs.HftpFileSystem.getFileStatus(HftpFileSystem.java:234)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:649)
	at org.apache.hadoop.tools.DistCp.checkSrcPath(DistCp.java:614)
	at org.apache.hadoop.tools.DistCp.copy(DistCp.java:631)
	at org.apache.hadoop.tools.DistCp.run(DistCp.java:838)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.tools.DistCp.main(DistCp.java:865)

"
HADOOP-4098,Update and add PHP & Python clients to hive packaging directory,php & python metastore server client libraries should be added to the hive packaging
HADOOP-4097,Hive interaction with speculative execution is broken,"Hive does not cleanup output files when tasks are killed. With speculative execution, partial result files may get left behind. The commit step of the results should detect and remove any partial output files."
HADOOP-4095,[Hive] enhance describe table & partition,"describe should take another option that will show all the details of the table
describe should take partition information to describe schema of that partition"
HADOOP-4094,[Hive]implement hive-site.xml similar to hadoop-site.xml,currently there is only one configuration file for hive that stores both defaults and site specific config parameters. change hive to take hive-stie.xml that overwrites what is specified in hive-default.xml 
HADOOP-4093,[Hive]unify Table.getCols() & get_fields(),"getCols() is the correct call for Hive native objects and get_fields() is the correct call for custom serde tables. getCols() should return correct values even for custom erde tables. As a side effect, this should store field names for custom serde tables in the metadata for query purposes."
HADOOP-4091,The command line interface requires that you enter everything as one big text -- this makes it difficult to test out commands by pasting them from my SQL file.  I've been using the -f option instead for now.,"The command line interface requires that you enter everything as one big text -- this makes it difficult to test out commands by pasting them from my SQL file.  I've been using the -f option instead for now.

This was reported by Chad."
HADOOP-4090,The configuration file lists two paths to hadoop directories (bin and conf).  Startup should check that these are valid directories and give appropriate messages.,"The configuration file lists two paths to hadoop directories (bin and conf).  Startup should check that these are valid directories and give appropriate messages.

Reported by Chad."
HADOOP-4089,Check if the tmp file used in the CLI exists before using it.,"Currently the CLI needs /tmp/<user-name> directory to be present, otherwise it gives an error.

We should create it automatically when we launch the CLI.

This was reported by Chad."
HADOOP-4087,Make Hive metastore server to work for PHP & Python clients,"RawStore object in HiveMetaStore is not thread-safe. This is ok in local mode (where most of the java client work) but not for Python & PHP clients who need a stand-alone remote server. The client interface for Python is little old so need to update that as well.
"
HADOOP-4086,Add limit to Hive QL,"Add a limit feature to the Hive Query language.

so you can do the following things:

SELECT * FROM T LIMIT 10;

and this would just return the 10 rows.

No gaurantees are made on which 10 rows are returned by the query.
"
HADOOP-4084,Add explain plan capabilities to Hive QL,"Adding explain plan for queries in hive.

The current proposal is to support something like:

EXPLAIN [EXTENDED]
SELECT ....

This will output the following:

Abstract Syntax Tree:

Number of Stages:

Dependencies between Stages:

Plan for each stage:

If EXTENDED keyword is used then much more information will be emitted where as without that keyword only logical information will be emitted.

e.g. In case of a group by query 

EXPLAIN
SELECT T.c1, count(1) FROM T GROUP BY T.c1;

The explain plan itself has two stages

Stage1 and Stage2

Stage1 will have the plan for generating the partial aggregates
and Stage2 will have the plan for generating the complete aggregates.

I also plan to convert the parse and semantic analysis tests so that they use this for finding differences in the plan instead of the programmatic plan dumps that we are using today (tests/queries/positive).
"
HADOOP-4083,change new config attribute queue.name to mapred.job.queue.name,The new queue.name attribute should really be prefixed by mapred.job.
HADOOP-4078,TestKosmosFileSystem fails on trunk,"org.apache.hadoop.fs.kfs.TestKosmosFileSystem.testDirs fails with error messgae:
{noformat}
Error Message
expected:<7> but was:<1>

Stacktrace
junit.framework.AssertionFailedError: expected:<7> but was:<1>
	at org.apache.hadoop.fs.kfs.TestKosmosFileSystem.testDirs(TestKosmosFileSystem.java:77)
{noformat}"
HADOOP-4077,Access permissions for setting access times and modification times for files,HADOOP-1869 implements an API to set access and modification times for files. We need to decide what access checks to make before allowing a client to use this new API.
HADOOP-4076,fuse-dfs REAME lists wrong ant flags and is not specific in some place,-Dcompile.c++=1 ==> -Dlibhdfs=1 and it could be more specific about how to get fuse and such.
HADOOP-4075,test-patch.sh should output the ant commands that it runs,Would be helpful for debugging.
HADOOP-4074,releaseaudit target must be re-written so that the rat jar file isn't in the classpath,"The nightly build and the Hudson patch testing process copies the rat-0.5.1.jar into the hadoop lib directory so that the releaseaudit target works.  This puts it in the general classpath.  Since Hive was committed, this jar file contains older versions of classes needed by Hive.  This rat jar file should be put elsewhere or configured in another way so that it doesn't need to be in the classpath. "
HADOOP-4073,Turn on find bugs for Hive,Hive is a large set of code and it should have findbugs turned on.
HADOOP-4071,"FSNameSystem.isReplicationInProgress should add an underReplicated block to the neededReplication queue using method ""add"" not ""update""","We have a datanode that did not get decommission done for days. It turned out that there was an under replicated block that was never placed in the neededReplication queue and therefore did not get replicated. The following debug line showed the problem:

"" DEBUG org.apache.hadoop.dfs.StateChange: UnderReplicationBlocks.update blk_-7437651423871278837_0 curReplicas 8
curExpectedReplicas 10 oldReplicas 9 oldExpectedReplicas  10 curPri  2 oldPri  2""

The block was not in the neededReplication queue, but the update method concluded that the block was under replicated and the priority level did not change, so it did not add the block to the needReplication queue.

The solution is that in stead of using the update method, the name node should use the add method to add the block to the neededReplication queue. The add method guarantees success if the block is indeed under replicated."
HADOOP-4070,[Hive] Provide a mechanism for registering UDFs from the query language,UDFs (user defined functions) in Hive are currently actually built-in functions. This issue is to develop a packaging and registration mechanism so users can add their own functions at runtime.
HADOOP-4069,TestKosmosFileSystem can fail when run through ant test on systems shared by users,"TestKosmosFileSystem has some test cases that try to verify paths under /tmp/test/. If a user is running ant test on a system that is shared, this could result in test failures if these paths are created by another user who has used the system. The test cases can be modified to either use one of the standard data directories set up for tests by Hadoop, or they can atleast append the user name when referring to these directories, like /tmp/test-<username>/. "
HADOOP-4067,Issue with permissions on test-libhdfs.sh while carrying out unit test on trunk /branch,"We are encountering some issue while carrying out unit test for trunk using jdk1.6.0_07-32bit
(command) : ""${ANT_HOME}/bin/ant -Dlibhdfs=1 -Dtest.junit.output.format=xml -Dtest.output=yes test test-libhdfs"" 

--------------------------------------------------------------------------------
test-libhdfs:
    [mkdir] Created dir: /xx/hadoop-0.19.0-dev/build/test/libhdfs
    [mkdir] Created dir: /xx/hadoop-0.19.0-dev/build/test/libhdfs/logs
    [mkdir] Created dir: /xx/hadoop-0.19.0-dev/build/test/libhdfs/hdfs/name
     [exec] ./tests/test-libhdfs.sh
     [exec] make: execvp: ./tests/test-libhdfs.sh: Permission denied
     [exec] make: *** [test] Error 127

BUILD FAILED
/xx/hadoop-0.19.0-dev/build.xml:1088: exec returned: 2
"
HADOOP-4066,fuse-dfs readme cleanup and point to wiki instead,"stop having to duplicate changes in both the readme and wiki

This is a change to the README only and as such should not require any QA or anything and obviously no unit tests :)
"
HADOOP-4063,Separate spill thresholds for serialization/accounting in MapTask,"In MapTask, there is a single parameter controlling the threshold for starting a spill thread concurrently with collection. However, some users may want to set different thresholds for the serialization buffer (holding record bytes) and the accounting buffer (holding record metadata)."
HADOOP-4062,IPC client does not need to be synchronized on the output stream when a connection is closed,"Currently when a connection on the client side is closed, the close method is synchronized on the output stream. The synchronization is not necessary and has introduced a side effect that the socket can not be closed immediately when meanwhile applications are sending large requests."
HADOOP-4061,Large number of decommission freezes the Namenode,"On 1900 nodes cluster, we tried decommissioning 400 nodes with 30k blocks each. Other 1500 nodes were almost empty.

When decommission started, namenode's queue overflowed every 6 minutes.

Looking at the cpu usage,  it showed that every 5 minutes org.apache.hadoop.dfs.FSNamesystem$DecommissionedMonitor thread was taking 100% of the CPU for 1 minute causing the queue to overflow.

{noformat}
  public synchronized void decommissionedDatanodeCheck() {
    for (Iterator<DatanodeDescriptor> it = datanodeMap.values().iterator();
         it.hasNext();) {
      DatanodeDescriptor node = it.next();
      checkDecommissionStateInternal(node);
    }
  }
{noformat}




"
HADOOP-4060,[HOD] Make HOD to roll log files on the client ,"Currently HOD writes a log file on the client in the cluster directory, named hod-<username>.log. This file is appended to for each run of hod allocation that runs on the same cluster directory. Thus, the file can become quite large - particularly if the job is queued for a long time. If there are problems with the HOD client subsequently, this could result in stale file handles pointing to this large file, and cause disk space to fill up. Another problem is that a large log file is usually unusable for a user."
HADOOP-4056,Unit test for DynamicSerDe,"Unit test for the following:

DynamicSerDe/normal TProtocol with and without thrift compatibility mode on including basic, map and list types and a list of maps or something like that
DynamicSerDe/TCTLSeparatedProtocol (with thrift mode on even though it's not really thrift compatible) with basic, map and list types.
DynamicSerDe getFields and getFieldsFromExpression

Also improve general debugability of DynamicSerDe by adding toString methods and better handling unknown field names passed in.

NOTE: at this time, not adding unit tests for nested *types*  in dynamic serde even though this is supported. Just not a priority right now.

"
HADOOP-4054,"During edit log loading, an underconstruction file's lease gets removed twice","The lease is removed on line 605: old = fsDir.unprotected(path, mtime);
It then gets removed again on line 637: fsNamesys.leaseManager.removeLease(cons.clientName, path);

The second removal results in an error message in the log, which is very annoying."
HADOOP-4053,Schedulers need to know when a job has completed,"The JobInProgressListener interface is used by the framework to notify Schedulers of when jobs are added, removed, or updated. Right now, there is no way for the Scheduler to know that a job has completed. jobRemoved() is called when a job is retired, which can happen many hours after a job is actually completed. jobUpdated() is called when a job's priority is changed. We need to notify a listener when a job has completed (either successfully, or has failed or been killed). "
HADOOP-4050,TestFairScheduler failed on Linux,"{noformat}
Testsuite: org.apache.hadoop.mapred.TestFairScheduler
Tests run: 13, Failures: 0, Errors: 13, Time elapsed: 0.042 sec

Testcase: testAllocationFileParsing took 0.004 sec
        Caused an ERROR
/home/tsz/hadoop/latest/build/contrib/fairscheduler/test/data/./test-pools (No such file or directory)
java.io.FileNotFoundException: /home/tsz/hadoop/latest/build/contrib/fairscheduler/test/data/./test-pools (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:179)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:70)
        at java.io.FileWriter.<init>(FileWriter.java:46)
        at org.apache.hadoop.mapred.TestFairScheduler.setUp(TestFairScheduler.java:215)

...
{noformat}
"
HADOOP-4046,WritableComparator's constructor should be protected instead of private,"Currently, it isn't possible to subclass WritableComparator in a meaningful way. We should make this constructor protected:

{code}
private WritableComparator(Class keyClass, boolean createInstances)
{code}"
HADOOP-4045,Increment checkpoint if we see failures in rollEdits,"In _FSEditLog::rollEdits_, if we encounter an error during opening edits.new, we remove  the store directory associated with it. At this point we should also increment checkpoint on all other directories."
HADOOP-4041,IsolationRunner does not work as documented,"IsolationRunner does not work as documented in the tutorial.

The tutorial  says ""To use the IsolationRunner, first set keep.failed.tasks.files to true (also see keep.tasks.files.pattern).""

Should be:
  keep.failed.task.files (not tasks)

After the above was set (quoted from my message on hadoop-core):
> After the task
> hung, I failed it via the web interface.  Then I went to the node that was
> running this task
>
>   $ cd ...local/taskTracker/jobcache/job_200808071645_0001/work
> (this path is already different from the tutorial's)
>
>   $ hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml
> Exception in thread ""main"" java.lang.NullPointerException
>         at
> org.apache.hadoop.mapred.IsolationRunner.main(IsolationRunner.java:164)
>
> Looking at IsolationRunner code, I see this:
>
>     164     File workDirName = new File(lDirAlloc.getLocalPathToRead(
>     165                                   TaskTracker.getJobCacheSubdir()
>     166                                   + Path.SEPARATOR + taskId.getJobID() 
>     167                                   + Path.SEPARATOR + taskId
>     168                                   + Path.SEPARATOR + ""work"",
>     169                                   conf). toString());
>
> I.e. it assumes there is supposed to be a taskID subdirectory under the job
> dir, but:
>  $ pwd
>  ...mapred/local/taskTracker/jobcache/job_200808071645_0001
>  $ ls
>  jars  job.xml  work
>
> -- it's not there."
HADOOP-4040,Remove the hardcoded ipc.client.connection.maxidletime setting from the TaskTracker.Child.main(),This setting affects the dfs client idle timeout unless the jobconf of the user's job overrides it. I propose we remove this setting altogether.
HADOOP-4038,Ability for JobTracker to drain running jobs before a shutdown,"It would be nice to have operating controls on the JobTracker that lets new jobs to get rejected while continuing to process existing jobs. This is handy while shutting down the JobTracker.

Similarly, while restarting the JobTracker, it woudl be nice if it can go into this state where it refuses to accept new jobs. It allows the system administrator to ensure that all relevant TaskTrackers have joined the cluster before the cluster is opened for general use."
HADOOP-4037,HadoopQA contrib -1 comments due to inconsistency in eclipse plugin declared jars,"Eclipse declared jars:
   lib/kfs-0.1.3.jar
   lib/log4j-1.2.13.jar

Present jars:
   lib/kfs-0.2.0.jar
   lib/log4j-1.2.15.jar

This inconsistency causes HadoopQA's comment:
      -1 core tests.  The patch failed core unit tests.

"
HADOOP-4036,Increment InterTrackerProtocol version number due to changes in HADOOP-3759,"HADOOP-3759 introduced a new variable called {{ResourceStatus}} in the {{TaskTrackerStatus}} field. I think, by convention, this requires incrementing the version number of the {{InterTrackerProtocol}}."
HADOOP-4035,Modify the capacity scheduler (HADOOP-3445) to schedule tasks based on memory requirements and task trackers free memory,"HADOOP-3759 introduced configuration variables that can be used to specify memory requirements for jobs, and also modified the tasktrackers to report their free memory. The capacity scheduler in HADOOP-3445 should schedule tasks based on these parameters. A task that is scheduled on a TT that uses more than the default amount of memory per slot can be viewed as effectively using more than one slot, as it would decrease the amount of free memory on the TT by more than the default amount while it runs. The scheduler should make the used capacity account for this additional usage while enforcing limits, etc."
HADOOP-4030,LzopCodec shouldn't be in the default list of codecs i.e. io.compression.codecs,"Since we do not ship the native lzo codecs by default, we shouldn't have it in io.compression.codecs. Users might be thrown off by the errors which come up when the framework tries to load the lzo codecs (by default)."
HADOOP-4029,NameNode should report status and performance for each replica of image and log,"The administrator should have immediate access to the status of each replica. In addition to reporting whether the replica is active or not, measured performance should be reported so as to indicate whether a replica might impact cluster performance or might be in danger of failing.

We're probably not close to doing automated replica fail over, but we can give the administrator some clues."
HADOOP-4027,"When streaming utility is run without specifying mapper/reducer/input/output options, it returns 0.","When the below command is run i.e without specifying any of the streaming options, it returns 0 whereas a user would expect it to return a non-zero value:
$ $HADOOP_HOME/bin/hadoop [--config dir] jar $HADOOP_HOME/hadoop-streaming.jar"
HADOOP-4026,bad coonect ack with first bad link,"wordcount/hi/ dir is the input dir 

when i execute :

# bin/hadoop dfs -copyFromLocal wordcount/hi wordcount/ins

i get the foll msg

08/08/25 13:43:30 INFO dfs.DFSClient: Exception in
createBlockOutputStream java.io.IOException: Bad connect ack with
firstBadLink 10.232.25.69:50010
08/08/25 13:43:30 INFO dfs.DFSClient: Abandoning block
blk_-3916191835981679734
08/08/25 13:43:36 INFO dfs.DFSClient: Exception in
createBlockOutputStream java.io.IOException: Bad connect ack with
firstBadLink 10.232.25.69:50010
08/08/25 13:43:36 INFO dfs.DFSClient: Abandoning block
blk_-7058774921272589893
08/08/25 13:43:42 INFO dfs.DFSClient: Exception in
createBlockOutputStream java.io.IOException: Bad connect ack with
firstBadLink 10.232.25.69:50010
08/08/25 13:43:42 INFO dfs.DFSClient: Abandoning block
blk_3767065959322874247
08/08/25 13:43:48 INFO dfs.DFSClient: Exception in
createBlockOutputStream java.io.IOException: Bad connect ack with
firstBadLink 10.232.25.69:50010
08/08/25 13:43:48 INFO dfs.DFSClient: Abandoning block
blk_-8330992315825789947
08/08/25 13:43:54 WARN dfs.DFSClient: DataStreamer Exception:
java.io.IOException: Unable to create new block.
08/08/25 13:43:54 WARN dfs.DFSClient: Error Recovery for block
blk_-8330992315825789947 bad datanode[1]
copyFromLocal: Could not get block locations. Aborting...

when i examine the log file of the slave, i see this

2008-08-25 13:42:18,140 INFO org.apache.hadoop.dfs.DataNode:
STARTUP_MSG: /************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = slave1/10.232.25.96 STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.16.4 STARTUP_MSG:   build =
http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.16 -r
652614; compiled by 'hadoopqa' on Fri May  2 00:18:12 UTC 2008
************************************************************/
2008-08-25 13:42:18,634 INFO org.apache.hadoop.dfs.Storage: Storage
directory /etc/hadoop_install/hadoop-0.16.4/datanodedir is not
formatted.
2009-08-25 13:42:18,634 INFO org.apache.hadoop.dfs.Storage:
Formatting ...
2008-08-25 13:42:18,701 INFO org.apache.hadoop.dfs.DataNode: Registered
FSDatasetStatusMBean
2008-08-25 13:42:18,701 INFO org.apache.hadoop.dfs.DataNode: Opened
server at 50010
2008-08-25 13:42:18,705 INFO org.apache.hadoop.dfs.DataNode: Balancing
bandwith is 1048576 bytes/s
2008-08-25 13:42:18,911 INFO org.mortbay.util.Credential: Checking
Resource aliases
2008-08-25 13:42:19,013 INFO org.mortbay.http.HttpServer: Version
Jetty/5.1.4 2008-08-25 13:42:19,014 INFO org.mortbay.util.Container:
Started HttpContext[/static,/static]
2008-08-25 13:42:19,014 INFO org.mortbay.util.Container: Started
HttpContext[/logs,/logs]
2008-08-25 13:42:19,579 INFO org.mortbay.util.Container: Started
org.mortbay.jetty.servlet.WebApplicationHandler@11ff436
2008-08-25 13:42:19,658 INFO org.mortbay.util.Container: Started
WebApplicationContext[/,/]
2008-08-25 13:42:19,661 INFO org.mortbay.http.SocketListener: Started
SocketListener on 0.0.0.0:50075
2008-08-25 13:42:19,661 INFO org.mortbay.util.Container: Started
org.mortbay.jetty.Server@1b8f864
2008-08-25 13:42:19,706 INFO org.apache.hadoop.dfs.DataNode: New storage
id DS-860242092-10.232.25.96-50010-1219651939700 is assigned to data-
node 10.232.25.96:50010
2008-08-25 13:42:19,733 INFO org.apache.hadoop.metrics.jvm.JvmMetrics:
Initializing JVM Metrics with processName=DataNode, sessionId=null
2008-08-25 13:42:19,755 INFO org.apache.hadoop.dfs.DataNode:
10.232.25.96:50010In DataNode.run, data = FSDataset
{dirpath='/etc/hadoop_install/hadoop-0.16.4/datanodedir/current'}
2008-08-25 13:42:19,755 INFO org.apache.hadoop.dfs.DataNode: using
BLOCKREPORT_INTERVAL of 3538776msec Initial delay: 60000msec
2008-08-25 13:42:19,828 INFO org.apache.hadoop.dfs.DataNode: BlockReport
of 0 blocks got processed in 20 msecs
2008-08-25 13:45:43,982 INFO org.apache.hadoop.dfs.DataNode: Receiving
block blk_1031802361447574775 src: /10.232.25.197:40282
dest: /10.232.25.197:50010
2008-08-25 13:45:44,032 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
forwarding connect ack to upstream firstbadlink is
2008-08-25 13:45:44,081 INFO org.apache.hadoop.dfs.DataNode: Received
block blk_1031802361447574775 of size 3161 from /10.232.25.197
2008-08-25 13:45:44,081 INFO org.apache.hadoop.dfs.DataNode:
PacketResponder 0 for block blk_1031802361447574775 terminating
2008-08-25 13:45:44,105 INFO org.apache.hadoop.dfs.DataNode: Receiving
block blk_-1924738157193733587 src: /10.232.25.197:40285
dest: /10.232.25.197:50010
2008-08-25 13:45:44,106 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
forwarding connect ack to upstream firstbadlink is
2008-08-25 13:45:44,193 INFO org.apache.hadoop.dfs.DataNode: Received
block blk_-1924738157193733587 of size 6628 from /10.232.25.197
2008-08-25 13:45:44,193 INFO org.apache.hadoop.dfs.DataNode:
PacketResponder 0 for block blk_-1924738157193733587 terminating
2008-08-25 13:45:44,212 INFO org.apache.hadoop.dfs.DataNode: Receiving
block blk_7001275375373078911 src: /10.232.25.197:40287
dest: /10.232.25.197:50010
2008-08-25 13:45:44,213 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
forwarding connect ack to upstream firstbadlink is
008-08-25 13:45:44,256 INFO org.apache.hadoop.dfs.DataNode: Received
block blk_7001275375373078911 of size 3161 from /10.232.25.197
2008-08-25 13:45:44,256 INFO org.apache.hadoop.dfs.DataNode:
PacketResponder 0 for block blk_7001275375373078911 terminating
2008-08-25 13:45:44,277 INFO org.apache.hadoop.dfs.DataNode: Receiving
block blk_-7471693146363669981 src: /10.232.25.197:40289
dest: /10.232.25.197:50010
2008-08-25 13:45:44,278 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
forwarding connect ack to upstream firstbadlink is
2008-08-25 13:45:44,362 INFO org.apache.hadoop.dfs.DataNode: Received
block blk_-7471693146363669981 of size 6628 from /10.232.25.197
2008-08-25 13:45:44,362 INFO org.apache.hadoop.dfs.DataNode:
PacketResponder 0 for block blk_-7471693146363669981 terminating
2008-08-25 13:45:44,380 INFO org.apache.hadoop.dfs.DataNode: Receiving
block blk_-6619078097753318750 src: /10.232.25.197:40291
dest: /10.232.25.197:50010
2008-08-25 13:45:44,380 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
forwarding connect ack to upstream firstbadlink is
2008-08-25 13:45:44,424 INFO org.apache.hadoop.dfs.DataNode: Received
block blk_-6619078097753318750 of size 2778 from /10.232.25.197
2008-08-25 13:45:44,424 INFO org.apache.hadoop.dfs.DataNode:
PacketResponder 0 for block blk_-6619078097753318750 terminating
2008-08-25 13:45:44,440 INFO org.apache.hadoop.dfs.DataNode: Receiving
block blk_1527614673854389960 src: /10.232.25.197:40293
dest: /10.232.25.197:50010
2008-08-25 13:45:44,441 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
forwarding connect ack to upstream firstbadlink is
2008-08-25 13:45:44,526 INFO org.apache.hadoop.dfs.DataNode: Received
block blk_1527614673854389960 of size 4616 from /10.232.25.197
2008-08-25 13:45:44,526 INFO org.apache.hadoop.dfs.DataNode:
PacketResponder 0 for block blk_1527614673854389960 terminating
2008-08-25 13:47:21,331 INFO org.apache.hadoop.dfs.DataBlockScanner:
Verification succeeded for blk_1527614673854389960
2008-08-25 13:48:11,458 INFO org.apache.hadoop.dfs.DataBlockScanner:
Verification succeeded for blk_7001275375373078911

i don know what changes should i make n wer exactly the problem comes from.... 
kindly help me in resolving this issue...
Thanks in advance.. 
"
HADOOP-4024,"""ant test-patch"" and ""ant javadoc"" are inconsistent","The javadoc results from ""ant test-patch"" and ""ant javadoc"" are inconsistent.  See HADOOP-4023 for an example."
HADOOP-4023,javadoc warnings: incorrect references,"- ""ant test-patch"" with an empty patch file
{noformat}
  [javadoc] .../trunk/src/core/org/apache/hadoop/fs/FileSystem.java:34: package org.apache.hadoop.hdfs does not exist
  [javadoc] import org.apache.hadoop.hdfs.DistributedFileSystem;
  [javadoc]                              ^
  [javadoc] .../trunk/src/core/org/apache/hadoop/fs/FsShell.java:46: package org.apache.hadoop.hdfs does not exist
  [javadoc] import org.apache.hadoop.hdfs.DistributedFileSystem;
  [javadoc]                              ^
  [javadoc] .../trunk/src/mapred/org/apache/hadoop/mapred/Task.java:36: package org.apache.hadoop.hdfs does not exist
  [javadoc] import org.apache.hadoop.hdfs.DistributedFileSystem;
  [javadoc]                              ^
  [javadoc] Standard Doclet version 1.6.0
  [javadoc] Building tree for all the packages and classes...
  [javadoc] .../trunk/src/core/org/apache/hadoop/fs/FileSystem.java:56: warning - Tag @link: reference not found: DistributedFileSystem
  [javadoc] .../trunk/src/core/org/apache/hadoop/metrics/MetricsUtil.java:34: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] .../trunk/src/core/overview.html: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.NameNode
  [javadoc] Generating /home/tsz/hadoop/trunk/build/docs/api/stylesheet.css...
  [javadoc] 6 warnings
{noformat}

- ""ant javadoc""
{noformat}
  [javadoc] .../trunk/src/core/org/apache/hadoop/metrics/MetricsUtil.java:34: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] .../trunk/src/core/overview.html: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.NameNode
  [javadoc] Generating .../trunk/build/docs/api/stylesheet.css...
  [javadoc] 2 warnings
{noformat}"
HADOOP-4021,failmon build seems to be recursively pulling in hadoop jar and tar.gz file into it's lib directory,"HadoopQA is taking a very long time to run patch builds and tests on hudson.zones because, I believe, failmon build is broken.  If I look at trunk/src/contrib/failmon/lib during a build I see this:
-rw-r--r--   1 hudson      38015 Aug 25 16:04 commons-logging-1.0.4.jar
-rw-r--r--   1 hudson      26202 Aug 25 16:04 commons-logging-api-1.0.4.jar
drwxr-xr-x   2 hudson          2 Aug 25 16:04 hadoop-688101_HADOOP-3854_PATCH-12388706
-rw-r--r--   1 hudson    2222615 Aug 25 14:43 hadoop-688101_HADOOP-3854_PATCH-12388706-core.jar
-rw-r--r--   1 hudson     123580 Aug 25 14:43 hadoop-688101_HADOOP-3854_PATCH-12388706-examples.jar
-rw-r--r--   1 hudson    1110090 Aug 25 14:43 hadoop-688101_HADOOP-3854_PATCH-12388706-test.jar
-rw-r--r--   1 hudson      50222 Aug 25 16:04 hadoop-688101_HADOOP-3854_PATCH-12388706-tools.jar
-rw-r--r--   1 hudson   74019000325 Aug 25 16:03 hadoop-688101_HADOOP-3854_PATCH-12388706.tar.gz
-rw-r--r--   1 hudson     391834 Aug 25 16:04 log4j-1.2.15.jar

I believe that failmon is recursively pulling these files in (note the size of the tar.gz file).


"
HADOOP-4018,limit memory usage in jobtracker,"We have seen instances when a user submitted a job with many thousands of mappers. The JobTracker was running with 3GB heap, but it was still not enough to prevent memory trashing from Garbage collection; effectively the Job Tracker was not able to serve jobs and had to be restarted.

One simple proposal would be to limit the maximum number of tasks per job. This can be a configurable parameter. Is there other things that eat huge globs of memory in job Tracker?"
HADOOP-4015,Need counters for spils and merges in mappers and reducers.,"
Currently, it is hard to know whether a mapper did spills and how many if it did.
It is even harder to figure out how many merges happened in a mapper or a reducer.
Those data should be made available through counters.
"
HADOOP-4014,DFS upgrade fails on Windows,"FileUtil.HardLink#createHardLink() didn't work on Windows, and DFS upgrade of Datanode fails.

The windows command 'fsutil' requires the arguments link name first as follows,
> fsutil hardlink create <link name> <target>
But the current FileUtil passes the target first.
"
HADOOP-4012,Providing splitting support for bzip2 compressed files,"Hadoop assumes that if the input data is compressed, it can not be split (mainly due to the limitation of many codecs that they need the whole input stream to decompress successfully).  So in such a case, Hadoop prepares only one split per compressed file, where the lower split limit is at 0 while the upper limit is the end of the file.  The consequence of this decision is that, one compress file goes to a single mapper. Although it circumvents the limitation of codecs (as mentioned above) but reduces the parallelism substantially, as it was possible otherwise in case of splitting.

BZip2 is a compression / De-Compression algorithm which does compression on blocks of data and later these compressed blocks can be decompressed independent of each other.  This is indeed an opportunity that instead of one BZip2 compressed file going to one mapper, we can process chunks of file in parallel.  The correctness criteria of such a processing is that for a bzip2 compressed file, each compressed block should be processed by only one mapper and ultimately all the blocks of the file should be processed.  (By processing we mean the actual utilization of that un-compressed data (coming out of the codecs) in a mapper).

We are writing the code to implement this suggested functionality.  Although we have used bzip2 as an example, but we have tried to extend Hadoop's compression interfaces so that any other codecs with the same capability as that of bzip2, could easily use the splitting support.  The details of these changes will be posted when we submit the code."
HADOOP-4011,shuffling backoff 5 minutes despite the whole cluster is idle,"
A reducer of a mapred job got a read time out exception during fetching the last map output segment.
The reducer waited 5 minutes before re-fetch the map output, even though the whole cluster was idle during that period.

"
HADOOP-3998,Got an exception from ClientFinalizer when the JT is terminated,"This happens when we terminate the JT using _control-C_. It throws the following exception
{noformat}
Exception closing file my-file
java.io.IOException: Filesystem closed
        at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:193)
        at org.apache.hadoop.hdfs.DFSClient.access$700(DFSClient.java:64)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:2868)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:2837)
        at org.apache.hadoop.hdfs.DFSClient$LeaseChecker.close(DFSClient.java:808)
        at org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:205)
        at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:253)
        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:1367)
        at org.apache.hadoop.fs.FileSystem.closeAll(FileSystem.java:234)
        at org.apache.hadoop.fs.FileSystem$ClientFinalizer.run(FileSystem.java:219)
{noformat}
Note that _my-file_ is some file used by the JT.

Also if there is some file renaming done, then the exception states that the earlier file does not exist. I am not sure if this is a MR issue or a DFS issue. Opening this issue for investigation."
HADOOP-3995,"renameTo(src, dst) does not restore src name in case of quota failure.","How to reproduce :

set up quota such that 'hadoop -mv src dst' results in a 'quota excceeded' exception.

* Now try : {{hadoop -mv src dst/diffname}}
   ** This fails as expected
   ** buf 'src' does not exist after this, you will find 'diffname' directory in its place

I think fix should go into 0.18.1"
HADOOP-3992,Synthetic Load Generator for NameNode testing,For performance and correctness testing. The goal is to faithfully simulate the load of a 5000-node cluster with a (comparatively) tiny number of nodes. 
HADOOP-3991,updates to hadoop-ec2-env.sh for 0.18.0,"src/contrib/ec2/bin/hadoop-ec2-env-sh on 0.18.0 still has values for the 0.17.x branch.  Here is a diff.  I used this to create an EC2 image, which works for my hadoop streaming project.  The java version is the latest one, as instructed by the docs, but I left the java URL blank, as it looks like it isn't persistent.

ChimpBook4:bin karl$ diff hadoop-ec2-env.sh- hadoop-ec2-env.sh-foo
43c43
< HADOOP_VERSION=0.17.0
---
> HADOOP_VERSION=0.18.0
77c77
< JAVA_VERSION=1.6.0_05
---
> JAVA_VERSION=1.6.0_07"
HADOOP-3986,JobClient should not have a static configuration,Post HADOOP-3743 use of static commandLineConfig and JobShell is deprecated. They should be removed.
HADOOP-3985,TestHDFSServerPorts fails on trunk,"{{TestHDFSServerPorts}} fails on trunk with

{noformat}
junit.framework.AssertionFailedError
        at org.apache.hadoop.hdfs.TestHDFSServerPorts.testDataNodePorts(TestHDFSServerPorts.java:186)
{noformat}"
HADOOP-3982,Ability to add libjars through non-commnad line path ,"-libjars options is available only if the job has been submitted through JobShell class. Since the function 'setCommnadLineConfig' is a protected function, non mapred classes can't take advantage of this functionality. By making this function public, other classes that submit jobs can utilize this functionality. "
HADOOP-3981,Need a distributed file checksum algorithm for HDFS,"Traditional message digest algorithms, like MD5, SHA1, etc., require reading the entire input message sequentially in a central location.  HDFS supports large files with multiple tera bytes.  The overhead of reading the entire file is huge. A distributed file checksum algorithm is needed for HDFS."
HADOOP-3975,test-patch can report the modifications found in the workspace along with the error message,"When test-patch is run on a developer workspace which contains modifications, it errors out by saying ""ERROR: can't run in a workspace that contains modifications"". It appears this is printed when svn stat command returns a non-empty output. It would be helpful to report this output, so that developers can be alerted to what modifications are causing this error. Sometimes these modifications may not be because of a direct code change they've done, but remnants of an incorrect cleanup of the last build, etc."
HADOOP-3973,Breaking out of a task,"Occasionally a mapper/reducer is done without need to see more input data. If we provide a way to break out and skip reading the rest of input then job finishs earlier and resources are spared.
"
HADOOP-3970,Counters written to the job history cannot be recovered back,"Counters that are written to the JobHistory are stringified using {{Counters.makeCompactString()}}. The format in which this api converts the counter into a string is _groupname.countername:value_. The problem is that _groupname_ and _countername_ can contain a '.' and hence recovering the counter becomes difficult. Since JobHistory can be used for various purposes, reconstructing the counter object back might be useful. One such usecase is HADOOP-3245. There should be some way to recover the counter object back from its string representation and also to keep the string version readable."
HADOOP-3968,test-libhdfs fails on trunk,"HADOOP-3095 changed getFileBlockLocations API and HADOOP-3664 removed old one. libhdfs was missed. On trunk test-libhdfs fails, we need to change hdfs.c
"
HADOOP-3966,"Place the new findbugs warnings introduced by the patch in the /tmp directory when ""ant test-patch"" is run.","New findbugs warnings introduced by the patch should be made available in the PATCH_DIR(i.e /tmp) when ""ant test-patch"" is run. It should be available in both .html and .xml format.


"
HADOOP-3965,Make DataBlockScanner package private,"{{DataBlockScanner}} should be a package private class rather than public.
""public"" can be removed if we move {{TestInterDatanodeProtocol}} into {{org.apache.hadoop.hdfs.server.datanode}} package.
This will require to make some changes to the  import section."
HADOOP-3964,javadoc warnings by failmon,"There are javadoc warnings caused by failmon:
{noformat}
  [javadoc] Loading source files for package org.apache.hadoop.contrib.failmon...
  [javadoc] Constructing Javadoc information...
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging""
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging.impl""
{noformat}"
HADOOP-3963,libhdfs should never exit on its own but rather return errors to the calling application,"A a library, libhdfs would be better to return errors rather than directly exiting. This way the calling application can determine what to do on error. And it is more unix library like to handle errors this way.

Some applications like fuse-dfs need to handle the exit themselves after emitting an appropriate error message to its log.

"
HADOOP-3962,"Shell command ""fs -count"" should support paths with different file systsms","If there are two different file systems in the path list, ""fs -count"" does not work.  For example,
{noformat}
bash-3.2$ ./bin/hadoop fs -count / file:///    
          13            4              27369 hdfs://nnn:9000/
count: Wrong FS: file:/, expected: hdfs://nnn:9000
Usage: java FsShell [-count[-q] <path>]
{noformat}"
HADOOP-3961,resource estimation works badly in some cases,"The disk space estimation introduced in HADOOP-657 performs badly under some circumstances.  In particular, if maps have very small output, the initial estimate of output size equals input size throws everything off.

Fix is to use a smaller initial estimate, and to update it appropriately."
HADOOP-3960,Couple of mapred unit test are failing in Hudson builds.,"Most frequent ones are 

{{org.apache.hadoop.mapred.TestMapRed.testMapred}} and {{org.apache.hadoop.mapred.TestMiniMRDFSSort.testMapReduceSort}}

"
HADOOP-3959,[HOD] --resource_manager.options is not passed to qsub,"--resource_manager.options is ignored in HOD. It isnt passed to the qsub command. This is a problem, as it makes it impossible to add additional resource contraints for scheduling (i.e. give me 64bit nodes)
"
HADOOP-3958,TestMapRed ignores failures of the test case,"TestMapRed checks for correctness of the data after several map/reduce jobs, but only prints the failure to stderr rather than fail the junit test. The test has been failing for a long time and was not reported as broken."
HADOOP-3957,Fix javac warnings in DistCp and the corresponding tests,There are a few javac warning in DistCp and TestCopyFiles.
HADOOP-3954,Skip records enabled as default. ,"HADOOP-153 added feature to skip records that fail the task.
I believe this feature should not be enabled as default."
HADOOP-3953,Sticky bit for directories,"Our users (especially Pig) heavily use /tmp for temporary storage. 
Permission are set to 777.

However, this means any users can rename and also remove (by moving to .Trash) other users directories and files.
It would be nice if we can have a sticky bit like unix. 

Copy&Pasted from manpage.

STICKY DIRECTORIES
       When  the  sticky  bit  is set on a directory, files in that directory may be unlinked or renamed only by
       root or their owner.  Without the sticky bit, anyone able to write to the directory can delete or  rename
       files.  The sticky bit is commonly found on directories, such as /tmp, that are world-writable."
HADOOP-3952,TestDataJoin references dfs.MiniDFSCluster instead of hdfs.MiniDFSCluster,The data join test doesn't compile...
HADOOP-3951,The package name used in FSNamesystem is incorrect,FSNamesystem is in org.apache.hadoop.hdfs.server.namenode package.  The name org.apache.hadoop.fs.FSNamesystem used in the codes is incorrect.
HADOOP-3950,TestMapRed and TestMiniMRDFSSort failed on trunk,"- TestMapRed failed on Windows and Linux
{noformat}
Testcase: testMapred took 7.36 sec
	Caused an ERROR
Not a file: file:/d:/@sze/hadoop/trunk/mapred.loadtest/intermediateouts/part-00000/data
java.io.IOException: Not a file: file:/d:/@sze/hadoop/trunk/mapred.loadtest/intermediateouts/part-00000/data
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:195)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:740)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1036)
	at org.apache.hadoop.mapred.TestMapRed.launch(TestMapRed.java:614)
	at org.apache.hadoop.mapred.TestMapRed.testMapred(TestMapRed.java:248)
{noformat}

- TestMiniMRDFSSort failed on Windows 
{noformat}
Testcase: testMapReduceSort took 21.312 sec
	Caused an ERROR
File does not exist: /tmp/sortvalidate/recordstatschecker/part-00000
java.io.FileNotFoundException: File does not exist: /tmp/sortvalidate/recordstatschecker/part-00000
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:404)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:677)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1416)
	at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1411)
	at org.apache.hadoop.mapred.SortValidator$RecordStatsChecker.checkRecords(SortValidator.java:370)
	at org.apache.hadoop.mapred.SortValidator.run(SortValidator.java:561)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.mapred.TestMiniMRDFSSort.runSortValidator(TestMiniMRDFSSort.java:73)
	at org.apache.hadoop.mapred.TestMiniMRDFSSort.testMapReduceSort(TestMiniMRDFSSort.java:99)
{noformat}"
HADOOP-3949,javadoc warnings: Multiple sources of package comments found for package,"Beginning from Hudson build #3046, there are javadoc warnings in trunk.
See http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3046/artifact/trunk/current/patchJavadocWarnings.txt
{noformat}
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging""
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.codec""
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.codec.binary""
  [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging.impl""
{noformat}"
HADOOP-3948,Separate Namenodes edits and fsimage ,NameNode's _edits_ and _fsimage_ should be separated with an option of having them in their own separate directories.
HADOOP-3947,TaskTrackers fail to connect back upon a re-init action,"It seems that if the {{JobTracker}} asks the {{TaskTracker}} to re-initialize itself, the {{TaskTracker}} never connects back to the {{Jobtracker}}."
HADOOP-3946,TestMapRed fails on trunk,"TestMapRed fails on trunk with following exception:
java.io.IOException: Not a file: file:/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/mapred.loadtest/intermediateouts/part-00000/data
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:198)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:740)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1024)
	at org.apache.hadoop.mapred.TestMapRed.launch(TestMapRed.java:614)
	at org.apache.hadoop.mapred.TestMapRed.testMapred(TestMapRed.java:248)

The failure is introduced by HADOOP-3664. Hudson detected the failure, but with commit of 3664 failure occurs on trunk."
HADOOP-3944,TupleWritable listed as public class but cannot be used without methods private to the package,"Reading the hadoop-core javadocs, it appears as though TupleWritable can be used outside of the
org.apache.hadoop.mapred.join.* package.  A user can import TupleWritable but cannot use it correctly without the setWritten, and clearWritten methods being public.   It seems as the though the intent was to make TupleWritable hidden from the user as it is dependent on CompositeRecordReader.  As a possible solution, classes within a package can be made invisible to the user by omitting 'public' from the class definition.  In the case of TupleWritable, this removes the javadoc link from other classes in mapred.join and it's not clear if these classes should be hidden from the user.
"
HADOOP-3943,NetworkTopology.pseudoSortByDistance does not need to be a synchronized method,"When all requests to namenode are read, most handlers block on NetworkTopology object lock when sorting replicas. Removing this necessary lock would reduce the contention in namenode and therefore improve its throughput."
HADOOP-3942,Update DistCp documentation,There are a few new options added to DistCp.  We should update the documentation.
HADOOP-3941,Extend FileSystem API to return file-checksums/file-digests,"Suppose we have two files in two locations (may be two clusters) and these two files have the same size.  How could we tell whether the content of them are the same?

Currently, the only way is to read both files and compare the content of them.  This is a very expensive operation if the files are huge.

So, we would like to extend the FileSystem API to support returning file-checksums/file-digests."
HADOOP-3940,Reduce often attempts in memory merge with no work,"ReduceTask.ReduceCopier.ShuffleRamManager initializes numRequiredMapOutputs to 0, so one of the predicates in ShuffleRamManager::waitForDataToMerge, {{numPendingRequests < numRequiredMapOutputs}}, is false until the first map output is fetched and false again after the last map output is fetched. The InMemFSMergeThread thread will loop busily in this state."
HADOOP-3939,DistCp should support an option for deleting non-existing files.,"One use case of DistCp is to sync two directories.  Currently, DistCp has an -update option for overwriting dst files if src is different from dst.  However, it is not enough for sync.  If there are some files in dst but not exist in src, there is no easy way to delete them.  We should add a new option, say -delete, so that DistCp will delete the non-existing in dst."
HADOOP-3938,Quotas for disk space management,"Directory quotas for bytes limit the number of bytes used by files in and below the directory. Operation is independent of name quotas (HADOOP-3187), but the implementation is parallel. Each file is charged according to its length multiplied by its intended replication factor."
HADOOP-3937,Job history may get disabled due to overly long job names,"Since Hadoop 0.17, the job history logs include the job's name in the filename. However, this can lead to overly long filenames, because job names may be arbitrarily long. When a filename is too long for the underlying OS, file creation fails and the JobHistory class silently disables history from that point on. This can lead to days of lost history until somebody notices the error in the log.

Proposed solution: Trim the job name to a reasonable length when selecting a filename for the history file."
HADOOP-3935,Extract classes from DataNode.java,DataNode.java is becoming hard to navigate with over 3000 lines of code. I suggest moving some of the classes out into their own files in the same package. This will also make it easier to see how the classes depend on each other and to see what code belongs where.
HADOOP-3934,Update log4j from 1.2.13 to 1.2.15,Hadoop is currently using log4j version 1.2.13 and this version does not support the use of a non-default syslog port when specifying the syslog server destination.   Syslogging is important to us and even more so with audit logging introduced in hadoop-0.18.  The current released version of log4j is 1.2.15 which does support the use of specifying a non-default syslog port (introduced in log4j 1.2.14) as well as many other syslog enhancements and fixes (http://logging.apache.org/log4j/1.2/changes-report.html).
HADOOP-3933,DataNode's BlockSender sends more data than necessary,DataNode's BlockSender sends more data than necessary because of how it computes endOffset.
HADOOP-3931,Bug in MapTask.MapOutputBuffer.collect leads to an unnecessary and harmful 'reset',"The unnecessary reset of the io.sort.mb buffer when bufindex==bufvoid leads to wrong, very large, temporary key buffer being created and passed along to the Buffer.write. This causes MapBufferTooSmallException as the Buffer thinks there isn't enough space to write this record into. If a combiner is defined the record does not go through the combiner and hits the disk directly.

It's already fixed in hadoop-0.18."
HADOOP-3930,Decide how to integrate scheduler info into CLI and job tracker web page,"We need a way for job schedulers such as HADOOP-3445 and HADOOP-3476 to provide info to display on the JobTracker web interface and in the CLI. The main things needed seem to be:
* A way for schedulers to provide info to show in a column on the web UI and in the CLI - something as simple as a single string, or a map<string, int> for multiple parameters.
* Some sorting order for jobs - maybe a method to sort a list of jobs.

Let's figure out what the best way to do this is and implement it in the existing schedulers.

My first-order proposal at an API: Augment the TaskScheduler with

* public Map<String, String> getSchedulingInfo(JobInProgress job) -- returns key-value pairs which are displayed in columns on the web UI or the CLI for the list of jobs.
* public Map<String, String> getSchedulingInfo(String queue) -- returns key-value pairs which are displayed in columns on the web UI or the CLI for the list of queues.
* public Collection<JobInProgress> getJobs(String queueName) -- returns the list of jobs in a given queue, sorted by a scheduler-specific order (the order it wants to run them in / schedule the next task in / etc).
* public List<String> getQueues();"
HADOOP-3928,add DF fuse dfs unit test,"Ensure that DF works in fuse_dfs.c which calls libhdfs.

"
HADOOP-3927,Be able to create a Configuration out an stream (with config XML content),"While it is possible to write a {{Configuration}} as XML to a stream, it is not possible to read from a stream a {{Configuration}}.

The use case is to persist a given {{Configuration}} (more specifically a {{JobConf}}) for later retrieval an use.

Currently I can do it only by first writing the XML content to a temporary file and then give a URL to that temporary file to the {{Configuration}} as {{addResource}} or in the constructor."
HADOOP-3925,Configuration paramater to set the maximum number of mappers/reducers for a job,The JobTracker can be prone to a denial-of-service attack if a user submits a job that has a very large number of tasks. This has happened once in our cluster. It would be nice to have a configuration setting that limits the maximum tasks that a single job can have. 
HADOOP-3924,Add a 'Killed' job status,"It is not possible, via APIs, to know if a job has been killed, only that has not been successful.
"
HADOOP-3923,Deprecate org.apache.hadoop.mapred.StatusHttpServer,"StatusHttpServer is extending org.apache.hadoop.http.HttpServer.  The only additional feature in StatusHttpServer is TaskGraphServlet.  StatusHttpServer is used by TaskTracker and JobTracker.  It turns that only JobTracker uses TaskGraphServlet but not TaskTracker.

So, we should deprecate StatusHttpServer, let TaskTracker and JobTracker use HttpServer directly, and add TaskGraphServlet to JobTracker."
HADOOP-3921,Clover breaks nightly builds,"Since 8 July the nightly builds on Hudson have failed with the following error:

   [clover] Processing files at 1.3 source level.
   [clover] Updating database at '/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-trunk/workspace/trunk/build/test/clover/db/hadoop_coverage.db'
   [clover] /var/tmp/clover35762.tmp/src35763.tmp/org/apache/hadoop/fs/FileUtil.java.tmp:508:10:unexpected token: OSType
   [clover] line 508: unexpected token: OSType
   [clover] ** Error(s) occurred and the instrumentation process can't continue.

"
HADOOP-3919,hadoop conf got slightly mangled by 3772 ,"As pointed out by Vinod, 3772 let some [fairly minor, work-around-able] issues into hadoop-default.xml.

This patch fixes them.  No code changes, no tests."
HADOOP-3918,Improve fuse-dfs error message when classpath not set or set incorrectly,"Currently libhdfs does an exit(1) when it cannot create a hadoop object. fuse does not like this and instead fuse_dfs should return -EIO and log to syslog and stderr a message. Also if it is obvious somehting is wrong, like the CLASSPATH is null or does not contain something of the form hadoop-[a-zA-Z-].jar immediately print an error and exit

Note that it is tough to debug this issue because when fuse-dfs is not in debug mode, it is running as a fuse module and stderr is redirected, so all one gets is ""software abort"" when libhdfs exits. So, this will greatly improve things since fuse-dfs can print a proper error and the user can quickly see what the problem is.

"
HADOOP-3914,checksumOk implementation in DFSClient can break applications,"One of our non-map-reduce applications (written in C and using libhdfs to access dfs) stopped working after switch from 0.16 to 0.17.
The problem was finally traced down to failures in checksumOk.

I would assume, the purpose of checksumOk is for a DfsClient to indicate to a sending Datanode that the checksum of the received block is okay. This must be useful in the replication pipeline.
How checksumOk is implemented is that any IOException is caught and ignored, probably because it is not essential for the client that the message is successful.

But it proved fatal for our application because this application links in a 3rd-party library which seems to catch socket exceptions before libhdfs.

Why was there an Exception? In our case the application reads a file into the local buffer of the DFSInputStream large enough to hold all data, the application reads to the end  and the checksumOK is sent successfully at that time. But then the application does some other stuff and comes back to re-read the file (still open). It is then when it calls checksumOk again and crashes.

It can easily be avoided by adding a Boolean making sure that checksumOk is called exactly once when EOS is encountered. Redundant calls to checksumOk do not seem to make sense anyhow."
HADOOP-3913,Distcp allows copy even if directory permissions are 000.,"distcp initiated from 0.17.0 and 0.18.0 permissions turned ON, directory with permission 000, data gets copied from
0.18.0 to 0.17.0 

Data gets copied between different versions of hadoop when tried by the same user. It gets failed only if other
user(other than the user who started cluster) tries to copy data using distcp.
"
HADOOP-3911,' -blocks ' option not being recognized,"Somehow depending on the order of options, GenericOptionsParser throws an error.

This fail. 
{noformat}
[knoguchi@gsgw2001 tmp]$ ~/branch-0.18/bin/hadoop fsck
Usage: DFSck <path> [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]

[knoguchi@tmp]$ ~/branch-0.18/bin/hadoop fsck -files -blocks -locations /user/knoguchi
java.io.FileNotFoundException: File -blocks does not exist.
        at org.apache.hadoop.util.GenericOptionsParser.validateFiles(GenericOptionsParser.java:278)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:233)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:315)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:134)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:119)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:59)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.dfs.DFSck.main(DFSck.java:137)

...Status: HEALTHY
 Total size:    2769 B
 Total dirs:    4
 Total files:   3
 Total blocks (validated):      3 (avg. block size 923 B)
...
{noformat}

This works.
{noformat}
[knoguchi@tmp]$ ~/branch-0.18/bin/hadoop fsck -blocks -locations -files /user/knoguchi
/user/knoguchi/.Trash <dir>
/user/knoguchi/hod-logs <dir>
/user/knoguchi/hod-logs/____ <dir>
/user/knoguchi/hod-logs/____/aaa.tar.gz 1024 bytes, 1 block(s):  OK
0. blk_-5724352734215884188_0 len=1024 repl=3 [11.111.11.11:11111, 22.22.22.22:11111, 33.33.33.33:11111]
...
/user/knoguchi/mapredsystem <dir>
Status: HEALTHY
 Total size:    2769 B
 Total dirs:    4
 Total files:   3
 Total blocks (validated):      3 (avg. block size 923 B)
 {noformat}"
HADOOP-3910,Are ClusterTestDFSNamespaceLogging and ClusterTestDFS still valid tests?,"ClusterTestDFSNamespaceLogging and ClusterTestDFS are not executed by ""ant test"" since their names do not begin with ""Test"". So these two tests have not be run for a long time.

I ran these two tests manually. Trunk passed ClusterTestDFSNamespaceLogging but failed on ClusterTestDFS. I am not sure whether they are still valid tests."
HADOOP-3908,Better error message if llibhdfs.so doesn't exist ,"IF for whatever reason, libhdfs.so doesn't exist when compiling fuse-dfs, should offer a better error message - something like:

<echo>libhdfs.so doesn't seem to exist in ${libhdfs_build.dir}. Please check you are compiling with both -Dfusedfs=1 and -Dlibhdfs=1</echo>

"
HADOOP-3907,INodeDirectoryWithQuota should be in its own .java file,"I can't do a clean build because INodeDirectoryWithQuota isn't being found. compile trace to follow.

the underlying cause is that it is in a file different to its name, and javac doesnt know where to find the source file to build it. Dirty builds would work, but not clean ones. Please move  INodeDirectoryWithQuota to a toplevel file."
HADOOP-3905,Create a generic interface for edits log.,Create a generic interface that would cover all edit log operations and be useful for implementing journaling functionality on different storage sources.
HADOOP-3904,A few tests still using old hdfs package name,"ClusterTestDFS, ClusterTestDFSNamespaceLogging, TestDistributedUpgrade are still using the old hdfs package name ""org.apache.hadoop.dfs""."
HADOOP-3901,CLASSPATH in bin/hadoop script is set incorrectly for cygwin,"In Cygwin, paths are required to be translated by cygpath.  In the bin/hadoop script, the CLASSPATH variable may be modified after cygpath translation.  In such case, CLASSPATH will be set incorrectly."
HADOOP-3899,trunk does not compile,"Compilation fails saying:
{code}
    [javac] /home/amarsri/workspace/trunk/src/core/org/apache/hadoop/net/ScriptBasedMapping.java:34: cannot find symbol
    [javac] symbol: class CachedDNSToSwitchMapping
    [javac] public final class ScriptBasedMapping extends CachedDNSToSwitchMapping
{code}

HADOOP-3620 missed adding new files to the trunk."
HADOOP-3897,SecondaryNameNode fails with NullPointerException,SecondaryNameNode fails while checkpoint an image which has file under Construction with NullPointerException
HADOOP-3894,"DFSClient chould log errors better, and provide better diagnostics","In my test runs I see a stack trace from DFSClient, because it isn't logging through the log APIs in its close() method. It should use the logger, for better error reporting"
HADOOP-3892,Include Unix group name in JobConf,A feature that was requested for the fair scheduler was to use a user's unix group name as their pool name so their jobs automatically have the right resource allocations. This might be useful for HADOOP-3445 as well.
HADOOP-3889,distcp: Better Error Message should be thrown when accessing source files/directory with no read permission,"Distcp: Better Error Message should be thrown when accessing/copying source files/directory with no read permission . So that it would help user to somehow avoid copying that subdirectory or get the owner to change the permissions.


$ hadoop dfs -lsr /user/scoopy/
Found 1 items
/user/scoopy/string1       <r 3>   2054687 2008-08-01 10:41        rwx------       scoopy 	data


--> Trying to copy the file which tom don't have permission to read 


$ logname
tom

$ hadoop distcp -i hftp://namenode1:8121/user/scoopy/string1 hdfs://namenode2:8121/user/tom/
08/07/31 17:36:00 INFO util.CopyFiles: srcPaths=[hftp://namenode1:8121/user/scoopy/string1]
08/07/31 17:36:00 INFO util.CopyFiles: destPath=hdfs://namenode2:8121/user/tom/
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.io.IOException: invalid xml directory content
        at org.apache.hadoop.dfs.HftpFileSystem$LsParser.fetchList(HftpFileSystem.java:184)
        at org.apache.hadoop.dfs.HftpFileSystem$LsParser.listStatus(HftpFileSystem.java:199)
        at org.apache.hadoop.dfs.HftpFileSystem$LsParser.listStatus(HftpFileSystem.java:207)
        at org.apache.hadoop.dfs.HftpFileSystem.listStatus(HftpFileSystem.java:214)
        at org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:933)
        at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:603)
        at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:743)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:763)
Caused by: org.xml.sax.SAXException: Unrecognized entry: RemoteException
        at org.apache.hadoop.dfs.HftpFileSystem$LsParser.startElement(HftpFileSystem.java:151)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:501)
        at
com.sun.org.apache.xerces.internal.parsers.AbstractXMLDocumentParser.emptyElement(AbstractXMLDocumentParser.java:179)
        at
com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:377)
        at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2740)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:647)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)
        at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:508)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
        at org.apache.hadoop.dfs.HftpFileSystem$LsParser.fetchList(HftpFileSystem.java:182)
        ... 9 more

----"
HADOOP-3887,Distcp help needs to be more elaborated.,"""hadoop distcp --help"" command gives help regarding the execution of the distcp command but it doesn't give the details about copying data between different versions of Hadoop.

The example displayed by the help documentation is :
hadoop distcp -p -update ""hdfs://A:8020/user/foo/bar"" ""hdfs://B:8020/user/foo/baz""

while copying between different versions, distcp uses hftp protocol instead of hdfs for source path.
"
HADOOP-3886,"Error in javadoc of Reporter, Mapper and Progressable","The javadoc for Reporter says:

""In scenarios where the application takes an insignificant amount of time to process individual key/value pairs""

Shouldn't this read /significant/ instead of insignificant?"
HADOOP-3884,eclipse plugin build is broken with current eclipse versions,"Eclipse plugin build is broken with current versions of Eclipse.

The problem seems to be that the constructor for org.eclipse.core.runtime.Status has changed.
"
HADOOP-3883,TestFileCreation fails once in a while,TestFileCreation fails once in a while.
HADOOP-3875,Fix TaskTracker's heartbeat timer to note the time the hearbeat RPC returned to decide next heartbeat time,"The tasktracker notes the time the heartbeat RPC was fired, rather than when it returned to decide when to send the next one. If the RPC spends a lot of time in the JobTracker's queue then the above exacerbates the load on the JobTracker."
HADOOP-3873,DistCp should have an option for limiting the number of files/bytes being copied,"A single DistCp command may potentially copies a huge number of files/bytes.  In such case, DistCp will run a long time and there is no way stop it nicely.  It would be good if DistCp have an option to limit the number of files/bytes being copied.  Once the limit is reached, DistCp will terminate and return success.  All files copied are guaranteed to be good and there is no partially copied file."
HADOOP-3868,Classes defined in tools dir cannot be executed without first created a jar,"Without creating hadoop-*-tools.jar, we cannot run tools like distcp.  For example,
{noformat}
bash-3.2$ ./bin/hadoop distcp
java.lang.NoClassDefFoundError: org/apache/hadoop/tools/DistCp
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.tools.DistCp
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:252)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)
Could not find the main class: org.apache.hadoop.tools.DistCp.  Program will exit.
{noformat}"
HADOOP-3866,Improve Hadoop Jobtracker Admin,"A number of enhancements to the jobtracker jsp that allow for searching, scheduling,  and navigating a large number of jobs.  These enhancements are to be added as subtasks, to follow."
HADOOP-3865,SecondaryNameNode runs out of memory,SecondaryNameNode has memory leak. If we leave secondary namenode to run for a while doing several checkpoints it runs out of heap.
HADOOP-3864,JobTracker lockup due to JobInProgress.initTasks taking significant time for large jobs on large clusters,"JobInProgress.initTasks takes significant amount of time on a large cluster for large jobs (55k maps * 3 splits), during which the JobInProgress object is locked up.

Simultaneously the JobClient is calling JobTracker.getTaskCompletionEvents which locks the JobTracker & tries to lock the JobInProgress, there-by it starves all heartbeats which are trying to lock the JobTracker - resulting in a lockup."
HADOOP-3863,Use a thread-local rather than static ENCODER/DECODER variables in Text for synchronization,Text.{ENCODER|DECODER} are static variables which need to be synchronized in Text.{read|write}String; given that lots of RPC code calls Text.{read|write}String the contention for this lock shows up very significantly on large clusters.
HADOOP-3861,Make MapFile.Reader and Writer implement java.io.Closeable,Both MapFile.Reader and Writer have a close() method with the right signature. They just need to declare that they implement Closeable.
HADOOP-3860,Compare name-node performance when journaling is performed into local hard-drives or nfs.,"The goal of this issue is to measure how the name-node performance depends on where the edits log is written to.
Three types of the journal storage should be evaluated:
# local hard drive;
# remote drive mounted via nfs;
# nfs filer.
"
HADOOP-3859,1000  concurrent read on a single file failing  the task/client,"After fixing Hadoop-3633, some users started seeing their tasks fail with 

{noformat}
08/07/29 05:13:07 INFO mapred.JobClient: Task Id : task_200807290511_0001_m_000846_0, Status : FAILED
java.io.IOException: Could not obtain block: blk_-7893038518783920880 file=/tmp/files111
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1430)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1281)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1385)
        at java.io.DataInputStream.read(DataInputStream.java:83)
        at org.apache.hadoop.mapred.LineRecordReader$LineReader.backfill(LineRecordReader.java:88)
        at org.apache.hadoop.mapred.LineRecordReader$LineReader.readLine(LineRecordReader.java:114)
        at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:179)
        at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:211)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2122)
{noformat}

This happened when hundreds of mappers pulled the same file concurrently.
"
HADOOP-3855,Fix import of MiniDFSCluster in TestCompressedEmptyMapOutputs.java,The merge of HADOOP-3827 to branch-0.18 had the wrong import ...
HADOOP-3854,org.apache.hadoop.http.HttpServer should support user configurable filter,"Filters provide universal functions such as authentication, logging and auditing, etc.  HttpServer should support configurable filters, so that individual site administrators could possibly configure filters for their web site."
HADOOP-3853,Move multiple input format extension to library package,"The code for supporting multiple paths with different input formats and mappers for each path (HADOOP-372) is implemented as a extension to MapReduce, so it should go in org.apache.hadoop.mapred.lib, not the core. (See https://issues.apache.org/jira/browse/HADOOP-372?focusedCommentId=12615185#action_12615185)"
HADOOP-3852,"If ShellCommandExecutor had a toString() operator that listed the command run, its error messages may be more meaningful","It may be easier to identify what causes error commands to be returned on an exec if the command is included in the error message. this can be done with 
* a toString() operator on the class that includes the list of arguments (when non null)
* a test that this works with arguments with spaces, and a null array
* relevant use of the operator when an execute fails, such as in TaskTracker"
HADOOP-3851,spelling error in FSNamesystemMetrics log message,FSNamesystemMetrics spells its classname wrong in the constructor log info message
HADOOP-3849,MapEventsFetcherThread doesn't wait for the heartbeat-interval if it doesn't have events to fetch,"The 'notify' done by FetchStatus.getMapEvents causes the MapEventsFetcherThread to immediately run to the JobTracker (getTaskCompletionEvents). 

On a 3500 node cluster, I saw that each TaskTracker calls JobTracker.getTaskCompletionEvents multiple times per-second. This caused the JobTracker's RPC queues to back-up resulting in each RPC spending more than 120s in the queue - leading to shuffle proceeding very very slowly."
HADOOP-3848,TaskTracker.localizeJob calls getSystemDir for each task rather than caching it ,"We should call it once and cache that value, I see 100k+ RPCs for large jobs..."
HADOOP-3846,CreateEditsLog used for benchmark misses creating parent directories,"CreateEditsLog generates a tree of directory structure while generating random files for benchmarks. I see that when generating new directories for depth of more than 2, we get a new directory name for parent directory. This was not logged into edits makes it an invalid edits file. "
HADOOP-3844,include message of local exception in Client call failures,"When Client fails with a local exception, that exception is retained, but the message is not propagated to the new exception text, which becomes simply  ""Call failed on local exception""

The forthcoming patch will change such messages to include that of the nested exception, so producing test reports containing useful data such as

java.io.IOException: Call failed on local exception: Connection refused"
HADOOP-3842,There is a window where the JobTracker is in the RUNNING state (i.e ready to accept jobs) and never executes them.,"Prior to HADOOP-3412, job tracker could accept jobs without even offering service (i.e without {{JobTracker.offerService()}} being called). In such a case the job stays in JT's memory and job execution was guaranteed. With HADOOP-3412, {{JobTracker.submitJob()}} adds the job to JT's local structures and passes it to the scheduler. Scheduler gets initialized in {{JobTracker.offerService()}} and hence calling {{JobTracker.submitJob()}} before calling {{JobTracker.offerService()}} is actually a no-op. The job stays in JT's memory but never gets initialized. This is 
- backward incompatible 
- erroneous as there is a window where the jobtracker is ready to accept jobs, accepts them and never executes them."
HADOOP-3841,merge phase runs out of disk space,We observe that reduce tasks run out of disk space during merging (after fetching all map output) although there would be enough space if the framework did not try to generate too large merge files.
HADOOP-3840,Support pluggable speculative execution,"HADOOP-3412 introduced an way to plug in a job scheduler for MapReduce. However, the job schedulers all use JobInProgress.obtainNewMapTask or obtainNewReduceTask to select tasks to run from each job, which uses a threshold-based speculative execution algorithm that has several shortcomings (see JIRAs about the scheduler not speculating tasks that freeze after having 80% progress for example). As a first step towards supporting better speculative execution policies while not breaking backwards compatibility, it makes sense to make the speculative execution policy pluggable. Luckily this is easy - we just need an interface around obtainNewMapTask and obtainNewReduceTask. This JIRA suggests adding a TaskSelector abstract class which, given a TaskTracker and a JobInProgress, chooses a task to run on the tracker. A default implementation that uses the current methods in JobInProgress is provided. Both TaskSchedulers in trunk are changed to use TaskSelector.

In addition, there are methods to count how many speculative tasks a job needs, since TaskInProgress.hasSpeculative() may not work if we change the algorithm for selecting speculative tasks. This count is needed for some schedulers, such as a fair scheduler."
HADOOP-3837,hadop streaming does not use progress reporting to detect hung tasks,"Hadoop streaming (StreamJob.java) sets mapred.task.timeout to 0. This effectively means that if tasks hang (either due to bad user code or machine related issue), these tasks never encounters a timeout. This causes the entire job to hang.

I propose that hadoop streaming not set the timeout to 0. By default, the settings in hadoop-default.xml should be effective for streaming jobs."
HADOOP-3836,TestMultipleOutputs will fail if it is ran more than one times,"TestMultipleOutputs will success after ""ant clean"" but it will fail if running it more than one times.
"
HADOOP-3832,Create more unit tests for testing HDFS appends,"Create more unit tests to test HDFS appends. One interesting unit test would be introduce ""appends"" to existing unit test TestDatanodeDeath."
HADOOP-3831,slow-reading dfs clients do not recover from datanode-write-timeouts,"Some of our applications read through certain files from dfs (using libhdfs) much slower than through others, such that they trigger the write timeout introduced in 0.17.x into the datanodes. Eventually they fail.

Dfs clients should be able to recover from such a situation.

In the meantime, would setting
dfs.datanode.socket.write.timeout=0
in hadoop-site.xml help?

Here are the exceptions I see:

DataNode:

2008-07-24 00:12:40,167 WARN org.apache.hadoop.dfs.DataNode: xxx:50010:Got exception while serving blk_3304550638094049
753 to /yyy:
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.
SocketChannel[connected local=/xxx:50010 remote=/yyy:42542]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:170)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:144)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:105)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105) 
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.dfs.DataNode$BlockSender.sendChunks(DataNode.java:1774)
        at org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1813)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1039) 
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:968)
        at java.lang.Thread.run(Thread.java:619)

DFS Client:

08/07/24 00:13:28 WARN dfs.DFSClient: Exception while reading from blk_3304550638094049753 of zzz from xxx:50010: java.io.IOException: Premeture EOF from inputStream
    at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:100)
    at org.apache.hadoop.dfs.DFSClient$BlockReader.readChunk(DFSClient.java:967)
    at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:236)
    at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:191)
    at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)
    at org.apache.hadoop.dfs.DFSClient$BlockReader.read(DFSClient.java:829)
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1352)
    at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1388)
    at java.io.DataInputStream.read(DataInputStream.java:83)

08/07/24 00:13:28 INFO dfs.DFSClient: Could not obtain block blk_3304550638094049753 from any node:  java.io.IOException: No live nodes contain current block
"
HADOOP-3829,Narrown down skipped records based on user acceptable value,"This is an incremental step over HADOOP-153.
If the number of skipped records in the neighborhood of a bad record are not acceptable to the user, then narrow down the skipped range to the user acceptable value."
HADOOP-3828,Write skipped records' bytes to DFS,"This is an incremental step over HADOOP-153, which provides the base skipping functionality."
HADOOP-3827,Jobs with empty map-outputs and intermediate compression fail,"The corner case where there are zero map-outputs doesn't pass the codec to the IFile.Writer leading to un-compressed data and subsequently failure on the reduce when it tries to decompress that data.

The straight-forward fix is to pass the codec:
{noformat}
           Writer<K, V> writer = new Writer<K, V>(job, finalOut, 
-                                                 keyClass, valClass, null);
+                                                 keyClass, valClass, codec);
{noformat}"
HADOOP-3824,Refactor org.apache.hadoop.mapred.StatusHttpServer,"StatusHttpServer is defined in the mapred package but it is used by hdfs classes DataNode, FSNamesystem, SecondaryNameNode."
HADOOP-3823,Refactor org.apache.hadoop.mapred.StatusHttpServer,"StatusHttpServer is defined in the mapred package but it is used by hdfs classes DataNode, FSNamesystem, SecondaryNameNode."
HADOOP-3821,SequenceFile's Reader.decompressorPool or Writer.decompressorPool gets into an inconsistent state when calling close() more than once,"SequenceFile.Reader uses a decompressorPool to reuse Decompressor instances. The Reader obtains such an instance from the pool on object creation and returns it back to the pool it when {{close()}} is called.

SequenceFile.Reader implements the {{java.io.Closable}} interface and it's spec on the {{close()}} method says:

{quote}
Closes this stream and releases any system resources associated 
with it. If the stream is already closed then invoking this 
method has no effect.
{quote}

This spec is violated by the Reader implementation, because calling {{close()}} multiple times has really bad implications. 
When you call {{close()}} twice, one and the same Decompressor instances will be returned to the pool two times and the pool would now maintain duplicated references to the same Decompressor instances. When other Readers now request instances from the pool it might happen that two Readers get the same Decompressor instance.

The correct behavior would be to just ignore a second call to {{close()}}.

The exact same issue applies to the SequenceFile.Writer as well.

We were having big trouble with this, because we were observing sporadic exceptions from merge operations. The strange thing was that executing the same merge again usually succeeded. But sometimes it took multiple attempts to complete a merge successfully. It was very hard to debug that the root cause was some duplicated Decompressor references in the decompressorPool.

Exceptions that we observed in production looked like this (we were using hadoop 0.17.0):

{noformat}
java.io.IOException: unknown compression method
at org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater.decompress(BuiltInZlibInflater.java:47)
at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:80)
at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:74)
at java.io.DataInputStream.readFully(DataInputStream.java:178)
at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:56)
at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)
at org.apache.hadoop.io.SequenceFile$Reader.nextRawKey(SequenceFile.java:1995)
at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:3002)
at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2760)
at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2625)
at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:2644)
{noformat}

or 

{noformat}
java.io.IOException: zero length keys not allowed
at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.appendRaw(SequenceFile.java:1340)
at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2626)
at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:2644)
{noformat}

The following snippet reproduces the problem:

{code:java}
    public void testCodecPool() throws IOException {
        Configuration conf = new Configuration();
        LocalFileSystem fs = new LocalFileSystem();
        fs.setConf(conf);
        fs.getRawFileSystem().setConf(conf);

        // create a sequence file
        Path path = new Path(""target/seqFile"");
        SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, path, Text.class, NullWritable.class, CompressionType.BLOCK);
        writer.append(new Text(""key1""), NullWritable.get());
        writer.append(new Text(""key2""), NullWritable.get());
        writer.close();

        // Create a reader which uses 4 BuiltInZLibInflater instances
        SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);
        // Returns the 4 BuiltInZLibInflater instances to the CodecPool
        reader.close();
        // The second close erroneously returns the same 4 BuiltInZLibInflater instances to the CodecPool again
        reader.close();

        // The first reader gets 4 BuiltInZLibInflater instances from the CodecPool
        SequenceFile.Reader reader1 = new SequenceFile.Reader(fs, path, conf);
        // read first value from reader1
        Text text = new Text();
        reader1.next(text);
        assertEquals(""key1"", text.toString());
        // The second reader gets the same 4 BuiltInZLibInflater instances from the CodePool as reader1
        SequenceFile.Reader reader2 = new SequenceFile.Reader(fs, path, conf);
        // read first value from reader2
        reader2.next(text);
        assertEquals(""key1"", text.toString());
        // read second value from reader1
        reader1.next(text);
        assertEquals(""key2"", text.toString());
        // read second value from reader2 (this throws an exception)
        reader2.next(text);
        assertEquals(""key2"", text.toString());
        
        assertFalse(reader1.next(text));
        assertFalse(reader2.next(text));
    }
{code}

It fails with the exception:

{noformat}
java.io.EOFException
	at java.io.DataInputStream.readByte(DataInputStream.java:243)
	at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:324)
	at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:345)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1835)
	at CodecPoolTest.testCodecPool(CodecPoolTest.java:56)
{noformat}

But this is just a very simple test that shows the problem. Much more weired things can happen when running in a complex production environment. Esp. heavy concurrency makes the behavior much more exciting. ;-)"
HADOOP-3820,"gridmix-env has a syntax error, and wrongly defines USE_REAL_DATASET by default","Syntax error due to missing double-quotes:
{noformat}
export ALL_HOD_OPTIONS=""-c ${HOD_CONFIG} ${HOD_OPTIONS}
{noformat}

And, the following line:
{noformat}
export USE_REAL_DATASET=false
{noformat}
is unnecessary."
HADOOP-3819,can not get svn revision # at build time if locale is not english,"My locale is zh_TW.UTF-8, so 'svn info' shows messages in chinese. But
src/saveVersion.sh expects english from output.

I suggest that we add clear LANG, LC_* in saveVersion.sh before calling svn."
HADOOP-3817,Maven build from hadoop-core,"Maven users would benefit from having Hadoop in a public maven repository, along with an archetype to quickly create the ""hello world"" maven examples.
By having hadoop in Maven repositories it is simpler to handle version increments for projects that build on top of Hadoop-core.
If this is of interest please vote for it and I will attempt to offer a patch into the hadoop build scripts that publish it for maven use - if no one votes, then I will not bother..."
HADOOP-3816,KFS changes for faster directory listing,"To improve directory listing performance in KFS, the KFS client library code has been updated with new APIs.  This JIRA is for propogating those changes into the Hadoop code base."
HADOOP-3815,[Build] Have an ant build property to exclude sources and docs while building a hadoop tar-ball.,This helps developers.
HADOOP-3814,"[HOD] Remove dfs.client.buffer.dir generation, as this is removed in Hadoop 0.19.","In HADOOP-3756, the dfs.client.buffer.dir parameter was removed from hadoop-default.xml, as this was no longer used. HOD generates this parameter in the hadoop-site it generates. This must be removed as well."
HADOOP-3813,RPC queue overload of JobTracker,"On a cluster with about 1700 nodes, when a job with about 100,000 maps and 10,000 reduces completed, the JobTracker, even with 80 handlers, could not handle the rpc call load during promotion of the job, such that at the end, because of the discarded heartbeats, the JobTracker lost nearly all TaskTrackers (about 10 TaskTrackers left). Promotion took more than 40 minutes.
They reconnected and everything recovered, but this might have been just luck.
Shouldn't there be an adaptive throttling of the rate in heartbeats and TaskCompletionEvents?

Sample messsages:
2008-07-22 18:21:55,831 WARN org.apache.hadoop.ipc.Server: Call queue overflow discarding oldest call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@115f6b6, false, true, 18137) from xxx
2008-07-22 18:21:55,834WARN org.apache.hadoop.ipc.Server: Call queue overflow discarding oldest call getTaskCompletionEvents(job_200807190635_0012, 119567, 50) from yyy
...
2008-07-22 19:02:28,821 WARN org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9020, call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@19d32fa, false, true, 18199) from zzz: discarded for being too old (40936)
2008-07-22 19:02:28,821 WARN org.apache.hadoop.ipc.Server: IPC Server handler 34 on 9020, call getTaskCompletionEvents(job_200807190635_0012, 119567, 50) from uuu: discarded for being too old (40978)
"
HADOOP-3810,NameNode seems unstable on a cluster with little space left,"NameNode seems not very responsive and unstable when the cluster has very little space left. The clients timeout. The main problem is that it is not clear to the user what is going on. Once I have more details about a NameNode that was in this state, I will fill in here.

If there is not enough space left on a cluster, it is ok for clients to receive something like ""DiskOutOfSpace"" exception. 

Right now it looks like NameNode tries too hard find a node with any space left and ends up being slow to respond to clients. If the CPU taken by chooseTarger() is the main cause, there are two possible fixes :

# chooseTarget() iterates and takes quite a bit of CPU for allocating datanodes. Usually this not much of a problem. It takes even more cpu when it needs to search multiple racks for a datanode. We could probably reduce some CPU for these searches. The benefit should be measurable.
# Once NameNode can not find any datanode that has space on a rack, it could mark the rack as ""full"" and skip searching the rack for next one minute or so. This flag gets cleared after a minute or if any new node is added to the rack.
#* Of course, this might not be optimal w.r.t disk space usage.. but only for a short duration. Once a cluster is mostly full, the user does expect errors.
#* On the flip side, this fix does not require extremely CPU optimized version of chooseTarget(). 
#* I think it is reasonable for NameNode to throw DiskOutOfSpace exception, even though it could have found space if it searched much more extensively.

---
edit : minor changes

 "
HADOOP-3809,TestCLI fails on trunk point out that moveFromLocal/put output has changed,"TestCLI failed detecting a change in output of put and moveFromLocal 
{noformat}
Testcase: testAll took 24.112 sec
  FAILED
One of the tests failed. See the Detailed results to identify the command that failed
junit.framework.AssertionFailedError: One of the tests failed. See the Detailed results to identify the command that failed
  at org.apache.hadoop.cli.TestCLI.displayResults(TestCLI.java:272)
  at org.apache.hadoop.cli.TestCLI.tearDown(TestCLI.java:141)
{noformat}
"
HADOOP-3808,[HOD] Include job tracker RPC in notes attribute after job submission,"Currently, after a job is submitted, HOD records the Jobtracker URL and the NameNode URL in the 'notes' attribute of the Torque resource manager. This helps in building centralized administration tools that need to get a full picture of all allocated clusters etc. Including the Jobtracker RPC port in this attribute would be useful to build such similar tools. One of them is an idle job tracker that can be centrally run to clear up any clusters that aren't automatically cleaned up due to some faulty nodes in the cluster or resource manager problems."
HADOOP-3806,Remove debug message from Quicksort,QuickSort.java:78 prints debug information to stdout; this line should be removed.
HADOOP-3805,improve fuse-dfs write performance which is 33% slower than hadoop dfs -copyFromLocal,"on hadoop17 running on the namenode/fuse mount point, fuse is 33% slower for writing a 1GB file. I don't know right now how to improve this but thought I should open up a JIRA for it.

Note - this benchmark is with no tuning or special fuse config params.
"
HADOOP-3804,NPE in FSNamesystem.addStoredBlock(...),"{noformat}
2008-07-21 22:30:43,489 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 59403,
  call blockReceived(DatanodeRegistration(DN_HOST:58052, storageID=DS-YYYYYY, infoPort=55243, ipcPort=50020), [Lorg.apache.hadoop.hdfs.protocol.Block;@11e
725, [Ljava.lang.String;@15e4ae5) from DN_HOST:49527: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.addStoredBlock(FSNamesystem.java:2800)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.blockReceived(FSNamesystem.java:3138)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.blockReceived(NameNode.java:637)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)
{noformat}"
HADOOP-3802,IPC - Heartbeat exceptions filling up log files,"We have a datanode (10.0.0.93) that is in a semi-live state.  An ssh session is able to do a connect but is unable to send or receive any data.  The connection is closed immediately after the connection is established.  Because of this, the name node's logs are full of the following message:

2008-07-21 09:36:10,800 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1on 9000, call sendHeartbeat(10.0.0.193:50010, 917638877184, 20480, 644100882925, 0, 0) from 10.0.0.193:55908: error: org.apache.hadoop.dfs.IncorrectVersionException: Unexpected version of data node. Reported: -11. Expecting = -13.
org.apache.hadoop.dfs.IncorrectVersionException: Unexpected version of data node. Reported: -11. Expecting = -13.
        at org.apache.hadoop.dfs.NameNode.verifyVersion(NameNode.java:682)
        at org.apache.hadoop.dfs.NameNode.verifyRequest(NameNode.java:669)
        at org.apache.hadoop.dfs.NameNode.sendHeartbeat(NameNode.java:557)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

Approximately generating 100MB a minute."
HADOOP-3798,compile-core-test fails to compile,"compile-core-test fails to compile on branch-0.18

compile-core-test:
   [javac] Compiling 7 source files to /branch-0.18/build/test/classes
   [javac] Note: Some input files use unchecked or unsafe operations.
   [javac] Note: Recompile with -Xlint:unchecked for details.
   [javac] Compiling 238 source files to /branch-0.18/build/test/classes
   [javac] /branch-0.18/src/test/org/apache/hadoop/mapred/TestJobShell.java:27: package org.apache.hadoop.hdfs does not exist
   [javac] import org.apache.hadoop.hdfs.MiniDFSCluster;
   [javac]                              ^
   [javac] /branch-0.18/src/test/org/apache/hadoop/mapred/TestJobShell.java:38: cannot find symbol
   [javac] symbol  : class MiniDFSCluster
   [javac] location: class org.apache.hadoop.mapred.TestJobShell
   [javac]     MiniDFSCluster dfs = null;
   [javac]     ^
   [javac] /branch-0.18/src/test/org/apache/hadoop/mapred/TestJobShell.java:45: cannot find symbol
   [javac] symbol  : class MiniDFSCluster
   [javac] location: class org.apache.hadoop.mapred.TestJobShell
   [javac]       dfs = new MiniDFSCluster(conf, 2 , true, null);
   [javac]                 ^
   [javac] Note: Some input files use or override a deprecated API.
   [javac] Note: Recompile with -Xlint:deprecation for details.
   [javac] 3 errors"
HADOOP-3796,"fuse-dfs should take rw,ro,trashon,trashoff,protected=blah mount arguments rather than them being compiled in","Right now read only versus read write and other options are compile time. We should change these to be options just like any other mount options.
"
HADOOP-3795,NameNode does not save image if different dfs.name.dir have different checkpoint stamps,There is a case where namenode does not save image file during startup even if their checkpoint times are different. 
HADOOP-3794,KFS implementation needs to return directory modification time,"Kosmos filesystem object returns 0 for directory modification time.  Instead, it should return the appropriate value."
HADOOP-3793,HADOOP-3724 reopened,"Namenode was running release hadoop-2008-06-26_11-02-19 on one of our test clusters
and could not restart because of same exception as reported in HADOOP-3724.

I list it below again.

Because of comments made in HADOOP-3724 we upgraded to hadoop-2008-07-17_12-21-22 and tried again -- still same exception.
We then deleted the edits.new file and restarted. This helped, but, of course, we lost files and history.

Till this is fixed, I would be scared to use 0.18.x in production.

2008-07-17 17:48:14,027 INFO org.apache.hadoop.dfs.Storage: Edits file edits.new of size 521210751 edits # 3314377 loaded in 140 seconds.
2008-07-17 17:48:16,625 ERROR org.apache.hadoop.fs.FSNamesystem: FSNamesystem initialization failed.
java.io.IOException: saveLeases found path /xxx but no matching entry in namespace.
        at org.apache.hadoop.dfs.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4368)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:877)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:895)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:81)
        at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:252)
        at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)
        at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:825)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:834)
2008-07-17 17:48:16,625 INFO org.apache.hadoop.ipc.Server: Stopping server on 4600
2008-07-17 17:48:16,627 ERROR org.apache.hadoop.dfs.NameNode: java.io.IOException: saveLeases found path /xxx but no matching entry in namespace.
        at org.apache.hadoop.dfs.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4368)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:877)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:895)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:81)
        at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:252)
        at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)
        at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:825)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:834)
"
HADOOP-3792,"exit code from ""hadoop dfs -test ..."" is wrong for Unix shell","The exit code for the ""test"" command in hadoop.fs.FsShell is backwards relative to the Unix shell, which interprets an exit code of 0 as true and anything else as false."
HADOOP-3791,Use generics in ReflectionUtils,"The attached patch makes ReflectionUtils.newInstance use generics appropriately. I've also gone through and updated everywhere that calls ReflectionUtils.newInstance to remove now-redundant casts, or to add further generics where appropriate. The overall result is about 17 fewer @SupressWarnings(""unchecked"") needed in the source tree. "
HADOOP-3790,Add more unit tests to test appending to files in HDFS,"A new feature ""appends to HDFS files"" have been implemented in HADOOP-1700. There are a set of unit tests in TestFileAppend.java and TestFileAppend2.java. But we would like to have more unit tests."
HADOOP-3786,Changes in HOD documentation,"After reviewing bugs 3505 & 3668, following changes are required in HOD documentation.

1. HOD user guide : 
===============
Term HDFS should be used instead of DFS under following sections:
3.9. Capturing HOD exit codes in Torque
4.3. hod Fails With an error code and error message

2. HOD admin guide:
================
'Torque' should be a  hyperlink in Pre-requisites section of hod admin guide. 
"
HADOOP-3785,FileSystem cache should be case-insensitive,"We cache FileSystem instances based on URI scheme & authority, plus username.  Elsewhere we compare URI schemes and authorities case-insensitively, but the cache is case-sensitive.  In particular, the Cache.Key#equals() and Cache.Key#hashCode() should be made case-insensitive.

This should not be a blocker, since the worst it causes is a few extra FileSystem instances to be cached."
HADOOP-3784,Cleanup optimization of reads and change it to a flag and remove #ifdefs,"Looks like optimized reads work so let's make them part of the regular core of code.  But, should allow a flag and custom sized buffer.
"
HADOOP-3783,"""deprecated filesystem name"" warning on EC2","When logged in to the master node, in a Hadoop cluster on EC2, launched with src/contrib/ec2/bin/hadoop-ec2 ...

Various bin/hadoop commands (jar, distcp, fs) display this warning one or more times:

08/07/17 14:46:23 WARN fs.FileSystem: ""ip-10-251-203-207.ec2.internal:50001"" is a deprecated filesystem name. Use ""hdfs://ip-10-251-203-207.ec2.internal:50001/"" instead.

"
HADOOP-3780,JobTracker should synchronously resolve the tasktracker's network location when the tracker registers,"This issue is inspired by HADOOP-3620. In JobTracker, the network address of tracker gets resolved asynchronously. Now it can be done inline i.e while the trackers register. This is of great help for HADOOP-3245 where this enhancement makes the design simpler."
HADOOP-3778,seek(long) in DFSInputStream should catch socket exception for retry later,"HADOOP-2346 introduced data read/write timeout. when data stream borken, DFSClient will retry in read/write methods, but no such mechanism found when seek(long) calling blockReader.skip(diff) (DFSClient.java #1582), will let IOException throw to application.  i met NPE when using MapFile in hbase.

i'm supposing in the seek(long) method, let done be 'false' will causing retry (via 'blockEnd = -1'), a patch will attached later for review."
HADOOP-3777,Failure to load native lzo libraries causes job failure,HADOOP-2664 added Lzop to the list of codecs; lzo will throw from initIDs when the native lzo libraries are not loaded. The exception should be caught and ignored.
HADOOP-3776,NPE in NameNode with unknown blocks,"When a datanode has a block that NameNode does not have, it results in an NPE at the NameNode. And one of these cases results in an infinite loop of these errors because DataNode keeps invoking the same RPC that resulted in this NPE.

One way to reproduce :

 * On a single DN cluster, start writing a large file (something like {{'bin/hadoop fs -put 5Gb 5Gb'}})
 * Now, from a different shell, delete this file ({{bin/hadoop fs -rm 5Gb}})
 * Most likely you will hit this.
 * The cause is that when DataNode invokes {{blockReceived()}} to inform about the last block it received, the file is already deleted and results in an NPE at the namenode. The way DataNode works, it basically keep invoking the same RPC with same block and results in the same error.

When block does not exist in NameNode's blocksMap, it basically does not belong to the cluster. Let me know if you need the trace. Basically the NPE is at FSNamesystem.java:2800 (on trunk)."
HADOOP-3775,Namenode trapped in an infinite loop,"2008-07-16 21:56:46,150 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on xxx/.Trash/Current/machine.com/
machine.com/job_200807161951_0145/job.jar file does not exist.
2008-07-16 21:56:46,150 INFO org.apache.hadoop.dfs.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], sortedLeases.size()=: 2
2008-07-16 21:56:46,150 INFO org.apache.hadoop.fs.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], src=xxx/.Trash/Current/machine.com
/machine.com/job_200807161951_0145/job.jar
2008-07-16 21:56:46,150 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on xxx/.Trash/Current/machine.com/
machine.com/job_200807161951_0145/job.jar file does not exist.
2008-07-16 21:56:46,150 INFO org.apache.hadoop.dfs.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], sortedLeases.size()=: 2
2008-07-16 21:56:46,150 INFO org.apache.hadoop.fs.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], src=xxx/.Trash/Current/machine.com
/machine.com/job_200807161951_0145/job.jar
2008-07-16 21:56:46,150 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on xxx/.Trash/Current/machine.com/
machine.com/job_200807161951_0145/job.jar file does not exist.
2008-07-16 21:56:46,150 INFO org.apache.hadoop.dfs.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], sortedLeases.size()=: 2
2008-07-16 21:56:46,150 INFO org.apache.hadoop.fs.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], src=xxx/.Trash/Current/machine.com
/machine.com/job_200807161951_0145/job.jar
2008-07-16 21:56:46,150 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on xxx/.Trash/Current/machine.com/
machine.com/job_200807161951_0145/job.jar file does not exist.
2008-07-16 21:56:46,150 INFO org.apache.hadoop.dfs.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], sortedLeases.size()=: 2
2008-07-16 21:56:46,150 INFO org.apache.hadoop.fs.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], src=xxx/.Trash/Current/machine.com
/machine.com/job_200807161951_0145/job.jar
2008-07-16 21:56:46,150 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on xxx/.Trash/Current/machine.com/
machine.com/job_200807161951_0145/job.jar file does not exist.
2008-07-16 21:56:46,150 INFO org.apache.hadoop.dfs.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], sortedLeases.size()=: 2
2008-07-16 21:56:46,150 INFO org.apache.hadoop.fs.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_-1021744751, pendingcreates: 1], src=xxx/.Trash/Current/machine.com
/machine.com/job_200807161951_0145/job.jar"
HADOOP-3774,Typos in shell output,"{noformat}
[ ~/hadoop-0.18.01]$ ./bin/hadoop --config ~/1157/ jar hadoop-0.18.01-examples.jar randomtextwriter
randomtextwriter [-outFormat <output format class>] <input>
Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenod
-jt <local|jobtracker:port>    specify a job tracker
-files <comma separated list of fiels>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma seperated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]
{noformat}
- <input> should be <output>
- ""fiels"" should be ""files""
- ""namenod"" should be ""namenode""
- ""seperated "" should be ""separated """
HADOOP-3773,Setting the conf twice in Pipes Submitter,"In Submitter.java, 
{noformat}
    228     setIfUnset(conf, ""mapred.output.key.class"", textClassname);
    229     setIfUnset(conf, ""mapred.output.value.class"", textClassname);
    230     setIfUnset(conf, ""mapred.output.key.class"", textClassname);
    231     setIfUnset(conf, ""mapred.output.value.class"", textClassname);
{noformat}

I think this meant to set mapred.*map*output.{key,value}.class instead of setting the same conf twice.
(It doesn't really matter since mapred.mapoutput.{key,value}.class defaults to mapred.output.{key,value}.class.)
"
HADOOP-3772,Proposed hadoop instrumentation API,"We want to evolve the Hadoop metrics subsystem into a more generic Instrumentation facility.  The ultimate goal is to add structured logging to Hadoop, with causal tags, a la X-trace.  The existing metrics framework is not quite suitable for our needs, since the implementation and interface are tightly coupled. There's no way to use the metrics instrumentation points for anything other than metricss, and there's no way for a metrics context to find out what event just happened.  

We want to tease apart the generic notion of hookable instrumentation points, from the specifics of the information recording. The latter ought to  be pluggable at run-time.  
"
HADOOP-3771,JobClient.runJob() should not kill the job on IOExceptions,Currently the {{JobClient.runJob()}} submits a job to the job tracker and then periodically asks the JT for the job's progress. On successive IOExceptions the JobClient kills the job. This is not a desired behaviour since the JobClient is issuing a kill-job command to the JT which is not reachable. This is a problem for HADOOP-3245 since its highly possible that the JT can come up anytime and then it makes no sense to kill the job.
HADOOP-3770,"improve composition, submission and result collection of gridmix","Current gridmix submits jobs using a set of scripts, which is inconvenient and the results are difficult to collect.  To improve the gridmix submission and results collection, we implemented a new program  using JobControl to submit and collect the results of jobs 
Also the new gridmix allows to have more different types of jobs such as, pig jobs, jobs with combiners etc. "
HADOOP-3769,expose static SampleMapper and SampleReducer classes of GenericMRLoadGenerator class for gridmix reuse,"Currently, the SampleMapper and SampleReducer are package static in GenericMRLoadGenerator class.  In order for gridmix to  reuse them,
we need to make them public. 

Also, we need a function which can returns the jobClient from the Job class. so that gridmix can get the job status information. "
HADOOP-3766,fuse-dfs should allows permissions to be viewed/set,"FUSE has support for chmod,chown,chgrp operation. We should pass permissions sets through to libhdfs, and return permission information from libhdfs back to the kernel"
HADOOP-3765,fuse-dfs should access hdfs as the client user,"Users accessing hdfs mount through fuse-dfs do so as the user running fuse_dfs executable. In this case, it would be better to run fuse-dfs as some privileged user, and use separate connections to the hdfs for each client user accessing the mount.

See HADOOP-3536 for more discussion."
HADOOP-3764,libhdfs should allow connection to the DFS as a given user,"Consider a gateway service (e.g. NFS server->HDFS gateway, fuse-dfs).

In these examples, different users will connect to the the gateway service. In each case, libhdfs should facilitate connection using the user's security context. This ensures that existing permissions, ownership, group-ownership in the DFS are respected.
"
HADOOP-3762,Task tracker died due to OOM ,"When running about 100 moderate jobs on a small cluster (with 19 Task Trackers),
the task trackers all died due to OOM.
I got a chance to dump the jstack strace of a task tracker before it died.
Its image size was close 4GB!
I saw 1200+ threads of DFSClient.LeaseChecker.
Clearly we have a severe resource leakage problem!

"
HADOOP-3761,"FileSplit's default constructor should be made ""public""","mapred.FileSplit's default constructor is currently ""package"" visible. We should make it ""public"" so we can support a GenericSplit class outside mapred package, which could create FileSplit instances using reflection and serialize/deserialize FileSplit through Writable interface.
"
HADOOP-3760,DFS operations fail because of Stream closed error,DFS operations fail because of  {{java.io.IOException: Stream closed.}}. 
HADOOP-3759,Provide ability to run memory intensive jobs without affecting other running tasks on the nodes,"In HADOOP-3581, we are discussing how to prevent memory intensive tasks from affecting Hadoop daemons and other tasks running on a node. A related requirement is that users be provided an ability to run jobs which are memory intensive. The system must provide enough knobs to allow such jobs to be run while still maintaining the requirements of HADOOP-3581."
HADOOP-3758,Excessive exceptions in HDFS namenode log file,"I upgraded a big cluster, out of which 10 nodes did not get upgraded.  
The namenode log showed excessive exceptions, causing the namenode log to ate the entire partition space, in this case close to 700GB log file was generated on the namenode.  
"
HADOOP-3756,"dfs.client.buffer.dir isn't used in hdfs, but it's still in conf/hadoop-default.xml","It looks like the usage of the config variable dfs.client.buffer.dir was removed in 612903, but the variable lives on in the conf/hadoop-default.xml configuration file.  If this variable really isn't used anymore, maybe we should remove it from the hadoop-default.xml file?  
"
HADOOP-3755,the gridmix scripts do not work with hod 0.4,"Hod 0.4 requires a config dir is created before a cluster can be allocated.
"
HADOOP-3754,Support a Thrift Interface to access files/directories in HDFS,"Thrift is a cross-language RPC framework. It supports automatic code generation for a variety of languages (Java, C++, python, PHP, etc) It would be nice if HDFS APIs are exposed though Thirft. It will allow applications written in any programming language to access HDFS."
HADOOP-3752,Audit logging fails to record rename,The audit log (HADOOP-3336) must also record rename events.
HADOOP-3751,Assign tasktrackers more than one task per hearbeat,"Currently each TaskTracker gets one and only one new task to run per heartbeat. Also, a TaskTracker immediately rushes to the JobTracker when a task completes without honouring the heartbeat interval (default of 5s).

The problem with this is multi-fold:
1. This is a utilization bottleneck, especially when the TaskTracker just starts up. We should be assigning atleast 50% of it's capacity.
2. If the individual tasks are very short i.e. run for less than the heartbeat interval the TaskTracker serially runs _one task at a time_.
3. For jobs with small maps, the TaskTracker never gets a chance to schedule reduces till _all maps are complete_. This means shuffle doesn't overlap with maps at all, another sore-point.

Overall, the right approach is to let the TaskTracker advertise the number of available map and reduce slots in each heartbeat and the JobTracker (i.e the Scheduler - HADOOP-3412/HADOOP-3445) should decide how many tasks and which maps/reduces the TaskTracker should be assigned. Also, we should ensure that the TaskTracker doesn't run to the JobTracker every-time a task completes - maybe we should hard-limit to the heartbeat interval or maybe run to the JobTracker when there are more than one completed tasks in a given heartbeat interval etc. 

Lets discuss."
HADOOP-3750,Fix and enforce module dependencies,"Since HADOOP-2916 Hadoop Core consists of modules in independent source trees: core, hdfs, mapred (and others). The dependencies between them should be enforced to avoid cyclic dependencies. At present they all have dependencies on each other.

"
HADOOP-3748,Flag to make tasks to send counter information only at the end of the task,"Currently counters are streaming from the task to the jobtracker as the task progresses. If the number of counters is large this has a significant impact on the network traffic as well as in the JobTracker load.

The should be a flag, for example by counter-group, that indicates that the counters are to be reported at the end of the task. By default this flag should be set to false for all counter-groups maintaining the current behavior."
HADOOP-3747,Add counter support to MultipleOutputs,"Add a new group of counters, ie ""multipleoutputs"" and make {{MultipleOutputs}} to keep a  counter per name output for the number of records and size.

Add also a switch to enable/disable the counters."
HADOOP-3746,A fair sharing job scheduler,"The default job scheduler in Hadoop has a first-in-first-out queue of jobs for each priority level. The scheduler always assigns task slots to the first job in the highest-level priority queue that is in need of tasks. This makes it difficult to share a MapReduce cluster between users because a large job will starve subsequent jobs in its queue, but at the same time, giving lower priorities to large jobs would cause them to be starved by a stream of higher-priority jobs. Today one solution to this problem is to create separate MapReduce clusters for different user groups with Hadoop On-Demand, but this hurts system utilization because a group's cluster may be mostly idle for long periods of time. HADOOP-3445 also addresses this problem by sharing a cluster between different queues, but still provides only FIFO scheduling within a queue.

This JIRA proposes a job scheduler based on fair sharing. Fair sharing splits up compute time proportionally between jobs that have been submitted, emulating an ""ideal"" scheduler that gives each job 1/Nth of the available capacity. When there is a single job running, that job receives all the capacity. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so that everyone gets roughly the same amount of compute time. This lets short jobs finish in reasonable amounts of time while not starving long jobs. This is the type of scheduling used or emulated by operating systems - e.g. the Completely Fair Scheduler in Linux. Fair sharing can also work with job priorities - the priorities are used as weights to determine the fraction of total compute time that a job should get. 

In addition, the scheduler will support a way to guarantee capacity for particular jobs or user groups. A job can be marked as belonging to a ""pool"" using a parameter in the jobconf. An ""allocations"" file on the JobTracker can assign a minimum allocation to each pool, which is a minimum number of map slots and reduce slots that the pool must be guaranteed to get when it contains jobs. The scheduler will ensure that each pool gets at least its minimum allocation when it contains jobs, but it will use fair sharing to assign any excess capacity, as well as the capacity within each pool. This lets an organization divide a cluster between groups similarly to the job queues in HADOOP-3445.

*Implementation Status:*

I've implemented this scheduler using a version of the pluggable scheduler API in HADOOP-3412 that works with Hadoop 0.17. The scheduler supports fair sharing, pools, priorities for weighing job shares, and a text-based allocation config file that is reloaded at runtime whenever it has changed to make it possible to change allocations without restarting the cluster. I will also create a patch for trunk that works with the latest interface in the patch submitted for HADOOP-3412.

The actual implementation is simple. To implement fair sharing, the scheduler keeps track of a ""deficit"" for each job - the difference between the amount of compute time it should have gotten on an ideal scheduler, and the amount of compute time it actually got. This is a measure of how ""unfair"" we've been to the job. Every few hundred milliseconds, the scheduler updates the deficit of each job by looking at how many tasks each job had running during this interval vs. how many it should have had given its weight and the set of jobs that were running in this period. Whenever a task slot becomes available, it is assigned to the job with the highest deficit - unless there were one or more jobs who were not meeting their pool capacity guarantees, in which case we choose among those ""needy"" jobs based again on their deficit.

*Extensions:*

Once we keep track of pools, weights and deficits, we can do a lot of interesting things with a fair scheduler. One feature I will probably add is an option to give brand new jobs a priority boost until they have run for, say, 10 minutes, to reduce response times even further for short jobs such as ad-hoc queries, while still being fair to longer-running jobs. It would also be easy to add a ""maximum number of tasks"" cap for each job as in HADOOP-2573 (although with priorities and pools, this JIRA reduces the need for such a cap - you can put a job in its own pool to give it a minimum share, and set its priority to VERY_LOW so it never takes excess capacity if there are other jobs in the cluster). Finally, I may implement ""hierarchical pools"" - the ability for a group to create pools within its pool, so that it can guarantee minimum allocations to various types of jobs but ensure that together, its jobs get capacity equal to at least its full pool."
HADOOP-3743,"-libjars, -files and -archives options do not work with 0.18",I am not sure how to get -libjars and -files working with 0.18. I tried all the options but cannot get it runnning. I am filing this as a blocker until we find out that its not broken. -- in that case we need to update the docs with an example to say how it works. 
HADOOP-3741,SecondaryNameNode has http server on dfs.secondary.http.address but without any contents,"SecondaryNameNode has http server on dfs.secondary.http.address but does not have any information on it. It could publish useful information about NameNode it is talking to, checkpoints, the current configured checkpoint interval and checkpoint size and also configured directories."
HADOOP-3740,Make JobInProgress pluggable,"By allowing a pluggable JobInProgess it will be possible for provide implementations that can do a sophisticated task provisioning to the JobTracker. 

For example, by providing alternate implementations of the {{obtainNewMapTask}}, {{obtainNewReduceTask}} and {{updateTaskInProgress}} it would be possible to implement a license server that allows to throttle use of external resources (ie webservices, databases) so at any given time there are not more than N tasks using a given resource. For this a task could be tagged with the names of external resources and the license server would keep track of the tasks running per tag, if the counter reaches zero then the {{obtainNew*Task}} method could return NULL instead of a task.
 "
HADOOP-3737,CompressedWritable throws OutOfMemoryError,"We were seeing OutOfMemoryErrors with stack traces like the following (Hadoop 0.17.0):

{noformat}
java.lang.OutOfMemoryError
        at java.util.zip.Deflater.init(Native Method)
        at java.util.zip.Deflater.<init>(Deflater.java:123)
        at java.util.zip.Deflater.<init>(Deflater.java:132)
        at org.apache.hadoop.io.CompressedWritable.write(CompressedWritable.java:71)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1016)
        [...]
{noformat}

A Google search found the following long-standing issue in Java in which use of java.util.zip.Deflater causes an OutOfMemoryError:

[http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4797189]

CompressedWritable instantiates a Deflater, but does not call {{deflater.end()}}.  It should do that in order to release the Deflater's resources immediately, instead of waiting for the object to be finalized.

We applied this change locally and saw much improvement in the stability of memory usage of our app.

This may also affect the SequenceFile compression types, because org.apache.hadoop.io.compress.zlib.BuiltInZlib{Deflater,Inflater} extend java.util.zip.{Deflater,Inflater}.  org.apache.hadoop.io.compress.Compressor defines an end() method, but I do not see that this method is ever called."
HADOOP-3732,Block scanner should read block information during initialization.,"We see a lot of warning messages on each data-node during startup and upgrading from 0.17 to 0.18 saying:
{code}
2008-07-08 22:22:15,711 WARN org.apache.hadoop.dfs.DataNode: Block /grid/3/hadoop/var/hdfs/data/current/blk_3359714082706415785 does not have a metafile!
{code}
The message received twice for each block, because the block information is first read buy the data-node itself and then by the block scanner."
HADOOP-3730,add new JobConf constructor that disables loading default configurations,"Similar to the {{Configuration(boolean loadDefauls)}} , {{JobConf}} should have such constructor.

This would allow supporting default values on the JT side as the client would not submit its default values in the job.xml. 

It would address HADOOP-3287

"
HADOOP-3728,Cannot run more than one instance of examples.SleepJob at the same time.,All instances of org.apache.hadoop.examples.SleepJob use the same file to store temporary datas. So it is not possible to run more than one of them at the same time.
HADOOP-3726,TestCLI loses exception details on setup/teardown,"The TestCLI test case catches exceptions during setup and teardown and prints them, instead of just relaying them up to the test runner, for them to be caught and logged."
HADOOP-3725,TestMiniMRMapRedDebugScript loses exception details,"The test class org.apache.hadoop.mapred.TestMiniMRMapRedDebugScript catches and prints exception details then calls fail(), rather than relaying the exception up to the calling test framework. this make the full stack trace harder to obtain. It could be a relic of the class having its own main() entry point.

trivial to fix: add Exception to the test signature, move the catch/print into the main() method for that entry point"
HADOOP-3724,Namenode does not start due to exception throw while saving Image,"Re-start of namenode failed with this stack trace while savingImage during initialization

{noformat}
2008-07-09 00:20:21,470 INFO org.apache.hadoop.ipc.Server: Stopping server on 9000
2008-07-09 00:20:21,493 ERROR org.apache.hadoop.dfs.NameNode: java.io.IOException: saveLeases found path /foo/bar/jambajuice but no matching entry in namespace.  
at org.apache.hadoop.dfs.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4376)  
at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:874)  
at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:892)  
at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:81)   
at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)   
at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:252)   
at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)   
at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)   
at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)   
at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:822)  
at org.apache.hadoop.dfs.NameNode.main(NameNode.java:831)
{noformat}

Looks like it was throwing IOException in saveFilesUnderConstruction

Before restart NameNode was killed while some jobs were running. Upon looking at the namenode log before the stopping of namenode, there were many entries like this 

{noformat}
2008-07-09 00:12:55,301 INFO org.apache.hadoop.fs.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_-510679348, pendingcreates: 1], src=/foo/bar/jambajuice
2008-07-09 00:12:55,301 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on /foo/bar/jambajuice  file does not exist.
{noformat}

These 2 lines are repeated forever every second, to a point where I see that a 7 node cluster had namenode log with size close to 41G.

Could not find any other information about the file as there were not previous namenode logs. 
"
HADOOP-3723,libhdfs only accepts O_WRONLY and O_RDONLY so does not accept things like O_WRONLY | O_CREAT,"hdfs.c in the hdfsOpenFile method checks for flags == O_WRONLY rather than  flags & O_WRONLY and same for reads.
"
HADOOP-3722,Provide a unified way to pass jobconf options from bin/hadoop,"Often when running a job it is useful to override some jobconf parameters from jobconf.xml for that particular job - for example, setting the job priority, setting the number of reduce tasks, setting the HDFS replication level, etc. Currently the Hadoop examples, streaming, pipes, etc take these extra jobconf parameters in different was: the examples in hadoop-examples.jar use -Dkey=value, streaming uses -jobconf key=value, and pipes uses -jobconf key1=value1,key2=value2,etc. Things would be simpler if bin/hadoop could take the jobconf parameters itself, so that you could run for example bin/hadoop -Dkey=value jar [whatever] as well as bin/hadoop -Dkey=value pipes [whatever]. This is especially useful when an organization needs to require users to use a particular property, e.g. the name of a queue to use for scheduling in HADOOP-3445. Otherwise, users may confuse one way of passing parameters with another and may not notice that they forgot to include certain properties.

I propose adding support in bin/hadoop for jobconf options to be specified with -C key=value. This would have the effect of setting hadoop.jobconf.key=value in Java's system properties. The Configuration class would then be modified to read any system properties that begin with hadoop.jobconf and override the values in hadoop-site.xml.

I can write a patch for this pretty quickly if the design is sound. If there's a better way of specifying jobconf parameters uniformly across Hadoop commands, let me know."
HADOOP-3721,CompositeRecordReader::next is unnecessarily complex,"Much of the join logic can be made more readable and its interfaces cleaned up. In particular, CompositeRecordReader::next would benefit from some refactoring."
HADOOP-3720,dfsadmin -refreshNodes should re-read the config file.,"hadoop dfsadmin -refreshNodes reads files defined by _dfs.hosts_ and _dfs.hosts.exclude_. If these values are not defined during namenode startup, then refreshNodes command assumes there are no files. So, while namenode is running, once cannot just edit the config file and add entries to these config variables and expect the namenode to read them. Namenode should re-read the config file for these variables (_dfs.hosts_ and _dfs.hosts.exclude_) when refreshNodes command is invoked. "
HADOOP-3719,Chukwa,"We'd like to contribute Chukwa, a data collection and analysis framework being developed at Yahoo!.  Chukwa is a natural complement to Hadoop, since it is built on top of HDFS and Map-Reduce, and since Hadoop clusters are a key use case.

"
HADOOP-3718,KFS: write(int v) API writes out an integer rather than a byte ,"The KFS Outputstream code has an incorrect implementation for write(int) interface; it should write out a byte, but the code writes out an integer. "
HADOOP-3716,KFS listStatus() returns NULL on empty directories," KosmosFileSystem listStatus() API incorrectly returns NULL on a valid directory, that is empty."
HADOOP-3714,Bash tab completion support,"The attached Bash script adds support for tab completion of most arguments to the main Hadoop script ({{bin/hadoop}}). Namely, it allows tab completion of all the command names, subcommands for the {{fs}}, {{dfsadmin}}, {{job}}, {{namenode}} and {{pipe}} commands, arguments of the {{jar}} command and most arguments to the {{fs}} subcommands (completing local and dfs paths as appropriate).

The file can be dropped into /etc/bash_completion.d/ on Debian-like distros, and it should then start working the next time Bash is started."
HADOOP-3713,broken symlinks in jobcache when local tasks are done but job is in progress,"When all running tasks on a tasktracker are done, not all links for  /<mapred.local.dir>/taskTracker/jobcache/<job>/work are deleted. This is resulting in new tasks from the same job scheduled on this node to fail with

 2008-07-07 17:44:49,756 INFO org.apache.hadoop.mapred.TaskTracker: LaunchTaskAction: task_200807071715_0022_r_000295_0
 2008-07-07 17:44:49,773 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing task_200807071715_0022_r_000295_0:
 java.io.IOException: Mkdirs failed to create /tmp3/taskTracker/jobcache/job_200807071715_0022/work
 at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:680)
        at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:1274)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:915)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1310)
       at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:2251)

$  ls -lt /tmp3/taskTracker/jobcache/job_200807071715_0022/work
lrwxrwxrwx 1 user users 135 Jul  7 17:44 /tmp3/taskTracker/jobcache/job_200807071715_0022/work -> /tmp0/taskTracker/jobcache/job_200807071715_0022/work
$  ls -lt /tmp0/mapred-local/taskTracker/jobcache/job_200807071715_0022/work
ls: /tmp0/taskTracker/jobcache/job_200807071715_0022/work: No such file or directory

Earlier tasks scheduled on this tasktracker have completed successfully

2008-07-07 17:44:44,926 INFO org.apache.hadoop.mapred.TaskRunner: task_200807071715_0022_r_000004_0 done; removing files.
2008-07-07 17:44:44,931 INFO org.apache.hadoop.mapred.TaskRunner: task_200807071715_0022_r_000176_0 done; removing files.
2008-07-07 17:44:44,958 INFO org.apache.hadoop.mapred.TaskRunner: task_200807071715_0022_r_000210_0 done; removing files.
2008-07-07 17:44:49,486 INFO org.apache.hadoop.mapred.TaskRunner: task_200807071715_0022_r_000153_0 done; removing files.

"
HADOOP-3711,Streaming input is not parsed properly to find the separator,"Sometimes, finding field separator position in the input line for streaming fails with following exception:
org.apache.hadoop.streaming.PipeMapRed: java.lang.IllegalArgumentException: splitPos must be in the range [0, 17]: 74
	at org.apache.hadoop.streaming.UTF8ByteArrayUtils.splitKeyVal(UTF8ByteArrayUtils.java:152)
	at org.apache.hadoop.streaming.PipeMapRed.splitKeyVal(PipeMapRed.java:337)
	at org.apache.hadoop.streaming.PipeMapRed$MROutputThread.run(PipeMapRed.java:363)

"
HADOOP-3709,Lock hierarchy violation in namenode while handling hearbeats,"The heartbeat processing code recently got rearranged via HADOOP-3254. FSNamesystem.handleHeartbeat acquires the hearbeat lock and then invoke blockReportProcessed. This method tries to acquire the global FSNamesystem lock. This is a lock hierarchy violation. This leads to deadlock.

The heatbeat processing code should acquire only the heartbeat lock. It should not acquire the global lock, otherwise heartprocessing become too heavyweight.

This code occurs only on trunk and not o 018 branch. Surprise!"
HADOOP-3707,Frequent DiskOutOfSpaceException on almost-full datanodes,"On a datanode which is completely full (leaving reserve space),  we frequently see

target node reporting, 
{noformat}
2008-07-07 16:54:44,707 INFO org.apache.hadoop.dfs.DataNode: Receiving block blk_3328886742742952100 src: /11.1.11.111:22222 dest: /11.1.11.111:22222
2008-07-07 16:54:44,708 INFO org.apache.hadoop.dfs.DataNode: writeBlock blk_3328886742742952100 received exception org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Insufficient space for an additional block
2008-07-07 16:54:44,708 ERROR org.apache.hadoop.dfs.DataNode: 33.3.33.33:22222:DataXceiver: org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Insufficient space for an additional block
        at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:444)
        at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:716)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.<init>(DataNode.java:2187)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1113)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:976)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Sender reporting 
{noformat}
2008-07-07 16:54:44,712 INFO org.apache.hadoop.dfs.DataNode: 11.1.11.111:22222:Exception writing block blk_3328886742742952100 to mirror 33.3.33.33:22222
java.io.IOException: Broken pipe
        at sun.nio.ch.FileDispatcher.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
        at sun.nio.ch.IOUtil.write(IOUtil.java:75)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:53)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:140)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:144)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:105)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveChunk(DataNode.java:2292)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2411)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2476)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1204)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:976)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Since it's not constantly happening,  my guess is whenever datanode gets some small space available, namenode over-assigns blocks which can fail the block
pipeline.
(Note, before 0.17, namenode was much slower in assigning blocks)

"
HADOOP-3706,CompositeInputFormat: Unable to wrap custom InputFormats,"I am unable to use a custom InputFormat with the CompositeInputFormat as the classloader that is used by Parser is unable to find my class.

To reproduce (although I've got an example program, if that's preferred?):
1) Create a custom InputFormat (I made a copy of SequenceFileInputFormat and named it MyInputFormat)
2) Create a program using CompositeInputFormat [Set ""mapred.join.expr"" to CompositeInputFormat.compose(""outer"", MyInputFormat.class, plist)]
3) Create jar file
4) Run job (must be via the jar - the problem cannot be reproduced in Local mode)

Doing so causes the following exception:
{code}
Caused by: java.io.IOException
	at org.apache.hadoop.mapred.join.Parser$WNode.parse(Parser.java:274)
	at org.apache.hadoop.mapred.join.Parser.reduce(Parser.java:463)
	at org.apache.hadoop.mapred.join.Parser.parse(Parser.java:481)
	at org.apache.hadoop.mapred.join.CompositeInputFormat.setFormat(CompositeInputFormat.java:77)
	at org.apache.hadoop.mapred.join.CompositeInputFormat.validateInput(CompositeInputFormat.java:118)

Caused by: java.lang.ClassNotFoundException: my.custom.input.format.MyInputFormat
	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:268)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at org.apache.hadoop.mapred.join.Parser$WNode.parse(Parser.java:270)
{code}

Should  the line on Parser.java:271 be something like:
{code}
jobConf.getClassByName(sb.toString());
{code}
instead of:
{code}
Class.forName(sb.toString()).asSubclass(InputFormat.class)
{code}
to ensure the correct classloader is used?
"
HADOOP-3705,CompositeInputFormat is unable to parse InputFormat classes with names containing '_' or '$',"If I try to set  ""mapred.join.expr"" with my own InputFormat class that contains a  '_' character (or if my InputFormat class is a static inner class),  I get the following exception thrown when I run my MapReduce job: 

Exception in thread ""main"" java.io.IOException: Unexpected: 95
	at org.apache.hadoop.mapred.join.Parser$Lexer.next(Parser.java:161)
	at org.apache.hadoop.mapred.join.Parser.parse(Parser.java:479)
	at org.apache.hadoop.mapred.join.CompositeInputFormat.setFormat(CompositeInputFormat.java:77)
	at org.apache.hadoop.mapred.join.CompositeInputFormat.validateInput(CompositeInputFormat.java:118)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:705)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:973)

If the class contains '$' the exception reads Unexpected: 95.

"
HADOOP-3703,[HOD] logcondense needs to use the new pattern of output in hadoop dfs -lsr,"In Hadoop 0.18, the format of *hadoop dfs -ls* and *hadoop dfs -lsr* commands has changed. It is a Unix like output now, with the filename being printed as the last column of the file listing as opposed to the first. logcondense.py needs to use this new format."
HADOOP-3702,add support for chaining Maps in a single Map and after a Reduce [M*/RM*],"On the same input, we usually need to run multiple Maps one after the other without no Reduce. We also have to run multiple Maps after the Reduce.

If all pre-Reduce Maps are chained together and run as a single Map a significant amount of Disk I/O will be avoided. 

Similarly all post-Reduce Maps can be chained together and run in the Reduce phase after the Reduce.
"
HADOOP-3700,Define User exception which can be thrown by user code to skip records,"Currently there is no way, where user code can throw a specific Exception to notify the framework about skipping that record and move forward. 
This can be implemented in map, reduce, combine, and output format (maybe) as well as input method.
To get the skip behavior, the user needs to write code that throws these exceptions, so this would not change the systems default behavior. "
HADOOP-3699,Create a UI for viewing configured queues and related information,"HADOOP-3445 implements multiple queues in the JobTracker as part of the new resource manager for Hadoop (HADOOP-3421). There needs to be a way for users and administrators to see information related to these queues. This JIRA is for tracking the requirements, approach and implementation of this UI."
HADOOP-3698,Implement access control for submitting jobs to queues in the JobTracker,"HADOOP-3445 implements multiple queues in the JobTracker as part of the new resource manager for Hadoop (HADOOP-3421). There needs to be a mechanism to control who can submit jobs to a specified queue. This JIRA is for tracking the requirements, approach and implementation for the same."
HADOOP-3695,[HOD] Have an ability to run multiple slaves per node,"Currently HOD launches at most one slave per node. For purposes of testing a large number of slaves on much fewer resources - for e.g. testing scalability of clusters, it will be useful if it can provision multiple slaves per node."
HADOOP-3694,"if MiniDFS startup time could be improved, testing time would be reduced","Its taking me 140 minutes to run a test build; looking into the test results its the 20s startup delay of every MiniDFS cluster that is slowing things down. If we could find out why it is taking so long and cut it down, every test case that relied on a cluster would be speeded up. "
HADOOP-3693,"Fix documentation for Archives, distcp and native libraries","Include the changes suggested by Corinne in Hadoop archives, Distcp user guide and Native libraries documentation."
HADOOP-3692,Fix documentation for Cluster setup and Quick start guides,Include the changes suggested by Corinne in Cluster setup and quickstart guides
HADOOP-3691,Fix mapred docs,Include the changes suggested by Corinne in Map/Reduce  and streaming documentation
HADOOP-3690,TestHDFSServerPorts fails on 0.18 branch,"I just svn update'd branch-0.18 and looks like TestHDFSServerPorts test is failing.
This has to be fixed, promoting this as blocker.
{noformat}
Testcase: testNameNodePorts took 2.485 sec
Testcase: testDataNodePorts took 1.249 sec
  FAILED
null
junit.framework.AssertionFailedError
  at org.apache.hadoop.dfs.TestHDFSServerPorts.testDataNodePorts(TestHDFSServerPorts.java:183)

Testcase: testSecondaryNodePorts took 1.222 sec
{noformat}"
HADOOP-3689,TestHDFSServerPorts fails on 0.18 branch,"I just svn update'd branch-0.18 and looks like TestHDFSServerPorts test is failing.
This has to be fixed, promoting this as blocker.
{noformat}
Testcase: testNameNodePorts took 2.485 sec
Testcase: testDataNodePorts took 1.249 sec
  FAILED
null
junit.framework.AssertionFailedError
  at org.apache.hadoop.dfs.TestHDFSServerPorts.testDataNodePorts(TestHDFSServerPorts.java:183)

Testcase: testSecondaryNodePorts took 1.222 sec
{noformat}"
HADOOP-3688,Fix up HDFS docs,"The first three files are corrections from Corinne. The patch has the fixes.

Forrest docs only. No code. No new tests."
HADOOP-3685,Unbalanced replication target ,"In HADOOP-3633, namenode was assigning some datanodes to receive  hundreds of blocks in a short period which caused datanodes to go out of memroy(threads).
Most of them were from remote rack.

Looking at the code, 

{noformat}
    166           chooseLocalRack(results.get(1), excludedNodes, blocksize,
    167                           maxNodesPerRack, results);
{noformat}

was sometimes not choosing the local rack of the writer(source).  

As a result, when a datanode goes down, other datanodes on the same rack were getting large number of blocks from remote racks.
"
HADOOP-3684,The data_join should allow the user to implement a customer cloning function,"Currently, the framework uses serialization/deserialization to clone the values passed to the resuce function.
This amounts to a very heavy weight deep copy of the value objects.
That is way too expensive. Although that may be a generic way to work for all possible value classes, thus good as a default way,
the framework should allow the user to implemet an application specific yet efficient cloning function.
 "
HADOOP-3683,Hadoop dfs metric FilesListed shows number of files listed instead of operations,"Hadoop dfs metric FilesListed shows number of files listed, it should be number of fileListing operations instead. "
HADOOP-3682,"duplicate ""Map Completion Events"" in tasktracker fetch status","we saw duplicate ""MapCompletionEvents"" in FetchStatus from TaskTracker. This prevents the ReduceTask from getting all the map locations, and the ReduceTask is waiting forever.

This is a printout of the MapCompletionEvents from a FetchStatus from the TaskTracker:

988	0	task_200806270248_6185_m_000980_0	980	Map	SUCCEEDED
989	0	task_200806270248_6185_m_000787_0	787	Map	SUCCEEDED
990	0	task_200806270248_6185_m_000749_0	749	Map	SUCCEEDED
991	0	task_200806270248_6185_m_000848_0	848	Map	SUCCEEDED
992	0	task_200806270248_6185_m_000919_0	919	Map	SUCCEEDED
993	0	task_200806270248_6185_m_000836_0	836	Map	SUCCEEDED
994	0	task_200806270248_6185_m_000838_0	838	Map	SUCCEEDED
995	0	task_200806270248_6185_m_000936_0	936	Map	SUCCEEDED
996	0	task_200806270248_6185_m_000933_0	933	Map	SUCCEEDED
997	0	task_200806270248_6185_m_000856_0	856	Map	SUCCEEDED
998	0	task_200806270248_6185_m_000885_0	885	Map	SUCCEEDED
999	0	task_200806270248_6185_m_000804_0	804	Map	SUCCEEDED
1000	0	task_200806270248_6185_m_000986_0	986	Map	SUCCEEDED *first copy begins*
1001	0	task_200806270248_6185_m_000999_0	999	Map	SUCCEEDED
1002	0	task_200806270248_6185_m_000990_0	990	Map	SUCCEEDED
1003	0	task_200806270248_6185_m_000985_0	985	Map	SUCCEEDED
1004	0	task_200806270248_6185_m_000968_0	968	Map	SUCCEEDED
1005	0	task_200806270248_6185_m_001000_0	1000	Map	SUCCEEDED
1006	0	task_200806270248_6185_m_000996_0	996	Map	SUCCEEDED
1007	0	task_200806270248_6185_m_000974_0	974	Map	SUCCEEDED
1008	0	task_200806270248_6185_m_000998_0	998	Map	SUCCEEDED
1009	0	task_200806270248_6185_m_000982_0	982	Map	SUCCEEDED
1010	0	task_200806270248_6185_m_001010_0	1010	Map	SUCCEEDED
1011	0	task_200806270248_6185_m_000959_0	959	Map	SUCCEEDED
1012	0	task_200806270248_6185_m_000991_0	991	Map	SUCCEEDED *first copy ends*
1013	0	task_200806270248_6185_m_000986_0	986	Map	SUCCEEDED *second copy begins*
1014	0	task_200806270248_6185_m_000999_0	999	Map	SUCCEEDED
1015	0	task_200806270248_6185_m_000990_0	990	Map	SUCCEEDED
1016	0	task_200806270248_6185_m_000985_0	985	Map	SUCCEEDED
1017	0	task_200806270248_6185_m_000968_0	968	Map	SUCCEEDED
1018	0	task_200806270248_6185_m_001000_0	1000	Map	SUCCEEDED
1019	0	task_200806270248_6185_m_000996_0	996	Map	SUCCEEDED
1020	0	task_200806270248_6185_m_000974_0	974	Map	SUCCEEDED
1021	0	task_200806270248_6185_m_000998_0	998	Map	SUCCEEDED
1022	0	task_200806270248_6185_m_000982_0	982	Map	SUCCEEDED
1023	0	task_200806270248_6185_m_001010_0	1010	Map	SUCCEEDED
1024	0	task_200806270248_6185_m_000959_0	959	Map	SUCCEEDED
1025	0	task_200806270248_6185_m_000991_0	991	Map	SUCCEEDED *second copy ends*

"
HADOOP-3681,Infinite loop in dfs close,"We had dfsClient -put  hang outputting 

{noformat}
2008-06-28 10:05:12,595 WARN org.apache.hadoop.dfs.DFSClient: DataStreamer Exception: java.net.SocketTimeoutException:
timed out waiting for rpc response
2008-06-28 10:05:12,595 WARN org.apache.hadoop.dfs.DFSClient: Error Recovery for block null bad datanode[0]
2008-06-28 10:05:51,067 INFO org.apache.hadoop.dfs.DFSClient: Could not complete file
/_temporary/_task_200806262325_4136_r_000408_0/part-00408
retrying...
2008-06-28 10:05:52,898 INFO org.apache.hadoop.dfs.DFSClient: Could not complete file
/_temporary/_task_200806262325_4136_r_000408_0/part-00408
retrying...
2008-06-28 10:05:54,893 INFO org.apache.hadoop.dfs.DFSClient: Could not complete file
/_temporary/_task_200806262325_4136_r_000408_0/part-00408
retrying...
2008-06-28 10:05:56,920 INFO org.apache.hadoop.dfs.DFSClient: Could not complete file
/_temporary/_task_200806262325_4136_r_000408_0/part-00408
retrying...
2008-06-28 10:05:57,765 INFO org.apache.hadoop.dfs.DFSClient: Could not complete file
/_temporary/_task_200806262325_4136_r_000408_0/part-00408
retrying...
2008-06-28 10:05:58,199 INFO org.apache.hadoop.dfs.DFSClient: Could not complete file
/_temporary/_task_200806262325_4136_r_000408_0/part-00408
retrying...
[repeats forever]
{noformat}"
HADOOP-3679,"calls to junit Assert::assertEquals invert arguments, causing misleading error messages, other minor improvements.","JUnit Assert::assertEquals takes its expected and actual arguments in a particular order, but many unit tests invert them. The error message from a failed assertion can be misleading."
HADOOP-3678,"Avoid spurious ""DataXceiver: java.io.IOException: Connection reset by peer"" errors in DataNode log","When a client reads data using read(), it closes the sockets after it is done. Often it might not read till the end of a block. The datanode on the other side keeps writing data until the client connection is closed or end of the block is reached. If the client does not read till the end of the block, Datanode writes an error message and stack trace to the datanode log. It should not. This is not an error and it just pollutes the log and confuses the user."
HADOOP-3677,Problems with generation stamp upgrade,"# The generation stamp upgrade renames blocks' meta-files so that the name contains the block generation stamp as stated in HADOOP-2656.
If a data-node has blocks that do not belong to any files and the name-node asks the data-node to remove those blocks 
during or before the upgrade started the data-node will remove the blocks but not the meta-files because their names 
are still in the old format which is not recognized by the new code. So we can end up with a number of garbage files which
will be hard to recognize that they are unused and the system will never remove them automatically.
I think this should be handled by the upgrade code in the end, but may be it will be right to fix HADOOP-3002 for the 0.18 release,
which will avoid scheduling block removal when the name-node is in safe-mode.
# I was not able to get the upgrade -force option to work. This option lets the name-node proceed with a distributed upgrade even if
the data-nodes are not able to complete their local upgrades. Did we test this feature at all for the generation stamp upgrade?"
HADOOP-3673,Deadlock in Datanode RPC servers,"There is a deadlock scenario in the way Lease Recovery is triggered using the Datanode RPC server via HADOOP-3310.

Each Datanode has dfs.datanode.handler.count handler threads (default of 3). These handler threads are used to support the generation-stamp-dance protocol as described in HADOOP-1700.

Let me try to explain the scenario with an example. Suppose, a cluster has two datanodes. Also, let's assume that dfs.datanode.handler.count is set to 1. Suppose that there are two clients, each writing to a separate file with a replication factor of 2. Let's assume that both clients encounter an IO error and triggers the generation-stamp-dance protocol. The first client may invoke recoverBlock on the first datanode while the second client may invoke recoverBlock on the second datanode. Now, each of the datanode will try to make a getBlockMetaDataInfo() to the other datanode. But since each datanode has only 1 server handler threads, both threads will block for eternity. Deadlock!"
HADOOP-3670,JobTracker running out of heap space,"The JobTracker on our 0.17.0 installation runs out of heap space rather quickly, with less than 100 jobs (at one time even after just 16 jobs).
Running in 64-bit mode with larger heap space does not help -- it will use up all available RAM.

2008-06-28 05:17:06,661 INFO org.apache.hadoop.ipc.Server: IPC Server handler 62 on 9020, call he
artbeat(org.apache.hadoop.mapred.TaskTrackerStatus@6f81c6, false, true, 17384) from xxx.xxx.xxx.xxx
:51802: error: java.io.IOException: java.lang.OutOfMemoryError: GC overhead limit exceeded
java.io.IOException: java.lang.OutOfMemoryError: GC overhead limit exceeded


"
HADOOP-3668,Clean up HOD documentation,"Include the changes in HOD documentation suggested by Corinne Chandel.

Terminology:
-          Map/Reduce
-          HDFS               (not DFS)
-          NameNode
-          TaskTracker
-          For example      (not For e.g.)

Other changes:
-          Remove most italics (except some in the User Guide)
-          Various edits
-          Spell-check"
HADOOP-3667,Remove deprecated methods in JobConf,"Remove the following deprecated methods in org.apache.hadoop.mapred.JobConf
setInputPath(Path)
addInputPath(Path)
getInputPaths()
getOutputPath()
setOutputPath(Path)
getSystemDir()
setMapOutputCompressionType(CompressionType style)
getMapOutputCompressionType()"
HADOOP-3665,WritableComparator newKey() fails for NullWritable,"It is not possible to use NullWritable as a key in order to suppress key value in output.

Syndrome exception:
Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableComparator can not access a member of class org.apache.hadoop.io.NullWritable with modifiers ""private""

The problem is that NullWritable is a singleton and does not provide public non-parametric constructor. The following code in WritableComparator causes the exception: return (WritableComparable)keyClass.newInstance();

Proposed simple solution is to use ReflectionUtils instead (it requires modification as well).

This issue is probably related to HADOOP-2922"
HADOOP-3664,Remove deprecated methods introduced in changes to validating input paths (HADOOP-3095),"Remove FileInputFormat#listPaths, FileInputFormat#validateInput, FileInputFormat#getSplitsForPaths, FileSystem#getFileBlockLocations(Path, long, long)"
HADOOP-3662,Misleading log message in computeReplicationWork(),"FSNamesystem.computeReplicationWork() removes a block from neededReplications when the block has reached required replication, and prints the following message:
{code}
NameNode.stateChangeLog.info(""BLOCK* ""
    + ""Removing block "" + block
    + "" from neededReplications as it does not belong to any file."");
{code}
The message should rather say 
{code}
""as it reached replication "" + numEffectiveReplicas + ""; required "" + requiredReplication
{code}
"
HADOOP-3661,Normalize fuse-dfs handling of moving things to trash wrt the way hadoop dfs does it (only when non posix trash flag is enabled in compile),"Currently fuse-dfs is very crude about moving things to Trash. It can only move them once since it doesn't use any numbering scheme to prevent overwrites.

Should look at the code that hadoop dfs uses to move things to the trash and have the same behavior, i think.

Leaving it as major because it's pretty annoying and you can't tell what's wrong when you get an EIO when deleting something.
"
HADOOP-3660,Add replication factor for injecting blocks in the data node cluster ,"Data node cluster is used for testing and benchmarking.
This jira improves it as follows:
 - allows the user to specify the location of the data node dirs
 - allows a replication factor to be specified for injected blocks.
"
HADOOP-3659,Patch to allow hadoop native to compile on Mac OS X,This patch makes the autoconf script work on Mac OS X.  LZO needs to be installed (including the optional shared libraries) for the compile to succeed.  You'll want to regenerate the configure script using autoconf after applying this patch.
HADOOP-3658,Incorrect destination IP logged for receiving blocks,"Looking at the datanode log on 11,11,11,11, I see log messages

2008-06-26 06:17:06,542 INFO org.apache.hadoop.dfs.DataNode: Receiving block blk_-6227562494169624558 src: /22.22.22.22:60118 dest: /22.22.22.22:50010
2008-06-26 06:17:06,544 INFO org.apache.hadoop.dfs.DataNode: Receiving block blk_-5852727174809951081 src: /33.33.33.33:60113 dest: /33.33.33.33:50010
...
2008-06-26 06:17:54,740 INFO org.apache.hadoop.dfs.DataNode: Received block blk_-3283780755887143287 src: /44.44.44.44:38528 dest: /44.44.44.44:50010 of size
157339
2008-06-26 06:17:54,748 INFO org.apache.hadoop.dfs.DataNode: Received block blk_493831167536490902 src: /55.55.55.55:59008 dest: /55.55.55.55:50010 of size 49
5

It confused me not to see 11.11.11.11 as the destination."
HADOOP-3657,HDFS writes get stuck trying to recoverBlock,"A few reduces got stuck in a sort500 job with the following thread dump:

{noformat}
""main"" prio=10 tid=0x0805b800 nid=0x1951 waiting for monitor entry [0xf7e6d000..0xf7e6e1f8]
   java.lang.Thread.State: BLOCKED (on object monitor)
  at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:2485)
  - waiting to lock <0xe905e8f8> (a java.util.LinkedList)
  - locked <0xe905e928> (a org.apache.hadoop.dfs.DFSClient$DFSOutputStream)
  at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:155)
  at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)
  - locked <0xe905e928> (a org.apache.hadoop.dfs.DFSClient$DFSOutputStream)
  at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)
  - locked <0xe905e928> (a org.apache.hadoop.dfs.DFSClient$DFSOutputStream)
  at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:58)
  - locked <0xe905e928> (a org.apache.hadoop.dfs.DFSClient$DFSOutputStream)
  at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:39)
  at java.io.DataOutputStream.writeInt(DataOutputStream.java:181)
  at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1014)
  - locked <0xe90889e8> (a org.apache.hadoop.io.SequenceFile$Writer)
  at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.write(SequenceFileOutputFormat.java:70)
  at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:298)
  at org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)
  at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:316)
  at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2157)

""DataStreamer for file /rw/out/_temporary/_attempt_200806261801_0006_r_000712_0/part-00712 block blk_-3923696991063961587_9628"" daemon prio=10 tid=0x08413c00 nid=0x367a in Object.wait() [0xd00e4000..0xd00e4f20]
   java.lang.Thread.State: WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  at java.lang.Object.wait(Object.java:485)
  at org.apache.hadoop.ipc.Client.call(Client.java:701)
  - locked <0xf167d540> (a org.apache.hadoop.ipc.Client$Call)
  at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
  at org.apache.hadoop.dfs.$Proxy2.recoverBlock(Unknown Source)
  at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2186)
  at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1400(DFSClient.java:1737)
  at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1891)
  - locked <0xe905e8f8> (a java.util.LinkedList)
{noformat}"
HADOOP-3656,fetcher should re-use connection when it needs to fetch multiple segments from the same task tracker,"
In the current implementation, the fetcher will fetch one segment a time from a task tracker.
In the case where a job has N mappers per tracker, each reducer will need N trips to each tracker.
That will generate a lot of network traffic when N and the number of reduces is large.
The problem will be improved if the fetcher can retrieve multiple segments from a tracker per connection, either through http keep alive or 
through application level protocol.
"
HADOOP-3655,provide more control options for the junit run,"with a few more properties, people running the junit tests could
 * choose a faster forking policy
 * increase the allocated memory (useful on 64 bit java)
 * ask for more console-level logging
 * halt on the first failure"
HADOOP-3653,test-patch target not working on hudson.zones.apache.org due to HADOOP-3480,"HADOOP-3480 has broken the test-patch target run by the Hudson Hadoop-Patch job.  This is causing all patches to get a -1 on contrib testing.

Also, the message is very confusing now.  This is what the console shows:

{quote}
    ...
     [exec]     [junit] 08/06/26 14:34:57 INFO ipc.Server: IPC Server handler 9 on 44137: exiting
     [exec] 
     [exec] BUILD SUCCESSFUL
     [exec] Total time: 4 minutes 2 seconds
     [exec] Some jars are not declared in the Eclipse project.
     [exec] 
    ...
     [exec] 
     [exec]     -1 contrib tests.  The patch failed contrib unit tests.
     [exec] 
    ...
{quote}

The -1 comes from the ""Some jars are not declared in the Eclipse project"" line which is a warning caused by HADOOP-3480.  This needs to be improved.

The problem arises from the fact that additional jar files are copied into the lib/ directory for clover (code coverage), checkstyle, etc by the test-patch process on Hudson.  Also, there are src/test/lib/ftp*.jar referenced in the ECLIPSE_DECLARED_JARS variable:

{quote}
echo $PRESENT_JARS
lib/clover.jar lib/commons-cli-2.0-SNAPSHOT.jar lib/commons-codec-1.3.jar lib/commons-httpclient-3.0.1.jar lib/commons-logging-1.0.4.jar lib/commons-logging-api-1.0.4.jar lib/commons-net-1.4.1.jar lib/excluded/checkstyle-all-4.3.jar lib/jets3t-0.6.0.jar lib/jetty-5.1.4.jar lib/jetty-ext/commons-el.jar lib/jetty-ext/jasper-compiler.jar lib/jetty-ext/jasper-runtime.jar lib/jetty-ext/jsp-api.jar lib/junit-3.8.1.jar lib/kfs-0.1.3.jar lib/log4j-1.2.13.jar lib/oro-2.0.8.jar lib/rat-0.5.1.jar lib/servlet-api.jar lib/slf4j-api-1.4.3.jar lib/slf4j-log4j12-1.4.3.jar lib/xmlenc-0.52.jar

echo $ECLIPSE_DECLARED_JARS
lib/commons-cli-2.0-SNAPSHOT.jar lib/commons-codec-1.3.jar lib/commons-httpclient-3.0.1.jar lib/commons-logging-1.0.4.jar lib/commons-logging-api-1.0.4.jar lib/commons-net-1.4.1.jar lib/jets3t-0.6.0.jar lib/jetty-5.1.4.jar lib/jetty-ext/commons-el.jar lib/jetty-ext/jasper-compiler.jar lib/jetty-ext/jasper-runtime.jar lib/jetty-ext/jsp-api.jar lib/junit-3.8.1.jar lib/kfs-0.1.3.jar lib/log4j-1.2.13.jar lib/oro-2.0.8.jar lib/servlet-api.jar lib/slf4j-api-1.4.3.jar lib/slf4j-log4j12-1.4.3.jar lib/xmlenc-0.52.jar src/test/lib/ftplet-api-1.0.0-SNAPSHOT.jar src/test/lib/ftpserver-core-1.0.0-SNAPSHOT.jar src/test/lib/ftpserver-server-1.0.0-SNAPSHOT.jar src/test/lib/mina-core-2.0.0-M2-20080407.124109-12.jar
{quote}

Brice, can you rework this?"
HADOOP-3652,Remove deprecated class OutputFormatBase,Deprecated OutputFormatBase class has to be removed from org.apache.hadoop.mapred
HADOOP-3650,Unit test TestMiniMRDFSSort.testMapReduceSort fails on windows with a timeout,"Unit test TestMiniMRDFSSort.testMapReduceSort fails on windows with a timeout

I see this on the console:
[junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:29:19,435 WARN  mapred.ReduceTask (ReduceTask.java:run(928)) - attempt_200806252017_0003_r_000000_0 copy failed: attempt_200806252017_0003_m_000002_0 from localhost
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:29:19,435 WARN  mapred.ReduceTask (ReduceTask.java:run(930)) - java.net.SocketTimeoutException: Read timed out
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    [junit] attempt_200806252017_0003_r_000000_0:     at java.lang.reflect.Constructor.newInstance(Constructor.java:494)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1225)
    [junit] attempt_200806252017_0003_r_000000_0:     at java.security.AccessController.doPrivileged(Native Method)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.net.www.protocol.http.HttpURLConnection.getChainedException(HttpURLConnection.java:1219)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:906)
    [junit] attempt_200806252017_0003_r_000000_0:     at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.getInputStream(ReduceTask.java:1217)
    [junit] attempt_200806252017_0003_r_000000_0:     at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.getMapOutput(ReduceTask.java:1067)
    [junit] attempt_200806252017_0003_r_000000_0:     at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:976)
    [junit] attempt_200806252017_0003_r_000000_0:     at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:925)
    [junit] attempt_200806252017_0003_r_000000_0: Caused by: java.net.SocketTimeoutException: Read timed out
    [junit] attempt_200806252017_0003_r_000000_0:     at java.net.SocketInputStream.socketRead0(Native Method)
    [junit] attempt_200806252017_0003_r_000000_0:     at java.net.SocketInputStream.read(SocketInputStream.java:129)
    [junit] attempt_200806252017_0003_r_000000_0:     at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
    [junit] attempt_200806252017_0003_r_000000_0:     at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
    [junit] attempt_200806252017_0003_r_000000_0:     at java.io.BufferedInputStream.read(BufferedInputStream.java:313)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:681)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:626)
    [junit] attempt_200806252017_0003_r_000000_0:     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:957)
    [junit] attempt_200806252017_0003_r_000000_0:     ... 4 more
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:29:20,154 INFO  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1564)) - Task attempt_200806252017_0003_r_000000_0: Failed fetch #2 from attempt_200806252017_0003_m_000002_0
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:29:20,154 INFO  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1575)) - Failed to fetch map-output from attempt_200806252017_0003_m_000002_0 even after MAX_FETCH_RETRIES_PER_MAP retries...  reporting to the JobTracker
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:29:20,154 WARN  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1636)) - attempt_200806252017_0003_r_000000_0 adding host localhost to penalty box, next contact in 8 seconds
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:29:21,201 INFO  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1427)) - attempt_200806252017_0003_r_000000_0: Got 5 map-outputs from previous failures
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:29:31,216 INFO  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1486)) - attempt_200806252017_0003_r_000000_0 Scheduled 1 of 5 known outputs (0 slow hosts and 4 dup hosts)
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:30:08,482 INFO  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1390)) - attempt_200806252017_0003_r_000000_0 Need another 20 map output(s) where 1 is already in progress
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:30:09,451 INFO  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1413)) - attempt_200806252017_0003_r_000000_0: Got 0 new map-outputs & number of known map outputs is 4
    [junit] attempt_200806252017_0003_r_000000_0: 2008-06-25 20:30:09,451 INFO  mapred.ReduceTask (ReduceTask.java:fetchOutputs(1486)) - attempt_200806252017_0003_r_000000_0 Scheduled 0 of 4 known outputs (0 slow hosts and 4 dup hosts)
    [junit] 2008-06-25 20:31:00,779 INFO  mapred.JobClient (JobClient.java:runJob(1037)) - Task Id : attempt_200806252017_0003_m_000002_0, Status : KILLED

"
HADOOP-3649,ArrayIndexOutOfBounds in FSNamesystem.getBlockLocationsInternal,"A job-submission failed with:

{noformat}
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: 2
  at org.apache.hadoop.dfs.FSNamesystem.getBlockLocationsInternal(FSNamesystem.java:772)
  at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:709)
  at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:685)
  at org.apache.hadoop.dfs.NameNode.getBlockLocations(NameNode.java:257)
  at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)

  at org.apache.hadoop.ipc.Client.call(Client.java:707)
  at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
  at org.apache.hadoop.dfs.$Proxy0.getBlockLocations(Unknown Source)
  at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
  at org.apache.hadoop.dfs.$Proxy0.getBlockLocations(Unknown Source)
  at org.apache.hadoop.dfs.DFSClient.callGetBlockLocations(DFSClient.java:299)
  at org.apache.hadoop.dfs.DFSClient.getBlockLocations(DFSClient.java:320)
  at org.apache.hadoop.dfs.DistributedFileSystem.getFileBlockLocations(DistributedFileSystem.java:122)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:241)
  at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:686)
  at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:966)
  at org.apache.hadoop.mapred.SortValidator$RecordStatsChecker.checkRecords(SortValidator.java:360)
  at org.apache.hadoop.mapred.SortValidator.run(SortValidator.java:559)
  at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
  at org.apache.hadoop.mapred.SortValidator.main(SortValidator.java:574)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
  at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
  at org.apache.hadoop.test.AllTestDriver.main(AllTestDriver.java:79)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
{noformat}"
HADOOP-3647,Corner-case in IFile leads to failed tasks,"A couple of reduce tasks failed at IFile.Reader.next, one with:

{noformat}
java.lang.NegativeArraySizeException
   at org.apache.hadoop.mapred.IFile$Reader.readNextBlock(IFile.java:246)
   at org.apache.hadoop.mapred.IFile$Reader.next(IFile.java:298)
   at org.apache.hadoop.mapred.Merger$Segment.next(Merger.java:134)
   at org.apache.hadoop.mapred.Merger$MergeQueue.adjustPriorityQueue(Merger.java:225)
   at org.apache.hadoop.mapred.Merger$MergeQueue.next(Merger.java:242)
   at org.apache.hadoop.mapred.Task$ValuesIterator.readNextKey(Task.java:720)
   at org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:679)
   at org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.next(ReduceTask.java:225)
   at org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)
   at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:316)
   at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2157)
{noformat}

On a related note, another failed at:
{code:title=IFile.java:380}
      // Position for the next record
      long skipped = dataIn.skip(recordLength);
      if (skipped != recordLength) {
        throw new IOException(""Failed to skip past record of length: "" + 
                              recordLength);
      }
{code}
where recordLength was *-17*.
"
HADOOP-3646,Providing bzip2 as codec,"Hadoop recognizes gzip compressed input and automatically decompresses the data before providing it to the mapper. But Hadoop can not split a gzip stream due to the very nature of the gzip compression. Consequently one gzip stream (e.g a whole file) can go to only one mapper.  On the contrary Bzip2 compressed stream can be split across its block delimiters.

We are interested in extending Hadoop to support splittable bzip2 with a codec.  (https://issues.apache.org/jira/browse/HADOOP-1823  uses input reader to split the bzip2 files, which must be provided by the user and can handle FileInputFormat.  If a user wants to use some other input format or wants to do custom record handling, he must write a new input reader!)

We have a patch now that provides a basic bzip2 codec equivalent to the current gzip codec.  We are in the process of extending that to support splitting."
HADOOP-3645,MetricsTimeVaryingRate returns wrong value for metric_avg_time,MetricsTimeVaryingRate seems to return getPreviousIntervalNumOps for metric_avg_time. It should return getPreviousIntervalAverageTime instead.
HADOOP-3644,TestLocalJobControl test gets OutOfMemoryError on 64-bit Java,"The TestLocalJobControl unit test fails on 64-bit Java on Linux with an OutOfMemoryError. Here is the exact Java environment:

$ java -version
java version ""1.5.0_07""
Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_07-b03)
Java HotSpot(TM) 64-Bit Server VM (build 1.5.0_07-b03, mixed mode)

The test runs fine with 32-bit Java. The problem is likely that some of the data structures become bigger when using 64-bit pointers. As a fix, I've suggested simply increasing the memory available to JUnit."
HADOOP-3643,jobtasks.jsp when called for running tasks should ignore completed TIPs,"There is a mismatch between the number of running tasks shown as _running_ and actual _running_ tasks. The number shown as the hyperlink (i.e on jobdetails.jsp) is less than the one shown once clicked (i,e in jobtasks.jsp). In the framework, a TIP can be _complete_ as well as _running_. {{TaskInProgress.isComplete()}} checks for atleast one successful task attempt while {{TaskInProgress.isRunning()}} checks if there are running tasks. Both of which can be true when there are speculative tasks. Hence jobtasks.jsp should ignore completed tasks if _running_ tasks are to be shown."
HADOOP-3642,add a HadoopIOException that can be thrown in any method that has IOException on its signature,"I find myself having to throw IOExceptions a lot, and create new ones -but the classes signature varies from java1.5 to 1.6, and the base IOException is fairly meaningless. If Hadoop added a HadoopIOException, it could be thrown whenever hadoop's own code needed to create new IOExceptions, and possibly be differentiated in the catch() logic. 

The biggest disadvantage of doing this is that as IOException is built into the JVM, you can be sure that the far end will be able to deserialize it under RMI, without having the rest of hadoop on the classpath. This is not a feature of hadoop, so should not be an issue. For those of us who do use RMI, well, we'd better get our classpaths right."
HADOOP-3641,Exception while killing the JobTracker,"{code}
Exception in thread ""Thread-5"" java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$KeyIterator.next(TreeMap.java:1152)
        at org.apache.hadoop.dfs.DFSClient.close(DFSClient.java:212)
        at org.apache.hadoop.dfs.DistributedFileSystem.close(DistributedFileSystem.java:228)
        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:1379)
        at org.apache.hadoop.fs.FileSystem.closeAll(FileSystem.java:230)
        at org.apache.hadoop.fs.FileSystem$ClientFinalizer.run(FileSystem.java:215)
{code}
This is when I manually try to kill a JobTracker."
HADOOP-3640,NativeS3FsInputStream read() method for reading a single byte is incorrect,"From Albert Chern:

I think there may be a bug in the read() method of NativeS3InputStream, which looks like this:

{code}
public synchronized int read() throws IOException {
    int result = in.read();
    if (result > 0) {
        pos += result;
    }
    return result;
}
{code}

The return value of InputStream.read() should be the next byte in the range 0 to 255, or -1 if there are no more bytes.  So shouldn't this method look something like this?

{code}
public synchronized int read() throws IOException {
    int result = in.read();
    if (result > -1) {
        pos ++;
    }
    return result;
}
{code}"
HADOOP-3639,Exception when closing DFSClient while multiple files are open,"DFSClient.close uses an iterator for closing files which are still open. Closing a file removes that file from the list of open files, which in turn breaks the iterator."
HADOOP-3638,Cache the iFile index files in memory to reduce seeks during map output serving,The iFile index files can be cached in memory to reduce seeks during map output serving.
HADOOP-3635,Uncaught exception in DataBlockScanner,"I see bunch of datanodes stop verifying local blocks.

"".out"" showed 
{noformat}
-rw-r--r--  1 hdfs users 614 Jun 23 10:24 datanode.out

Exception in thread ""org.apache.hadoop.dfs.DataBlockScanner@aadc97"" java.lang.NumberFormatException: For input string: ""121195228080date=""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Long.parseLong(Long.java:412)
        at java.lang.Long.valueOf(Long.java:518)
        at org.apache.hadoop.dfs.DataBlockScanner$LogEntry.parseEntry(DataBlockScanner.java:351)
        at org.apache.hadoop.dfs.DataBlockScanner.assignInitialVerificationTimes(DataBlockScanner.java:481)
        at org.apache.hadoop.dfs.DataBlockScanner.run(DataBlockScanner.java:534)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Namenode log also showed 
{noformat}
2008-06-23 10:24:12,831 WARN org.apache.hadoop.dfs.DataBlockScanner: RuntimeException during DataBlockScanner.run() : java.lang.NumberFormatException: For input string: ""121195228080date=""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Long.parseLong(Long.java:412)
        at java.lang.Long.valueOf(Long.java:518)
        at org.apache.hadoop.dfs.DataBlockScanner$LogEntry.parseEntry(DataBlockScanner.java:351)
        at org.apache.hadoop.dfs.DataBlockScanner.assignInitialVerificationTimes(DataBlockScanner.java:481)
        at org.apache.hadoop.dfs.DataBlockScanner.run(DataBlockScanner.java:534)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Datanode was still up and running but no verification.
Jstack didn't show DataBlockScanner.
"
HADOOP-3633,Uncaught exception in DataXceiveServer,"Observed dfsclients timing out to some datanodes.
Datanode's  '.out' file had 
{noformat}
Exception in thread ""org.apache.hadoop.dfs.DataNode$DataXceiveServer@82d37"" java.lang.OutOfMemoryError: unable to create new native thread
  at java.lang.Thread.start0(Native Method)
  at java.lang.Thread.start(Thread.java:597)
  at org.apache.hadoop.dfs.DataNode$DataXceiveServer.run(DataNode.java:906)
  at java.lang.Thread.run(Thread.java:619)
{noformat}

Datanode was still running but not much activity besides verification.
Jstack showed no DataXceiveServer running."
HADOOP-3631,Transfer of image from secondary name node should not interrupt service,"The transfer of the new image prepared by the secondary name node can interfere with client services. Clients observe delays in completing RPCs. In general, administrative activities should not be observed by the clients. For large clusters, administrators are reluctant to run the secondary name node leading to excessive edit logs. (Excessive in the sense that if the cluster must be restarted, a long time is required to process the log.)

Maybe the new image does not have to be transfered; it could be fetched when needed.

Maybe the priority of the transfer task can be reduced so that the transfer is not observed.

Maybe a different transfer protocol is more appropriate.
"
HADOOP-3630,CompositeRecordReader: key and values can be in uninitialized state if files being joined have no records,"I am using org.apache.hadoop.mapred.join.CompositeInputFormat to do an outer-join across a number of SequenceFiles. This works fine in most circumstances, but I get NullPointerExceptions/uninitialized data (where Writable#readFields() has not been called) when some of the files being joined have no records in them.

"
HADOOP-3627,HDFS allows deletion of file while it is stil open,"This was a single node cluster, so my DFSClient was from same machine. In a terminal I was writing to a HDFS file, while on another terminal deleted the same file. Deletion succeeded, and the write client failed. If the write was still going on, then the next block commit would result in exception saying, the block does not belong to any file. If the write was about to close, then we get an exception in completing a file because getBlocks fails. 

Should we allow deletion of file? Even if we do, should the write fail?"
HADOOP-3626,map/reduce clients should not access the system dir directly,Map/Reduce's use of the system directory with clients creating directories and files directly is very insecure. We should change the job client to create a staging directory in the user's home directory on the same file system as the system directory. 
HADOOP-3624,CreateEditsLog could be improved to create tree directory structure,CreateEditsLog generates files in one directory. Processing an edits file with lot of files takes a lot of time while starting NameNode. It would be good to have the directory creation logic of NNThroughputBenchmark used in CreateEditsLog as well.
HADOOP-3623,LeaseManager needs refactoring.,"LeaseManager is a name-node class, which contains (after HADOOP-3310) static members and methods serving data-node code.
I think we should keep the code related to different components of hdfs separatly, especially in view of the upcoming changes in hdfs packaging HADOOP-2884.
The separation may be achieved by introducing a new class DatanodeLeaseManager and then moving all related methods into the new class."
HADOOP-3622,pig unit test is very slow after integration with hadoop 18 code.,"After integrating with hadoop 18 candidate one of the Pig unit tests is running very slow to the point that it gets killed due to time out.

The details of the problem are at http://issues.apache.org/jira/browse/PIG-253.

Arun was looking at this issue."
HADOOP-3620,Namenode should synchronously resolve a datanode's network location when the datanode registers,Release 0.18.0 removes the rpc timeout. So the namenode is ok to resolve a datanode's network location when the datanode registers. This could remove quite a lot of unnecessary code in both datanode and namenode to handle asynchronous network location resolution and avoid many potential bugs.
HADOOP-3617,Writes from map serialization include redundant checks for accounting space,"In BlockingBuffer::write, the collector verifies that there is sufficient space to hold metadata when it returns to MapOutputBuffer::collect. This check is redundant for serialized records that make more than one call to write."
HADOOP-3615,DatanodeProtocol.versionID should be 16L,DatanodeProtocol.versionID should be updated from 15L to 16L
HADOOP-3614,TestLeaseRecovery fails when run with assertions enabled.,"I used -ea jvm options to run the test, and it fails in FSDataset.findBlockFile(Block) on 
{code}
assert b.generationStamp == GenerationStamp.WILDCARD_STAMP;
{code}
Without asserts on the test passes."
HADOOP-3611,TestNetworkTopology could be cleaned up slightly,"The TestNetworkTopology test
(a) has some of its private static final modifiers in the wrong order
(b) in an assertEquals, has the static and dynamic values wrong, so the error message won't be correct"
HADOOP-3610,[HOD] HOD does not automatically create a cluster directory for the script option,"HADOOP-3483 fixed the allocate command to create a cluster directory if the directory does not exist. However, it missed doing this for the script command. As most of the customers use the script option this is a blocker for Hadoop 0.18."
HADOOP-3607,Link broken after src restructuring,There is a dead link in src/docs/src/documentation/content/xdocs/streaming.xml .
HADOOP-3606,Update streaming documentation,Streaming document needs to be updated with changes to 0.18
HADOOP-3605,Added an abort on unset AWS_ACCOUNT_ID to luanch-hadoop-master,"I wanted to test out playing with the ec2 contributed shell scripts to kick off a Hadoop EC2 cluster however I found that without a set AWS_ACCOUNT_ID the script would still continue to run but not properly set the authorization groups due to the lack of an AWS Account ID parameter.

I have attached a quick patch to launch-hadoop-master to double check the existence of AWS_ACCOUNT_ID and abort the script as early as possible and present a useful error message to the user.


Below is the pach in cut and paste


--- hadoop-0.17.0/src/contrib/ec2/bin/launch-hadoop-master	2008-05-15 16:20:14.000000000 +0900
+++ hadoop-0.17.0-modified/src/contrib/ec2/bin/launch-hadoop-master	2008-06-20 11:58:59.000000000 +0900
@@ -29,6 +29,12 @@
 bin=`cd ""$bin""; pwd`
 . ""$bin""/hadoop-ec2-env.sh
 
+if [ -z $AWS_ACCOUNT_ID ]; then
+  echo ""AWS_ACCOUNT_ID is not configured properly!  Please check""
+  echo ""AWS_ACCOUNT_ID: $AWS_ACCOUNT_ID""
+  exit -1
+fi
+
 echo ""Testing for existing master in group: $CLUSTER""
 MASTER_EC2_HOST=`ec2-describe-instances | awk '""RESERVATION"" == $1 && ""'$CLUSTER_MASTER'"" == $4, ""RESERVATION"" == $1 && ""'$CLUSTER_MASTER'"" != $4'`
 MASTER_EC2_HOST=`echo ""$MASTER_EC2_HOST"" | awk '""INSTANCE"" == $1 && ""running"" == $6 {print $4}'`
@@ -108,4 +114,4 @@
 ssh $SSH_OPTS ""root@$MASTER_EC2_HOST"" ""chmod 600 /root/.ssh/id_rsa""
 
 MASTER_IP=`dig +short $MASTER_EC2_HOST`
-echo ""Master is $MASTER_EC2_HOST, ip is $MASTER_IP, zone is $MASTER_EC2_ZONE.""
\ No newline at end of file
+echo ""Master is $MASTER_EC2_HOST, ip is $MASTER_IP, zone is $MASTER_EC2_ZONE.""
"
HADOOP-3604,Reduce stuck at shuffling phase,"I was running gridmix with Hadoop 0.18.
I set the map output compression to true.
Most of the jobs completed just fine.
Three jobs, however, got stuck.
Each has one reducer stuck at shuffling phase.
Here is the log:


2008-06-20 00:06:01,264 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=SHUFFLE, sessionId=
2008-06-20 00:06:01,415 INFO org.apache.hadoop.streaming.PipeMapRed: PipeMapRed exec [/bin/cat]
2008-06-20 00:06:01,463 INFO org.apache.hadoop.mapred.ReduceTask: ShuffleRamManager: MemoryLimit=134217728, MaxSingleShuffleLimit=33554432
2008-06-20 00:06:01,474 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library
2008-06-20 00:06:01,475 INFO org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2008-06-20 00:06:01,476 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,477 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,477 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,478 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,478 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,486 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,486 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,487 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,487 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,488 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,488 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,489 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,489 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,489 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,493 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,496 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,496 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,496 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,497 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,497 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor
2008-06-20 00:06:01,500 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0 Thread started: Thread for merging on-disk files
2008-06-20 00:06:01,500 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0 Thread waiting: Thread for merging on-disk files
2008-06-20 00:06:01,502 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0 Need another 270 map output(s) where 0 is already in progress
2008-06-20 00:06:01,503 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0 Thread started: Thread for merging in memory files
2008-06-20 00:06:01,503 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0: Got 0 new map-outputs & number of known map outputs is 0
2008-06-20 00:06:01,504 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-06-20 00:06:06,654 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0: Got 269 new map-outputs & number of known map outputs is 269
2008-06-20 00:06:06,656 INFO org.apache.hadoop.mapred.ReduceTask: attempt_200806192318_0450_r_000016_0 Scheduled 229 of 269 known outputs (0 slow hosts and 40 dup hosts)
2008-06-20 00:06:07,163 INFO org.apache.hadoop.mapred.ReduceTask: Shuffling 846183 bytes (210104 raw bytes) into RAM-FS from attempt_200806192318_0450_m_000089_0
2008-06-20 00:06:07,163 INFO org.apache.hadoop.mapred.ReduceTask: Shuffling 820890 bytes (204371 raw bytes) into RAM-FS from attempt_200806192318_0450_m_000083_0
2008-06-20 00:06:07,166 INFO org.apache.hadoop.mapred.ReduceTask: Shuffling 835672 bytes (208085 raw bytes) into RAM-FS from attempt_200806192318_0450_m_000122_0

"
HADOOP-3603,Setting spill threshold to 100% fails to detect spill for records,"If io.sort.record.percent is set to 1.0, the simultaneous collection and spill is disabled. However, if one exhausts the offset allocation before the serialization allocation, the limit will not be detected and the write will block forever."
HADOOP-3601,Hive as a contrib project,"Hive is a data warehouse built on top of flat files (stored primarily in HDFS). It includes:
- Data Organization into Tables with logical and hash partitioning
- A Metastore to store metadata about Tables/Partitions etc
- A SQL like query language over object data stored in Tables
- DDL commands to define and load external data into tables

Hive's query language is executed using Hadoop map-reduce as the execution engine. Queries can use either single stage or multi-stage map-reduce. Hive has a native format for tables - but can handle any data set (for example json/thrift/xml) using an IO library framework.

Hive uses Antlr for query parsing, Apache JEXL for expression evaluation and may use Apache Derby as an embedded database for MetaStore. Antlr has a BSD license and should be compatible with Apache license.

We are currently thinking of contributing to the 0.17 branch as a contrib project (since that is the version under which it will get tested internally) - but looking for advice on the best release path."
HADOOP-3599,"The new setCombineOnceOnly shouldn't take a JobConf, since it is a method on JobConf","The method:

{code}
@Deprecated
public void setCombineOnceOnly(JobConf conf, boolean value) {
  conf.setBoolean(""mapred.combine.once"", value);
}
{code}

shouldn't take a JobConf."
HADOOP-3598,Map-Reduce framework needlessly creates temporary _${taskid} directories for Maps,"The staging directory for task-outputs (i.e. ${mapred.out.dir}/_temporary/_${taskid}) should only be created when Maps produce output on HDFS, which usually isn't the case. This plays very badly with HDFS quotas and may lead to thousands of temp names in the FS namespace, there-by overhauling the quotas. IAC, it isn't good to needlessly create these directories."
HADOOP-3597,SortValidator always uses the default file system irrespective of the actual input,"In SortValidator, the underlying file system should be obtained from the given inputs (sortInput and sortOutput). It is currently assumed to be the default (HDFS) always. 

So, the following usage does not work

bin/hadoop jar hadoop-0.19.0-dev-test.jar testmapredsort -sortInput har:///user/jothipn/foo.har/user/jothipn/input -sortOutput  output

"
HADOOP-3595,Remove deprecated mapred.combine.once functionality,"HADOOP-3586 added deprecated, backwards compatible combiner semantics in support of users requiring them in 0.18. This should be removed in a future release."
HADOOP-3594,Guaranteeing that combiner is called at least once,"In 18, hadoop decides how many times to call combiner on both map and reduce sides. The possible number is between 0 and N. 

While having multiple invocations can be useful, not invoking combiner at all can have serious consequences for a range of functions called algebraic (http://classweb.gmu.edu/kersch/inft864/Readings/Shoshani/DataCube/DataCubeTechReport.pdf). The main properties of such functions is that the intermediate and final computations are different and that the first invokation transforms the data to a different form. A most common example of this is AVERAGE function. While it is possible to workaround this issue by annotating each tuple, it seems that it would be much easier and faster if hadoop always guaranteed at least a single invocation.
 
Not having this guarantee will break all sorts of existing combiners."
HADOOP-3593,Update MapRed tutorial,This issue is about updating the mapred tutorial with the changes from 0.17 to 0.18.
HADOOP-3592,org.apache.hadoop.fs.FileUtil.copy() will leak input streams if the destination can't be opened,"FileUtil.copy()  relies on IOUtils.copyBytes() to close the incoming streams, which it does. Normally.

But if dstFS.create() raises any kind of IOException, then the inputstream ""in"", which was created in the line above, will never get closed, and hence be leaked.

      InputStream in = srcFS.open(src);
      OutputStream out = dstFS.create(dst, overwrite);
      IOUtils.copyBytes(in, out, conf, true);

Some try/catch wrapper around the open operations could close the streams if any exception gets thrown at that point in the copy process."
HADOOP-3590,Null pointer exception in JobTracker when the task tracker is not yet resolved,"2008-06-17 13:29:56,711 INFO org.apache.hadoop.mapred.JobTracker: Got heartbeat from: xxx (initialContact: false acceptNewTasks: true) with responseId: 0
2008-06-17 13:29:56,711 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 123, call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@123, true, true, -1) from xxx error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
       at org.apache.hadoop.mapred.JobTracker.getParentNode(JobTracker.java:1327)
       at org.apache.hadoop.mapred.JobInProgress.findNewMapTask(JobInProgress.java:1142)
       at org.apache.hadoop.mapred.JobInProgress.obtainNewMapTask(JobInProgress.java:685)
       at org.apache.hadoop.mapred.JobTracker.getNewTaskForTaskTracker(JobTracker.java:1708)
       at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:1431)
       at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)
----
Looks like a corner case. This happens when the task tracker just joins the JT and asks for a task to run. In a case where the task tracker is not yet resolved, this can lead to null pointer exception. Remember that the task trackers are  added to separate queue and resolved by a separate thread i.e there is no forced resolution for task trackers. There is no side effect of this bug since the task tracker will try again and also the job runs to completion."
HADOOP-3588,Bug report for archives,"I tested archives. Here is the bug report:

Documentation:
Add examples to section 3 to illustrate the use of listing/cating files in an archive.
 
User interface:
Case 1:
$ hadoop archive -archiveName xx.har /conf
Index: 0, Size: 0

Case 2: archive does not occur in destination
$ hadoop archive -archiveName /dd/xx.har /conf /cc
08/06/17 22:36:35 INFO mapred.JobClient: Running job: job_200806172141_0005
08/06/17 22:36:36 INFO mapred.JobClient:  map 0% reduce 0%
08/06/17 22:36:39 INFO mapred.JobClient:  map 100% reduce 0%
08/06/17 22:36:46 INFO mapred.JobClient: Job complete: job_200806172141_0005
...
$ hadoop dfs -ls /dd
Found 1 items
drwxr-xr-x   - hairong supergroup          0 2008-06-17 22:36 /dd/xx.har
$ hadoop dfs -ls /cc
ls: Cannot access /cc: No such file or directory.

Case 3:  archive gets overwritten without any warning
$ hadoop dfs -ls /dd
Found 1 items
drwxr-xr-x   - hairong supergroup          0 2008-06-17 22:36 /dd/xx.har
$ hadoop archive -archiveName xx.har /conf /dd
08/06/17 22:43:46 INFO mapred.JobClient: Running job: job_200806172141_0006
08/06/17 22:43:47 INFO mapred.JobClient:  map 0% reduce 0%
08/06/17 22:43:49 INFO mapred.JobClient:  map 100% reduce 0%
08/06/17 22:43:56 INFO mapred.JobClient: Job complete: job_200806172141_0006
...
 $ hadoop dfs -ls /dd
Found 1 items
drwxr-xr-x   - hairong supergroup          0 2008-06-17 22:43 /dd/xx.har

Case 4: src & dst are the same - I think we should not allow this
$ hadoop archive -archiveName xx.har /conf /conf
08/06/17 22:52:16 INFO mapred.JobClient: Running job: job_200806172141_0008
08/06/17 22:52:17 INFO mapred.JobClient:  map 0% reduce 0%
08/06/17 22:52:22 INFO mapred.JobClient:  map 100% reduce 0%
08/06/17 22:52:30 INFO mapred.JobClient: Job complete: job_200806172141_0008
...
$ hadoop dfs -ls /conf
Found 3 items
-rw-r--r--   1 hairong supergroup       3889 2008-06-17 21:43 /conf/hadoop-site.xml
-rw-r--r--   1 hairong supergroup       2844 2008-06-17 21:43 /conf/log4j.properties
drwxr-xr-x   - hairong supergroup          0 2008-06-17 22:52 /conf/xx.har

Case 5: dst is a file - more informative message
$ hadoop dfs -ls /hadoop-site.xml
Found 1 items
-rw-r--r--   1 hairong supergroup       3889 2008-06-17 23:06 /hadoop-site.xml
$ hadoop archive -archiveName xx.har /conf /hadoop-site.xml
08/06/17 23:06:54 INFO mapred.JobClient: Running job: job_200806172141_0009
Job failed!

Functionality:
Case 5: error message is better to be ""op"" not allowed.
$ hadoop dfs -ls har:///test/xx.har/conf
Found 2 items
-rw-r--r--  10 hairong supergroup       3889 2008-06-17 23:24 /test/xx.har/conf/hadoop-site.xml
-rw-r--r--  10 hairong supergroup       2844 2008-06-17 23:24 /test/xx.har/conf/log4j.properties
$ hadoop dfs -ls har:///test/xx.har/conf/hadoop-site.xml
Found 1 items
-rw-r--r--  10 hairong supergroup       3889 2008-06-17 23:24 /test/xx.har/conf/hadoop-site.xml
$ hadoop dfs -rm har:///test/xx.har/conf/hadoop-site.xml
rm: Har: delete not implemented
$ hadoop dfs -mv har:///test/xx.har/conf/hadoop-site.xml har:///test/xx.har/conf/hadoop-default.xml
mv: Failed to rename har:/test/xx.har/conf/hadoop-site.xml to har:/test/xx.har/conf/hadoop-default.xml

Case 6: count is not implemented
$ hadoop dfs -count har:///test/xx.har/conf
Can not find listing for har:///test/xx.har/conf


"
HADOOP-3587,contrib/data_join needs unit tests,"To insure against future incompatibility, there should be unit tests for the contrib/data_join framework."
HADOOP-3586,keep combiner backward compatible with earlier versions of hadoop,"In hadoop 16 and earlier, the combiner was guaranteed to run once and only once for each map. In 17 this compatibility was slightly broken: the combiner does not run if a single <K,V> occupies the entire sort buffer. In 18, this is further changed to where the combiner can be called multiple times on both map and reduce sides.

This breaks Pig's current implementation of the combiner and it is not easy to fix in a short period of time.

We would like to ask that for a way for an application to ask for a backward compatible behavior for some period of time until it can adjust to the new behavior."
HADOOP-3585,Hardware Failure Monitoring in large clusters running Hadoop/HDFS,"At IBM we're interested in identifying hardware failures on large clusters running Hadoop/HDFS. We are working on a framework that will enable nodes to identify failures on their hardware using the Hadoop log, the system log and various OS hardware diagnosing utilities. The implementation details are not very clear, but you can see a draft of our design in the attached document. We are pretty interested in Hadoop and system logs from failed machines, so if you are in possession of such, you are very welcome to contribute them; they would be of great value for hardware failure diagnosing.



Some details about our design can be found in the attached document failmon.doc. More details will follow in a later post."
HADOOP-3581,Prevent memory intensive user tasks from taking down nodes,"Sometimes user Map/Reduce applications can get extremely memory intensive, maybe due to some inadvertent bugs in the user code, or the amount of data processed. When this happens, the user tasks start to interfere with the proper execution of other processes on the node, including other Hadoop daemons like the DataNode and TaskTracker. Thus, the node would become unusable for any Hadoop tasks. There should be a way to prevent such tasks from bringing down the node."
HADOOP-3580,Using a har file as input for the Sort example fails,"Using a har file as input for the Sort example fails

I ran the randomwriter example and then ran the archive on the output of randomwriter to create a new HAR file. I specified this har file as the input for sort program and it failed.

STEP1: bin/hadoop jar hadoop-0.18.0-dev-examples.jar randomwriter input
STEP2: bin/hadoop archive -archiveName foo.har /user/jothipn/input /user/jothipn
STEP3:  bin/hadoop jar hadoop-0.18.0-dev-examples.jar sort har:///user/jothipn/foo.har/user/jothipn/input output

java.lang.ArrayIndexOutOfBoundsException: 17
        at org.apache.hadoop.mapred.FileInputFormat.getBlockIndex(FileInputFormat.java:334)
        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:248)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:686)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:966)
        at org.apache.hadoop.examples.Sort.run(Sort.java:147)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.examples.Sort.main(Sort.java:158)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:53)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
"
HADOOP-3577,Tools to inject blocks into name node and simulated data nodes for testing,"This jira adds the following 2 features to src/test
1) it extends DataNodeCluster to allow blocks to be injected for simulted data nodes.
2) a new command, CreatedEditsLog which creates an NN image edits logs that has a transactions for new files. The edits log file can be copied to the NN's image.

Using the two in conjunction one can inject a set of blocks in a NN and in a set of Datanodes and use it for testing, for example a cluster of 3K DNs"
HADOOP-3576,hadoop dfs -mv throws NullPointerException,"hadoop dfs -mv command throws NullPointerException while moving a directory to its subdirecotry. In 0.17 version, such a move was not allowed. 

Consider the example
{noformat}
[lohit@ hadoop-core-trunk]$ ./bin/hadoop dfs -mv /a/b /a/b/c
mv: java.io.IOException: java.lang.NullPointerException
[lohit@ hadoop-core-trunk]$ 
{noformat}

After this, the namespace of /a/b is gone
{noformat}
[lohit@ hadoop-core-trunk]$ ./bin/hadoop dfs -lsr /a
[lohit@ hadoop-core-trunk]$ 
{noformat}

Restarting the namenode recovers this namespace and everything seems to be normal.
On the other hand, before restarting the namenode, if we delete the directory /a, it succeeds. 
{noformat}
[lohit@ hadoop-core-trunk]$ ./bin/hadoop dfs -rmr /a
Deleted hdfs://jamba.juice:8020/a
[lohit@ hadoop-core-trunk]$ 
{noformat}

But, restarting now, throws an exception on NameNode and NameNode wouldn't start.
{noformat}
2008-06-17 00:58:50,422 ERROR org.apache.hadoop.dfs.NameNode: java.lang.NullPointerException
  at org.apache.hadoop.dfs.FSNamesystem.changeLease(FSNamesystem.java:4339)
  at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:561)
  at org.apache.hadoop.dfs.FSImage.loadFSEdits(FSImage.java:846)
  at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:675)
  at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:289)
  at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:80)
  at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)
  at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:252)
  at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)
  at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)
  at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)
  at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:825)
  at org.apache.hadoop.dfs.NameNode.main(NameNode.java:834)
{noformat}

In hadoop 0.17, we never allowed such a move.
{noformat}
[lohit@ branch-0.17]$ ./bin/hadoop dfs -mv /a/b /a/b/c
mv: Failed to rename /a/b to /a/b/c
[lohit@ branch-0.17]$ 
{noformat}

This is the issue seen with HADOOP-3561. Opening this JIRA to fix the underlying problem while HADOOP-3561 could be committed. "
HADOOP-3575,clover target broken after src restructuring,"After HADOOP-2916, the clover target no longer instruments the code since src/java patch no longer exists in Hadoop."
HADOOP-3572,setQuotas usage interface has some minor bugs.,"bin/hadoop  dfsadmin -setQuota 5 /tmp/a

here /tmp/a is a file.

setQuota: java.io.FileNotFoundException: Directory does not exist: /tmp/a

The error message should not say filenotfoundexception but something like input should be directory."
HADOOP-3571,ArrayIndexOutOfBoundsException in BlocksMap$BlockInfo.setPrevious,"2008-06-16 17:31:13,565 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 52750, call commitBlockSynchronization(blk_-5603224380662485577_2034, 2034, 80499712, false, false, [Lorg.apache.hadoop.dfs.DatanodeID;@73bc22) from xx.xx.xx.xx:51581: error: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: -2
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: -2
        at org.apache.hadoop.dfs.BlocksMap$BlockInfo.setPrevious(BlocksMap.java:94)
        at org.apache.hadoop.dfs.BlocksMap$BlockInfo.listInsert(BlocksMap.java:212)
        at org.apache.hadoop.dfs.DatanodeDescriptor.addBlock(DatanodeDescriptor.java:173)
        at org.apache.hadoop.dfs.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:1734)
        at org.apache.hadoop.dfs.NameNode.commitBlockSynchronization(NameNode.java:385)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)
"
HADOOP-3570,Including user specified jar files in the client side classpath path in Hadoop 0.17 streaming,"Hadoop 0.17 streaming allows specifying multiple user specific jar files on the command line along with the hadoop-streaming.jar. These user specific jar files are not automatically  added on the client side class path when hadoop runs the streaming job

For example, in the following streaming job <user-app.jar> is not automatically included in client side path before executing the streaming job
 
hadoop  jar -libjars <user-app.jar> $HADOOP_HOME/hadoop-streaming.jar \
        -input </input-data-path> \
        -output </output data path> \
        -inputformat  <Input-format> \
        -outputformat <OutputFormat> \
        -mapper /bin/cat -reducer /bin/cat \
        -additionalconfspec  <xxxx> \
        -jobconf mapred.reduce.tasks=3\

Workaround:
Current work around is to set these user specific jar files in
HADOOP_CLASSPATH environment variable on the client side before running above example.

"
HADOOP-3569,KFS input stream read() returns 4 bytes instead of 1,The call to KFSInputStream:read() returns 4 bytes; it should return 1 byte (in conformance to java.io.inputstream read() signature).
HADOOP-3564,Sometime after successful  hod allocation datanode fails to come up with java.net.BindException for dfs.datanode.ipc.address,"From Jira: HADOOP-3283 which introduced new conf parameter dfs.datanode.ipc.address which defaults to 50020.
When static dfs of hadoop version 0.18.0 running and its conf is not having dfs.datanode.ipc.address specified, then datanode start with 50020 port for ipc, w
When we  use  hod allocate without using static dfs.datanode on some machine fails to come. On further investigation  it has been found sometimes when torque provides list nodes, that list also contain some static dfs node.
When hodring tries to start datanode on a machine where a static dfs datanode of hadoop 0.18.0 is running, hod's dynamic dfs datanode fails to come with exception -: java.net.BindException: Problem binding to /0.0.0.0:50020 : Address already in use
beacuse hod provides ports for dfs.datanode.address and dfs.datanode.http.address. 
 
"
HADOOP-3563,Seperate out datanode and namenode functionality of generation stamp upgrade process,"This is needed to refactor source code in appropriate packages, HADOOP-2885"
HADOOP-3561,"With trash enabled, 'hadoop fs -rmr .' still fully deletes the working dir","With trash enabled, it should be more difficult to fully delete the working dir:

{noformat}
bash$ bin/hadoop fs -lsr /
drwxr-xr-x   - username supergroup          0 2008-06-13 15:51 /user
drwxr-xr-x   - username supergroup          0 2008-06-13 15:52 /user/username
drwxr-xr-x   - username supergroup          0 2008-06-13 15:52 /user/username/.Trash
drwx------   - username supergroup          0 2008-06-13 15:52 /user/username/.Trash/Current
-rw-r--r--   1 username supergroup     232060 2008-06-13 15:52 /user/username/.Trash/Current/file.txt
pineapple-lm:~/work/18-commit chrisdo$ bin/hadoop fs -rmr .
rmr: Failed to move to trash: hdfs://localhost/user/username
bash$ bin/hadoop fs -lsr /
drwxr-xr-x   - username supergroup          0 2008-06-13 15:52 /user
bash$
{noformat}

In 0.17, the working dir remains."
HADOOP-3560,Archvies sometimes create empty part files.,Archvies creates a bunch of empty part files sometimes which are not necessary.
HADOOP-3559,test-libhdfs fails on linux,"test-libhdfs fails on linux

test-libhdfs:
    [mkdir] Created dir: /workspace/trunk/build/test/libhdfs
    [mkdir] Created dir: /workspace/trunk/build/test/libhdfs/logs
    [mkdir] Created dir: /workspace/trunk/build/test/libhdfs/dfs/name
     [exec] ./tests/test-libhdfs.sh	
     [exec] 08/06/13 03:25:15 INFO dfs.NameNode: STARTUP_MSG: 
     [exec] /************************************************************
     [exec] STARTUP_MSG: Starting NameNode
     [exec] STARTUP_MSG:   host = hudson/NNN.NNN.NNN.NNN
     [exec] STARTUP_MSG:   args = [-format]
     [exec] STARTUP_MSG:   version = 0.18.0
     [exec] STARTUP_MSG:   build = http://svn.apache.org/repos/asf/hadoop/core/trunk -r 667040; compiled by 'hudsonqa' on Fri Jun 13 03:24:28 UTC 2008
     [exec] ************************************************************/
     [exec] Re-format filesystem in ../../../build/test/libhdfs/dfs/name ? (Y or N) 08/06/13 03:25:15 INFO fs.FSNamesystem: fsOwner=hudsonqa,users
     [exec] 08/06/13 03:25:15 INFO fs.FSNamesystem: supergroup=supergroup
     [exec] 08/06/13 03:25:15 INFO fs.FSNamesystem: isPermissionEnabled=true
     [exec] 08/06/13 03:25:15 INFO dfs.FSNamesystemMetrics: Initializing FSNamesystemMeterics using context object:org.apache.hadoop.metrics.spi.NullContext
     [exec] 08/06/13 03:25:16 INFO fs.FSNamesystem: Registered FSNamesystemStatusMBean
     [exec] 08/06/13 03:25:16 INFO dfs.Storage: Image file of size 82 saved in 0 seconds.
     [exec] 08/06/13 03:25:16 INFO dfs.Storage: Storage directory ../../../build/test/libhdfs/dfs/name has been successfully formatted.
     [exec] 08/06/13 03:25:16 INFO dfs.NameNode: SHUTDOWN_MSG: 
     [exec] /************************************************************
     [exec] SHUTDOWN_MSG: Shutting down NameNode at hudson/NNN.NNN.NNN.NNN
     [exec] ************************************************************/
     [exec] starting namenode, logging to /workspace/trunk/build/test/libhdfs/logs/hadoop-hudsonqa-namenode-hudsonqa.out
     [exec] starting datanode, logging to /workspace/trunk/build/test/libhdfs/logs/hadoop-hudsonqa-datanode-hudsonqa.out
     [exec] CLASSPATH=/workspace/trunk/src/c++/libhdfs/tests/conf:/workspace/trunk/conf:/workspace/trunk/src/c++/libhdfs/tests/conf:/workspace/trunk/conf:/home/hudsonqa/tools/java/jdk1.5.0_11-32bit/lib/tools.jar:/workspace/trunk/build/classes:/workspace/trunk/build:/workspace/trunk/build/test/classes:/workspace/trunk/lib/commons-cli-2.0-SNAPSHOT.jar:/workspace/trunk/lib/commons-codec-1.3.jar:/workspace/trunk/lib/commons-httpclient-3.0.1.jar:/workspace/trunk/lib/commons-logging-1.0.4.jar:/workspace/trunk/lib/commons-logging-api-1.0.4.jar:/workspace/trunk/lib/commons-net-1.4.1.jar:/workspace/trunk/lib/jets3t-0.6.0.jar:/workspace/trunk/lib/jetty-5.1.4.jar:/workspace/trunk/lib/junit-3.8.1.jar:/workspace/trunk/lib/kfs-0.1.3.jar:/workspace/trunk/lib/log4j-1.2.13.jar:/workspace/trunk/lib/oro-2.0.8.jar:/workspace/trunk/lib/servlet-api.jar:/workspace/trunk/lib/slf4j-api-1.4.3.jar:/workspace/trunk/lib/slf4j-log4j12-1.4.3.jar:/workspace/trunk/lib/xmlenc-0.52.jar:/workspace/trunk/lib/jsp-2.0/*.jar LD_PRELOAD=/workspace/trunk/build/libhdfs/libhdfs.so /workspace/trunk/build/libhdfs/hdfs_test
     [exec] 08/06/13 03:25:22 WARN fs.FileSystem: ""localhost:23000"" is a deprecated filesystem name. Use ""hdfs://localhost:23000/"" instead.
     [exec] 08/06/13 03:25:24 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 0 time(s).
     [exec] 08/06/13 03:25:25 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 1 time(s).
     [exec] 08/06/13 03:25:26 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 2 time(s).
     [exec] 08/06/13 03:25:27 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 3 time(s).
     [exec] 08/06/13 03:25:28 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 4 time(s).
     [exec] 08/06/13 03:25:29 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 5 time(s).
     [exec] 08/06/13 03:25:30 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 6 time(s).
     [exec] 08/06/13 03:25:31 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 7 time(s).
     [exec] 08/06/13 03:25:32 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 8 time(s).
     [exec] 08/06/13 03:25:33 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:23000. Already tried 9 time(s).
     [exec] Exception in thread ""main"" java.io.IOException: Call failed on local exception
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:710)
     [exec] 	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
     [exec] 	at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source)
     [exec] 	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:319)
     [exec] 	at org.apache.hadoop.dfs.DFSClient.createRPCNamenode(DFSClient.java:103)
     [exec] 	at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:173)
     [exec] 	at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:67)
     [exec] 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1335)
     [exec] 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:56)
     [exec] 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1346)
     [exec] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:209)
     [exec] 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:114)
     [exec] Caused by: java.net.ConnectException: Connection refused
     [exec] 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
     [exec] 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:527)
     [exec] 	at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:100)
     [exec] 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:300)
     [exec] 	at org.apache.hadoop.ipc.Client$Connection.access$1700(Client.java:177)
     [exec] 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:781)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:696)
     [exec] 	... 11 more
     [exec] Call to org.apache.hadoop.fs.FileSystem::get failed!
     [exec] Oops! Failed to connect to hdfs!
     [exec] no datanode to stop
     [exec] no namenode to stop
     [exec] exiting with 255
     [exec] make: *** [test] Error 255"
HADOOP-3556,Substitute the synchronized code in MD5Hash to avoid lock contention. Use ThreadLocal instead.,"Currently the MD5Hash class uses a singleton instance of the MessageDigester. The access to this instance is synchronized, so MD5Hash has performance problems when used from several Threads. I propose to substitute the singleton instance by a TheadLocal instances cache. I will provide a patch.
"
HADOOP-3552,forrest doc for hadoop commands,Document hadoop command line options in forrest. HADOOP-2908 covered the DFS options. This jira is for commands other than the DFS Shell.
HADOOP-3549,meaningful errno values in libhdfs,"A comment near the top of hdfs.h says that ""All APIs set errno to meaningful values."" Unfortunately this is a big fat lie -- bigger and fatter still now that we have permissions in HDFS. If Hadoop throws an AccessControlException, then libhdfs sets errno to EINTERNAL, even though this is not an ""internal"" error in any sense of the word.

With the attached patches, allow libhdfs to derive a useful errno value when an exception is thrown. I've implemented handling for AccessControlException and a few others whose semantics seem obvious.

Tangential question: The signatures for invokeMethod() and constructNewObjectOfClass() in hdfsJniHelper.h disagreed with their comments. Was the ""exc"" argument of those functions removed in the past, or never implemented? I (re)instated it in my patch."
HADOOP-3548,The tools.jar is not included in the distribution,The build.xml doesn't copy all of the jars from the build dir into the release.
HADOOP-3547,Improve documentation about distributing native libraries via DistributedCache,An illustrative example (with a code snippet) for distributing/loading native libraries via DistributedCache would aid a lot.
HADOOP-3546,TaskTracker re-initialization gets stuck in cleaning up,"If TaskTracker gets reinit action, it is stuck in joining task cleanup thread. "
HADOOP-3545,"archive  is failing with ""Illegal Capacity"" error","I ran the randomwriter example and then tried to archive the directory created by the randomwriter. I seem to be getting an ""Illegal Capacity"" error. 
The following is the output when run on the cluster with 50 nodes. 
The number reported after the ""Illegal capacity""  is half the number of maps created by the random writer (here 48)

jothipn@mymachine ~/hadoop-0.18.0-dev $ bin/hadoop jar hadoop-0.18.0-dev-examples.jar randomwriter -Dtest.randomwriter.maps_per_host=1 input4

<<FOLLOWED BY>>

jothipn@mymachine ~/hadoop-0.18.0-dev $ bin/hadoop archive -archiveName foo.har /user/jothipn/input4 /user/jothipn
08/06/12 08:06:43 WARN fs.FileSystem: ""mymachine:54039"" is a deprecated filesystem name. Use ""hdfs://mymachine:54039/"" instead.
08/06/12 08:06:43 WARN fs.FileSystem: ""mymachine:54039"" is a deprecated filesystem name. Use ""hdfs://mymachine:54039/"" instead.
08/06/12 08:06:43 WARN fs.FileSystem: ""mymachine:54039"" is a deprecated filesystem name. Use ""hdfs://mymachine:54039/"" instead.
Illegal Capacity: -24
"
HADOOP-3544,"The command ""archive"" is missing in the example in  docs/hadoop_archives.html (and pdf)","currently, the example reads

hadoop -archiveName foo.har /user/hadoop/dir1 /user/hadoop/dir2 /user/zoo/

it should be 

hadoop archive -archiveName foo.har /user/hadoop/dir1 /user/hadoop/dir2 /user/zoo/"
HADOOP-3543,Need to increment the year field for the copyright notice,Generated documentation is still using 2007.
HADOOP-3542,Hadoop archives should not create _logs file in the final archive directory.,the _logs files directory in the hadoop archives destination should not be created since it confuses the users.
HADOOP-3541,Namespace recovery from the secondary image should be documented.,"HADOOP-2585 introduced a new feature which lets starting name-node from an image stored on the secondary node.
This need to be documented in ""HDFS Admin Guide""."
HADOOP-3539,Cygwin: cygpath displays an error message in running bin/hadoop script,"- When running ./bin/hadoop in cygwin, we see an error message from cypath.
For example,
{noformat}
bash-3.2$ ./bin/hadoop namenode -format
cygpath: cannot create short name of :\cygdrive\d\@sze\hadoop\latest\hadoop-*-tools.jar:\cygdrive\d\@sze\hadoop\latest\build\hadoop-*-tools.jar
...
{noformat}

- The output message is confusing for incorrect commands.
For example,
{noformat}
bash-3.2$ ./bin/hadoop foo
cygpath: cannot create short name of :\cygdrive\d\@sze\hadoop\latest\hadoop-*-tools.jar:\cygdrive\d\@sze\hadoop\latest\build\hadoop-*-tools.jar
java.lang.NoClassDefFoundError: foo
Exception in thread ""main"" bash-3.2$ 
{noformat}
"
HADOOP-3537,Datanode not starting up with  java.lang.StringIndexOutOfBoundsException in NetworkTopology.remove,"When we brought up one of our clusters, couple of datanodes failed to start up with this error messages on the log.

2008-06-11 17:25:06,418 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=DataNode, sessionId=null
2008-06-11 17:26:06,748 INFO org.apache.hadoop.dfs.DataNode: Problem connecting to server: namenode/11.111.11.111:8020
2008-06-11 17:27:07,763 INFO org.apache.hadoop.dfs.DataNode: Problem connecting to server: namenode/11.111.11.111:8020
2008-06-11 17:28:08,778 INFO org.apache.hadoop.dfs.DataNode: Problem connecting to server: namenode/11.111.11.111:8020
2008-06-11 17:29:09,793 INFO org.apache.hadoop.dfs.DataNode: Problem connecting to server: namenode/11.111.11.111:8020
2008-06-11 17:29:43,235 ERROR org.apache.hadoop.dfs.DataNode: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
  at java.lang.String.substring(String.java:1938)
  at java.lang.String.substring(String.java:1905)
  at org.apache.hadoop.net.NetworkTopology$InnerNode.getNextAncestorName(NetworkTopology.java:119)
  at org.apache.hadoop.net.NetworkTopology$InnerNode.remove(NetworkTopology.java:201)
  at org.apache.hadoop.net.NetworkTopology.remove(NetworkTopology.java:354)
  at org.apache.hadoop.dfs.FSNamesystem.registerDatanode(FSNamesystem.java:2071)
  at org.apache.hadoop.dfs.NameNode.register(NameNode.java:536)
  at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

  at org.apache.hadoop.ipc.Client.call(Client.java:557)
  at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)
  at org.apache.hadoop.dfs.$Proxy4.register(Unknown Source)
  at org.apache.hadoop.dfs.DataNode.register(DataNode.java:456)
  at org.apache.hadoop.dfs.DataNode.runDatanodeDaemon(DataNode.java:2694)
  at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:2729)
  at org.apache.hadoop.dfs.DataNode.main(DataNode.java:2850)

"
HADOOP-3536,Support permissions in fuse-dfs,"Currently permissions are unsupported by fuse-dfs.

This manifests itself as two issues:
 * Users accessing a fuse-dfs mount do so as the user running fuse_dfs executable. In this case, it would be better to run fuse-dfr as some privileged user, and use Hadoop API calls determine whether the current user was privileged enough to perform the action.

 * Users cannot view/change permissions on the mounted volume. See  HADOOP-3264"
HADOOP-3535,IOUtils.close needs better documentation,The JavaDoc for IOUtils.close is incorrect and misleading.
HADOOP-3534,The namenode ignores ioexceptions in close,"The namenode node in FSNamesystem.close ignores and IOExceptions from closing the namespace.

{code}
    IOUtils.close(LOG, dir);
{code}

which only logs any exceptions in dir.close() at the debug level."
HADOOP-3533,The api to JobTracker and TaskTracker have changed incompatibly,"The JobTracker and TaskTracker classes are currently both public, and while working on HADOOP-3532, I discovered that their API's have been changed incompatibly. I believe the best course of action is to make their visibility package private.

Thoughts?"
HADOOP-3532,Create build targets to create api change reports using jdiff,"I'd like to add support for jdiff to our build scripts, to enable generating api difference reports between versions."
HADOOP-3531,Hod does not  report job tracker failure on hod client side when job tracker fails to come up,"Hod does not  report job tracker failure on hod client side when job tracker fails to come up. 
When max-master-failure > 1
hod client does not properly show why job tracker failed to come up, while in case namenode proper error message is displayed.
Also in namenode failure ringmaster log contains information such as -: ""Detected errors (3) beyond allowed number of failures (2). Flagging error to client""
while no such information is there in ringmaster log for job tracker failures"
HADOOP-3528,Metrics FilesCreated and files_deleted metrics do not match. ,"This JIRA is to fix the files created vs files deleted count. Looks like files created counts number of file created while files deleted counts number of inodes deleted. For now, it would be good to fix files created to match files deleted. "
HADOOP-3523,"[HOD] If a job does not exist in Torque's list of jobs, HOD allocate on previously allocated directory fails.","HADOOP-3483 addressed the issue where a dead cluster could be reallocated without having to issue warnings to users to clean up the directory themselves, provided the job is completed. It missed one case, where the job no longer exists in the Torque queue. When tried in that case, HOD fails with a bad error message:
ERROR - qstat error: exit code: 153 | signal: False | core False
CRITICAL - op: allocate hod-clusters/test 3 failed: <type 'exceptions.TypeError'> 'NoneType' object is unsubscriptable

This should be addressed to avoid user concerns.
"
HADOOP-3521,"Hadoop mapreduce task metrics, unable to send metrics data.","Found the following new metrics in tasktracker:

mapTaskSlots (short)
reduceTaskSlots (short)
tasks_failed_timeout (int)
tasks_failed_ping (int)

After configuring Simon to match the new metrics, this error was thrown in the task tracker log:

Exception in thread ""Timer thread for monitoring mapred"" java.lang.ClassCastException: java.lang.Long
        at com.yahoo.simon.hadoop.metrics.Client.sendBlurb(Client.java:266)
        at com.yahoo.simon.hadoop.metrics.SimonContext.emitRecord(SimonContext.java:160)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.emitRecords(AbstractMetricsContext.java:304)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:290)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:50)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:249)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)
"
HADOOP-3520,Generation stamp upgrade fails TestDFSUpgradeFromImage,"TestDFSUpgradeFromImage occasionally fails doing distributed upgrade related to generation stamps with the following NPE:
{code}
java.lang.NullPointerException
	at org.apache.hadoop.dfs.UpgradeManagerDatanode.initializeUpgrade(UpgradeManagerDatanode.java:54)
	at org.apache.hadoop.dfs.DataStorage.verifyDistributedUpgradeProgress(DataStorage.java:402)
	at org.apache.hadoop.dfs.DataStorage.doTransition(DataStorage.java:232)
	at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.java:139)
	at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:268)
	at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:185)
	at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:2952)
	at org.apache.hadoop.dfs.DataNode.instantiateDataNode(DataNode.java:2907)
	at org.apache.hadoop.dfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:401)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:267)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:162)
	at org.apache.hadoop.dfs.TestDFSUpgradeFromImage.testUpgradeFromImage(TestDFSUpgradeFromImage.java:180)
{code}
"
HADOOP-3519,NPE in DFS FileSystem rename,"I think this is a regression:
{noformat}
% bin/hadoop fs -mkdir /a/b
% bin/hadoop fs -mv /a/b /c/d
mv: java.io.IOException: java.lang.NullPointerException
{noformat}"
HADOOP-3517,The last InMemory merge may be missed,"This is post HADOOP-3366. The inmem merge thread has the loop:
{code}
        while (!exitInMemMerge) {
            ramManager.waitForDataToMerge();
            doInMemMerge();
          }
{code}
The fetchOutputs, at the end of copying everything, does the following:
{code}
        exitInMemMerge = true; 
        ramManager.close();
{code}
Now if the merge thread is doing a merge (inside the doInMemMerge method) when the exitInMemMerge is set to true, the loop will break and the last merge of the files that got shuffled recently will be skipped. ramManager.close(), that internally does a final notify to the merge thread also won't have any effect in this case."
HADOOP-3516,TestHarFileSystem.testArchives fails with NullPointerException,"TestHarFileSystem.testArchives fails with NPE:

{code}
java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobClient.getSystemDir(JobClient.java:1478)
	at org.apache.hadoop.tools.HadoopArchives.archive(HadoopArchives.java:315)
	at org.apache.hadoop.tools.HadoopArchives.run(HadoopArchives.java:653)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.fs.TestHarFileSystem.testArchives(TestHarFileSystem.java:131)
{code}"
HADOOP-3515,Protocol changes to allow appending to the last partial  crc chunk of a file,"To support ""appending"" to an existing file, we need the ability to append data to the last partial crc chunk of the file. 
"
HADOOP-3514,"Reduce seeks during shuffle, by inline crcs",The number of seeks can be reduced by half in the iFile if we move the crc into the iFile rather than having a separate file.
HADOOP-3513,Improve NNThroughputBenchmark log messages.,"The block report benchmark should not print return values of heartbeats.
It should also leave safe mode before creating files."
HADOOP-3512,Split map/reduce tools into separate jars,"Distcp, Logalyzer, and Archiver should be a separate tools jar, so that it can be overridden by the user."
HADOOP-3511,Namenode should not restore the root's quota if the quota was not in the image,"When namenode loads the image, it sets the root's quota to be an invalid value when the quota was not saved in the image."
HADOOP-3509,FSNamesystem.close() throws NullPointerException,"During NameNode startup, if there is an exception thrown before FSNamesystem.dir is created, we will get a NullPointerException.
{noformat}
2008-06-06 12:07:21,203 ERROR org.apache.hadoop.dfs.NameNode: java.lang.NullPointerException
	at org.apache.hadoop.dfs.FSNamesystem.close(FSNamesystem.java:465)
	at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:253)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:822)
	at org.apache.hadoop.dfs.NameNode.main(NameNode.java:831)
{noformat}"
HADOOP-3506,Occasional NPE in Jets3tFileSystemStore,"In extraordinary circumstances (eg. S3 outages), calling S3FileSystem functions will throw NullPointerExceptions when trying to read from the filesystem. I've traced this down to calls to Jets3tFileSystemStore.get().

Both get() functions catch an S3ServiceException, and check its error code using a string comparison. However, the underlying libs3t library will sometimes produce an exception with a null error code string. This results in an NPE that propagates all the way to the S3FileSystem, and to the caller.

To fix this, Jets3tFileSystemStore lines 196 and 212 should be changed from:
bq.         if (e.getS3ErrorCode().equals(""NoSuchKey"")) {
to:
bq.         if (""NoSuchKey"".equals(e.getS3ErrorCode())) {

"
HADOOP-3505,omissions in HOD documentation,"There's a couple HOD limitations that really trip up the unwary.
Two I've encountered are that  hod can't take relative paths on the command line, and that if you pass hod a tarball with a modified /conf, then the cluster will fail mysteriously to be initialized.

I don't see any references to either in the documentation, and it'd be great to write this down."
HADOOP-3504,Reduce task hangs after java.net.SocketTimeoutException,"When running gridmix, I saw 11 reduce tasks hanging. I manually failed the tasks and they re-ran and then finished.

Here is the task tracker logs:
syslog logs

al on-disk merge with 14 files
2008-06-05 19:02:49,804 INFO org.apache.hadoop.mapred.Merger: Merging 14 sorted segments
2008-06-05 19:03:03,663 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 14 segments left of total size: 1476315198 bytes
2008-06-05 19:03:03,731 WARN org.apache.hadoop.fs.FileSystem: ""hostname:56007"" is a deprecated filesystem name. Use ""hdfs://hostname:56007/"" instead.
2008-06-05 19:03:27,301 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=1/0/0 in:0=1/2626 [rec/s] out:0=0/2626 [rec/s]
2008-06-05 19:03:27,347 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=10/0/0 in:0=10/2626 [rec/s] out:0=0/2626 [rec/s]
2008-06-05 19:03:27,578 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=100/0/0 in:0=100/2627 [rec/s] out:0=0/2627 [rec/s]
2008-06-05 19:03:28,380 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=226/1
2008-06-05 19:03:35,276 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=1000/842/0 in:0=1000/2634 [rec/s] out:0=842/2634 [rec/s]
2008-06-05 19:03:38,667 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=2434/2274
2008-06-05 19:03:45,301 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=10000/9892/0 in:3=10000/2644 [rec/s] out:3=9892/2644 [rec/s]
2008-06-05 19:03:48,716 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=15057/14957
2008-06-05 19:03:59,056 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=24946/24887
2008-06-05 19:04:11,742 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=34653/34433
2008-06-05 19:04:22,548 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=42930/42803
2008-06-05 19:04:32,635 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=57737/57686
2008-06-05 19:04:42,662 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=76224/76063
2008-06-05 19:04:52,666 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=99423/99307
2008-06-05 19:04:52,802 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=100000/99795/0 in:36=100000/2712 [rec/s] out:36=99795/2712 [rec/s]
2008-06-05 19:05:02,754 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=127265/127145
2008-06-05 19:05:12,758 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=185310/185202
2008-06-05 19:05:15,858 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=200000/199974/0 in:73=200000/2735 [rec/s] out:73=199974/2735 [rec/s]
2008-06-05 19:05:22,772 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=218164/218082
2008-06-05 19:05:55,316 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=242591/242411
2008-06-05 19:06:13,678 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=242591/242412
2008-06-05 19:07:23,173 WARN org.apache.hadoop.dfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_-3463507617208131068_33273java.net.SocketTimeoutException: 69000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/NNN.NNN.NNN.125:59802 remote=/NNN.NNN.NNN.125:57834]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:162)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:150)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:123)
	at java.io.DataInputStream.readFully(DataInputStream.java:178)
	at java.io.DataInputStream.readLong(DataInputStream.java:399)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2045)

2008-06-05 19:11:22,535 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=243534/243374
2008-06-05 19:11:22,536 WARN org.apache.hadoop.dfs.DFSClient: Error Recovery for block blk_-3463507617208131068_33273 bad datanode[0] NNN.NNN.NNN.125:57834
2008-06-05 19:11:24,388 WARN org.apache.hadoop.dfs.DFSClient: Error Recovery for block blk_-3463507617208131068_33273 in pipeline NNN.NNN.NNN.125:57834, NNN.NNN.NNN.107:58706, NNN.NNN.NNN.122:52897: bad datanode NNN.NNN.NNN.125:57834
"
HADOOP-3503,Race condition when client and namenode start block recovery simultaneously,"When a client detects a error while writing to a block, it starts the generation-stamp-protocol to remove stale replicas. At the same time, if the namenode experiences a lease expiry event for that file, the namenode starts the generation-stamp-protocol for the same block. Now, the client and thr namenode ping-pongs trying to stamp the block replicas. This ping-pong can continue for a long time."
HADOOP-3502,Quota API needs documentation in Forrest,This issue aims to add quota documentation in Forrest.
HADOOP-3501,deprecate InMemoryFileSystem,"As of HADOOP-2095, InMemoryFileSystem is no longer used.  Its design was optimized for a particular application and it is thus not a good general-purpose RAM-based FileSystem implementation, so it ought to be removed."
HADOOP-3500,"decommission node is both in the ""Live Datanodes"" with ""In Service"" status, and in the ""Dead Datanodes"" of the dfs namenode web ui.","try to decommission a node by the following the steps:
(1) write the hostname of node which will be decommissioned in a file (the exclude file)
(2) specified the absolute path of the exclude file as a configuration parameter dfs.hosts.exclude.
(3) run ""bin/hadoop dfsadmin -refreshNodes"".

It is surprising that the node is found both in the ""Live Datanodes"" with ""In Service"" status, and in the ""Dead Datanodes"" of the dfs namenode web ui. When copy new data to the HDFS, its Used size is increasing as other un-decommissioned nodes. Obviously it is in service. Restarting the HDFS or waiting a long time(two day) havn't make the decommission yet.

the more strange thing, If nodes are configured as the include nodes by similar steps, then these include nodes and
the exclude node are all only in the ""Dead Datanodes"" lists. 

I did many times tests in both 0.17.0 and 0.15.1. The results is same. So i think there maybe bugs."
HADOOP-3499,"decommission node is both in the ""Live Datanodes"" with ""In Service"" status, and in the ""Dead Datanodes"" of the dfs namenode web ui.","try to decommission a node by the following the steps:
(1) write the hostname of node which will be decommissioned in a file (the exclude file)
(2) specified the absolute path of the exclude file as a configuration parameter dfs.hosts.exclude.
(3) run ""bin/hadoop dfsadmin -refreshNodes"".

It is surprising that the node is found both in the ""Live Datanodes"" with ""In Service"" status, and in the ""Dead Datanodes"" of 

the dfs namenode web ui. When copy new data to the HDFS, its Used size is increasing as other un-decommissioned nodes. 

Obviously it is in service. Restarting the HDFS or waiting a long time(two day) havn't make the decommission yet.

the more strange thing, If nodes are configured as the include nodes by similar steps, then these include nodes and
the exclude node are all only in the ""Dead Datanodes"" lists. 

I did many times tests in both 0.17.0 and 0.15.1. The results is same. So i think there maybe bugs."
HADOOP-3498,File globbing alternation should be able to span path components,"For example, {/a/b,/c/d} should expand to /a/b and /c/d. This change would also permit a consistent syntax for specifying multiple input paths to MapReduce, streaming and Pig by specification of a single glob path with alternation {/a/b,/c/d}, rather than a collection of comma separated glob paths /a/b,/c/d.

This change would also make globbing more consistent with bash, which supports this feature."
HADOOP-3497,File globbing with a PathFilter is too restrictive,"Consider the file hierarchy

{noformat}
/a
/a/b
{noformat}

Calling the globStatus method on FileSystem with a path of {noformat}/*/*{noformat} and a PathFilter that only accepts {{/a/b}} returns no matches. It should return a single match: {{/a/b}}.

"
HADOOP-3496,TestHarFileSystem.testArchives fails,"org.apache.hadoop.fs.TestHarFileSystem.testArchives fails on trunk with the following exception

java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:255)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:684)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:964)
	at org.apache.hadoop.fs.TestHarFileSystem.testArchives(TestHarFileSystem.java:184)"
HADOOP-3493,TestStreamingFailure fails.,"TestStreamingFailure fails with the below assertion failure
junit.framework.AssertionFailedError: Streaming Job Failure code expected expected:<5> but was:<4>
	at org.apache.hadoop.streaming.TestStreamingFailure.testCommandLine(TestStreamingFailure.java:73)"
HADOOP-3492,add forrest documentation for user archives,add forrest documentation for archives.
HADOOP-3491,Name-node shutdown causes InterruptedException in ResolutionMonitor,"ResolutionMonitor.run() catches a general Exception and logs them no matter what.
It should explicitly catch the InterruptedException and exit gracefully."
HADOOP-3490,inconsistent output using hadoop fs -dus when using globbing,"prompt> hadoop fs -dus  ""/foo/bar/sc*""  /foo/bar/sc0  ""/foo/bar/sc1""

hdfs://hadoopmc:8020/foo/bar/sc0   76748722272
hdfs://hadoopmc:8020/foo/bar/sc1  2353927217668
/foo/bar/sc0        76748722272
/foo/bar/sc1  2353927217668 

Some entries in the output are perpended with ""hdfs://"" while others are not. Globbing with dus results in inconsistent output. This behavior might break some scripts that are assuming consistency
"
HADOOP-3489,NPE in SafeModeMonitor,"If dfsadmin issues -safemode leave command when name-node is in the extended period of safe mode the SafeModeMonitor throws the following exception:
{code}
Exception in thread ""org.apache.hadoop.dfs.FSNamesystem$SafeModeMonitor@dc86eb"" java.lang.NullPointerException
	at org.apache.hadoop.dfs.FSNamesystem$SafeModeMonitor.run(FSNamesystem.java:3914)
	at java.lang.Thread.run(Thread.java:619)
{code}
The problem is that when SafeModeMonitor.run() wakes up it tries to check whether it canLeave() the safe mode, but the safe mode is off already,
and therefore the ""safemode"" field is null."
HADOOP-3488,"the rsync command in hadoop-daemon.sh also rsync the logs folder from the master, what deletes the datanode / tasktracker log files.",A stop and start deletes the log files on the datanodes.
HADOOP-3487,Balancer should not allocate a thread per block move,It will be more efficient to use a thread pool to provision thread for each block move.
HADOOP-3486,Change default for initial block report to 0 sec and document it in hadoop-defaults.xml,"HADOOP-2326 added a new config parameter for the initial block report that gets sent after DataNode registration.
The default value for this parameter was 60 sec.

This Jira changes the default to 0 sec -  the approriate default for 1 node or small clusters (say  less than 50 nodes)
This Jira also documents the config parameter in  hadoop-defaults.xml
"
HADOOP-3485,fix writes,"1. dfs_write should return the #of bytes written and not 0
2. implement dfs_flush
3. uncomment/fix dfs_create
4. fix the flags argument passed to libhdfs openFile to get around the bug in hadoop-3723
5. Since I am adding a write unit test, I noticed the unit tests are in the wrong directory - should be in contrib/fuse-dfs/src/test and not contrib/fuse-dfs/test"
HADOOP-3483,[HOD] Improvements with cluster directory handling,"The following improvements are asked for from users related to cluster directory handling:
- Create a new cluster directory if one does not exist.
- If a cluster directory points to a dead cluster, currently allocate fails with a message asking user to deallocate it first. Instead, it should issue a warning, deallocate the cluster and automatically allocate a fresh one."
HADOOP-3480,Need to update Eclipse template to reflect current trunk,"Since there are new libraries (e.g. src/test/lib) and new contrib package (e.g. src/contrib/fuse-dfs) added, the current Eclipse template does not work.

Also, it would be great if there is a README for the Eclipse template."
HADOOP-3479,Implement configuration items useful for Hadoop resource manager (v1),"HADOOP-3421 lists requirements for a new resource manager for Hadoop. Implementation for these will require support for new configuration items in Hadoop. This JIRA is to define such configuration, and track it's implementation."
HADOOP-3478,The algorithm to decide map re-execution on fetch failures can be improved,The algorithm to decide map re-execution on fetch failures can be improved.
HADOOP-3476,Code cleanup needed in fuse-dfs,"Some of the comments in fuse-dfs are not related to the codes.  For example, search ""facebook"" in src/contrib/fuse-dfs/configure.ac"
HADOOP-3471,TestIndexedSort sometimes fails,"After testing an array of equal values, TestIndexedSort::testAllEqual introduces a min and max value at random intervals:
{noformat}
    int diff = r.nextInt(SAMPLE);
    values[diff] = 9;
    values[(diff + r.nextInt(SAMPLE >>> 1)) % SAMPLE] = 11;
{noformat}

The max value can overwrite the min value and produce a spurious failure."
HADOOP-3468,Compile error: FTPFileSystem.java:26: cannot access org.apache.commons.net.ftp.FTP,"    [javac] Compiling 496 source files to d:\@sze\hadoop\latest\build\classes
    [javac] d:\@sze\hadoop\latest\src\java\org\apache\hadoop\fs\ftp\FTPFileSystem.java:26: cannot access org.apache.commons.net.ftp.FTP
    [javac] bad class file: d:\@sze\hadoop\latest\lib\commons-net-1.4.1.jar(org/apache/commons/net/ftp/FTP.class)
    [javac] class file has wrong version 50.0, should be 49.0
    [javac] Please remove or make sure it appears in the correct subdirectory of the classpath.
    [javac] import org.apache.commons.net.ftp.FTP;
    [javac]                                   ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 1 error

"
HADOOP-3467,The javadoc for FileSystem.deleteOnExit should have more description,"- The comment for processDeleteOnExit() should be converted to javadoc
- The javadoc for deleteOnExit(Path f) should describe how it related to JVM termination.
"
HADOOP-3465,org.apache.hadoop.streaming.StreamXmlRecordReader,"I downloaded and installed the 0.17.0 version this morning.

I'm trying to use the StreamXmlRecordReader to parse a file that is formatted like this:

<results>
<row>
<FIELD1>value</FIELD1>
.....  many fields.
</row>
</results>


Each logical row has about 1,371 characters in it.

I have the following settings in my job.

 conf.set(""stream.recordreader.begin"", ""<row>"");
        conf.set(""stream.recordreader.end"", ""</row>"");
        
        conf.set(""stream.recordreader.maxrec"", ""500000"");


When I run my tests, the TaskTracker shows me a severely truncated row like this:

Processing record=<row>
                <FIELD1><![CDATA[]]></FIELD2>
                <FIELD2><![CDATA[TL]]></FIELD2>
                <FIELD3><![CDATA[0003779]]></FIELD3>
                <FIELD4><![CDATA[ABCD]]></FIELD4>


I've tried setting the maxrec limits but even the default should be (as I read the code) more than big enough to handle ~1,371 characters from <row> to </row>.

And as you might expect, the XML parser in my Mapper task blows up because most of the <row> </row> is missing.


"
HADOOP-3464,[HOD] HOD can improve error messages by reporting failures on compute nodes back to hod client,"This issue addresses error messages w.r.t failures on compute nodes, while HADOOP-3151 addresses error messages in hod client."
HADOOP-3463,hadoop scripts don't change directory to hadoop_home,"The hadoop scripts don't change the current directory to one that should be local, which means that your home directory will likely be mounted across the cluster. It would be much better to cd in hadoop-daemon.sh to the hadoop home directory."
HADOOP-3462,reduce task failures during shuffling should not count against number of retry attempts,
HADOOP-3461,Remove dfs.StringBytesWritbale,"In dfs, some classes, such as INodeFileUnderConstruction use, StringBytesWritbale as member type.  The values are converted from String to StringBytesWritbale in setter and converted back to String in getter.  These overhead is unnecessary.

StringBytesWritbale is not used in RPC nor dfs image.  No version change will be introduced."
HADOOP-3460,SequenceFileAsBinaryOutputFormat,"Add an OutputFormat to write raw bytes as keys and values to a SequenceFile.

In C++-Pipes, we're using SequenceFileAsBinaryInputFormat to read Sequencefiles.
However, we current don't have a way to *write* a sequencefile efficiently without going through extra (de)serializations.

I'd like to store the correct classnames for key/values but use BytesWritable to write
(in order for the next java or pig code to be able to read this sequencefile).
"
HADOOP-3459,Change dfs -ls listing to closely match format on Linux,Change the dfs -ls command output to closely match the output format on Linux
HADOOP-3455,IPC.Client synchronisation looks weak,"Looking at HADOOP-3453 , its clear that Client.java is inconsistently synchronized

1. the running and shouldCloseConnection flags are not always read/written in synchronized blocks, even though they are properties used to share information between threads. They should be marked as volatile for access outside synchronized blocks, and all read-check-update operations must be synchronized.

2. there are multiple calls to System.currentTimeMillis() in synchronized blocks; this is a slow native operation and should ideally be done unsynchronized.

3. Synchronizing on the (out) stream is dangerous as its value changes during the life of the class, and sometimes it is null. These blocks should all synchronize on the Client instead.

4.  There are a number of places where InterruptedExceptions are caught and ignored in a sleep-wait loop:
     } catch (InterruptedException e) {
      }

   This isn't dangerous, but it does make the client harder to stop. These code fragments should be looked at carefully."
HADOOP-3454,Text.find incorrectly searches beyond the end of the buffer,"Text.find() does not pay attention to the length field.  So, this code:

{code}
    public void testTextFind()
    {
        Text t = new Text(""FIND AN I"");
        t.set(new byte[] { (byte) 'F' });

        assert t.getLength() == 1 : ""Length should be 1"";
        assert t.find( ""F"") == 0 : ""Found F at "" + t.find(""F"");
        assert t.find( ""I"") == -1 : ""Found I at "" + t.find(""I"");
    }
{code}

incorrectly throws an assertion because it finds the I at position 1, even though the Text is only one byte long.

I think to fix this it is enough to change this line in Text.find:

ByteBuffer src = ByteBuffer.wrap(this.bytes);

to 

ByteBuffer src = ByteBuffer.wrap(this.bytes,0,this.length);"
HADOOP-3453,ipc.Client.close() throws NullPointerException,"There are two possible cases that Client.close() throws NullPointerException

- Exception in thread ""IPC Client (47) connection to localhost/127.0.0.1:3070 from tsz"" java.lang.NullPointerException
    at org.apache.hadoop.ipc.Client$Connection.close(Client.java:521)
    at org.apache.hadoop.ipc.Client$Connection.run(Client.java:434)

- Exception in thread ""Thread-2"" java.lang.NullPointerException
    at org.apache.hadoop.ipc.Client$Connection.close(Client.java:519)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:317)
    at org.apache.hadoop.ipc.Client$Connection.access$1700(Client.java:175)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:766)
    at org.apache.hadoop.ipc.Client.call(Client.java:680)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
    ..."
HADOOP-3452,fsck exit code would be better if non-zero when FS corrupt,fsck exit code would be better if behaved like other sys admin tools and exit code indicated healthy/corrupt or other  errors.
HADOOP-3451,test-libhdfs fails on Linux,"test-libhdfs fails on Linux

Console output:
     [exec] Opened /tmp/testfile.txt for writing successfully...
     [exec] Wrote 14 bytes
     [exec] Current position: 14
     [exec] Flushed /tmp/testfile.txt successfully!
     [exec] hdfsAvailable: 14
     [exec] Current position: 1
     [exec] Read following 13 bytes:
     [exec] ello, World!
     [exec] Read following 14 bytes:
     [exec] Hello, World!
     [exec] hdfsCopy(remote-local): Success!
     [exec] hdfsCopy(remote-remote): Success!
     [exec] hdfsMove(local-local): Success!
     [exec] hdfsMove(remote-local): Success!
     [exec] hdfsRename: Success!
     [exec] hdfsCopy(remote-remote): Success!
     [exec] hdfsCreateDirectory: Success!
     [exec] hdfsSetReplication: Success!
     [exec] hdfsGetWorkingDirectory: hdfs://localhost:23000/user/hadoopqa
     [exec] hdfsSetWorkingDirectory: Success!
     [exec] hdfsGetWorkingDirectory: /tmp
     [exec] hdfsGetDefaultBlockSize: 67108864
     [exec] hdfsGetCapacity: 1645537427456
     [exec] hdfsGetUsed: 24576
     [exec] hdfsGetPathInfo - SUCCESS!
     [exec] Name: /tmp, Type: D, Replication: 0, BlockSize: 0, Size: 0, LastMod: Tue May 27 03:27:35 2008
     [exec] Name: hdfs://localhost:23000/tmp/newdir, Type: D, Replication: 0, BlockSize: 0, Size: 0, LastMod: Tue May 27 03:27:35 2008
     [exec] Name: hdfs://localhost:23000/tmp/testfile.txt, Type: F, Replication: 2, BlockSize: 67108864, Size: 14, LastMod: Tue May 27 03:27:35 2008
     [exec] Name: hdfs://localhost:23000/tmp/testfile2.txt, Type: F, Replication: 1, BlockSize: 67108864, Size: 14, LastMod: Tue May 27 03:27:35 2008
     [exec] Exception in thread ""main"" java.lang.NoSuchMethodError: getFileCacheHints
     [exec] stopping datanode
     [exec] stopping namenode
     [exec] exiting with 1
     [exec] make: *** [test] Error 1"
HADOOP-3450,Add tests to Local Directory Allocator for asserting their URI-returning capability,"Original comment:

{quote}Local directory allocator returns a bare path, without a URI specifier.  This means that calling Path.getFileSystem will do the wrong thing with the returned path.   Should really stick a ""file://"" in front.

Also it's test cases need to be improved to make sure this class works fine.
{quote}

Only the latter needed to be done (see below for discussion)."
HADOOP-3449,problem with bytewritable tostring,"The BytesWritable toString method is not bidirectional. What it means is we cannot get back the bytes from the toString form of bytesWritable. Here is the snippet

public String toString() { 
    StringBuffer sb = new StringBuffer(3*size);
    for (int idx = 0; idx < size; idx++) {
      // if not the first, put a blank separator in
      if (idx != 0) {
        sb.append(' ');
      }
     *String num = Integer.toHexString((int) bytes[idx]);*
      // if it is only one digit, add a leading 0.
      if (num.length() < 2) {
        sb.append('0');
      }
      sb.append(num);
    }
    return sb.toString();
  }


This is not the correct way to convert byte to string. This works well for positive numbers but fails for negative numbers

String num = Integer.toHexString((int) bytes[idx] && 0XFF);
http://forum.java.sun.com/thread.jspa?threadID=252591&messageID=2272668


"
HADOOP-3448,Add some more hints of the problem when datanode and namenode don't match,"When there is a mismatch between name and data mode, and you are running with -ea set, then Datanode.handshake() bails out with an assertion  ""Data-node and name-node layout versions must be the same."";

However, this message doesnt actually say which version numbers are at fault. A better error message would include the version information, so pointing the finger of blame would be easier."
HADOOP-3447,JobTracker should only keep runnable jobs in the priority list,"Currently the JobTracker keeps the _non-runnable jobs_ in the priority list. Non runnable jobs are deleted only when they are *too* old (by default if they have finished 24hrs ago). A newly submitted job gets added into this list and the whole list is sorted. Here the JobTracker does some useless computation by sorting all the non-runnable jobs too. Also the scheduling code scans the priority list *searching* for a _runnable_ job. We could either
1) have a separate list for _non running_ jobs
2) give runnable jobs higher priority over non-runnable ones.
----
_Note: Here, runnable jobs means jobs that are either running or queued._"
HADOOP-3446,The reduce task should not flush the in memory file system before starting the reducer,"In the case where the entire reduce inputs fit in ram, we currently force the input to disk and re-read it before giving it to the reducer. It would be much better if we merged from the ramfs and any spills to feed the reducer its input."
HADOOP-3445,Implementing core scheduler functionality in Resource Manager (V1) for Hadoop,"The architecture for the Hadoop Resource Manager (V1) is described in HADOOP-3444. This Jira proposes implementation details on the core scheduling piece - the changes to the JT to handle Orgs, queues, guaranteed capacities, user limits, and ultimately, scheduling a task on a TT. 

As per the architecture in HADOOP-3444, the JT contains a new component, Job Queue Manager (JQM), to handle queues of jobs. Each queue represents a queue in an Org (one queue per Org). Job queues are backed up by disk based storage. 

We now look at some details. Terminology: 
* A queue has *excess capacity* if it does not have enough jobs (queued or running) to take up its guaranteed capacity. Excess capacity needs to be distributed to queues that have none. 
* Queues that have given up excess capacity to other queues are called *low queues*, for the sake of this discussion. Queues that are running on additional capacity are called *high queues*.

For each queue, the JT keeps track of the following: 
* Guaranteed capacity (GC): the capacity guaranteed to the queue, set up through configuration. The sum of all GCs is equal to the grid capacity. Since we're handling Map and Reduce slots differently, we will have a GC for each, i.e., a CG-M for maps and a GC-R for reducers. The sum of all GC-Ms is equal to the sum of all map slots available in the Grid, and the same for GC-Rs. 
* Allocated capacity (AC): the current capacity of the queue. This can be higher or lower than the GC because of excess capacity distribution. The sum of all ACs is equal to the grid capacity. As above, each queue will have a AC-M and AC-R. 
* Timer for claiming containers: can just be the # of seconds the queue can wait till it needs its capacity back. There will be separate timers for claiming map and reduce slots (we will likely want to take more time to claim reduce slots, as reducers take longer to run). 
* # of containers being used, i.e., the number of running tasks associated with the queue (C-RUN).  Each queue will have a C-RUN-M and C-RUN-R. 
* Whether any jobs are queued. 
* The number of Map and Reduce containers used by each user. 

Every once in a while (this can be done periodically, or based on events), the JT looks at redistributing capacity. This can result in excess capacity being given to queues that need them, and capacity being claimed by queues. 

*Excess capacity is redistributed as follows*:
   * The algorithm below is in terms of tasks, which can be map or reduce tasks. It is the same for both. The JT will run the algorithm to redistribute excess capacity for both Maps and Reduces. 
   * The JT checks each queue to see if it has excess capacity. A queue has excess capacity if the number of running tasks associated with the queue is less than the allocated capacity of the queue (i.e., if C-RUN < AC) and there are no jobs queued. 
      ** Note: a tighter definition is if C-RUN plus the number of tasks required by the waiting jobs is less than AC, but we don't need that level of detail. 
   * If there is at least one queue with excess capacity, the total excess capacity is the sum of excess capacities of each queue. The JT figures out the queues that this capacity can be distributed to. These are queues that need capacity, where C-RUN = AC (i.e., the queue is running at max capacity) and there are queued jobs. 
   * The JT now figures out how much excess capacity to distribute to each queue that needs it. This can be done in many ways. 
      ** Distribute capacity in the ratio of each Org's guaranteed capacity. So if queues Q1, Q2, and Q3 have guaranteed capacities of GC1, GC2, and GC3, and if Q3 has N containers of excess capacity, Q1 gets (GC1*N)/(GC1+GC2) additional capacity, while Q2 gets (GC2*N)/(GC1+GC2). 
      ** You could use some other ratio that uses the number of waiting jobs. The more waiting jobs a queue has, the more its share of excess capacity.
   * For each queue that needs capacity, the JT increments its AC with the capacity it is allocated. At the same time, the JT appropriately decrements the AC of queues with excess capacity. 

*Excess capacity is reclaimed as follows*: 
* The algorithm below is in terms of tasks, which can be map or reduce tasks. It is the same for both. The JT will run the algorithm to reclaim excess capacity for both Maps and Reduces. 
* The JT determines which queues are low queues (if AC < GC). If a low queue has a job waiting, then we need to reclaim its resources. Capacity to be reclaimed = GC-AC. 
* Capacity is re-claimed from any of the high queues (where AC > GC). 
* JT decrements the AC of the high queue from which capacity is to be claimed, and increments the AC of the low queue. The decremented AC of the high queue cannot go below its GC, so the low queue may get its capacity back from more than one queue. 
* The JT also starts a timer for the low queue (this can be an actual timer, or just a count, perhaps representing seconds, which can be decremented by the JT periodically). 
* If a timer goes off, the JT needs to instruct some high queue to kill some of their tasks. How do we decide which high queues to claim capacity from? 
** The candidates are those high queues which are running more tasks than they should be, i.e., where C-RUN > AC. 
** Among these queues, the JT can pick those that are using the most excess capacity (i.e. queues with a higher value for (C-RUN - AC)/AC ). 
* How does a high queue decide which tasks to kill? 
** Ideally, you want to kill tasks that have started recently or made the least progress. You might want to use the same algorithm you use to decide which tasks to speculatively run (though that algorithm needs to be fixed). 
** Note: it is expensive to kill tasks, so we need to focus on getting better at deciding which tasks to kill. 

Within a queue, a user's limit can dynamically change depending on how many users have submitted jobs. This needs to be handled in a way similar to how we handle excess capacity between queues. 

*When a TT has a free Map slot*:
# TT contacts JT to give it a Map task to run. 
# JT figures out which queue to approach first (among all queues that have capacity, i.e., where C-RUN-M < AC-M). This can be done in a few ways:
      ** Round-robin, so every queue/Org has the same chance to get a free container. 
      ** JT can pick the queue with the maximum unused capacity. 
# JT needs to pick a job which can use the slot. 
      ** If it has no running jobs from that queue, it gets one from the JQM. 
         *** JT asks for the first Job in the selected queue, via the JQM. If the job's user's limit is maxed out, the job is returned to the queue and JT asks for the next job. This continues till the JT finds a suitable job. 
         *** Or else, JT has a list of users in the queue whose jobs it is running, and it can figure out which of these users are over their limit. It asks the JQM for the first job in the queue whose user is not in a list of maxed-out users it provides. 
      ** If the JT already has a list of running jobs from the queue, it looks at each (in order of priority) till it finds one whose user's limit has not been exceeded.
   # If there is no job in the queue that is eligible to run (the queue may have no queued jobs), the JT picks another queue using the same steps. 
   # The JT figures out which Map task from the job to run on the free TT using the same algorithm as today (find a  locality match using the job's cache, then look for failed tasks or tasks on the rack, etc). 
   # JT increments C-RUN-M and the number of Map containers used by the job's user. It then returns the task to the TT. 

*When a TT has a free Reduce slot*: This is similar to what happens with a free Map slot, except that: 
   * we can use a different algorithm to decide which Reduce task to run from a give job. I'm not sure what we do today for Reduce tasks (I think we just pick the first one), but if it needs to be improved, that's a separate issue.
   * Since there is no preemption of jobs based on priorities, we will not have the situation where a job's Reducers are blocking containers as they're waiting for Maps to run and there are no Map slots to run. 

*When a task fails or completes*: JT decrements C-RUN and the # of containers used by the user. 
"
HADOOP-3443,map outputs should not be renamed between partitions,"If a map finishes with out having to spill its data buffer, the map outputs are sorted and written to disk. However, no care is taken to make sure that the same partition is used to write it out before it is renamed. On nodes with multiple disks assigned to the task trackers, this will likely cause an addition read/write cycle to disk that is very expensive."
HADOOP-3441,Pass the size of the MapReduce input to JobInProgress,"Currently, there's no easy way for the JobInProgress to know how large the job's input data is.

This patch corrects the problem, by storing the size of the input split's data through the RawSplit.  The sizes of each split are then totaled up and made available via JobInProgress.getInputSize().  

This is needed, among other reasons, so that the JobInProgress knows how much data it's being run on, which will help build smarter schedulers."
HADOOP-3440,TaskRunner creates a symlink with name 'null' if a file is added to DistributedCache without fragment,"When adding a file to DistributedCache by  either :

1. DistributedCache.addCacheArchive(URI, conf) with URI not defining fragment(e.g. not as new URI(""path#fragment"") )
2. DistributedCache.addCacheFile(URI, conf)  with URI not defining fragment(e.g. not as new URI(""path#fragment"") )
3. DistributedCache.addArchiveToClassPath(Path, conf) 

and one sets DistributedCache.createSymlink, TaskRunner creates a link with name 'null' in the working directory. 

I think the behavior should be that if fragment is null then no sym link should be created, adding in localizeCache() of DistributedCache the following
{code}
     boolean doSymlink = getSymlink(conf);
+    if(cache.getFragment() == null) {
+      doSymlink = false;
+    }
FileSystem fs = getFileSystem(cache, conf);
{code}
     


"
HADOOP-3434,Retain cause of bind failure in Server.bind,"When a datanode can't start listening on a port, there's no explanation why; the message/stack is lost:

Caused by: java.net.BindException: Problem binding to localhost/127.0.0.1:8022
at org.apache.hadoop.ipc.Server.bind(Server.java:168)
at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:279)
at org.apache.hadoop.dfs.DataNode.(DataNode.java:185)
at org.apache.hadoop.dfs.ExtDataNode.(ExtDataNode.java:55)
at org.smartfrog.services.hadoop.components.datanode.DatanodeImpl.sfStart(DatanodeImpl.java:60)
"
HADOOP-3433,dfs.hosts.exclude not working as expected,"We had to decommission a lot of hosts.

Therefore, we added them to dfs.hosts.exclude and called 'dfsadmin -refreshNodes'.
The list of excluded nodes appeared in the list of dead nodes (and still remained the list of live nodes), but there was no replication going on for more than 20 hours (no NameSystem.addStoredBlock messages in the namenode log).

A few hours ago we stopped a datanode from that list. After it moved from the live node list to the dead node list (double entry), replication started immediately and completed after about 1 hour (replicated ~ 10,000 blocks).

Somehow, mere exclusion does not trigger replication as it should."
HADOOP-3431,hadoop streaming should kill all children processes on task failure,"The current hadoop streaming will leave the children processes running, when the task failed. We should kill the children processes.
"
HADOOP-3430,Implement 'ant test' as map/reduce task,It would be very helpful if we could make 'ant test' target to run as Map/Reduce program. It helps testing patches locally before submitting to hudson. 
HADOOP-3429,Increase the buffersize for the streaming parent java process's streams,"We saw improved performance when we increased the buffersize for Pipes (HADOOP-1788). In the streaming case, the buffersize is 8K (default for BufferedOutputStream). We should set that to 128k."
HADOOP-3427,"In ReduceTask::fetchOutputs, wait for result can be improved slightly","The getCopyResult call in ReduceTask::fetchOutputs waits for 2 seconds if the results list is empty. This can be improved to wait only when sufficient number of fetches (above a certain threshold) have been scheduled. The threshold should ensure that all the fetcher threads would be busy enough, and in this case, the call to getCopyResult would return only when a fetcher thread wakes it up."
HADOOP-3426,Datanode does not start up if the local machines DNS isnt working right and dfs.datanode.dns.interface==default,"This is the third Java project I've been involved in that doesnt work on my home network, due to implementation issues with  java.net.InetAddress.getLocalHost(), issues that only show up on an unamanged network. Fortunately my home network exists to find these problems early.

In hadoop, if the local hostname doesnt resolve, the datanode does not start up:

Caused by: java.net.UnknownHostException: k2: k2
at java.net.InetAddress.getLocalHost(InetAddress.java:1353)
at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:185)
at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:184)
at org.apache.hadoop.dfs.DataNode.(DataNode.java:162)
at org.apache.hadoop.dfs.ExtDataNode.(ExtDataNode.java:55)
at org.smartfrog.services.hadoop.components.datanode.DatanodeImpl.sfStart(DatanodeImpl.java:60)

While this is a valid option in a production (non-virtual) cluster, if you are playing with VMWare/Xen private networks or on a home network, you can't rely on DNS. 

1. In these situations, its usually better to fall back to using ""localhost"" or 127.0.0.1 as a hostname if Java can't work it out for itself,
2. Its often good to cache this if used in lots of parts of the system, otherwise the 30s timeouts can cause problems of their own.

"
HADOOP-3425,Partitioner Happilly accepts negative int number and data gets lost in Hadoop framework,"Using Partitioner, 

 If user passes negative partition number, framework happily accepts it. Data goes to wrong location and (many) reducers get zero data.  Suggested resolutions:

 1) Prevent the problem from start. partitioner checks the range and throws an exception if that' out of range.

 2) Have a more generic check: Compare counters to see if all data gets past Shuffle stage. No leak. Per feedback we got from Owen, this idea get a bit complicated when considering having combiners.

 Example:  using  my_id.hashCode() % numPartitions creates negative numbers and data gets lost in the framework. Reducers get zero rows ( while data is actually in  partitions index with negative numbers).
"
HADOOP-3424,the value returned from getPartition should be checked to make sure it is in the range 0..#reduces-1,"Since HADOOP-2919, map tasks haven't complained if getPartition returns numbers outside of the valid range. This leads to applications where some of the transient output is ignored (all of the rows marked for reduce -1, for example, are discarded, because reduce -1 doesn't exist.)"
HADOOP-3422,"Ganglia counter metrics are all reported with the metric name ""value"", so the counter values can not be seen","The JobInProgress class reports all metrics with the name ""value"". The FileMetrics class puts all of the tags into the name when reporting the individual values, but the Ganglia Context does not put the tags into the name..

This patch modifies the context to build names for the counter metrics out of the tag values. This enables the user to see the indivdual counter values with the ganglia web tool, on a per job basis"
HADOOP-3419,TestFsck fails once in a while on WINDOWS/LINUX,"Once in a while, TestFsck seemed to fail on Windows. After running the test in a loop for 25 times, I could see it fail 2 or so times even in LINUX. Looks like getBlockLocations was being called few places even before files created on dfs had been replicated. And some times, we corrupt block even before it had been replicated."
HADOOP-3418,"NameNode does not restart if parent directory of a ""FileUnderConstruction"" is deleted.","How to reproduce :

{{$ bin/hadoop fs -put largeFile tmp/tmpFile}}
...before this finishes
{{$ bin/hadoop fs -rmr tmp}}
Now restart NameNode.
Restart fails with :
{noformat}
2008-05-20 02:21:34,731 ERROR org.apache.hadoop.dfs.NameNode: java.io.IOException: saveLeases found path /user/rangadi/tmp/tmpFile but no matching entry in namespace.
        at org.apache.hadoop.dfs.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4215)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:848)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:866)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:82)
        at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:253)
        at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)
{noformat}"
HADOOP-3417,JobClient should not have a static configuration for cli parsing,"HADOOP-1622 introduced a static configuration variable into JobClient that prevents it from being used by multiple threads. Furthermore, the cli processing that HADOOP-1622 added should be in the GenericOptionParser instead of JobShell so that all of the implementations of Tool can use them."
HADOOP-3415,"JobEndNotifier isnt synchronized, doesnt check state before acting","
JobEndNotifier is pretty hazardous inside.

1. the static startNotifier isnt synchronized, and doesnt check for being already running before it creates a new worker thread. It should be sycnhronized and a no-op if there is a live thread.

2. stopNotifier() should be a no-op if already stopped. It MUST NOT call thread.interrupt() in such a state, as thread may be null. 

3. the registerNotification method also assumes that the static queue is non null.

Things would be a lot safer by making this class part of a JobTracker, not a singleton with static methods, as then you could more safely make assumptions about object state. This would not only eliminate a lot of reentrancy problems, but tie the life of the notifier to that of its owner, the JobTracker.
"
HADOOP-3413,SequenceFile.Reader doesn't use the Serialization framework,"Currently SequenceFile.Reader only works with Writables, since it doesn't use the new Serialization framework. This is a glaring considering that SequenceFile.Writer uses the Serializer and handles arbitrary types via the SerializationFactory."
HADOOP-3412,Refactor the scheduler out of the JobTracker,"First I would like warn you that my proposition is assumed to be very naive. I just hope that reading it won't make you lose time.

h4. The aim
It seems to me that improving Hadoop scheduling could be very profitable. But, it is hard to implement and compare schedulers, because the scheduling logic is mixed within the rest of the JobTracker.
This bug is the first step of an attempt to improve the Hadoop scheduler. It re-implements the current scheduling algorithm in a separate class called JobScheduler. This new class is instantiated in the JobTracker.

h4. Bug fixed as a side effects
This patch probably cannot be submited as it is.
A first difficulty is that it does not have exactly the same behaviour than the current JobTracker. More precisely, it doesn't re-implement things like code that seems to be never called or concurency problems.
I wrote TOCONFIRM where my proposition differ from the current implementation, so you can find them easily.
I know that fixing bugs silently is bad. So, independently of what you decide about this patch, I will open issues for bugs that you confirm.

h4. Other side effects
Another side effect of this patch is to add documentation about each step of the scheduling. I hope that it will help future improvement by lowering the level required to contribute to the scheduler.
It also reduces the complexity and the granularity of the JobTracker (making it more parallel).

h4. The future
If you feel that this is a step the right direction, I will try to propose a JobSchedulerInterface that many JobSchedulers could implement and to propose alternatives to the current « FifoJobScheduler ».  If some of you have ideas about that please tell ^^ I will also open issues for things marked as FIXME in the patch."
HADOOP-3411,TestDistributedUpgrade fails once in a while,"While running a patch build, there was a test failure for TestDistributedUpgrade. This does not fail always
Here is the link http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Patch/2492/testReport/org.apache.hadoop.dfs/TestDistributedUpgrade/testDistributedUpgrade/

and stack trace 
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.dfs.UpgradeManagerDatanode.initializeUpgrade(UpgradeManagerDatanode.java:54)
	at org.apache.hadoop.dfs.DataStorage.verifyDistributedUpgradeProgress(DataStorage.java:402)
	at org.apache.hadoop.dfs.DataStorage.doTransition(DataStorage.java:232)
	at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.java:139)
	at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:268)
	at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:185)
	at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:2910)
	at org.apache.hadoop.dfs.DataNode.instantiateDataNode(DataNode.java:2865)
	at org.apache.hadoop.dfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:397)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:263)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:158)
	at org.apache.hadoop.dfs.TestDistributedUpgrade.testDistributedUpgrade(TestDistributedUpgrade.java:112)
{noformat}"
HADOOP-3410,KFS implementation needs to return file modification time,"The KFS glue code in fs/kfs currently returns 0 for file modification time.  This needs to be fixed to return the correct value.  I will provide a patch:
 - jar file for kfs-0.1.3.jar; this should replace lib/kfs-0.1.jar
 - patch with the fix"
HADOOP-3409,NameNode should save the root inode into fsimage,"Currently namenode does not save the root inode into the image. Therefore the root's attributes like modification time, permission, and quota in the future are not persistent."
HADOOP-3408,Change FSNamesytem status metrics to IntValue,"HADOOP-3058 introduced new FSNamesystem status metrics. Most of them were LongValue, but there might be cases where LongValues are not properly  handled by metrics collectors. For now, we should change it to IntValue."
HADOOP-3406,Document controls for profiling maps & reduces,HADOOP-2367 and further improvements added the ability to profile map and reduce tasks. We should document these in http://hadoop.apache.org/core/docs/current/mapred_tutorial.html.
HADOOP-3405,Make mapred internal classes package-local,"In the work for HADOOP-544, we have realized that some of the public classes in the mapred package should have package-local scope. The classes which are public, but should not be, are : 
MapTaskStatus, ReduceTaskStatus, JobSubmissionProtocol, CompletedJobStatusStore
"
HADOOP-3404,JobHistory creates history files and doesn't close it when #maps & #reduces is 0,"A client application was submitting jobs every so often. After some time the JobTracker stopped accepting requests. On digging a bit, it looked like the history files for a bunch of jobs were not closed although the jobs were complete. Those jobs were with 0 maps and 0 reducers."
HADOOP-3403,Job tracker's ExpireTackers thread gets NullPointerException if a tasktracker is lost.,"JobTracker's ExpireTrackers gets NullPOinterException, if a tasktracker is lost.
JobTracker's log shows the following:

2008-05-16 04:48:08,170 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker 'tracker_XXXXX.com:XXXXX.com/98.136.100.125:58462'
2008-05-16 04:48:08,170 INFO org.apache.hadoop.mapred.TaskInProgress: Error from attempt_200805160431_0001_m_000001_0: Lost task tracker: tracker_XXXXX.com:XXXXX.com/98.136.100.125:58462
2008-05-16 04:48:08,171 ERROR org.apache.hadoop.mapred.JobTracker: Tracker Expiry Thread got exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobInProgress.failedTask(JobInProgress.java:1489)
	at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:537)
	at org.apache.hadoop.mapred.JobInProgress.failedTask(JobInProgress.java:1595)
	at org.apache.hadoop.mapred.JobTracker.lostTaskTracker(JobTracker.java:2079)
	at org.apache.hadoop.mapred.JobTracker$ExpireTrackers.run(JobTracker.java:301)
	at java.lang.Thread.run(Thread.java:619)
"
HADOOP-3402,Add example code to support run terasort on hadoop,"It would be good to be able to run the terasort (http://www.hpl.hp.com/hosted/sortbenchmark/) on Hadoop. I need to write:
 * distributed data generator
 * sorter
 * result checker"
HADOOP-3401,"Update FileBench to use the ""work"" directory for SequenceFileOutputFormat","HADOOP-3041 changed SequenceFileOutputFormat::getRecordReader to use a different property for its output path. This is set by a package-private, static function in FileOutputFormat. Since FileBench uses this OutputFormat directly and is not in o.a.h.mapred, it either needs to specify this parameter manually or setWorkOutputPath needs to be public."
HADOOP-3400,Facilitate creation of temporary files in HDFS,"There are a set of applications that use HDFS to create temporary files. The application would ideally like these files to be automatically deleted when the application process exits. This is similar to the File.deleteOnExit() in the Java API.

One proposal is to add a new method in FileSystem

public void deleteOnExit(Path)

    This API requests that the file or directory denoted by this abstract pathname be deleted when the virtual machine terminates. Deletion will be attempted only for normal termination of the virtual machine, as defined by the Java Language Specification. Once deletion has been requested, it is not possible to cancel the request. This method should therefore be used with care.

This method can be implemented entirely in the client side code, e.g. FileSystem.java will keep a cache of all the pathnames specified by the above API. FileSystem.close will invoke delete() on all the pathnames found in the cache. "
HADOOP-3399,Debug log not removed in ipc client,"I think a debug message got committed in HADOOP-2188.

{noformat}
$ bin/hadoop fs -ls
08/05/15 23:50:07 INFO ipc.Client: Build a connection to localhost:127.0.0.1:8020
{noformat}"
HADOOP-3398,ReduceTask::closestPowerOf2 is inefficient,"ReduceTask computes the ""closest power of 2"" using loops

{code}
private static int getClosestPowerOf2(int value) {
  int power = 0;
  int approx = 1;
  while (approx < value) {
    ++power;
    approx = (approx << 1);
  }
  if ((value - (approx >> 1)) < (approx - value)) {
    --power;
  }
  return power;
}
{code}

This could be improved."
HADOOP-3396,Unit test TestDatanodeBlockScanner fails on Windows,"Unit test fails on Windows: TestDatanodeBlockScanner

I see this assertion:
junit.framework.AssertionFailedError
	at org.apache.hadoop.dfs.TestDatanodeBlockScanner.testBlockCorruptionPolicy(TestDatanodeBlockScanner.java:201)

and this in the standard error:
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start...
Waiting for the Mini HDFS Cluster to start..."
HADOOP-3393,TestHDFSServerPorts fails on LINUX (NFS mounted directory) and on WINDOWS," HADOOP-2656  changes the way BlockScanner and BlockScannerThread are created. TestHDFSServerPorts tries to create a DataNode object, but expects a failure while starting it. As part of creating DataNode object we create BlockScanner object but not its corresponding BlockScannerThread as Daemon. While in shutdown() we check if BlockScannerThread is not null, then call BlockScanner.shutdown() to close a reference to LogFile it holds.

Now, on NFS mount and Windows, if a reference to file remains deleting the directory either leaves a .nfsXXXXX file or is not allowed (on windows). This was causing the test to fail. "
HADOOP-3391,HADOOP-3248 introduced a findbugs warning.,"HADOOP-3248 introduced a findbugs warning. It was committed without fixing the warning I think by mistake.
"
HADOOP-3390,Remove deprecated ClientProtocol.abandonFileInProgress(),ClientProtocol.abandonFileInProgress() was deprecated in 0.17 and is not used anywhere currently.
HADOOP-3388,TestDatanodeBlockScanner failed while trying to corrupt replicas,Blocks now have generation stamp associated with them. This unit test does a Block.toString() to find out the name of the block. Instead it should use Block.getBlockName(). 
HADOOP-3384,streaming process hang when no input to task + a large stderr debug output,"We've seen streaming task hang forever, (thus hang the job), when 
1) There's no input to a mapper or a reducer
2) Streaming process writes some debug statements.
    (Either flush() or a large output  to cause a flush)

PipeMapper/Reducer waits for the streaming process to finish.
Streaming process stuck on writing to stdout/stderr.

This is due to no MROutputThread/MRErrThread being created until first input record is passed to map() or reduce()."
HADOOP-3382,Memory leak when files are not cleanly closed,"{{FSNamesystem.internalReleaseCreate()}} in invoked on files that are open for writing but not cleanly closed. e.g. when client invokes {{abandonFileInProgress()}} or when lease expires. It deletes the last block if it has a length of zero. The block is deleted from the file INode but not from {{blocksMap}}. Then leaves a reference to such file until NameNode is restarted. When this happens  HADOOP-3381 multiplies amount of memory leak.
"
HADOOP-3381,INode interlinks can multiply effect of memory leaks,"Say a directory 'DIR' has a directory tree under it with 10000 files and directories. Each INode keeps refs to parent and children. When DIR is deleted, memory wise we essentially delete link from its parent (and delete the all the blocks from {{blocksMap}}). We don't modify its children. This is ok since this will form an island of references and will be gc-ed. Thats when everything is perfect. But if there is a bug that leaves a ref from a valid object (there is a suspect, I will another jira) to even one of these 10000 files, it could hold up all the INode and related objects. This can make a smaller mem leak many times more severe.


"
HADOOP-3379,"Document the ""stream.non.zero.exit.status.is.failure"" knob for streaming","HADOOP-2057 added a useful feature: ""stream.non.zero.exit.status.is.failure"" to optionally treat non-zero exit code from streaming apps as fatal. We should document this on http://hadoop.apache.org/core/docs/current/streaming.html."
HADOOP-3378,"Incorrect sort order if WritableComparable.compareTo() does not return -1, 0 or 1","I've found that incorrect sort orders are returned in some cases if the WritableComparable.compareTo() method doesn't return  -1, 0 or 1. 

I believe this is a bug as the compareTo() interface states that the returned int be only a -ve or +ve number, and will potentially catch a lot of people out who decide to write a WritableComparator (well it caught me out anyway!). 

I'll attach an example application to demonstrate -- I simply modified the sort example to specify a non-default comparator to sort LongWritable , i.e.:

public int compare(WritableComparable a, WritableComparable b)	{
  LongWritable longA =(LongWritable) a;
  LongWritable longB =(LongWritable) b;

  return (int) (longA.get() - longB.get());  // wrong sort order
  // return (int) Math.signum(longA.get() - longB.get());  // correct sort order
}


When I run the application through Hadoop on my input data it returns the incorrect sort order."
HADOOP-3377,Use StringUtils#replaceAll instead of ,"A minor cleanup. 
In the TaskRunner, we can read : ""When hadoop moves to JDK1.5, replace [TaskRunner.replaceAll] with String#replace""
This patch do so, removing ~30 lines."
HADOOP-3376,[HOD] HOD should have a way to detect and deal with clusters that violate/exceed resource manager limits,"Currently If we set up resource manager/scheduler limits on the jobs submitted, any HOD cluster that exceeds/violates these limits may 1) get blocked/queued indefinitely or 2) blocked till resources occupied by old clusters get freed. HOD should detect these scenarios and deal intelligently, instead of just waiting for a long time/ for ever. This means more and proper information to the submitter.

(Internal) Use Case:
     If there are no resource limits, users can flood the resource manager queue preventing other users from using the queue. To avoid this, we could have various types of limits setup in either resource manager or a scheduler - max node limit in torque(per job limit), maxproc limit in maui (per user/class), maxjob limit in maui(per user/class) etc. But there is one problem with the current setup - for e.g if we set up maxproc limit in maui to limit the aggregate number of nodes by any user over all jobs, 1) jobs get queued indefinitely if jobs exceed max limit and 2) blocked if it asks for nodes < max limit, but some of the resources are already used by jobs from the same user. This issue addresses how to deal with scenarios like these."
HADOOP-3375,Lease paths are sometimes not removed from LeaseManager.sortedLeasesByPath,"In LeaseManager,
- removeExpiredLease(Lease) does not remove paths from sortedLeasesByPath
- removeLease(...) should do sortedLeasesByPath.remove(src) no matter lease.hasPath() returns true or false."
HADOOP-3374,An ipc log message is printed in FsShell,"When a fs shell command is executed, an ipc log message ""Build a connection to hostname/123.123.123.123:9000"" is printed.  For example 
{noformat}
bash-3.2$ ./bin/hadoop fs -ls /
08/05/12 17:31:05 INFO ipc.Client: Build a connection to hostname/123.123.123.123:9000
     0           2008-05-12 17:29  drwxr-xr-x  tsz  supergroup  /user
{noformat}"
HADOOP-3372,TestUlimit fails on LINUX,"Looks like TestUlimit fails on Linux
{noformat}
Testcase: testCommandLine took 26.607 sec
  FAILED
output is wrong expected:<786432> but was:<Error occurred during initialization of VM
Could not reserve enough space for object heap>
junit.framework.ComparisonFailure: output is wrong expected:<786432> but was:<Error occurred during initialization of VM
Could not reserve enough space for object heap>
  at org.apache.hadoop.streaming.TestUlimit.runProgram(TestUlimit.java:120)
  at org.apache.hadoop.streaming.TestUlimit.testCommandLine(TestUlimit.java:90)

{noformat}"
HADOOP-3370,failed tasks may stay forever in TaskTracker.runningJobs,"The net effect of this is that, with a long-running TaskTracker, it takes long long time for ReduceTasks on that TaskTracker to fetch map outputs - TaskTracker does that for all reduce tasks in TaskTracker .runningJobs, including those stale ReduceTasks. There is a 5-second delay between 2 requests, which makes it a long time for a running reducetask to get the map output locations, when there are tens of stale ReduceTasks. Of course this also blows up the memory but that is not a too big problem at its rate.

I've verified the bug by adding an html table for TaskTracker.runningJobs on TaskTracker http interface, on a 2-node machine, with a single mapper single reducer job, in which mapper succeeds and reducer fails. I can still see the ReduceTask in TaskTracker.runningJobs, while it's not in the first 2 tables (TaskTracker.tasks and TaskTracker.runningTasks).


Details:

TaskRunner.run() will call TaskTracker.reportTaskFinished() when the task fails,
which calls TaskTracker.TaskInProgress.taskFinished,
which calls TaskTracker.TaskInProgress.cleanup(),
which calls TaskTracker.tasks.remove(taskId).

In short, it remove a failed task from TaskTracker.tasks, but not TaskTracker.runningJobs.

Then the failure is reported to JobTracker.

JobTracker.heartbeat will call processHeartbeat, 
which calls updateTaskStatuses, 
which calls tip.getJob().updateTaskStatus, 
which calls JobInProgress.failedTask,
which calls JobTracker.markCompletedTaskAttempt, 
which puts the task to trackerToMarkedTasksMap, 

and then JobTracker.heartbeat will call removeMarkedTasks,
which call removeTaskEntry, 
which removes it from trackerToTaskMap.

JobTracker.heartbeat will also call JobTracker.getTasksToKill,
which reads from trackerToTaskMap for <tracker, task> pairs,
and ask tracker to KILL the task or job of the task.

In the case there is only one task for a specific job on a specific tracker 
and that task failed (NOTE: and that task is not the last failed try of the
job - otherwise JobTracker.getTasksToKill will pick it up before 
removeMarkedTasks comes in and remove it from trackerToTaskMap), the task 
tracker will not receive the KILL task or KILL job message from the JobTracker.
As a result, the task will remain in TaskTracker.runningJobs forever.


Solution:
Remove the task from TaskTracker.runningJobs at the same time when we remove it from TaskTracker.tasks.
"
HADOOP-3369,Fast block processing during name-node startup.,"The block report processing during the startup period should be optimized.
As noted in HADOOP-3022 during cluster startup all blocks are under-replicated 
because they have not been reported by name-nodes yet.
Currently, we routinely move blocks to the neededReplications queue when they
are first reported and then remove them from the list when other nodes report it.
In ideal situation we end up adding all blocks into neededReplications queue first
only in order to remove all of them in the end. "
HADOOP-3368,Can commons-logging.properties be pulled from hadoop-core?,"In the root of hadoop-core.jar is a log4j.properties and a commons-logging.properties

while this provides good standalone functionality to hadoop, it complicates anyone else trying to control the logging, and use the libraries in-process.

In particular, there is a commons-logging.properties file that selects Log4J as the back end. This is not needed as
 -log4j is automatically picked up if it is on the classpath 
 -if it is not on the classpath, asking for it is generally considered bad form
If you look at the commons-logging configuration details:
 http://commons.apache.org/logging/guide.html#Configuration
you will see that that such a properties file takes priority over any setting through system properties, which makes it very hard to override the settings without adding multiple commons-logging.properties files and playing with their priority settings 

If you pull the commons-logging.properties file from hadoop-core log4j will still be picked up by default, but it becomes easier for people to turn on different logging infrastructures if they want to. It should have no visible impact on the end user experience (unlike pulling log4j.properties)"
HADOOP-3366,Shuffle/Merge improvements,This is intended to be a meta-issue to track various improvements to shuffle/merge in the reducer.
HADOOP-3365,SequenceFile.Sorter.MergeQueue.next does an unnecessary copy of the key,"SequenceFile.Sorter.MergeQueue.next does an unnecessary copy of the *rawKey*. We should remove that copy, and move code which adjusts the _heap_ from {{next()}} to {{getKey()}}, which should then just return {{ms.getKey()}}, thereby eliminating the extra copy."
HADOOP-3364,Faster image and log edits loading.,"This patch optimizes code to provide faster load of fsimage and edits log.
I implemented ideas mentioned in HADOOP-3022. Namely, removed unnecessary object allocations,
and implemented optimized loading which avoids unnecessary name-space tree lookups 
if consecutive files belong to the same parent.
I changed saveImage algorithm so that it writes first all children of the same directory,
and then goes inside of its sub-directories. This does not change the format of the image, 
just changes the order of the stored objects. 
This should make loading faster after the image is saved with the new version.
The advantages in performance are
load/save fsimage: 15-20%
load edits: 5-10%
In terms of performance I expected somewhat more from this changes.
Especially for edits, but it turned out that recent changes substantially slowed down
edits loading. ADD and CLOSE operations first remove existing file with all its blocks,
then include at back with potentially new blocks, and then ADD additionally replaces
just inserted inode by inode-under-construction for the same file.
This is very inefficient, but hard to fix.
I'll do it in a separate jira.
Other changes:
- I combined most of the UTF8 references in one place at least for FSImage.
- Included log messages about the startup progress with load/save times and file sizes.
- Removed pre-crc-upgrade code from FSEdits, which was missed by the crc-remove patch.
"
HADOOP-3363,HDFS throws a InconsistentFSStateException when the name node starts up on a directory that isnt formatted,"I deploy a namenode with directories that exist but are empty, an InconsistentFSStateException is thrown instead of the FS entering the unformatted state -the check for upgrade state fails when there is no file system there at all."
HADOOP-3361,Implement renames for NativeS3FileSystem,Amazon S3 now supports a copy object operation (http://docs.amazonwebservices.com/AmazonS3/2006-03-01/UsingCopyingObjects.html). We can use this to properly support renames in NativeS3FileSystem.
HADOOP-3358,The FileSystem operations in JobInProgress.garbageCollect should be moved out of the synchronized block,"In garbageCollect, the jobTracker deletes files in the filesystem but that is after locking the JobTracker/JobInProgress. This should be changed to move the filesystem operations outside the locks. I encountered HADOOP-3357 and that effectively locked up the jobTracker."
HADOOP-3355,Configuration should accept decimal and hexadecimal values,"This issue is created after the discussions in HADOOP-2461.
To summarise, Configuration should also accept hexadecimal values such as 0x1000, in addition to decimal values. Octal representation is not supported, since it may confuse some users."
HADOOP-3354,Test Failure on trunk with timeout error,"TestEditLog failed on trunk. Only suspecting exception from log is

{noformat}
    [junit] 2008-05-06 11:34:31,441 INFO  ipc.Server (Server.java:run(899)) - IPC Server handler 1 on 36391: exiting
    [junit] Exception in thread ""IPC Client (47) connection to localhost/127.0.0.1:36385 from an unknown user"" java.lang.NoClassDefFoundError: org/apache/hadoop/io/IOUtils
    [junit] 	at org.apache.hadoop.ipc.Client$Connection.close(Client.java:527)
    [junit] 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:430)
    [junit] 2008-05-06 11:34:31,444 INFO  ipc.Server (Server.java:run(499)) - Stopping IPC Server Responder
    [junit] 2008-05-06 11:34:31,550 INFO  dfs.DataBlockScanner (DataBlockScanner.java:run(567)) - Exiting DataBlockScanner thread.
    [junit] Running org.apache.hadoop.dfs.TestEditLog
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestEditLog FAILED (timeout)
{noformat}

Here is the console output (http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/482/console)

There are also these NoClassDefFoundError in many of the timeout test failures. "
HADOOP-3351,Fix history viewer ,"The following issues has to be fixed in history viewer:
1. The appends made to the StringBuffer for each iteration can cause out of memory error, these appends can be removed and a print can be called for each iteration.
2. Should printSpilts() for the job be made optional? like bin/hadoop job -history <output-dir> -splits.
or Instead of printing splits separately, they can be printed with task details.
"
HADOOP-3350,distcp should permit users to limit the number of maps,"distcp provides no user-configurable setting to affect the number of maps run during a copy. For large clusters, the current calculation- min(#bytes / 256MB, #nodes * 20)- can be overly aggressive."
HADOOP-3349,"FSNamesystem.changeLease(src, dst) incorrectly updates the paths inside a lease","Suppose we have a closed file /foo/a and a lease /foo/aa.  Calling rename(/foo/a, /foo/b) will have a side effect ""Modified Lease for file /foo/aa to new path /foo/ba""."
HADOOP-3348,TestUrlStreamHandler hangs on LINUX,"TestUrlStreamHandler sets setURLStreamHandlerFactory as
{noformat}
FsUrlStreamHandlerFactory factory =
        new org.apache.hadoop.fs.FsUrlStreamHandlerFactory();
    java.net.URL.setURLStreamHandlerFactory(factory);
{noformat}

After this, MiniDFSCluster seems to hang while Datanodes tries to register in setNewStorageID, specifically at
{noformat}
rand = SecureRandom.getInstance(""SHA1PRNG"").nextInt(Integer.MAX_VALUE);
{noformat}

jstack output shows that the main thread is stuck in RawLocalFileSystem$LocalFSFileInputStream.read

(Attaching the jstack)"
HADOOP-3345,Enhance the hudson-test-patch target,"Add eclipse and python testing to patch builds.
Clean up Jira message to put +1/-1 at the beginning of each line.
Fix some defects in the hudson-test-patch target."
HADOOP-3344,"libhdfs: always builds 32bit, even when x86_64 Java used","The makefile for libhdfs is hard-coded to compile 32bit libraries. It should perhaps compile dependent on which Java is set.

The relevant lines are:

LDFLAGS = -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm -shared -m32 -Wl,-x
CPPFLAGS = -m32 -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/$(PLATFORM)

$OS_ARCH can be (e.g.) amd64 if you're using a 64bit java on the x86_64 platform. So while gcc will try to link against the correct libjvm.so, it will fail because libhdfs is to be built 32bit (because of -m32)

{noformat}
     [exec] /usr/bin/ld: skipping incompatible /usr/java64/latest/jre/lib/amd64/server/libjvm.so when searching for -ljvm
     [exec] /usr/bin/ld: cannot find -ljvm
     [exec] collect2: ld returned 1 exit status
     [exec] make: *** [/root/def/hadoop-0.16.3/build/libhdfs/libhdfs.so.1] Error 1
{noformat}

The solution should be to specify -m32 or -m64 depending on the os.arch detected.

There are 3 cases to check:
 * 32bit OS, 32bit java => libhdfs should be built 32bit, specify -m32
 * 64bit OS, 32bit java => libhdfs should be built 32bit, specify -m32
 * 64bit OS, 64bit java => libhdfs should be built 64bit, specify -m64"
HADOOP-3342,Better safety of killing jobs via web interface,"Although the option to kill jobs via the web interface is turned off by default, it should be made safer. Currently the ""kill"" action and its confirmation is just a link so it could be triggered by a crawler or by a browser's pre-fetching mechanism. The attached patch makes it work only with ""POST"" so that e.g. well-behaved crawlers shouldn't be able to access it."
HADOOP-3341,make key-value separators in hadoop streaming fully configurable,"By default, hadoop streaming uses TAB as the separator in all places.  However in some environments, user may want to use customized separators (e.g, ^A = \u0001).

The separator logic in hadoop streaming is very convoluted. Here is a brief summary:

InputFormat {
    KeyValueLineRecordReader.java:59:
S1: String sepStr = job.get(""key.value.separator.in.input.line"", ""\t"");
}

Mapper {
    PipeMapper.java:88: 
S2: clientOut_.write('\t');

    Call mapper process

    PipeMapRed.java:124:
S3: String mapOutputFieldSeparator = job_.get(""stream.map.output.field.separator"", ""\t"");
    PipeMapRed.java:128:
    this.numOfMapOutputKeyFields = job_.getInt(""stream.num.map.output.key.fields"", 1);
}


Reducer {
    PipeReducer.java:78:
S4: clientOut_.write('\t');

    Call reducer process

    PipeMapRed.java:125:
S5: String reduceOutputFieldSeparator = job_.get(""stream.reduce.output.field.separator"", ""\t"");
    PipeMapRed.java:129:
    this.numOfReduceOutputKeyFields = job_.getInt(""stream.num.reduce.output.key.fields"", 1);
}

OutputFormat {
    TextOuputFormat.java:112:
S6: String keyValueSeparator = job.get(""mapred.textoutputformat.separator"", ""\t"");
}

Short-cuts: 
1. In case we use ""TextInputFormat"", S1 and S2 are not used at all. Lines are directly feed into the mapper (through the value part of the key-value pair - keys, which are offsets, are directly ignored).
2. For jobs with no reducers, The ""Reducer"" step is skipped.


We need to make S3 and S4 configurable, possibly under the following names for conformity:
stream.map.input.field.separator
stream.reduce.input.field.separator


Then, by specifying: -jobconf key.value.separator.in.input.line=^A -jobconf stream.map.input.field.separator=^A -jobconf stream.map.output.field.separator=^A -jobconf stream.reducer.input.field.separator=^A -jobconf stream.reducer.output.field.separator=^A -jobconf mapred.textoutputformat.separator=^A, we will be able to use ^A instead of TAB in every place!

Maybe hadoop streaming can also provide a single option to override these 6 options.
"
HADOOP-3340,hadoop dfs metrics shows 0,"Hadoop dfs Mbean has the following metrics showing as 0.

BlocksReplicated (unimplemented)

ReadMetadataOpNum
HeartbeatsNum
BlockReportsAverageTime"
HADOOP-3339,DFS Write pipeline does not detect defective datanode correctly if it times out.,"When DFSClient is writing to DFS, it does not correctly detect the culprit datanode (rather datanodes do not inform) properly if the bad node times out. Say, the last datanode in in 3 node pipeline is is too slow or defective. In this case, pipeline removes the first two datanodes in first two attempts. The third attempt has only the 3rd datanode in the pipeline and it will fail too. If the pipeline detects the bad 3rd node when the first failure occurs, the write will succeed in the second attempt. 

I will attach example logs of such cases. I think this should be fixed in 0.17.x.
"
HADOOP-3338,trunk doesn't compile after HADOOP-544 was committed,"For all the discussion about deprectatoin, etc, the patch for HADOOP-544 has broken trunk! It no longer compiles on Hudson. The contrib eclipse plugin relied on an API that you considered non-Public (JobStatus and perhaps others). Given we have no well defined public API yet, I don't think you can assume that classes declared ""public"" aren't actually public. At a minimum, the release notes should indicated that methods were removed and not deprecated.

Build failure:
http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/476/console

{quote}
compile:
[echo] contrib: eclipse-plugin
[javac] Compiling 45 source files to /zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-trunk/workspace/trunk/build/contrib/eclipse-plugin/classes
[javac] /zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-trunk/workspace/trunk/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopServer.java:119: cannot find symbol
[javac] symbol : method getJobId()
[javac] location: class org.apache.hadoop.mapred.JobStatus
[javac] String jobId = status.getJobId();
[javac] ^
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 1 error
{quote}

Can we get this fixed today?

"
HADOOP-3337,Name-node fails to start because DatanodeInfo format changed.,"HADOOP-3283 introduced a new field ipcPort in DatanodeInfo, which was not reflected in the reading/writing file system image files.
Particularly, reading edits generated by the previous version of hadoop throws the following exception:
{code}
08/05/02 00:02:50 ERROR dfs.NameNode: java.lang.IllegalArgumentException: No enum const class org.apache.hadoop.dfs.DatanodeInfo$AdminStates.0?
/56.313
	at java.lang.Enum.valueOf(Enum.java:192)
	at org.apache.hadoop.io.WritableUtils.readEnum(WritableUtils.java:399)
	at org.apache.hadoop.dfs.DatanodeInfo.readFields(DatanodeInfo.java:318)
	at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:90)
	at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:499)
	at org.apache.hadoop.dfs.FSImage.loadFSEdits(FSImage.java:794)
	at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:664)
	at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:280)
	at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:81)
	at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:276)
	at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:257)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:133)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:777)
	at org.apache.hadoop.dfs.NameNode.main(NameNode.java:786)
{code}
and startup fails."
HADOOP-3336,Direct a subset of namenode RPC events for audit logging ,A non-persistent transaction log will permit managers of HDFS installations to monitor and reconstruct user activity in HDFS for forensic analysis and maintenance.
HADOOP-3335,'make clean' in src/c++/libhdfs does 'rm -rf /*'," If 'make clean' is invoked from the command line in src/c++/libhdfs, rather than by the ant build, it tries to remove all files on your system.  I just discovered this the hard way. "
HADOOP-3334,Move lease handling codes out from FSNamesystem,Lease handling is a big component of FSNamesystem.  It is better to put this component in its own class.
HADOOP-3333,job failing because of reassigning same tasktracker to failing tasks,"We have a long running a job in a 2nd atttempt. Previous job was failing and current jobs risks to fail as well, because  reduce tasks failing on marginal TaskTrackers are assigned repeatedly to the same TaskTrackers (probably because it is the only available slot), eventually running out of attempts.
Reduce tasks should be assigned to the same TaskTrackers at most twice, or TaskTrackers need to get some better smarts to find  failing hardware.
BTW, mapred.reduce.max.attempts=12, which is high, but does not help in this case."
HADOOP-3332,improving the logging during shuffling,"Below is an excerpt from the log file of a reducer. 
A same set of of messages about fetching schedule is logged every second. 
Yet, the critical information --- which hosts were slow --- was not there. 

  
2008-05-01 00:33:13,215 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0 Need another 3 map output(s) where 1 is already in progress 
2008-05-01 00:33:14,216 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures 
2008-05-01 00:33:14,216 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0 Got 2 known map output location(s); scheduling... 
2008-05-01 00:33:14,216 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0 Scheduled 0 of 2 known outputs (2 slow hosts and 0 dup hosts) 
2008-05-01 00:33:14,216 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0 Need another 3 map output(s) where 1 is already in progress 
2008-05-01 00:33:15,217 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures 
2008-05-01 00:33:15,217 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0 Got 2 known map output location(s); scheduling... 
2008-05-01 00:33:15,217 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0 Scheduled 0 of 2 known outputs (2 slow hosts and 0 dup hosts) 
2008-05-01 00:33:15,217 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0 Need another 3 map output(s) where 1 is already in progress 
2008-05-01 00:33:16,218 INFO org.apache.hadoop.mapred.ReduceTask: task_200804302255_0002_r_000720_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures 
"
HADOOP-3331,"Return code for ""hadoop dfs -test"" does not match standard return codes from System.exit","The output of ""hadoop dfs -test"" does the opposite of what normal Unix commands to (which is also the opposite of the recommended return codes in the javadocs for System.exit). Normal commands return zero for success and non-zero for error, but ""hadoop dfs -test"" does this opposite. This makes writing shell scripts that use ""hadoop dfs -test"" clunky and unnatural since you can't do standard stuff like this:

{noformat}
if hadoop dfs -test -e /missing/file/name; then
    # Do something only if the file exists
else
    # Do something else if the file is missing
fi
{noformat}

Creating a patch for this would introduce a breaking change and would require changing the Ant DFS task as well."
HADOOP-3330,Hudson should also count javac warnings generated by test code,"Hudson should \-1 patches that add javac warnings in test code, as it does with code in core and contrib."
HADOOP-3329,DatanodeDescriptor objects stored in FSImage may be out dated.,"INodeFileUnderConstruction.targets is a DatanodeDescriptor array which stores the locations for last block.  These DatanodeDescriptors are serialized and stored in the FSImage.  However, a DatanodeDescriptor contains information like IP address and ports.  These information may be out dated after a datanode restart.

We should probably only stored the storageID of a DatanodeDescriptor in FSImage and then lookup the DatanodeDescriptor object from the datanodeMap."
HADOOP-3328,DFS write pipeline : only the last datanode needs to verify checksum,"Currently all the datanodes in DFS write pipeline verify checksum. Since the current protocol includes acks from  the datanodes, an ack from the last node could also serve as verification that checksum ok. In that sense, only the last datanode needs to verify checksum. Based on [this comment|http://issues.apache.org/jira/browse/HADOOP-1702?focusedCommentId=12575553#action_12575553] from HADOOP-1702, CPU consumption might go down by another 25-30% (4/14) after HADOOP-1702. 

Also this would make it easier to use transferTo() and transferFrom() on intermediate datanodes since they don't need to look at the data."
HADOOP-3327,Shuffling fetchers waited too long between map output fetch re-tries,
HADOOP-3326,ReduceTask should not sleep for 200 ms while waiting for merge to finish,"Currently the merge code in Reduce task does:

{code}
            // Wait for the on-disk merge to complete
            while (localFSMergeInProgress) {
              Thread.sleep(200);
            }
            
            //wait for an ongoing merge (if it is in flight) to complete
            while (mergeInProgress) {
              Thread.sleep(200);
            }
{code}"
HADOOP-3322,Hadoop rpc metrics do not get pushed to the MetricsRecord,The MetricsRecord for rpc metrics do not get pushed to the client library. Fix is simple and involves invoking the update() method of the MetricsRecord class.
HADOOP-3320,NPE in NetworkTopology.getDistance(),"I am seeing the following exception:
{code}
08/04/28 20:15:09 ERROR dfs.NNThroughputBenchmark: java.lang.NullPointerException
	at org.apache.hadoop.net.NetworkTopology.getDistance(NetworkTopology.java:443)
	at org.apache.hadoop.dfs.ReplicationTargetChooser.getPipeline(ReplicationTargetChooser.java:457)
	at org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:126)
	at org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:72)
	at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1140)
	at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:300)
	at org.apache.hadoop.dfs.NNThroughputBenchmark$BlockReportStats.addBlocks(NNThroughputBenchmark.java:786)
	at org.apache.hadoop.dfs.NNThroughputBenchmark$BlockReportStats.generateInputs(NNThroughputBenchmark.java:775)
	at org.apache.hadoop.dfs.NNThroughputBenchmark$OperationStatsBase.benchmark(NNThroughputBenchmark.java:203)
	at org.apache.hadoop.dfs.NNThroughputBenchmark.runBenchmark(NNThroughputBenchmark.java:1035)
	at org.apache.hadoop.dfs.NNThroughputBenchmark.main(NNThroughputBenchmark.java:1053)
{code}
It looks like the problem is in NetworkTopology.chooseRandom(String, String), which returns null in certain cases, but the value returned is never checked for null."
HADOOP-3319,[HOD]checknodes prints errors messages on stdout,"When pbsnodes doesn't exist in the path or when a queue name is missing, checknodes prints errors messages on stdout instead of stderr. This should change."
HADOOP-3318,"Hadoop streaming doesn't recognize ""Darwin"" as an OS but Soylatte (OpenJDK port to Mac) reports that rather than ""Mac OS X""",Simple fix.
HADOOP-3317,add default port for hdfs namenode,Perhaps we should add a default port number for the HDFS namenode.  This would simplify URIs.  Instead of hdfs://host.net:9999/foo/bar folks would be able to just use hdfs://host.net/foo/bar in most cases.  Thoughts?
HADOOP-3313,RPC::Invoker makes unnecessary calls to System.currentTimeMillis ,"RPC::Invoker::invoke(...) makes two calls to System.curentTimeMillis(), but discards the result if debug logging is disabled (as it almost always will be)."
HADOOP-3310,Lease recovery for append,"In order to support file append, a GenerationStamp is associated with each block.  Lease recovery will be performed when there is a possibility that the replicas of a block in a lease may have different GenerationStamp values.

For more details, see the documentation in HADOOP-1700."
HADOOP-3309,Unit test fails on Windows: org.apache.hadoop.mapred.TestMiniMRDFSSort.unknown,"Unit test fails on Windows: org.apache.hadoop.mapred.TestMiniMRDFSSort.unknown

*Recent changes:*
# HADOOP-3061. Writable types for doubles and bytes.
# HADOOP-3300. Fix locking of explicit locks in NetworkTopology.
# HADOOP-2910. Throttle IPC Clients during bursts of requests or server slowdown. Clients retry connection for up to 15 minutes when socket connection times out.
# HADOOP-3285. Fix input split locality when the splits align to fsblocks.
# HADOOP-3127. Deleting files in trash should really remove them.
# HADOOP-3268. file:// URLs issue in TestUrlStreamHandler under Windows.
# HADOOP-1979. Speed up fsck by adding a buffered stream. 

*Failed due to timeout*
junit.framework.AssertionFailedError: Timeout occurred

*Output from the console before it timed out:*
[junit] 2008-04-24 13:24:09,556 INFO  util.Container (Container.java:stop(156)) - Stopped org.mortbay.jetty.servlet.WebApplicationHandler@4ac216
[junit] 2008-04-24 13:24:09,696 INFO  util.Container (Container.java:stop(156)) - Stopped WebApplicationContext[/,/]
[junit] 2008-04-24 13:24:09,852 INFO  util.Container (Container.java:stop(156)) - Stopped HttpContext[/logs,/logs]
[junit] 2008-04-24 13:24:10,009 INFO  util.Container (Container.java:stop(156)) - Stopped HttpContext[/static,/static]
[junit] 2008-04-24 13:24:10,009 INFO  util.Container (Container.java:stop(156)) - Stopped org.mortbay.jetty.Server@1b5340c
[junit] 2008-04-24 13:24:10,009 INFO  fs.FSNamesystem (FSEditLog.java:printStatistics(849)) - Number of transactions: 310 Total time for transactions(ms): 62 Number of syncs: 254 SyncTimes(ms): 23395 20970 
[junit] 2008-04-24 13:24:10,071 ERROR fs.FSNamesystem (FSNamesystem.java:run(1963)) - java.lang.InterruptedException
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1741)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1774)
[junit] 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:359)
[junit] 	at org.apache.hadoop.dfs.FSNamesystem$ResolutionMonitor.run(FSNamesystem.java:1940)
[junit] 	at java.lang.Thread.run(Thread.java:595)

[junit] 2008-04-24 13:24:10,321 INFO  ipc.Server (Server.java:stop(1001)) - Stopping server on 3903
[junit] 2008-04-24 13:24:10,321 INFO  ipc.Server (Server.java:run(336)) - Stopping IPC Server listener on 3903
[junit] 2008-04-24 13:24:10,321 INFO  ipc.Server (Server.java:run(506)) - Stopping IPC Server Responder
[junit] 2008-04-24 13:24:10,321 INFO  mapred.MiniMRCluster (MiniMRCluster.java:waitUntilIdle(231)) - Waiting for task tracker tracker_buildsystem:localhost/127.0.0.1:3943 to be idle.
[junit] 2008-04-24 13:24:10,321 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 0 on 3903: exiting
[junit] 2008-04-24 13:24:10,321 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 1 on 3903: exiting
[junit] 2008-04-24 13:24:10,321 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 2 on 3903: exiting
[junit] 2008-04-24 13:24:10,337 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 9 on 3903: exiting
[junit] 2008-04-24 13:24:10,337 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 3 on 3903: exiting
[junit] 2008-04-24 13:24:10,337 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 4 on 3903: exiting
[junit] 2008-04-24 13:24:10,337 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 5 on 3903: exiting
[junit] 2008-04-24 13:24:10,337 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 8 on 3903: exiting
[junit] 2008-04-24 13:24:10,337 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 6 on 3903: exiting
[junit] 2008-04-24 13:24:10,337 INFO  ipc.Server (Server.java:run(927)) - IPC Server handler 7 on 3903: exiting
[junit] 2008-04-24 13:24:11,321 INFO  mapred.MiniMRCluster (MiniMRCluster.java:waitUntilIdle(231)) - Waiting for task tracker tracker_buildsystem:localhost/127.0.0.1:3943 to be idle.
[junit] 2008-04-24 13:24:12,321 INFO  mapred.MiniMRCluster (MiniMRCluster.java:waitUntilIdle(231)) - Waiting for task tracker tracker_buildsystem:localhost/127.0.0.1:3943 to be idle.
"
HADOOP-3308,Improve QuickSort by excluding values eq the pivot from the partition,"The current implementation of QuickSort naively partitions on either side of the pivot. We can improve this by partitioning on either side of the set of values equal to the pivot. This assumes that comparing keys is expensive compared to swaps and index comparisons (which it certainly is in MapTask, and should be in general)."
HADOOP-3307,Archives in Hadoop.,This is a new feature for archiving and unarchiving files in HDFS. 
HADOOP-3304,"[HOD] logcondense fails if DFS has files that are not log files, but match a certain pattern","logcondense works by listing files in dfs and match them against a certain pattern. This pattern is incorrect in the sense that it can potentially match files that are not log files. And this can cause it to fail and not delete files correctly as it should.

It should use a correct regular expression that will only list log files. Also, possibly it should log a stack trace if it happens, but continue to delete files and not stop."
HADOOP-3303,Hadoop Streaming shoulf fail if user process exits with non-zero exit code,"Hadoop streaming quietly ignores the user process exit code. This is extremely dangerous because the user won't notice the problem while the output data is likely to be corrupted or incomplete.

User process may have read all input data into input buffer, and then finds a problem in the input data, and exit with an error code and hopes that hadoop streaming can capture that.

The current hadoop streaming just put this exit code into a log file and ignores it.
"
HADOOP-3302,Support Maven-based builds,"The reasons I would like to use maven are:
- the possibility to define artifact templates to define a kind of standard layout/design by artifact
- it is not necessary for every developer to come up with his own ant build-file and process
- the possibility to define and resolve dependencies transitively

But there are also some disadvantages/concerns I identified:
Maven is downloading a lot of plugins from a central repository that is not under my control
- What's about the licenses of these plugins? How do I know I am allowed to use them for a commercial product?
- What's about security? How can I be sure, that the plugins are not manipulated and contain the original (delivered by the JAR provider for e.g. junit-jar) contents. I observed, that some plugins didn't pass the md5 checks but have been installed anyway.
"
HADOOP-3301,Misleading error message when S3 URI contains hostname containing an underscore,"As reported in http://www.nabble.com/Not-able-to-back-up-to-S3-td16737029.html#a16737029, when the hostname in an S3 URI contains an underscore the exception reports a problem with S3 credentials. In the absence of URI complaining (see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6587184) we can check that the host is non-null before checking user info (since URI.getHost returns null if the host contains an underscore). "
HADOOP-3300,FindBugs warnings in NetworkTopology,"There are a couple of places where explicit locks aren't released correctly:

org.apache.hadoop.net.NetworkTopology.add(Node) does not release lock on all exception paths

and

org.apache.hadoop.net.NetworkTopology.remove(Node) does not release lock on all exception paths "
HADOOP-3299,org.apache.hadoop.mapred.join.CompositeInputFormat does not initialize  TextInput format files with the configuration resulting in an NullPointerException,"The input formats are not initialized with the Configuration object before the isSplitable method is called.

bin/hadoop jar hadoop-0.16.3-examples.jar  join -r 1 -inFormat org.apache.hadoop.mapred.KeyValueTextInputFormat -outFormat org.apache.hadoop.mapred.TextOutputFormat  -joinOp outer datajoin/input datajoin/output -outKey org.apache.hadoop.io.Text -outValue org.apache.hadoop.mapred.join.TupleWritable
08/04/22 15:05:33 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
Job started: Tue Apr 22 15:05:33 GMT-08:00 2008
08/04/22 15:05:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
08/04/22 15:05:33 INFO mapred.FileInputFormat: Total input paths to process : 2
java.lang.NullPointerException
	at org.apache.hadoop.mapred.KeyValueTextInputFormat.isSplitable(KeyValueTextInputFormat.java:44)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:185)
	at org.apache.hadoop.mapred.join.Parser$WNode.getSplits(Parser.java:304)
	at org.apache.hadoop.mapred.join.Parser$CNode.getSplits(Parser.java:374)
	at org.apache.hadoop.mapred.join.CompositeInputFormat.getSplits(CompositeInputFormat.java:129)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:542)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:803)
	at org.apache.hadoop.examples.Join.run(Join.java:149)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.examples.Join.main(Join.java:158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:52)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
"
HADOOP-3298,Add support for transitive native library dependencies in DistributedCache ,"[Hadoop 1660] add the tasks's working directory to it's JAVA_LIBRARY_PATH, this allows to distribute JNI based native libs with the DistributedCache by symbolically linking them to the tasks's working directory.

There are many instances when the JNI components have transitive dependencies to other native libraries. Even if we have symlinks for these in  tasks's working directory, they'll not resolved as transitive dependencies of native libraries are looked up in the shell's LD_LIBRARY_PATH and not in the JAVA_LIBRARY_PATH. This can be tackled by adding CWD to JAVA_LIBRARY_PATH in the *hadoop-env.sh*.
A better way to do this would be to add the CWD only in environment of the Task Tracker as only it needs it.
"
HADOOP-3297,The way in which ReduceTask/TaskTracker gets completion events during shuffle can be improved,"Certain things like poll frequency, number of events fetched in one go, etc. can probably be improved to improve the shuffle performance. This would affect the task-->tasktracker and the tasktracker-->jobtracker shuffle related RPCs."
HADOOP-3296,Some levels are skipped while creating the task cache in JobInProgress,"Consider the following piece of code
{code:title=JobInProgress.createCache()|borderStyle=solid}
Node node = jobtracker.resolveAndAddToTopology(host);
for (int j = 0; j < maxLevel; j++) {
          node = JobTracker.getParentNode(node, j);
          .....
{code}
With {{maxLevel > 2}} the caches will be created in the following order
||j||node-level||
|0|0|
|1|1|
|2|3|
|3|6|
which is not as desired.
"
HADOOP-3295,Allow TextOutputFormat to use configurable separators,"TextOutputFormat use hardcoded tab as key-value separator. We should allow configurable separators like ^A, etc.
"
HADOOP-3294,distcp leaves empty blocks afte successful execution,"I copied around 40 TB between two hadoop clusters, with distcp running on source.

Job was *successful*, but one destination file was empty because of its only block being empty.
None of the distcp log files have any mentioning of this file.

There were a couple of messages in the namenode server log of the destination cluster referencing the file:

hadoop-xxxnamenode-yyy.log.2008-04-19:2008-04-19 02:19:15,666 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.allocateBlock: destinationDir/_distcp_tmp_z0g93p/fileName. blk_-9209890281741927376
hadoop-xxx-namenode-yyy.log.2008-04-19:2008-04-19 02:54:45,820 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on destinationDir/_distcp_tmp_z0g93p/fileName file does not exist.

distcp should not rely on the user to double-check.
Would it make sense to add a reducer  to compare destination file sizes with source files sizes and do some appropriate action?"
HADOOP-3293,"When an input split spans cross block boundary, the split location should be the host having most of bytes on it. ",
HADOOP-3289,Hadoop should have a way to know when JobTracker is really ready to accept jobs.,"Hadoop throws an org.apache.hadoop.mapred.JobTracker$IllegalStateException when we try to submit jobs while JT is still initializing and cannot accept jobs yet. This might be because of various reasons, like job submitted too early, or JT waiting for response from NN which might be in safemode (HADOOP-2213) or JT fails to clean-up mapred system directory(HADOOP-3276). This causes problems in HoD or any other user scripts automatically submitting jobs.

To deal with such problems, we need to have a way either to find out the state of the job tracker so that this can be checked before launching any job, or, otherwise, a way to determine if JT can accept any jobs now. Currently there is no api/command line interface to check this in Hadoop. Even job submission doesn't return any specific error code, even in presence of IllegalStateException error. So, the only reliable way, for HOD/scripts to detect such exceptions as these, is to search for exception strings in the output of these commands, which is kind of nasty.

So, it would be good if Hadoop can provide an api/error code/cmd line utility to check if JT is really ready to accept any jobs. Otherwise HoD/user scripts will be left with resorting to (unreliable) way of sleeping for arbitrary amounts of time and retrying w/o knowing the actual reason."
HADOOP-3287,Being able to set default job configuration values on the jobtracker,"The jobtracker hadoop-site.xml carries custom configuration for the cluster and the 'final' flag allows to fix a value ignoring any override by a client when submitting a job.

There are several properties for which a cluster may want to set some default values (different from the ones in the hadoop-default.xml), for example:

 * enabling/disabling compression
 * type of compression, record/block
 * number of task retries
 * block replication factor
 * job priority
 * tasks JVM options

The cluster default values should apply to submitted jobs when the job submitter does not care about those values. When the job submitter cares, it should include its preferred values. Using the final flag on the jobtracker hadoop-site.xml will lock the value ignoring the value set in the client jobconf.

Currently the only way of doing this is to distribute the jobtracker hadoop-site.xml to all clients and make sure they use it when creating the job configuration.

There are situations where this is not practical:

 * In a shared cluster with several clients submitting jobs. It requires redistributing the hadoop-site.xml to all clients.
 * In a cluster where the jobs are dispatched by a webapp application. It requires rebundling and redeploying the webapp.

The current behavior happens because the jobconf when serialized, to be sent to the jobtracker, sends all the values found in the hadoop-default.xml bundled with the hadoop JAR file. On the jobtracker side, all those values override all but the 'final' properties of the jobtracker hadoop-site.xml.

According to the javadocs of the Configuration.write(OutpuStream) this should not happen ' Writes non-default properties in this configuration.'

If taken the javadocs as the proper behavior this is a bug in the current implementation and it could be easily fixed by avoiding writing default values on write.

This is a generalization of the problem mentioned in Hadoop-3171.
"
HADOOP-3286,Gridmix jobs'  output dir names may collide,"

Gridmix jobs use time suffix to differentiate output dir names. 
The current time granularity of second may not be sufficient, causing multiple jobs having the same output dir.

"
HADOOP-3285,map tasks with node local splits do not always read from local nodes,"I ran a simple map/reduce job counting the number of records in the input data.
The number of reducers was set to 1.
I did not set the number of mappers. Thus by default, all splits except the last split of a file contain one dfs block (128MB in my case).
The web gui indicated that 99% of map tasks were with local splits.
Thus I expected that most of the dfs reads should have come from the local data nodes.
However, when I examine the traffic of the ethernet interfaces, 
I found about 50% traffic of each node were through the loopback interface and other 50% were through the ethernet card!
Also,  the switch monitoring indicated that a lot of traffic went through the links and cross racks!
This indicated that the data locality feature does not work as expected.

To confirm that, I set the number of map tasks to a very high number so that it forced the split size down to about 27MB.
The web gui indicated that 99% of map tasks were with local splits, as expected.
The ethernet interface monitor showed that almost 100% traffic went through the loopback interface, as it should be. 
I found about 50% traffic of each node were through the loopback interface and other 50% were through the ethernet card!
Also,  the switch monitoring indicated that there were very little traffic through the links and cross racks.

This implies that some corner cases are not handled properly.

"
HADOOP-3283,Need a mechanism for data nodes to update generation stamps.,"For implementing file append feature (HADOOP-1700), a generation stamp is added to each block.  We need a mechanism for data nodes to update generation stamps for lease recovery."
HADOOP-3282,TestCheckpoint occasionally fails because of the port issues.,"This is related to timing issues. If MiniDFSCluster does not stop fast enough the name-node won't start because the port is busy.
So the tests should reset the configuration port properties before each server startup, because servers change those values to the actual port numbers they run.
"
HADOOP-3280,virtual address space limits break streaming apps,"HADOOP-2765 added a mandatory, hard virtual address space limit to streaming apps based on the Java process's -Xmx setting.

This makes it impossible to run a 64-bit streaming app that needs large address spaces under a 32-bit JVM, even if one is otherwise willing to dramatically increase the -Xmx setting without cause. Also, unlike Java's -Xmx limit, the virtual address space limit for an arbitrary UNIX process does not necessarily correspond to RAM usage, so it's likely to be a relatively difficult to configure limit.

2765 was originally opened to allow an optional wrapper script around streaming tasks, one use case for which was setting a ulimit. That approach seems much less intrusive and more flexible than the final implementation. The ulimit can also be trivially set by the streaming task itself without any support from Hadoop.

Marking this as an 0.17 blocker because it will break deployed apps and there is no workaround available."
HADOOP-3279,TaskTracker should check for SUCCEEDED task status in addition to COMMIT_PENDING status when it fails maps due to lost map outputs,"Post HADOOP-3140, tasks could be marked as SUCCEEDED by the TaskTracker. Hence, the TaskTracker should check for SUCCEEDED task status in addition to COMMIT_PENDING status when it fails maps due to lost map outputs"
HADOOP-3277,hod should better errors message when deallocate is fired on non allocated directory.,"Current when deallocate on a directory which not allocated hod shows following error -:
[
CRITICAL - '<cluster_dir_path>' is not a valid cluster directory.
]

It should show some better error message e.g. CRITICAL - '<cluster_dir_path>' directory is not allocated.
"
HADOOP-3275,Reduce task does not handle map outputs fetching efficiently ,"
I ran a job just counting the number of records in the input data (with combiner)
The map phase took about less than 10 minutes.
But the shuffling took additional 30 minutes!

After examining the code and experimenting a few tweakings, we discovered the probe_sample_size (50/100) in task tracker is too small. 
The fetchers just cannot be kept busy. After changing that probing size to 5000, the shuffling time reduce to 13 minutes.

With that setting, the fetching (30 threads) became bottleneck. That is basically limited by how many http fetches a thread can do per second.
To further improve shuffling, we may need to consider to use keep alive http connection and fetch multiple segments per http connection.

"
HADOOP-3274,The default constructor of BytesWritable should not create a 100-byte array.,"The default byte array should be an empty array and the empty array should be stored in a static final variable.  Then, there is no unnecessary memory allocation."
HADOOP-3272,Reduce redundant copy of Block object in BlocksMap.map hash map,"Looks like we might have redundant copy of Block object as Key for BlocksMap.map hash map. We should restore this to using same object for both Key, Value to save space. "
HADOOP-3271,JobHistory.log and DefaultJobHistory.MasterIndex are missing in 0.17 dev,"There used to be JobHistory.log (MasterIndex in Default HistoryParser class) in 0.15.3 to 0.16.2 which provides the high level overview of the job history.  For HADOOP-2178, this information is no longer being provided by the DefaultHistoryParser class.  In addition, the parser class can only read job log files from dfs, not from an input stream.  

For performance monitoring, we have programs that depends on the pure parser class and JobHistory.log to extract job information.  Please continue to provide a pure parser class and equivalent function of the JobHistory.log."
HADOOP-3270,Constant DatanodeCommand should be stored in static fianl immutable variables.,"DatanodeCommand like DNA_REGISTER, DNA_BLOCKREPORT and DNA_FINALIZE are constant command.  New objects should not be created for each time when using these commands."
HADOOP-3269,NameNode doesn't startup when restarted after running an MR job,"If the following is done in sequence the namenode doesn't start up (tried all these on a single node cluster setup)

1) Build a fresh hadoop jar from trunk

2) bin/start-all.sh

3) bin/hadoop dfs -put <somedir> input

4) bin/hadoop jar build/hadoop-0.18.0-dev-examples.jar wordcount input output

5) bin/stop-all.sh

6) bin/start-all.sh

Namenode doesnt start up. It fails with following exception:
2008-04-17 16:05:32,852 ERROR org.apache.hadoop.dfs.NameNode: 
org.apache.hadoop.fs.permission.AccessControlException: ugi = null
    at org.apache.hadoop.dfs.PermissionChecker.<init>(PermissionChecker.java:49)
    at org.apache.hadoop.dfs.FSNamesystem.checkPermission(FSNamesystem.java:4188)
    at org.apache.hadoop.dfs.FSNamesystem.checkTraverse(FSNamesystem.java:4167)
    at org.apache.hadoop.dfs.FSNamesystem.getFileInfo(FSNamesystem.java:1554)
    at org.apache.hadoop.dfs.FSNamesystem.changeLease(FSNamesystem.java:4358)
    at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:590)
    at org.apache.hadoop.dfs.FSImage.loadFSEdits(FSImage.java:843)
    at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:717)
    at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:281)
    at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:81)
    at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:274)
    at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:255)
    at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:133)
    at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
    at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
    at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:782)
    at org.apache.hadoop.dfs.NameNode.main(NameNode.java:791)
"
HADOOP-3268,TestUrlStreamHandler.testFileUrls fails on Windows,"org.apache.hadoop.fs.TestUrlStreamHandler.testFileUrls fails on Windows with this exception:

java.net.URISyntaxException: Illegal character in authority at index 7: file://C:\hudson\workspace\Hadoop-WindowsTest\trunk\build\test/thefile
	at java.net.URI$Parser.fail(URI.java:2816)
	at java.net.URI$Parser.parseAuthority(URI.java:3154)
	at java.net.URI$Parser.parseHierarchical(URI.java:3065)
	at java.net.URI$Parser.parse(URI.java:3021)
	at java.net.URI.<init>(URI.java:578)
	at org.apache.hadoop.fs.TestUrlStreamHandler.testFileUrls(TestUrlStreamHandler.java:119)

"
HADOOP-3266,"Remove HOD changes from CHANGES.txt, as they are now inside src/contrib/hod","HOD changes were originally documented in the CHANGES.txt file.

However, to assist ease of commiting patches - as hod committers (me) don't have access to the main CHANGES.txt file, and also because HOD is typically deployed and installed to separate locations, it made sense to have a separate CHANGES.txt for HOD.  This is already present under src/contrib/hod. This issue is to remove the older entries in the main CHANGES.txt, so there is no duplication."
HADOOP-3265,Remove deprecated API getFileCacheHints,"Following HADOOP-2027, please remove getFileCacheHints deprecated API"
HADOOP-3264,libhdfs does not provide permissions in API,"There is no support in libhdfs methods for obtaining or setting the permissions, owner or group-owner of files in hdfs."
HADOOP-3263,job history browser throws exception if job name or user name is null.,Job history browser throws exception if job name or user name is null.  Instead it should print a null string for user/job name if they are null.
HADOOP-3261,Allow Mapper and Reducer classes to be created a Spring Beans,This improvement abstracts the creation of Mappers and Reducers  in factory classes and adds the ability to define a spring beans.xml file in the classpath and have the Mapper and Reducer classes be created as spring beans.  This is transparent to the rest of Hadoop but the benefit is that is allows Mapper and Reducer classes to have their dependencies injected versus having them hardcoded.  This will help in the ability to test various mapreduce jobs.
HADOOP-3260,FSNamesystem.blockReportProcessed(DatanodeRegistration) should re-use the DatanodeDescriptor obtained in FSNamesystem.gotHeartbeat(...),"Instead of searching for the DatanodeDescriptor in FSNamesystem.blockReportProcessed(DatanodeRegistration), it should re-use the descriptor obtained in FSNamesystem.gotHeartbeat(...)."
HADOOP-3259,Configuration.substituteVars() needs to handle security exceptions,"Inside Configuration.substituteVars(), there is a call to System.getProperty(var); this contains the implicit assumption that the JVM will never block access to a system property, because if that is the case -such as when the Configuration is running under a restrictive security manager, a SecurityException gets thrown. This will get thrown all the way up the tree. 

Better to have some plan to handle it in situ, such as a log@warn level then leave the property unexpanded. "
HADOOP-3258,FileOutputFormat should have a method to create custom files under the outputdir with a unique name per task to avoid name collision,"Currently, if a M/R code creates a file, it is the responsibility of the M/R code to avoid file name collisions from different tasks.

Hadoop should provide an API that creates unique file names based on the task type (map or reduce) and the task ID. Similarly to how output files, part-#####, are created.

The proposed patch adds 2 static methods to the {{FileOutputFormat}}

{nofomat}
  public static String getUniqueName(JobConf conf, String name);
  public static Path getPathForCustomFile(JobConf conf, String name);
{nofomat}

The first one adds task type and task ID to the given name.

The second gives a PATH to a file in the working outputdir using a file name namespaced by the first method.
"
HADOOP-3256,JobHistory file on HDFS should not use the 'job name',"HADOOP-2178 introduced the feature of saving jobhistory logs on HDFS.

Unfortunately the following code:
{noformat}
        // setup the history log file for this job
        String logFileName = jobUniqueString +  
                             ""_"" + user+ ""_"" + jobName;
        if (logFileName.length() > MAX_FILENAME_SIZE) {
          logFileName = logFileName.substring(0, MAX_FILENAME_SIZE-1);
        }
{noformat}
is vulnerable to user-provided job names. 

Specifically I ran into 'URISyntaxException' with jobs whose names include a "":"".

The easy fix is to ensure that we do not use the human-friendly job names and only the jobid.

The long term fix is to ensure that Path handles filenames with _any_ characters."
HADOOP-3254,"FSNamesystem.gotHeartbeat(..., Object[] xferResults, Object[] deleteList) should not use Object[] as pass-by-reference parameters","In FSNamesystem.gotHeartbeat(..., Object[] xferResults, Object[] deleteList), xferResults initially equals to Object[] { null, null}.  If there are blocks to be replicated, xferResults[0] and xferResults[1] will be set to a Block[] and DatanodeInfo[][], respectively.  The caller of gotHeartbeat will cast these object to the original classes.  For example, in NameNode.sendHeartbeat(...),
{code}
    if (xferResults[0] != null) {
      assert(deleteList[0] == null);
      return new BlockCommand((Block[]) xferResults[0], (DatanodeInfo[][]) xferResults[1]);
    }
{code}
The use of deleteList is similar."
HADOOP-3253,"In DatanodeDescriptor, elements in replicateBlocks or invalidateBlocks may be removed without processing",
HADOOP-3251,WARN message on command line when a hadoop jar command is executed,"When a job is run, a WARN message is given on the command line. This does not affect the cluster functionality, but is bad user experience. Hence, marking this a blocker for 0.17.0

The message is:
08/04/14 20:53:46 WARN fs.FileSystem: ""namenode:59440"" is a deprecated filesystem name. Use ""hdfs://namenode:59440/"" instead.
"
HADOOP-3250,Extend FileSystem API to allow appending to files,Provide an API to allow applications to append data to pre-existing files.
HADOOP-3248,Improve Namenode startup performance,"One of the things that would need to be addressed as part of Namenode scalability is the HDFS recovery performance especially in scenarios where the number of files is large. There are instances where the number of files are in the vicinity of 20 million and in such cases the time taken for namenode startup is prohibitive. Here are some benchmark numbers on the time taken for namenode startup. These times do not include the time to process block reports.

Default scenario for 20 million files with the max  java heap size set to 14GB : 40 minutes

Tuning various java options such as young size, parallel garbage collection, initial java heap size : 14 minutes

As can be seen, 14 minutes is still a long time for the namenode to recover and code changes are required to bring this time down further. To this end some prototype optimizations were done to reduce this time. Based on some timing analysis saveImage and loadFSImage where the primary methods that were consuming most of the time. Most of the time was being spent on doing object allocations. The goal of the optimizations is to reduce the number of memory allocations as much as possible.

Optimization 1: saveImage() 
======================
Avoid allocation of the UTF8 object.

Old code
=======
new UTF8(fullName).write(out);

New Code
========
out.writeUTF(fullName)


Optimization 2: saveImage()
======================
Avoid object allocation of the PermissionStatus Object and the FsPermission object. This is to be done for Directories and for files.

Old code
=======
fileINode.getPermissionStatus().write(out)

New Code
=========
out.writeBytes(fileINode.getUserName())
out.writeBytes(fileINode.getGroupName())
out.writeShort(fileINode.getFsPermission().toShort())

Optimization 3
============
loadImage() could use the same mechanism where we would avoid allocating the PermissionStatus object and the FsPermission object.

Optimization 4
============
A hack was tried out to avoid the cost of object allocation from saveImage() where the fullName was being constructed using string concatenation. This optimization also helped improve performance

Overall these optimizations helped bring down the overall startup time down to slightly over 7 minutes. Most of all the remaining time is now spent in loadFSImage() since we allocate the INode and INodeDirectory objects. Any further optimizations will need to focus on loadFSImage()
"
HADOOP-3247,gridmix scripts have a few bugs,"
maxentToSameCluster should use NUM_OF_LARGE_JOBS_FOR_ENTROPY_CLASS

webdata_sort.large's input dir spec need to use glob syntax.
"
HADOOP-3246,FTP client over HDFS,"An FTP client that stores content directly into HDFS allows data from FTP serves to be stored directly into HDFS instead of first copying the data locally and then uploading it into HDFS. The benefits are apparent from an administrative perspective as large datasets can be pulled from FTP servers with minimal human intervention.
"
HADOOP-3245,Provide ability to persist running jobs (extend HADOOP-1876),This could probably extend the work done in HADOOP-1876. This feature can be applied for things like jobs being able to survive jobtracker restarts.
HADOOP-3244,"MiniMRCluster should take a ""conf"" object in it's constructor","MiniMRCluster should take a ""conf"" object in it's constructor. This is required if we want to pass some non-default configurations like system directory.

MiniMRCluster should use this configuration to create it's internal Job Tracker."
HADOOP-3243,DoubleWritable,
HADOOP-3242,"SequenceFileAsBinaryRecordReader seems always to read from the start of a file, not the start of the split.",
HADOOP-3241,DFSFileInfo should also have field to say if the file is underconstrction,It would be good to have a flag which says if the DFSFileInfo object corresponds to a file under construction or not.
HADOOP-3240,TestJobShell should not create files in the current directory,"After TestJobShell is done, a file ""files_tmp"" will be created in the current directory.  Testing files should be created under the test directory and be cleaned up once the test is done."
HADOOP-3239,exists() calls logs FileNotFoundException in namenode log,"exists() was modified to invoke getFileStatus() internally. But getFileStatus() throws FileNotFoundException for files which does not exists and this is logged in RPC$Server for each exists() call. One way to get rid of these messages is at the Name Node, catch FileNotFoundException and return null. In this case, RPC would not log it in namenode log. But at the client end we might have to check for null on all calls of getFileStatus(). Other option at client end is to construct FileNotFoundException() when getFileInfo() returns null. "
HADOOP-3238,Unit test failed on windows: TestMiniMRWithDFSWithDistinctUsers,"Unit test failed on windows: TestMiniMRWithDFSWithDistinctUsers

I see this before it times out:


    [junit] 2008-04-09 20:34:05,821 INFO  dfs.DataNode (DataNode.java:shutdown(492)) - Waiting for threadgroup to exit, active threads is 1
    [junit] 2008-04-09 20:34:06,821 INFO  dfs.DataNode (DataNode.java:shutdown(492)) - Waiting for threadgroup to exit, active threads is 1
    [junit] 2008-04-09 20:34:07,821 INFO  dfs.DataNode (DataNode.java:shutdown(492)) - Waiting for threadgroup to exit, active threads is 1
    [junit] 2008-04-09 20:34:08,820 INFO  dfs.DataNode (DataNode.java:shutdown(492)) - Waiting for threadgroup to exit, active threads is 1
    [junit] 2008-04-09 20:34:09,820 INFO  dfs.DataNode (DataNode.java:shutdown(492)) - Waiting for threadgroup to exit, active threads is 1
    [junit] 2008-04-09 20:34:10,820 INFO  dfs.DataNode (DataNode.java:shutdown(492)) - Waiting for threadgroup to exit, active threads is 1
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers FAILED (timeout)"
HADOOP-3237,Unit test failed on windows: TestDFSShell.testErrOutPut,"Unit test failed on windows: TestDFSShell.testErrOutPut


Standard Error

javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testURIPaths(TestDFSShell.java:386)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=FSNamesystemStatus
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.FSNamesystem.registerMBean(FSNamesystem.java:4257)
	at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:293)
	at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:255)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:133)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testURIPaths(TestDFSShell.java:386)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testText(TestDFSShell.java:472)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
text: File /texttest/file.gz does not exist.
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testCopyToLocal(TestDFSShell.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
copyToLocal: null
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testCount(TestDFSShell.java:614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testFilePermissions(TestDFSShell.java:728)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
chown: could not get status for '/nonExistentFile': java.io.FileNotFoundException: File does not exist: /nonExistentFile

chown: could not get status for 'unknownFile': java.io.FileNotFoundException: File does not exist: /user/hadoopqa/unknownFile

javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testDFSShell(TestDFSShell.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
cat: null
rm: cannot remove /test/mkdirs/myFile1: No such file or directory.
cp: Cannot copy /test/dir1 to its subdirectory /test/dir1/dir2
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:852)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testGet(TestDFSShell.java:1053)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
get: Checksum error: /blk_-1566589498752841793:of:/test/get/testGet.txt at 0
"
HADOOP-3234,Write pipeline does not recover from first node failure sometimes.,"While investigating HADOOP-3132, we had a misconfiguration that resulted in client writing to first datanode in the pipeline with 15 second write timeout. As a result, client breaks the pipeline marking the first datanode (DN1) as the bad node. It then restarts the next pipeline with the rest of the of the datanodes. But the next (second) datanode was stuck waiting for the the earlier block-write to complete. So the client repeats this procedure until it runs out the datanodes and client write fails.

I think this should be a blocker either for 0.16 or 0.17."
HADOOP-3232,Datanodes time out,"I recently upgraded to 0.16.2 from 0.15.2 on our 10 node cluster.
Unfortunately we're seeing datanode timeout issues. In previous versions we've often seen in the nn webui that one or two datanodes ""last contact"" goes from the usual 0-3 sec to ~200-300 before it drops down to 0 again.

This causes mild discomfort but the big problems appear when all nodes do this at once, as happened a few times after the upgrade.
It was suggested that this could be due to namenode garbage collection, but looking at the gc log output it doesn't seem to be the case."
HADOOP-3230,Add command line access to named counters,"It would be convenient to be able to access counters by name from the command line. Example usage:

bin/hadoop job -counter <job-id> <group-name> <counter-name>"
HADOOP-3229,Map OutputCollector does not report progress on writes,"It seem that the collector implementation used during the map phase does not report progress on writing.
"
HADOOP-3226,Run combiner when merging spills from map output,"When merging spills from the map, running the combiner should further diminish the volume of data we send to the reduce.
"
HADOOP-3225,FsShell showing null instead of a error message,"Tried the following in 0.17 and trunk:

User is nn_sze here
{noformat}
$ ./bin/hadoop fs -lsr /
/foo    <dir>           2008-04-09 17:19        rwx------       nn_sze  supergroup
/foo/bar        <dir>           2008-04-09 17:19        rwxr-xr-x       nn_sze  supergroup
{noformat}

User is now tsz (non-superuser)
{noformat}
bash-3.2$ ./bin/hadoop fs -ls /foo
ls: null
bash-3.2$ ./bin/hadoop fs -put README.txt /foo
put: null
bash-3.2$ ./bin/hadoop fs -rmr /foo/bar
rmr: null
{noformat}"
HADOOP-3224,hadoop dfs -du /dirPath does not work with hadoop-0.17 branch,
HADOOP-3223,Hadoop dfs -help for permissions contains a typo,"The command 'hadoop dfs -help' shows command-line options, that include
a description of

E.g.: 754 is same as u=rwx,g=rw,o=r

I believe this description is not correct, it should be:

E.g.: 754 is same as u=rwx,g=rx,o=r"
HADOOP-3222,fsck should require superuser privilege,"Currently, any user can run fsck.  It should require superuser privilege."
HADOOP-3221,"Need a ""LineBasedTextInputFormat""","In many ""pleasantly"" parallel applications, each process/mapper processes the same input file (s), but with computations are controlled by different parameters.
(Referred to as ""parameter sweeps"").

One way to achieve this, is to specify a set of parameters (one set per line) as input in a control file (which is the input path to the map-reduce application, where as the input dataset is specified via a config variable in JobConf.).

It would be great to have an InputFormat, that splits the input file such that by default, one line is fed as a value to one map task, and key could be line number. i.e. (k,v) is (LongWritable, Text).

If user specifies the number of maps explicitly, each mapper should get a contiguous chunk of lines (so as to load balance between the mappers.)

The location hints for the splits should not be derived from the input file, but rather, should span the whole mapred cluster.

(Is there a way to do this without having to return an array of nSplits*nTaskTrackers ?)

Increasing the replication of the ""real"" input dataset (since it will be fetched by all the nodes) is orthogonal, and one can use DistributedCache for that.

(P.S. Please chose a better name for this InputFormat. I am not in love with  ""LineBasedText"" name.)
"
HADOOP-3220,Safemode log message need to be corrected.,"When name-node safemode enters the extension period stage the message that is logged and printed on the web UI should say
""The ratio of reported blocks 0.xxx has reached the threshold 0.yyy""
rather than
""The ratio of reported blocks 0.xxx has *not* reached the threshold 0.yyy""
"
HADOOP-3217,[HOD] Be less agressive when querying job status from resource manager.,"After a job is submitted, HOD queries torque periodically until it finds the job to be running / completed (due to error). The initial rate of query is once every 0.5 seconds for 20 times, and then once every 10 seconds. This is probably a tad too aggressive as we find that Torque sometimes returns some odd errors under heavy load in the cluster (HADOOP-3216). It may be better to query at a more relaxed rate. "
HADOOP-3214,JobInProgress.garbageCollect delete <job-dir> twice.,"In JobInProgress.garbageCollect, the following codes delete <job-dir> twice.
{code}
      // JobClient always creates a new directory with job files
      // so we remove that directory to cleanup
      FileSystem fs = FileSystem.get(conf);
      fs.delete(new Path(profile.getJobFile()).getParent(), true);
        
      // Delete temp dfs dirs created if any, like in case of 
      // speculative exn of reduces.  
      Path tempDir = new Path(conf.getSystemDir(), jobId); 
      fs.delete(tempDir, true); 
{code}

Below is the clean-up trace copied from HADOOP-3182:
    * FileSystem.delete <job-dir> by JobTracker as user_account
      at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:1637)
      at org.apache.hadoop.mapred.JobInProgress.isJobComplete(JobInProgress.java:1396)
      at org.apache.hadoop.mapred.JobInProgress.completedTask(JobInProgress.java:1357)
      at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:565)
      at org.apache.hadoop.mapred.JobTracker$TaskCommitQueue.run(JobTracker.java:2270)
      <job-dir> is obtained by *profile.getJobFile().getParent()*

    * FileSystem.delete <job-dir> again by JobTracker as user_account
      at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:1642)
      at org.apache.hadoop.mapred.JobInProgress.isJobComplete(JobInProgress.java:1396)
      at org.apache.hadoop.mapred.JobInProgress.completedTask(JobInProgress.java:1357)
      at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:565)
      at org.apache.hadoop.mapred.JobTracker$TaskCommitQueue.run(JobTracker.java:2270)
      <job-dir> is obtained by *new Path(conf.getSystemDir(), jobId)*

Is there any case that these two paths are distinct?"
HADOOP-3213,Provide Hadoop Pipes tutorial,"Hadoop pipes is a neat (and more efficient than streaming) way of writing C++ Hadoop map-reduce applications. It would be great to have a tutorial for beginners and advanced users, similar to the Java MapRed tutorial."
HADOOP-3208,WritableDeserializer does not pass the Configuration to deserialized Writables,"{{WritableDeserializer}} does not hold a reference to the Configuration object, so it does not pass it to deserialized writables, but uses ReflectionUtils.newInstance(writableClass, null), which results in objects defining Configurable interface to be initialized with null configuration. 

Serialization framework have been introduced in 0.17, so this bug affects 0.17. "
HADOOP-3206,Remove deprecated class UTF8,Deprecated class UTF8  needs to be removed. And references to it can be replaced by Text.
HADOOP-3205,Read multiple chunks directly from FSInputChecker subclass into user buffers,"Implementations of FSInputChecker and FSOutputSummer like DFS do not have access to full user buffer. At any time DFS can access only up to 512 bytes even though user usually reads with a much larger buffer (often controlled by io.file.buffer.size). This requires implementations to double buffer data if an implementation wants to read or write larger chunks of data from underlying storage.

We could separate changes for FSInputChecker and FSOutputSummer into two separate jiras.

"
HADOOP-3204,LocalFSMerger needs to catch throwable,"I haven't used trunk(0.17) so I don't know what would happen when FSError is thrown within LocalFSMerger thread.

Does it have the same problem as HADOOP-3154?  
"
HADOOP-3203,TaskTracker::localizeJob doesn't provide the correct size to LocalDirAllocator,"In TaskTracker::localizeJob:
{code}
    // Get sizes of JobFile and JarFile
    // sizes are -1 if they are not present.
    FileSystem fileSystem = FileSystem.get(fConf);
    FileStatus status[] = fileSystem.listStatus(new Path(jobFile).getParent());
    long jarFileSize = -1;
    long jobFileSize = -1;
    for(FileStatus stat : status) {
      if (stat.getPath().toString().contains(""job.xml"")) {
        jobFileSize = stat.getLen();
      } else {
        jobFileSize = -1;
      }
      if (stat.getPath().toString().contains(""job.jar"")) {
        jarFileSize = stat.getLen();
      } else {
        jarFileSize = -1;
      }
    }
{code}

One or both of jobFileSize and jarFileSize will be -1."
HADOOP-3202,"Deprecate org.apache.hadoop.fs.FileUtil.fullyDelete(FileSystem fs, Path dir)","We should use FileSystem.delete(path, recursive=true) instead.
See also https://issues.apache.org/jira/browse/HADOOP-771?focusedCommentId=12586566#action_12586566"
HADOOP-3201,namenode should be able to retrieve block metadata from a datanode,"The Append design (HADOOP-1700) requires that the namenode be able to contact the datanode when a lease expires. 

When a lease expires, the namenode has to fix up the size of the last block of the file that was being written to. The namenode contacts the datanodes, retrieves the block generation stamp and the length of the lastblock of all known replicas, determines which replicas are good and which one are to be deleted, stamps the winning replicas with a new generation stamp and deletes losing replicas. Once this process is complete, the namenode can allow a new writer to append to this file. Details of this design are in HADOOP-1700.

A few options available to us:

1. The namenode sends the request as a response to the next heartbeat RPC from the datanode. This methodology is currently used for requesting block reports from datanodes.

2. The datanode has an RPC server. A pool of threads in the namenode can be used to make RPCs to the datanodes.

"
HADOOP-3198,ReduceTask should handle rpc timeout exception for getting recordWriter,"After shuffling and sorting, the reduce task is ready for the final phase --- reduce. 
The first thing is to create a record writer. 
That call may fail due to rpc timeout. 

java.net.SocketTimeoutException: timed out waiting for rpc response 
at org.apache.hadoop.ipc.Client.call(Client.java:559) 
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212) 
at org.apache.hadoop.dfs.$Proxy1.getFileInfo(Unknown Source) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59) 
at org.apache.hadoop.dfs.$Proxy1.getFileInfo(Unknown Source) 
at org.apache.hadoop.dfs.DFSClient.getFileInfo(DFSClient.java:548) 
at org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:380) 
at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:598) 
at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:106) 
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:366) 
at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2126) 


Then the whole reduce task failed, and all the work of shuflling and sorting is gone! 

The reduce task should handle this case better. It is worthwile to try a few times before it gives up. 
The stake is too high to give up at the first try. 
"
HADOOP-3195,TestFileSystem fails randomly,"TestFileSystem sometimes fails.  For example, https://issues.apache.org/jira/browse/HADOOP-1911?focusedCommentId=12585857#action_12585857
Add a sleep() to make it more robust.  See also https://issues.apache.org/jira/browse/HADOOP-3139?focusedCommentId=12585821#action_12585821"
HADOOP-3193,Discovery of corrupt block reported in name node log,Any discovery of a corrupt/unreadable block must be reported in the name node log.
HADOOP-3189,Performance benchmark to compare HDFS with local utilities,"Test to compare performance of HDFS with local utilities like {{cat}} and {{dd}}.
"
HADOOP-3187,Quotas for name space management,"Create a quota mechanism for name space management. 

Quota set at a directory by super user restricts that number of names in and below that directory.

Quota tested by create() and rename()."
HADOOP-3186,Incorrect permission checking on  mv,"{noformat}
/data   <dir>           2008-04-04 00:47        rwxr-xr-x       root   users
/data/testdir   <dir>           2008-04-04 23:44        rwxrwxrwx       knoguchi        users
/user/knoguchi/test     <dir>           2008-04-04 23:45        rwxrwxrwx       knoguchi        users

[knoguchi]$  hadoop dfs -mv /user/knoguchi/test /data/testdir
mv: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=knoguchi, access=WRITE, inode=""data"":root:users:rwxr-xr-x
{noformat}

Since target directory, /data/testdir, exists, it should not check the permission of the parent directory"
HADOOP-3184,"HOD gracefully exclude ""bad"" nodes during ring formation","HOD clusters sometimes fail to allocate due to a single ""bad"" node. During ring formation, the entire ring should not be dependent upon every single node being good. Instead, it should either exclude any ring member that does not adequately join the ring in a specified amount of time.

This is a frequent HOD user issue (although not directly caused by HOD).

Examples of bad nodes: Missing java, incorrect version of HOD or Hadoop, local name-cache corrupt, slow network links, drives just beginning to fail, etc.

Many of these conditions are known, and we can monitor for those separately, but this enhancement would shield users from unknown failure conditions that we haven't yet anticipated. This way, a user will get a cluster, instead of hanging indefinitely.
"
HADOOP-3183,Unit test fails on Windows: TestJobShell.testJobShell,"Unit test fails on Windows: TestJobShell.testJobShell

junit.framework.AssertionFailedError: not failed 
	at org.apache.hadoop.mapred.TestJobShell.testJobShell(TestJobShell.java:74)

The MR job is failing:

task_200804021954_0001_m_000001_0: 2008-04-02 19:54:35,795 WARN  conf.Configuration (Configuration.java:loadResource(893)) - C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/mapred/local/1_0/taskTracker/jobcache/job_200804021954_0001/task_200804021954_0001_m_000001_0/job.xml:a attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
task_200804021954_0001_m_000001_0: 2008-04-02 19:54:35,858 INFO  jvm.JvmMetrics (JvmMetrics.java:init(67)) - Initializing JVM Metrics with processName=MAP, sessionId=
task_200804021954_0001_m_000001_0: 2008-04-02 19:54:36,904 INFO  mapred.MapTask (MapTask.java:run(182)) - numReduceTasks: 1
task_200804021954_0001_m_000001_0: 2008-04-02 19:54:40,356 INFO  mapred.TaskRunner (Task.java:done(387)) - Task 'task_200804021954_0001_m_000001_0' done.
2008-04-02 19:54:41,730 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(420)) - Error from task_200804021954_0001_m_000000_0: java.io.IOException: file file_tmpfile not found
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:69)
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:42)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)

2008-04-02 19:54:45,104 INFO  mapred.TaskRunner (MapTaskRunner.java:close(41)) - task_200804021954_0001_m_000000_0 done; removing files.
2008-04-02 19:54:45,244 INFO  mapred.JobTracker (JobTracker.java:removeMarkedTasks(974)) - Removed completed task 'task_200804021954_0001_m_000000_0' from 'tracker_seemeight-dx.ds.corp.yahoo.com:localhost/127.0.0.1:2085'
2008-04-02 19:54:45,385 INFO  mapred.TaskRunner (Task.java:discardTaskOutput(502)) - Discarded output of task 'task_200804021954_0001_m_000000_0' - hdfs://localhost:2052/test/output/_temporary/_task_200804021954_0001_m_000000_0
2008-04-02 19:54:45,556 INFO  mapred.JobClient (JobClient.java:runJob(1046)) - Task Id : task_200804021954_0001_m_000000_0, Status : FAILED
task_200804021954_0001_m_000000_0: 2008-04-02 19:54:38,060 WARN  conf.Configuration (Configuration.java:loadResource(893)) - C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/mapred/local/0_0/taskTracker/jobcache/job_200804021954_0001/task_200804021954_0001_m_000000_0/job.xml:a attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
task_200804021954_0001_m_000000_0: 2008-04-02 19:54:38,185 INFO  jvm.JvmMetrics (JvmMetrics.java:init(67)) - Initializing JVM Metrics with processName=MAP, sessionId=
task_200804021954_0001_m_000000_0: 2008-04-02 19:54:39,028 INFO  mapred.MapTask (MapTask.java:run(182)) - numReduceTasks: 1
task_200804021954_0001_m_000000_0: 2008-04-02 19:54:40,153 WARN  mapred.TaskTracker (TaskTracker.java:main(2118)) - Error running child
task_200804021954_0001_m_000000_0: java.io.IOException: file file_tmpfile not found
task_200804021954_0001_m_000000_0: 	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:69)
task_200804021954_0001_m_000000_0: 	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:42)
task_200804021954_0001_m_000000_0: 	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
task_200804021954_0001_m_000000_0: 	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
task_200804021954_0001_m_000000_0: 	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)
2008-04-02 19:54:45,963 INFO  mapred.JobInProgress (JobInProgress.java:findNewMapTask(1087)) - Choosing rack-local task tip_200804021954_0001_m_000000
2008-04-02 19:54:45,963 INFO  mapred.JobTracker (JobTracker.java:createTaskEntry(879)) - Adding task 'task_200804021954_0001_m_000000_1' to tip tip_200804021954_0001_m_000000, for tracker 'tracker_seemeight-dx.ds.corp.yahoo.com:localhost/127.0.0.1:2094'
2008-04-02 19:54:45,978 INFO  mapred.TaskTracker (TaskTracker.java:startNewTask(1261)) - LaunchTaskAction: task_200804021954_0001_m_000000_1
2008-04-02 19:54:45,994 WARN  conf.Configuration (Configuration.java:loadResource(893)) - C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/mapred/local/1_0/taskTracker/jobcache/job_200804021954_0001/job.xml:a attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
2008-04-02 19:54:46,087 WARN  fs.FileSystem (FileSystem.java:fixName(153)) - ""localhost:2052"" is a deprecated filesystem name. Use ""hdfs://localhost:2052/"" instead.
2008-04-02 19:54:46,228 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1522)) - task_200804021954_0001_r_000000_0 0.16666667% reduce > copy (1 of 2 at 0.00 MB/s) > 
2008-04-02 19:54:47,618 INFO  mapred.JobClient (JobClient.java:runJob(995)) -  map 50% reduce 16%
2008-04-02 19:54:49,180 INFO  dfs.DataNode (DataNode.java:readBlock(1045)) - 127.0.0.1:2064 Served block blk_-5114990675256207114 to /127.0.0.1
2008-04-02 19:54:49,242 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1522)) - task_200804021954_0001_r_000000_0 0.16666667% reduce > copy (1 of 2 at 0.00 MB/s) > 
2008-04-02 19:54:51,085 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(420)) - Error from task_200804021954_0001_m_000000_1: java.io.IOException: file file_tmpfile not found
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:69)
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:42)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)

2008-04-02 19:54:52,241 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1522)) - task_200804021954_0001_r_000000_0 0.16666667% reduce > copy (1 of 2 at 0.00 MB/s) > 
2008-04-02 19:54:53,709 INFO  mapred.TaskRunner (MapTaskRunner.java:close(41)) - task_200804021954_0001_m_000000_1 done; removing files.
2008-04-02 19:54:53,787 INFO  mapred.JobInProgress (JobInProgress.java:findNewMapTask(1084)) - Choosing data-local task tip_200804021954_0001_m_000000
2008-04-02 19:54:53,787 INFO  mapred.JobTracker (JobTracker.java:createTaskEntry(879)) - Adding task 'task_200804021954_0001_m_000000_2' to tip tip_200804021954_0001_m_000000, for tracker 'tracker_seemeight-dx.ds.corp.yahoo.com:localhost/127.0.0.1:2094'
2008-04-02 19:54:53,787 INFO  mapred.JobTracker (JobTracker.java:removeMarkedTasks(974)) - Removed completed task 'task_200804021954_0001_m_000000_1' from 'tracker_seemeight-dx.ds.corp.yahoo.com:localhost/127.0.0.1:2094'
2008-04-02 19:54:53,803 INFO  mapred.TaskTracker (TaskTracker.java:startNewTask(1261)) - LaunchTaskAction: task_200804021954_0001_m_000000_2
2008-04-02 19:54:53,834 WARN  conf.Configuration (Configuration.java:loadResource(893)) - C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/mapred/local/1_0/taskTracker/jobcache/job_200804021954_0001/job.xml:a attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
2008-04-02 19:54:53,897 INFO  mapred.TaskRunner (Task.java:discardTaskOutput(502)) - Discarded output of task 'task_200804021954_0001_m_000000_1' - hdfs://localhost:2052/test/output/_temporary/_task_200804021954_0001_m_000000_1
2008-04-02 19:54:53,975 WARN  fs.FileSystem (FileSystem.java:fixName(153)) - ""localhost:2052"" is a deprecated filesystem name. Use ""hdfs://localhost:2052/"" instead.
2008-04-02 19:54:54,662 INFO  mapred.JobClient (JobClient.java:runJob(1046)) - Task Id : task_200804021954_0001_m_000000_1, Status : FAILED
task_200804021954_0001_m_000000_1: 2008-04-02 19:54:48,383 WARN  conf.Configuration (Configuration.java:loadResource(893)) - C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/mapred/local/1_0/taskTracker/jobcache/job_200804021954_0001/task_200804021954_0001_m_000000_1/job.xml:a attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
task_200804021954_0001_m_000000_1: 2008-04-02 19:54:48,415 INFO  jvm.JvmMetrics (JvmMetrics.java:init(67)) - Initializing JVM Metrics with processName=MAP, sessionId=
task_200804021954_0001_m_000000_1: 2008-04-02 19:54:48,649 INFO  mapred.MapTask (MapTask.java:run(182)) - numReduceTasks: 1
task_200804021954_0001_m_000000_1: 2008-04-02 19:54:49,180 WARN  mapred.TaskTracker (TaskTracker.java:main(2118)) - Error running child
task_200804021954_0001_m_000000_1: java.io.IOException: file file_tmpfile not found
task_200804021954_0001_m_000000_1: 	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:69)
task_200804021954_0001_m_000000_1: 	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:42)
task_200804021954_0001_m_000000_1: 	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
task_200804021954_0001_m_000000_1: 	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
task_200804021954_0001_m_000000_1: 	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)
2008-04-02 19:54:55,240 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1522)) - task_200804021954_0001_r_000000_0 0.16666667% reduce > copy (1 of 2 at 0.00 MB/s) > 
2008-04-02 19:54:56,520 INFO  dfs.DataNode (DataNode.java:readBlock(1045)) - 127.0.0.1:2064 Served block blk_-5114990675256207114 to /127.0.0.1
2008-04-02 19:54:57,223 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(420)) - Error from task_200804021954_0001_m_000000_2: java.io.IOException: file file_tmpfile not found
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:69)
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:42)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)

2008-04-02 19:55:01,065 INFO  mapred.TaskRunner (MapTaskRunner.java:close(41)) - task_200804021954_0001_m_000000_2 done; removing files.
2008-04-02 19:55:01,159 INFO  mapred.JobInProgress (JobInProgress.java:findNewMapTask(1084)) - Choosing data-local task tip_200804021954_0001_m_000000
2008-04-02 19:55:01,159 INFO  mapred.JobTracker (JobTracker.java:createTaskEntry(879)) - Adding task 'task_200804021954_0001_m_000000_3' to tip tip_200804021954_0001_m_000000, for tracker 'tracker_seemeight-dx.ds.corp.yahoo.com:localhost/127.0.0.1:2094'
2008-04-02 19:55:01,159 INFO  mapred.JobTracker (JobTracker.java:removeMarkedTasks(974)) - Removed completed task 'task_200804021954_0001_m_000000_2' from 'tracker_seemeight-dx.ds.corp.yahoo.com:localhost/127.0.0.1:2094'
2008-04-02 19:55:01,175 INFO  mapred.TaskTracker (TaskTracker.java:startNewTask(1261)) - LaunchTaskAction: task_200804021954_0001_m_000000_3
2008-04-02 19:55:01,206 WARN  conf.Configuration (Configuration.java:loadResource(893)) - C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/mapred/local/1_0/taskTracker/jobcache/job_200804021954_0001/job.xml:a attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
2008-04-02 19:55:01,253 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1522)) - task_200804021954_0001_r_000000_0 0.16666667% reduce > copy (1 of 2 at 0.00 MB/s) > 
2008-04-02 19:55:01,284 INFO  mapred.TaskRunner (Task.java:discardTaskOutput(502)) - Discarded output of task 'task_200804021954_0001_m_000000_2' - hdfs://localhost:2052/test/output/_temporary/_task_200804021954_0001_m_000000_2
2008-04-02 19:55:01,346 WARN  fs.FileSystem (FileSystem.java:fixName(153)) - ""localhost:2052"" is a deprecated filesystem name. Use ""hdfs://localhost:2052/"" instead.
2008-04-02 19:55:01,690 INFO  mapred.JobClient (JobClient.java:runJob(1046)) - Task Id : task_200804021954_0001_m_000000_2, Status : FAILED
task_200804021954_0001_m_000000_2: 2008-04-02 19:54:55,708 WARN  conf.Configuration (Configuration.java:loadResource(893)) - C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/mapred/local/1_0/taskTracker/jobcache/job_200804021954_0001/task_200804021954_0001_m_000000_2/job.xml:a attempt to override final parameter: hadoop.tmp.dir;  Ignoring.
task_200804021954_0001_m_000000_2: 2008-04-02 19:54:55,724 INFO  jvm.JvmMetrics (JvmMetrics.java:init(67)) - Initializing JVM Metrics with processName=MAP, sessionId=
task_200804021954_0001_m_000000_2: 2008-04-02 19:54:55,974 INFO  mapred.MapTask (MapTask.java:run(182)) - numReduceTasks: 1
task_200804021954_0001_m_000000_2: 2008-04-02 19:54:56,520 WARN  mapred.TaskTracker (TaskTracker.java:main(2118)) - Error running child
task_200804021954_0001_m_000000_2: java.io.IOException: file file_tmpfile not found
task_200804021954_0001_m_000000_2: 	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:69)
task_200804021954_0001_m_000000_2: 	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:42)
task_200804021954_0001_m_000000_2: 	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
task_200804021954_0001_m_000000_2: 	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
task_200804021954_0001_m_000000_2: 	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)
2008-04-02 19:55:03,673 INFO  dfs.DataNode (DataNode.java:readBlock(1045)) - 127.0.0.1:2064 Served block blk_-5114990675256207114 to /127.0.0.1
2008-04-02 19:55:04,001 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(420)) - Error from task_200804021954_0001_m_000000_3: java.io.IOException: file file_tmpfile not found
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:69)
	at testshell.ExternalMapReduce.map(ExternalMapReduce.java:42)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:219)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)

2008-04-02 19:55:07,250 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1522)) - task_200804021954_0001_r_000000_0 0.16666667% reduce > copy (1 of 2 at 0.00 MB/s) > 
2008-04-02 19:55:08,218 INFO  mapred.TaskRunner (MapTaskRunner.java:close(41)) - task_200804021954_0001_m_000000_3 done; removing files.
2008-04-02 19:55:08,281 INFO  mapred.TaskInProgress (TaskInProgress.java:incompleteSubTask(519)) - TaskInProgress tip_200804021954_0001_m_000000 has failed 4 times.
2008-04-02 19:55:08,281 INFO  mapred.JobInProgress (JobInProgress.java:addTrackerTaskFailure(741)) - TaskTracker at 'tracker_seemeight-dx.ds.corp.yahoo.com' turned 'flaky'
2008-04-02 19:55:08,281 INFO  mapred.JobInProgress (JobInProgress.java:failedTask(1556)) - Aborting job job_200804021954_0001
2008-04-02 19:55:08,312 INFO  dfs.StateChange (FSNamesystem.java:allocateBlock(1332)) - BLOCK* NameSystem.allocateBlock: /test/output/_logs/history/localhost_1207191264285_job_200804021954_0001_hadoopqa_external job. blk_1520406918918394204
2008-04-02 19:55:08,328 INFO  dfs.DataNode (DataNode.java:writeBlock(1078)) - Receiving block blk_1520406918918394204 src: /127.0.0.1:2241 dest: /127.0.0.1:2064
2008-04-02 19:55:08,343 INFO  dfs.DataNode (DataNode.java:writeBlock(1078)) - Receiving block blk_1520406918918394204 src: /127.0.0.1:2242 dest: /127.0.0.1:2066
2008-04-02 19:55:08,359 INFO  dfs.StateChange (FSNamesystem.java:addStoredBlock(2833)) - BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:2066 is added to blk_1520406918918394204 size 3263
2008-04-02 19:55:08,359 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1943)) - Received block blk_1520406918918394204 of size 3263 from /127.0.0.1
2008-04-02 19:55:08,359 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1961)) - PacketResponder 0 for block blk_1520406918918394204 terminating
2008-04-02 19:55:08,375 INFO  dfs.StateChange (FSNamesystem.java:addStoredBlock(2833)) - BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:2064 is added to blk_1520406918918394204 size 3263
2008-04-02 19:55:08,375 INFO  dfs.DataNode (DataNode.java:run(2037)) - Received block blk_1520406918918394204 of size 3263 from /127.0.0.1
2008-04-02 19:55:08,390 INFO  dfs.DataNode (DataNode.java:run(2095)) - PacketResponder 1 for block blk_1520406918918394204 terminating
2008-04-02 19:55:08,437 INFO  mapred.JobInProgress (JobInProgress.java:kill(1409)) - Killing job 'job_200804021954_0001'"
HADOOP-3182,JobClient creates submitJobDir with SYSTEM_DIR_PERMISSION ( rwx-wx-wx),"JobClient creates submitJobDir with SYSTEM_DIR_PERMISSION ( rwx-wx-wx ) which causes problem while sharing a cluster.
Consider the case where userA starts jobtracker/tasktrackers and userB submits a job to this cluster. When userB creates submitJobDir it is created with rwx-wx-wx which cannot be read by tasktracker started by userA"
HADOOP-3180,add name of faulty class to WritableName.getClass IOException upon ClassNotFoundException ,"WritableName.getClass raises an IOException when it cannot load the class. The message is ""WritableName can't load class"" without the name of the class.  Although one can get this from getCause, easier to see it just from the message.

propose change it to:

      IOException newE = new IOException(""WritableName can't load class: "" + name);

I will attach a patch.
"
HADOOP-3178,gridmix scripts for small and medium jobs need to be changed to handle input paths differently,"The gridmix scripts failed to run small and medium jobs as tracked in  HADOOP-3162. Based on that fix, the gridmix scripts need to be changed to handle input paths differently.

The scripts that submit the small and medium jobs need to be changed. Example of a change:
${VARINFLTEXT}/part-00000,${VARINFLTEXT}/part-00001,${VARINFLTEXT}/part-00002
nees to be changed to
${VARINFLTEXT}/{part-00000,part-00001,part-00002}"
HADOOP-3177,Expose DFSOutputStream.fsync API though the FileSystem interface,"In the current code, there is a DFSOutputStream.fsync() API that allows a client to flush all buffered data to the datanodes and also persist block locations on the namenode. This API should be exposed through the generic API in the org.hadoop.fs."
HADOOP-3176,Change lease record when a open-for-write-file gets renamed,"When a file/directory is removed, the namenode should repair the leases so that the existing leases point to the  appropriate file. The lease record has the pathname of the file that it refers to.

This is required because the namenode has to invoke lease-recovery on the correct file if the client that was writing to a file dies.

"
HADOOP-3175,"""-get file -"" does not work","{noformat}
$ bin/hadoop fs -get file - > /dev/null
Usage: java FsShell -get [-ignoreCrc] [-crc] <src> <localdst>
get: Illegal option -
{noformat}
In 0.16, it used to write 'file' to stdout.
"
HADOOP-3174,Improve documentation and supply an example for MultiFileInputFormat,"MultiFileInputFormat has been discussed several times in mailing lists, and it is clear that documentation could be improved and an example showing the usage of it could be helpful. "
HADOOP-3173,inconsistent globbing support for dfs commands,"hadoop dfs -mkdir /user/*/bar creates a directory ""/user/*/bar"" and you cant deleted /user/* as -rmr expands the glob

$ hadoop dfs -mkdir /user/rajive/a/*/foo
$ hadoop dfs -ls /user/rajive/a
Found 4 items
/user/rajive/a/*	<dir>		2008-04-04 16:09	rwx------	rajive	users
/user/rajive/a/b	<dir>		2008-04-04 16:08	rwx------	rajive	users
/user/rajive/a/c	<dir>		2008-04-04 16:08	rwx------	rajive	users
/user/rajive/a/d	<dir>		2008-04-04 16:08	rwx------	rajive	users

$ hadoop dfs -ls /user/rajive/a/*
/user/rajive/a/*/foo	<dir>		2008-04-04 16:09	rwx------	rajive	users

$ hadoop dfs -rmr /user/rajive/a/*
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/*
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/b
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/c
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/d


I am not able to escape '*' from being expanded.

$ hadoop dfs -rmr '/user/rajive/a/*'
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/*
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/b
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/c
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/d

$ hadoop dfs -rmr  '/user/rajive/a/\*'
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/*
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/b
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/c
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/d

$ hadoop dfs -rmr  /user/rajive/a/\* 
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/*
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/b
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/c
Moved to trash: hdfs://namenode-1:8020/user/rajive/a/d


"
HADOOP-3171,speculative execution should not have default value on hadoop-default.xml bundled in the Hadoop JAR,"Having a default value for speculative execution in the hadoop-default.xml bundled in the Hadoop JAR file does not allow a cluster to control the default behavior. 

*ON in hadoop-default.xml (current behavior)*

* ON in JT hadoop-site.xml
 * present in job.xml, job's value is used
 * not-present in job.xml, ON is taken as default from the hadoop-default.xml present in the client's JAR/conf (*)
* ON FINAL in the JT hadoop-site.xml
 * present or not present in the job.xml, ON is used
* OFF in JT hadoop-site.xml
 * present in job.xml, job's value is used
 * not-present in job.xml, ON is taken as default from the hadoop-default.xml present in the client's JAR/conf
* OF FINAL in the JT hadoop-site.xml
 * present or not present in the job.xml, OFF is used

*OFF in hadoop-default.xml (not current behavior)*

* ON in JT hadoop-site.xml
 * present in job.xml, job's value is used
 * not-present in job.xml, OFF is taken as default from the hadoop-default.xml present in the client's JAR/conf (*)
* ON FINAL in the JT hadoop-site.xml
 * present or not present in the job.xml, ON is used
* OFF in JT hadoop-site.xml
 * present in job.xml, job's value is used
 * not-present in job.xml, ON is taken as default from the hadoop-default.xml present in the client's JAR/conf
* OF FINAL in the JT hadoop-site.xml
 * present or not present in the job.xml, OFF is used

---

Still is desirable for the JT to have a default value. To avoid having to support 2 hadoop-default.xml files, one for the JT and other for the clients, the easiest why is to remove it from the hadoop-default.xml and have the default value in the code when getting the config property (thing that may be already happening).
"
HADOOP-3170,FileSystem.getUri().resolve() should handle Path created on Windows,"FileSystem.getUri() returns URI constructed from string ""file:///"" This when applied to absolute Paths created on Windows breaks. For example when using a patch constructed like this with DistributedCache(). makeAbsolute(Path) seems to return path with drive letter followed by colon. URI.resolve() treats this part as authority for local filesystem on windows. "
HADOOP-3169,LeaseChecker daemon should not be started in DFSClient constructor,"LeaseChecker daemon periodically renew leases with NameNode.  However, if there is no file creation, LeaseChecker daemon should not be started for saving resources and better performance."
HADOOP-3168,reduce amount of logging in hadoop streaming,"logs every 100 records:

  void maybeLogRecord() {
    if (numRecRead_ >= nextRecReadLog_) {
      String info = numRecInfo();
      logprintln(info);
      logflush();
      //nextRecReadLog_ *= 10;                                                                         
      nextRecReadLog_ += 100;
    }
  }

ouch."
HADOOP-3167,"NameNode to report frequency, length, and type of garbage collection","Right now, we don't know for how long, nor how frequently, the Hadoop NameNode engages in garbage collection. We can re-constitute this data from logs, but it'd be much better if Hadoop could report this as a metric along with all other metrics reported, so we can graph these alongside.
"
HADOOP-3166,"SpillThread throws ArrayIndexOutOfBoundsException, which is ignored by MapTask","There is a flaw in the value bounds checking introduced by HADOOP-2919. When the index table determines the end offset of the value, it fails to detect that the next entry falls off the end of the array. This remains undetected by MapTask, which only notes IOExceptions from the spill thread. Both the boundary checking and the exception handling must be fixed."
HADOOP-3165,FsShell no longer accepts stdin as a source for -put/-copyFromLocal,"If ""-"" is specified as the source, then -put/-copyFromLocal should take input from stdin as in previous versions."
HADOOP-3164,Use FileChannel.transferTo() when data is read from DataNode.,"HADOOP-2312 talks about using FileChannel's [{{transferTo()}}|http://java.sun.com/javase/6/docs/api/java/nio/channels/FileChannel.html#transferTo(long,%20long,%20java.nio.channels.WritableByteChannel)] and [{{transferFrom()}}|http://java.sun.com/javase/6/docs/api/java/nio/channels/FileChannel.html#transferFrom(java.nio.channels.ReadableByteChannel,%20long,%20long)] in DataNode. 

At the time DataNode neither used NIO sockets nor wrote large chunks of contiguous block data to socket. Hadoop 0.17 does both when data is seved to clients (and other datanodes). I am planning to try using transferTo() in the trunk. This might reduce DataNode's cpu by another 50% or more.

Once HADOOP-1702 is committed, we can look into using transferFrom().
"
HADOOP-3162,Map/reduce stops working with comma separated input paths,"When a job is given a comma separated input file list, FileInputFormat class throws an exception, complaining the input is invalid:

org.apache.hadoop.mapred.InvalidInputException: Input path doesnt exist : hdfs:/
namenode:port/gridmix/data/MonsterQueryBlockCompressed/part-
00000,/gridmix/data/MonsterQueryBlockCompressed/part-00001,/gridmix/data/Monster
QueryBlockCompressed/part-00002
        at org.apache.hadoop.mapred.FileInputFormat.validateInput(FileInputForma
t.java:213)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:705)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:973)
        at org.apache.hadoop.mapred.GenericMRLoadGenerator.run(GenericMRLoadGene
rator.java:189)
"
HADOOP-3161,TestFileAppend fails on Mac since HADOOP-2655 was committed,"org.apache.hadoop.dfs.TestFileAppend.testCopyOnWrite fails with this exception:

java.io.IOException: stat: illegal option -- c
	at org.apache.hadoop.fs.FileUtil$HardLink.getLinkCount(FileUtil.java:557)
	at org.apache.hadoop.dfs.DatanodeBlockInfo.detachBlock(DatanodeBlockInfo.java:117)
	at org.apache.hadoop.dfs.FSDataset.detachBlock(FSDataset.java:652)
	at org.apache.hadoop.dfs.TestFileAppend.testCopyOnWrite(TestFileAppend.java:188)

Mac command should be
stat -f$l"
HADOOP-3160,remove exists() from ClientProtocol and NameNode,Following HADOOP-2634 please remove exists() from ClientProtocol and NameNode in 0.18
HADOOP-3159,FileSystem cache keep overwriting cached value,"Consider the following:
{code}
Configuration conf1 = new Configuration();
FileSystem fs1 = FileSystem.get(conf1); //conf1 may be modified, the cache key stored will use the modified values.

Configuration conf2 = new Configuration(); //create another conf, which won't have the modified values.
FileSystem fs2 = FileSystem.get(conf2); //may initialize another FileSystem instead of returning the cached FileSystem, since conf2 does not have the modified values initially.
{code}
Therefore, FileSystem.get(conf) may keeps creating FileSystem and replaces the cached one."
HADOOP-3157,TestMiniMRLocalFS fails in trunk on Windows,"TestMiniMRLocalFS fails with this stacktrace on Windows

Testcase: testWithLocal took 299.637 sec
  Caused an ERROR
No FileSystem for scheme: C
java.io.IOException: No FileSystem for scheme: C
  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1278)
  at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:53)
  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1292)
  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:191)
  at org.apache.hadoop.filecache.DistributedCache.getTimestamp(DistributedCache.java:414)
  at org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobClient.java:605)
  at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:700)
  at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:973)
  at org.apache.hadoop.mapred.MRCaching.launchMRCache(MRCaching.java:196)
  at org.apache.hadoop.mapred.TestMiniMRLocalFS.testWithLocal(TestMiniMRLocalFS.java:56)

"
HADOOP-3155,reducers stuck at shuffling ,"This happened with hadoop-0.16.2:

In relatively small job (a few hundreds of mappers and reducers), reducers were stuck at shuffling.
I saw the lines like the following repeated hundreds of thousands of times over a few hours:

2008-04-02 17:17:44,640 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:44,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:44,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:44,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:17:46,643 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:46,643 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:46,643 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:46,643 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:17:48,645 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:48,645 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:48,645 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:48,645 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:17:50,647 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:50,647 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:50,647 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:50,647 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:17:52,649 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:52,650 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:52,650 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:52,650 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:17:54,651 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:54,652 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:54,652 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:54,652 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:17:56,654 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:56,654 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:56,654 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:56,654 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:17:58,656 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:58,656 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:58,656 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:58,656 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:00,658 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:00,658 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:00,658 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:00,658 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:02,660 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:02,661 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:02,661 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:02,661 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:04,662 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:04,663 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:04,663 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:04,663 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:06,664 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:06,665 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:06,665 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:06,665 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:08,667 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:08,667 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:08,667 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:08,667 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:10,669 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:10,669 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:10,669 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:10,669 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:12,671 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:12,671 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:12,671 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:12,671 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:14,673 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:14,674 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:14,674 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:14,674 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:16,675 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:16,676 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:16,676 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:16,676 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:18,678 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:18,678 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:18,678 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:18,678 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:20,680 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:20,680 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:20,680 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:18:20,680 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts)
2008-04-02 17:18:22,682 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:18:22,682 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:18:22,682 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
"
HADOOP-3154,Job successful but dropping records (when disk full),"I have a mapreduce code that takes an input and just shuffles.
# of input should be equal to # of output. 

However, when disks of the nodes were filled accidentally, I started to see some records dropping, although jobs themselves were successful.

{noformat}
08/03/30 00:17:04 INFO mapred.JobClient: Job complete: job_200803292134_0001
08/03/30 00:17:04 INFO mapred.JobClient: Counters: 11
08/03/30 00:17:04 INFO mapred.JobClient:   Job Counters
08/03/30 00:17:04 INFO mapred.JobClient:     Launched map tasks=23
08/03/30 00:17:04 INFO mapred.JobClient:     Launched reduce tasks=4
08/03/30 00:17:04 INFO mapred.JobClient:   Map-Reduce Framework

08/03/30 00:17:04 INFO mapred.JobClient:   Map-Reduce Framework
08/03/30 00:17:04 INFO mapred.JobClient:     Map input records=6852926
08/03/30 00:17:04 INFO mapred.JobClient:     Map output records=6852926
08/03/30 00:17:04 INFO mapred.JobClient:     Map input bytes=18802382982
08/03/30 00:17:04 INFO mapred.JobClient:     Map output bytes=21278202852
08/03/30 00:17:04 INFO mapred.JobClient:     Combine input records=0
08/03/30 00:17:04 INFO mapred.JobClient:     Combine output records=0
08/03/30 00:17:04 INFO mapred.JobClient:     Reduce input groups=6722633
08/03/30 00:17:04 INFO mapred.JobClient:     Reduce input records=6839731
08/03/30 00:17:04 INFO mapred.JobClient:     Reduce output records=6839731
{noformat}

"
HADOOP-3153,[HOD] Hod should deallocate cluster if there's a problem in writing information to the state file,"Consider a scenario where hod runs allocate successfully, but isn't able to save teh allocated information to the clusters.state file. In such a case, it gets an error and exits. But the cluster remains allocated, and unfortunately the user cannot deallocate the cluster now unless he knows the cluster directory.

It is better if HOD can deallocate the cluster in such an error condition."
HADOOP-3152,Make index interval configuable when using MapFileOutputFormat for map-reduce job,"Per discussion with Doug Cutting on hadoop user mailing around Mar 21,
thread title ""MapFile and MapFileOutputFormat"". Currently, there is no way 
to change the index interval for the output MapFile in a map-reduce job.
As suggested, adding a static method MapFile(Configuration, int) to set the
index interval and stores in Configuration, then MapFile.Writer constructor
reads the setting from configuration may be a good idea. 

I also noticed that Hbase did similar things in HBASE-364."
HADOOP-3151,Hod should have better error messages.,"(1)
gsgw1037:/homes/arkady/CDSR> hod allocate -n 4 -d xx
error: bin/hod failed to start.
error: invalid 'clusterdir' specified in section hod (--hod.clusterdir): xx
error: 1 problem found.
Check your command line options and/or your configuration file /grid/0/kryptonite/hod/conf/hodrc

   * Directory 'xx' does not exist - the message shuold say ""Non-existing directory"" (or ""directory not found""), not ""invalid""
   * checking the configuration file will not help, the advice is confusing
   * checking the command line options is the obvious thing to do -- the advise is unnecessary.

(2) when hod does not understand the command, it prints the whole help options message -- which clobbers the screen and is confusing. It should give a meaningful message -- and may say ""type hod help for more info""


"
HADOOP-3150,Move task file promotion into the task,We need to move the task file promotion from the JobTracker to the Task and move it down into the output format.
HADOOP-3149,supporting multiple outputs for M/R jobs,"The outputcollector supports writing data to a single output, the 'part' files in the output path.

We found quite common that our M/R jobs have to write data to different output. For example when classifying data as NEW, UPDATE, DELETE, NO-CHANGE to later do different processing on it.

Handling the initialization of additional outputs from within the M/R code complicates the code and is counter intuitive with the notion of job configuration.

It would be desirable to:

# Configure the additional outputs in the jobconf, potentially specifying different outputformats, key and value classes for each one.
# Write to the additional outputs in a similar way as data is written to the outputcollector.
# Support the speculative execution semantics for the output files, only visible in the final output for promoted tasks.


To support multiple outputs the following classes would be added to mapred/lib:

* {{MOJobConf}} : extends {{JobConf}} adding methods to define named outputs (name, outputformat, key class, value class)
* {{MOOutputCollector}} : extends {{OutputCollector}} adding a {{collect(String outputName, WritableComparable key, Writable value)}} method.
* {{MOMapper}} and {{MOReducer}}: implement {{Mapper}} and {{Reducer}} adding a new {{configure}}, {{map}} and {{reduce}} signature that take the corresponding {{MO}} classes and performs the proper initialization.

The data flow behavior would be: key/values written to the default (unnamed) output (using the original OutputCollector {{collect}} signature) take part of the shuffle/sort/reduce processing phases. key/values written to a named output from within a map don't.

The named output files would be named using the task type and task ID to avoid collision among tasks (i.e. 'new-m-00002' and 'new-r-00001').

Together with the setInputPathFilter feature introduced by HADOOP-2055 it would become very easy to chain jobs working on particular named outputs within a single directory.

We are using heavily this pattern and it greatly simplified our M/R code as well as chaining different M/R. 

We wanted to contribute this back to Hadoop as we think is a generic feature many could benefit from.
"
HADOOP-3147,Wildcard handling in DFS is buggy,"I tried the following sequence of operations 
1) bin/hadoop namenode -format
2) bin/start-all.sh
3) bin/hadoop dfs -mkdir input
4) bin/hadoop dfs -rmr in*   
 _output_ :   rmr: cannot remove index.html: No such file or directory.
5) bin/hadoop dfs -rmr inp*   
 _output_ :  Deleted hdfs://localhost:9000/user/name/input
----
Btw how come _index.html_ occurred there?"
HADOOP-3146,DFSOutputStream.flush should be renamed as DFSOutputStream.fsync,"Starting fron hadoop-0.17, calling flush of dfs becomes very expensive.
The current implementation of flush should be renamed as sync.
"
HADOOP-3144,better fault tolerance for corrupted text files,"every once in a while - we encounter corrupted text files (corrupted at source prior to copying into hadoop). inevitably - some of the data looks like a really really long line and hadoop trips over trying to stuff it into an in memory object and gets outofmem error. Code looks same way in trunk as well .. 

so looking for an option to the textinputformat (and like) to ignore long lines. ideally - we would just skip errant lines above a certain size limit."
HADOOP-3143,Decrease the number of slaves in TestMiniMRDFSSort to 3.,Hudson.zones gets pretty overloaded making TestMiniMRDFSSort flaky. I think we should drop the number of slaves (ie. DataNodes and TaskTrackers) from 4 to 3.
HADOOP-3141,periodic ConcurrentModificationException in IPC Server,"Seeing this exception in the Namenode log (429 of them yesterday alone)

2008-03-30 00:00:32,332 WARN org.apache.hadoop.ipc.Server: Exception in Responder java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$KeyIterator.next(HashMap.java:827)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:475)"
HADOOP-3140,JobTracker should not try to promote a (map) task if it does not write to DFS at all,"In most cases, map tasks do not write to dfs.
Thus, when they complete, they should not be put into commit_pending queue at all.
This will improve the task promotion significantly.

 "
HADOOP-3139,DistributedFileSystem.close() deadlock and FileSystem.closeAll() warning,"Koji found the following:

*DistributedFileSystem.close() deadlock*
My dfs -ls hang. 
Ctrl-Z showed a deadlock state.
{noformat}
""Thread-0"":
        at org.apache.hadoop.dfs.DistributedFileSystem.close(DistributedFileSystem.java:190)
        - waiting to lock <0xedde8788> (a org.apache.hadoop.dfs.DistributedFileSystem)
        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:1231)
        - locked <0xee0baf88> (a org.apache.hadoop.fs.FileSystem$Cache)
        at org.apache.hadoop.fs.FileSystem.closeAll(FileSystem.java:169)
        at org.apache.hadoop.fs.FileSystem$ClientFinalizer.run(FileSystem.java:154)
        - locked <0xee0bae40> (a org.apache.hadoop.fs.FileSystem$ClientFinalizer)
""main"":
        at org.apache.hadoop.fs.FileSystem$Cache.remove(FileSystem.java:1201)
        - waiting to lock <0xee0baf88> (a org.apache.hadoop.fs.FileSystem$Cache)
        at org.apache.hadoop.fs.FileSystem.close(FileSystem.java:1085)
        at org.apache.hadoop.dfs.DistributedFileSystem.close(DistributedFileSystem.java:192)
        - locked <0xedde8788> (a org.apache.hadoop.dfs.DistributedFileSystem)
        at org.apache.hadoop.fs.FsShell.close(FsShell.java:1698)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:1712)

Found 1 deadlock.
{noformat}

*FileSystem.closeAll() warning*
{noformat}
08/03/31 16:48:42 INFO fs.FileSystem: FileSystem.closeAll() threw an exception:
java.io.IOException: HftpFileSystem(=org.apache.hadoop.dfs.HftpFileSystem@111111) and
Key(=null@hftp://namenode-nn:4444) do not match.
{noformat}"
HADOOP-3138,distcp fail copying to /user/<username>/<newtarget> (with permission on),"When distcp-ing to /user/<username>/<newtarget>, I get an error with 

Copy failed: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=knoguchi, access=WRITE, inode=""user"":superuser:superusergroup:rwxr-xr-x

{noformat} 
        at org.apache.hadoop.dfs.PermissionChecker.check(PermissionChecker.java:173)
        at org.apache.hadoop.dfs.PermissionChecker.check(PermissionChecker.java:154)
        at org.apache.hadoop.dfs.PermissionChecker.checkPermission(PermissionChecker.java:102)
        at org.apache.hadoop.dfs.FSNamesystem.checkPermission(FSNamesystem.java:4037)
        at org.apache.hadoop.dfs.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4007)
        at org.apache.hadoop.dfs.FSNamesystem.mkdirsInternal(FSNamesystem.java:1576)
        at org.apache.hadoop.dfs.FSNamesystem.mkdirs(FSNamesystem.java:1559)
        at org.apache.hadoop.dfs.NameNode.mkdirs(NameNode.java:422)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:899)

        at org.apache.hadoop.ipc.Client.call(Client.java:512)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
        at org.apache.hadoop.dfs.$Proxy0.mkdirs(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy0.mkdirs(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient.mkdirs(DFSClient.java:550)
        at org.apache.hadoop.dfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:184)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:980)
        at org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:735)
        at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:525)
        at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:596)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:612)
{noformat} 

In distcp set up, we have 
{noformat} 
      if (!dstExists || !dstIsDir) {
        Path parent = destPath.getParent();
        dstfs.mkdirs(parent);
        logPath = new Path(parent, filename);
      }
{noformat} 
We should check if parent path exists before calling mkdir? 
"
HADOOP-3137,[HOD] Update hod version number,At a minimum update the version of hod in the VERSION file to indicate new version. The best solution is if we can pick it automated from Hadoop build.
HADOOP-3136,Assign multiple tasks per TaskTracker heartbeat,"In today's logic of finding a new task, we assign only one task per heartbeat.

We probably could give the tasktracker multiple tasks subject to the max number of free slots it has - for maps we could assign it data local tasks. We could probably run some logic to decide what to give it if we run out of data local tasks (e.g., tasks from overloaded racks, tasks that have least locality, etc.). In addition to maps, if it has reduce slots free, we could give it reduce task(s) as well. Again for reduces we could probably run some logic to give more tasks to nodes that are closer to nodes running most maps (assuming data generated is proportional to the number of maps). For e.g., if rack1 has 70% of the input splits, and we know that most maps are data/rack local, we try to schedule ~70% of the reducers there.

Thoughts?"
HADOOP-3135,if the 'mapred.system.dir' in the client jobconf is different from the JobTracker's value job submission fails,"Until Hadoop 0.13 or so, at submission time the full path of the job.xml and all supporting files in DFS was given by the client to the jobtracker.

Since 0.15 onwards (we did not test 0.14) the jobclient is obtaining the job ID from the jobtracker and creating the directory for all the supporting files using the a system-dir computed from the local jobconf.

Line 696-7 in the JobClient:

    String jobId = jobSubmitClient.getNewJobId();
    Path submitJobDir = new Path(job.getSystemDir(), jobId);

This makes submissions to fail when the value of the 'mapred.system.dir' on the client is different from the one in the JobTracker.

A simple way o fixing this would be to introduce a new method in the JobSubmissionProtocol 'getSystemDir()' that would return the jobtracker system dir and use that dir for uploading all the files on submission.

----
For the future: A more comprehensive way of this doing would to obtain a base jobConf from the jobtracker, carrying final information for each element and the construct the job.xml on the client using the final semantics. And, in this case the 'mapred.system.dir' property should be set as final in the jobtracker. As there may be some configuration properties that are sensitive and for security reasons should not be exposed to the clients a new flag 'private' could be introduced and only properties that don't have the 'private' flag would be send over from the jobtracker to the jobclient for job.xml resolution."
HADOOP-3132,DFS writes stuck occationally,"This problem happens in 0.17 trunk

As reported in hadoop-3124,
I saw reducers waited 10 minutes for writing data to dfs and got timeout.
The client retries again and timeouted after another 19 minutes.

During the period of write stuck, all the nodes in the data node pipeline were functioning fine.
The system load was normal.
I don't believe this was due to slow network cards/disk drives or overloaded machines.
I believe this and hadoop-3033 are related somehow.
"
HADOOP-3131,enabling BLOCK compression for map outputs breaks the reduce progress counters,"Enabling map output compression and setting the compression type to BLOCK causes the progress counters during the reduce to go crazy and report progress counts over 100%.

This is problematic for speculative execution because it thinks the tasks are doing fine."
HADOOP-3130,Shuffling takes too long to get the last map output.,"
I noticed that towards the end of shufflling, the map output fetcher of the reducer backs off too aggressively.
I attach a fraction of one reduce log of my job.
Noticed that the last map output was not fetched in 2 minutes.

"
HADOOP-3129,[hod] hod leaves mapredsystem directories,"In hod, mapredsystem dir is created for each torque job using the torque jobid.

I believe we now create it under /user/<username>/mapredsystem/<torque jobid> 

Files are deleted after job is done, but the directory remain untouched.
Hod should delete these directories when the cluster is deallocated."
HADOOP-3128,TestDFSPermission due to not throwing exception.,"This is the consequence of HADOOP-3108.
setPermission nad setOwner should throw RemoteException if it is not FileNotFoundException."
HADOOP-3127,rm /user/<username>/.Trash/____ only moves it back to .Trash ,"In 0.15 or before, any dfs -rm call for  files under Trash were deleted. 
From 0.16, it just renames it back to Trash.

In Trash.java:moveToTrash() 
{noformat}
if (path.toString().startsWith(trash.toString()))
{noformat}

seems like trash.toString() is fully qualified, and path.toString() is not."
HADOOP-3124,DFS data node should not use hard coded 10 minutes as write timeout.,"This problem happens in 0.17 trunk

I saw reducers waited 10 minutes for writing data to dfs and got timeout.
The client retries again and timeouted after another 19 minutes.

After looking into the code, it seems that the dfs data node uses 10 minutes as timeout for wtiting data into the data node pipeline.
I thing we have three issues:

1. The 10 minutes timeout value is too big for writing a chunk of data (64K) through the data node pipeline.
2. The timeout value should not be hard coded.
3. Different datanodes in a pipeline should use different timeout values for writing to the downstream.
A reasonable one maybe (20 secs * numOfDataNodesInTheDownStreamPipe).
For example, if the replication factor is 3, the client uses 60 secs, the first data node use 40 secs, the second datanode use 20secs.
"
HADOOP-3123,Build native libraries on Solaris,The current native build scripts need to be changed to run on Solaris.
HADOOP-3121,"dfs -lsr fail with ""Could not get listing ""","(This happened a lot when namenode was extremely slow due to some other reasons.)

% hadoop dfs -lsr /   

failed with 

> Could not get listing for /aaa/bbb/randomfile

It's probably because  file  was deleted between  items = fs.listStatus( and ls(items[i] 

I think lsr should ignore this case and continue.
"
HADOOP-3119,Text.getBytes(),"As said in javadoc, Text.getBytes() returns ""raw bytes"", but nothing said that it returns *internal* array which length can be greater than Text.getLength().


Maybe to provide toArray method, which will create new array of actual data?"
HADOOP-3118,Namenode NPE while loading fsimage after a cluster upgrade from older disk format,"FSDirectory.unprotectedDelete: failed to remove /user/ran gadi/10Mb because it does not exist
2008-03-27 22:00:10,904 ERROR org.apache.hadoop.dfs.NameNode: 
java.lang.NullPointerException
         at
org.apache.hadoop.dfs.StringBytesWritable.<init>(StringBytesWritable.java:39)
         at
org.apache.hadoop.dfs.INodeFileUnderConstruction.<init>(INode.java:795)
         at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:528)
         at org.apache.hadoop.dfs.FSImage.loadFSEdits(FSImage.java:766)
         at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:640)
         at org.apache.hadoop.dfs.FSImage.doUpgrade(FSImage.java:250)
         at
org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:217)
         at
org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:80)
         at
org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:274)
         at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:255)
         at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:133)
         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
         at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:846)
         at org.apache.hadoop.dfs.NameNode.main(NameNode.java:855)

"
HADOOP-3117,Speculative execution configuration is confusing,"In Hadoop 0.16 the single switch for speculative execution was replaced by 2 switches for maps and reduces. The old switch was retained as well and overrides the new switches if it is turned on. This is confusing. We should go back to a single config variable that has one value from the set  -- 'true, false, map, reduce'.

"
HADOOP-3115,Remove javadoc warnigns in trunk due to FileChannel,"trunk has javadoc warnings :
src/java/org/apache/hadoop/net/SocketInputStream.java:137: warning - Tag @link: reference not found: FileChannel#transferFrom(ReadableByteChannel, long, long)
src/java/org/apache/hadoop/net/SocketOutputStream.java:132: warning - Tag @link: reference not found: FileChannel#transferTo(long, long, WritableByteChannel)

These needs to be removed."
HADOOP-3114,TestDFSShell fails on Windows.,"TestDFSShell.testPut() fails with the following exception:
{code}
Pathname /C:/Hadoop/build/test/data/f1 from C:/Hadoop/build/test/data/f1 is not a valid DFS filename.
java.lang.IllegalArgumentException: Pathname /C:/Hadoop/build/test/data/f1 from C:/Hadoop/build/test/data/f1 is not a valid DFS filename.
	at org.apache.hadoop.dfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:112)
	at org.apache.hadoop.dfs.DistributedFileSystem.exists(DistributedFileSystem.java:184)
	at org.apache.hadoop.dfs.TestDFSShell.testPut(TestDFSShell.java:214)
{code}
Looks like that Path treats windows drive letter incorrectly.
Not sure which path introduced it, but needs to be fixed for 0.17 imo."
HADOOP-3113,DFSOututStream.flush() should flush data to real block file on DataNode.,"DFSOutputStream has a method called flush() that persists block locations on the namenode and sends all outstanding data to all datanodes in the pipeline. However, this data goes to the tmp file on the datanode(s). When the block is closed, the tmp files is renamed to be the real block file. If the datanode(s) dies before the block is compete, then entire block is lost. This behaviour wil be fixed in HADOOP-1700.

However, in the short term, a configuration paramater can be used to allow datanodes to write to the real block file directly, thereby avoiding writing to the tmp file. This means that data that is flushed successfully by a client does not get lost even if the datanode(s) or client dies.

The Namenode already has code to pick the largest replica (if multiple datanodes have different sizes of this block). Also, the namenode has code to not trigger replication request if the file is still being written to.

The only caveat that I can think of is that the block report periodicity should be much much smaller that the lease timeout period. A block report adds the being-written-to blocks to the blocksMap thereby avoiding any cleanup that a lease expiry processing might have otherwise done.

Not all requirements specified by HADOOP-1700 are supported by this approach, but it could still be helpful (in the short term) for a wide range of applications.



"
HADOOP-3112,Distcp fails with null pointer exception.,"Copy failed: java.lang.NullPointerException
        at org.apache.hadoop.io.serializer.SerializationFactory.<init>(SerializationFactory.java:52)
        at org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:910)
        at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:846)
        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:394)
        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:285)
        at org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:743)
        at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:520)
        at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:591)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:607)

this is the null pointer exception you get when using 0.17 to distcp.

i was running using hod 
with hadoop --config hoddir distcp .....................
After investigating further with help from chris, we found out that it is using conf.getStrings which does not have a default. Also, hadoop default conf directory is not added to the classpath  when we run bin/hadoop on the client side. 
On copying hadoop-default.xml to my config directory it worked fine. 

we can add hadoop default conf directory to the classpath? or  use a default in the call for conf.getStrings(""io.serializations"")?

I am marking this as a blocker for someone to take a look. "
HADOOP-3111,Remove HBase from Hadoop contrib,"HBase will be releasing version 0.1.0 probably tomorrow (3/28) It is functionally equivalent to the code in Hadoop-contrib-hbase except that numerous bugs have been fixed. No new patches have been applied to
Hadoop-contrib-hbase.

"
HADOOP-3110,NameNode does logging in critical sections just about everywhere,"e.g., FSNameSystem.addStoredBlock (but almost every method has logging in its critical sections)

This method is synchronized and it's spitting something out to Log.info every block stored. Normally not a big deal, but since this is in the name node and these are critical sections...

We shouldn't even do any logging at all in critical sections, so even the info and warn are bad.  But, in many places in the code, it would be hard to tease these out (although eventually they really should be), but the system could start using something like an AsyncAppender and see how it improves performance. 

Even though the log may have a buffer, the writing and doing the formatting and stuff cause a drag on performance with 100s/1000s of machines trying to talk to the name node.

At a minimum, the most often  triggered Log.info could be changed to Log.debug.

for reference: http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/AsyncAppender.html"
HADOOP-3109,RPC should accepted connections even when rpc queue is full (ie undo part of HADOOP-2910),"HADOOP-2910 changed HDFS to stop accepting new connections when the rpc queue is full. It should continue to accept connections and let the OS  deal with limiting connections.

HADOOP-2910's decision to not read from open sockets when queue is full is exactly right -  backup on the
client sockets and they will just wait( especially with HADOOP-2188 that removes client timeouts).
However we should continue to  accept connections:

The OS refuses new connections after a large number of connections are open (this is configurable parameter). With this patch, we have  new lower limit for # of open connections when the RPC queue is full.
The problem is that when there is a surge of requests, we would stop
accepting connection and clients will get a connection failed (a change from old behavior).
Instead if you continue to accept connections it is likely that the surge will be over shortly and
clients will get served. Of course if the surge lasts a long time the OS will stop accepting connections
and clients will fail and there not much one can do (except raise the os limit).

I propose that we continue accepting connections, but not read from
connections when the RPC queue is full. (ie undo part of 2910 work back to the old behavior).
"
HADOOP-3108,NPE in FSDirectory.unprotectedSetPermission,"Not sure if this is fixed in later release, but I'm seeing many NPE in the namenode log.
Permission is disabled on this cluster.
{noformat} 
2008-03-27 03:22:39,984 INFO org.apache.hadoop.ipc.Server: IPC Server handler 18 on 8020, 
call setPermission(/user/knoguchi/file0, rwxr-xr-x) from 99.9.99.9:55555: 
error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSDirectory.unprotectedSetPermission(FSDirectory.java:411)
        at org.apache.hadoop.dfs.FSDirectory.setPermission(FSDirectory.java:405)
        at org.apache.hadoop.dfs.FSNamesystem.setPermission(FSNamesystem.java:716)
        at org.apache.hadoop.dfs.NameNode.setPermission(NameNode.java:297)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:899)
{noformat} "
HADOOP-3107,fsck failing with NPE,"2008-03-27 07:35:35,954 WARN /: /fsck?path=%2F:
From the namenode log, 

java.lang.NullPointerException
         at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:152)
         at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:153)
         at org.apache.hadoop.dfs.NamenodeFsck.fsck(NamenodeFsck.java:126)
         at org.apache.hadoop.dfs.FsckServlet.doGet(FsckServlet.java:48)
         at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
         at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
         at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
         at
org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
         at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
         at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
         at
org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
         at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
         at org.mortbay.http.HttpServer.service(HttpServer.java:954)
         at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
         at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
         at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
         at
org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
         at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
         at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)"
HADOOP-3106,Update documentation in mapred_tutorial to add Debugging,Update mapred_tutorial to add How to debug map reduce programs with http://wiki.apache.org/hadoop/HowToDebugMapReducePrograms
HADOOP-3105,compile-core-test fails for branch 0.16,"TestMultithreadedMapRunner uses an api FileSystem.delete(Path, boolean) which is not present in 0.16
Thus making compilation failure."
HADOOP-3104,MultithreadMapRunner keeps consuming records even if trheads are not available,"The ExecutorService execute() method does not block when there are not threads available, it queues up the runnables until there are threads. 

The problem is that all key/values are read and kept in memory for the task, with large datasets this will create a OOM exception.

Have to figure out how to use the execute in blocking fashion."
HADOOP-3103,[HOD] Hadoop.tmp.dir should not be set to cluster directory,"Currently HOD generates  hadoop.tmp.dir pointing to the cluster directory(hod.clusterdir), which users typically create in their home directories. This hadoop.tmp.dir is used on the client side when a jar is run(org/apache/hadoop/util/RunJar.java). The user home directories might be NFS mounted in some environments, which might result in a hit, for example when running a jar.

So, HOD *should not* generate hadoop.tmp.dir pointing to cluster directory. Instead, it should use hod.temp-dir, which *is* the location that dfs.client.buffer.dir uses to alleviate any such NFS hit problems. Further, though currently hadoop.tmp.dir is not used on hadoop daemons side, it would be good if HOD generates it on daemons side too, and thus preclude any related problems in future."
HADOOP-3101,'bin/hadoop job' should display the help and silently exit,Currently 'bin/hadoop job' displays help and throws an exception. Like other commands (e.g dfs) it should silently come out.
HADOOP-3100,Develop tests to test the DFS command line interface,"Develop tests to test the DFS command line interface. The DFS commands to test are:

           [-ls <path>]
           [-lsr <path>]
           [-du <path>]
           [-dus <path>]
           [-mv <src> <dst>]
           [-cp <src> <dst>]
           [-rm <path>]
           [-rmr <path>]
           [-expunge]
           [-put <localsrc> <dst>]
           [-copyFromLocal <localsrc> <dst>]
           [-moveFromLocal <localsrc> <dst>]
           [-get [-crc] <src> <localdst>]
           [-getmerge <src> <localdst> [addnl]]
           [-cat <src>]
           [-copyToLocal [-crc] <src> <localdst>]
           [-moveToLocal [-crc] <src> <localdst>]
           [-mkdir <path>]
           [-setrep [-R] [-w] <rep> <path/file>]
           [-touchz <path>]
           [-test -[ezd] <path>]
           [-stat [format] <path>]
           [-tail [-f] <file>]"
HADOOP-3099,"Need new options in distcp for preserving ower, group and permission","Currently, distcp -p preserves replication# and block size.  Since permissions are introduced, distcp should provide options for preserving ower, group and permission."
HADOOP-3098,"dfs -chown does not like ""_"" underscore in user name",":~$ hadoop dfs -chown aaa_bbb  /user/knoguchi/test.txt
chown: 'aaa_bbb' does not match expected pattern for [owner][:group].

in 0.16.1, only alphabets and numbers are allowed.  Shouldn't '_' be allowed?

I couldn't find any standard, but in Solaris10, it's defined as 

http://docs.sun.com/app/docs/doc/816-5174/6mbb98uhg?a=view

bq. The login (login) and role (role) fields accept a string of no more than eight bytes consisting of characters from the set of alphabetic characters, numeric characters, period (.), underscore (_), and hyphen (-). The first character should be alphabetic and the field should contain at least one lower case alphabetic character. A warning message is displayed if these restrictions are not met.

"
HADOOP-3096,Improve documentation about the Task Execution Environment in the Map-Reduce tutorial,We should improve the 'Task Execution & Environment' section in the Map-Reduce tutorial with information missing from http://wiki.apache.org/hadoop/TaskExecutionEnvironment.
HADOOP-3095,Validating input paths and creating splits is slow on S3,"A call to listPaths on S3FileSystem results in an S3 access for each file in the directory being queried. If the input contains hundreds or thousands of files this is prohibitively slow. This method is called in FileInputFormat.validateInput and FileInputFormat.getSplits. This would be easy to fix by overriding listPaths (all four variants) in S3FileSystem to not use listStatus which creates a FileStatus object for each subpath. However, listPaths is deprecated in favour of listStatus so this might be OK as a short term measure, but not longer term.

But it gets worse: FileInputFormat.getSplits goes on to access S3 a further six times for each input file via these calls:

1. fs.isDirectory
2. fs.exists
3. fs.getLength
4. fs.getLength
5. fs.exists (from fs.getFileBlockLocations)
6. fs.getBlockSize

So it would be best to change getSplits to use listStatus, and only access S3 once for each file. (This would help HDFS too.) This change would require some care since FileInputFormat has a protected method listPaths which subclasses can override (although, in passing I notice validateInput doesn't use listPaths - is this a bug?).

For input validation, one approach would be to disable it for S3 by creating a custom FileInputFormat. In this case, missing files would be detected during split generation. Alternatively, it may be possible to cache the input paths between validateInput and getSplits."
HADOOP-3094,BytesWritable.toString prints bytes above 0x80 as FFFFFF80,The printing of BytesWritable as a series of bytes does the wrong thing with negative bytes and expands them out to 32 bits before printing them.
HADOOP-3093,"ma/reduce throws the following exception if ""io.serializations"" is not set:","map/reduce throws the following exception if ""io.serializations"" is not set:

java.lang.NullPointerException
        at org.apache.hadoop.io.serializer.SerializationFactory.<init>(SerializationFactory.java:52)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:325)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:177)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2073)

Map/reduce should use org.apache.hadoop.io.serializer.WritableSerialization as the default class as it had in the past.
"
HADOOP-3092,"Show counter values from ""job -status"" command","It would be useful if the ""job -status"" command showed counter values, like the JobClient does (when the job has completed)."
HADOOP-3091,hadoop dfs -put should support multiple src,"It would be good to have hadoop dfs -put to support multiple sources like LINUX cp
hadoop dfs -put <localsrc> ... <dst>
"
HADOOP-3089,streaming should accept stderr from task before first key arrives,"Stderr output from a streaming task is not collected until the {{MRErrorThread}} is started by {{PipeMapRed.startOutputThreads()}}, which is done on the first call to {{map()}} or {{reduce()}}.

This makes it difficult to debug failures in starting up the task process. It can also lead to deadlock when a task receives no input keys but produces significant stderr output: the process will block on writing to stderr, while streaming will block waiting for the process to exit.

We should start the {{MRErrorThread}} when the process is forked, and then add the {{reporter}} later to enable stderr output serve as a keep-alive."
HADOOP-3088,hadoop dfs commands showing inconsistent behaviour..,"We're seeing inconsistent behavior with hadoop commands

hadoop dfs -mkdir a/*
Will create a directory called '*' which cannot be deleted using rm (which successfully does expand the *).

Additionally, I am having a problem I didn't have before:
If I want to dfsput large number of files without having to spawn the dfs -put for every file.. I cannot just do a 
hadoop dfs -put local_folder/file_* dfs_folder/

I get :
""Usage: java FsShell [-put <localsrc> <dst>]""

The shell does the expansion but -put has a problem with more than one. This is a fairly rudimentary functionality to have that can improve data movement efficiency.

Also, can I have an alternative for this? I need to push all files that appear in a local folder to the same dest folder again and again.
"
HADOOP-3087,JobInfo session object is not refreshed in loadHistory.jsp  if same job is accessed again.,"JobInfo object is not refreshed in loadHistory.jsp  if same job is accessed again.  In loadhistory.jsp, the jobInfo object is stored as session attribute. And if the same job is accessed again, the object is not refreshed. This becomes a problem if first time the object accessed was during job running and it doesnt refresh even after completing the job."
HADOOP-3086,Test case was missed in commit of HADOOP-3040,Test case was missed in commit of HADOOP-3040. 
HADOOP-3085,pushMetric() method of various metric util classes should catch exceptions,"
pushMetric() method of various metric util classes should catch exception.
Otherwise, any misconfigured metric will cause the entire metric data not sent out.
"
HADOOP-3084,distcp fails for files with zero length,"distcp fails for files with zero length. This is a regression from 0.15.3

distcp hftp://<namenode:port>/dir1/file1 file2         
08/03/24 23:09:45 INFO util.CopyFiles: srcPaths=[hftp://<namenode:port>/dir1/file1]
08/03/24 23:09:45 INFO util.CopyFiles: destPath=file2
08/03/24 23:09:45 INFO util.CopyFiles: srcCount=1
08/03/24 23:09:46 INFO mapred.JobClient: Running job: job_200803242306_0001
08/03/24 23:09:47 INFO mapred.JobClient:  map 0% reduce 0%
08/03/24 23:10:01 INFO mapred.JobClient: Task Id : task_200803242306_0001_m_000000_0, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:448)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2084)

08/03/24 23:10:18 INFO mapred.JobClient: Task Id : task_200803242306_0001_m_000000_1, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:448)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2084)

08/03/24 23:10:33 INFO mapred.JobClient: Task Id : task_200803242306_0001_m_000000_2, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:448)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2084)

^@08/03/24 23:10:48 INFO mapred.JobClient:  map 100% reduce 100%
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:894)
        at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:526)
        at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:596)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:612)"
HADOOP-3083,Remove lease when file is renamed,"When a file/directory is removed, the namenode should remove lease(s) for files that were affected by the rename."
HADOOP-3081,The static method FileSystem.mkdirs() should use the correct permissions to create directory,"There is a static method FileSystem.mkdirs() that create the directory first and then sets permissions in it. Instead, it should create the directory with the correct permissions.

Otherwise, even if dfs.permission is set to off, one might see job submission failures with the following stack trace:

         at org.apache.hadoop.dfs.PermissionChecker.check(PermissionChecker.java:173)
        at 
org.apache.hadoop.dfs.PermissionChecker.checkTraverse(PermissionChecker.java:129)
        at 
org.apache.hadoop.dfs.PermissionChecker.checkPermission(PermissionChecker.java:99)
        at org.apache.hadoop.dfs.FSNamesystem.checkPermission(FSNamesystem.java:4031)
        at org.apache.hadoop.dfs.FSNamesystem.checkOwner(FSNamesystem.java:3986)
        at org.apache.hadoop.dfs.FSNamesystem.setPermission(FSNamesystem.java:715)
        at org.apache.hadoop.dfs.NameNode.setPermission(NameNode.java:297)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:899)

        at org.apache.hadoop.ipc.Client.call(Client.java:512)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
        at org.apache.hadoop.dfs.$Proxy4.setPermission(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at 
org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at 
org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy4.setPermission(Unknown Source)
        at 
org.apache.hadoop.dfs.DistributedFileSystem.setPermission(DistributedFileSystem.java:351)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:218)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:688)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:124)
        at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:2114)"
HADOOP-3080,Remove flush calls from JobHistory,"The flush calls from JobHistory should be removed. This has performance impact when lots of events are being logged to the history files since the JT lock (e.g. lostTracker where the tracker had ran a lot of tasks, and we call failedTask in a loop, etc.)"
HADOOP-3079,Regular expressions does not work properly in dfs commands like bin/hadoop fs -ls,"When i do
$ hadoop fs -ls s*
ls: Cannot access src: No such file or directory.
It picks up the directory src in local file system.

but when i do,
$ hadoop fs -ls sc*
/user/amarsri/scr       <r 3>   20      2008-03-18 12:22        rw-rw-rw-       amarsri group
/user/amarsri/scripts/my.pl     <r 3>   146     2008-03-24 14:39        rw-rw-rw-       amarsri supergroup
/user/amarsri/scripts/xyz.pl    <r 3>   146     2008-03-24 14:39        rw-rw-rw-       amarsri supergroup
It lists the directories properly.

""$ hadoop fs -ls s*"" used to work well earlier."
HADOOP-3078,bin/hadoop fs -chmod doesnot change execution permissions for files.,"bin/hadoop fs -chmod doesnt change execution permissions for files.

I have two files in scripts directory
$ hadoop fs -lsr scripts
/user/amarsri/scripts/my.pl     <r 3>   146     2008-03-24 14:39        rw-r--r--       amarsri supergroup
/user/amarsri/scripts/xyz.pl    <r 3>   146     2008-03-24 14:39        rw-r--r--       amarsri supergroup

When i ran -chmod 777 on the file, file has only read permissions but no execution permissions.
$ hadoop fs -chmod 777 /user/amarsri/scripts/my.pl
$ hadoop fs -lsr scripts
/user/amarsri/scripts/my.pl     <r 3>   146     2008-03-24 14:39        rw-rw-rw-       amarsri supergroup
/user/amarsri/scripts/xyz.pl    <r 3>   146     2008-03-24 14:39        rw-r--r--       amarsri supergroup

When I do -chmod -R 777 on the directory, permissions are given to the directory, and the directories inside them, but files dont get execution permissions.
$ hadoop fs -chmod -R 777 /user/amarsri/scripts/
$ hadoop fs -lsr scripts
/user/amarsri/scripts/my.pl     <r 3>   146     2008-03-24 14:39        rw-rw-rw-       amarsri supergroup
/user/amarsri/scripts/xyz.pl    <r 3>   146     2008-03-24 14:39        rw-rw-rw-       amarsri supergroup
""$hadoop fs -ls"" lists scripts as the following.
/user/amarsri/scripts   <dir>           2008-03-24 14:39        rwxrwxrwx       amarsri supergroup
"
HADOOP-3076,"[HOD] If a cluster directory is specified as a relative path, an existing script.exitcode file will not be deleted.","Create a cluster directory, and use it in a script which fails so that the script.exitcode file is created in this directory
Now using the same directory, try to use the script option again, specifying the cluster directory as a relative path.
This will fail to remove the script.exitcode file."
HADOOP-3075,Streaming reducer stuck after detecting corrupted inputs ,"

A reducer of a streaming job  was stuck at 89%  after detecting exception from child.
Below is the stack:

2008-03-22 19:29:00,649 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=155701/155680/0 in:710=155701/219 [rec/s] out:710=155680/219 [rec/s]
2008-03-22 19:29:00,667 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.lang.RuntimeException: problem advancing
	at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:238)
	at org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.next(ReduceTask.java:314)
	at org.apache.hadoop.streaming.PipeReducer.reduce(PipeReducer.java:67)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:394)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2073)
Caused by: java.io.IOException: File is corrupt!
	at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1855)
	at org.apache.hadoop.io.SequenceFile$Reader.nextRawKey(SequenceFile.java:1972)
	at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:3005)
	at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2763)
	at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.readNextKey(ReduceTask.java:275)
	at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:236)

The task did not fail but just got stuck.
Speculative execution did not kick in.


The hadoop was built off trunk Version: 0.17.0-dev, r638716

"
HADOOP-3074,URLStreamHandler for the DFS,"This issue aims at providing a handler to resolve DFS URLs (""hdfs://host:port/file/to/path""), so that such URLs can be read using the URL API (mainly {{InputStream url.openStream()}}).

This allows the use of a {{URLClassLoader}} which would serve classes directly from the DFS.
"
HADOOP-3073,SocketOutputStream.close() should close the channel.,
HADOOP-3072,Multiple mappers join into reducer,"We would like to have multiple different mappers that takes input from different locations and output records with the same format. So the output of the mappers could be joint in the reducer easily. We know this can be achieved in the current framework. But the way to achieve it is not natural, less flexibility (need code change). Thus this would be a convenient feature to have. "
HADOOP-3070,"Trash not being expunged, Trash Emptier thread gone by NPE","We noticed that the users' trash were not being expunged by the namenode. 

jstack didn't show the Trash.Emptier thread and .out file showed 

Exception in thread ""Trash Emptier"" java.lang.NullPointerException
  at org.apache.hadoop.fs.Trash.expunge(Trash.java:146)
  at org.apache.hadoop.fs.Trash$Emptier.run(Trash.java:233)
  at java.lang.Thread.run(Thread.java:619)


It seems like this happens when it hits the user's directory which doesn't contain the .Trash."
HADOOP-3069,A failure on SecondaryNameNode truncates the primary NameNode image.,"When the primary name-node pulls the new image from the secondary, 
and the transfer fails for some reason then the primary considers the new image, 
which may not be completely transfered yet or may be not transfered at all, 
as a valid one and will roll it into the new files system image, which will be either corrupted or empty.
The problem here is that the error message from the secondary node does not reach the primary.
And this happens because TransferFsImage.getFileServer() closes the connection output stream 
in its finalize section. The secondary later sends the error reply which cannot be received by the primary
and causes the following exception on the secondary:
{code}
08/03/21 12:16:52 ERROR NameNode.Secondary: java.io.FileNotFoundException: \hadoop-data\hdfs\namesecondary\destimage.tmp (The system cannot find the file specified)
08/03/21 12:16:56 WARN /: /getimage?getimage=1: 
java.lang.IllegalStateException: Committed
	at org.mortbay.jetty.servlet.ServletHttpResponse.resetBuffer(ServletHttpResponse.java:212)
	at org.mortbay.jetty.servlet.ServletHttpResponse.sendError(ServletHttpResponse.java:375)
	at org.apache.hadoop.dfs.SecondaryNameNode$GetImageServlet.doGet(SecondaryNameNode.java:485)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
{code}
But the exception does not effect the behavior of the primary node. Since the stream is closed the primary thinks 
the file transfer was successfully finished and acts further accordingly.
There 2 bugs that need to be fixed here.
# The error message should be delivered to the primary, and the primary should not corrupt its image in case of an error.
# The doGet() method of both HttpServlet-s should catch not only IOException-s but any exceptions. 
If we miss NPE or SecurityException the main image will truncated."
HADOOP-3068,hadoop streaming tasks hang for when stream.non.zero.exit.is.failure==true and reduce processes exit with non zero status,"When I set *stream.non.zero.exit.is.failure* to true and run a streaming job with reducers that exit with a non-zero status, those tasks fail apparently waiting for something.

...
2008-03-21 13:33:53,715 INFO org.apache.hadoop.streaming.PipeMapRed: R/W/S=65501/1/0 in:334=65501/196 [rec/s] out:0=1/196 [rec/s]
2008-03-21 13:33:53,719 INFO org.apache.hadoop.streaming.PipeMapRed: mapRedFinished
2008-03-21 13:34:11,228 INFO org.apache.hadoop.streaming.PipeMapRed: Records R/W=65536/2
2008-03-21 13:34:11,235 INFO org.apache.hadoop.streaming.PipeMapRed: PipeMapRed.waitOutputThreads(): subprocess exitted with code 1
2008-03-21 13:34:11,235 INFO org.apache.hadoop.streaming.PipeMapRed: MRErrorThread done
2008-03-21 13:34:11,238 INFO org.apache.hadoop.streaming.PipeMapRed: MROutputThread done
2008-03-21 13:34:11,245 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:331)
        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:475)
        at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:110)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:399)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2113)

After that the task still shows up with status:Running, but it just hangs there and when/if all tasks get into this state, the whole cluster hangs.

BTW, may I suggest that we make *stream.non.zero.exit.is.failure* default to true after this is fixed?"
HADOOP-3067,DFSInputStream 'pread' does not close its sockets,"{{DFSInputStream.read(int, buffer)}} does not close the sockets it opens. Main reason this problem did not show up till now is that pread interface is not used much.

TestCrcCorruption failure first reported in HADOOP-2902 is caused by this bug. Hadoop 0.17 uses more file descriptors for each thread waiting on socket io. Since client does not close sockets, it leaves a lot of DataNode threads waiting in the unit test.
"
HADOOP-3066,Should not require superuser privilege to query if hdfs is in safe mode,"Currently, Superuser privilege is required to query hdfs to determine if it is in safe mode.

While entering or leaving safe mode should require Superuser privilege, merely asking if it is in safe mode should not."
HADOOP-3065,Namenode does not process block report if the rack-location script is not provided on namenode,"Per HADOOP-1985, If the namenode does not have a dnsToSwicthMapping specified, it does not process block reports from datanode(s). In the ResolutionMonitor, we have the following piece of code:

          List<String> rName = dnsToSwitchMapping.resolve(dnHosts);
          if (rName == null) {
            continue;
          }

Instead, we should probably set ""DEFAULT RACK"" for this datanode.

Also, when a processBlockRepont command is dropped by the namenode, it should log a message to facilitate debugging. The code in FSNamesystem.processReport could be something like this:

    if (node.getNetworkLocation().equals(NetworkTopology.UNRESOLVED)) {
      LOG.info(""Ignoring block report from "" + nodeID.getName() +
                      "" because rack location for this datanode is still to be resolved."");
      return null; //drop the block report if the dn hasn't been resolved
    }



"
HADOOP-3064,Exception with file globbing closures,"Using file globbing to select various input paths, like so:

conf.setInputPath(new Path(""mr/input/glob/2008/02/{02,08}""));

gives an exception:

Exception in thread ""main"" java.io.IOException: Illegal file pattern:
Expecting set closure character or end of range, or } for glob {02 at
3
       at org.apache.hadoop.fs.FileSystem$GlobFilter.error(FileSystem.java:1023)
       at org.apache.hadoop.fs.FileSystem$GlobFilter.setRegex(FileSystem.java:1008)
       at org.apache.hadoop.fs.FileSystem$GlobFilter.<init>(FileSystem.java:926)
       at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:826)
       at org.apache.hadoop.fs.FileSystem.globPaths(FileSystem.java:873)
       at org.apache.hadoop.mapred.FileInputFormat.validateInput(FileInputFormat.java:131)
       at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:541)
       at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:809)

The code for JobConf.getInputPaths tokenizes using
a comma as the delimiter, producing two paths
""mr/input/glob/2008/02/{02"" and ""08}""."
HADOOP-3063,BloomMapFile - fail-fast version of MapFile for sparsely populated key space,"The need for this improvement arose when working with large ancillary MapFile-s (essentially used as external dictionaries). For each invokation of map() / reduce() it was necessary to perform several look-ups in these MapFile-s, and in case of sparsely populated key-space the cost of finding that a key is absent was too high.

This patch implements a subclass of MapFile that creates a Bloom filter from all keys, so that accurate tests for absence of keys can be performed quickly and with 100% accuracy.

Writer.append() operations update a DynamicBloomFilter, which is then serialized when the Writer is closed. This filter is loaded in memory when a Reader is created. Reader.get() operation first checks the filter for the key membership, and if the key is absent it immediately returns null without doing any further IO."
HADOOP-3062,Need to capture the metrics for the network ios generate by dfs reads/writes and map/reduce shuffling  and break them down by racks ,"In order to better understand the relationship between hadoop performance and the network bandwidth, we need to know 
what the aggregated traffic data in a cluster and its breakdown by racks. With these data, we can determine whether the network 
bandwidth is the bottleneck when certain jobs are running on a cluster.
"
HADOOP-3061,Writable for single byte and double,Implementations of Writable for a single byte and a double.
HADOOP-3060,MiniMRCluster is ignoring parameter taskTrackerFirst ,"Looks like MiniMRCluster is ignoring the parameter taskTrackerFirst. 
With current code, MiniMRCluster starts jobtracker first, and then starts tasktracker irrespective of taskTrackerFirst parameter.
Mapred either has to remove the parameter or provide taskTracker starting first functionality."
HADOOP-3059,0.16.1 breaks backwards API compatibility with fix for HADOOP-2808,"HADOOP-2808 introduced a new signature for the copy method that takes an overwrite flag. 

Before this patch the behavior, for the pre-existing copy method, was that if the destination existed an IOException was thrown. After this patch the behavior, for the pre-existing copy method, is that if the destination exists it is overwritten.

The behavior of the pre-existing copy method should not have changed."
HADOOP-3058,Hadoop DFS to report more replication metrics,"Currently, the namenode and each datanode reports 'blocksreplicatedpersec.'

We'd like to be able to graph pending replications, vs number of under replicated blocks, vs. replications per second, so that we can get a better idea of the replication activity within the DFS.
"
HADOOP-3056,distcp seems to be broken in 0.16.1,"Starting from 0.16.1 distcp no longer works when running on source between two 0.16.1 installations. It seems to copy okay, but then at the end all copied files are deleted. Log files are empty. The job ends successfully.

BTW. so far, even with 0.16.0, we were unsuccessful to run distcp on the target successfully, except for small amounts of data (< 2 TB)."
HADOOP-3055,missing output for 'successful' jobs,"Same job with 2000 reducers ended apparently successfully, but twice 2 reduce output directories (different directories each time) were missing and the corresponding temporary _task_xxx directories still existed (these tasks did not have speculatively executed companions)."
HADOOP-3053,New Server Framework for Hadoop RPC,This is a new Server framework for Hadoop RPC which replaces the current Server class.  This new framework uses a reactor model to allow better throughput and better handling of clients.  
HADOOP-3052,distch -- tool to do parallel ch* ,"Build a tool to do parallel ch{mod,grp,own} on files.

This would have the advantage over the shell -R commands in that name nodes syncs from multiple clients are effectively batched.
"
HADOOP-3051,DataXceiver: java.io.IOException: Too many open files,"I just ran an experiment with the latest nightly build hadoop-2008-03-15 available and after 2 minutes I'm getting a tons of ""java.io.IOException: Too many open files"" exceptions as shown here:

{noformat} 2008-03-19 20:08:09,303 ERROR org.apache.hadoop.dfs.DataNode: 
141.30.xxx.xxx:50010:DataXceiver: java.io.IOException: Too many open files
     at sun.nio.ch.IOUtil.initPipe(Native Method)
     at sun.nio.ch.EPollSelectorImpl.<init>(Unknown Source)
     at sun.nio.ch.EPollSelectorProvider.openSelector(Unknown Source)
     at sun.nio.ch.Util.getTemporarySelector(Unknown Source)
     at sun.nio.ch.SocketAdaptor.connect(Unknown Source)
     at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1114)
     at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:956)
     at java.lang.Thread.run(Unknown Source){noformat}

I ran the same experiment with same high workload (50 dfs clients with 40 streams each writing concurrently files on a 8 nodes DFS cluster) with the 0.16.1 release and no exception is thrown. So it looks like a bug to me..."
HADOOP-3050,Cluster fall into infinite loop trying to replicate a block to a target that aready has this replica.,"This happened during a test run by Hudson. So fortunately we have all logs present.
http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1987/console
Search for TestDecommission. And look for block blk_167544198419718831 that is being replicated to node 127.0.0.1:65168 over and over again.
The issue needs to be investigated. I am making it a blocker until it is."
HADOOP-3049,MultithreadedMapRunner eats RuntimeExceptions,If a runtime exception is thrown within a map() it is not reported to the runner and the task continues processing other records ignoring the exception.
HADOOP-3048,Stringifier,"Storing arbitrary objects in the configuration has been discussed before in HADOOP-449 and HADOOP-1873. Although enabling such functionality has the risk of encouraging people to put big binary objects in the configuration, for some use cases passing objects to tasks is absolutely necessary. 

This issue will track the implementation of a Stringifier interface which stringifies and destringifies objects. Using this implementation, developers can store objects in the configuration and restore them later. 

Any thoughts ?"
HADOOP-3047,Unit test failure in TestStreamingExitStatus,"ant -Dtestcase=TestStreamingExitStatus test

Testcase: testMapFailOk took 1.733 sec
Testcase: testMapFailNotOk took 1.175 sec
Testcase: testReduceFailOk took 1.158 sec
Testcase: testReduceFailNotOk took 1.162 sec
    Caused an ERROR
Failed to delete /home/dhruba/sleet/build/contrib/streaming/test/data/out/_temporary/_reduce_58xgir
java.lang.RuntimeException: Failed to delete /home/dhruba/sleet/build/contrib/streaming/test/data/out/_temporary/_reduce_58xgir
    at org.apache.hadoop.streaming.UtilTest.recursiveDelete(UtilTest.java:48)
    at org.apache.hadoop.streaming.UtilTest.recursiveDelete(UtilTest.java:44)
    at org.apache.hadoop.streaming.UtilTest.recursiveDelete(UtilTest.java:44)
    at org.apache.hadoop.streaming.TestStreamingExitStatus.tearDown(TestStreamingExitStatus.java:70)
"
HADOOP-3046,Text and BytesWritable's raw comparators should use the lengths provided instead of rebuilding them from scratch using readInt,It would be much faster to use the key length provided by the raw compare function rather than rebuilding the integer lengths back up from bytes twice for every comparison in the sort.
HADOOP-3044,NNBench does not use the right configuration for the mapper,NNbench does not use the job configuration for getting property values in the Mapper. Therefore all the pre-configured property values do not get passed to the mapper.
HADOOP-3043,The Javadoc needs to fixed in JobConf.getOutputPath to talk about the new layout that includes _temporary,The Javadoc needs to fixed in JobConf.getOutputPath to talk about the new layout that includes _temporary.
HADOOP-3042,Update the Javadoc in JobConf.getOutputPath to reflect the actual temporary path,The javadoc needs to be fixed to include the _temporary in the job's temp output path.
HADOOP-3041,"Within a task, the value ofJobConf.getOutputPath() method is modified","Until 0.16.0 the value of the getOutputPath() method, if queried within a task, pointed to the part file assigned to the task. 

For example: /user/foo/myoutput/part_00000

In 0.16.1, now it returns an internal hadoop for the task output temporary location.

For the above example: /user/foo/myoutput/_temporary/part_00000

This change breaks applications that use the getOutputPath() to compute other directories.

IMO, this has always being broken, Hadoop should not change the values of properties injected by the client, instead it should use private properties or internal helper methods. 
"
HADOOP-3040,"Streaming should assume an empty key if the first character on a line is the seperator (stream.map.output.field.separator, by default, tab)","Streaming should assume an empty key if the first character on a line is the seperator (stream.map.output.field.separator, by default, tab). And the value as the whole line excluding the seperator."
HADOOP-3039,Runtime exceptions not killing job,"If a map or reduce task threw a runtime exception such as an NPE, the task, and ultimately the job, would fail in short order. In 0.16.0, when the reduce tasks started throwing NPEs, the tasks just hung. Eventually they timed out and were killed. But task has to get killed immediately if it throws NPE.

Thread dump shows:
""DestroyJavaVM"" prio=10 tid=0x0805f800 nid=0x6b5a waiting on condition [0x00000000..0xbfffcc90]
   java.lang.Thread.State: RUNNABLE

""Thread-12"" prio=10 tid=0x083f1400 nid=0x6b87 in Object.wait() [0xa2f37000..0xa2f37eb0]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0xa3af62a0> (a java.util.LinkedList)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1680)
	- locked <0xa3af62a0> (a java.util.LinkedList)

""Comm thread for task_200803181240_0001_r_000000_0"" daemon prio=10 tid=0x0841f000 nid=0x6b6f waiting on condition [0xa307c000..0xa307c130]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.mapred.Task$1.run(Task.java:283)
	at java.lang.Thread.run(Unknown Source)

""org.apache.hadoop.dfs.DFSClient$LeaseChecker@edf3f6"" daemon prio=10 tid=0x083fc400 nid=0x6b6d waiting on condition [0xa30cd000..0xa30cd1b0]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.dfs.DFSClient$LeaseChecker.run(DFSClient.java:626)
	at java.lang.Thread.run(Unknown Source)

""IPC Client connection to localhost/127.0.0.1:9000"" daemon prio=10 tid=0x083f6800 nid=0x6b6c in Object.wait() [0xa311d000..0xa311e030]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0xa4ac0860> (a org.apache.hadoop.ipc.Client$Connection)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:247)
	- locked <0xa4ac0860> (a org.apache.hadoop.ipc.Client$Connection)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:286)

It looks like Task is waiting for DataStreamer thread to get closed. 
When I did  streamer.setDaemon(true), the behavior was fine.

"
HADOOP-3036,Fix findBugs warnings in UpgradeUtilities.,"There are 2 findBugs warning in UpgradeUtilities.
- toString() applied to a String
- not closing InputStream.
"
HADOOP-3035,Data nodes should inform the name-node about block crc errors.,"Currently if a crc error occurs when data-node replicates a block to another node it throws an exception, and continues.
{code}
    [junit] 2008-03-17 19:46:11,855 INFO  dfs.DataNode (DataNode.java:transferBlocks(811)) - 127.0.0.1:3730 Starting thread to transfer block blk_-1962819020391742554 to 127.0.0.1:3740
    [junit] 2008-03-17 19:46:11,855 INFO  dfs.DataNode (DataNode.java:writeBlock(1067)) - Receiving block blk_-1962819020391742554 src: /127.0.0.1:3791 dest: /127.0.0.1:3740
    [junit] 2008-03-17 19:46:11,855 INFO  dfs.DataNode (DataNode.java:receiveBlock(2504)) - Exception in receiveBlock for block blk_-1962819020391742554 java.io.IOException: Unexpected checksum mismatch while writing blk_-1962819020391742554 from /127.0.0.1
    [junit] 2008-03-17 19:46:11,871 INFO  dfs.DataNode (DataNode.java:run(2626)) - 127.0.0.1:3730:Transmitted block blk_-1962819020391742554 to /127.0.0.1:3740
    [junit] 2008-03-17 19:46:11,871 INFO  dfs.DataNode (DataNode.java:writeBlock(1192)) - writeBlock blk_-1962819020391742554 received exception java.io.IOException: Unexpected checksum mismatch while writing blk_-1962819020391742554 from /127.0.0.1
    [junit] 2008-03-17 19:46:11,871 ERROR dfs.DataNode (DataNode.java:run(979)) - 127.0.0.1:3740:DataXceiver: java.io.IOException: Unexpected checksum mismatch while writing blk_-1962819020391742554 from /127.0.0.1
    [junit]     at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveChunk(DataNode.java:2246)
    [junit]     at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2416)
    [junit]     at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2474)
    [junit]     at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1173)
    [junit]     at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:956)
    [junit]     at java.lang.Thread.run(Thread.java:595)
{code}
The data-node should report the error to the name-node so that the corrupted replica could be removed and replicated."
HADOOP-3034,Need to be able to evacuate a datanode,"It would be very helpful if there were some way to evacuate data from one or more nodes.

This scenario arise fairly often when several nodes need to be powered down at nearly the same time.  Currently, they can only be taken down a few at a time (n-1 nodes at a time where n is the replication factor) and then you have to wait until all files on these nodes have been replicated.

One implementation would be to be to allow the nodes in question be put into read only mode and mark all blocks on those nodes as not counting as replicants.  This should cause the namenode to copy these blocks and as soon as fsck shows no under-replicated files, the nodes will be known to be clear for power-down.

"
HADOOP-3033,"Datanode fails write to DFS file with exception message ""Trying to change block file offset""","A write to a DFS block failed with the lastdatanode in the pipeline reporting this error:

Receiving block blk_-7279084187433655573 src: /xx.xx.xx.xx:xx dest: /xx.xx.xx.xx:50010
Changing block file offset of block blk_-7279084187433655573 from 9043968 to 9043968 meta file offset to 70663
Changing block file offset of block blk_-7279084187433655573 from 111935488 to 112001024 meta file offset to 875015
Exception in receiveBlock for block blk_-7279084187433655573 java.io.IOException: Trying to change block file offset of block blk_-7279084187433655573 to 112001024 but actual size of file is 111935488
PacketResponder 0 for block blk_-7279084187433655573 Interrupted.
PacketResponder 0 for block blk_-7279084187433655573 terminating
writeBlock blk_-7279084187433655573 received exception java.io.IOException: Trying to change block file offset of block blk_-7279084187433655573 to 112001024 but actual size of file is 111935488
DataXceiver: java.io.IOException: Trying to change block file offset of block blk_-7279084187433655573 to 112001024 but actual size of file is 111935488"
HADOOP-3031,Remove compiler warnings for ant test,"The following warnings need to be removed from ant tests 
src/test/org/apache/hadoop/mapred/EmptyInputFormat.java:35: warning: [unchecked] unchecked conversion
    [javac] found   : org.apache.hadoop.mapred.SequenceFileRecordReader
    [javac] required: org.apache.hadoop.mapred.RecordReader<K,V>
    [javac]     return new SequenceFileRecordReader(job, (FileSplit) split);
src/test/org/apache/hadoop/mapred/TestReduceTask.java:91: warning: [unchecked] unchecked call to ValuesIterator(org.apache.hadoop.io.SequenceFile.Sorter.RawKeyValueIterator,org.apache.hadoop.io.RawComparator<KEY>,java.lang.Class<KEY>,java.lang.Class<VALUE>,org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Progressable) as a member of the raw type org.apache.hadoop.mapred.ReduceTask.ValuesIterator
    [javac]       new ReduceTask.ValuesIterator(rawItr, WritableComparator.get(Text.class),
"
HADOOP-3030,InMemoryFileSystem.reserveSpaceWithChecksum does not look at failures while reserving space for the file in question,"The return statement code in InMemoryFileSystem.reserveSpaceWithCheckSum looks like
{noformat}
  return (mfs.reserveSpace(f, size) && mfs.reserveSpace(getChecksumFile(f), checksumSize));
{noformat}

This should be broken up to check for successful reserveSpace for each of the components. In some cases, we might reserve space for the first component and fail while doing the same for the second (checksum file). This will lead to wastage of space since we don't un-reserve the space we got for the first component. This usually won't happen due to the policy associated with creating a file in the InMemoryFileSystem (since the checksum component is usually very small) but still it should be fixed."
HADOOP-3029,"Misleading log message ""firstbadlink"" printed by datanodes","HADOOP-1707 introduces a  DataNode log message of the form ""forwarding connect ack to upstream firstbadlink is"". This log message does not really mean that any bad links were found. This log message should be changed so that it does not get printed in INFO level.
"
HADOOP-3027,JobTracker shuts down during initialization if the NameNode is down,"When the JobTracker is initializing and trying to connect to the NameNode, it shuts itself down if the NameNode is unreachable for more than one iteration of the connect loop. It can be easily reproduced if the JobTracker is started before the NameNode is started. The JobTracker will shut itself down in a few seconds. The problem seems to be with adding a shutdown hook in the FileSystem in the case where the same hook has been added before.

2008-03-17 09:45:20,979 INFO org.apache.hadoop.mapred.JobTracker: JobTracker up at: 9101
2008-03-17 09:45:20,979 INFO org.apache.hadoop.mapred.JobTracker: JobTracker webserver: 50030
2008-03-17 09:45:21,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s).
2008-03-17 09:45:22,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s).
2008-03-17 09:45:23,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s).
2008-03-17 09:45:24,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s).
2008-03-17 09:45:25,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s).
2008-03-17 09:45:26,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s).
2008-03-17 09:45:27,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s).
2008-03-17 09:45:28,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s).
2008-03-17 09:45:29,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s).
2008-03-17 09:45:30,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 10 time(s).
2008-03-17 09:45:31,406 INFO org.apache.hadoop.mapred.JobTracker: problem cleaning system directory: /tmp/hadoop/mapred/system
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
	at sun.nio.ch.SocketAdaptor.connect(Unknown Source)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:174)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:623)
	at org.apache.hadoop.ipc.Client.call(Client.java:546)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:211)
	at org.apache.hadoop.dfs.$Proxy4.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:312)
	at org.apache.hadoop.dfs.DFSClient.createRPCNamenode(DFSClient.java:94)
	at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:158)
	at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:69)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1255)
	at org.apache.hadoop.fs.FileSystem.access$400(FileSystem.java:53)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1272)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:191)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:96)
	at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:702)
	at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:135)
	at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:2266)
2008-03-17 09:45:41,410 FATAL org.apache.hadoop.mapred.JobTracker: java.lang.IllegalArgumentException: Hook previously registered
	at java.lang.ApplicationShutdownHooks.add(Unknown Source)
	at java.lang.Runtime.addShutdownHook(Unknown Source)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1269)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:191)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:96)
	at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:702)
	at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:135)
	at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:2266)

2008-03-17 09:45:41,412 INFO org.apache.hadoop.mapred.JobTracker: SHUTDOWN_MSG: "
HADOOP-3025,ChecksumFileSystem needs to support the new delete method,"The method FileSystem.delete(path) has been deprecated in favor of the new method delete(path, recursive). Temporary files gets created in the MapReduce framework and when the time for deletion comes, they are deleted via delete(path, recursive). This doesn't delete the associated checksum files. This has a big impact when the FileSystem is the InMemoryFileSystem, where space is at a premium and wasting space here might hurt the performance of MapReduce jobs overall. One solution to this problem is to implement the method delete(path, recursive) in the ChecksumFileSystem but is there is a reason why it was left out as part of HADOOP-771?"
HADOOP-3022,Fast Cluster Restart,This item introduces a discussion of how to reduce the time necessary to start a large cluster from tens of minutes to a handful of minutes.
HADOOP-3019,want input sampler & sorted partitioner,"The input sampler should generate a small, random sample of the input, saved to a file.

The partitioner should read the sample file and partition keys into relatively even-sized key-ranges, where the partition numbers correspond to key order.

Note that when the sampler is used for partitioning, the number of samples required is proportional to the number of reduce partitions.  10x the intended reducer count should give good results."
HADOOP-3018,Eclipse plugin fails to compile due to missing RPC.stopClient() method,HADOOP-2870 removed the RPC.stopClient() method which is called by org.apache.hadoop.eclipse.dfs.DFSLocationsRoot.
HADOOP-3012,dfs -mv file to user home directory fails silently if the user home directory does not exist,"dfs -mv file to user home directory fails silently if the user home directory does not exist. This was working before. It will move the file if the user home directory exists.

Here is the sequence:
1. bin/hadoop dfs -lsr /
/file1  <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup

2. bin/hadoop dfs -mv /file1 file2

3. bin/hadoop dfs -lsr /
/file1  <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup

4. hadoop dfs -lsr /
/file1  <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup
/user   <dir>           2008-03-13 20:07        rwxr-xr-x       hadoopqa        supergroup
/user/hadoopqa  <dir>           2008-03-13 20:07        rwxr-xr-x       hadoopqa        supergroup
/user/hadoopqa/file0    <r 3>   0       2008-03-13 20:07        rw-r--r--       hadoopqa        supergroup

5. bin/hadoop dfs -mv /file1 file2

6. bin/hadoop dfs -lsr /
/user   <dir>           2008-03-13 20:07        rwxr-xr-x       hadoopqa        supergroup
/user/hadoopqa  <dir>           2008-03-13 20:08        rwxr-xr-x       hadoopqa        supergroup
/user/hadoopqa/file0    <r 3>   0       2008-03-13 20:07        rw-r--r--       hadoopqa        supergroup
/user/hadoopqa/file2    <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup

In step #2, it fails to move the file.
In step #5, it moves the file as /user/hadoopqa directory exists."
HADOOP-3011,Distcp deleting target directory,"I believe this happens when 

1) src is a file and dst is a directory.

2) -update is set

% hadoop  distcp -p -update  hdfs://srcnn:9999/user/knoguchi/fileA    hdfs://dstnn:9999/user/knoguchi/dir 


This will delete the hdfs://dstnn:9999/user/knoguchi/dir .  "
HADOOP-3010,ConcurrentModificationException from org.apache.hadoop.ipc.Server$Responder in JobTracker,"I see lot of these in my JobTracker log 
{noformat}
WARN org.apache.hadoop.ipc.Server: Exception in Responder java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$KeyIterator.next(HashMap.java:827)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:475)
{noformat}"
HADOOP-3009,TestFileCreation fails while restarting cluster,
HADOOP-3008,SocketIOWithTimeout does not handle thread interruption,"If a thread is waiting in {{SocketIOWithTimeout.select()}} for IO and is interrupted by another thread, select() returns zero. But current implementation does not check if the thread is interrupted and goes back to waiting. But it should check if the thread is interrupted.

In this case, I am thinking of throwing {{InterruptedIOException}} http://java.sun.com/j2se/1.5.0/docs/api/java/io/InterruptedIOException.html."
HADOOP-3007,DataNode pipelining : failure on mirror results in failure on upstream datanode,"When datanode is transfering data to other datanodes (as opposed to a client write()), DN currently receiving the data is supposed to continued to receive even if the downstream (mirror) datanode fails.

I think there are two minor bugs in receiveBlock() :
# mirrorSock masks this.mirrorSock, which could be set to null by other members like receiveChunk() to indicate failure.
# when this.mirrorSock is non null, failure to write to it at in receiveBlock() should be handled same way as in receiveChunk().

I am not sure if this should block 0.16.1, but should surely block 0.16.2."
HADOOP-3006,DataNode sends wrong length in header while pipelining.,"Bug introduced by HADOOP-2758. This part is removed in HADOOP-1702, but 1702 is not yet ready. I will submit a patch. 

The first datanode in the pipeline receives data with non-interleaving checksum and sends interleaving checksum to mirror (this mismatch is temporary till HADOOP-1702). The packet size differs in these two cases and DataNode did not correct the packet size in the header.
"
HADOOP-3004,HOD allocate command does not accept '~',"hod allocate seems to throw error when I specify directory relative to my home
[lohit@hod]$ hod allocate --hod.clusterdir=""~/clusterdir"" --hod.nodecount=5
error: bin/hod failed to start.
error: invalid 'clusterdir' specified in section hod (--hod.clusterdir): ~/clusterdir
error: 1 problem found.
Check your command line options and/or your configuration file /hod/conf/hodrc"
HADOOP-3003,FileSystem cache key should be updated after a FileSystem object is created,"In FileSystem.get(uri, conf), it first creates a cache key from the uri and the conf and then lookups the corresponding FileSystem object in the cache.  If the object is not found, it initializes a FileSystem object and put it to the cache with the key.  However, during FileSystem creation, the conf might be modified.  In such case, the key should be updated before putting it to the cache."
HADOOP-3002,HDFS should not remove blocks while in safemode.,"I noticed that data-nodes are removing blocks during a rather prolonged distributed upgrade when the name-node is in safe mode.
This happened on my experimental cluster with accelerated block report rate.
By definition in safe mode the name-node should not
- accept client requests to change the namespace state, and
- schedule block replications and/or block removal for the data-nodes.

We don't want any unnecessary replications until all blocks are reported during startup.
We also don't want to remove blocks if safe mode is entered manually.
In heartbeat processing we explicitly verify that the name-node is in safe-mode and do not return any block commands to the data-nodes.
Block reports can also return block commands, which should be banned during safe mode.
"
HADOOP-3001,FileSystems should track how many bytes are read and written,It would be nice if the file systems could track the number of bytes read and written for each class of FileSystem. Map/Reduce could then report these numbers back as counters to provide information about how much data is read and written.
HADOOP-2998,Calling DFSClient.close() should not throw IOException when it is already closed.,"In Java, if a stream is opened, it can be closed for more than one times without any exception.  For example,
{code}
OutputStream out = ...;
...
out.close();
out.close(); //no exception thrown here
{code}

So, DFSClient.close() should also do the same."
HADOOP-2997,Add test for non-writable serializer,It would be useful to have a unit test that tests MapReduce works using a non-writable serializer.
HADOOP-2996,StreamUtils abuses StringBuffers,"Code does 

sb.append(""foo"" + n)

which corresponds to

sb.append(new StringBuffer(""foo"").append(n).toString())

patch fixes this.

"
HADOOP-2995,StreamBaseRecordReader's getProgress returns just 0 or 1,"StreamBaseRecordReader.getProgress uses integer math, so progress is either 0 or 1. The patch fixes this.

"
HADOOP-2994,DFSClient calls toString on strings.,DFSClient needlessly calls toString on Strings. patch fixes this.
HADOOP-2993,Specify which JAVA_HOME should be set,"Quickstart page (http://hadoop.apache.org/core/docs/current/quickstart.html) specifies that user should set JAVA_HOME under ""1."" of ""Required Software"".  But, it does not specify where this should be set.  My instinct (as would be the instinct of any un*x user) was to set this in his/her shell/profie/rcfile.  Please specify that this should be specified in conf/hadoop-env.sh.  Yes, this is clarified later under ""Download"", but is easy to miss, especially since the user will think that he/she had already set JAVA_HOME.
"
HADOOP-2992,Sequential distributed upgrades.,"The distributed upgrades were tested in the case of one distributed upgrade only.
It turned out to be that current implementation of the framework does not support multiple upgrades.
This feature is needed for the appends. See HADOOP-2656."
HADOOP-2986,"All of the instances of ""_temporary"" should be pulled into a single static final","We should pull all of the string literals for ""_temporary"" to references to a static final string, probably in FileOutputFormat."
HADOOP-2985,LocalJobRunner gets NullPointerException if there is no output directory,"The local job runner gets a null pointer, if the job doesn't have an output directory.

java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:61)
	at org.apache.hadoop.fs.Path.<init>(Path.java:50)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:117)

The Job Tracker also gets an ERROR log message, which is incorrect and should be removed."
HADOOP-2984,Distcp should have forrest documentation,We really should have a page on how to use distcp.
HADOOP-2983,[HOD] local_fqdn() returns None when gethostbyname_ex doesnt return any FQDNs.,"For some reason (probably in our local DNS setup) gethostbyname_ex() does not return any fully qualified hostnames. This has never been an issue, everything has worked fully with the hostnames.

However, this causes HOD to fail, as local_fqn() in util.py returns None. 

{noformat}
Python 2.5.1 (r251:54863, Sep 21 2007, 16:05:06)
[GCC 3.4.6 20060404 (Red Hat 3.4.6-3)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import socket, os
>>> socket.gethostbyname_ex(os.uname()[1])
('bohol', [], ['130.209.252.70'])
>>>
{noformat}

The solution is to fix local_fqdn in until.py, such that it falls back to the contents of the variable me if fqdn is still None.

Ta muchly.

Craig"
HADOOP-2982,[HOD] checknodes should look for free nodes without the jobs attribute,Modify the checknodes logic to compute number of available nodes by looking for the number of free nodes without a jobs attribute. This is more accurate than current computation.
HADOOP-2981,Follow Apache process for getting ready to put crypto code in to project,As described in http://www.apache.org/dev/crypto.html we need to update some text files and notify the US government before we put code using crypto into Hadoop.
HADOOP-2978,JobHistory log format for COUNTER is ambigurous ,"For the lines like: 

Job JOBID=""job_200803072233_0001"" FINISH_TIME=""1204929332820"" JOB_STATUS=""SUCCESS"" FINISHED_MAPS=""24"" FINISHED_REDUCES=""15"" FAILED_MAPS=""0"" FAILED_REDUCES=""0"" COUNTERS=""Job Counters .Launched map tasks=24,Job Counters .Launched reduce tasks=15Map-Reduce Framework.Map input records=2894276,Map-Reduce Framework.Map output records=2894276,Map-Reduce Framework.Map input bytes=3227015845,Map-Reduce Framework.Map output bytes=3232268034,Map-Reduce Framework.Combine input records=0,Map-Reduce Framework.Combine output records=0,Map-Reduce Framework.Reduce input groups=2526981,Map-Reduce Framework.Reduce input records=2894276,Map-Reduce Framework.Reduce output records=2894276""

The extracted value for COUNTERS is 

Job Counters .Launched map tasks


which is clearly wrong.

"
HADOOP-2976,Blocks staying underreplicated (for unclosed file),"We had two files staying underreplicated for over a day.
I checked that these under-replicated blocks are not corrupted.
(They were both task tmp files and most likely didn't get closed.)

Taking one file, /aaa/_task_200803040823_0001_r_000421_0/part-00421


Namenode log showed

namenode.log.2008-03-04 2008-03-04 16:19:21,478 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.allocateBlock: /aaa/_task_200803040823_0001_r_000421_0/part-00421.  blk_-7848645760735416126
2008-03-04 16:19:24,357 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 11.1.111.111:22222 is added to blk_-7848645760735416126

On the datanode 11.1.111.111, it showed 

2008-03-04 16:19:24,358 INFO org.apache.hadoop.dfs.DataNode: Received block blk_-7848645760735416126 from /55.55.55.55 and operation failed at /22.2.222.22



On the second datanode 22.2.222.22, it showed 

2008-03-04 16:19:21,578 INFO org.apache.hadoop.dfs.DataNode: Exception writing to mirror 33.3.33.33
java.net.SocketException: Connection reset
  at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
  at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
  at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
  at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
  at java.io.DataOutputStream.write(DataOutputStream.java:90)
  at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveChunk(DataNode.java:1333)
  at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:1386)
  at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:938)
  at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:804)
  at java.lang.Thread.run(Thread.java:619)

2008-03-04 16:19:24,358 ERROR org.apache.hadoop.dfs.DataNode: DataXceiver: java.net.SocketException: Broken pipe
  at java.net.SocketOutputStream.socketWrite0(Native Method)
  at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
  at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
  at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
  at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
  at java.io.DataOutputStream.flush(DataOutputStream.java:106)
  at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:1394)
  at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:938)
  at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:804)
  at java.lang.Thread.run(Thread.java:619)


"
HADOOP-2974,ipc unit tests fail due to connection errors,"ipc unit tests fail due to connection errors:

Failing tests:
org.apache.hadoop.ipc.TestIPC.unknown
org.apache.hadoop.ipc.TestIPCServerResponder.unknown
org.apache.hadoop.ipc.TestRPC.testSlowRpc
org.apache.hadoop.ipc.TestRPC.testCalls

Changes:
# HADOOP-2346. Utilities to support timeout while writing to sockets. DFSClient and DataNode sockets have 10min write timeout.
# HADOOP-2906. Add an OutputFormat capable of using keys, values, and config params to map records to different output files.
# HADOOP-2756. NPE in DFSClient while closing DFSOutputStreams under load.
# HADOOP-2934. The namenode was encountreing a NPE while loading leases from the fsimage. Fixed.
# HADOOP-2925. Fix HOD to create mapred system directory using a naming convention that will avoid clashes in multi-user shared cluster scenario.
# HADOOP-2911. Make the information printed by the HOD allocate and info commands less verbose and clearer.
# HADOOP-2883. Write failures and data corruptions on HDFS files. The write timeout is back to what it was on 0.15 release. Also, the datnodes flushes the block file buffered output stream before sending a positive ack for the packet back to the client.
# HADOOP-2861. INCOMPATIBLE CHANGE. Improve the user interface for the HOD commands. Command line structure has changed.

Error logs:
 [junit] Running org.apache.hadoop.ipc.TestIPC
    [junit] 2008-03-07 10:50:04,291 INFO  metrics.RpcMetrics (RpcMetrics.java:<init>(53)) - Initializing RPC Metrics with hostName=0, port=4785
    [junit] 2008-03-07 10:50:04,354 INFO  ipc.Server (Server.java:run(443)) - IPC Server Responder: starting
    [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(303)) - IPC Server listener on 4785: starting
    [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(861)) - IPC Server handler 0 on 4785: starting
    [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(861)) - IPC Server handler 1 on 4785: starting
    [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(861)) - IPC Server handler 2 on 4785: starting
    [junit] 2008-03-07 10:50:04,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 1 time(s).
    [junit] 2008-03-07 10:50:04,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 1 time(s).
    [junit] 2008-03-07 10:50:05,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 2 time(s).
    [junit] 2008-03-07 10:50:05,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 2 time(s).
    [junit] 2008-03-07 10:50:06,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 3 time(s).
    [junit] 2008-03-07 10:50:06,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 3 time(s).
    [junit] 2008-03-07 10:50:07,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 4 time(s).
    [junit] 2008-03-07 10:50:07,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 4 time(s).
    [junit] 2008-03-07 10:50:08,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 5 time(s).
    [junit] 2008-03-07 10:50:08,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 5 time(s).
    [junit] 2008-03-07 10:50:09,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 6 time(s).
    [junit] 2008-03-07 10:50:09,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 6 time(s).
    [junit] 2008-03-07 10:50:10,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 7 time(s).
    [junit] 2008-03-07 10:50:10,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 7 time(s).
    [junit] 2008-03-07 10:50:11,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 8 time(s).
    [junit] 2008-03-07 10:50:11,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 8 time(s).
    [junit] 2008-03-07 10:50:12,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 9 time(s).
    [junit] 2008-03-07 10:50:12,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 9 time(s).
    [junit] 2008-03-07 10:50:13,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 10 time(s).
    [junit] 2008-03-07 10:50:13,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 10 time(s).
    [junit] 2008-03-07 10:50:14,435 FATAL ipc.TestIPC (TestIPC.java:run(92)) - Caught: java.net.BindException: Cannot assign requested address: no further information
    [junit] 2008-03-07 10:50:14,435 FATAL ipc.TestIPC (TestIPC.java:run(92)) - Caught: java.net.BindException: Cannot assign requested address: no further information

"
HADOOP-2973,Unit test fails on Windows: org.apache.hadoop.dfs.TestLocalDFS.testWorkingDirectory,"Unit test fails on Windows: org.apache.hadoop.dfs.TestLocalDFS.testWorkingDirectory

Error from failure:
junit.framework.AssertionFailedError: expected:<hdfs://localhost:1925/user/SYSTEM> but was:<hdfs://localhost:1925/user/hadoopqa>
	at org.apache.hadoop.dfs.TestLocalDFS.testWorkingDirectory(TestLocalDFS.java:81)

Changes:
# HADOOP-2931. IOException thrown by DFSOutputStream had wrong stack trace in some cases.
# HADOOP-2758. Reduce buffer copies in DataNode when data is read from HDFS, without negatively affecting read throughput.
# HADOOP-2833. Do not use ""Dr. Who"" as the default user in JobClient. A valid user name is required.
# HADOOP-2809.  Fix HOD syslog config syslog-address so that it works.
# HADOOP-2847.  Ensure idle cluster cleanup works even if the JobTracker becomes unresponsive to RPC calls.
# HADOOP-2819. The following methods in JobConf are removed: getInputKeyClass() setInputKeyClass getInputValueClass() setInputValueClass(Class theClass) setSpeculativeExecution getSpeculativeExecution().
# HADOOP-2820. The following classes in streaming are removed : StreamLineRecordReader StreamOutputFormat StreamSequenceRecordReader.
# HADOOP-2219. A new command ""df -count"" that counts the number of files and directories.
# HADOOP-2912. MiniDFSCluster restart should wait for namenode to exit safemode. This was causing TestFsck to fail."
HADOOP-2972,org.apache.hadoop.dfs.TestDFSShell.testErrOutPut fails on Windows with NullPointerException,"org.apache.hadoop.dfs.TestDFSShell.testErrOutPut fails on Windows with NullPointerException

Here is the exception:
2008-03-07 10:18:45,699 INFO  util.ThreadedServer (ThreadedServer.java:run(656)) - Stopping Acceptor ServerSocket[addr=0.0.0.0/0.0.0.0,port=0,localport=4781]
2008-03-07 10:18:45,699 ERROR dfs.DataNode (DataNode.java:run(2623)) - Exception: java.lang.NullPointerException
	at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:858)
	at org.apache.hadoop.dfs.DataNode.processCommand(DataNode.java:722)
	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:599)
	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:2621)
	at java.lang.Thread.run(Thread.java:595)

2008-03-07 10:18:46,118 ERROR dfs.DataNode (DataNode.java:run(2623)) - Exception: java.lang.NullPointerException
	at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:858)
	at org.apache.hadoop.dfs.DataNode.processCommand(DataNode.java:722)
	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:599)
	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:2621)
	at java.lang.Thread.run(Thread.java:595)

2008-03-07 10:18:46,615 INFO  http.SocketListener (SocketListener.java:stop(212)) - Stopped SocketListener on 0.0.0.0:4781
2008-03-07 10:18:46,615 INFO  util.Container (Container.java:stop(156)) - Stopped org.mortbay.jetty.servlet.WebApplicationHandler@1cb7a1"
HADOOP-2971,SocketTimeoutException in unit tests,"
TestJobStatusPersistency failed and contained DataNode stacktraces similar to the following :

{noformat}
2008-03-07 21:27:00,410 ERROR dfs.DataNode (DataNode.java:run(976)) - 127.0.0.1:57790:DataXceiver: java.net.SocketTimeoutException: 0 millis 
timeout while waiting for Unknown Addr (local: /127.0.0.1:57790) to be ready for read
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:188)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:135)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:121)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2434)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1170)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:953)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

This is mostly related to HADOOP-2346. The error is strange. socket.getRemoteSocketAddress() returned null implying this socket is not connected yet. But we have already read a few bytes from it!.

"
HADOOP-2970,Wrong class definition for hodlib/Hod/hod.py for Python < 2.5.1,"Running HOD with python 2.5 (<2.5.1) leads to the following error: 

Traceback (most recent call last):
  File ""hod"", line 47, in ?
    from hodlib.Hod.hod import hodRunner
  File ""/mnt/scratch/grid/hod/hod-trunk-421/hodlib/Hod/hod.py"", line 488
    class hodHelp():
                  ^
SyntaxError: invalid syntax

"
HADOOP-2966,Relative path for script option is not handled properly.,"Ran hod as -:
hod allocate -d cdir -n 5 -s scripts/hodscript.sh

hod throws following error -:
[
CRITICAL - Invalid script file (--hod.script or -s) specified : scripts/hdp
]

It works for absolute path.
hod should also handle relative path for script file
"
HADOOP-2961,[HOD] Hod expects port info though external host is not mentioned.,"When external host is not specified in gridservice-mapred or hdfs sections, there is no point in HOD validating the port numbers or hostnames."
HADOOP-2959,"When a mapper needs to run a combiner, it should create one and reuse it, instead of creating one per partition per spill",
HADOOP-2958,Test utility no longer works in trunk,"Filebench no longer works in trunk, due to HADOOP-2391 performing a check for the existence of the output directory (it improperly sets it to the file location, which works due to URI.resolve semantics)"
HADOOP-2955,ant test fail for TestCrcCorruption with OutofMemory.,"TestCrcCorruption sometimes corrupts the metadata for crc and leads to corruption in the length of of bytes of checksum (second field in metadata). This does not happen always but somtimes since corruption is random in the test.

I put in a debug statement in the allocation to see how many bytes were being allocated and ran it for few times. This is one of the allocation in 
BlockSender:sendBlock() 

 int maxChunksPerPacket = Math.max(1,
                      (BUFFER_SIZE + bytesPerChecksum - 1)/bytesPerChecksum);
        int sizeofPacket = PKT_HEADER_LEN + 
        (bytesPerChecksum + checksumSize) * maxChunksPerPacket;
        LOG.info(""Comment: bytes to allocate "" + sizeofPacket);
        ByteBuffer pktBuf = ByteBuffer.allocate(sizeofPacket);


The output in one of the allocations is 

 dfs.DataNode (DataNode.java:sendBlock(1766)) - Comment: bytes to allocate 1232596786

So we should check for number of bytes being allocated in sendBlock (should be less than the block size? -- seems like a good default).

"
HADOOP-2954,"In streaming, map-output cannot have empty keys","Here is the analysis, when the mapper and reducer both are /bin/cat,

default key field separator: '\t' (or tab)


for ex, if the input line is:

\tSDSDFIKSDFSDFJS

the input for the mapper ('cat' in this case) is:

\tSDSDFIKSDFSDFJS

-

the output of the mapper is split into a key, value pair as below:

(key, value) -> (\tSDSDFIKSDFSDFJS, """")
(i.e. the value is empty)

the function which splits the output into key,value pair for
streaming jobs, ignores the first character of the line

-

from the above (key, value) pair, the input for the reducer is:
(key followed by separator followed by value)

\tSDSDFIKSDFSDFJS\t

if the reducer is set to NONE, the above line is the output of
the map task

-

the output of the reducer ('cat' in this case) is:

\tSDSDFIKSDFSDFJS\t

-

if the line starts with the field separator, it is possible that
the output of the mapper can be assigned to different reducers because
it is possible that the line contains more than once instance of the
field separator - for ex:

input-line=\tABCDEFGH
key=\tABCDEFGH
value=
(value is empty)
output-line=\tABCDEFGH\t

line=\tABCDEFGHYH\tJHUHJH
key=\tABCDEFGHYH
value=JHUHJH
output-line=\tABCDEFGHYH\tJHUHJH

assuming defaults (HashPartitioner), they are likely to be assigned to
different reducers because the keys are different.

The streaming contract  says that from beginning of the line upto the first tab is the key, so key should be empty string. But it is not.
"
HADOOP-2951,"contrib package provides a utility to build or update an index
A contrib package to update an index using Map/Reduce","This contrib package provides a utility to build or update an index
using Map/Reduce.

A distributed ""index"" is partitioned into ""shards"". Each shard corresponds
to a Lucene instance. org.apache.hadoop.contrib.index.main.UpdateIndex
contains the main() method which uses a Map/Reduce job to analyze documents
and update Lucene instances in parallel.

The Map phase of the Map/Reduce job formats, analyzes and parses the input
(in parallel), while the Reduce phase collects and applies the updates to
each Lucene instance (again in parallel). The updates are applied using the
local file system where a Reduce task runs and then copied back to HDFS.
For example, if the updates caused a new Lucene segment to be created, the
new segment would be created on the local file system first, and then
copied back to HDFS.

When the Map/Reduce job completes, a ""new version"" of the index is ready
to be queried. It is important to note that the new version of the index
is not derived from scratch. By leveraging Lucene's update algorithm, the
new version of each Lucene instance will share as many files as possible
as the previous version.

The main() method in UpdateIndex requires the following information for
updating the shards:
  - Input formatter. This specifies how to format the input documents.
  - Analysis. This defines the analyzer to use on the input. The analyzer
    determines whether a document is being inserted, updated, or deleted.
    For inserts or updates, the analyzer also converts each input document
    into a Lucene document.
  - Input paths. This provides the location(s) of updated documents,
    e.g., HDFS files or directories, or HBase tables.
  - Shard paths, or index path with the number of shards. Either specify
    the path for each shard, or specify an index path and the shards are
    the sub-directories of the index directory.
  - Output path. When the update to a shard is done, a message is put here.
  - Number of map tasks.

All of the information can be specified in a configuration file. All but
the first two can also be specified as command line options. Check out
conf/index-config.xml.template for other configurable parameters.

Note: Because of the parallel nature of Map/Reduce, the behaviour of
multiple inserts, deletes or updates to the same document is undefined."
HADOOP-2949,"[HOD] Hod should not check for the pkgs directory in gridservice-hdfs or mapred sections, if tarball is specified.","Ringmaster validates the pkgs path for gridservice-mapred and gridservice-hdfs section when tarball option is used and hodrc also contians pkgs path. This is not necessary, as it does not use the pkgs path."
HADOOP-2947,[HOD] Hod should redirect stderr and stdout of Hadoop daemons to assist debugging,"Copied from internal bug details from Koji:

==========================
Sometimes JobTracker/TaskTracker starts consuming 99% cpu and stops responding to 'jstack' call.  In those cases,
usually it still responds to kill -QUIT signal which forces the jvm to dump the stack to stdout.  

Please have the stdout of JT/TT redirected to a file. 

Adding stderr. 
If thread has an uncaught exception, it prints out to stderr and dies.
=========================="
HADOOP-2946,"[HOD] Hod should not check for the pkgs directory in gridservice-hdfs, if the external option is specified","If the gridservice-hdfs option is specified, the pkgs option is not used. So, it need not be validated."
HADOOP-2944,redesigned plugin has missing functionality,"It is not possible to add a new Hadoop server to the list of servers in the Hadoop eclipse plugin.  In org/apache/hadoop/eclipse/servers/RunOnHadoopWizard.java, the private class variable ""createNewPage"" is never initialized (although it should be at line 97), and therefore clicking on ""Finish"" in the wizard results in a NullPointerException when this variable is accessed.

After fixing this in a local client, I had further problems connecting to a server using the SOCKS protocol."
HADOOP-2943,Compression for intermediate map output is broken,"It looks like SequenceFile::RecordCompressWriter and SequenceFile::BlockCompressWriter weren't updated to use the new serialization added in HADOOP-1986. This causes failures in the merge when mapred.compress.map.output is true and mapred.map.output.compression.type=BLOCK:

{noformat}
java.io.IOException: File is corrupt!
        at org.apache.hadoop.io.SequenceFile$Reader.readBlock(SequenceFile.java:1656)
        at org.apache.hadoop.io.SequenceFile$Reader.nextRawKey(SequenceFile.java:1969)
        at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:2985)
        at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:2785)
        at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:2494)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:654)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:740)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:212)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2077)
{noformat}

mapred.map.output.compression.type=RECORD works for Writables, but should be updated."
HADOOP-2942,Unit test fails on Windows: org.apache.hadoop.dfs.TestDFSShell.testErrOutPut,"Unit test fails on Windows: org.apache.hadoop.dfs.TestDFSShell.testErrOutPut

*Here are the changes:*
- HADOOP-2931. IOException thrown by DFSOutputStream had wrong stack trace in some cases. (Michael Bieniosek via rangadi) (detail/ViewSVN)
- HADOOP-2758. Reduce buffer copies in DataNode when data is read from HDFS, without negatively affecting read throughput. (rangadi) (detail/ViewSVN)
- move previous change log to the end of the section rather than the beginning (detail/ViewSVN)
- HADOOP-2833. Do not use ""Dr. Who"" as the default user in JobClient. A valid user name is required. (Tsz Wo (Nicholas), SZE via rangadi) (detail/ViewSVN)
- HADOOP-2809.  Fix HOD syslog config syslog-address so that it works.  Contributed by Hemanth Yamijala. (detail/ViewSVN)
- HADOOP-2847.  Ensure idle cluster cleanup works even if the JobTracker becomes unresponsive to RPC calls. Contributed by Hemanth Yamijala. (detail/ViewSVN)
- HADOOP-2819. The following methods in JobConf are removed: getInputKeyClass() setInputKeyClass getInputValueClass() setInputValueClass(Class theClass) setSpeculativeExecution getSpeculativeExecution(). Contributed by Amareshwari Sri Ramadasu. (detail/ViewSVN)
- HADOOP-2820. The following classes in streaming are removed : StreamLineRecordReader StreamOutputFormat StreamSequenceRecordReader. Contributed by Amareshwari Sri Ramadasu. (detail/ViewSVN)
- HADOOP-2219. A new command ""df -count"" that counts the number of files and directories.  (Tsz Wo (Nicholas), SZE via dhruba) (detail/ViewSVN)
- HADOOP-2912. MiniDFSCluster restart should wait for namenode to exit safemode. This was causing TestFsck to fail.  (Mahadev Konar via dhruba) (detail/ViewSVN)


*Standard Error:*
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testURIPaths(TestDFSShell.java:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=FSNamesystemStatus
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.FSNamesystem.registerMBean(FSNamesystem.java:4183)
	at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:290)
	at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:252)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:132)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testURIPaths(TestDFSShell.java:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testText(TestDFSShell.java:415)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
text: File /texttest/file.gz does not exist.
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testCopyToLocal(TestDFSShell.java:460)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
copyToLocal: null
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testCount(TestDFSShell.java:557)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testFilePermissions(TestDFSShell.java:671)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
chown: could not get status for '/nonExistentFile': File /nonExistentFile does not exist.
chown: could not get status for 'unknownFile': File unknownFile does not exist.
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testDFSShell(TestDFSShell.java:717)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
cat: File /test/mkdirs/myFile1 does not exist.
rm: cannot remove /test/mkdirs/myFile1: No such file or directory.
cp: Cannot copy /test/dir1 to its subdirectory /test/dir1/dir2
javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:52)
	at org.apache.hadoop.dfs.namenode.metrics.NameNodeStatistics.<init>(NameNodeStatistics.java:40)
	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:69)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:177)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:163)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:866)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:113)
	at org.apache.hadoop.dfs.TestDFSShell.testGet(TestDFSShell.java:959)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
get: Checksum error: /blk_3985910936379526606:of:/test/get/testGet.txt at 0
"
HADOOP-2941,Streaming map task is not failed when the subprocess exited abnormally.,"The launched streaming process exited with a non-zero exit code. But the task was succesful. thereby, job was succesful.

Job tracker logs show: 
08/03/05 14:30:43 INFO mapred.TaskTracker: Task task_200803051430_0001_m_000000_1 is done.
08/03/05 14:30:43 INFO mapred.TaskRunner: Saved output of task 'task_200803051430_0001_m_000000_1' to hdfs://localhost:46796/testing/out
08/03/05 14:30:43 INFO mapred.JobInProgress: Task 'task_200803051430_0001_m_000000_1' has completed tip_200803051430_0001_m_000000 successfully.

But the syslog of the task has :
2008-03-05 14:30:43,174 INFO org.apache.hadoop.streaming.PipeMapRed: PipeMapRed.waitOutputThreads(): subprocess failed with code 1 in org.apache.hadoop.streaming.PipeMapRed

We could not know whether the job was successful or not, until we validate the output and found that there is no output."
HADOOP-2939,Make the Hudson patch process an executable ant target,The automatic Hadoop patch testing does some fairly intricate processing.  It would be useful if this was exposed to the developers as an ant target.
HADOOP-2938,some of the fs commands don't globPaths.,"Some of the 'hadoop fs' commands don't globPaths. e.g:

{noformat}
$ bin/hadoop fs -ls '/user/rangadi/2Gb-*'
/user/rangadi/2Gb-1     <r 3>   808587264       2008-03-05 00:36        rw-r--r--       rangadi supergroup
/user/rangadi/2Gb-2     <r 3>   812191744       2008-03-05 00:36        rw-r--r--       rangadi supergroup
$ bin/hadoop fs -rm '/user/rangadi/2Gb-*'
rm: cannot remove /user/rangadi/2Gb-*: No such file or directory.
{noformat}

Mostly related to HADOOP-2063. I think all the commands that use {{DelayedExceptionThrowing}} are affected."
HADOOP-2936,HOD should generate hdfs://host:port on the client side configs.,"with Hadoop-1967,  if the fs.default.name is just host:port it prints out warnings when hadoop shell commands are run. Hod should change this to hdfs://host:port so that users do not see these warnings..."
HADOOP-2935,unit tests failing with nullpointer exception.,"null
java.lang.NullPointerException
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:2328)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:2283)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:51)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:67)
        at org.apache.hadoop.dfs.DFSTestUtil.createFile(DFSTestUtil.java:136)
        at org.apache.hadoop.dfs.DFSTestUtil.createFiles(DFSTestUtil.java:114)
        at org.apache.hadoop.dfs.TestCrcCorruption.thistest(TestCrcCorruption.java:84)
        at org.apache.hadoop.dfs.TestCrcCorruption.testCrcCorruption(TestCrcCorruption.java:223)


I am getting this error while running ant test. TestCrcCorruption failed iwth this error on trunk.


"
HADOOP-2934,NPE while loading  FSImage,"André Martin reported on core-user mailing list :

{noformat} 
[...]
2008-03-02 01:25:29,887 ERROR org.apache.hadoop.dfs.NameNode: java.lang.NullPointerException
    at org.apache.hadoop.dfs.FSImage.readINodeUnderConstruction(FSImage.java:950)
    at org.apache.hadoop.dfs.FSImage.loadFilesUnderConstruction(FSImage.java:919)
    at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:749)
    at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:634)
    at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:223)
    at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:79)
    at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:261)
    at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:242)
    at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
    at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:176)
    at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:162)
    at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:851)
    at org.apache.hadoop.dfs.NameNode.main(NameNode.java:860)
{noformat}

Block object should be allocated before calling {{readFields()}} in FSImage.java : {code}
BlockInfo[] blocks = new BlockInfo[numBlocks];
for (int i = 0; i < numBlocks; i++) {
  blocks[i].readFields(in);
}
{code}

"
HADOOP-2932,"Trash initialization generates ""deprecated filesystem name"" warning even if the name is correct.","HADOOP-1967 made it mandatory to prefix the value of ""fs.default.name"" with ""hdfs://"".
# During name-node initialization the value of the ""fs.default.name"" is set to <host>:<port> without the prefix even if the original name was prefixed with ""hdfs://"". This makes the Trash constructor, which is called with the modified configuration print the following warning:
{code}
08/03/03 17:29:36 WARN fs.FileSystem: ""<host>:<port>"" is a deprecated filesystem name. Use ""hdfs://<host>:<port>/"" instead.
08/03/03 17:29:36 WARN fs.FileSystem: ""<host>:<port>"" is a deprecated filesystem name. Use ""hdfs://<host>:<port>/"" instead.
{code}
The warning is printed twice because FileSystem.getDefaultUri() is called twice during new Trash() and then inside Trash.getEmptier().
# Other than that the name-node never checks the correctness of the ""fs.default.name"", which it should.

As a side note the Trash class should be rearranged IMO. 
- The Trash.getEmtier() should be replaced by 
{code}
  static public Runnable getEmptier(Configuration conf) throws IOException
{code}
- Trash should have only one private constructor, the one that is called in Emtier.run().
- Then we can replace 
{code} new Trash(conf).getEmptier() {code}
with
{code} Trash.getEmptier(conf) {code}
in order to avoid unnecessary creation of the Trash object on the stack."
HADOOP-2931,exception in DFSClient.create: Stream closed,"When writing to the dfs from hbase, I get this exception:

java.io.IOException: Stream closed.
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:1506)
        at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:382)
        at org.apache.hadoop.dfs.DistributedFileSystem.create(DistributedFileSystem.java:123)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:436)
        at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:827)
        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:379)
        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:270)
        at org.apache.hadoop.hbase.HLog.rollWriter(HLog.java:230)
        at org.apache.hadoop.hbase.HRegionServer$LogRoller.run(HRegionServer.java:539)

I'm not totally sure what this means, though, because DFSClient.java:1506 is an instance variable initialization:

    private IOException lastException = new IOException(""Stream closed."");

"
HADOOP-2930,"make {start,stop}-balancer.sh work even if hadoop-daemon.sh isn't in the PATH","The {start,stop}-balancer.sh scripts assume that hadoop-daemon.sh is in the PATH.  Added same boilerplate as other start/stop scripts to remove this assumption (attaching patch)."
HADOOP-2929,need regression tests for setting up cluster and running jobs with different users,"Currently, there is no regression tests for setting up cluster and running jobs with different users.  So, some bugs like HADOOP-2915 cannot be discovered in the regression tests.  

HADOOP-2915 is due to some changes after the permission patches were committed.  This bug did not exist in earlier builds.  For example, build #378 (Jan 24, 2008), which is a build after the permission patches were committed, works fine.  The problem of HADOOP-2915 cannot be reproduced in build #378.

http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/378/
"
HADOOP-2928,"Remove deprecated methods getContentLength() in ClientProtocol, NameNode, FileSystem, DistributedFileSystem and DFSClient","The methods getContentLength() in ClientProtocol, NameNode, FileSystem, DistributedFileSystem and DFSClient are deprecated HADOOP-2219"
HADOOP-2927,Unit test fails on Windows: org.apache.hadoop.fs.TestDU.testDU,"Unit test fails on Windows: org.apache.hadoop.fs.TestDU.testDU

Here is the output from the test: org.apache.hadoop.fs.TestDU.testDU:
junit.framework.AssertionFailedError: expected:<2048> but was:<4096>
	at org.apache.hadoop.fs.TestDU.testDU(TestDU.java:82)

The failure points to the new test code in TestDU.java that just went in as part of HADOOP-2845"
HADOOP-2925,[HOD] Create mapred system directory using a naming convention that will avoid clashes in multi-user shared cluster scenario.,"Currently, HOD generates the name of the mapredsystem directory using the name /mapredsystem/hostname-of-jobtracker.

In HADOOP-2899, we ran into a scenario where this naming convention could lead to problems in case dfs permissions are enabled. While the bug should ideally be addressed in Hadoop M/R, it will be better that HOD does not generate names that can potentially clash across runs. One way to solve the problem is to do what HOD already does for local log and temp directories - name it using username.torque-job-id, which is going to be pretty unique mostly."
HADOOP-2924,HOD is trying to bring up task tracker on  port which is already in close_wait state,"While bringing up task tracker using random ports, HOD is not checking whether the port is in CLOSE_WAIT state. So when it starts task tracker, we will be getting an address bind error on that port. We can avoid this error if we check for CLOSE_WAIT state on that port before starting the tasktracker.

"
HADOOP-2923,Check in missing files from HADOOP-2603,The SequenceFileAsBinaryInputFormat files got missed in the checkin for HADOOP-2603. So I'm checking them in now.
HADOOP-2919,Create fewer copies of buffer data during sort/spill,"Currently, the sort/spill works as follows:

Let r be the number of partitions
For each call to collect(K,V) from map:
* If buffers do not exist, allocate a new DataOutputBuffer to collect K,V bytes, allocate r buffers for collecting K,V offsets
* Write K,V into buffer, noting offsets
* Register offsets with associated partition buffer, allocating/copying accounting buffers if nesc
* Calculate the total mem usage for buffer and all partition collectors by iterating over the collectors
* If total mem usage is greater than half of io.sort.mb, then start a new thread to spill, blocking if another spill is in progress

For each spill (assuming no combiner):
* Save references to our K,V byte buffer and accounting data, setting the former to null (will be recreated on the next call to collect(K,V))
* Open a SequenceFile.Writer for this partition
* Sort each partition separately (the current version of sort reuses, but still requires wrapping, indices in IntWritable objects)
* Build a RawKeyValueIterator of sorted data for the partition
* Deserialize each key and value and call SequenceFile::append(K,V) on the writer for this partition

There are a number of opportunities for reducing the number of copies, creations, and operations we perform in this stage, particularly since growing many of the buffers involved requires that we copy the existing data to the newly sized allocation."
HADOOP-2918,"Enhance log messages to better debug ""No lease on file"" message",HADOOP-2669 describes the scenario where clients lose the lease on a file that it was writing to. This JIRA will provide better logging message to debug this problem.
HADOOP-2915,"mapred output files and directories should be created as the job submitter, not tasktracker or jobtracker","Quoted from an email sending to core-dev by Andy Li:
{quote}
For example, assuming I have installed Hadoop with an account 'hadoop' and I am going to run my program with user account 'test'. I have created an input folder as /user/test/input/ with user 'test' and the permission is set to 0775.
/user/test/input      <dir>          2008-02-27 01:20 rwxr-xr-x      test  hadoop

When I run the MapReduce code, the output I specified will be set to user 'hadoop' instead of 'test'.
/bin/hadoop jar /tmp/test_perm.jar -m 57 -r 3 ""/user/test/input/l"" ""/user/test/output/""

The directory ""/user/test/output/"" will have the following permission and user:group.
/user/test/output    <dir>          2008-02-27 03:53        rwxr-xr-x hadoop  hadoop
{quote}
"
HADOOP-2912,Unit test fails: org.apache.hadoop.dfs.TestFsck.testFsck. This is a regression,"Unit test fails: org.apache.hadoop.dfs.TestFsck.testFsck. This is a regression

It fails on Linux and Windows

Error from the console:
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.SafeModeException: Cannot delete /srcdat. Name node is in safe mode.
Safe mode will be turned off automatically.
	at org.apache.hadoop.dfs.FSNamesystem.deleteInternal(FSNamesystem.java:1469)
	at org.apache.hadoop.dfs.FSNamesystem.delete(FSNamesystem.java:1448)
	at org.apache.hadoop.dfs.NameNode.delete(NameNode.java:388)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:899)

	at org.apache.hadoop.ipc.Client.call(Client.java:512)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
	at org.apache.hadoop.dfs.$Proxy0.delete(Unknown Source)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at org.apache.hadoop.dfs.$Proxy0.delete(Unknown Source)
	at org.apache.hadoop.dfs.DFSClient.delete(DFSClient.java:419)
	at org.apache.hadoop.dfs.DistributedFileSystem.delete(DistributedFileSystem.java:162)
	at org.apache.hadoop.dfs.DFSTestUtil.cleanup(DFSTestUtil.java:214)
	at org.apache.hadoop.dfs.TestFsck.testFsck(TestFsck.java:75)

Recent changes:
   1. HADOOP-1986. Add support for a general serialization mechanism for Map Reduce.
   2. HADOOP-2800. Deprecate SetFile.Writer constructor not the whole class. Contributed by Johan Oskarsson.
   3. HADOOP-1985. This addresses rack-awareness for Map tasks and for HDFS in a uniform way. Contributed by Devaraj Das.
   4. HADOOP-2063. A new parameter to dfs -get command to fetch a file even if it is corrupted. (Tsz Wo (Nicholas), SZE via dhruba)
   5. HADOOP-2894. Fix a problem to do with tasktrackers failing to connect to JobTracker upon reinitialization. Contributed by Owen O'Malley.
   6. HADOOP-2871. Fixed the CHANGES.txt problem.
   7. HADOOP-2727. Fixes a problem to do with file: URI in the JobHistory init. Contributed by Amareshwari Sri Ramadasu."
HADOOP-2911,[HOD] Make the information printed by allocate and info commands less verbose and clearer,"Currently hod prints the following information as part of the allocate command at an 'INFO' verbose level:

Service Registry started
Torque job id
Ringmaster ..
HDFS UI at ..
Mapred UI at ..

The ""service registry started"" line is misleading and isn't very useful. This should be removed.
Torque job id is misleading in terminology because job id also clashes with Mapred ""job id"". The proposal is to make it something like 'cluster id' or something similar.
Ringmaster information is useful for debugging, but not for regular users.

Similar changes should be made to the hod info command as well. The naming should be clear and consistent. There, we print all of the above information and in addition, also print the number of nodes as min,max. Because we don't really support the min,max number of nodes, we can remove that and probably print only the number of nodes allocated.

All information must continue to be printed at a higher verbosity level.

"
HADOOP-2910,Throttle IPC Client/Server during bursts of requests or server slowdown,"I propose the following to avoid an IPC server being swarmed by too many requests and connections
1. Limit call queue length or limit the amount of memory used in the call queue. This can be done by including the size of a request in the header and storing unmarshaled requests in the call queue. 
2. If the call queue is full or queue buffer is full, stop reading requests from sockets. So requests stay at the server's system buffer or at the client side and thus eventually throttle the client. 
3. Limit the total number of connections. Do not accept new connections if the connection limit is exceeded. (Note: this solution is unfair to new connections.) 
4. If receive out of memory exception, close the current connection. 

"
HADOOP-2909,Improve IPC idle connection management,"IPC server determines if a connection is idle or not by checking if the connection does not have any IO activity for a predefined max idle time. An idle connection will be closed even if the connection still has outstanding requests or replies. This causes RPC failures when a server becomes slow or if a request takes a long time to be served. In jira, I'd like to propose the following changes to IPC idle management:
1. Add data structures to the IPC server that keep track of outstanding requests.
2. IPC server does not close a connection that has outstanding requests/replies even when it has no IO activities for a while.
3. The default client-side max idle time should be in several minutes not 1 second. 
4. The server-side max idle time should be greater than the client-side max idle time, for example, twice of the client-side max idle time. So server mainly deals with clients that are crashed without closing 
its connections. 
"
HADOOP-2908,forrest docs for dfs shell commands and semantics.,add forrest documents for dfs shell command behaviours and semantics.
HADOOP-2906,output format classes that can write to different files depending on  keys and/or config variable,"I've a few apps that require to write out data into different files/directories depending on keys and/or configuration variables.
I've implemented such classes for those apps. I noticed that many other users have similar need from time to time.
So I think it may be a good idea to contribute to Hadoop mapred.lib package so that other users can benefit from it.
"
HADOOP-2905,fsck -move triggers NPE in namenode,"If I run hadoop fsck / -move, then the fsck fails to move any corrupt files.  In the namenode logs, I see this error message repeated 3 times:

2008-02-26 21:19:07,500 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 10000, call mkdirs(/lost+found, null) from x.x.x.135:60819: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.INode.setPermission(INode.java:123)
        at org.apache.hadoop.dfs.INode.setPermissionStatus(INode.java:86)
        at org.apache.hadoop.dfs.INode.<init>(INode.java:79)
        at org.apache.hadoop.dfs.INodeDirectory.<init>(INode.java:319)
        at org.apache.hadoop.dfs.FSDirectory.mkdirs(FSDirectory.java:633)
        at org.apache.hadoop.dfs.FSNamesystem.mkdirsInternal(FSNamesystem.java:1569)
        at org.apache.hadoop.dfs.FSNamesystem.mkdirs(FSNamesystem.java:1544)
        at org.apache.hadoop.dfs.NameNode.mkdirs(NameNode.java:420)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:910)
2008-02-26 21:19:07,503 WARN org.apache.hadoop.dfs.NameNode: Cannot initialize /lost+found .
"
HADOOP-2904,3 minor fixes in the rpc metrics area.,"3 minor fixes in the rpc metrics moddule:
 corrects the Logger name, fixes the metrics tag (port instead of server name), logs the initialization of the rpc metrics."
HADOOP-2903,Data type mismatch exception raised from pushMetric,"incrMetric takes a int argument. However in pushMetric since getPreviousIntervalAverageTime() returns a long an exception was raised.

Fix is to cast getPreviousIntervalAverageTime() to an int"
HADOOP-2902,"replace accesss of ""fs.default.name"" with FileSystem accessor methods","HADOOP-1967 added accessor methods to set the default filesystem.  We should start using them.

While doing this, we should also replace uses of ""local"" and ""host:port"" with proper URIs, e.g., ""file:///"" and ""hdfs://host:port/"".  This will silence warnings about the use of old-format names.
"
HADOOP-2901,the job tracker should not start 2 info servers,The job tracker should not start a second info server for job history.
HADOOP-2899,[HOD] hdfs:///mapredsystem directory not cleaned up after deallocation ,"Each submitted job creates a hdfs:///mapredsystem directory, created by (I guess) the hodring process. Problem is that it's not cleaned up at the end of the process; a use case would be:

- user A allocates a cluster, the hodring is svrX, so a /mapredsystem/srvX directory is created

- user A deallocates the cluster, but that directory is not cleaned up

- user B allocates a cluster, and the first node chosen as hodring is svrX, so hodring tries to write hdfs:///mapredsystem but it fails

- allocation succeeds, but there's no hodring running; looking at
0-jobtracker/logdir/hadoop.log under the temporary directory I can read:

2008-02-26 17:28:42,567 WARN org.apache.hadoop.mapred.JobTracker: Error starting tracker: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=B, access=WRITE, inode=""mapredsystem"":hadoop:supergroup:rwxr-xr-x

I guess a possible solution would be to clean up those directories during the deallocation process. 
"
HADOOP-2898,HOD should allow setting MapReduce UI ports within a port range,"HOD currently does now allow to explicitly specify ports or a port range in the MapReduce and HDFS sections, but this could be useful. A typical example would involve firewall settings that allow only a certain range of ports. "
HADOOP-2897,[HOD] Remove script option from the core hod framework,"Hod currently allows the user to specify and run a hadoop script after allocation, and deallocating as soon as the script is done. For e.g.

hod -m 3 -z ~/hadoop.script

allocates 3 nodes, and runs ~/hadoop.script, then deallocates

This is a convenient way single line wrapper around 4-5 commands that users have to write themselves. We have this because:

- hod 0.3 does not provide an easy way to combine these into a single operation, because of the HOD shell.
- even in hod 0.4, users have to carefully write some error checking code to make sure their cluster is allocated successfully, before running the script and their HADOOP_CONF_DIR should be set correctly.
- users can free up clusters as soon as they are done.

The requirements make sense. But having this as part of the core hod interface seems incorrect. The interface should be an orthogonal set of commands that each just do one thing well. The script option should be converted to a simple wrapper that can be part of the hod project. This way, users can enjoy the benefits of not having to write such a script themselves, while the hod codebase can still be clean.

One disadvantage if we change this is that users will need to remember one more command. But given hod 0.4 is a new interface anyway, it is better to address now, rather than later. And we can alleviate this a bit by making sure options are consistently named between hod and the wrapper script."
HADOOP-2896,Using transient jetty servers as guis is a bad idea,Using transient jetty servers (ie. one that last 30 minutes) is a very poor replacement for a gui. I would much rather have bin/hadoop job -history out-dir print a textual summary rather than start a jetty server on the client machine that needs to be queried by the user.
HADOOP-2895,String for configuring profiling should be customizable,"The choice of options that Hadoop uses may not be appropriate for all cases, so it would make sense to make this string configurable."
HADOOP-2894,task trackers can't survive a job tracker bounce,If your Job Tracker fails and is restarted all of the Task Trackers will die when the new one comes up.
HADOOP-2891,The dfsclient on exit deletes files that are open and not closed.,"the dfsclient has a shutdown hook that deletes files that have been left open but not closed. We should not be cleaning up files that have been open and not closed. 
"
HADOOP-2890,HDFS should recover when  replicas of block have different sizes (due to corrupted block),"We had a case where reading a file caused IOException.
08/02/25 17:23:02 INFO fs.DFSClient: Could not obtain block blk_-8333897631311887285 from any node:  java.io.IOException: No live nodes contain current block

hadoop fsck said the block was healthy.
[lohit]$ hadoop fsck part-04344 -files -blocks -locations | grep 8333897631311887285
21. -8333897631311887285 len=134217728 repl=3 [74.6.129.238:50010, 74.6.133.231:50010, 74.6.128.158:50010]

Looking for logs about the block showed this message in namenode log
17:26:23,543 WARN org.apache.hadoop.fs.FSNamesystem: Inconsistent size for block blk_-8333897631311887285 reported from 74.6.133.231:50010 current size is 134217728 reported size is 134205440

So, the namenode was expecting 134217728 while the actual block size was 134205440

Dhruba took a look at the logs further and we found out this is what had happend
1. While the file was being created this block was replicated to three nodes of which 2 nodes had correct sized block, but the third node has partial/truncated block. (but the metadata was same on all nodes)
2. Later after 3 days namenode was restarted, at which point the 3rd node reported warning message about incorrect block size. (Namenode logged this)
3. After few days the first 2 nodes went down and the 3rd node replicated the partial/truncated block to two new nodes. 
4. Now when we tried to read this block, we hit the IOException
5. On all the nodes, the metadata corresponded to the original valid block while the block itself was missing around 12K of data.

Two problems which could be fixed here
1. When namenode identifies replicas with different blocksize (point 2 above). It could choose the biggest block and discard the small block. If the block is not the last block, then its size has to be equal to the block size, anything less than that could be considered bad block.
2. Datanode Block periodic verifier could also verify that the metadata has the correct size as that of the actual block present. Any changes should be reported/recovered considering what would be done in above step.
"
HADOOP-2888,Enhancements to gridmix scripts,"I would like to propose enhancements to the gridmix scripts to make it:
1. easier to setup parameters for the test run and data generation (makes it easier to automate the runs using something like hudson)
2. ensure the benchmarks wait until they are completed (makes it easier to automate the runs using something like hudson)

Here are the details:
Ability to override these parameters in gridmix-env
* HADOOP_HOME
* GRID_MIX_HOME
* EXAMPLE_JAR
* APP_JAR
* STREAM_JAR
* GRID_MIX_DATA
* GRID_MIX_PROG

Ability to override these parameters in generateData.sh
* COMPRESSED_DATA_BYTES
* UNCOMPRESSED_DATA_BYTES
* INDIRECT_DATA_BYTES

Ability for the tests submitted to the same cluster to wait until they are done. Changes will be in:
* submissionScripts/monsterQueriesToSameCluster
* submissionScripts/maxentToSameCluster
* submissionScripts/textSortToSameCluster
* submissionScripts/webdataScanToSameCluster
* submissionScripts/webdataSortToSameCluster"
HADOOP-2886,Track individual RPC metrics.,"There is currently no mechanism to track performance metrics at the granularity of a specific RPC. So For e.g. if we wanted to capture average latency for the openFile RPC or for the createFile RPC the current infrastructure does not support that.

The implementation involves having a simple HashMap where every new Rpc metric being added would be inserted into the HashMap. Since there is a mechanism to obtain RPC latencies already (without the name of the specific RPC), the identification of what RPC is involved would be done by doing a lookup on the HashMap.
"
HADOOP-2883,Extensive write failures,"With the new release 0.16.0 we experience extensive write failures under heavy load.

The job shuffles 300TB on 1400 nodes and runs 3 waves of 2500 reducers. Each reducer uses libhdfs to write to around 70 dfs files simultaneously. We did not experience particular write problems up to nightly build #835 on hadoopqa (Jan 28),
but now with released 0.16.0 (candidate 2) we see a lot of exceptions related to 'all datanodes are bad':

typical exception(s):

08/02/22 10:34:47 WARN fs.DFSClient: Error Recovery for block blk_434406883423887779 in pipeline xxx.xxx.xxx.146:50010, xxx.xxx.xxx.224:50010: bad datanode xxx.xxx.xxx.146:50010
08/02/22 10:34:51 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:34:51 WARN fs.DFSClient: Error Recovery for block blk_-1957866292089920792 in pipeline xxx.xxx.xxx.147:50010, xxx.xxx.xxx.10:50010: bad datanode xxx.xxx.xxx.147:50010
08/02/22 10:34:54 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:34:54 WARN fs.DFSClient: Error Recovery for block blk_-5265240773298481019 in pipeline xxx.xxx.xxx.152:50010, xxx.xxx.xxx.71:50010: bad datanode xxx.xxx.xxx.152:50010
08/02/22 10:34:54 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:34:54 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed outxxx.xxx.xxx.166:50010
08/02/22 10:34:55 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:00 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:00 WARN fs.DFSClient: Error Recovery for block blk_8456718220685890569 in pipeline xxx.xxx.xxx.158:50010, xxx.xxx.xxx.225:50010: bad datanode xxx.xxx.xxx.158:50010
08/02/22 10:35:00 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:00 WARN fs.DFSClient: Error Recovery for block blk_1420965154382429572 in pipeline xxx.xxx.xxx.169:50010, xxx.xxx.xxx.221:50010: bad datanode xxx.xxx.xxx.169:50010
08/02/22 10:35:00 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:00 WARN fs.DFSClient: Error Recovery for block blk_-519424763987472708 in pipeline xxx.xxx.xxx.154:50010, xxx.xxx.xxx.37:50010: bad datanode xxx.xxx.xxx.154:50010
08/02/22 10:35:00 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:00 WARN fs.DFSClient: Error Recovery for block blk_-8376556524788296783 in pipeline xxx.xxx.xxx.154:50010, xxx.xxx.xxx.212:50010: bad datanode xxx.xxx.xxx.154:50010
08/02/22 10:35:00 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:00 WARN fs.DFSClient: Error Recovery for block blk_-2429564741658530079 in pipeline xxx.xxx.xxx.160:50010, xxx.xxx.xxx.105:50010: bad datanode xxx.xxx.xxx.160:50010
08/02/22 10:35:00 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:00 WARN fs.DFSClient: Error Recovery for block blk_-6653210787685458124 in pipeline xxx.xxx.xxx.143:50010, xxx.xxx.xxx.37:50010: bad datanode xxx.xxx.xxx.143:50010
08/02/22 10:35:01 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:01 WARN fs.DFSClient: Error Recovery for block blk_7515160028005424426 in pipeline xxx.xxx.xxx.167:50010, xxx.xxx.xxx.152:50010: bad datanode xxx.xxx.xxx.167:50010
08/02/22 10:35:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:03 WARN fs.DFSClient: Error Recovery for block blk_-7191475898558388503 in pipeline xxx.xxx.xxx.139:50010, xxx.xxx.xxx.6:50010: bad datanode xxx.xxx.xxx.139:50010
08/02/22 10:35:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:03 WARN fs.DFSClient: Error Recovery for block blk_-340745015348833165 in pipeline xxx.xxx.xxx.141:50010, xxx.xxx.xxx.153:50010: bad datanode xxx.xxx.xxx.141:50010
08/02/22 10:35:04 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:04 WARN fs.DFSClient: Error Recovery for block blk_-6861254790596076341 in pipeline xxx.xxx.xxx.157:50010, xxx.xxx.xxx.224:50010: bad datanode xxx.xxx.xxx.157:50010
08/02/22 10:35:14 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:14 INFO fs.DFSClient: Abandoning block blk_6188945400680100475
08/02/22 10:35:14 INFO fs.DFSClient: Waiting to find target node: xxx.xxx.xxx.161:50010
08/02/22 10:35:43 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:47 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:48 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:49 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:49 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:50 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:50 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:50 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:53 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:54 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:57 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:35:57 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:03 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:04 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:06 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:06 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
08/02/22 10:36:07 INFO fs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: Read timed out
Exception in thread ""main"" java.io.IOException: All datanodes xxx.xxx.xxx.83:50010 are bad. Aborting...
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:1839)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1479)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1571)
Call to org.apache.hadoop.fs.FSDataOutputStream::write failed!"
HADOOP-2880,libhdfs: O_WRONLY/O_RDONLY different when including fcntl.h,"In hdfs.h, 

#ifndef O_RDONLY
#define O_RDONLY 1
#endif

#ifndef O_WRONLY
#define O_WRONLY 2
#endif

but on linux, 

$ grep ""define O_RDONLY"" /usr/include/*/*
/usr/include/asm-i386/fcntl.h:#define O_RDONLY       00
/usr/include/asm-x86_64/fcntl.h:#define O_RDONLY             00
/usr/include/bits/fcntl.h:#define O_RDONLY           00

$ grep ""define O_WRONLY"" /usr/include/*/*
/usr/include/asm-i386/fcntl.h:#define O_WRONLY       01
/usr/include/asm-x86_64/fcntl.h:#define O_WRONLY             01
/usr/include/bits/fcntl.h:#define O_WRONLY           01


It took me a while to debug when hdfsOpenFile was trying to 'write' when I meant 'read'.
"
HADOOP-2878,Hama code contribution,"*Introduction*

Hama will develop a high-performance and large-scale parallel matrix computational package based on Hadoop Map/Reduce. It will be useful for a massively large-scale Numerical Analysis and Data Mining, which need the intensive computation power of matrix inversion, e.g. linear regression, PCA, SVM and etc. It will be also useful for many scientific applications, e.g. physics computations, linear algebra, computational fluid dynamics, statistics, graphic rendering and many more.

Hama approach proposes the use of 3-dimensional Row and Column (Qualifier), Time space and multi-dimensional Columnfamilies of Hbase (BigTable Clone), which is able to store large sparse and various type of matrices (e.g. Triangular Matrix, 3D Matrix, and etc.). its auto-partitioned sparsity sub-structure will be efficiently managed and serviced by Hbase. Row and Column operations can be done in linear-time, where several algorithms, such as structured Gaussian elimination or iterative methods, run in O(the number of non-zero elements in the matrix / number of mappers) time on Hadoop Map/Reduce.

So, it has a strong relationship with the hadoop project, and it would be great if the ""hama"" can become a contrib project of the hadoop

*Current Status*

In its current state, the 'hama' is buggy and needs filling out, but generalized matrix interface and basic linear algebra operations was implemented within a large prototype system. In the future, We need new parallel algorithms based on Map/Reduce for performance of heavy decompositions and factorizations. It also needs tools to compose an arbitrary matrix only with certain data filtered from hbase array structure.

It would be great if we can collaboration with the hadoop members."
HADOOP-2873,Namenode fails to re-start after cluster shutdown - DFSClient: Could not obtain blocks even all datanodes were up & live,"Namenode fails to re-start with the following exception:

 2008-02-21 14:20:48,831 INFO org.apache.hadoop.dfs.NameNode: STARTUP_MSG:
 /************************************************************
 STARTUP_MSG: Starting NameNode
 STARTUP_MSG:   host = se09/141.76.xxx.xxx
 STARTUP_MSG:   args = []
 STARTUP_MSG:   version = 2008-02-19_11-01-48
 STARTUP_MSG:   build = http://svn.apache.org/repos/asf/hadoop/core/trunk -r 628999; compiled by 'hudson' on Tue Feb 19 11:09:05 UTC 2008
 ************************************************************/
 2008-02-21 14:20:49,367 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing RPC Metrics with serverName=NameNode, port=8000
 2008-02-21 14:20:49,374 INFO org.apache.hadoop.dfs.NameNode: Namenode up at: se09.inf.tu-dresden.de/141.76.xxx.xxx:8000
 2008-02-21 14:20:49,378 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null
 2008-02-21 14:20:49,381 INFO org.apache.hadoop.dfs.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.spi.NullContext
 2008-02-21 14:20:49,501 INFO org.apache.hadoop.fs.FSNamesystem: fsOwner=amartin,students
 2008-02-21 14:20:49,501 INFO org.apache.hadoop.fs.FSNamesystem: supergroup=supergroup
 2008-02-21 14:20:49,501 INFO org.apache.hadoop.fs.FSNamesystem: isPermissionEnabled=true
 2008-02-21 14:20:49,788 INFO org.apache.hadoop.ipc.Server: Stopping server on 8000
 2008-02-21 14:20:49,790 ERROR org.apache.hadoop.dfs.NameNode: java.io.IOException: Created 13 leases but found 4
     at org.apache.hadoop.dfs.FSImage.loadFilesUnderConstruction(FSImage.java:935)
     at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:749)
     at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:634)
     at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:223)
     at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:79)
     at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:261)
     at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:242)
     at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
     at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:176)
     at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:162)
     at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:851)
     at org.apache.hadoop.dfs.NameNode.main(NameNode.java:860)

 2008-02-21 14:20:49,791 INFO org.apache.hadoop.dfs.NameNode: SHUTDOWN_MSG:
 /************************************************************
 SHUTDOWN_MSG: Shutting down NameNode at se09/141.76.xxx.xxx
 ************************************************************/ 

Cluster restart was needed since the DFS client produced the following error message even all datanodes were up:

 08/02/21 14:04:35 INFO fs.DFSClient: Could not obtain block blk_-4008950704646490788 from any node:  java.io.IOException: No live nodes contain current block

"
HADOOP-2872,Default value for hadoop.job.history.location is broken,"The default value for hadoop.job.history.location is currently:

file://${hadoop.log.dir}/history

which assumes that hadoop.log.dir is an absolute path. If the path is relative, the job tracker dies with:

08/02/21 23:43:04 FATAL mapred.JobTracker: java.lang.IllegalArgumentException: Wrong FS: file://./logs/history, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:268)
        at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:54)
        at org.apache.hadoop.fs.RawLocalFileSystem.exists(RawLocalFileSystem.java:223)
        at org.apache.hadoop.fs.FilterFileSystem.exists(FilterFileSystem.java:147)
        at org.apache.hadoop.mapred.JobHistory.init(JobHistory.java:124)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:706)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:124)
        at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:2146)

I think that the right fix is to make the default value:

""history""

and when building, use the Path code:

{code}
new Path(get(""hadoop.log.dir"", conf), get(""hadoop.job.history.location""));
{code}
"
HADOOP-2871,Unit tests (16) fail on Windows due to java.lang.IllegalArgumentException causing MiniMRCluster to not start up,"Unit tests (16) fail on Windows due to java.lang.IllegalArgumentException causing MiniMRCluster to not start up. It works fine on Linux and Solaris

I suspect it is due to this fix: HADOOP-2178. Job History on DFS

All changes that went in:
1. HADOOP-2769. TestNNThroughputBnechmark should not use a fixed port for the namenode http port.
2. HADOOP-2192. Error messages from ""dfs mv"" command improved.
3. HADOOP-2178. Job History on DFS.
4. HADOOP-2730. HOD documentation update.
5. HADOOP-2371. Commit for the html/pdf docs.
6. HADOOP-2766. Enables setting of HADOOP_OPTS env variable for the hadoop daemons through HOD.

Failing tests:
	org.apache.hadoop.ipc.TestSocketFactory.unknown
	org.apache.hadoop.mapred.TestClusterMRNotification.unknown
	org.apache.hadoop.mapred.TestClusterMapReduceTestCase.unknown
	org.apache.hadoop.mapred.TestEmptyJobWithDFS.unknown
	org.apache.hadoop.mapred.TestJobStatusPersistency.unknown
	org.apache.hadoop.mapred.TestMRServerPorts.testJobTrackerPorts
	org.apache.hadoop.mapred.TestMRServerPorts.testTaskTrackerPorts
	org.apache.hadoop.mapred.TestMiniMRBringup.unknown
	org.apache.hadoop.mapred.TestMiniMRClasspath.unknown
	org.apache.hadoop.mapred.TestMiniMRDFSCaching.unknown
	org.apache.hadoop.mapred.TestMiniMRDFSSort.unknown
	org.apache.hadoop.mapred.TestMiniMRLocalFS.unknown
	org.apache.hadoop.mapred.TestMiniMRMapRedDebugScript.unknown
	org.apache.hadoop.mapred.TestMiniMRTaskTempDir.unknown
	org.apache.hadoop.mapred.TestMiniMRWithDFS.unknown
	org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath.unknown

Snippet from console:
    [junit] java.lang.IllegalArgumentException: Wrong FS: file://C:\hudson\workspace\Hadoop-WindowsTest\trunk/build/test/logs/history, expected: file:///
    [junit] 	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:268)
    [junit] 	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:54)
    [junit] 	at org.apache.hadoop.fs.RawLocalFileSystem.exists(RawLocalFileSystem.java:223)
    [junit] 	at org.apache.hadoop.fs.FilterFileSystem.exists(FilterFileSystem.java:147)
    [junit] 	at org.apache.hadoop.mapred.JobHistory.init(JobHistory.java:124)
    [junit] 	at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:706)
    [junit] 	at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:124)
    [junit] 	at org.apache.hadoop.mapred.MiniMRCluster$JobTrackerRunner.run(MiniMRCluster.java:73)
    [junit] 	at java.lang.Thread.run(Thread.java:595)
    [junit] 2008-02-21 10:21:29,319 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:30,318 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:31,318 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:32,318 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:33,317 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:34,317 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:35,316 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:36,316 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:37,316 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:38,315 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start...
    [junit] 2008-02-21 10:21:39,315 INFO  mapred.MiniMRCluster (MiniMRCluster.java:<init>(281)) - Waiting for JobTracker to start..."
HADOOP-2870,Datanode.shutdown() and Namenode.stop() should close all rpc connections,"Currently this two cleanup methods do not close all existing rpc connections. If a mini dfs cluster gets shutdown and then restarted as we do in TestFileCreation, RPCs in second mini cluster reuse the unclosed connections opened in the first run but there is no server running to serve the request. So the client get stuck waiting for the response forever if client side timeout gets removed as suggested by hadoop-2811."
HADOOP-2869,Deprecate and remove SequenceFile.setCompressionType,"The right context for compression is either:

a) SequenceFile.createWriter
b) SequenceFileOutputFormat.setCompressionType
c) JobConf.setMapOutputCompressionType

Hence, we need to remove (deprecate for now) SequenceFIle.setCompressionType."
HADOOP-2867,Add a task's cwd to it's LD_LIBRARY_PATH,"HADOOP-1660 added the task's cwd to it's java.library.path which means only java task's can take advantage via System.load or System.loadLibrary... we should enhance it to support Hadoop Pipes applications by adding it to the LD_LIBRARY_PATH so they can use dlopen/dlsym etc.
"
HADOOP-2865,FsShell.ls() should print file attributes first then the path name.,"When we had a handful of attributes this looked OK. But after the permissions the ls output is just unreadable.
This how the output looks now:
{code}
/CHANGES.txt    <r 2>   174779  2008-01-29 10:00        rw-r--r--       shv     supergroup
/LICENSE.txt    <r 2>   11358   2008-01-28 17:27        rw-r--r--       shv     supergroup
/NOTICE.txt     <r 2>   101     2008-01-29 12:06        rw-r--r--       shv     supergroup
/Work   <dir>           2008-01-23 17:43        rwxr-xr-x       shv     supergroup
/Work/hadoop-data       <dir>           2008-01-23 17:43        rwxr-xr-x       shv     supergroup
{code}
This is how it should look:
{code}
2   174779  2008-01-29 10:00       -rw-r--r--       shv     supergroup     /CHANGES.txt
2   11358   2008-01-28 17:27       -rw-r--r--       shv     supergroup     /LICENSE.txt
2   101     2008-01-29 12:06       -rw-r--r--       shv     supergroup     /NOTICE.txt
            2008-01-23 17:43       drwxr-xr-x       shv     supergroup     /Work
            2008-01-23 17:43       drwxr-xr-x       shv     supergroup     /Work/hadoop-data
{code}
"
HADOOP-2863,FSDataOutputStream should not flush() inside close().,"Why does FSDataOutputStream.close() call flush()? This stream itself does not store any data that it needs to flush. It is a wrapper and it should just invoke its outputstream's close().

For. e.g one bad side effect is that, in the case of DFSOutputStream which extends FSOutputSummer, flush() inside close sends the current data even though FSOutputSummer might have some data.. this left over data will be sent in side close() (so it sends data in two different packets instead of one). Other filesystems might have similar side effects.

I will submit a patch.

"
HADOOP-2862,[HOD] Support PBS env vars in hod configuration,"In some batch environments, eg using Torque PBS, scratch spaces are provided on cluster nodes for where jobs should put their temporary files. These are automatically cleaned up when the job exists by an epilogue script.

For instance, in our local Torque cluster, all nodes have a /scratch partition. For each job, the prologue script creates a scratch folder owned by the user at /scratch/pbstmp.$PBS_JOBID - $PBS_JOBID is then the env var containing the job id, as set by pbs_mom.

Would it be possible to use these env vars in the configuration of hod. For instance, say I want to create an hdfs on demand using hod, but that the hdfs space should be in /scratch/pbstmp.$PBS_JOBID, not in /tmp/hod say. This would involve HOD supporting env vars in configuration, but knowing when to substitute the env var with it's current value (ie not until running on the correct node where the operation should take place)."
HADOOP-2861,[HOD] Improve the user interface for the HOD commands,"The current command line for HOD is not user-friendly or conventional. Following improvements are required:

- We shouldn't require the quotes around the operation.
- Can we define a HOD_CLUSTER_DIR environment variable which would be used by default ?

With these two, one can imagine a very simple command line for HOD. Something like:

hod-allocate 3
hod-deallocate

hod-allocate 3 ~/hod-clusters/test
hod-deallocate ~/hod-clusters/test

This would also be backward compatible for people who're used to the current model."
HADOOP-2858,[HOD] The Idle cluster clean-up code should check for idleness based on last job,
HADOOP-2857,libhdfs: no way to set JVM args other than classpath,"I would like a way to set other Java system properties and/or other Java VM arguments when using libhdfs - i.e. I want to be able to say start a profiler, attach a debugger, or increase the memory available to the VM.

At present, new JVMs are initialised from the getJNIEnv(void) method in src/c++/libhdfs/hdfsJniHelper.c
This method initialises the -Djava.class.path JVM argument from the CLASSPATH environment variable. I am proposing that another env variable should be added, the contents of which is passed (almost) verbatim as JVM arguments.

Eg, say the env var JVM_ARGS is used. The string from the env var would be tokenised on ""one or more spaces"", and would be passed as additional JVM arguments, by make a larger JavaVMOption options[] array, and setting vm_args.nOptions = 1+ number of passed arguments.

Only flaw I can see with this is that none of the passed parameters can contain spaces."
HADOOP-2855,[HOD] HOD fails to allocate a cluster if the tarball specified is a relative path,"Run hod -t my-tar.tar.gz -o ""allocate hod-cluster 3"". Ringmaster fails to come up. The log shows the exception as an invalid URL for the tarball file. Basically HOD should translate the relative path to an absolute path and send it to the ringmaster.

"
HADOOP-2854,Remove the deprecated ipc.Server.getUserInfo(),The method org.apache.hadoop.ipc.Server.getUserInfo() is deprecated and is no longer used by other codes.
HADOOP-2853,Add Writable for very large lists of key / value pairs,"Some map-reduce jobs need to aggregate and process very long lists as a single value. This usually happens when keys from a large domain are mapped into a small domain, and their associated values cannot be aggregated into few values but need to be preserved as members of a large list. Currently this can be implemented as a MapWritable or ArrayWritable - however, Hadoop needs to deserialize the current key and value completely into memory, which for extremely large values causes frequent OOM exceptions. This also works only with lists of relatively small size (e.g. 1000 records).

This patch is an implementation of a Writable that can handle arbitrarily long lists. Initially it keeps an internal buffer (which can be (de)-serialized in the ordinary way), and if the list size exceeds certain threshold it is spilled to an external SequenceFile (hence the name) on a configured FileSystem. The content of this Writable can be iterated, and the data is pulled either from the internal buffer or from the external file in a transparent way."
HADOOP-2852,Update gridmix to avoid artificially long tail,"The MaxEntropy test in the gridmix benchmark is submitted late into the queue, iterating past the point where the cluster is saturated. This dilutes the throughput measurement- the purpose of this benchmark- by making the tail overly dependent on the performance of a single job."
HADOOP-2848,"[HOD] If a cluster directory is deleted, hod -o list must show it, and deallocate should work.","Currently if the cluster directory is deleted, all state about the cluster is lost. While this in itself is not a problem, at least recovery in the sense of being able to list the torque job id and deallocation to clear up the nodes should happen correctly."
HADOOP-2847,[HOD] Idle cluster cleanup does not work if the JobTracker becomes unresponsive to RPC calls,"In some erroneous conditions, the Hadoop JobTracker becomes unresponsive to RPC calls (for e.g. if a misconfiguration causes the JobTracker to run out of memory). In such cases, a cluster allocated by HOD no longer runs any jobs and is wastefully holding up nodes. The usual idle cluster cleaner should deallocate the cluster ideally, but it does not.

"
HADOOP-2845,dfsadmin disk utilization report on Solaris is wrong,"dfsadmin reports 2x disk utilization on some platforms (Solaris, MacOS). The reason for this is that org.apache.hadoop.fs.DU is relying on du's default block size when reporting sizes and assuming they are 1024 byte blocks. This works fine on Linux, but du Solaris and MacOS uses 512-byte blocks to report disk usage.

DU should use ""du -sk"" instead of ""du -s"" to force the command to report sizes based on 1024 byte blocks.
"
HADOOP-2844,A SequenceFile.Reader object is not closed properly in CopyFiles,"In CopyFiles line 186, the SequenceFile.Reader sl is never closed."
HADOOP-2843,mapred.join access control is overly restrictive,"Currently, most of the o.a.h.mapred.join code is package-private, permitting meaningful extension only from MultiFilterRecordReader and JoinRecordReader. It should be opened up to permit users to add operations not strictly descending from the existing hierarchy."
HADOOP-2842,NameNode should throw FileNotFoundException for nonexistent files,"Currently namenode throws an IOException with a ""File does not exist"" message for nonexistent files. It would be better to throw FileNotFound exception. So the client does not need to parse the error message to find out the real cause of the I/O error."
HADOOP-2841,Dfs methods should not throw RemoteException,Dfs should unwrap the RemoteException and throw the real cause of the error to the user. This allows the user to find out the real cause without examining the remote exception. This also allows the user to use the dfs interface without aware of the implementation details (i.e. rpc).
HADOOP-2840,Gridmix test script fails to run java sort tests,"The gridmix test script fails to run the java sort tests

The script makes use of APP_JAR instead of EXAMPLE_JAR to run the sort job"
HADOOP-2839,Remove deprecated methods in FileSystem,Remove deprecated methods like listPath and globPath and fix all the use of these deprecated methods in FsShell.
HADOOP-2838,Add HADOOP_LIBRARY_PATH config setting so Hadoop will include external directories for jni,"Currently there is no way to configure Hadoop to use external JNI directories. I propose we add a new variable like HADOOP_CLASS_PATH that is added to the JAVA_LIBRARY_PATH before the process is run.

Now the users can set environment variables using mapred.child.env. They can do the following 
X=Y : set X to Y
X=$X:Y : Append Y to X (which should be taken from the tasktracker)"
HADOOP-2833,"JobClient.submitJob(...) should not use ""Dr Who"" as a default username","In JobClient line 530, we have
{code}
String user = System.getProperty(""user.name"");
job.setUser(user != null ? user : ""Dr Who"");
{code}
Since a ugi is already obtained in the earlier codes, we should get the username from ugi, not System.getProperty(""user.name"").  Also, Exception should be thrown if the username is null, instead of using a default."
HADOOP-2832,bad code indentation in DFSClient,A few lines of code in DFSClient are mis-indented because of the presence of tabs. The tabs should be removed. 
HADOOP-2831,Remove the deprecated INode.getAbsoluteName(),"The deprecated INode.getAbsoluteName()  should be removed so that INode.parent could be eliminated later on.

INode.getAbsoluteName() currently is only used in FSImage.writeINodeUnderConstruction(...) .  It also introduces unnecessary recursion."
HADOOP-2828,Remove deprecated methods in Configuration.java,"The following methods in Configuration.java needs to be removed as they are deprecated.
getObject(String name)
setObject(String name, Object value)
get(String name, Object defaultValue)
set(String name, Object value)
Iterator entries()"
HADOOP-2827,Remove deprecated NetUtils.getServerAddress,This deprecated method was introduced in release 0.16 via HADOOP-2404. It should be removed in 0.17
HADOOP-2826,"FileSplit.getFile(), LineRecordReader. readLine() need to be removed","The methods FileSplit.getFile(), LineRecordReader. readLine() need to be removed as they are deprecated."
HADOOP-2825,MapOutputLocation.getFile() needs to be removed,"The method MapOutputLocation.getFile(FileSystem fileSys, 
                                                                         Path localFilename, 
                                                                         int reduce,
                                                                         Progressable pingee,
                                                                         int timeout)
needs to be removed, as it is deprecated."
HADOOP-2824,One of MiniMRCluster constructors needs tobe removed,"The constructor 
MiniMRCluster(int jobTrackerPort,
               int taskTrackerPort,
               int numTaskTrackers,
               String namenode,
               boolean taskTrackerFirst) needs to be removed, as it is deprecated."
HADOOP-2823,"SimpleCharStream.getColumn(),  getLine() methods to be removed.","SimpleCharStream.getColumn(),  getLine() methods need to be removed, as they are deprecated"
HADOOP-2822,Remove deprecated classes in mapred,"The classes InputFormatBase, PhasedFileSystem need to be removed, as they are deprecated."
HADOOP-2821,Remove deprecated classes in util,"The classes ShellUtil and ToolBase in 'util' need to removed, as they are deprecated."
HADOOP-2820,Remove deprecated classes in streaming,"The following classes in streaming need to be removed :
       StreamLineRecordReader
       StreamOutputFormat
       StreamSequenceRecordReader
as they are deprecated and lying around."
HADOOP-2819,Remove deprecated methods in JobConf(),"The following methods in  JobConf
    getInputKeyClass()
    setInputKeyClass
     getInputValueClass()
    setInputValueClass(Class theClass)
    setSpeculativeExecution
     getSpeculativeExecution()
need to be removed as there are deprecated methods lying around."
HADOOP-2818,"Remove deprecated Counters.getDisplayName(),  getCounterNames(),   getCounter(String counterName) ","Counters.getDisplayName(),  getCounterNames(),   getCounter(String counterName)  need to removed as they are deprecated in 0.16."
HADOOP-2817,Remove deprecated mapred.tasktracker.tasks.maximum and clusterStatus.getMaxTasks(),Have to remove deprecated mapred.tasktracker.tasks.maximum and clusterStatus.getMaxTasks(). These are deprecated as we introduce mapred.tasktracker.{map/reduce}.tasks.maximum.
HADOOP-2816,Cluster summary at name node web has confusing report for space utilization,"In one example:
Cluster Summary
Capacity	:	1.15 PB
DFS Remaining	:	192 TB
DFS Used	:	717 TB
DFS Used%	:	62 %

Why is Capacity not equal Used plus Remaining?

(The answer is that there is an estimated reserve for local files.)

The presentation should be easily understood by the user."
HADOOP-2815,Allowing processes to cleanup dfs on shutdown,"Pig creates temp files that it wants to be removed at the end of the processing. The code that removes the temp file is in the shutdown hook so that they get removed both under normal shutdown as well as when process gets killed.

The problem that we are seeing is that by the time the code is called the DFS might already be closed and the delete fails leaving temp files behind. Since we have no control over the shutdown order, we have no way to make sure that the files get removed.

One way to solve this issue is to be able to mark the files as temp files so that hadoop can remove them during its shutdown.

The stack trace I am seeing is

at org.apache.hadoop.dfs.DFSClient.checkOpen(DFSClient.java:158)
        at org.apache.hadoop.dfs.DFSClient.delete(DFSClient.java:417)
        at org.apache.hadoop.dfs.DistributedFileSystem.delete(DistributedFileSystem.java:144)
        at org.apache.pig.backend.hadoop.datastorage.HPath.delete(HPath.java:96)
        at org.apache.pig.impl.io.FileLocalizer$1.run(FileLocalizer.java:275)"
HADOOP-2814,NPE in datanode during TestDataTransferProtocol.,"The test passes. But there is an NPE in datanode (using branch-0.16) :
{noformat}
...
2008-02-13 21:25:37,534 INFO  dfs.TestDataTransferProtocol (TestDataTransferProtocol.java:sendRecvData(71)) - Testing : wrong bytesPerChecksum while writing
2008-02-13 21:25:37,535 INFO  dfs.DataNode (DataNode.java:writeBlock(1048)) - Receiving block blk_7408940144175038455 src: /127.0.0.1:4964
6 dest: /127.0.0.1:49637
2008-02-13 21:25:37,536 ERROR dfs.DataNode (DataNode.java:run(961)) - 127.0.0.1:49637:DataXceiver: java.lang.NullPointerException
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.close(DataNode.java:2001)
        at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:131)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.<init>(DataNode.java:1993)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1074)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:938)
        at java.lang.Thread.run(Thread.java:595)

2008-02-13 21:25:37,537 INFO  dfs.TestDataTransferProtocol (TestDataTransferProtocol.java:sendRecvData(87)) - Got EOF as expected.
...
{noformat}
"
HADOOP-2813,Unit test fails on Linux: org.apache.hadoop.fs.TestDU.testDU,"Unit test fails on Linux: org.apache.hadoop.fs.TestDU.testDU. This is a regression

Changes that went in yesterday were:
HADOOP-2725. Modify distcp to avoid leaving partially copied files at the destination after encountering an error.
HADOOP-2193. 'fs -rm' and 'fs -rmr' show error message when the target file does not exist.

Here is the error from the test run:
org.apache.hadoop.util.Shell$ExitCodeException: /usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_i0ebq9/.nfs00000000004aa47f00001ba0': No such file or directory
/usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_i0ebq9/.nfs00000000004aa48000001b9f': No such file or directory
/usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_r3r1fo/.nfs00000000004aa48200001ba2': No such file or directory
/usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_r3r1fo/.nfs00000000004aa48300001ba1': No such file or directory

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:161)
	at org.apache.hadoop.util.Shell.run(Shell.java:100)
	at org.apache.hadoop.fs.DU.getUsed(DU.java:53)
	at org.apache.hadoop.fs.TestDU.testDU(TestDU.java:62)
	at org.apache.hadoop.fs.TestDU.testDU(TestDU.java:71)"
HADOOP-2811,"method Counters.makeCompactString() does not insert separator char ',' between the counters of different groups.","The corrent code is:
{code}
 public synchronized String makeCompactString() {
    StringBuffer buffer = new StringBuffer();
      for(Group group: this){
      boolean first = true;
      for(Counter counter: group) {
        if (first) {
          first = false;
        } else {
          buffer.append(',');
        }
        buffer.append(group.getDisplayName());
        buffer.append('.');
        buffer.append(counter.getDisplayName());
        buffer.append('=');
        buffer.append(counter.getCounter());
      }
    }
    return buffer.toString();
  }
{code}
The correct code should be like:
{code}
 public synchronized String makeCompactString() {
    StringBuffer buffer = new StringBuffer();
    boolean first = true;
    for(Group group: this){
      
      for(Counter counter: group) {
        if (first) {
          first = false;
        } else {
          buffer.append(',');
        }
        buffer.append(group.getDisplayName());
        buffer.append('.');
        buffer.append(counter.getDisplayName());
        buffer.append('=');
        buffer.append(counter.getCounter());
      }
    }
    return buffer.toString();
  }
{code}

"
HADOOP-2810,Need new Hadoop Core logo,We need a new Hadoop core logo.
HADOOP-2809,"[HOD] Syslog configuration, syslog-address, does not work in HOD 0.4","Specify the parameter syslog-address as host:port in the ringmaster or hodring sections in a hod configuration file. Run a hod allocation. It fails with error code 5/6. The ringmaster fails to start. No logs are created.

The problem seems to be that we are passing in the syslog-address in a format different from what is expected to the hodLogger object."
HADOOP-2808,"FileUtil::copy ignores ""overwrite"" formal","o.a.h.fs.FileUtil::copy takes an ""overwrite"" parameter, but FileUtil::checkDest() throws an IOException if the destination exists. It should be modified to ignore this check if overwrite is true."
HADOOP-2807,distcp creating a file instead of a target directory (with single file source dir),"Source file. 

[knoguchi@~]$ hadoop dfs -ls a/b
Found 1 items
/user/knoguchi/a/b/c    <r 3>   8       2008-02-10 20:58

Ran distcp 

1) run distcp hdfs://nn1:____/user/knoguchi/a/b   hdfs://nn2:____/user/knoguchi/newdir

2) run distcp hdfs://nn1:____/user/knoguchi/a/b/c hdfs://nn2:____/user/knoguchi/newfile

Result
% run dfs -ls new\*
/user/knoguchi/newdir   <r 3>   8       2008-02-11 04:59
/user/knoguchi/newfile  <r 3>   8       2008-02-11 05:06



For (1), I expected /user/knoguchi/newdir/c  (which is the case in version 0.14)
Related? HADOOP-1499 
"
HADOOP-2806,Streaming has no way to force entire record (or null) as key,"I think perhaps streaming needs a ""-allkey"" or ""-nullkey"" option? Otherwise, I'm concerned there is a subtle streaming documentation problem.

These two docs:

http://hadoop.apache.org/core/docs/current/streaming.html
http://wiki.apache.org/hadoop/HadoopStreaming (Should be merged with above?)

... seem to ignore that streaming, by default, splits key/value on TAB. Sure, they mention it, but in all the simple (no separator) examples, they don't seem to take into account that streaming may inconsistently decide whether the whole line is the key, or just up to the first tab, should one occur. This means that some records might be sorted differently as compared to others based on whether or not there's a tab?

Here's a very simple pair of examples, that to the naive, should produce the same output, but do not:

> [hod] (marco) >> run dfs -fs local -cat str-tabs
> a       1
> b       3
> a       4
> 
> [hod] (marco) >> run dfs -put str-tabs str-tabs
> 
> [hod] (marco) >> run jar hadoop-streaming.jar -input str-tabs -output str-tabs.out -mapper /bin/cat -reducer /bin/cat     
> [blah blah blah]
> 
> [hod] (marco) >> run dfs -cat str-tabs.out/part-00000
> a       4
> a       1
> b       3

Compare to this negative-test:
> [hod] (marco) >> run dfs -fs local -cat str-notabs
> a 1
> b 3
> a 4
> 
> [hod] (marco) >> run dfs -put str-notabs str-notabs
> 
> [hod] (marco) >> run jar hadoop-streaming.jar -input str-notabs -output str-notabs.out -mapper /bin/cat -reducer /bin/cat
> [blah blah blah]
> 
> [hod] (marco) >> run dfs -cat str-notabs.out/part-00000
> a 1
> a 4
> b 3
> 
"
HADOOP-2804,Formatable changes log as html,"We could do a better job of generating useful release notes.  As a first step, I suggest we follow Lucene's lead and add html formatting to our CHANGES.txt.  An example output is here:

http://hudson.zones.apache.org/hudson/view/Lucene/job/Lucene-trunk/lastSuccessfulBuild/artifact/trunk/build/docs/changes/Changes.html

This should then be included with our release documentation that is published on the website for each release.

After this is committed, the next step is for committers to agree on a CHANGES.txt convention that will flag major features/improvements (such as a well placed *).  Then the flagged Jira's can be further highlighted when generating the html.

Thoughts?"
HADOOP-2802,"Fix javac warnings shown in ""ant test-core""","When running ""ant test-core"", javac shows the following warnings:
{code}
  [javac] Compiling 170 source files to /trunk/build/test/classes
    [javac] /trunk/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:43: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]       collector.collect(key, value);
    [javac]                        ^
    [javac] /trunk/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:59: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]         collector.collect(key, value);
    [javac]                          ^
    [javac] /trunk/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:43: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]       collector.collect(key, value);
    [javac]                        ^
    [javac] /trunk/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:59: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]         collector.collect(key, value);
    [javac]                          ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 4 warnings
{code}"
HADOOP-2800,SetFile.Writer deprecated by mistake?,"The SetFile.Writer class is deprecated with the description ""pass a Configuration too"". I don't see any reason why that class should be deprecated, I assume the first constructor were supposed to be deprecated and the tag was misplaced."
HADOOP-2799,Replace org.apache.hadoop.io.Closeable with java.io.Closeable,"Since java.io.Closeable is provided in Java 1.5,  we should replace org.apache.hadoop.io.Closeable."
HADOOP-2797,Withdraw CRC upgrade from HDFS,"HDFS will no longer support upgrades from versions without CRCs for block data. Users upgrading from version 0.13 or earlier must first upgrade to an intermediate (0.14, 0.15, 0.16, 0.17) version before upgrade to 0.18 or later."
HADOOP-2796,For script option hod should exit with distinguishable exit codes for script code and hod exit code.,"For hod script option, the exit code should distinguishable between hod exit code and script exit code.
e.g.
If script command contains the streaming command at end and that fails due to input path not found, its value exit cod will 5 which overlaps with hod exit code 5 which means ""job execution failure""
It would hod throws some distinguishable exit codes 
e.g
For above examples 64 +5 =69 and we should this to get exact exit code of hod script command user should subtract 64 from exit code 
"
HADOOP-2794,"hod list command throws python exception, when clusters.state file contains some directory path which actually does not exist ","hod list command throws following python exception when clusters.state file contains some directory path which actually
does not exits -:
CRITICAL/50 hod:340 - op: list failed: <type 'exceptions.OSError'> [Errno 2] No such file or directory: '[Path to non-exstent directory'

Following are the steps to repro -:
1. Create directory tree cdir/clusterDir say at $HOME dir.
2. Run hod allocate as -: hod -c <confPath> -b 4 -o ""allocate ~/cdir/clusterDir 5"" 
3. rename cdir to cdir1.
4. Run hod list as -: hod -o list
    hod will list  the allocated directories up to the the point it encounters non-existant path entry in ~/.hod/clusters.state file afer that hod will throw python exception and will stop
Note -:
There is more issue. if cdir exists but not clusrterDir. Then hod will create clusterDir (a new empty directory)

		
"
HADOOP-2793,Links for worst performing shuffle tasks are broken in Analyze Job.,The links for the worst performing shuffle tasks have a space causing then to break.
HADOOP-2792,Wrong times are shown in Analyse Job's map analysis,"While trying to analyze the job, the map related analysis w.r.t average times/worst case timings are incorrect. It looks something like this
{code}
Time taken by best performing Map task tip_200802070501_0004_m_000014 : 333990hrs, 4mins, 13sec
{code}"
HADOOP-2790,TaskInProgress.hasSpeculativeTask is very inefficient,"Each call to JobInProgress.findNewTask can call TaskInProgress.hasSpeculativeTask once per a task. Each call to hasSpeculativeTask calls System.getCurrentTimeMillis, which can result in hundreds of thousands of calls to getCurrentTimeMillis. Additionally, it calls TaskInProgress.isOnlyCommitPending, which calls .values() on the map from task id to host name and iterates through them to see if any of the tasks are in commit pending. It would be better to have a commit pending boolean flag in the TaskInProgress. It also looks like there are other opportunities here, but those jumped out at me. We should also look at this method in the profiler."
HADOOP-2789,Race condition in ipc.Server prevents responce being written back to client.,"I encountered a race condition in ipc.Server when writing the response
back to the socket. Sometimes the write SelectKey is being canceled
when it should not be, and thus the full response never gets
written. This results in clients timing out on the socket while waiting for the response.

I am attaching a unit test that demonstrates the problem. It follows
closely the TestIPC test, however the socket output buffer is set
smaller than the result being sent back, so that partial writes
occur. I also put random sleep in the client to help provoke the race
condition.

On my machine this fails over half of the time.

Looking at the code in ipc.Server.java. The problem is manifested in
Responder.doAsyncWrite(). If I comment out the key.cancel() line, then
everything works fine. 

So we need to identify when to safely cancel the key.

I tried the following:

{noformat}
    private void doAsyncWrite(SelectionKey key) throws IOException {
      Call call = (Call)key.attachment();
      if (call == null) {
        return;
      }
      if (key.channel() != call.connection.channel) {
        throw new IOException(""doAsyncWrite: bad channel"");
      }
      if (processResponse(call.connection.responseQueue)) {
          synchronized(call.connection.responseQueue) {
              if (call.connection.responseQueue.size() == 0) {
                  LOG.info(""Cancelling key for call ""+call.toString()+ "" key: ""+ key.toString());
                  key.cancel();          // remove item from selector.
              } else {
                  LOG.warn(""NOT REALLY DONE: ""+call.toString()+ "" key: ""+ key.toString());
              }
          }
      }
    }
{noformat}

And this does catch some of the cases (EG, the LOG.warn message gets hit), but i still hit the race condition.
"
HADOOP-2788,chgrp missing from hadoop dfs options,"hadoop dfs command shows the various hadoop dfs commands available.

chgrp is missing from the list and there are two chown's. Looks like the second one should be chgrp instead of chown:

           [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
           [-chown [-R] [OWNER][:[GROUP]] PATH...]
           [-chown [-R] GROUP PATH...]"
HADOOP-2787,"The constant org.apache.hadoop.fs.permission.FsPermission.UMASK_LABEL should be ""dfs.umask"", instead of ""hadoop.dfs.umask""","For all other DFS configuration parameter names, we use the form dfs.xxx(.yyy).  So, for consistency, the constant org.apache.hadoop.fs.permission.FsPermission.UMASK_LABEL should be ""dfs.umask"", instead of ""hadoop.dfs.umask"""
HADOOP-2785,Typo in peridioc block verification patch,"The patch for HADOOP-2012 uses 60 seconds instead of 600 seconds in one place.

When there are very few blocks, Datanode spreads the verification 10 minutes apart (instead of days apart). But the patch scheduled them 1 min apart.

"
HADOOP-2783,hod/hodlib/Common/xmlrpc.py uses HodInterruptException without importing it,"And because of this no-import of HodInterruptException in hod/hodlib/Common/xmlrpc.py, HOD fails with an exception, when it gets interrupted by user(e.g ^C) while it is performing any xmlrpc request. One line fix."
HADOOP-2780,Socket receive buffer size on datanode too large,HADOOP-1707 introduced code that created the socket receive buffer size to be 1MB by default. This should be changed back to 128K.
HADOOP-2779,build scripts broken by moving hbase to subproject,The build scripts fail with hbase moving out.
HADOOP-2775,[HOD] Put in place unit test framework for HOD,"HOD does not have any unit tests in place currently. This issue is to decide on a framework that would be effective for our python code base. Something on the lines of pyUnit would be good. The fix should put in place any dependencies needed for running the tests, and should also define some tests that demonstrate how to write further tests. We would not be defining a complete unit test suite for the entire code base right now, but would incrementatlly add tests as changes are made."
HADOOP-2774,Add counters to show number of key/values that have been sorted and merged in the maps and reduces,"For each *pass* of the sort and merge, I would like a count of the number of records. So for example, if the map output 100 records and they were sorted once, the counter would be 100. If it spilled twice and was merged together, it would be 200. Clearly in a multi-level merge, it may not be a multiple of the number of map output records. This would let the users easily see if they have values like io.sort.mb or io.sort.factor set too low."
HADOOP-2771,changing the number of reduces dramatically changes the time of the map time,"By changing the number of reduces, the time for an individual map changes radically. By running the same program and data with different numbers of reduces (2500, 7500, 25000) the times for each map changed radically (0:50, 1:20, 5h)."
HADOOP-2769,TestNNThroughputBenchmark should not used a fixed http port,"HADOOP-2601 only fixed the main namenode port. It also needed to fix the namenode http port. I get failures like:

{quote}
Testcase: testNNThroughput took 1.048 sec
        Caused an ERROR
Address already in use java.net.BindException: Address already in use
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:359)
        at java.net.ServerSocket.bind(ServerSocket.java:319)
        at java.net.ServerSocket.<init>(ServerSocket.java:185)
        at org.mortbay.util.ThreadedServer.newServerSocket(ThreadedServer.java:391)
        at org.mortbay.util.ThreadedServer.open(ThreadedServer.java:477)
        at org.mortbay.util.ThreadedServer.start(ThreadedServer.java:503)
        at org.mortbay.http.SocketListener.start(SocketListener.java:203)
        at org.mortbay.http.HttpServer.doStart(HttpServer.java:761)
        at org.mortbay.util.Container.start(Container.java:72)
        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:182)
        at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:294)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:235)
        at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:175)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:161)
        at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:843)
        at org.apache.hadoop.dfs.NNThroughputBenchmark.<init>(NNThroughputBenchmark.java:74)
        at org.apache.hadoop.dfs.NNThroughputBenchmark.runBenchmark(NNThroughputBenchmark.java:769)
        at org.apache.hadoop.dfs.TestNNThroughputBenchmark.testNNThroughput(TestNNThroughputBenchmark.java:32)
{quote}"
HADOOP-2768,DFSIO write performance benchmark shows a regression,"We are seeing a performance regression in DFSIO write benchmark, possibly due to Hadoop-1707. 

Here is the data:
- Aggregate cluster throughput went from 0.30G to 0.14G with 39 concurrent clients, on 20 nodes
- Aggregate cluster throughput went from 1.15G to 0.85G with 39 concurrent clients, on 100 nodes
"
HADOOP-2767,org.apache.hadoop.net.NetworkTopology.InnerNode#getLeaf does not return the last node on a rack when used with an excluded node,"I have written some test code that shows NetworkTopology.InnerNode#getLeaf will never return the last node on the rack if it is called with an excludedNode (for example the first node on the rack). 

Consequently I suspect that NetworkTopology.chooseRandom() will never returns the last node on the remote rack for the second replica in DFS. 

I have some test code that demonstrates this problem at the getLeaf level, although it is necessary to change the visibility of the NetworkTopology.InnerNode, NetworkTopology.InnerNode#getLeaf and NetworkTopology.getNode from private to package default to run the test. 

TODO: Demonstrate problem at NetworkTopology.chooseRandom level, then submit candidate fix. "
HADOOP-2766,[HOD] No way to set HADOOP_OPTS environment variable to the Hadoop daemons through HOD,"For purposes of performance tuning it should be possible to set some environment variables that Hadoop honors before launching the Hadoop daemons. For e.g. to set the heap size of the JVM, we should set the HADOOP_HEAPSIZE variable. These can be configured through hodrc using the gridservice-mapred.envs environment variable. This works for everything *except* HADOOP_OPTS, which is also useful, for e.g. to pass in Garbage collection parameters to the JVM, like -XX:+UseParallelGC. This is because HOD tries to set HADOOP_OPTS from the gridservice-mapred.java-opts variable, but it does not read the java-opts variable anywhere properly.

"
HADOOP-2765,setting memory limits for tasks,"here's the motivation:

we want to put a memory limit on user scripts to prevent runaway scripts from bringing down nodes. this setting is much lower than the max. memory that can be used (since most likely these tend to be scripting bugs). At the same time - for careful users, we want to be able to let them use more memory by overriding this limit.

there's no good way to do this. we can set ulimit in hadoop shell scripts - but they are very restrictive. there doesn't seem to be a way to do a setrlimit from Java - and setting a ulimit means that supplying a higher Xmx limit from the jobconf is useless (the java process will be limited by the ulimit setting when the tasktracker was launched).

what we have ended up doing (and i think this might help others as well) is to have a stream.wrapper option. the value of this option is a program through which streaming mapper and reducer scripts are execed. in our case, this wrapper is small C program to do a setrlimit and then exec of the streaming job. the default wrapper puts a reasonable limit on the memory usage - but users can easily override this wrapper (eg by invoking it with different memory limit argument). we can use the wrapper for other system wide resource limits (or any environment settings) as well in future.

This way - JVMs can stick to mapred.child.opts as the way to control memory usage. This setup has saved our ass on many occasions while allowing sophisticated users to use high memory limits.

Can submit patch if this sounds interesting."
HADOOP-2764,specify different heap size for namenode/jobtracker vs. tasktracker/datanodes,"tasktrackers/datanodes should be run with low memory settings. theres a lot of competition for memory on slave nodes and these tasks don't need much memory anyway and best to keep heap setting low.

namenode needs higher memory and there's usually lots to spare on separate box.

hadoop-env.sh can provide different heap settings for central vs. slave daemons."
HADOOP-2763,Replication Monitor timing out repeatedly,"I upgraded a Hadoop installation to the Jan 28 nightly build.
DFS contains 5+ M files.

Fsck reported 1 hour after leaving safemode, 5274 under-replicated blocks with 25 single replications, 3 hours later 433 under-replicated with still 20 single replications.

The namenode log shows repeated timeouts of the replication monitor for the same blocks:

2008-02-01 03:41:24,184 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask datanode to replicate blk_2984271423661664080 to datanode(s) datanode1 datanode2
2008-02-01 03:51:14,104 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_2984271423661664080
2008-02-01 03:51:22,303 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask datanode to replicate blk_2984271423661664080 to datanode(s) datanode3 datanode4
2008-02-01 04:01:14,150 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_2984271423661664080
2008-02-01 04:01:19,344 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask datanode to replicate blk_2984271423661664080 to datanode(s) datanode5 datanode6
...

The datanode seems to be successfully transmitting the blocks:

2008-02-01 03:42:06,284 INFO org.apache.hadoop.dfs.DataNode: datanode Starting thread to transfer block blk_2984271423661664080 to datanode1, datannode2
2008-02-01 03:42:09,535 INFO org.apache.hadoop.dfs.DataNode: datanode:Transmitted block blk_2984271423661664080 to /datanode1

2008-02-01 03:52:06,238 INFO org.apache.hadoop.dfs.DataNode: datanode Starting thread to transfer block blk_2984271423661664080 to datanode3,datanode4
2008-02-01 03:52:09,470 INFO org.apache.hadoop.dfs.DataNode: datanode:Transmitted block blk_2984271423661664080 to /datanode3


The destination datanodes seem to have problems receiving these blocks (some time later for a different attempt):

2008-02-01 06:43:06,541 INFO org.apache.hadoop.dfs.DataNode: Receiving block blk_2984271423661664080 from /datanode
2008-02-01 06:43:09,647 INFO org.apache.hadoop.dfs.DataNode: Exception in receiveBlock for block blk_2984271423661664080 java.net.SocketException: Connection reset
2008-02-01 06:43:09,647 INFO org.apache.hadoop.dfs.DataNode: writeBlock blk_2984271423661664080 received exception java.net.SocketException: Connection reset

But I was successfully transferring the block between the two datanodes using scp."
HADOOP-2762,Better documentation of controls for memory limits on hadoop daemons and Map-Reduce tasks,"We have had a spate of questions about memory usage of hadoop daemons and Map-Reduce jobs and how to configure them. 

We should better document *mapred.child.java.opts* in the Map-Reduce tutorial and cluser_setup.html and link to http://hadoop.apache.org/core/docs/r0.15.3/cluster_setup.html#Configuring+the+Environment+of+the+Hadoop+Daemons. "
HADOOP-2760,Distcp must be able to copy from .16 to .15,"Typically the ""from"" source files are referenced via HTTP, a strategy to evade version incompatibilities. In .16, with permission checking, there is no presumption that any particular file can be read via the web server.

There must be some alternative for supplying user credentials so as to enable down-version copies."
HADOOP-2758,Reduce memory copies when data is read from DFS,"Currently datanode and client part of DFS perform multiple copies of data on the 'read path' (i.e. path from storage on datanode to user buffer on the client). This jira reduces these copies by enhancing data read protocol and implementation of read on both datanode and the client. I will describe the changes in next comment.

Requirement is that this fix should reduce CPU used and should not cause regression in any benchmarks. It might not improve the benchmarks since most benchmarks are not cpu bound."
HADOOP-2756,NPE in DFSClient in hbase under load,"Saw this in logs:

{code}
2008-01-31 18:55:02,128 ERROR org.apache.hadoop.hbase.HRegionServer: Compaction failed for region TestTable,0009438931,1201805282651
java.lang.NullPointerException
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:2262)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:51)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:67)
        at org.apache.hadoop.hbase.HStoreFile.writeInfo(HStoreFile.java:365)
        at org.apache.hadoop.hbase.HStore.compact(HStore.java:1236) 
        at org.apache.hadoop.hbase.HRegion.compactStores(HRegion.java:775)
        at org.apache.hadoop.hbase.HRegion.compactIfNeeded(HRegion.java:707)
        at org.apache.hadoop.hbase.HRegionServer$CompactSplitThread.run(HRegionServer.java:253)
{code}

Look to see if the response data method needs to be made volatile (There's a test for null just before we use it on line #2262)."
HADOOP-2755,"dfs fsck extremely slow, dfs ls times out","I upgraded a Hadoop installation to the Jan 28 nightly build.
DFS contains 2.4+ M files.
Upgrade finished but not finalized.

Before finalizing I wanted to run fsck on the DFS. It hardly progressed after 6 hours (not finished yet). With the '-files' option turned on, it lists about 300 entries in 10 minutes.

And when I tried to list a subdirectory with 100,000 files, it repeatedly (about 20 attempts) timed out.
Changing timeout from 1 to 10 minutes did not help."
HADOOP-2754,Path filter for Local file system list .crc files,"If we write a path filter for local file system, it  lists .crc files also.
If dont pass any filter, it lists paths properly without any .crc files. But we write a filter, it does list .crc files also."
HADOOP-2742,'Analyse this job' link in job history shows -1 on the UI some times,"'Analyse this job' link in job history shows -1 on the UI some times
The job tracker logs show
java.lang.ArrayIndexOutOfBoundsException: -1
               at org.apache.hadoop.mapred.analysejobhistory_jsp._jspService(analysejobhistory_jsp.java:151)
               at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
               at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
               at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
               at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
               at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
               at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
               at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)     10         at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
              at org.mortbay.http.HttpServer.service(HttpServer.java:954)
              at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
              at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
              at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
              at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
              at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
              at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-2740,Modify HOD to work with changes mentioned in HADOOP-2404,HADOOP-2404 proposes a change to configuration variable names introduced in HADOOP 0.16. These are used by HOD. This bug is to modify HOD to use the new names. Both these bugs must be committed together for things to work.
HADOOP-2738,Text is not subclassable because set(Text) and compareTo(Object) access the other instance's private members directly,"Text objects should not access other Text objects private members directly. Both set(Text) and compareTo(Object) do.
Because these two methods access private members of the other object, Text is not subclassable. Either these two methods should be modified to use the accessors that are already available, or Text should be declared as a final class, because as it exists today it is not subclassable."
HADOOP-2737,HOD fails to allocate nodes if the hadoop version is a string ( e.g Hadoop full ),"HOD is not able to allocate nodes if the hadoop version 
is a string ( like full ) . The ring master logs are 
given below. Probably we can improve the error logging for this 
error too.

RING MASTER LOG:-
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[2008-01-29 05:44:55,420] CRITICAL/50 ringMaster:571 - Exception in 
creating Hdfs and Map/Reduce descriptor objects: 
      <type 'exceptions.TypeError'> int() argument must be a string or a number, not 'NoneType'.
[2008-01-29 05:44:55,431] CRITICAL/50 ringMaster:956 - Traceback (most 
recent call last):
   File 
""###Path to Ringmaster directory###/ringMaster.py"", 
line 947, in main
     rm = RingMaster(cfg, log)
   File 
""###Path to Ringmaster directory###/ringMaster.py"", 
line 556, in __init__
     hdfs = Hdfs(hdfsDesc, workDirs, 0, version=int(hadoopVers['minor']))
TypeError: int() argument must be a string or a number, not 'NoneType'

[2008-01-29 05:44:55,437] ERROR/40 ringmaster:332 - bin/ringmaster 
failed to start.<type 'exceptions.Exception'> int() argument must be a 
string or a number, not 'NoneType'. Stack trace follows:
Traceback (most recent call last):
   File ""ringmaster"", line 328, in <module>
     ret = main(ringMasterOptions,log)
   File 
""###Path to Ringmaster directory###/ringMaster.py"", 
line 957, in main
     raise Exception(e)
Exception: int() argument must be a string or a number, not 'NoneType'
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
HADOOP-2735,Setting default tmp directory for java createTempFile (java.io.tmpdir),"On our cluster, we've seen Pig(http://incubator.apache.org/pig/) filling up the /tmp and failing. 
(also inefficient since all the local tasks were spilling to the  same disk)

Pig is simply using java api createTempFile, 

http://java.sun.com/j2se/1.5.0/docs/api/java/io/File.html#createTempFile(java.lang.String,%20java.lang.String,%20java.io.File

Can we add -Djava.io.tmpdir=""./tmp"" somewhere ?

so that, 

1) Tasks can utilize all disks when using tmp
2) Any undeleted tmp files will be deleted by the tasktracker when task(job?) is done.


The easiest way is to set it inside mapred.child.java.opts in the config, but this can be overwritten if the users set their own task heapsize.
"
HADOOP-2734,docs link to lucene.apache.org,The forrest documentation still links to lucene.apache.org.  This should be updated to hadoop.apache.org.
HADOOP-2733,Compiler warnings in TestClusterMapReduceTestCase and TestJobStatusPersistency,"The following four warnings were introduced by HADOOP-1876.
{code}
    [javac] hadoop/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:26: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]       collector.collect(key, value);
    [javac]                        ^
    [javac] hadoop/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:42: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]         collector.collect(key, value);
    [javac]                          ^
    [javac] hadoop/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:26: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]       collector.collect(key, value);
    [javac]                        ^
    [javac] hadoop/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:42: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]         collector.collect(key, value);
    [javac]                          ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 4 warnings
{code}
"
HADOOP-2732,"ab{5[6-9],[6-9][6-9]}.gz should not be treated as an illegal glob","Running  hadoop dfs -ls ""/xx/ab{5[6-9],[6-9][6-9]}.gz""  returns the following error message:
ls: Illegal file pattern: Unexpected end of a group for glob ab{5[6-9],[6-9][6-9]}.gz at 20"
HADOOP-2730,Update HOD documentation,"Update the HOD documentation to provide some more details on usage, setup and configuration."
HADOOP-2727,Web UI links to Hadoop homepage has to change to new hadoop homepage,Currently all the web UI has links to hadoop home page as lucene.apache.org/hadoop. These links have to be changed to hadoop.apache.org/core
HADOOP-2725,Distcp truncates some files when copying,"We used distcp to copy ~100 TB of data across two clusters ~1400 nodes each.

Command used (it was run on the src cluster):
hadoop distcp -log /logdir/logfile hdfs://src-namenode:8600//src-dir-1 hdfs://src-namenode:8600//src-dir-2 ... hdfs://src-namenode:8600//src-dir-n hdfs://tgt-namenode:8600//dst-dir

Distcp completed without errors, but when we checked the file sizes on the src and tgt clusters, we noticed differences in file sizes for 9 files (~6 GB).

src-file-1 666762714 bytes -> tgt-file-1 134217728 bytes
src-file-2 673791814 bytes -> tgt-file-2 536870912 bytes
src-file-3 692172075 bytes -> tgt-file-3 0 bytes

All target files are truncated at block boundaries (some have 0 size).


I looked at the log files, and noticed a few things:

1. There are 31059 log files (same as the number of Maps the job had).

2. 246 of the log files are non-empty.

3. All non-empty log files are of the form:

SKIP: hdfs://src-namenode/src-dir-a/src-file-x
SKIP: hdfs://src-namenode/src-dir-b/src-file-y
SKIP: hdfs://src-namenode/src-dir-c/src-file-z

4. All 9 files which were truncated were included in the log files as skipped files.

5. All 9 files were the last entry in their respective log files.

e.g.
Non-empty logfile 1:

SKIP: hdfs://src-namenode/src-dir-a/src-file-x
SKIP: hdfs://src-namenode/src-dir-b/src-file-y
SKIP: hdfs://src-namenode/src-dir-c/src-file-z  <-- Truncated file

Non_empty logfile 2:
SKIP: hdfs://src-namenode/src-dir-p/src-file-m
SKIP: hdfs://src-namenode/src-dir-q/src-file-n  <-- Truncated file"
HADOOP-2723,Hadoop 2367- Does not respect JobConf.getProfileEnabled(),"Looks like it should add a guard in TaskRunner.run()

   if (conf.getProfileEnabled()) {
        if (conf.getProfileTaskRange(t.isMapTask()
                                     ).isIncluded(t.getPartition())) {
          File prof = TaskLog.getTaskLogFile(taskid, TaskLog.LogName.PROFILE);
          vargs.add(""-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,""+
                    ""verbose=n,file=""+prof.toString());
        }
      }


Also, how come these conf values were not added to hadoop-default.xml?"
HADOOP-2721,Use job control for tasks (and therefore for pipes and streaming),"We should use the setsid command when the task is launched to create a new session. We should be able to use the setsid program when we launch the bash process to create a new session. That will allow us to kill the entire session with a single signal and remove the need for the ping methods in both TaskTracker.Child and pipes.

The patch uses setsid when creating new tasks sothat subprocesses of this process will be with in this new session(and this process will be the process leader for all the subprocesses). Thus killing the subprocesses becomes easy(just by killing all the processes in this process group) when killing the task."
HADOOP-2720,Update HOD in Hadoop 0.16,"Between the time of submission of the HOD patch to Hadoop SVN and now, there have been a bunch of changes made to local repositories of HOD. This patch is to synchronize the source changes. Post this patch, HOD will only be developed out of Hadoop SVN."
HADOOP-2717,NPE while closing file.,"Saw this NPE in one of the hudson builds : 
{code}
java.lang.NullPointerException
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:2277)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:51)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:67)
	at org.apache.hadoop.dfs.TestCheckpoint.writeFile(TestCheckpoint.java:49)
	at org.apache.hadoop.dfs.TestCheckpoint.testSecondaryNamenodeError2(TestCheckpoint.java:231)
	at org.apache.hadoop.dfs.TestCheckpoint.testCheckpoint(TestCheckpoint.java:412)
{code}
{code}
        if (response != null) {
          response.join(); // line 2277
        }
{code}


"
HADOOP-2716,Balancer should require superuser privilege,"Balancer uses NamenodeProtocol.  Currently, there is no permission checking for NamenodeProtocol.  We should require superuser privilege for running Balancer."
HADOOP-2714,Unit test fails on Windows: rg.apache.hadoop.dfs.TestDecommission,"Unit test fails consistently on Windows with a timeout:

Test: org.apache.hadoop.dfs.TestDecommission

Here is a snippet of the console:
[junit] Waiting for node 127.0.0.1:4042 to change state to DECOMMISSIONED
    [junit] Name: 127.0.0.1:4042
    [junit] State          : Decommission in progress
    [junit] Total raw bytes: 80030941184 (74.53 GB)
    [junit] Remaining raw bytes: 45235447130(42.13 GB)
    [junit] Used raw bytes: 20622 (20.14 KB)
    [junit] % used: 0%
    [junit] Last contact: Fri Jan 25 09:26:36 PST 2008

    [junit] Waiting for node 127.0.0.1:4042 to change state to DECOMMISSIONED
    [junit] Name: 127.0.0.1:4042
    [junit] State          : Decommission in progress
    [junit] Total raw bytes: 80030941184 (74.53 GB)
    [junit] Remaining raw bytes: 45235447130(42.13 GB)
    [junit] Used raw bytes: 20622 (20.14 KB)
    [junit] % used: 0%
    [junit] Last contact: Fri Jan 25 09:26:36 PST 2008

    [junit] Waiting for node 127.0.0.1:4042 to change state to DECOMMISSIONED
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestDecommission FAILED (timeout)"
HADOOP-2713,Unit test fails on Windows: org.apache.hadoop.dfs.TestDatanodeDeath,"Unit test fails consistently on Windows with a timeout:

Test: org.apache.hadoop.dfs.TestDatanodeDeath

Here is a snippet of the console log:
[junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] 2008-01-25 09:10:47,841 WARN  fs.FSNamesystem (PendingReplicationBlocks.java:pendingReplicationCheck(209)) - PendingReplicationMonitor timed out block blk_2509851293741663991
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] 2008-01-25 09:10:52,839 INFO  dfs.StateChange (FSNamesystem.java:pendingTransfers(3249)) - BLOCK* NameSystem.pendingTransfer: ask 127.0.0.1:3773 to replicate blk_2509851293741663991 to datanode(s) 127.0.0.1:3767
    [junit] 2008-01-25 09:10:53,526 INFO  dfs.DataNode (DataNode.java:transferBlocks(786)) - 127.0.0.1:3773 Starting thread to transfer block blk_2509851293741663991 to 127.0.0.1:3767
    [junit] 2008-01-25 09:10:53,526 INFO  dfs.DataNode (DataNode.java:writeBlock(1035)) - Receiving block blk_2509851293741663991 from /127.0.0.1
    [junit] 2008-01-25 09:10:53,526 INFO  dfs.DataNode (DataNode.java:writeBlock(1147)) - writeBlock blk_2509851293741663991 received exception java.io.IOException: Block blk_2509851293741663991 has already been started (though not completed), and thus cannot be created.
    [junit] 2008-01-25 09:10:53,526 ERROR dfs.DataNode (DataNode.java:run(948)) - 127.0.0.1:3767:DataXceiver: java.io.IOException: Block blk_2509851293741663991 has already been started (though not completed), and thus cannot be created.
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:638)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.<init>(DataNode.java:1949)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1060)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:925)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2008-01-25 09:10:53,526 WARN  dfs.DataNode (DataNode.java:run(2366)) - 127.0.0.1:3773:Failed to transfer blk_2509851293741663991 to 127.0.0.1:3767 got java.net.SocketException: Software caused connection abort: socket write error
    [junit] 	at java.net.SocketOutputStream.socketWrite0(Native Method)
    [junit] 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    [junit] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    [junit] 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
    [junit] 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
    [junit] 	at java.io.DataOutputStream.flush(DataOutputStream.java:106)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1621)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:2360)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] File simpletest.dat has 3 blocks:  The 0 block has only 2 replicas  but is expected to have 3 replicas.
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestDatanodeDeath FAILED (timeout)"
HADOOP-2709,Fix trivial typeos in EC2 scripts,"The ec2-run-instances needs a capital K not a lower case k flag.

Index: src/contrib/ec2/bin/create-hadoop-image
===================================================================
--- src/contrib/ec2/bin/create-hadoop-image     (revision 608611)
+++ src/contrib/ec2/bin/create-hadoop-image     (working copy)
@@ -11,7 +11,7 @@
 AMI_IMAGE=`ec2-describe-images -a | grep fedora-core4-base | awk '{print $2}'`

 echo ""Starting a fedora core base AMI with ID $AMI_IMAGE.""
-OUTPUT=`ec2-run-instances $AMI_IMAGE -k $KEY_NAME`
+OUTPUT=`ec2-run-instances $AMI_IMAGE -K $KEY_NAME`
 BOOTING_INSTANCE=`echo $OUTPUT | awk '{print $6}'` "
HADOOP-2705,io.file.buffer.size should default to a value larger than 4k,"Tests using HADOOP-2406 suggest that increasing this to 32k from 4k improves read times for block, lzo compressed SequenceFiles by over 40%; 32k is a relatively conservative bump."
HADOOP-2703,New files under lease (before close) still shows up as MISSING files/blocks in fsck,"On 0.15.3 which includes HADOOP-2540 (""Empty blocks make fsck report corrupt, even when it isn't"") fix, we are still seeing fsck reporting missing blocks when they aren't.   Basically,  any unclosed small files under lease shows up as MISSING.

Can we have an option in fsck to skip those files under lease?
Or  count those files/blocks separately?"
HADOOP-2700,mapred.Task class is not public,"org.apache.hadoop.mapred.Task is not public. So, access to the Task.Counter class to retrieve the counters from outside of the mapred package is not possible. 

The solution is change make the Task class public, or move th Counter class outside of the Task class."
HADOOP-2699,Unit test intermittently fails on Mac OS: ,"The unit test TestBalancer intermittently fails on the Mac OS. I see this in the console:

[junit] 2008-01-23 18:05:47,577 INFO  jvm.JvmMetrics (RpcMetrics.java:setTags(49)) - Initializing RPC Metrics with serverName=NameNode, port=0
    [junit] javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=RpcStatistics
    [junit] 	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
    [junit] 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
    [junit] 	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:50)
    [junit] 	at org.apache.hadoop.ipc.metrics.RpcMgt.<init>(RpcMgt.java:37)
    [junit] 	at org.apache.hadoop.ipc.metrics.RpcMetrics.<init>(RpcMetrics.java:60)
    [junit] 	at org.apache.hadoop.ipc.Server.<init>(Server.java:969)
    [junit] 	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:393)
    [junit] 	at org.apache.hadoop.ipc.RPC.getServer(RPC.java:355)
    [junit] 	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:119)
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:174)
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:160)
    [junit] 	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:836)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:195)
    [junit] 	at org.apache.hadoop.dfs.TestBalancer.test(TestBalancer.java:194)
    [junit] 	at org.apache.hadoop.dfs.TestBalancer.testBalancer0(TestBalancer.java:260)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:585)
    [junit] 	at junit.framework.TestCase.runTest(TestCase.java:154)
    [junit] 	at junit.framework.TestCase.runBare(TestCase.java:127)
    [junit] 	at junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit] 	at junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit] 	at junit.framework.TestResult.run(TestResult.java:109)
    [junit] 	at junit.framework.TestCase.run(TestCase.java:118)
    [junit] 	at junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit] 	at junit.framework.TestSuite.run(TestSuite.java:203)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
    [junit] 2008-01-23 18:05:47,585 INFO  dfs.NameNode (NameNode.java:initialize(125)) - Namenode up at: localhost/127.0.0.1:65488
    [junit] 2008-01-23 18:05:47,590 INFO  jvm.JvmMetrics (JvmMetrics.java:init(51)) - Cannot initialize JVM Metrics with processName=NameNode, sessionId=null - already initialized
    [junit] javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
    [junit] 	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
    [junit] 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
    [junit] 	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:50)
    [junit] 	at org.apache.hadoop.dfs.namenode.metrics.NameNodeMgt.<init>(NameNodeMgt.java:36)
    [junit] 	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:67)
    [junit] 	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:127)
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:174)
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:160)
    [junit] 	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:836)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:195)
    [junit] 	at org.apache.hadoop.dfs.TestBalancer.test(TestBalancer.java:194)
    [junit] 	at org.apache.hadoop.dfs.TestBalancer.testBalancer0(TestBalancer.java:260)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:585)
    [junit] 	at junit.framework.TestCase.runTest(TestCase.java:154)
    [junit] 	at junit.framework.TestCase.runBare(TestCase.java:127)
    [junit] 	at junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit] 	at junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit] 	at junit.framework.TestResult.run(TestResult.java:109)
    [junit] 	at junit.framework.TestCase.run(TestCase.java:118)
    [junit] 	at junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit] 	at junit.framework.TestSuite.run(TestSuite.java:203)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)"
HADOOP-2691,Some junit tests fail with the exception: All datanodes are bad. Aborting...,"Some junit tests fail with the following exception:
java.io.IOException: All datanodes are bad. Aborting...
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:1831)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1479)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1571)
The log contains the following message:
2008-01-19 23:00:25,557 INFO  dfs.StateChange (FSNamesystem.java:allocateBlock(1274)) - BLOCK* NameSystem.allocateBlock: /srcdat/three/3189919341591612220. blk_6989304691537873255
2008-01-19 23:00:25,559 INFO  fs.DFSClient (DFSClient.java:createBlockOutputStream(1982)) - pipeline = 127.0.0.1:40678
2008-01-19 23:00:25,559 INFO  fs.DFSClient (DFSClient.java:createBlockOutputStream(1982)) - pipeline = 127.0.0.1:40680
2008-01-19 23:00:25,559 INFO  fs.DFSClient (DFSClient.java:createBlockOutputStream(1985)) - Connecting to 127.0.0.1:40678
2008-01-19 23:00:25,570 INFO  dfs.DataNode (DataNode.java:writeBlock(1084)) - Receiving block blk_6989304691537873255 from /127.0.0.1
2008-01-19 23:00:25,572 INFO  dfs.DataNode (DataNode.java:writeBlock(1084)) - Receiving block blk_6989304691537873255 from /127.0.0.1
2008-01-19 23:00:25,573 INFO  dfs.DataNode (DataNode.java:writeBlock(1169)) - Datanode 0 forwarding connect ack to upstream firstbadlink is 
2008-01-19 23:00:25,573 INFO  dfs.DataNode (DataNode.java:writeBlock(1150)) - Datanode 1 got response for connect ack  from downstream datanode with firstbadlink as 
2008-01-19 23:00:25,573 INFO  dfs.DataNode (DataNode.java:writeBlock(1169)) - Datanode 1 forwarding connect ack to upstream firstbadlink is 
2008-01-19 23:00:25,574 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1802)) - Received block blk_6989304691537873255 of size 34 from /127.0.0.1
2008-01-19 23:00:25,575 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1819)) - PacketResponder 0 for block blk_6989304691537873255 terminating
2008-01-19 23:00:25,575 INFO  dfs.StateChange (FSNamesystem.java:addStoredBlock(2467)) - BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:40680 is added to blk_6989304691537873255 size 34
2008-01-19 23:00:25,575 INFO  dfs.DataNode (DataNode.java:close(2013)) - BlockReceiver for block blk_6989304691537873255 waiting for last write to drain.
2008-01-19 23:01:31,577 WARN  fs.DFSClient (DFSClient.java:run(1764)) - DFSOutputStream ResponseProcessor exception  for block blk_6989304691537873255java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.io.DataInputStream.readFully(DataInputStream.java:176)
	at java.io.DataInputStream.readLong(DataInputStream.java:380)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:1726)

2008-01-19 23:01:31,578 INFO  fs.DFSClient (DFSClient.java:run(1653)) - Closing old block blk_6989304691537873255
2008-01-19 23:01:31,579 WARN  fs.DFSClient (DFSClient.java:processDatanodeError(1803)) - Error Recovery for block blk_6989304691537873255 bad datanode[0] 127.0.0.1:40678
2008-01-19 23:01:31,580 WARN  fs.DFSClient (DFSClient.java:processDatanodeError(1836)) - Error Recovery for block blk_6989304691537873255 bad datanode 127.0.0.1:40678
2008-01-19 23:01:31,580 INFO  fs.DFSClient (DFSClient.java:createBlockOutputStream(1982)) - pipeline = 127.0.0.1:40680
2008-01-19 23:01:31,580 INFO  fs.DFSClient (DFSClient.java:createBlockOutputStream(1985)) - Connecting to 127.0.0.1:40680
2008-01-19 23:01:31,582 INFO  dfs.DataNode (DataNode.java:writeBlock(1084)) - Receiving block blk_6989304691537873255 from /127.0.0.1
2008-01-19 23:01:31,584 INFO  dfs.DataNode (DataNode.java:writeBlock(1196)) - writeBlock blk_6989304691537873255 received exception java.io.IOException: Reopen Block blk_6989304691537873255 is valid, and cannot be written to.
2008-01-19 23:01:31,584 ERROR dfs.DataNode (DataNode.java:run(997)) - 127.0.0.1:40680:DataXceiver: java.io.IOException: Reopen Block blk_6989304691537873255 is valid, and cannot be written to.
	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:613)
	at org.apache.hadoop.dfs.DataNode$BlockReceiver.<init>(DataNode.java:1996)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1109)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:982)
	at java.lang.Thread.run(Thread.java:595)

2008-01-19 23:01:31,585 INFO  fs.DFSClient (DFSClient.java:createBlockOutputStream(2024)) - Exception in createBlockOutputStream java.io.EOFException

The log shows that blk_6989304691537873255 was successfully written to two datanodes. But dfsclient timed out waiting for a response from the first datanode. It tried to recover from the failure by resending the data to the second datanode. However, the recovery failed because the second datanode threw an IOException when it detected that it already had the block. It would be nice that the second datanode does not throw an exception for a finalized block during a recovery.
"
HADOOP-2690,Adding support into build.xml to build a special hadoop jar file that has the MiniDFSCluster and MiniMRCluster classes among others necessary for building and running the unit tests of Pig on the local mini cluster,"In order to build Pig and run its unit tests with ant, Pig needs to be built and run against a hadoop jar file that not only has the MiniDFSCluster and MiniMRCluster classes but also has classes from several 3rd party libraries.  I added a target to the build.xml file of Hadoop for that purpose (see the ""Compile code for Pig"" section in the attached build.xml).   I also attached the resulted hadoop jar with this report. "
HADOOP-2687,1707 added errant INFO-level logging to DFSClient,"Marking this a blocker because it looks bad and it looks unintentional.  If it this turns out to not be a blocker for Hadoop, it will become one for HBase because 2/3rds of our DEBUG-level logs are filled with these DFSClient messages.  Below is a sample:

{code}
...
2008-01-23 01:17:31,506 DEBUG org.apache.hadoop.hbase.HStore: compaction for HStore 1028785192/info needed.
2008-01-23 01:17:31,506 DEBUG org.apache.hadoop.hbase.HRegion: 1028785192/info needs compaction
2008-01-23 01:17:31,506 INFO org.apache.hadoop.hbase.HRegion: starting compaction on region .META.,,1
2008-01-23 01:17:31,506 DEBUG org.apache.hadoop.hbase.HStore: started compaction of 4 files using hdfs://aa0-000-12.u.powerset.com:9123/hbase123/.META./compaction.dir for 1028785192/info
2008-01-23 01:17:31,548 INFO org.apache.hadoop.fs.DFSClient: Allocating new block
2008-01-23 01:17:31,549 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX.0:50010
2008-01-23 01:17:31,549 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..142:50010
2008-01-23 01:17:31,549 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..139:50010
2008-01-23 01:17:31,549 INFO org.apache.hadoop.fs.DFSClient: Connecting to XX.XX.XX..140:50010
2008-01-23 01:17:31,635 INFO org.apache.hadoop.fs.DFSClient: Closing old block blk_-1043800255529565106
2008-01-23 01:17:31,660 INFO org.apache.hadoop.fs.DFSClient: Allocating new block
2008-01-23 01:17:31,661 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..140:50010
2008-01-23 01:17:31,661 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..139:50010
2008-01-23 01:17:31,661 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..141:50010
2008-01-23 01:17:31,661 INFO org.apache.hadoop.fs.DFSClient: Connecting to XX.XX.XX..140:50010
2008-01-23 01:17:31,748 INFO org.apache.hadoop.fs.DFSClient: Closing old block blk_-8906552400781425824
2008-01-23 01:17:31,793 INFO org.apache.hadoop.fs.DFSClient: Allocating new block
2008-01-23 01:17:31,794 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..140:50010
2008-01-23 01:17:31,794 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..139:50010
2008-01-23 01:17:31,794 INFO org.apache.hadoop.fs.DFSClient: pipeline = XX.XX.XX..141:50010
2008-01-23 01:17:31,794 INFO org.apache.hadoop.fs.DFSClient: Connecting to XX.XX.XX..140:50010
2008-01-23 01:17:31,881 INFO org.apache.hadoop.fs.DFSClient: Closing old block blk_3933085663541821686
2008-01-23 01:17:31,902 DEBUG org.apache.hadoop.hbase.HStore: moving 1028785192/info/1089676733326611803 in hdfs:/XX.XX.XX.:9123/hbase123/.META./compaction.dir to 1028785192/info/8081034940986786580 in hdfs://XX.XX.XX.:9123/hbase123/.META. for 1028785192/info
2008-01-23 01:17:31,992 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region .META.,,1. Took 0sec
...
{code}

This issue was originally reported over in hadoop-user by Billy Preston."
HADOOP-2683,Provide a way to specifiy login out side an RPC,"Requirements AFIK :

It is required in some special cases (benchmarks etc) to invoke NameNode functionality without an RPC. For this users should be able to set user information that is otherwise available only an RPC.

Patch for HADOOP-1298 includes a change to Server.java so that {{Server.getUserInfo()}} does not need to in an RPC. This probably will be replaced by patch here. 

Please include any other Jira's that depend on this.

Proposed fix:
- UserGroupInformation becomes an abstract class
- public static UserGroupInformation.getUserInfo() is added. which usually just returns Server.getUserInfo();
- public static UserGroupInformation.setUserInfo(UserGroupInformation) sets  a thread local that will returned if Server.getUserInfo() returns null. 
- all invocations of Server.getUserInfo() will be replaced by UserGroupInformation.getUserInfo().

"
HADOOP-2680,Unit test fails with a timeout on nightly build: org.apache.hadoop.dfs.TestDFSStorageStateRecovery,"The unit test: TestDFSStorageStateRecovery has failed a couple of times with a timeout on the nightly build:

Here are the logs from the failures:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/373/console
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/369/console


Here is a build where it passed:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/365/console

Here is the part where it failed:

[junit] 2008-01-22 12:16:39,499 INFO  dfs.TestDFSStorageStateRecovery (TestDFSStorageStateRecovery.java:log(84)) - ============================================================
    [junit] 2008-01-22 12:16:39,502 INFO  dfs.TestDFSStorageStateRecovery (TestDFSStorageStateRecovery.java:log(85)) - ***TEST 34*** NAME_NODE recovery: numDirs=1 testCase=17 current=false previous=true previous.tmp=false removed.tmp=true
    [junit] 2008-01-22 12:16:39,499 INFO  ipc.Server (Server.java:run(939)) - IPC Server handler 4 on 36207: exiting
    [junit] 2008-01-22 12:16:39,499 INFO  ipc.Server (Server.java:run(939)) - IPC Server handler 3 on 36207: exiting
    [junit] 2008-01-22 12:16:39,499 INFO  ipc.Server (Server.java:run(939)) - IPC Server handler 7 on 36207: exiting
    [junit] 2008-01-22 12:16:39,499 INFO  ipc.Server (Server.java:run(353)) - Stopping IPC Server listener on 36207
    [junit] 2008-01-22 12:16:39,499 INFO  ipc.Server (Server.java:run(939)) - IPC Server handler 2 on 36207: exiting
    [junit] 2008-01-22 12:16:40,153 INFO  jvm.JvmMetrics (RpcMetrics.java:setTags(49)) - Initializing RPC Metrics with serverName=NameNode, port=0
    [junit] javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=RpcStatistics
    [junit] 	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
    [junit] 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
    [junit] 	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:50)
    [junit] 	at org.apache.hadoop.ipc.metrics.RpcMgt.<init>(RpcMgt.java:37)
    [junit] 	at org.apache.hadoop.ipc.metrics.RpcMetrics.<init>(RpcMetrics.java:60)
    [junit] 	at org.apache.hadoop.ipc.Server.<init>(Server.java:969)
    [junit] 	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:393)
    [junit] 	at org.apache.hadoop.ipc.RPC.getServer(RPC.java:355)
    [junit] 2008-01-22 12:16:40,156 INFO  dfs.NameNode (NameNode.java:initialize(125)) - Namenode up at: localhost/127.0.0.1:36768
    [junit] 	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:119)
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:174)
    [junit] 2008-01-22 12:16:40,156 INFO  jvm.JvmMetrics (JvmMetrics.java:init(51)) - Cannot initialize JVM Metrics with processName=NameNode, sessionId=null - already initialized
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:160)
    [junit] 	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:849)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:195)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:134)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:86)
    [junit] 	at org.apache.hadoop.dfs.TestDFSStorageStateRecovery.testStorageStates(TestDFSStorageStateRecovery.java:193)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:585)
    [junit] 	at junit.framework.TestCase.runTest(TestCase.java:154)
    [junit] 	at junit.framework.TestCase.runBare(TestCase.java:127)
    [junit] 2008-01-22 12:16:40,158 INFO  dfs.NameNodeMetrics (NameNodeMetrics.java:<init>(74)) - Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.spi.NullContext
    [junit] 	at junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit] 	at junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit] 	at junit.framework.TestResult.run(TestResult.java:109)
    [junit] 	at junit.framework.TestCase.run(TestCase.java:118)
    [junit] 	at junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit] 	at junit.framework.TestSuite.run(TestSuite.java:203)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
    [junit] javax.management.InstanceAlreadyExistsException: hadoop.dfs:service=NameNode,name=NameNodeStatistics
    [junit] 	at com.sun.jmx.mbeanserver.RepositorySupport.addMBean(RepositorySupport.java:452)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1410)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
    [junit] 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:337)
    [junit] 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:497)
    [junit] 	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:50)
    [junit] 	at org.apache.hadoop.dfs.namenode.metrics.NameNodeMgt.<init>(NameNodeMgt.java:36)
    [junit] 	at org.apache.hadoop.dfs.NameNodeMetrics.<init>(NameNodeMetrics.java:67)
    [junit] 	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:127)
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:174)
    [junit] 	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:160)
    [junit] 	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:849)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:195)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:134)
    [junit] 	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:86)
    [junit] 	at org.apache.hadoop.dfs.TestDFSStorageStateRecovery.testStorageStates(TestDFSStorageStateRecovery.java:193)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:585)
    [junit] 	at junit.framework.TestCase.runTest(TestCase.java:154)
    [junit] 	at junit.framework.TestCase.runBare(TestCase.java:127)
    [junit] 	at junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit] 	at junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit] 	at junit.framework.TestResult.run(TestResult.java:109)
    [junit] 	at junit.framework.TestCase.run(TestCase.java:118)
    [junit] 	at junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit] 	at junit.framework.TestSuite.run(TestSuite.java:203)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
    [junit] 2008-01-22 12:16:40,193 INFO  fs.FSNamesystem (FSNamesystem.java:setConfigurationParameters(321)) - fsOwner=hudson,hudson
    [junit] 2008-01-22 12:16:40,195 INFO  fs.FSNamesystem (FSNamesystem.java:setConfigurationParameters(325)) - supergroup=supergroup
    [junit] 2008-01-22 12:16:40,195 INFO  fs.FSNamesystem (FSNamesystem.java:setConfigurationParameters(326)) - isPermissionEnabled=true
    [junit] 2008-01-22 12:16:40,198 INFO  dfs.Storage (Storage.java:doRecover(369)) - Recovering storage directory /export/home/hudson/hudson/jobs/Hadoop-Nightly/workspace/trunk/build/test/data/name1 from previous rollback.
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestDFSStorageStateRecovery FAILED (timeout) "
HADOOP-2679,There is a small typeo in hdfs_test.c when testing the success of the local hadoop initialization,"    hdfsFS lfs = hdfsConnect(NULL, 0);
    if(!fs) {
        fprintf(stderr, ""Oops! Failed to connect to 'local' hdfs!\n"");
        exit(-1);
    } 

The test should be
    hdfsFS lfs = hdfsConnect(NULL, 0);
    if(!lfs) {
        fprintf(stderr, ""Oops! Failed to connect to 'local' hdfs!\n"");
        exit(-1);
    } "
HADOOP-2670,doDF frequently brings task down due to lack of memory,"we are running with -Xmx 1024M. Every once in a while, we see tasks failing because of:

java.io.IOException: java.io.IOException: Cannot allocate memory
	at java.lang.UNIXProcess.(UNIXProcess.java:148)
	at java.lang.ProcessImpl.start(ProcessImpl.java:65)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:451)
	at java.lang.Runtime.exec(Runtime.java:591)
	at java.lang.Runtime.exec(Runtime.java:464)
	at org.apache.hadoop.fs.DF.doDF(DF.java:60)
	at org.apache.hadoop.fs.DF.getAvailable(DF.java:99)
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:259)
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:289)
	at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:155)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:1475)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.openBackupStream(DFSClient.java:1442)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:1600)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:140)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:122)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:39)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:822)
	at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.write(SequenceFileOutputFormat.java:69)
	at org.apache.hadoop.mapred.ReduceTask$2.collect(ReduceTask.java:304)
	at com.facebook.hive.streaming.HiveJoin$JoinReduce.reduce(HiveJoin.java:546)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:322)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1743)

when the task re-runs - it succeeds. it seems like that this is an edge case where the garbage collector needs to be run before trying to spawn external process. (going to try it out). any other ideas?

"
HADOOP-2669,DFS client lost lease during writing into DFS files,"
I have a program that reads a block compressed sequence file, does some processing on the records and writes the
processed records into another  block compressed sequence file.
During execution of the program, I got the following exception: 

org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.LeaseExpiredException: No lease on xxxxx/part-00000
        at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:976)
        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:293)
        at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)

        at org.apache.hadoop.ipc.Client.call(Client.java:482)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at org.apache.hadoop.dfs.$Proxy0.addBlock(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy0.addBlock(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:1554)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:1500)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1626)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:1602)
        at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:140)
        at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:100)
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:39)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.writeBuffer(SequenceFile.java:1181)
        at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.sync(SequenceFile.java:1198)

        at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.append(SequenceFile.java:1248)
        at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.write(SequenceFileOutputFormat.java:69)
     "
HADOOP-2664,lzop-compatible CompresionCodec,The current lzo codec is not compatible with the standard .lzo file format used by lzop.
HADOOP-2659,The commands in DFSAdmin should require admin privilege,"The commands in DFSAdmin and the corresponding RPC calls should require admin privilege.

DFSAdmin commands:
-report
-safemode
-refreshNodes
-finalizeUpgrade
-upgradeProgress
-metasave

ClientProtocol:
{code}
public void renewLease(String clientName) throws IOException;
public long[] getStats() throws IOException;
public DatanodeInfo[] getDatanodeReport(FSConstants.DatanodeReportType type) throws IOException;
public boolean setSafeMode(FSConstants.SafeModeAction action) throws IOException;
public void refreshNodes() throws IOException;
public void finalizeUpgrade() throws IOException;
public UpgradeStatusReport distributedUpgradeProgress(UpgradeAction action) throws IOException;
public void metaSave(String filename) throws IOException;
{code}
"
HADOOP-2658,Design and Implement a Test Plan to support appends to HDFS files,"HADOOP-1700 describes the design of supporting appends to HDFS files. This feature needs extensive testing, especially because the design explicitly analyzes many failure scenarios. A detailed test plan and test cases are needed to make this feature a reality."
HADOOP-2657,Enhancements to DFSClient to support flushing data at any point in time,"The HDFS Append Design (HADOOP-1700) requires that there be a public API to flush data written to a HDFS file that can be invoked by an application. This API (popularly referred to a fflush(OutputStream)) will ensure that data written to the DFSOutputStream is flushed to datanodes and any required metadata is persisted on Namenode.

This API has to handle the case when the client decides to flush after writing data that is not a exact multiple of io.bytes.per.checksum."
HADOOP-2656,Support for upgrading existing cluster to facilitate appends to HDFS files,HADOOP-1700 describes the design for supporting appends to HDFS files. This design requires a distributed-upgrade to existing cluster installations. The design specifies that the DataNode persist the 8-byte BlockGenerationStamp in the block metadata file. The upgrade code will introduce this new field in the block metadata file and initialize this value to 0.
HADOOP-2655,Copy on write for data and metadata files in the presence of snapshots,"If a DFS Client wants to append data to an existing file (appends, HADOOP-1700) and a snapshot is present, the Datanoed has to implement some form of a copy-on-write for writes to data and meta data files."
HADOOP-2652,Fix permission issues for HftpFileSystem,"Similar to HADOOP-2614, HftpFileSystem requires UserGroupInformation to access NameNode."
HADOOP-2649,The ReplicationMonitor sleep period should be configurable,"The HDFS Namenode computes replication work for datanodes once every 3 seconds. This should be a configurable  value. On large clusters, there could be many many blocks in the neededReplication queue and computing replication work for datanodes once every 3 seconds might consume lots of CPU on namenode."
HADOOP-2646,SortValidator broken with fully-qualified working directories,"The sort validator is broken by HADOOP-2567.  In particular, it no longer works when DistributedFileSystem#getWorkingDirectory() returns a fully-qualified path.
"
HADOOP-2645,Additional metrics  & jmx beans and cleanup to use the recent metrics libraries,"This patch does following:

Data node :
   Additional metrics
   Cleunup old metrics to use recent metrics lib
   MBeans for data node metrics

Namenode
   Cleunup old updates of metrics to use recent metrics lib
  Split Name none mbean to 2 MBeans - statistics and FSNamesystem status
  rename of name node mbean for consistency."
HADOOP-2640,MultiFileSplitInputFormat always returns 1 split when avgLengthPerSplit > Integer.MAX_VALUE,"pb is in findSize method, the partialLength variable is an integer and should be a long"
HADOOP-2639,Reducers stuck in shuffle,"I started sort benchmark on 500 nodes. It has 40000 maps and 900 reducers.
There are 11 reducers stuck in shuffle with 33% progress. I could see a node down which ran 80 maps on it. And all these reducers are trying to fetch map output from that node. "
HADOOP-2635,"If local file included a '%' character in a file name, we can't copy to dfs becuase RawLocalFileSystem.getPath() returns urlencoded '%25'","{code}
FileStatus fileStatus = new RawLocalFileStatus(new File(""udanax/Ageha100%.html""), getDefaultBlockSize());
LOG.info(fileStatus.getPath());
{code}

Log :
file:/root/workspace/hadoop/udanax/Ageha100%25.html"
HADOOP-2634,Deprecate exists() and isDir() to simplify ClientProtocol.,"ClientProtocol can be simplified by removing two methods
{code}
public boolean exists(String src) throws IOException;
public boolean isDir(String src) throws IOException;
{code}
This is a redundant api, which can be implemented in DFSClient as convenience methods using
{code}
public DFSFileInfo getFileInfo(String src) throws IOException;
{code}
Note that we already deprecated several Filesystem method and advised to use getFileStatus() instead.
Should we deprecate them in 0.16?"
HADOOP-2633,Revert change to fsck made as part of permissions implementation,Earlier change has unacceptable performance behavior.
HADOOP-2632,Discussion of fsck operation in the permissions regime,"Proposal: In 0.16, with permission checking enabled, fsck will just work regardless of permission settings.

The normal operation of fsck can reveal information about the name system that would otherwise be unavailable to a user via other commands that would be subject to permission checking. While not best, this situation seems tolerable for 0.16.
  1. It not certain what permission checking should be done, other than perhaps just restricting fsck to the superuser.
  2. The information revealed is not too privileged.
  3. The mischievous user has better things to do.
  4. fsck does not fit with either of the existing models for permission checking.

fsck is implemented as HTTP GET of a well-known URL. As such, the first thought is that it should operate with the permissions of web UI client. In 0.16, the web UI client is presumed to have the identity of some cluster-configured user. If the cluster-configured user is the superuser, web UI clients can browse the contents of any file. If the user is any other identity, the user would not have full access to the name space necessary for fsck to operate unless every directory had mode a+r. The alternative is to treat fsck according to the rules of other administrative commands. At present, fsck does no RPC requests, and so there is no option to apply the permission checking rules used by other commands.

If it is important to change 0.16 so that fsck checked user identity, an implementation would have to use a new RPC to obtain a ticket that authorized access to the fsck URL. The ticket would be passed as a parameter to HTTP GET. The implementation of fsck would check the the proffered ticket was valid before traversing the name space."
HADOOP-2626,RawLocalFileStatus is badly handling URIs,"as a result, files with special characters (that get encoded when translated to URIs) are badly handled using a local filesystem.

{{new Path(f.toURI().toString()))}} should be replaced by {{new Path(f.toURI().getPath()))}}

IMHO, each call to {{toURI().toString()}} should be considered suspicious. There's another one in the class CopyFiles at line 641."
HADOOP-2620,'bin/hadoop fs -help' does not list file permissions commands.,"'{{bin/hadoop fs -help}}' does not list chmod, chown, and chgrp. But '{{bin/hadoop fs -help chmod}}' etc  work as expected."
HADOOP-2614,dfs web interfaces should run as a configurable user account,"Currently, web interfaces (i.e. browseDirectory.jsp, browseBlock.jsp, etc.) run in each Datanode.  The web interfaces use the corresponding Datanode account (the user who ever started the Datanode) to connect to Namenode.  Usually, Datanodes are started by administrators.  As a consequence, the web interfaces have administrator access, which is undesirable.  The web interfaces account should be configurable."
HADOOP-2606,Namenode unstable when replicating 500k blocks at once,"We tried to decommission about 40 nodes at once, each containing 12k blocks. (about 500k total)
(This also happened when we first tried to decommission 2 million blocks)

Clients started experiencing  ""java.lang.RuntimeException: java.net.SocketTimeoutException: timed out waiting for rpc
response"" and namenode was in 100% cpu state. 

It was spending most of its time on one thread, 

""org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@7f401d28"" daemon prio=10 tid=0x0000002e10702800 nid=0x6718
runnable [0x0000000041a42000..0x0000000041a42a30]
   java.lang.Thread.State: RUNNABLE
        at org.apache.hadoop.dfs.FSNamesystem.containingNodeList(FSNamesystem.java:2766)
        at org.apache.hadoop.dfs.FSNamesystem.pendingTransfers(FSNamesystem.java:2870)
        - locked <0x0000002aa3cef720> (a org.apache.hadoop.dfs.UnderReplicatedBlocks)
        - locked <0x0000002aa3c42e28> (a org.apache.hadoop.dfs.FSNamesystem)
        at org.apache.hadoop.dfs.FSNamesystem.computeDatanodeWork(FSNamesystem.java:1928)
        at org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor.run(FSNamesystem.java:1868)
        at java.lang.Thread.run(Thread.java:619)


We confirmed that Namenode was not in the fullGC states when these problem happened.

Also, dfsadmin -metasave was showing ""Blocks waiting for replication"" was decreasing very slowly.

I believe this is not specific to decommission and same problem would happen if we lose one rack.


"
HADOOP-2605,leading slash in mapred.task.tracker.report.bindAddress,"TaskTracker incorrectly sets mapred.task.tracker.report.bindAddress with a slash in front of the host:port pair.
This described in more details here: [Deveraj|http://issues.apache.org/jira/browse/HADOOP-2404#action_12554551] and [Konstantin|http://issues.apache.org/jira/browse/HADOOP-2404#action_12554859]"
HADOOP-2603,SequenceFileAsBinaryInputFormat,"Add an InputFormat to read the raw bytes as keys, values from a SequenceFile"
HADOOP-2601,TestNNThroughput should not use a fixed namenode port,"TestNNThroughput failed with the following error: 
Address already in use
java.net.BindException: Address already in use
        at java.net.PlainSocketImpl.socketBind(Native Method)
        at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:359)
        at java.net.ServerSocket.bind(ServerSocket.java:319)
        at java.net.ServerSocket.<init>(ServerSocket.java:185)
        at org.mortbay.util.ThreadedServer.newServerSocket(ThreadedServer.java:391)
        at org.mortbay.util.ThreadedServer.open(ThreadedServer.java:477)
        at org.mortbay.util.ThreadedServer.start(ThreadedServer.java:503)
        at org.mortbay.http.SocketListener.start(SocketListener.java:203)
        at org.mortbay.http.HttpServer.doStart(HttpServer.java:761)
        at org.mortbay.util.Container.start(Container.java:72)
        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:182)
        at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:223)
        at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:129)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:174)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:160)
        at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:849)
        at org.apache.hadoop.dfs.NNThroughputBenchmark.<init>(NNThroughputBenchmark.java:57)
        at org.apache.hadoop.dfs.NNThroughputBenchmark.runBenchmark(NNThroughputBenchmark.java:752)
        at org.apache.hadoop.dfs.TestNNThroughputBenchmark.testNNThroughput(TestNNThroughputBenchmark.java:15)
"
HADOOP-2596,add SequenceFile.createWriter() method that takes block size as parameter,"Currently it is not possible to create a SequenceFile.Writer using a block size other than the default.

The createWriter() method should be overloaded with a signature receiving block size as parameter should be added to the the SequenceFile class.

With all the current signatures for this method there is a significant code duplication, if possible the createWriter() methods  should be refactored to avoid such duplication."
HADOOP-2586,Add version to servers' startup massage.,"It would be useful if hadoop servers printed hadoop version as a part of the startup message:
{code}
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = my-hadoop-host
STARTUP_MSG:   args = [-upgrade]
STARTUP_MSG: Version = 0.15.1, r599161
************************************************************/
{code}

This would simplify understanding the logs."
HADOOP-2585,Automatic namespace recovery from the secondary image.,"Hadoop has a three way (configuration controlled) protection from loosing the namespace image.
# image can be replicated on different hard-drives of the same node;
# image can be replicated on a nfs mounted drive on an independent node;
# a stale replica of the image is created during periodic checkpointing and stored on the secondary name-node.

Currently during startup the name-node examines all configured storage directories, selects the
most up to date image, reads it, merges with the corresponding edits, and writes to the new image back 
into all storage directories. Everything is done automatically.

If due to multiple hardware failures none of those images on mounted hard drives (local or remote) 
are available the secondary image although stale (up to one hour old by default) can be still 
used in order to recover the majority of the file system data.
Currently one can reconstruct a valid name-node image from the secondary one manually.
It would be nice to support an automatic recovery.
"
HADOOP-2583,Potential Eclipse plug-in UI loop when editing location parameters,"The UI might enter an infinite loop, when propagating parameters asynchronously.
Some functions are not yet implemented"
HADOOP-2582,"hadoop dfs -copyToLocal creates zero byte files, when source file does not exists ","hadoop dfs -copyToLocal with an no existing source file creates a zero byte destination file. It should throw an error message indicating the source file does not exists.

{noformat}
[lohit@ hadoop-trunk]$ hadoop dfs -get nosuchfile nosuchfile
[lohit@ hadoop-trunk]$ ls -l nosuchfile 
-rw-r--r--  1 lohit users 0 Jan 11 21:58 nosuchfile
[lohit@ hadoop-trunk]$
{noformat}"
HADOOP-2581,Counters and other useful stats should be logged into Job History log,"
The following stats are useful and  available to JT but not logged job history log:

1. The counters of each job
2. The counters of each mapper/reducer attempt
3. The info about the input splits (filename, split size, on which nodes)
3. The input split for each mapper attempt

Those data is useful and important for mining to find out performance related problems.





"
HADOOP-2576,Namenode performance degradation over time,"We have a cluster running the same applications again and again with a high turnover of files.

The performance of these applications seem to be correlated to the lifetime of the namenode:
After starting the namenode, the applications need increasingly more time to complete, with about 50% more time after 1 week. 

During that time the namenode average cpu usage increases from typically 10% to 30%, memory usage nearly doubles (although the average amount of data on dfs stays the same), and the average load factor increases by a factor of 2-3 (although not  significantly high, <2).

When looking at the namenode and datanode logs, I see a lot of asks to delete blocks coming from the namenode for blocks not in the blockmap of the datanodes, repeatedly for the same blocks.
When I counted the number of blocks asked by the namenode to be deleted, I noticed a noticeable increase with the lifetime of the namenode (a factor of 2-3 after 1 week).

This makes me wonder whether the namenode does not purge the list of invalid blocks from non-existing blocks.

But independently, the namenode has a degradation issue."
HADOOP-2574,bugs in mapred tutorial,"Sam Pullara sends me:
{noformat}
Phu was going through the WordCount example... lines 52 and 53 should have args[0] and args[1]:

http://lucene.apache.org/hadoop/docs/current/mapred_tutorial.html

The javac and jar command are also wrong, they don't include the directories for the packages, should be:

$ javac -classpath ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-core.jar -d classes WordCount.java 
$ jar -cvf /usr/joe/wordcount.jar WordCount.class -C classes .

{noformat}"
HADOOP-2571,javac generates a warning in test/o.a.h.io.FileBench,"FileBench generates the following warning in trunk:

{quote}
    [javac] {...}/src/test/org/apache/hadoop/io/FileBench.java:341: warning: [unchecked] unchecked cast
    [javac] found   : java.lang.Enum
    [javac] required: T
    [javac]       set.add((T)fullmap.get(c).get(s));
    [javac]                                    ^
{quote}

This should be suppressed."
HADOOP-2570,streaming jobs fail after HADOOP-2227,"HADOOP-2227 changes jobCacheDir. In streaming, jobCacheDir was constructed like this
{code}
File jobCacheDir = new File(currentDir.getParentFile().getParent(), ""work"");
{code}

We should change this to get it working. Referring to the changes made in HADOOP-2227, I see that the APIs used in there to construct the path are not public. And hard coding the path in streaming does not look good. thought?"
HADOOP-2569,Unit test times out on Solaris nightly build: mapred.TestMultiFileInputFormat,"Unit test failed in the nightly build: org.apache.hadoop.mapred.TestMultiFileInputFormat.unknown

junit.framework.AssertionFailedError: Timeout occurred

Logs are at:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/361/"
HADOOP-2567,add FileSystem#getHomeDirectory() method,"The FileSystem API would benefit from a getHomeDirectory() method.

The default implementation would return ""/user/$USER/"".

RawLocalFileSystem would return System.getProperty(""user.home"").

HADOOP-2514 can use this to implement per-user trash.
"
HADOOP-2566,need FileSystem#globStatus method,"To remove the cache of FileStatus in DFSPath (HADOOP-2565) without hurting performance, we must use file enumeration APIs that return FileStatus[] rather than Path[].  Currently we have FileSystem#globPaths(), but that method should be deprecated and replaced with a FileSystem#globStatus().

We need to deprecate FileSystem#globPaths() in 0.16 in order to remove the cache in 0.17.
"
HADOOP-2563,Remove deprecated FileSystem#listPaths(),"FileSystem#listPaths() has been deprecated for a few releases, and we should now remove it, upgrading everything to use FileSystem#listStatus().
"
HADOOP-2562,"globPaths does not support {ab,cd} as it claims to","Olga reports: 

According to 0.15 documentation, FileSystem::globPaths supports {ab,cd} matching. However, when I tried to use it with pattern /data/mydata/{data1,data2} I got no results even though I could find the individual files.
"
HADOOP-2559,DFS should place one replica per rack,"Currently, when writing out a block, dfs will place one copy to a local data node, one copy to a rack local node
and another one to a remote node. This leads to a number of undesired properties:

1. The block will be rack-local to two tacks instead of three, reducing the advantage of rack locality based scheduling by 1/3.

2. The Blocks of a file (especiallya  large file) are unevenly distributed over the nodes: One third will be on the local node, and two thirds on the nodes on the same rack. This may make some nodes full much faster than others, 
increasing the need of rebalancing. Furthermore, this also make some nodes become ""hot spots"" if those big 
files are popular and accessed by many applications.


"
HADOOP-2555,Refactor the HTable#get and HTable#getRow methods to avoid repetition of retry-on-failure logic,"The following code is repeated in every one of HTable#get and HTable#getRow methods:

{code:title=HTable.java|borderStyle=solid}
    MapWritable value = null;
    for (int tries = 0; tries < numRetries; tries++) {
      HRegionLocation r = getRegionLocation(row);
      HRegionInterface server =
        connection.getHRegionConnection(r.getServerAddress());
      
      try {
        value = server.getRow(r.getRegionInfo().getRegionName(), row, ts);  // This is the only line of code that changes significantly between methods
        break;
        
      } catch (IOException e) {
        if (e instanceof RemoteException) {
          e = RemoteExceptionHandler.decodeRemoteException((RemoteException) e);
        }
        if (tries == numRetries - 1) {
          // No more tries
          throw e;
        }
        if (LOG.isDebugEnabled()) {
          LOG.debug(""reloading table servers because: "" + e.getMessage());
        }
        tableServers = connection.reloadTableServers(tableName);
      }
      try {
        Thread.sleep(this.pause);
        
      } catch (InterruptedException x) {
        // continue
      }
    }
{code}

This should be factored out into a protected method that handles retry-on-failure logic to facilitate more robust testing and the development of new API methods.

Proposed modification:

// Execute the provided Callable against the server
protected <T> callServerWithRetries(Callable<T> callable) throws RemoteException;

The above code could then be reduced to:
{code:title=HTable.java|borderStyle=solid}
    MapWritable value = null;
    final connection;
    try {
      value = callServerWithRetries(new Callable<MapWritable>() {
            HRegionLocation r = getRegionLocation(row);
            HRegionInterface server =
                connection.getHRegionConnection(r.getServerAddress());
            server.getRow(r.getRegionInfo().getRegionName(), row, ts);
          });
    } catch (RemoteException e) {
      // handle unrecoverable remote exceptions
    }
{code}

This would greatly ease the development of new API methods by reducing the amount of code needed to implement a new method and reducing the amount of logic that needs to be tested per method."
HADOOP-2552,enable hdfs permission checking by default,"We should enable permission checking in dfs by default.  Currently, on upgrade, all file permissions are 777, so this is a back-compatible change.  After an upgrade folks can change owners and groups and limit permissions, and things will work as expected.

The current default, dfs.permissions=false, gives inconsistent behaviour: permissions are displayed in 'ls' and returned by the FileSystem APIs, but they're not enforced.  In future releases we will certainly want dfs.permissions=true to be the default, and making it so now will thus also avoid an incompatible change.

dfs.permissions=false should be an optional, non-default configuration that some sites may decide to use.  It is further defined in HADOOP-2543.
"
HADOOP-2551,hadoop-env.sh needs finer granularity,"We often configure our HADOOP_OPTS on the name node to have JMX running so that we can do JVM monitoring.  But doing so means that we need to edit this file if we want to run other hadoop commands, such as fsck.  It would be useful if hadoop-env.sh was refactored a bit so that there were different and/or cascading HADOOP_OPTS dependent upon which process/task was being performed.  "
HADOOP-2549,hdfs does not honor dfs.du.reserved setting,"running 0.14.4. one of our drives is smaller and is always getting disk full. i reset the disk reservation to 1Gig - but it was filled quickly again.

i put in some tracing in getnextvolume. the blocksize argument is 0. so every volume (regardless of available space) qualifies. here's the trace:

/* root disk chosen with 0 available bytes. format is <available>:<blocksize>*/
2008-01-08 08:08:51,918 WARN org.apache.hadoop.dfs.DataNode: Volume /var/hadoop/tmp/dfs/data/current:0:0

/* some other disk chosen with 300G space. */
2008-01-08 08:09:21,974 WARN org.apache.hadoop.dfs.DataNode: Volume /mnt/d1/hdfs/current:304725631026:0

i am going to default blocksize to something reasonable when it's zero for now.

this is driving us nuts since our automounter starts failing when we run out of space. so everything's broke.
"
HADOOP-2547,remove use of 'magic number' in build.xml,"build.xml has a call to touch task as 
 <touch datetime=""01/25/1971 2:00 pm"">
it should be changed to
 <touch millis=""0""> 

It will avoid using magic values like ""01/25/1971 2:00 pm"" and make the script easier to understand
"
HADOOP-2543,No-permission-checking mode for smooth transition to 0.16's permissions features. ,"In moving to 0.16,  which will support permissions, a mode of no-permission checking has been proposed to allow smooth transition to using the new permissions feature.
The idea is that at first 0.16 will be used for a period of time with permission checking off. 
Later after the admin has changed ownership and permissions of various files, the permission checking can be turned off.

This Jira defines what the semantics are of the no-permission-checking mode."
HADOOP-2541,Online Snapshotting Capability,"Modern file systems have the ability to create snapshots of the running file system without having to unmount.  HDFS should be offer similar capabilities to allow admins the ability to perform ""online"" backups of the file system such that files can be recovered after deletions or, for extra bonus points, catastrophic failures."
HADOOP-2540,"Empty blocks make fsck report corrupt, even when it isn't","If the name node crashes after blocks have been allocated and before the content has been uploaded, fsck will report the zero sized files as corrupt upon restart:

/user/rajive/rand0/_task_200712121358_0001_m_000808_0/part-00808: MISSING 1 blocks of total size 0 B

... even though all blocks are accounted for:

Status: CORRUPT
 Total size:    2932802658847 B
 Total blocks:  26603 (avg. block size 110243305 B)
 Total dirs:    419
 Total files:   5031
 Over-replicated blocks:        197 (0.740518 %)
 Under-replicated blocks:       0 (0.0 %)
 Target replication factor:     3
 Real replication factor:       3.0074053


The filesystem under path '/' is CORRUPT

In UFS and related filesystems, such files would get put into lost+found after an fsck and the filesystem would return back to normal.  It would be super if HDFS could do a similar thing.  Perhaps if all of the nodes stored in the name node's 'includes' file have reported in, HDFS could automatically run a fsck and store these not-necessarily-broken files in something like lost+found.  

Files that are actually missing blocks, however, should not be touched."
HADOOP-2538,NPE in TaskLog.java,"In the tasktracker web ui, if I go to

/tasklog?taskid=task_200801020752_0383_m_000000_0&all=true&plaintext=true

which corresponds to a short log (<4k), I get a 500 in the web ui, and this NPE in the tasktracker log:

2008-01-07 21:02:13,935 WARN /: /tasklog?taskid=task_200801020752_0383_m_000000_
0&all=true&plaintext=true: 
java.lang.NullPointerException
        at org.apache.hadoop.mapred.TaskLog.getTaskLogFile(TaskLog.java:48)
        at org.apache.hadoop.mapred.TaskLog$Reader.<init>(TaskLog.java:124)
        at org.apache.hadoop.mapred.TaskLogServlet.printTaskLog(TaskLogServlet.j
ava:44)
        at org.apache.hadoop.mapred.TaskLogServlet.doGet(TaskLogServlet.java:134
)

Note that /tasklog?taskid=task_200801020752_0383_m_000000_0&all=true&plaintext=true is an invalid url; the url should look like &plaintext=true&filter=STDOUT"
HADOOP-2537,make build process compatible with Ant 1.7.0,"This section of build.xml behaves differently between Ant 1.6.5 and Ant 1.7.0:

{code:xml}
    <touch datetime=""01/25/1971 2:00 pm"">
      <fileset dir=""${conf.dir}"" includes=""**/*.template""/>
      <fileset dir=""${contrib.dir}"" includes=""**/*.template""/>
    </touch>
{code}

In Ant 1.6.5, if the fileset is empty (which it is if you're building from a released tar.gz distribution) then this section silently passes.

In Ant 1.7.0, if the fileset is empty, the following error is raised and the build stops:
{quote}
BUILD FAILED
/home/ndaley/Downloads/hadoop-0.15.1/build.xml:151: Specify at least one source--a file or resource collection.
{quote}

"
HADOOP-2536,MapReduce for MySQL,Add support for running MapReduce jobs over data residing in a MySQL table.
HADOOP-2535,Remove support for deprecated mapred.child.heap.size and indentation fix in TaskRunner.java,TaskRunner.java (289-344) have wrong indentation - 4 spaces rather than the standard 2.
HADOOP-2534,File manager frontend for Hadoop DFS (with proof of concept).,"I had problems classifying this, but since it's not an improvement and neither a task, I thought I'd put it under ""wishes"". I like command line, but using hadoop fs -X ... leaves my fingers hurt after some time. I though it would be great to have a file manager-like front end to DFS. So I modified muCommander (Java-based) a little bit and voila -- it works _great_, especially for browsing/ uploading and deleting stuff.

I uploaded the binary and WebStart-launchable version here:

http://project.carrot2.org/varia/mucommander-hdfs

Look at screenshots, they will give you a clue about how it works. I had some thoughts about publishing the source code -- muCommander is GPLed... so I guess it can't reside in Hadoop's repository anyway, no matter what we do. If you need sources, let me know.

Finally, a few thoughts stemming from the coding session:

    *  DF utility does not work under Windows. This has been addressed recently on the mailing list (HADOOP-33), so it's not a big issue I guess.

    * I support the claim that it would be sensible to introduce a client interface to DFS and provide two implementations -- one with intelligent spooling on local disk (using DF) and one with some simpler form of spooling (in /tmp for example). Note the funky shape of the upload chart above resulting from delay between spooling and chunk upload. I don't know if this can be worked around in any way.

    * Incompatible protocol version causes exceptions. Since the protocol changes quite frequently (isn't it version 20 at the moment?), some way of choosing the connection protocol to Hadoop and keeping the most recent versions around would be very useful for external clients."
HADOOP-2529,DFS User Guide,"We need a user guide for DFS on the lines of http://lucene.apache.org/hadoop/docs/r0.15.1/mapred_tutorial.html . This could be the starting point for new users.

Though this is marked for 0.16, I think it could go in even after feature freeze date."
HADOOP-2524,Unit test fails on Solaris nightly build: dfs.TestDFSStorageStateRecovery,"Unit test failed on Solaris:

org.apache.hadoop.dfs.TestDFSStorageStateRecovery.unknown

junit.framework.AssertionFailedError: Timeout occurred

Complete log is available at:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/354/console"
HADOOP-2523,Unit test fails on Windows: TestDFSShell.testFilePermissions,"Unit test fails on Windows:

org.apache.hadoop.dfs.TestDFSShell.testFilePermissions

junit.framework.ComparisonFailure: expected:<SYSTEM> but was:<hadoopqa>
	at org.apache.hadoop.dfs.TestDFSShell.testFilePermissions(TestDFSShell.java:346)


Standard Error: 
text: /texttest/file.gz
cat: java.io.IOException: File does not exist: /test/mkdirs/myFile1

rm: Delete failed /test/mkdirs/myFile1
cp: Cannot copy /test/dir1 to its subdirectory /test/dir1/dir2
"
HADOOP-2516,HADOOP-1819 removed a public api JobTracker.getTracker in 0.15.0,"HADOOP-1819 removed a 0.14.0 public api {{JobTracker.getTracker}} in 0.15.0.

http://svn.apache.org/viewvc?view=rev&revision=575438 and
http://svn.apache.org/viewvc/lucene/hadoop/branches/branch-0.15/src/java/org/apache/hadoop/mapred/JobTracker.java?r1=573708&r2=575438&diff_format=h

There is a simple work-around i.e. use the return value of {{JobTracker.startTracker}} ... yet, is this a 0.15.2 blocker?"
HADOOP-2515,Addition of Matrix Input-Output format,"After discuss with gary bradski, i decided to try working for matrix processing on Hadoop.
If you have any advice, please let me know.

* A matrix of features
** Each row is one input vector.
** Each column coreesponds to a separate feature. 
*** indicates the i-th row vector

* Splitting the input matrix
** The input matrix is split into n submatrix, where n is the number of mapper.
** Computation is are on input vectors; only 1-D decomposition is done.
"
HADOOP-2514,Trash and permissions don't mix,"Shell command ""rm"" is really ""mv"" to trash with the expectation that the server will at some point really delete the contents of trash. With the advent of permissions, a user can ""mv"" folders that the user cannot ""rm"". The present trash feature as implemented would allow the user to suborn the server into deleting a folder in violation of the permissions model.

A related issue is that if anybody can mv a folder to the trash anybody else can mv that same folder from the trash. This may be contrary to the expectations of the user.

What is a better model for trash?"
HADOOP-2512,error stream handling in Shell executor ,"Fix a couple of issues while handling error stream in Shell (added in HADOOP-2344) :

# fix typo in {{System.getProperty(""line.seperator"")}}, currently it adds ""null"" instead of ""\n"".
# completed is not set to {{true}} when a process exits with an error.
# In normal error case, it reads errMsg (to create IOException) before waiting for errThread to complete, which results in in consistent error message. I will attach a patch."
HADOOP-2511,HADOOP-2344 introduced a javadoc warning,"{noformat}
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/util/Shell.java:70: warning - @param argument ""Interval"" is not a parameter name.
{noformat}
"
HADOOP-2509,Add rat target to build,"The ARAT tool should be run before each release:
java -cp rat-0.5.1.jar rat.Report hadoop-x.y.z

It's available from:
http://code.google.com/p/arat/

The output for 0.15.2 shows some missing license headers which must be fixed for 0.16.0:
http://people.apache.org/~siren/hadoop-0.15.2-rc0/rat-hadoop-0.15.2.txt
"
HADOOP-2503,REST Insert / Select,"On inserts I submit the row key as urlencoded and its stored that way with the urlencodeing intact.

But when I select with rest I send the row key as urlencoded as I stored it and get nothing back. I thank the problem is the rest interfase is urldecodeing the request before looking up the data in the table. I can select with the urlencoded row key with the shell and get results.

This makes data not selectable if it has any urlencodeing chrs.

So 1 of two options I see
Make the insert store the row key after urldecode or make the select look up the row key the way it is submitted with out urldecodeing the key.
"
HADOOP-2494,Set +x on contrib/*/bin/* in packaged tar bundle,"If you download a nightly, the hbase scripts are not executable."
HADOOP-2492,ConcurrentModificationException in org.apache.hadoop.ipc.Server.Responder,"I was running hadoop on 800 machines and after running a couple of jobs, and running 100% of the maps of the current job, the JobTracker stopped responding - *all* tasktrackers were lost ... When I looked at the JT logs, these seemed alarming:
2007-12-26 19:18:30,185 WARN org.apache.hadoop.ipc.Server: Exception in Responder java.util.ConcurrentModificationException
Following the above exception, I saw a whole lot of exceptions like:
2007-12-26 19:23:10,926 WARN org.apache.hadoop.ipc.Server: Call queue overflow discarding oldest call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@5a05f9, false, true, 1758) from 1.2.3.4:1234

From the number of exceptions to do with call queue overflow, it seemed like the jobtracker was not processing RPCs after it got the ConcurrentModificationException, and around that time the tasktrackers started getting timeouts on RPCs...

There were two occurrences of the ConcurrentModificationException but the first instance seemed to not have any effect on the call queue...  "
HADOOP-2487,Provide an option to get job status for all jobs run by or submitted to a job tracker,"This is an RFE for providing an RPC in Hadoop that can expose status information for jobs submitted to a JobTracker. Such a feature can be used for developing tools that can be used to analyse jobs.

It is possible that other information is also useful - such as running times of jobs, etc.

Comments ?"
HADOOP-2486,Dropping records at reducer.  InMemoryFileSystem NPE.,"Note: I'm really not sure if this is a bug in my code or in mapred. 

With my mapreduce job without combiner,  I sometimes see   # of total Map output records != # of total Reduce input records. What's weird to me is, when I rerun my code with exact same input, usually I get an expected #map output recs == #reduce output recs.

Both jobs finish successfully. No failed tasks. No speculative execution. 

I ran separate linecount mapred jobs on both the input and the output to see if  the counters are reporting the correct number. 


When I looked at all the 513 reducer counter, I found single reducer with different counts for the two runs. 
Only error stood out in that  reducer userlog is, 
{noformat} 
2007-12-22 00:19:07,640 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 done copying task_200712220008_0003_m_000288_0 output from qqq856.ppp.com.
2007-12-22 00:19:07,640 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 Copying task_200712220008_0003_m_000327_0 output from qqq887.ppp.com.
2007-12-22 00:19:07,640 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:380)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:423)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:386)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:716)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:637)

2007-12-22 00:19:07,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 done copying task_200712220008_0003_m_000228_0 output from qqq801.ppp.com.
2007-12-22 00:19:07,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 Copying task_200712220008_0003_m_000337_0 output from qqq841.ppp.com.
{noformat} 

Could this error be somehow related to my having different # of records? 
"
HADOOP-2481,NNBench should periodically report its progress,"When I run NNBench on a 100-node cluster, some map tasks fail with the error  message ""Task xx failed to report status for yy seconds. Killing!"". Map tasks should periodically reports its progress to prevent itself being killed."
HADOOP-2477,Unit test fails on Windows: TestCopyFiles.testCopyFromLocalToDfs,"Unit test failed on Windows: org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs

Exception:
org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot open filename /destdat/one/eight/3479184736143758567
	at org.apache.hadoop.dfs.NameNode.open(NameNode.java:234)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:401)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)
	at org.apache.hadoop.ipc.Client.call(Client.java:509)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
	at org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:839)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.<init>(DFSClient.java:831)
	at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:263)
	at org.apache.hadoop.dfs.DistributedFileSystem.open(DistributedFileSystem.java:114)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:262)
	at org.apache.hadoop.fs.TestCopyFiles.checkFiles(TestCopyFiles.java:144)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs(TestCopyFiles.java:288)

Standard Error:
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)
	at org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)
	at org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)
	at org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)
	at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)
	at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:235)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)

Waiting for the Mini HDFS Cluster to start...
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)
	at org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)
	at org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)
	at org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)
	at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)
	at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs(TestCopyFiles.java:283)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)

Waiting for the Mini HDFS Cluster to start..."
HADOOP-2476,Unit test fails on Windows: TestCopyFiles.testCopyFromLocalToLocal,"Unit test fails on Windows: org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal

Exception: 
java.io.FileNotFoundException: C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/data/destdat/three/three/7695082211392925393
	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:144)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:117)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:274)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:262)
	at org.apache.hadoop.fs.TestCopyFiles.checkFiles(TestCopyFiles.java:144)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:238)

Standard Error:
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)
	at org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)
	at org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)
	at org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)
	at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)
	at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:235)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)

Waiting for the Mini HDFS Cluster to start...
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)
	at org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)
	at org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)
	at org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)
	at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)
	at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs(TestCopyFiles.java:283)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)

Waiting for the Mini HDFS Cluster to start...
"
HADOOP-2475,"""dfs -cat"" fails if multiple files are requested","I try to concatenate several files using the command line client:

  $ bin/hadoop dfs -cat file1 file2

The first file works, but then I get the following message:
  cat: Unable to write to output stream.

The problem is that stdout is closed after the first file."
HADOOP-2473,EC2 termination script should support termination by group,"The termination script currently terminates all hadoop instances (after user confirmation). It is therefore not suitable when running multiple, independent clusters on EC2. This change would make it possible to run independent clusters in separate groups and terminate them independently."
HADOOP-2470,Open and isDir should be removed from ClientProtocol,Methods open and isDir in ClientProtocol are no longer used. DFSClient uses getBlockLocations and getFileInfo instead. So open and isDir should be removed from ClientProtocol.
HADOOP-2469,WritableUtils.clone should take Configuration rather than JobConf,"I'd like to use WritableUtils.clone but I'm not in a mapred context so don't have a JobConf to hand.  I do have a Configuration.

 Looking inside the clone implementation, it doesn't need JobConf; a Configuration will do."
HADOOP-2466,"FileInputFormat computeSplitSize() method, change visibility to protected and make it a member method","Currently the computeSplitSize() method is private and static. Because of this, to make a subclass of the FileInputFormat that changes the logic to compute the split size, the whole getSplits() method has to be rewritten.

By making it a member method and protected the logic to compute the split size can be changed and the getSplits() method can be reused.
"
HADOOP-2464,Test permissions related shell commands with DFS,"HADOOP-2336 adds FsShell commands for changing permissions for files. But it is not tested on DFS since that requires HADOOP-1298. Once HADOOP-1298 is committed, we should add unit tests for DFS."
HADOOP-2461,"Configuration should trim property names and accept decimal, hexadecimal, and octal numbers","I suggest two improvements in reading configuration:
- Suppose we have the following property in a conf file.
{code}
<property>
<name>
testing.property</name>
<value>something</value>
</property>
{code}
Try to get it by
{code}
Configuration conf = new Configuration();
String value = conf.get(""testing.property""); //value == null here
{code}
We will get null since there is an eol in 
{code}
<name>
testing.property</name>
{code}
I suggest to trim all property names.

- I also suggest configuration to accept decimal, hexadecimal, and octal numbers (e.g. 011 is 9, 0xA is 10)
It can be easily done by replacing Integer.parseInt(...) with Integer.decode(...) in Configuration.getInt(...), similarly, in Configuration.getLong(...)."
HADOOP-2460,NameNode could delete wrong edits file when there is an error,"
On one of the clusters two namenode directories were specified. The second directory had errors and edits log sync failed. Namenode was supposed to run with the good directory, but it exited with a 'fatal error' message.

Looks like it is caused by a bug in {{processIOError()}} in FSEditsLog.java. It removes the wrong directory since it passes in index into {{errorStreams}} rather than index into {{editStreams}} by mistake.

This probably should go in 0.15.2."
HADOOP-2459,Running 'ant docs tar' includes src/docs/build in the resulting tar file,src/docs/build (which is where forrest puts the built site) should not be included in the tar in this location.  It's already included under the top level docs directory.
HADOOP-2457,Add a 'forrest.home' property for the 'docs' target in build.xml,"I propose we add a *forrest.home* property which is used as the base for the Apache Forrest installation for the _docs_ target, currently it relies on forrest being on the PATH.

$ ant -Dforrest.home=<forrest installation> docs"
HADOOP-2456,German locale makes NameNode web interface crash,"When starting the NameNode with a German locale (or some other that uses "","" as the decimal separator), the web interface (dfshealth.jsp) crashes with a NumberFormatException.

To reproduce:

$ LC_ALL=de_DE bin/start-dfs.sh

Then copy some data to the store so that the percentage of used space is above 0%. Load the web interface in your browser and you'll get a HTTP 500 error message.

The problem is that FsShell gives you a localized number (eg. ""0,7"") which can't be parsed by Double.parseDouble()."
HADOOP-2453,wordcount-simple example gives ParseException with examples configuration file,"The example conf file has <executable-name> in the description which is causing the following exception:
Exception in thread ""main"" java.lang.RuntimeException: org.xml.sax.SAXParseException: The element type ""executable-name"" must be terminated by the matching end-tag ""</executable-name>"".
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:816)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:719)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:679)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:297)
        at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:341)
        at org.apache.hadoop.mapred.JobConf.setInputPath(JobConf.java:231)
        at org.apache.hadoop.mapred.pipes.Submitter.main(Submitter.java:356)
"
HADOOP-2452,Eclipse plug-in build.xml issue,"The build.xml for the Eclipse plug-in has a bad reference to the generated hadoop-core-*.jar and prevents the build from succeeding.
"
HADOOP-2449,Restore the  old NN Bench that was replaced by a MR NN Bench,"The old NN Bench did not use Map Reduce.

It was replaced by a new NN Bench that uses Map reduce.

The old NN Bench is useful and should be restored.
  - useful ofr simulated data niodes which do not work for Map reduce since the job configs need to be persistent.
  - a NN test that is independent of map reduce can be useful as it is one less variable in figuring out bottlenecks."
HADOOP-2447,HDFS should be capable of limiting the total number of inodes in the system,"The HDFS Namenode should be capable of limiting the total number of Inodes (files + directories). The can be done through a config variable, settable in hadoop-site.xml. The default should be no limit."
HADOOP-2446,TestHDFSServerPorts fails.,"This might be because I already have Namenode running on my machine. Its better if the unit tests could tolerate another DFS instance running on the same machine.  Otherwise we might get used to seeing unit test failures and miss the new failures.

I will attach the test output.
"
HADOOP-2442,Unit test failed: org.apache.hadoop.fs.TestLocalFileSystemPermission.testLocalFSsetOwner,"Unit test  failed on Linux:

Test: org.apache.hadoop.fs.TestLocalFileSystemPermission.testLocalFSsetOwner

junit.framework.ComparisonFailure: expected:<users> but was:<>
	at org.apache.hadoop.fs.TestLocalFileSystemPermission.testLocalFSsetOwner(TestLocalFileSystemPermission.java:130)

Standard Output

foo: rw-r--r--
bar: rw-r--r--
"
HADOOP-2437,final map output not evenly distributed across multiple disks,"It seems that the final merge output of map tasks for a particular job does not select the output location in random fashion.

This results in a job with a lot of map tasks eventually running out of taskTrackers asking for more tasks because the disk with most of the map outputs eventually has less disk space than specified by mapred.local.dir.minspacestart.

Maybe the start of round-robin selection of multiple locations should be randomized.

In our case:
110,000 maps, each about 3GB final output, on a 1300 node cluster.
Out of 4 locations and after processing about 79,000 maps, the selection for final map outputs 'file.out' looked like:
location1: 24,000
location2: 25
location3: 55,000
location4: 7

"
HADOOP-2436,Remove CopyFiles (distcp) from hadoop-core,"Currently, distcp is released as part of hadoop, the class file is included in hadoop-core. As I understand it, distcp is really an application that runs on top of map-reduce. When that class is included in hadoop-core, it's harder to use updated versions. The class is found in hadoop-core before the job jar?

Would it be possible to remove this class without causing harm?
"
HADOOP-2435,"HQL tutorial for SELECT, CREATE, INSERT doesnt work on the HQL test db","http://shell.hadoop.co.kr/PHPClient.php  only thing that works is the HELP command

http://wiki.apache.org/lucene-hadoop/Hbase/HbaseShell"
HADOOP-2434,MapFile.get on HDFS in TRUNK is WAY!!! slower than 0.15.x,"Stall happens down in SequenceFile in the first call to getPos inside readRecordLength.  I tried the johano patch from HADOOP-2172 that restores the positional cache but that didn't seem to be the issue here.

Here is data to support my assertion.

I wrote a little program to make a MapFile of 1M records.  I then did 1M random reads from same file.  Below are timings from a 0.15.0 and TRUNK as of this afternoon run.

0.15.x branch:

{code}
[stack@aa0-000-12 branch-0.15]$ ./bin/hadoop org.apache.hadoop.io.TestMapFile
.07/12/15 01:29:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
07/12/15 01:29:02 INFO io.TestMapFile: Writing 1000000 rows to testMapFileRandomAccess
07/12/15 01:32:04 INFO io.TestMapFile: Writing 1000000 records took 182009ms
07/12/15 01:32:04 INFO io.TestMapFile: Reading 1000000 random rows
07/12/15 01:48:02 INFO io.TestMapFile: Reading 1000000 random records took 958243ms
Time: 1,140.652
OK (1 test)
{code}

For the below test using TRUNK r604352, I amended the test so it output a log message every 100k reads:

{code}
[stack@aa0-000-12 hadoop-trunk]$ ./bin/hadoop org.apache.hadoop.io.TestMapFile
.07/12/15 01:56:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
07/12/15 01:56:34 INFO io.TestMapFile: Writing 1000000 rows to testMapFileRandomAccess
07/12/15 01:59:38 INFO io.TestMapFile: Writing 1000000 records took 183986ms
07/12/15 01:59:38 INFO io.TestMapFile: Reading 1000000 random rows
.........
{code}

After 20 minutes it still hadn't printed out the 'read 100k messages' (I had to leave -- will fill in final figures later)"
HADOOP-2432,"If HDFS is going to throw an exception ""File does not exist"" it should include the name of the file","There are three locations in Hadoop where an IOException is thrown with the message ""File does not exist"". Two of them do not include the name of the file."
HADOOP-2431,Test HDFS File Permissions,This jira is intended to provide junit tests to HADOOP-1298.
HADOOP-2427,Cleanup of mapred.local.dir after maptask is complete,"I see that after a map task is complete, its working directory (mapred.local.dir)/taskTracker/jobcache/<jobid>/<task_dir> is not deleted untill the job is complete. If map out files are stored in there, could this be created in different directory and the working directory cleaned up after map task is complete. One problem we are seeing is, if a map task creates files temporary files, they get accumulated and we may run out of disk space thus failing the job. Relying on the user to cleanup all temp files created is be error prone.  "
HADOOP-2425,TextOutputFormat should special case Text,TextOutputFormat is spending a noticeable amount of time encoding and then decoding the bytes in Text objects before they are sent to the output. We should handle this as a special case.
HADOOP-2424,lzop compatible CompressionCodec,"LzoCodec currently outputs at most {{io.compression.codec.lzo.buffersize}} (default 64k)- less the compression overhead- bytes per write (HADOOP-2402) in the following format:

{noformat}
[uncompressed block length(32)]
[compressed block length(32)]
[compressed block]
{noformat}

lzop (lzo-backed command-line utility) writes blocks in the following format:

{noformat}
[uncompressed block length(32)]
[compressed block length (32)]
[Adler-32|CRC-32 checksum of uncompressed block (32)]
[Adler-32|CRC-32 checksum of compressed block (32)]
[compressed block]
{noformat}

There's an additional ~32 byte header to the file. I don't know of a standard, but the lzop source should suffice.

Since we're using "".lzo"" as the default extension, it's worth considering being compatible with lzop, but not necessarily for all lzo-compressed blocks. For example, SequenceFiles should use the existing LzoCodec format."
HADOOP-2423,The codes in FSDirectory.mkdirs(...) is inefficient.,"FSDirectory.mkdirs(...) creates List<String> v to store all dirs.  e.g.

{code}
//Suppose 
src = ""/foo/bar/bas/""
//Then,
v = {""/"", ""/foo"", ""/foo/bar"", ""/foo/bar/bas""}
{code}

For each directory string *cur* in v, no matter *cur* already exists or not, it will try to do a unprotectedMkdir(cur, ...).  Then, *cur* is parsed to byte[][] in INodeDirectory.addNode (...).

We don't need to do the parsing for each string in v.  Instead, byte[][] should be stored.  Also, the loop should not continue once it finds an existing subdirectory."
HADOOP-2422,dfs -cat multiple files fail with 'Unable to write to output stream.',"In hadoop-0.14.3, 

hadoop dfs -cat file1 file2 
hello
koji


in hadoop-0.15.1

hadoop dfs -cat file1 file2 
hello
cat: Unable to write to output stream.


Each file has one line.   file1: 'hello'  file2: 'koji' "
HADOOP-2421,Release JDiff report of changes between different versions of Hadoop,"Similar to LUCENE-1083, it would be useful to report javadoc differences (ala [JDiff|http://www.jdiff.org/]) between Hadoop releases."
HADOOP-2420,Use exit code to detect normal errors while excuting 'ls' in Local FS,"Local FileSystem runs {{ls -ld}} find file permissions, owner and group for a file. Currently it parses message in the exception to check if the command returned an error due to expected conditions like missing file. 

HADOOP-2344 add interface to get error code returned by the external process. Local FS should use that.
"
HADOOP-2419,HADOOP-1965 breaks nutch,"When running nutch on trunk, nutch is unable to complete a fetch and the following exceptions are raised:

java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:180)
        at org.apache.nutch.protocol.Content.readFields(Content.java:158)
        at org.apache.nutch.util.GenericWritableConfigurable.readFields(GenericWritableConfigurable.java:38)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.spill(MapTask.java:536)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpillToDisk(MapTask.java:474)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$100(MapTask.java:248)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$1.run(MapTask.java:413)

Exception in thread ""SortSpillThread"" java.lang.NegativeArraySizeException
     at org.apache.hadoop.io.Text.readString(Text.java:388)
     at org.apache.nutch.metadata.Metadata.readFields(Metadata.java:243)
     at org.apache.nutch.protocol.Content.readFields(Content.java:151)
     at org.apache.nutch.util.GenericWritableConfigurable.readFields(GenericWritableConfigurable.java:38)
     at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.spill(MapTask.java:536)
     at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpillToDisk(MapTask.java:474)
     at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$100(MapTask.java:248)
     at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$1.run(MapTask.java:413)

After reverting HADOOP-1965 nutch works just fine."
HADOOP-2413,Is FSNamesystem.fsNamesystemObject unique?,"FSNamesystem is unique in almost all cases but it is not universally true.  So we should either remove the static variable FSNamesystem.fsNamesystemObject or make it final (so that it cannot be overwritten).

When I am working on HADOOP-1298, I use the convenient static method FSNamesystem.getFSNamesystem() to get ""the"" FSNamesystem object.  However, it keeps failing on TestCheckpoint.  Why?  It is because TestCheckpoint uses NameNode and SecondaryNameNode.  Both of them are creating FSNamesystem.  So FSNamesystem.fsNamesystemObject does not remain constant.  The kind of bug is hard to be detected."
HADOOP-2411,Add support for larger EC2 instance types,"Need to configure Hadoop to exploit the resources available on larger instance types: 64bit, extra CPUs, larger memory. See http://docs.amazonwebservices.com/AWSEC2/2007-08-29/DeveloperGuide/instance-types.html"
HADOOP-2410,Make EC2 cluster nodes more independent of each other,"The cluster start up scripts currently wait for each node to start up before appointing a master (to run the namenode and jobtracker on), and copying private keys to all the nodes, and writing the private IP address of the master to the hadoop-site.xml file (which is then copied to the slaves via rsync). Only once this is all done is hadoop started on the cluster (from the master). This can fail if any of the nodes fails to come up, which can happen as EC2 doesn't guarantee that you get a cluster of the size you ask for (I've seen this happen).

The process would be more robust if each node was told the address of the master as user metadata and then started its own daemons. This is complicated by the fact that the public DNS alias of the master resolves to a public IP address so cannot be used by EC2 nodes (see http://docs.amazonwebservices.com/AWSEC2/2007-08-29/DeveloperGuide/instance-addressing.html). Instead we need to use a trick (http://developer.amazonwebservices.com/connect/message.jspa?messageID=71126#71126) to find the private IP, and what's more we need to attempt to resolve the private IP in a loop until it is available since the DNS will only be set up after the master has started.

This change will also mean the private key doesn't need to be copied to each node, which can be slow and has dubious security. Configuration can be handled using the mechanism described in HADOOP-2409."
HADOOP-2406,Micro-benchmark to measure read/write times through InputFormats,"The attached test writes/reads XGB to/from the default filesystem through SequenceFileInputFormat and TextInputFormat, using LzoCodec, GzipCodec, and without compression, using both block and record compression for SequenceFiles.

The following results using 10GB of data through RawLocalFileSystem with 5 word keys, 20 word values (as generated by RandomTextWriter with the same seed for each file) are pretty stable:

Writes:
|| Format || Compression || Type || Time (sec) || Filesize (bytes) ||
| SEQ | LZO | BLOCK | 318 | 8 604 288 397 |
| SEQ | LZO | RECORD | 367 | 11 689 969 413 |
| SEQ | ZIP | BLOCK | 929 | 2 827 697 769 |
| SEQ | ZIP | RECORD | 1737 | 9 324 730 365 |
| SEQ |  |  | 201 | 11 282 745 683 |
| TXT | LZO |  | 742 | 12 671 065 769 |
| TXT | ZIP |  | 1320 | 2 597 397 680 |
| TXT |  |  | 392 | 10 818 058 643 |

Reads:
|| Format || Compression || Type || Time (sec) ||
| SEQ | LZO | BLOCK | 150 |
| SEQ | LZO | RECORD | 281 |
| SEQ | ZIP | BLOCK | 155 |
| SEQ | ZIP | RECORD | 548 |
| SEQ |  |  | 209 |
| TXT | LZO |  | 620 |
| TXT | ZIP |  | 355 |
| TXT |  |  | 284 |


Of note:
- Lzo compressed TextOutput is larger than the uncompressed output (HADOOP-2402); lzop cannot read it.
- Zip compression is expensive. Short values are responsible for the unimpressive compression for record-compressed SequenceFiles.
- TextInputFormat is slow (HADOOP-2285). TextOutputFormat also looks suspect."
HADOOP-2404,HADOOP-2185 breaks compatibility with hadoop-0.15.0,"HADOOP-2185 removed the following configuration parameters:

{noformat}
dfs.secondary.info.port
dfs.datanode.port
dfs.info.port
mapred.job.tracker.info.port
tasktracker.http.port
{noformat}

and changed the following configuration parameters:
{noformat}
dfs.secondary.info.bindAddress
dfs.datanode.bindAddress
dfs.info.bindAddress
mapred.job.tracker.info.bindAddress
mapred.task.tracker.report.bindAddress
tasktracker.http.bindAddress
{noformat}

without a backward-compatibility story.

Lots are applications/cluster-configurations are prone to fail hence, we need a way to keep things working as-is for 0.16.0 and remove them for 0.17.0."
HADOOP-2403,JobHistory log files contain data that cannot be parsed by org.apache.hadoop.mapred.JobHistory,"When some tasks failed, the job tracker writes an line to the history file with error message.
However, the error message may mess up with the history file format, choking the history parser. Here is an example:

MapAttempt TASK_TYPE=""MAP"" TASKID=""tip_200712102254_0001_m_000090"" TASK_ATTEMPT_ID=""task_200712102254_0001_m_000090_0"" TASK_STATUS=""FAILED"" FINISH_TIME=""1197327293253"" HOSTNAME=""XXXX:50050"" ERROR=""java.lang.IllegalArgumentException: Trouble to get key or value (<,> substituted by null 
. Key XML-Ori:

        <Root>
"
HADOOP-2402,Lzo compression compresses each write from TextOutputFormat,"Outputting with TextOutputFormat and Lzo compression generates a file such that each key, tab delimiter, and value are compressed separately."
HADOOP-2401,Lease holder information should be passed in ClientProtocol.abandonBlock(...),"Logically, only the lease holder can do abandonBlock(...).  However, since the file is visible to the other clients once it has been created.  It is possible for the other client to obtain block information for a file which is being created (i.e. not called complete(String, String) yet).  Then, they can do abandonBlock(...) with the obtained block information.

I suggest to add lease holder information as a parameter in abandonBlock(...). So that we can make sure only the lease holder can do abandonBlock(...)."
HADOOP-2399,Input key and value to combiner and reducer should be reused,"Currently, the input key and value are recreated on every iteration for input to the combiner and reducer. It would speed up the system substantially if we reused the keys and values. The down side of doing it, is that it may break applications that count on holding references to previous keys and values, but I think it is worth doing."
HADOOP-2398,"Additional Instrumentation for NameNode, RPC Layer and JMX support ","Additional Instrumentation is needed for name node and its rpc layer. Furthermore the instrumentation should be
visible via JMX, Java's standard monitoring tool.
"
HADOOP-2393,TaskTracker locks up removing job files within a synchronized method ,"we have some bad jobs where the reduces are getting stalled (for unknown reason). The task tracker kills these processes from time to time.

Everytime one of these events happens - other (healthy) map tasks in the same node are also killed. Looking at the logs and code up to 0.14.3 - it seems like the child tasks pings to the task tracker are timed out and the child task self-terminates.

tasktracker log:

// notice the good 10+ second gap in logs on otherwise busy node:
2007-12-10 09:26:53,047 INFO org.apache.hadoop.mapred.TaskRunner: task_0120_r_000001_47 done; removing files.                                       
2007-12-10 09:27:26,878 INFO org.apache.hadoop.mapred.TaskRunner: task_0120_m_000618_0 done; removing files.                                        
2007-12-10 09:27:26,883 INFO org.apache.hadoop.ipc.Server: Process Thread Dump: Discarding call ping(task_0149_m_000007_0) from 10.16.158.113:43941 
24 active threads                                                                                                                                   

... huge stack trace dump in logfile ...

something was going on at this time which caused to the tasktracker to essentially stall. all the pings are discarded. after stack trace dump:

2007-12-10 09:27:26,883 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 50050, call ping(task_0149_m_000007_0) from 10.16.158.113:43941:\
 discarded for being too old (21380)                                                                                                                
2007-12-10 09:27:26,883 WARN org.apache.hadoop.ipc.Server: IPC Server handler 1 on 50050, call ping(task_0149_m_000002_1) from 10.16.158.113:44183:\
 discarded for being too old (21380)                                                                                                                
2007-12-10 09:27:26,883 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 50050, call ping(task_0149_m_000007_0) from 10.16.158.113:43941:\
 discarded for being too old (10367)                                                                                                                
2007-12-10 09:27:26,883 WARN org.apache.hadoop.ipc.Server: IPC Server handler 1 on 50050, call ping(task_0149_m_000002_1) from 10.16.158.113:44183:\
 discarded for being too old (10360)                                                                                                                
2007-12-10 09:27:26,982 WARN org.apache.hadoop.mapred.TaskRunner: task_0149_m_000002_1 Child Error     

looking at code, failure of client to ping causes termination:

              else {                                                                                                                                
                // send ping                                                                                                                        
                taskFound = umbilical.ping(taskId);                                                                                                 
              }                                                                                                                                     
...
            catch (Throwable t) {                                                                                                                   
              LOG.info(""Communication exception: "" + StringUtils.stringifyException(t));                                                            
              remainingRetries -=1;                                                                                                                 
              if (remainingRetries == 0) {                                                                                                          
                ReflectionUtils.logThreadInfo(LOG, ""Communication exception"", 0);                                                                   
                LOG.warn(""Last retry, killing ""+taskId);                                                                                            
                System.exit(65);                                                                                                                    

exit code is 65 as reported by task tracker.

i don't see an option to turn off stack trace dump (which could be a likely cause) - and i would hate to bump up timeout because of this. Crap.
"
HADOOP-2391,Speculative Execution race condition with output paths,"I am tracking a problem where when speculative execution is enabled, there is a race condition when trying to read output paths from a previously completed job.  More specifically when reduce tasks run their output is put into a working directory under the task name until the task in completed.  The directory name is something like workdir/_taskid.  Upon completion the output get moved into workdir.  Regular tasks are checked for this move and not considered completed until this move is made.  I have not verified it but all indications point to speculative tasks NOT having this same check for completion and more importantly removal when killed.  So what we end up with when trying to read the output of previous tasks with speculative execution enabled is the possibility that previous workdir/_taskid will be present when the output directory is read by a chained job.  Here is an error when supports my theory:

Generator: org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot open filename /u01/hadoop/mapred/temp/generate-temp-1197104928603/_task_200712080949_0005_r_000014_1
        at org.apache.hadoop.dfs.NameNode.open(NameNode.java:234)
        at sun.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:389)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:644)
        at org.apache.hadoop.ipc.Client.call(Client.java:507)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:186)
        at org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:839)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.<init>(DFSClient.java:831)
        at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:263)
        at org.apache.hadoop.dfs.DistributedFileSystem.open(DistributedFileSystem.java:114)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1356)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1349)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1344)
        at org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(SequenceFileOutputFormat.java:87)
        at org.apache.nutch.crawl.Generator.generate(Generator.java:429)
        at org.apache.nutch.crawl.Generator.run(Generator.java:563)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:54)
        at org.apache.nutch.crawl.Generator.main(Generator.java:526)

I will continue to research this and post as I make progress on tracking down this bug."
HADOOP-2390,Document the user-controls for intermediate/output compression via forrest,"We should document the user-controls for compressing the intermediate and job outputs, including the types (record/block) and the various codecs in the hadoop website via forrest (mapred_tutorial.html)."
HADOOP-2382,include hadoop-default.html in subversion,"The hadoop-default.html file is included in releases, but is not included with other documents in subversion.  This makes publishing released documentation more difficult, since this file must be manually copied into place.  Thus I propose that this file be created by the ""docs"" target and stored in subversion with the other documentation."
HADOOP-2381,Support permission information in FileStatus,"In HADOOP-2288,  FileSystem API is changed to support access control.  FileStatus should also be changed to support permission information."
HADOOP-2378,last TaskCompletionEvent gets added to the job after the job is marked as completed,"In the JobInProgress the last TaskCompletionEvent gets added to the job after the completedTask() method is invoked.

The completedTask() method has to be invoked after the completion event is added to the job.

If not when the job is moved into completed before the last completion event is in the job causing a temporary inconsistency in the JT as one task completion event is still missing.

This blocks a fix for HADOOP-1876 where the job info is persisted as soon as the job is completed, thus the last completion event is missing.

"
HADOOP-2376,The sort example shouldn't override the number of maps,"The sort example currently overrides the number of maps. It should just take the default, because the current behavior can end up with a bad number of maps by default. In particular, I had a small data set where the sort picked too many maps and split blocks badly."
HADOOP-2373,Name node silently changes state,"1. The name node should create a log message when entering the ""extension"" period of safe mode after achieving the minimal replication threshold.

Logging state changes is a Good Idea. When diagnosing 2159, there is no evidence whether the name node is in the extension period or not.

2. When in safe mode, the name node should periodically explain _why_ it is still in safe mode. (2159 again)"
HADOOP-2372,Somebody left diagnostic prints in the code,"Setting replication using the hadoop dfs -setrep produces output  like this:

original rep = 2
Replication 5 set: /user/tdunning/foo1/part-00001
16:00
16:00
original rep = 1
setrep: java.net.URISyntaxException: Relative path in absolute URI: 16:00
2>|1690481|1969-12-31
/user/tdunning/foo1/part-00002|2>|1690481|1969-12-31
original rep = 2
Replication 5 set: /user/tdunning/foo1/part-00002
16:00
16:00
original rep = 1
setrep: java.net.URISynt"
HADOOP-2371,Candidate user guide for permissions feature of Hadoop DFS,"This feature introduces a POSIX-like permissions model for DFS in Hadoop 0.16. The user guide describes the behavior users of DFS will experience. In this release, the user identity model is very elementary: you are who your host says you are. It is understood that this is not ""secure"" in any meaningful sense, but rather the permissions model allows a cooperative community to share file system resources in an organized fashion. The user identity model will evolve in the coming months.

As mentioned in the draft, the content of the guide will be incorporated into the regular Hadoop documentation from Forrest. This document is being published as an early preview."
HADOOP-2369,Representative mix of jobs for large cluster throughput benchmarking,The benchmarking load will consist of a set of map/reduce jobs of varying types and sizes. The mix of jobs will emulate observed user loads on large Hadoop clusters.
HADOOP-2368,Lots of unit tests fail on Windows with exception: Login failed: CreateProcess,"A lot (68) unit tests failed on Windows due to an exception: Login failed: CreateProcess

Failing tests

org.apache.hadoop.dfs.TestBalancer.testBalancer0
org.apache.hadoop.dfs.TestBalancer.testBalancer1
org.apache.hadoop.dfs.TestBlockReplacement.testBlockReplacement
org.apache.hadoop.dfs.TestCheckpoint.testCheckpoint
org.apache.hadoop.dfs.TestCrcCorruption.testCrcCorruption
org.apache.hadoop.dfs.TestDFSFinalize.testFinalize
org.apache.hadoop.dfs.TestDFSMkdirs.testDFSMkdirs
org.apache.hadoop.dfs.TestDFSRollback.testRollback
org.apache.hadoop.dfs.TestDFSShell.testZeroSizeFile
org.apache.hadoop.dfs.TestDFSShell.testPut
org.apache.hadoop.dfs.TestDFSShell.testText
org.apache.hadoop.dfs.TestDFSShell.testCopyToLocal
org.apache.hadoop.dfs.TestDFSShell.testDFSShell
org.apache.hadoop.dfs.TestDFSShellGenericOptions.testDFSCommand
org.apache.hadoop.dfs.TestDFSStartupVersions.testVersions
org.apache.hadoop.dfs.TestDFSStorageStateRecovery.testStorageStates
org.apache.hadoop.dfs.TestDFSUpgrade.testUpgrade
org.apache.hadoop.dfs.TestDFSUpgradeFromImage.testUpgradeFromImage
org.apache.hadoop.dfs.TestDataTransferProtocol.testDataTransferProtocol
org.apache.hadoop.dfs.TestDatanodeReport.testDatanodeReport
org.apache.hadoop.dfs.TestDecommission.testDecommission
org.apache.hadoop.dfs.TestDistributedUpgrade.testDistributedUpgrade
org.apache.hadoop.dfs.TestEditLog.testEditLog
org.apache.hadoop.dfs.TestFSInputChecker.testFSInputChecker
org.apache.hadoop.dfs.TestFSOutputSummer.testFSOutputSummer
org.apache.hadoop.dfs.TestFileCorruption.testFileCorruption
org.apache.hadoop.dfs.TestFileCreation.testFileCreation
org.apache.hadoop.dfs.TestFileCreation.testFileCreationError1
org.apache.hadoop.dfs.TestFileCreation.testFileCreationSimulated
org.apache.hadoop.dfs.TestFileStatus.testFileStatus
org.apache.hadoop.dfs.TestFsck.testFsck
org.apache.hadoop.dfs.TestFsck.testFsckNonExistent
org.apache.hadoop.dfs.TestGetBlocks.testGetBlocks
org.apache.hadoop.dfs.TestHDFSServerPorts.testNameNodePorts
org.apache.hadoop.dfs.TestHDFSServerPorts.testDataNodePorts
org.apache.hadoop.dfs.TestHDFSServerPorts.testSecondaryNodePorts
org.apache.hadoop.dfs.TestInjectionForSimulatedStorage.testInjection
org.apache.hadoop.dfs.TestLocalDFS.testWorkingDirectory
org.apache.hadoop.dfs.TestModTime.testModTime
org.apache.hadoop.dfs.TestPread.testPreadDFS
org.apache.hadoop.dfs.TestPread.testPreadDFSSimulated
org.apache.hadoop.dfs.TestReplication.testReplicationSimulatedStorag
org.apache.hadoop.dfs.TestReplication.testReplication
org.apache.hadoop.dfs.TestReplication.testPendingReplicationRetry
org.apache.hadoop.dfs.TestRestartDFS.testRestartDFS
org.apache.hadoop.dfs.TestSeekBug.testSeekBugDFS
org.apache.hadoop.dfs.TestSetrepDecreasing.testSetrepDecreasing
org.apache.hadoop.dfs.TestSetrepIncreasing.testSetrepIncreasing
org.apache.hadoop.dfs.TestSetrepIncreasing.testSetrepIncreasingSimulatedStorage
org.apache.hadoop.dfs.TestSmallBlock.testSmallBlock
org.apache.hadoop.dfs.TestSmallBlock.testSmallBlockSimulatedStorage
org.apache.hadoop.dfs.TestTrash.testTrash
org.apache.hadoop.fs.TestCopyFiles.testCopyFromDfsToDfs
org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs
org.apache.hadoop.fs.TestCopyFiles.testCopyFromDfsToLocal
org.apache.hadoop.fs.TestCopyFiles.testCopyDfsToDfsUpdateOverwrite
org.apache.hadoop.fs.TestGlobPaths.testGlob
org.apache.hadoop.ipc.TestSocketFactory.testSocketFactory
org.apache.hadoop.mapred.TestEmptyJobWithDFS.testEmptyJobWithDFS
org.apache.hadoop.mapred.TestMRServerPorts.testJobTrackerPorts
org.apache.hadoop.mapred.TestMRServerPorts.testTaskTrackerPorts
org.apache.hadoop.mapred.TestMiniMRClasspath.testClassPath
org.apache.hadoop.mapred.TestMiniMRClasspath.testExternalWritable
org.apache.hadoop.mapred.TestMiniMRDFSCaching.testWithDFS
org.apache.hadoop.mapred.TestMiniMRMapRedDebugScript.testMapDebugScript
org.apache.hadoop.mapred.TestMiniMRWithDFS.testWithDFS
org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath.testJobWithDFS
org.apache.hadoop.security.TestUnixUserGroupInformation.testLogin"
HADOOP-2367,Get representative hprof information from tasks,It would be great to get a representative (2 or 3) sample of builtin java profiler for a sample of maps and reduces. I'd store the information in the userlog directory and make it available in the local file system of the submitting application.
HADOOP-2366,Space in the value for dfs.data.dir can cause great problems,"The following configuration causes problems:

<property>
  <name>dfs.data.dir</name>
  <value>/mnt/hstore2/hdfs, /home/foo/dfs</value>  
  <description>
  Determines where on the local filesystem an DFS data node  should store its bl
ocks.  If this is a comma-delimited  list of directories, then data will be stor
ed in all named  directories, typically on different devices.  Directories that 
do not exist are ignored.  
  </description>
</property>

The problem is that the space after the comma causes the second directory for storage to be "" /home/foo/dfs"" which is in a directory named <SPACE> which contains a sub-dir named ""home"" in the hadoop datanodes default directory.  This will typically cause the user's home partition to fill, but will be very hard for the user to understand since a directory with a whitespace name is hard to understand.

My proposed solution would be to trimLeft all path names from this and similar property after splitting on comma.  This still allows spaces in file and directory names but avoids this problem. "
HADOOP-2365,Result of HashFunction.hash() contains all identical values,"There is a small bug in HashFunction:112 - initvalue should be changed between the loop iterations in order to spread the hash values over the whole allowed range. Instead the current code uses a fixed initvalue = 0, which gives all identical hash values in the result array. As a result, BloomFilter-s have extremely high rate of false positives."
HADOOP-2363,Unit tests fail if there is another instance of Hadoop,"If you are running another Hadoop cluster or DFS, many unit tests fail because Namenode in MiniDFSCluster fails to bind to the right port. Most likely HADOOP-2185 forgot to set right defaults for MiniDFSCluster."
HADOOP-2361,hadoop version wrong in 0.15.1,"I downloaded 0.15.1 release, recompiled and executed ./bin/hadoop version. It says 0.15.2-dev picking it from build.xml"
HADOOP-2360,hadoop::RecordReader::read() throws exception in HadoopPipes::RecordWriter,"The jute record is in format:

  class SampleValue 
  {
       ustring data;
  }

And in HadoopPipes::RecordWriter::emit(), has code like this:

void SampleRecordWriterC::emit(const std::string& key, const std::string& value)
{
    if (key.empty() || value.empty()) {
        return;
    }

    hadoop::StringInStream key_in_stream(const_cast<std::string&>(key));
    hadoop::RecordReader key_record_reader(key_in_stream, hadoop::kCSV);
    EmitKeyT emit_key;
    key_record_reader.read(emit_key);

    hadoop::StringInStream value_in_stream(const_cast<std::string&>(value));
    hadoop::RecordReader value_record_reader(value_in_stream, hadoop::kCSV);
    EmitValueT emit_value;

    value_record_reader.read(emit_value);

    return;
}

And the code throw hadoop::IOException at the read() line.


In the mapper, I have faked record emitted by the following code:

std::string value;
EmitValueT emit_value;

emit_value.getData().assign(""FakeData"");

hadoop::StringOutStream value_out_stream(value);
hadoop::RecordWriter value_record_writer(value_out_stream, hadoop::kCSV);
value_record_writer.write(emit_value);

We haven't update to the up-to-date version of hadoop. But I've searched the tickets and didn't find one issuing this problem."
HADOOP-2359,PendingReplicationMonitor thread received exception. java.lang.InterruptedException,"I sometimes get the message:

07/12/05 19:01:36 WARN fs.FSNamesystem: PendingReplicationMonitor thread received exception. java.lang.InterruptedException: sleep interrupted

from mini-dfs cluster.

InterruptedExceptions should be handled quietly."
HADOOP-2358,Unit test failed on Windows: org.apache.hadoop.security.TestUnixUserGroupInformation.testLogin,"Output from failed test:

org.apache.hadoop.security.TestUnixUserGroupInformation.testLogin

javax.security.auth.login.LoginException: Login failed: CreateProcess: groups error=2
	at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:247)
	at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:265)
	at org.apache.hadoop.security.TestUnixUserGroupInformation.testLogin(TestUnixUserGroupInformation.java:41)
"
HADOOP-2352,Remove AC_LIB_CHECK from src/native/configure.ac to ensure libhadoop.so doesn't have a dependency on libz.so/liblzo.so,"We should AC_LIB_CHECK from src/native/configure.ac to ensure libhadoop.so doesn't have a dependency on libz.so/liblzo.so, the check for libz/liblzo headers via AC_HEADERS_CHECK is a stronger guarantee anyway."
HADOOP-2349,"FSEditLog.logEdit(byte op, Writable w1, Writable w2) should accept variable numbers of Writable, instead of two.","The new declaration should be
{code}
FSEditLog.logEdit(byte op, Writable ... w)
{code}
All Writable parameters should not be null."
HADOOP-2346,DataNode should have timeout on socket writes.,"If a client opens a file and stops reading in the middle, DataNode thread writing the data could be stuck forever. For DataNode sockets we set read timeout but not write timeout. I think we should add a write(data, timeout) method in IOUtils that assumes it the underlying FileChannel is non-blocking.
"
HADOOP-2345,new transactions to support HDFS Appends,"This JIRA adresses the changes needed to the transaction mechanism to support appending data to existing HDFS files. The details of this design is documented in HADOOP-1700.
"
HADOOP-2344,Free up the buffers (input and error) while executing a shell command before waiting for it to finish.,Process.waitFor() should be invoked after freeing up the input and error stream.  While fixing https://issues.apache.org/jira/browse/HADOOP-2231 we found that this might be a possible cause.
HADOOP-2342,create a micro-benchmark for measure local-file versus hdfs read,We should have a benchmark that measures reading a 10g file from hdfs and from local disk.
HADOOP-2341,Datanode active connections never returns to 0,"On trunk i continue to see the following in my data node logs:

2007-12-03 15:46:47,696 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 42
2007-12-03 15:46:48,135 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 41
2007-12-03 15:46:48,439 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 40
2007-12-03 15:46:48,479 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 39
2007-12-03 15:46:48,611 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 38
2007-12-03 15:46:48,898 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 37
2007-12-03 15:46:48,989 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 36
2007-12-03 15:46:51,010 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 35
2007-12-03 15:46:51,758 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 34
2007-12-03 15:46:52,148 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 33

This number never returns to 0, even after many hours of no new data being manipulated or added into the DFS.

Looking at netstat -tn i see significant amount of data in the send-q that never goes away:

tcp        0  34240 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:55792   ESTABLISHED 
tcp        0  38968 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:38169   ESTABLISHED 
tcp        0  38456 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:35456   ESTABLISHED 
tcp        0  29640 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:59845   ESTABLISHED 
tcp        0  50168 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:44584   ESTABLISHED 

When sniffing the network I see that the remote side (YY.YY.YY.YY) is returning a window size of 0
16:11:41.760474 IP XX.XX.XX.XXX.50010 > YY.YY.YY.YY.44584: . ack 3339984123 win 46 <nop,nop,timestamp 1786247180 885681789>
16:11:41.761597 IP YY.YY.YY.YY.44584 > XX.XX.XX.XXX.50010: . ack 1 win 0 <nop,nop,timestamp 885801786 1775711351>

Then we look at the stack traces on each datanode, I will have tons of threads that *never* go away in the following trace:
{code}
Thread 6516 (org.apache.hadoop.dfs.DataNode$DataXceiver@166068b6):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    java.net.SocketOutputStream.socketWrite0(Native Method)
    java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
    java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
    java.io.DataOutputStream.write(DataOutputStream.java:90)
    org.apache.hadoop.dfs.DataNode$BlockSender.sendChunk(DataNode.java:1400)
    org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1433)
    org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:904)
    org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:849)
    java.lang.Thread.run(Thread.java:619)
{code}

Unfortunately there's very little in the logs with exceptions that could point to this.  I have some exceptions the following, but nothing that points to problems between XX and YY:
{code}
2007-12-02 11:19:47,889 WARN  dfs.DataNode - Unexpected error trying to delete block blk_4515246476002110310. Block not found in blockMap. 
2007-12-02 11:19:47,922 WARN  dfs.DataNode - java.io.IOException: Error in deleting blocks.
        at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:750)
        at org.apache.hadoop.dfs.DataNode.processCommand(DataNode.java:675)
        at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:569)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1720)
        at java.lang.Thread.run(Thread.java:619)
{code}"
HADOOP-2337,Trash never closes FileSystem,"Trash opens FileSystem using Path.getFileSystem() but never closes it.
This happens even if Trash is disabled (trash.interval == 0). 
I think trash should not open file system if it is disabled.
I also think that NameNode should not create a trash Thread when trash is disabled, see NameNode.init().
"
HADOOP-2336,Shell commands to access and modify file permissions,"
Hadoop 0.16 includes file permissions in DFS and we need FsShell to support common file permissions related commands :
- chown
- chgrp
- chmod

Also output from some of the commands like {{ls -l}} will change to reflect new file properties. Aim is to make the above commands look like its Unix/Linux couterparts. They will of course support only the subset of the options."
HADOOP-2330,Preallocate transaction log to improve namenode transaction logging performance,"In the current implementation, the transaction log is opened in ""append"" mode and every new transaction is written to the end of the log. This means that new blocks get allocated to the edits file frequently.

It is worth measuring the performance improvement when big chunks of the transaction log are allocated up front. Adding new transactions do not cause frequent block allocations for the edits log.
"
HADOOP-2325,Require Java 6,"We should require Java 6 for release 0.17.  Java 6 is now available for OS/X.  Hadoop performs much better on Java 6.  And, finally, there are features of Java 6 (like 'df') that would be nice to use."
HADOOP-2323,JobTracker.close() prints stack traces for exceptions that are not errors,"JobTracker.close() prints a stack trace for an interrupted exception even though it was the method that interrupted the thread that threw the exception. For example:

{code}
      this.expireTrackers.stopTracker();
      try {
        this.expireTrackersThread.interrupt();
        this.expireTrackersThread.join();
      } catch (InterruptedException ex) {
        ex.printStackTrace();
      }
{code}

Well of course it is going to catch an InterruptedException after it just interrupted the thread!

This is *not* an error and should  *not* be dumped to the logs!

In other circumstances, catching InterruptedException is entirely appropriate. Just not in close where you've told the thread to shutdown and then interrupted it to ensure it does!"
HADOOP-2319,Build both 32 and 64 bit native libraries when compiling with a 64 bit JVM,"When a 32 bit JVM is used to build Hadoop, the 32 bit  native libraries are build (providing -Dcompile.native=true is present).  Likewise, a 64 bit JVM automatically builds a 64 bit native library.

It would be helpful if a 64 bit JVM built both 32 and 64 bit native libraries."
HADOOP-2318,All C++ builds should use the autoconf tools,"Currently we have -Dcompile.native and -Dcompile.c++ build flags.  In addition, builds for pipes and libhadoop use autoconf tools, but libhdfs does not, nor does 64bit libhdfs compile work.

All these builds should use autoconf tools, support 64bit compilation, and should occur when a single flag is present (-Dcompile.c++ seems like the better choice).

"
HADOOP-2314,TestBlockReplacement occasionally get into an infinite loop,"It turns out that in the case that tests an invalid deletion hint, either the newNode or source may be choosen to be deleted as an exessive replica since both of the nodes are on the same rack. The test assumes that only newNode will be deleted and wait for its deletion. This causes an infinite loop when source is chosen to be deleted."
HADOOP-2313,build does not fail when libhdfs build fails,"compile-libhdfs:
    [mkdir] Created dir: /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs
     [exec] gcc -g -Wall -O2 -fPIC -m32 -I/home/hadoopqa/tools/java/jdk1.5.0_11-64bit/include -I/home/hadoopqa/tools/java/jdk1.5.0_11-64bit/include/linux -c hdfs.c -o /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/hdfs.o
     [exec] gcc -g -Wall -O2 -fPIC -m32 -I/home/hadoopqa/tools/java/jdk1.5.0_11-64bit/include -I/home/hadoopqa/tools/java/jdk1.5.0_11-64bit/include/linux -c hdfsJniHelper.c -o /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/hdfsJniHelper.o
     [exec] gcc -L/home/hadoopqa/tools/java/jdk1.5.0_11-64bit/jre/lib/amd64/server -ljvm -shared -m32 -Wl,-x  -o /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/libhdfs.so.1 -Wl,-soname,libhdfs.so /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/hdfs.o /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/hdfsJniHelper.o \
     [exec] && ln -sf /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/libhdfs.so.1 /home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/libhdfs.so
     [exec] /usr/bin/ld: skipping incompatible /home/hadoopqa/tools/java/jdk1.5.0_11-64bit/jre/lib/amd64/server/libjvm.so when searching for -ljvm
     [exec] /usr/bin/ld: cannot find -ljvm
     [exec] collect2: ld returned 1 exit status
     [exec] make: *** [/home/hadoopqa/workspace/Hadoop-LinuxTest-0.15/branch/build/libhdfs/libhdfs.so.1] Error 1
     [exec] Result: 2
...
BUILD SUCCESSFUL"
HADOOP-2302, Streaming should provide an option for numerical sort of keys,It would be good to have an option for numerical sort of keys for streaming. 
HADOOP-2300,mapred.tasktracker.tasks.maximum is completely ignored,"HADOOP-1274 replaced the configuration attribute mapred.tasktracker.tasks.maximum with mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum and claims to use the deprecated mapred.tasktracker.tasks.maximum. However, because the new attributes are in hadoop-default.xml, the check to use the deprecated value will never trigger."
HADOOP-2298,ant target without source and docs ,Can we have an ant target or a -D option to build the hadoop tar without the source and documentation? This brings down the tar size from 11.5 MB to 5.6 MB. This would speed up distribution. 
HADOOP-2294,"In hdfs.h , the comment says you release the result of a hdfsListDirectory with a freehdfsFileInfo, but should say hdfsFreeFileInfo","This is ""only"" a documentation issue, but it _is_ in an API header file...

-dk
"
HADOOP-2292,HBaseAdmin.disableTable/enableTable aren't synchronous,"I'm trying to programmatically add a column family to a table.

I have code that looks like:

<code>
admin.disableTable(table);
try {
  admin.addColumn(table, new HColumnDescriptor(columnName));
} finally {
  admin.enableTable(table);
}

HTable ht = new HTable(config, table);
<code>

Two things sometimes go wrong here:

1. addColumn fails because the table is not disabled
2. new HTable() fails because the table is not enabled

I suspect that the enableTable/disableTable calls are not synchronous, ie. they return before they are finished.  I can work around this problem by inserting Thread.sleeps after the enableTable and disableTable calls.
"
HADOOP-2288,Change FileSystem API to support access control.,"- Some FileSystem methods like create and mkdir need an additional parameter for permission.

- FileSystem has to provide methods for setting permission, changing ownership, etc."
HADOOP-2285,TextInputFormat is slow compared to reading files.,"The LineRecordReader reads from the source byte by byte, which seems to be half as fast as if the readLine method was defined on the memory buffer directly instead of as an InputStream."
HADOOP-2284,BasicTypeSorterBase.compare calls progress on each compare,"The inner loop of the sort is calling progress on each compare. I think it would make more sense to call progress in the sort rather than the compare or at most every 10000 compares. In the performance numbers, the call to progress as part of the sort are consuming 12% of the total cpu time when running word count under the local runner."
HADOOP-2275,Erroneous detection of corrupted file when namenode fails to allocate any datanodes for newly allocated block,"It can so happen that the namenode allocated a block for a file and then fails to allocate any datanode for this block. The namenode delivers an exception to the client. The client retries. But the block remains associated with the file (until lease expiration). This causes all client retries to fail.

An fsck (before the lease expires) reports this block as a missing block.
"
HADOOP-2272,findbugs currently fails due to hadoop-streaming having moved,"If you do a fresh checkout of the trunk and try run findbugs you get the following error:

findbugs:
 [findbugs] Running FindBugs...
 [findbugs] Exception in thread ""main"" java.util.zip.ZipException: Error opening /tmp/x/trunk/build/hadoop-0.16.0-dev-streaming.jar
 [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ZipFileCodeBase.<init>(ZipFileCodeBase.java:61)
 [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ClassFactory.createFilesystemCodeBase(ClassFactory.java:96)
 [findbugs]     at edu.umd.cs.findbugs.classfile.impl.FilesystemCodeBaseLocator.openCodeBase(FilesystemCodeBaseLocator.java:63)
 [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ClassPathBuilder.processWorkList(ClassPathBuilder.java:381)
 [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ClassPathBuilder.build(ClassPathBuilder.java:192)
 [findbugs]     at edu.umd.cs.findbugs.FindBugs2.buildClassPath(FindBugs2.java:432)
 [findbugs]     at edu.umd.cs.findbugs.FindBugs2.execute(FindBugs2.java:160)
 [findbugs]     at edu.umd.cs.findbugs.FindBugs.runMain(FindBugs.java:1521)
 [findbugs]     at edu.umd.cs.findbugs.FindBugs2.main(FindBugs2.java:731)
 [findbugs] Output saved to /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.xml
     [xslt] Processing /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.xml to /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.html
     [xslt] Loading stylesheet /opt/java/findbugs/src/xsl/default.xsl
     [xslt] : Error! Premature end of file.
     [xslt] : Error! com.sun.org.apache.xml.internal.utils.WrappedRuntimeException: Premature end of file.
     [xslt] Failed to process /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.xml

BUILD FAILED
/tmp/x/trunk/build.xml:599: javax.xml.transform.TransformerException: javax.xml.transform.TransformerException: com.sun.org.apache.xml.internal.utils.WrappedRuntimeException: Premature end of file.

----

This is because 

build/hadoop-0.16.0-dev-streaming.jar 

is the wrong location, it should be 

build/contrib/streaming/hadoop-0.16.0-dev-streaming.jar 

I think this also explains why Hudson is currently giving a -1 to all new patches as findbugs is failing."
HADOOP-2271,chmod in ant package target fails,"If you checkout the trunk to a folder long than /tmp (in my case it is ""/home/adrian/workspace/hadoop-trunk"") and you execute the command:

ant clean package

You get the following error:

BUILD FAILED
/home/adrian/workspace/hadoop-trunk/build.xml:730: Execute failed: java.io.IOException: Cannot run program ""chmod"": java.io.IOException: error=7, Argument list too long

I also tried this from the folder ""/tmp/trunk"" and the package target worked fine so I imagine the argument list is becoming too long due to the folder being longer. A simple fix for this should be to set the ""parallel"" attribute on chmod to false."
HADOOP-2269,Complier warnings in JobControlTestUtils,"Introduced by HADOOP-2245, I'm not sure why the patch process didn't catch this and -1 it: http://issues.apache.org/jira/browse/HADOOP-2245#action_12544411

{noformat}
compile-core-test:
    [javac] Compiling 5 source files to trunk/build/test/classes
    [javac] Compiling 3 source files to trunk/build/test/classes
    [javac] trunk/src/test/org/apache/hadoop/mapred/jobcontrol/JobControlTestUtils.java:143: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]       output.collect(new Text(key.toString()), value);
    [javac]                     ^
    [javac] trunk/src/test/org/apache/hadoop/mapred/jobcontrol/JobControlTestUtils.java:151: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector
    [javac]         output.collect(dumbKey, data);
    [javac]                       ^
    [javac] 2 warnings
{noformat}"
HADOOP-2268,JobControl classes should use interfaces rather than implemenations,"See HADOOP-2202 for background on this issue. Arun C. Murthy agrees that when possible it is preferable to program against the interface rather than a concrete implementation (more flexible, allows for changes of the implementation in future etc.) JobControl currently exposes running, waiting, ready, successful and dependent jobs as ArrayList rather than List. I propose to change this to List.

I will code up a patch for this."
HADOOP-2258,Compilation warning in mapred.jobcontrol.Job.addDependingJob(),"The warning was introduced by HADOOP-2086. It says: 
{code}
[javac] java\org\apache\hadoop\mapred\jobcontrol\Job.java:221: warning: [unchecked] unchecked call to add(E) 
as a member of the raw type java.util.ArrayList
[javac]       return this.dependingJobs.add(dependingJob);
{code}
""Type safety: The method add(Object) belongs to the raw type ArrayList. References to generic type ArrayList<E> should be parameterized.""

I wonder why Hudson missed that warning?"
HADOOP-2256,TestBlockReplacement unit test failed.,"
I will attach the log. Part of the log :

{noformat}
junit] 2007-11-20 18:44:13,559 WARN  dfs.DataNode (DataNode.java:copyBlock(1128)) - Got exception 
while serving blk_-8824434176426942280 to 127.0.0.1:50011: java.io.IOException: 
Block blk_-8824434176426942280 is not valid.
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:549)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getMetaFile(FSDataset.java:466)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getMetaDataInputStream(FSDataset.java:480)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockSender.<init>(DataNode.java:1282)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.copyBlock(DataNode.java:1098)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:861)
    [junit] 	at java.lang.Thread.run(Thread.java:595)
{noformat}

Summery might look like same as HADOOP-2200 but symptoms in log are different and I think the reason is different.
"
HADOOP-2254,MiniMR tests timeout as a result of HADOOP-1281,"The following tests timeout with the patch and do not if I reverse it:
TestMiniMRClasspath
TestMiniMRLocalFS.unknown
TestMiniMRWithDFS.unknown
TestSpecialCharactersInOutputPath

I don't know, but this was reported by Hudson, and yet the patch was committed, why?
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1119/testReport/
"
HADOOP-2252,Four mapred unit tests failing on Windows,"These 4 mapred unit tests are failing on Windows:

org.apache.hadoop.mapred.TestMiniMRClasspath.unknown
org.apache.hadoop.mapred.TestMiniMRLocalFS.unknown
org.apache.hadoop.mapred.TestMiniMRWithDFS.unknown
org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath.unknown

All of them timed out:
junit.framework.AssertionFailedError: Timeout occurred"
HADOOP-2248,Word count example is spending 24% of the time in incrCounter,"When running under the local runner with the local file system, incrementing the user counters is consuming 24% of the cpu."
HADOOP-2247,Mappers fail easily due to repeated failures,"Related to HADOOP-2220, problem introduced in HADOOP-1158

At this scale hardcoding the number of fetch failures to a static number: in this case 3 is never going to work. Although the jobs we are running are loading the systems 3 failures can randomly occur within the lifetime of a map. Even fetching the data can cause enough load for so many failures to occur.

We believe that number of tasks and size of cluster should be taken into account. Based on which we believe that a ratio between total fetch attempts and total failed attempts should be taken into consideration.

Given our experience with a task should be declared ""Too many fetch failures"" based on:

failures > n /*could be 3*/ && (failures/total attempts) > k% /*could be 30-40%*/

Basically the first factor is to give some headstart to the second factor, second factor then takes into account the cluster size and the task size.

Additionally we could take recency into account, say failures and attempts in last one hour. We do not want to make it too small.

"
HADOOP-2246,"In CHANGES.txt, move HADOOP-1851 & HADOOP-1231 to INCOMPATIBLE CHANGES section","HADOOP-1851 redefines the way one would control compression for the intermediate and the final outputs of a job. HADOOP-1231 adds Generics to the framework and user code should be aware of that (at least for code that requires compilation with 0.15). In CHANGES.txt these two appear in NEW FEATURES and IMPROVEMENTS sections respectively. Ideally, they should be part of the INCOMPATIBLE CHANGES section."
HADOOP-2245,TestRecordMR and TestAggregates fail once in a while,"The tests org.apache.hadoop.mapred.lib.aggregate.TestAggregates.testAggregates and org.apache.hadoop.record.TestRecordMR.testMapred fail intermittently with the problem ""<path>/map_0000/file.out already exists"".  There is a patch on HADOOP-1642 that should address the issue.
"
HADOOP-2244,MapWritable.readFields needs to clear internal hash else instance accumulates entries forever,"A common framework pattern is to get an instance of a Writable, usually by reflection, and then just keep calling readFields to make new 'instances' of the particular Writable.

For example, the spill-to-disk that is run at the end of a map task gets instances of map output keys and values and then loops over the (sorted) map output calling readFields to make instances to write out to the filesystem (See around line #470 in the spill method).

If the particular Writable is an instance of MapWritable, currently we get funny results.  It has an internal hash map that is created on instantiation.  Each time the readFields method is called, the newly deserialized entries are added to the internal map.  The map needs to be reset when readFields is called so it doesn't just keep growing ad infinitum."
HADOOP-2239,Security:  Need to be able to encrypt Hadoop socket connections,"We need to be able to use hadoop over hostile networks, both internally and externally to the enterpise.  While authentication prevents unauthorized access, encryption should be used to prevent such things as packet snooping across the wire.  This means that hadoop client connections, distcp, etc, would use something such as SSL to protect the TCP/IP packets.  Post-Kerberos, it would be useful to use something similar to NFS's krb5p option."
HADOOP-2238,TaskGraphServlet does not set Content-Type,"While the taskgraph shows up properly in Firefox, it does not in Safari 3.0.  It instead downloads over and over a file every time the jobdetail page reloads."
HADOOP-2233,General example for modeling m/r load in Java,"This matches the hadoop.sort.(map|reduce).keep.percent interface in HADOOP-2127 and includes an ""indirect"" sort matching some user apps (when locality information is unavailable). It mostly, merely merges parts of the RandomWriter and Sort examples to effect sample loads in map/reduce."
HADOOP-2232,Add option to disable nagles algorithm in the IPC Server,"While investigating hbase performance, I found a bottleneck caused by
Nagles algorithm. For some reads I would get a bi-modal distribution
of read times, with about half the times being around 20ms, and half
around 200ms. I tracked this down to the well-known interaction between
Nagle's algorithm and TCP delayed acknowledgments. 

I found that calling setTcpNoDelay(true) on the server's socket
connection dropped all of my read times back to a constant 20 ms.

I propose a patch to have this TCP_NODELAY option be configurable. The
attacked patch allows one to set the TCP_NODELAY option on both the
client and the server side. Currently this is defaulted to false
(i.e., with Nagle's enabled).

To see the effect, I have included a Test which provokes the issue by
sending a MapWriteable over an IPC call. On my machine this test shows
a speedup of 117 times when using TCP_NODELAY.

These tests were done on OSX 10.4. Your milage may very with other
TCP/IP implementation stacks."
HADOOP-2231,"ShellCommand, in particular 'df -k', sometimes hang","We noticed that some pipes applications writing to dfs using libhdfs have about 6% chance of hanging when executing 'df -k' to find out whether there is enough space available on the local filesystem before opening a file for write.

Why not using File.getFreeSpace() or File.GetUsableSpace()?

The call stack is:
Exception in thread ""main"" java.io.IOException
         at org.apache.hadoop.fs.ShellCommand.runCommand
(ShellCommand.java:52)
         at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
         at org.apache.hadoop.fs.DF.getAvailable(DF.java:72)
         at org.apache.hadoop.fs.LocalDirAllocator
$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:264)
         at org.apache.hadoop.fs.LocalDirAllocator
$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:294)
         at
org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite
(LocalDirAllocator.java:155)
         at org.apache.hadoop.dfs.DFSClient
$DFSOutputStream.newBackupFile(DFSClient.java:1470)
         at org.apache.hadoop.dfs.DFSClient
$DFSOutputStream.openBackupStream(DFSClient.java:1437)
         at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk
(DFSClient.java:1579)
         at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk
(FSOutputSummer.java:140)
         at org.apache.hadoop.fs.FSOutputSummer.write1
(FSOutputSummer.java:100)
         at org.apache.hadoop.fs.FSOutputSummer.write
(FSOutputSummer.java:86)
         at org.apache.hadoop.fs.FSDataOutputStream
$PositionCache.write(FSDataOutputStream.java:39)
         at java.io.DataOutputStream.write(DataOutputStream.java:90)
         at java.io.FilterOutputStream.write(FilterOutputStream.java:80)

"
HADOOP-2229,Provide a simple login implementation,Give a simple implementation of HADOOP-1701.    Hadoop clients are assumed to be started within a Unix-like network which provides user and group management.  This implementation read user information from the OS and send them to the NameNode in plaintexts through RPC (see also HADOOP-2184).  NameNode trusts all information given and uses them for permission checking.
HADOOP-2228,Jobs fail because job.xml exists,"org.apache.hadoop.ipc.RemoteException: java.io.IOException: Target /var/storage/4/mapred/local/jobTracker/job_200711081903_3976.xml already exists
        at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:271)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:117)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:803)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:784)
        at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:134)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1479)
        at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)

        at org.apache.hadoop.ipc.Client.call(Client.java:470)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
        at $Proxy1.submitJob(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy1.submitJob(Unknown Source)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:397)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:345)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:250)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:282)
        at java.lang.Thread.run(Thread.java:619)

Perhaps related to HADOOP-1057, HADOOP-891 or to the rpc retry. It seems my job was submitted and actually finished despite the exception. Could it be that the job went in and the rpc retry decided to submit it again anyway?"
HADOOP-2227,wrong usage of mapred.local.dir.minspacestart ,"As I understand it, mapred.local.dir.minspacestart should be used to specify that a map or reduce task requires a minimum disk space to be executed. When several disks are available, and mapred.local.dir lists all of them, then the free space should be added up. But TaskTracker.enoughFreeSpace returns false whenever one of the disks listed in mapred.local.dir has less than the space specified by mapred.local.dir.minspacestart. 
"
HADOOP-2226,jobtasks.jsp wrongly classifies Running/Completed tasks,jobtasks.jsp wrongly classifies _Running_ TIPs as _Completed_ and vice-versa. A bug introduced by HADOOP-1839.
HADOOP-2220,Reduce tasks fail too easily because of repeated fetch failures,"Currently reduce tasks with more than MAX_FAILED_UNIQUE_FETCHES (= 5 hard-coded) failures to fetch output from different mappers will fail (I believe, introduced in HADOOP-1158)

This gives us some problems with longer running jobs with a large number of mappers in multiple waves:
Otherwise problem-less reduce tasks fail because of too many fetch failures due to resource contention, and new reduce tasks have to fetch all data from the already successfully executed mappers, introducing a lot of additional IO overhead. Also, the job will fail when the same reducer exhausts the maximum number of attempts.

The limit should be a function of number of mappers and/or waves of mappers, and should be more conservative (e.g. no need to let them fail when there are enough slots to start speculatively executed reducers and speculative execution is enabled). Also, we might consider to not count such a restart against the number of attempts."
HADOOP-2219,du like command to count number of files under a given directory,"To keep the total number of files on dfs low, we like the users to be able to easily find out how many files each of their directory contain.   

Currently, we only have fsck or dfs -lsr which takes time.

Can I ask for an option for du to show the total number of files (as well as the total size) of a given directory?
"
HADOOP-2218,Generalize StatusHttpServer so webdav server can use it,"I'd like to make HADOOP-496 stand alone, so that I can make a hadoop-webdav jar that works against stock hadoop.  The latest HADOOP-496 patch has only a small patch against StatusHttpServer, which generalizes it a little bit to make some private methods protected and changes HttpServlet to Servlet -- the rest is new files.  I'd like to get the part against StatusHttpServer committed."
HADOOP-2216,Job UI doesnot show running tasks and complete tasks correctly.,"In jobdetails.jsp, running tasks link shows some 100% complete tasks; complete tasks link shows some tasks which are still running (some which are not even 50% complete). "
HADOOP-2215,Change documentation in cluster_setup.html and mapred_tutorial.html post HADOOP-1274,Documentation has to be changed in cluster_setup.html and mapred_tutorial.html to reflect usage of mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum instead of mapred.tasktracker.tasks.maximum.
HADOOP-2213,Job submission gets Job tracker still initializing message while Namenode is in safemode,"While namenode is in safemode, if a user submits a job they receive 'Job tracker still initializing' exception. It would be good, if an appropriate error message is thrown.

Job started: Thu Nov 15 23:15:39 UTC 2007
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.mapred.JobTracker$IllegalStateException: Job tracker still initializing
        at org.apache.hadoop.mapred.JobTracker.ensureRunning(JobTracker.java:1505)
        at org.apache.hadoop.mapred.JobTracker.getNewJobId(JobTracker.java:1513)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)

        at org.apache.hadoop.ipc.Client.call(Client.java:482)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at $Proxy1.getNewJobId(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy1.getNewJobId(Unknown Source)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:452)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:753)
        at org.apache.hadoop.examples.RandomWriter.run(RandomWriter.java:274)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.examples.RandomWriter.main(RandomWriter.java:285)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:49)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)

"
HADOOP-2212,java.lang.ArithmeticException: / by zero in ChecksumFileSystem.open,"The ChecksumFileSystem uses a default bytesPerChecksum value of zero.  This number appears as a divisor in ChecksumFileSystem.getSumBufferSize, if it is not overriden in config."
HADOOP-2211,"The default task timeout for streaming should be large, but finite","Currently the default task timeout for Streaming is infinite, but that leads to corner cases where a job becomes wedged. I think it would be much better to a large timeout of 1 hour or so. Thoughts?"
HADOOP-2210,WebUI should also list current time,It would be good if WebUI also listed current time (on all pages). 
HADOOP-2209,SecondaryNamenode process should exit if it encounters Runtime exceptions,"I saw a case when the SecondaryNamenode encountered a OutOfMemory exception because the fsimage and edits file were too big to fit into the Secondary Namenode's memory. However, the process did not exit.

Exception in thread ""org.apache.hadoop.dfs.SecondaryNameNode@73eb904d"" java.lang.OutOfMemoryError: GC overhead limit
exceeded
  at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:45)
  at java.lang.StringBuffer.<init>(StringBuffer.java:91)
  at org.apache.hadoop.io.UTF8.toString(UTF8.java:129)
  at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:692)
  at org.apache.hadoop.dfs.SecondaryNameNode.doMerge(SecondaryNameNode.java:304)
  at org.apache.hadoop.dfs.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:273)
  at org.apache.hadoop.dfs.SecondaryNameNode.run(SecondaryNameNode.java:190)
  at java.lang.Thread.run(Thread.java:619)
		
"
HADOOP-2208,Reduce frequency of Counter updates in the task tracker status,"Currently,  We have counter updates from task tracker to job tracker on every heartbeat. Both counter name and the values are updated for every heartbeat. This can be improved by sending names and values for the first time and only the values after that.
The frequency can be reduced by doing update only when the counters got changed. "
HADOOP-2206,Design/implement a general log-aggregation framework for Hadoop,"I'd like to propose a log-aggregation framework which facilitates collection, aggregation and storage of the logs of the Hadoop Map-Reduce framework and user-jobs in HDFS. Clearly the design/implementation of this framework is heavily influenced and limited by Hadoop itself for e.g. lack of appends, not too many small files (think: stdout/stderr/syslog of each map/reduce task) and so on. 

This framework will be especially useful once HoD (HADOOP-1301) is used to provision dynamic, per-user, Map-Reduce clusters.

h4. Requirements:

*  Store the various logs to a configurable location in the Hadoop Distributed FileSystem
** User task logs (stdout, stderr, syslog)
** Map-Reduce daemons' logs (JobTracker and TaskTracker)
* Integrate well with Hadoop and ensure no adverse performance impact on the Map-Reduce framework.
* It must not use a HDFS file (or more!) per a task, which would swamp the NameNode capabilities.
* The aggregation system must be distributed and reliable.
* Facilities/tools to read the aggregated logs.
* The aggregated logs should be compressed.

h4. Architecture:

Here is a high-level overview of the log-aggregation framework:

h5. Logging
* Provision a cloud of log-aggregators in the cluster (outside of the Hadoop cluster, running on the subset of nodes in the cluster). Lets call each one in the cloud as a Log Aggregator i.e. LA.
* Each LA writes out 2 files per Map-Reduce cluster: an index file and a data file. The LA maintains one directory per Map-Reduce cluster on HDFS.
* The index file format is simple:
** streamid (_streamid_ is either daemon identifier e.g. tasktracker_foo.bar.com:57891 or $jobid-$taskid-(stdout|stderr|syslog) or individual task-logs)
** timestamp
** logs-data start offset
** no. of bytes
* Each Hadoop daemon (JT/TT) is given the entire list of LAs in the cluster.
* Each daemon picks one LA (at random) from the list, opens an exclusive stream with the LA after identifying itself (i.e. ${daemonid}) and sends it's logs. In case of error/failure to log it just connects to another LA as above and starts logging to it.
* The logs are sent to the LA by a new log4j appender. The appender provides some amount of buffering on the client-side.
* Implement a feature in the TaskTracker which lets it use the same appender to send out the userlogs (stdout/stderr/syslog) to the LA after task completion. This is important to ensure that logging to the LA at runtime doesn't hurt the task's performance (see HADOOP-1553). The TaskTracker picks an LA per task in a manner similar to the one it uses for it's own logs, identifies itself (<${jobid}, ${taskid}, {stdout|stderr|syslog}>) and streams the entire task-log at one go. In fact we can pick different LAs for each of the task's stdout, stderr and syslog logs - each an exclusive stream to a single LA.
* The LA buffers some amount of data in memory (say 16K) and then flushes that data to the HDFS file (per LA per cluster) after writing out an entry to the index file.
* The LA periodically purges old logs (monthly, fortnightly or weekly as today). 

h5. Getting the logged information

The main requirement is to implement a simple set of tools to query the LA (i.e. the index/data files on HDFS) to glean the logged information.

If we can think of each Map-Reduce cluster's logs as a set of archives (i.e. one file per cluster per LA used) we need the ability to query the log-archive to figure out the available streams and the ability to get one entire stream or a subset of time based on timestamp-ranges. Essentially these are simple tools which parse the index files of each LA (for a given Hadoop cluster) and return the required information.

h6. Query for available streams

The query just returns all the available streams in an cluster-log archive identified by the HDFS path.

It looks something like this for a cluster with 3 nodes which ran 2 jobs, first of which had 2 maps, 1 reduce and the second had 1 map, 1 reduce:
{noformat}
   $ la -query /log-aggregation/cluster-20071113
   Available streams:
   jobtracker_foo.bar.com:57893
   tasktracker_baz.bar.com:57841
   tasktracker_fii.bar.com:57891
   job_20071113_0001-task_20071113_0001_m_000000_0-stdout
   job_20071113_0001-task_20071113_0001_m_000000_0-stderr
   job_20071113_0001-task_20071113_0001_m_000000_0-syslog
   job_20071113_0001-task_20071113_0001_m_000001_0-stdout
   job_20071113_0001-task_20071113_0001_m_000001_0-stderr
   job_20071113_0001-task_20071113_0001_m_000001_0-syslog
   job_20071113_0001-task_20071113_0001_r_000000_0-stdout
   job_20071113_0001-task_20071113_0001_r_000000_0-stderr
   job_20071113_0001-task_20071113_0001_r_000000_0-syslog
   job_20071113_0001-task_20071113_0001_m_000000_0-stdout
   job_20071113_0001-task_20071113_0002_m_000000_0-stderr
   job_20071113_0001-task_20071113_0002_m_000000_0-syslog
   job_20071113_0001-task_20071113_0002_m_000001_0-stdout
   job_20071113_0001-task_20071113_0002_m_000001_0-stderr
   job_20071113_0001-task_20071113_0002_m_000001_0-syslog
   job_20071113_0001-task_20071113_0002_r_000000_0-stdout
   job_20071113_0001-task_20071113_0002_r_000000_0-stderr
   job_20071113_0001-task_20071113_0002_r_000000_0-syslog
{noformat}

h6. Get logged information per stream

The framework also offers the ability to query and fetch the actual log-data, per-stream for a given timestamp-range. It looks something like:
{noformat}
    $ la -fetch -daemon jt -range <t1:t2> /log-aggregation/cluster-20071113
    $ la -fetch -daemon tt1 /log-aggregation/cluster-20071113
    $ la -fetch -jobid <jobid> -taskid <taskid> -log <out|err|sys> -range <t1:t2> /log-aggregation/cluster-20071113
{noformat}



Thoughts?"
HADOOP-2205,Regenerate entire hadoop website since site.xml was changed by HADOOP-1917,"HADOOP-1917 changed src/docs/src/documentation/content/xdocs/site.xml, but did not regenerate docs/hdfs_design.html and docs/mailing_lists.html to reflect those changes."
HADOOP-2204,DFSTestUtil.waitReplication does not wait.,"This makes unit tests fail in unexpected ways.
In DFSTestUtil.java :

{code}
  /** wait for the file's replication to be done */
  static void waitReplication(FileSystem fs, Path fileName, short replFactor)  throws IOException {
    boolean good;
    do {
      good = true;
      //... 'good' is never accessed 
    } while(!good);
  }
{code}
"
HADOOP-2202,Job.java generates compiler warnings with Java6,"The following compiler warning is seen with Java6:

{noformat}
    [javac] /home/cutting/src/hadoop/trunk/src/java/org/apache/hadoop/mapred/jobcontrol/Job.java:221: warning: [unchecked] unchecked call to add(E) as a member of the raw type java.util.ArrayList
    [javac]       return this.dependingJobs.add(dependingJob);
    [javac]                                    ^
{noformat}
"
HADOOP-2200,TestBlockReplacement sometimes fails,"In current trunk, I'm seeing failures of TestBlockReplacement about one in four times."
HADOOP-2195,dfs mkdir command differs from POSIX standards,"Assuming the dfs commands follow POSIX standards, there are some problems with the DFS mkdir command. I compared the DFS output with that of RHEL 4u5:

1. mkdir a directory that exists:
Linux: mkdir: cannot create directory `test': File exists
DFS: No output

2. mkdir a directory with the name of a file that already exists:
- Linux: mkdir: `test/one' exists but is not a directory
- DFS: mkdir: java.io.IOException: Invalid directory name: <home directory>/test/one"
HADOOP-2194,dfs cat on a file that does not exist throws a java IOException,"The dfs cat command throws an IOException when the file does not exist:

bin/hadoop dfs -cat doesnotexist
cat: java.io.IOException: Cannot open filename<home directory>/doesnotexist]

In Linux, if a file does not exist, cat displays this:
cat: doesnotexist: No such file or directory"
HADOOP-2193,dfs rm and rmr commands differ from POSIX standards,"Assuming the dfs commands follow POSIX standards, there are some problems with the DFS rm and rmr commands. I compared the DFS output with that of RHEL 4u5:

In both cases, if the file/directory does not exist, it will not give any indication to the user.

1. rm a file/directory that does not exist:
Linux: rm: cannot remove `testarea/two': No such file or directory
DFS: rm: /testarea/two

2. rmr a file/directory that does not exist:
Linux: rm: cannot remove `testarea/two': No such file or directory
DFS: rm: /testarea/two"
HADOOP-2192,dfs mv command differs from POSIX standards,"Assuming the dfs commands follow POSIX standards, there are some problems with the DFS mv command. I compared the DFS output with that of RHEL 4u5

1. mv a file:
Linux: No output
DFS: Renamed /testarea/hadoop-site.xml to test

2. mv a file/directory that does not exist. The DFS file does not exist, but the output says it renamed the file:
Linux: mv: cannot stat `testarea/one': No such file or directory
DFS: mv: Rename failed /testarea/one

3. mv a directory to another directory:
Linux: No output
DFS: Renamed /testarea to testarea2"
HADOOP-2191,dfs du and dus commands differ from POSIX standards,"Assuming the dfs commands follow POSIX standards, there are some problems with the DFS du and dus commands. I compared the DFS output with that of RHEL 4u5

1. du a file/directory that does not exist:
Linux: du: cannot access `something': No such file or directory
DFS: Found 0 items

2. dus a file/directory that does not exist:
- Linux: du: cannot access `something': No such file or directory
- DFS:  /something      0"
HADOOP-2190,dfs ls and lsr commands differ from POSIX standards,"Assuming the dfs commands follow POSIX standards, there are some problems with the DFS ls  and lsr commands. I compared the DFS output with that of RHEL 4u5

1. ls a directory when there are no files/directories in that directory:
Linux: No output
DFS: Found 0 items

2. ls a file/directory that does not exist:
Linux: ls: /doesnotexist: No such file or directory
DFS: Found 0 items

3. lsr a directory that does not exist:
Linux: ls: /doesnotexist: No such file or directory
DFS: No output"
HADOOP-2189,Incrementing user counters should count as progress,An application that is incrementing the counters is making progress and should get credit for it and not be killed after 10 minutes.
HADOOP-2188,RPC should send a ping rather than use client timeouts,"Current RPC (really IPC) relies on client side timeouts to find ""dead"" sockets. I propose that we have a thread that once a minute (if the connection has been idle) writes a ""ping"" message to the socket. The client can detect a dead socket by the resulting error on the write, so no client side timeout is required. Also note that the ipc server does not need to respond to the ping, just discard it."
HADOOP-2187,FileSystem should return location information with byte ranges,"The FileSystem interface should provide location information with byte ranges rather than a String[][] of locations. I suggest that we deprecate FileSystem.getFileCacheHints and replace it with:
{code}
abstract public class FileSystem {
   ...
   public static class BlockInformation implements Writable {
      public BlockInformation(long start, String[] locations) {...}
      public String[] getHosts() {...}
      public long getStartingOffset() {...}
   }
   BlockInformation[] getFileLocations(Path f, long start, long length) { ... }
}
{code}
This will allow us to fix the FileInputFormat in map/reduce to make just one call per a file to the name node instead of one per a block."
HADOOP-2185,Server ports: to roll or not to roll.,"Looked at the issues related to port rolling. My impression is that port rolling is required only for the unit tests to run.
Even the name-node port should roll there, which we don't have now, in order to be able to start 2 cluster for testing say dist cp.

For real clusters on the contrary port rolling is not desired and some times even prohibited.
So we should have a way of to ban port rolling. My proposition is to
# use ephemeral port 0 if port rolling is desired
# if a specific port is specified then port rolling should not happen at all, meaning that a 
server is either able or not able to start on that particular port.

The desired port is specified via configuration parameters.
- Name-node: fs.default.name = host:port
- Data-node: dfs.datanode.port
- Job-tracker: mapred.job.tracker = host:port
- Task-tracker: mapred.task.tracker.report.bindAddress = host
  Task-tracker currently does not have an option to specify port, it always uses the ephemeral port 0, 
  and therefore I propose to add one.
- Secondary node does not need a port to listen on.

For info servers we have two sets of config variables *.info.bindAddress and *.info.port
except for the task tracker, which calls them *.http.bindAddress and *.http.port instead of ""info"".
With respect to the info servers I propose to completely eliminate the port parameters, and form 
*.info.bindAddress = host:port
Info servers should do the same thing, namely start or fail on the specified port if it is not 0,
and start on any free port if it is ephemeral.

For the task-tracker I would rename tasktracker.http.bindAddress to mapred.task.tracker.info.bindAddress
For the data-node the info dfs.datanode.info.bindAddress should be included into the default config.
Is there a reason why it is not there?

This is the summary of proposed changes:
|| Server || current name = value || proposed name = value ||
| NameNode | fs.default.name = host:port | same |
| | dfs.info.bindAddress = host | dfs.http.bindAddress = host:port |
| DataNode | dfs.datanode.bindAddress = host | dfs.datanode.bindAddress = host:port |
| | dfs.datanode.port = port | eliminate |
| | dfs.datanode.info.bindAddress = host | dfs.datanode.http.bindAddress = host:port |
| | dfs.datanode.info.port = port | eliminate |
| JobTracker | mapred.job.tracker = host:port | same |
| | mapred.job.tracker.info.bindAddress = host | mapred.job.tracker.http.bindAddress = host:port |
| | mapred.job.tracker.info.port = port | eliminate |
| TaskTracker | mapred.task.tracker.report.bindAddress = host | mapred.task.tracker.report.bindAddress = host:port |
| | tasktracker.http.bindAddress = host | mapred.task.tracker.http.bindAddress = host:port |
| | tasktracker.http.port = port | eliminate |
| SecondaryNameNode | dfs.secondary.info.bindAddress = host | dfs.secondary.http.bindAddress = host:port |
| | dfs.secondary.info.port = port | eliminate |

Do we also want to set some uniform naming convention for the configuration variables?
Like having hdfs instead of dfs, or info instead of http, or systematically using either datanode
or data.node would make that look better in my opinion.

So these are all +*api*+ changes. I would +*really*+ like some feedback on this, especially from 
people who deal with configuration issues on practice."
HADOOP-2184,RPC Support for user permissions and authentication.,"
Update 11/13/2007: What is proposed for 0.16.0 :

The client can set a user ticket (as defined in HADOOP-1701) for each connection and that ticket is made available to RPC calls at the server. The client can replace the ticket at any time. The main advantage is that rest of the the client RPCs don't need to be aware of the user tickets.

What RPC would ideally support in future :

In the current version of RPC, there is no authentication or data protection.  We propose to change the RPC framework, so that secure communication is possible.

The new RPC should:
- Compatible with current RPC
- Allow a pluggable security implementations (see HADOOP-1701)
- Support both secure and non-secure modes.

Here is a rough idea:
- Store security information (e.g. username, keys) in a ticket
- Use the ticket to establish a RPC connection
- Create secure sockets by the (subclass of) SocketFactory corresponding to the selected security implementations
- Send the data and RPC parameters with the secure sockets

When authentication is supported, the RPC callee should also initialize caller information during RPC setup and execute the RPC on the caller's behalf."
HADOOP-2183,Change RPC to provide secure communication,"In the current version of RPC, there is no authentication or data protection.  We propose to change the RPC framework, so that secure communication is possible.

The new RPC should:
- Compatible with current RPC
- Allow a pluggable security implementations (see HADOOP-1701)
- Support both secure and non-secure modes.

Here is a rough idea:
- Store security information (e.g. username, keys) in a ticket
- Use the ticket to establish a RPC connection
- Create secure sockets by the (subclass of) SocketFactory corresponding to the selected security implementations
- Send the data and RPC parameters with the secure sockets

When authentication is supported, the RPC callee should also initialize caller information during RPC setup and execute the RPC on the caller's behalf."
HADOOP-2182,Change RPC to provide secure communication,"In the current version of RPC, there is no authentication or data protection.  We propose to change the RPC framework, so that secure communication is possible.

The new RPC should:
- Compatible with current RPC
- Allow a pluggable security implementations (see HADOOP-1701)
- Support both secure and non-secure modes.

Here is a rough idea:
- Store security information (e.g. username, keys) in a ticket
- Use the ticket to establish a RPC connection
- Create secure sockets by the (subclass of) SocketFactory corresponding to the selected security implementations
- Send the data and RPC parameters with the secure sockets

When authentication is supported, the RPC callee should also initialize caller information during RPC setup and execute the RPC on the caller's behalf."
HADOOP-2181,Input Split details for maps should be logged,It would be nice if Input split details are logged someplace. This might help debugging failed map tasks
HADOOP-2178,Job history on HDFS,"This issue addresses the following items :

1.  Check for accuracy of job tracker history logs.

2.  After completion of the job, copy the JobHistory.log(Master index file) and the job history files to the DFS.

3. User can load the history with commands
bin/hadoop job -history <directory> 
or
bin/hadoop job -history <jobid>
This will start a stand-alone jetty and load jsps"
HADOOP-2174,distcp throws a NullPointerException in the close() method of mapper class due to the Reporter becoming invalid,"distcp occasionally throws a NullPointerException in the close() method of the mapper class, when the copy of the Reporter handle becomes invalid:

java.lang.NullPointerException
       at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.updateStatus(CopyFiles.java:242)
       at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:402)
       at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
       at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)

This can easily be fixed by checking whether the Reporter is null before updating the status. Also, the status could be updated once the invocation of copy(srcstat, dstpath, out, reporter); returns on line 373 of CopyFiles.java. Marking this as critical for a 0.15.1 release as Chris requested."
HADOOP-2172,"PositionCache was removed from FSDataInputStream, causes extremely bad MapFile performance",The PositionCache in FSDataInputStream seems to have been removed in HADOOP-1470. This causes for example MapFile.get usage to be  extremely slow as the file position isn't cached in memory.
HADOOP-2169,libhdfs makefile wrongly sets up DT_SONAME field of libhdfs.so,"For src/c++/libhdfs/Makefile:

{noformat}
LIB_NAME = hdfs
SO_TARGET = $(LIBHDFS_BUILD_DIR)/lib$(LIB_NAME).so.$(SHLIB_VERSION)
SO = $(LIBHDFS_BUILD_DIR)/lib$(LIB_NAME).so

$(SO_TARGET): $(COBJS)
	$(LD) $(LDFLAGS) -o $(SO_TARGET) -Wl,-soname,$(SO_TARGET) $(COBJS) \
	&& $(LINK) $(SO_TARGET) $(SO)
{noformat}

Basically the wrong value is passed for *-soname* flag to the linker, straight-forward fix."
HADOOP-2168,Pipes with a C++ record reader does not update progress in the map until it is 100%,"Currently C++ maps with C++ record readers do not update the progress, even if the application has updated it."
HADOOP-2167,"Reduce tips complete 100%, but job does not complete saying reduces still running.","Job's reduces are stuck at 99.43% progress and 2 reduces in running state and Job is not complete. 
But the reduce task list on the job tracker shows they are complete 100% and marked as SUCCEEDED and Finishtime is available jobtasks.jsp and jobhistory also.

With ipc.client.timeout = 600000, the exceptions on TT's running the reduces are
On one of the TTs, the logs show the following:
2007-11-07 08:34:16,092 INFO org.apache.hadoop.mapred.TaskTracker: Task task_200711070637_0001_r_000150_0 is done.
2007-11-07 08:35:34,013 INFO org.apache.hadoop.mapred.TaskTracker: Task task_200711070637_0001_r_000156_0 is done.
2007-11-07 08:42:44,751 ERROR org.apache.hadoop.mapred.TaskTracker: Caught exception: java.net.SocketTimeoutException: timedout waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:484)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at org.apache.hadoop.mapred.$Proxy0.heartbeat(Unknown Source)
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:897)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:799)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1193)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:2055)

2007-11-07 08:42:44,767 INFO org.apache.hadoop.mapred.TaskTracker: Resending 'status' to .................

On the other TT,
2007-11-07 08:40:30,484 INFO org.apache.hadoop.mapred.TaskTracker: Task task_200711070637_0001_r_000160_0 is done.
2007-11-07 08:42:45,508 ERROR org.apache.hadoop.mapred.TaskTracker: Caught exception: java.net.SocketTimeoutException: timedout waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:484)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at org.apache.hadoop.mapred.$Proxy0.heartbeat(Unknown Source)
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:897)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:799)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1193)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:2055)

2007-11-07 08:42:45,508 INFO org.apache.hadoop.mapred.TaskTracker: Resending 'status' to ..........

On JT logs, the reduce tasks are done successfully:
2007-11-07 06:39:09,151 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_200711070637_0001_r_000160_0' to tip tip_200711070637_0001_r_000160, for tracker 'x'
2007-11-07 08:42:45,708 INFO org.apache.hadoop.mapred.TaskRunner: Saved output of task 'task_200711070637_0001_r_000160_0' to 'y'
2007-11-07 08:42:45,708 INFO org.apache.hadoop.mapred.JobInProgress: Task 'task_200711070637_0001_r_000160_0' has completed tip_200711070637_0001_r_000160 successfully.

This would suggest that if tasks are done before the timeout, the problem occurs in progress update. This is also not consistent since other reduce tasks in the same situation are successful."
HADOOP-2165,Augment JobHistory to store tasks' userlogs,"It will be very useful to be able to see the job's userlogs (the stdout/stderr/syslog of the tasks) from the JobHistory page. It will greatly aid in debugging etc.

At the very minimum we should have links from the JobHistory to the logs on the TT."
HADOOP-2160,separate website from user documentation,"Currently the website only contains the documentation for a single release, the current release.  It would be better if the website also contained documentation for past releases, since not everyone is using the current release.  To implement this we should move the top-level of the website, including project and developer information, from the subversion trunk into a separate tree, so that only the user documentation is branched per release."
HADOOP-2158,hdfsListDirectory in libhdfs does not scale,"hdfsListDirectory makes one rpc call using deprecated fs.FileSystem.listPaths, and then two rpc calls for every entry in the returned array. When running a job with more than 3000 mappers each running a pipes application using libhdfs to scan a dfs directory with about 100-200 entries, this results in about 1M rpc calls to the namenode server overwhelming it.


hdfsListDirectory should call fs.FileSystem.listStatus instead.

I will submit a patch."
HADOOP-2151,FileSyste.globPaths does not validate the return list of Paths,"FileSystem.globPaths does not validate the return list of Paths.

Here is an example. 
Consider a directory structure like
/user/foo/DIR1/FILE1
/user/foo/DIR2

now if we pass an input path like ""/user/foo/*/FILE1"" to FileSystem.globPaths()
It returns 2 entries as shown below
/user/foo/DIR1/FILE1
/user/foo/DIR2/FILE1

Should globPaths validate this and return only valid Paths? This behavior was caught in FileSystem.validateInput() where an IOException is thrown while processing such a directory structure."
HADOOP-2149,Pure name-node benchmarks.,"h3. Pure name-node benchmark.

This patch starts a series of name-node benchmarks.
The intention is to have a separate benchmark for every important name-node operation.
The purpose of benchmarks is
# to measure the throughput for each name-node operation, and
# to evaluate changes in the name-node performance (gain or degradation) when optimization
or new functionality patches are introduced.

The benchmarks measure name-node throughput (ops per second) and the average execution time.
The benchmark does not involve any other hadoop components except for the name-node.
The name-node server is real, other components are simulated.
There is no RPC overhead. Each operation is executed by calling directly the respective name-node method.
The benchmark is multi-threaded, that is one can start multiple threads competing for the
name-node resources by executing concurrently the same operation but with different data.
See javadoc for more details.

The patch contains implementation for two name-node operations: file creates and block reports.
Implementation of other operations will follow.

h3. File creation benchmark.

I've ran two series of the file create benchmarks on the name-node with different number of threads.
The first series is run on the regular name-node performing an edits log transaction on every create.
The transaction includes a synch to the disk.
In the second series the name-node is modified so that the synchs are turned off.
Each run of the benchmark performs the same number 10,000 of creates equally distributed between
running threads. I used a 4 core 2.8Ghz machine.
The following two tables summarized the results. Time is in milliseconds.

|| threads || time (msec)\\with synch || ops/sec\\with synch ||
| 1 | 13074 | 764 |
| 2 | 8883 | 1125 |
| 4 | 7319 | 1366 |
| 10 | 7094 | 1409 |
| 20 | 6785 | 1473 |
| 40 | 6776 | 1475 |
| 100 | 6899 | 1449 |
| 200 | 7131 | 1402 |
| 400 | 7084 | 1411 |
| 1000 | 7181 | 1392 |

|| threads || time (msec)\\no synch || ops/sec\\no synch ||
| 1 | 4559 | 2193 |
| 2 | 4979 | 2008 |
| 4 | 5617 | 1780 |
| 10 | 5679 | 1760 |
| 20 | 5550 | 1801 |
| 40 | 5804 | 1722 |
| 100 | 5871 | 1703 |
| 200 | 6037 | 1656 |
| 400 | 5855 | 1707 |
| 1000 | 6069 | 1647 |

The results show:
# (Table 1) The new synchronization mechanism that batches synch calls from different threads works well.
For one thread all synchs cause a real IO making it slow. The more threads is used the more synchs are
batched resulting in better performance. The performance grows up to a certain point and then stabilizes
at about 1450 ops/sec.
# (Table 2) Operations that do not require disk IOs are constrained by memory locks.
Without synchs the one-threaded execution is the fastest, because there are no waits.
More threads start to intervene with each other and have to wait.
Again the performance stabilizes at about 1700 ops/sec, and does not degrade further.
# Our default 10 handlers per name-node is not the best choice neither for the io bound nor for the pure
memory operations. We should increase the default to 20 handlers and on big classes 100 handlers
or more can be used without loss of performance. In fact with more handlers more operations can be handled
simultaneously, which prevents the name-node from dropping calls that are close to timeout.

h3. Block report benchmark.

In this benchmarks each thread pretends it is a data-node and calls blockReport() with the same blocks.
All blocks are real, that is they were previously allocated by the name-node and assigned to the data-nodes.
Some reports can contain fake blocks, and some can have missing blocks.
Each block report consists of 10,000 blocks. The total number of reports sent is 1000.
The reports are equally divided between the data-nodes so that each of them sends equal number of reports.

Here is the table with the results.

|| data-nodes || time (msec) || ops/sec ||
| 1 | 42234 | 24 |
| 2 | 9412 | 106 |
| 4 | 11465 | 87 |
| 10 | 15632 | 64 |
| 20 | 17623 | 57 |
| 40 | 19563 | 51 |
| 100 | 24315 | 41 |
| 200 | 29789 | 34 |
| 400 | 23636 | 42 |
| 600 | 39682 | 26 |

I did not have time to analyze this yet. So comments are welcome.
"
HADOOP-2148,Inefficient FSDataset.getBlockFile(),"FSDataset.getBlockFile() first verifies that the block is valid and then returns the file name corresponding to the block.
Doing that it performs the data-node blockMap lookup twice. Only one lookup is needed here. 
This is important since the data-node blockMap is big.

Another observation is that data-nodes do not need the blockMap at all. File names can be derived from the block IDs,
there is no need to hold Block to File mapping in memory."
HADOOP-2145,need 'doc' target that runs forrest,"We should have an 'ant doc' target that runs forrest.  Ideally we'd check the forrest jars into subversion, so that this task works out of the box, w/o having to separately install forrest.  The ant task could both build the docs and copy them into the docs/ directory, rather than requiring this to be done manually."
HADOOP-2141,speculative execution start up condition based on completion time,"We had one job with speculative execution hang.

4 reduce tasks were stuck with 95% completion because of a bad disk. 

Devaraj pointed out 
bq . One of the conditions that must be met for launching a speculative instance of a task is that it must be at least 20% behind the average progress, and this is not true here.


It would be nice if speculative execution also starts up when tasks stop making progress.

Devaraj suggested 
bq. Maybe, we should introduce a condition for average completion time for tasks in the speculative execution check. "
HADOOP-2140,C and C++ files are missing Apache license header,Some of the C and C++ files are missing Apache License headers.
HADOOP-2134,Remove developer-centric requirements from overview.html,"Doug's comments on HADOOP-2105:

{blockquote}
There are still some asymmetries in overviewlhtml, e.g., subversion is only mentioned for Windows, and ant is mentioned in a separate section. I suggest that we drop mention of subversion and ant here altogether, as this is documentation for Hadoop users, not for Hadoop's developers. We might separately add mention of subversion and ant to Hadoop's contributor documentation, if we feel that's needed.
{blockquote}

I'm moving Jim's patch here since the old patch has already been committed to subversion."
HADOOP-2133,Codec pool is not used in o.a.h.i.S,
HADOOP-2132,Killing successfully completed jobs moves them to failed,"Once a job is in completed state, run hadoop job -kill <jobid>, it moves to failed queue.
"
HADOOP-2131,Speculative execution should be allowed for reducers only,"Consider hadoop jobs where maps fetch data from external systems, and emit the data. The reducers in this are identity reducers. The data processed by these jobs is huge. There could be slow nodes in this cluster and some of the reducers run twice as slow as their counterparts. This could result in a long tail. Speculative execution would help greatly in such cases. However given the current hadoop, we have to select speculative execution for both maps and reducers. In this case hurting the map performance as they are fetching data from external systems thereby overloading the external systems.

Speculative execution only on reducers would be a great way to solve this problem."
HADOOP-2130,Pipes submit job should be Non-blocking,"Pipes submitJob should be non blocking, similar to a JobClient's submit job. It will help us in monitoring the job as we would a normal Hadoop Job obtained via jobClient.submitJob()
"
HADOOP-2129,distcp between two clusters does not work if it is run on the target cluster,"I am trying to copy a directory (~100k files, ~500GB) between two clusters A and B (~70 nodes), using a command like:

hadoop distcp -log /logdir hdfs://namenode-of-A:8600/srcdir hdfs://namenode-of-B:8600/targetdir


I tried 4 ways of doing it:

1) Copy from A to B, by running distcp on A
2) Copy from A to B, by running distcp on B
3) Copy from B to A, by running distcp on B
4) Copy from B to A, by running distcp on A

Invocations 1 and 3 succeeded, but 2 and 4 failed.

I got a lot of errors of the type below:

07/10/30 20:52:11 INFO mapred.JobClient: Running job: job_200710180049_0115
07/10/30 20:52:12 INFO mapred.JobClient:  map 0% reduce 0%
07/10/30 20:54:41 INFO mapred.JobClient:  map 1% reduce 0%
07/10/30 20:56:52 INFO mapred.JobClient:  map 2% reduce 0%
07/10/30 20:57:41 INFO mapred.JobClient: Task Id : task_200710180049_0115_m_000184_0, Status : FAILED
java.io.IOException: Some copies could not complete. See log for details.
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:407)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)

followed by the job failing:

07/10/30 22:07:41 INFO mapred.JobClient:  map 99% reduce 100%
Copy failed: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:688)
        at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:481)
        at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:555)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:54)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:67)
        at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:566)"
HADOOP-2127,Add pipes sort example,It would be good to have a pipes sort example that we could use to benchmark the speed of a trivial pipes application versus the trivial java application.
HADOOP-2123,Enhancements and Improvement for Host monitor in a big cluster,"New Enhancements for HADOOP on big clusters. 
I put some new commands into HADOOP based on 0.14.0 and modified some bugs especially in the scheduling part. 

Enhancements:

1. Add Some commands to make it easy to monitor the cluster. 
hhosts - Display hosts status in the cluster.
hload - Display host load information including cpu usage, cpu usage and so on.
hslot - Change host available slot to improve efficiency. 
hopen - Enable a host on the fly.
hclose - Disable a host on the fly
2. Add Some commands to improve efficiency for a ""Big Cluster""
htop - Bring a job specified to the top prirority on the fly.
hbot - Bring a job specified to the lowest priority on the fly.
hjobkill - Kill a job
hjobs - List job information in detailed format. 
3. Enhance the task trunk dispatching method to make it more efficient when a job has many tasks.

4. Improved job submitting efficiency asuring right priority

[ Show » ] Shuguang Liu - 30/Oct/07 01:15 AM New Enhancements for HADOOP on big clusters. I put some new commands into HADOOP based on 0.14.0 and modified some bugs especially in the scheduling part. Enhancements: 1. Add Some commands to make it easy to monitor the cluster. hhosts - Display hosts status in the cluster. hload - Display host load information including cpu usage, cpu usage and so on. hslot - Change host available slot to improve efficiency. hopen - Enable a host on the fly. hclose - Disable a host on the fly 2. Add Some commands to improve efficiency for a ""Big Cluster"" htop - Bring a job specified to the top prirority on the fly. hbot - Bring a job specified to the lowest priority on the fly. hjobkill - Kill a job hjobs - List job information in detailed format. 3. Enhance the task trunk dispatching method to make it more efficient when a job has many tasks. 4. Improved job submitting efficiency asuring right priority 
"
HADOOP-2121,Unexpected IOException in DFSOutputStream.close(),"While running a test with datanodes with disk space limitations, Hairong noticed many IOExceptions like this :
{noformat}
java.io.IOException: Mismatch in writeChunk() args
 at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:1575)
 at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:140)
 at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:122)
 at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1715)
 at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:49)
 at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:64)
 at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:918)
 at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:72)
 at org.apache.hadoop.mapred.MapTask$DirectMapOutputCollector.close(MapTask.java:232)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:197)
 at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1935)
{noformat}

I will submit a patch. With the patch, we will still see an IOException, but an expected one."
HADOOP-2119,JobTracker becomes non-responsive if the task trackers finish task too fast,"I ran a job with 0 reducer on a cluster with 390 nodes.
The mappers ran very fast.
The jobtracker lacks behind on committing completed mapper tasks.
The number of running mappers displayed on web UI getting bigger and bigger.
The jos tracker eventually stopped responding to web UI.

No progress is reported afterwards.

Job tracker is running on a separate node.
The job tracker process consumed 100% cpu, with vm size 1.01g (reach the heap space limit).

"
HADOOP-2116,Job.local.dir to be exposed to tasks,"Currently, since all task cwds are created under a jobcache directory, users that need a job-specific shared directory for use as scratch space, create ../work. This is hacky, and will break when HADOOP-2115 is addressed. For such jobs, hadoop mapred should expose job.local.dir via localized configuration."
HADOOP-2115,Task cwds should be distributed across partitions,"Even when mapred.local.dir specifies a comma-separated list of partitions (typically one per physical disk), all tasks of the same job have current working directories that belong to only one partition. For side-effect tasks, that use local cwd as a scratch space, this overloads a single disk while other disks may be idle. Idially, each task should get a cwd on different partition. This is related to HADOOP-1991, but emphasizes performance impact."
HADOOP-2113,"Add ""-text"" command to FsShell to decode SequenceFile to stdout",FsShell should provide a command to examine SequenceFiles.
HADOOP-2112,TestMiniMRMapRedDebugScript fails due to a missing file,"The testcase fails consistently with the following error:

Waiting for the Mini HDFS Cluster to start...
java.io.IOException: build/test/debug/testscript.txt: No such file or directory
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:142)
        at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:826)
        at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:814)
        at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:795)
        at org.apache.hadoop.mapred.TestMiniMRMapRedDebugScript.launchFailMapAndDebug(TestMiniMRMapRedDebugScript.java:132)
        at org.apache.hadoop.mapred.TestMiniMRMapRedDebugScript.testMapDebugScript(TestMiniMRMapRedDebugScript.java:199)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at junit.framework.TestCase.runTest(TestCase.java:154)
        at junit.framework.TestCase.runBare(TestCase.java:127)
        at junit.framework.TestResult$1.protect(TestResult.java:106)
        at junit.framework.TestResult.runProtected(TestResult.java:124)
        at junit.framework.TestResult.run(TestResult.java:109)
        at junit.framework.TestCase.run(TestCase.java:118)
        at junit.framework.TestSuite.runTest(TestSuite.java:208)
        at junit.framework.TestSuite.run(TestSuite.java:203)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
"
HADOOP-2108,NullPointerException in JVMMetrics for OOM killed task,"I had a reduce task run out of memory and die in such a way that JVMMetrics.doThreadUpdates() throws a NullPointerException.

The aparent cause seems to be that the call to threadMXBean.getThreadInfo() on JVMMetrics:119 returns an array of ThreadInfo whose elements may be null.

Here's a relevant quote from the javadoc:
This method returns an array of the ThreadInfo objects,
     * each is the thread information about the thread with the same index
     * as in the ids array.
     * If a thread of the given ID is not alive or does not exist,
     * null will be set in the corresponding element 
     * in the returned array.  A thread is alive if 
     * it has been started and has not yet died.

My stacktrace looks like this:
java.lang.NullPointerException
	at org.apache.hadoop.metrics.jvm.JvmMetrics.doThreadUpdates(JvmMetrics.java:129)
	at org.apache.hadoop.metrics.jvm.JvmMetrics.doUpdates(JvmMetrics.java:79)
	at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:284)
	at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:50)
	at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:249)
	at java.util.TimerThread.mainLoop(Timer.java:512)
	at java.util.TimerThread.run(Timer.java:462)

On line 129,  there's an attempt to dereference the potientially null threadInfo value to get its current state.

The naive solution here is to check for null and count null values as ""terminated""... but it seems clear that a thread state of TERMINATED and a null ThreadInfo value are distinct cases and may need special treatment.

Guessing that this is a ""minor"" issue because it seems more cosmetic than mission critical.  I'm not sure what the upstream effects are of this method throwing the NPE, so i didn't set it to ""trivial""."
HADOOP-2107,Hadoop examples cannot execute on a single-node cluster post HADOOP-1622,"All hadoop examples on single-node  cluster, post HADOOP-1622, fail with:

{noformat}
Running 10 maps.
Job started: Fri Oct 26 17:23:51 IST 2007
07/10/26 17:23:51 WARN mapred.JobClient: No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).
07/10/26 17:23:52 INFO mapred.JobClient: Running job: job_200710261722_0001
07/10/26 17:23:53 INFO mapred.JobClient:  map 0% reduce 0%
07/10/26 17:24:07 INFO mapred.JobClient: Task Id : task_200710261722_0001_m_000000_0, Status : FAILED
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.examples.RandomWriter$RandomInputFormat
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:576)
	at org.apache.hadoop.mapred.JobConf.getInputFormat(JobConf.java:512)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:156)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1936)
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.examples.RandomWriter$RandomInputFormat
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:544)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:568)
	... 3 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.examples.RandomWriter$RandomInputFormat
	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:268)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:242)
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:524)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:542)
	... 4 more
{noformat}

The examples work once I reverted the HADOOP-1622 patch."
HADOOP-2105,Clarify requirements for Hadoop in overview.html,"The requirements section of overview.html does not make it clear that items 2, 3 are required on all platforms. It conflates the requirements with how to install on Ubuntu. Unless one reads carefully, it is not clear that this applies to Windows, for example."
HADOOP-2104,clover description attribute suppresses all other targets in -projecthelp,"Running ""ant -projecthelp"" should print a list of available targets; instead it outputs:

{noformat}
Buildfile: build.xml

Main targets:

 clover  Instrument the Unit tests using Clover.  Requires a Clover license and clover.jar in the ANT classpath.  To use, specify -Drun.clover=true on the command line.
Default target: compile
{noformat}

When ant has ""main"" targets- i.e. targets with descriptions- by default it only outputs those targets when run with -projecthelp. Since clover is the only target in build.xml with a description, it's the only target reported to the user. The description should either be removed or descriptions should be added to some subset of targets."
HADOOP-2103,HADOOP-2046 caused some javadoc anomalies,"Configuration.java, Mapper.java, and WritableComparable.java have either misformatted or missing fragments of javadoc."
HADOOP-2102,ToolBase doesn't keep configuration,ToolBase which has been superceded by ToolRunner doesn't pass in an existing configuration object and therefore won't pick up initial configuration resources.  One consequence of this is the nutch default and site.xml files are ignored.
HADOOP-2100,hadoop-daemon.sh script fails if HADOOP_PID_DIR doesn't exist,"HADOOP-1825 didn't fix this right... 

However it isn't critical (i.e. not a regression on previous releases) and 0.15.0 feature freeze is in place (it's nearly out of the door), I'm moving it to a separate jira and marking it for 0.16.0."
HADOOP-2098,File handles for log files are still open in case of jobs with 0 maps,When a job with zero maps is submitted the handle for the log file for that job is still open and can be seen using {{lsof}}. This over time could lead to {{Too many open files Exception}}.
HADOOP-2096,The file used to localize job.xml should be closed.,After localizing the job.xml file on the jobtracker the file should be closed.
HADOOP-2095,Reducer failed due to Out ofMemory,"One of the reducers of my job failed with the following exceptions.
The failure caused the whole job fail eventually.
Java heapsize was 768MB and sort.io.mb was 140.


2007-10-23 19:24:06,100 WARN org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Intermediate Merge of the inmemory files threw an exception: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.compress.DecompressorStream.(DecompressorStream.java:43)
	at org.apache.hadoop.io.compress.DefaultCodec.createInputStream(DefaultCodec.java:71)
	at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1345)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:1231)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:1154)
	at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:2726)
	at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:2543)
	at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:2297)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:1311)
2007-10-23 19:24:06,102 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 done copying task_200710231912_0001_m_001428_0 output .
2007-10-23 19:24:06,185 INFO org.apache.hadoop.fs.FileSystem: Initialized InMemoryFileSystem: ramfs://mapoutput31952838/task_200710231912_0001_r_000020_2/map_1423.out-0 of size (in bytes): 209715200
2007-10-23 19:24:06,193 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:378)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:449)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:738)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:665)

2007-10-23 19:24:06,193 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Copying task_200710231912_0001_m_001215_0 output from xxx
2007-10-23 19:24:06,188 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Copying task_200710231912_0001_m_001211_0 output from xxx
2007-10-23 19:24:06,185 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryOutputStream.close(InMemoryFileSystem.java:161)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:49)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:64)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.close(ChecksumFileSystem.java:312)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:49)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:64)
	at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:253)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:713)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:665)

2007-10-23 19:24:06,199 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Copying task_200710231912_0001_m_001247_0 output from .
2007-10-23 19:24:06,200 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:378)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:449)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:738)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:665)

2007-10-23 19:24:06,204 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Copying task_200710231912_0001_m_001422_0 output from .
2007-10-23 19:24:06,207 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:378)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:449)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:738)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:665)

2007-10-23 19:24:06,209 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Copying task_200710231912_0001_m_001278_0 output from .
2007-10-23 19:24:06,198 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.io.IOException: task_200710231912_0001_r_000020_2The reduce copier failed
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
2007-10-23 19:24:06,198 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:378)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:449)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:738)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:665)

2007-10-23 19:24:06,231 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Copying task_200710231912_0001_m_001531_0 output from .
2007-10-23 19:24:06,197 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:378)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:449)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:738)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:665)

2007-10-23 19:24:06,237 INFO org.apache.hadoop.mapred.ReduceTask: task_200710231912_0001_r_000020_2 Copying task_200710231912_0001_m_001227_0 output from .
2007-10-23 19:24:06,196 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:378)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:449)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:738)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:665)

"
HADOOP-2092,Pipes C++ task does not die even if the Java tasks die,"Pipes process does not die, when the java map task  dies.
Pipes process does not die even when the cluster is down."
HADOOP-2091,org.apache.hadoop.streaming tests failed,"The streaming tests failed after the fix for HADOOP-2080

Failing tests:
org.apache.hadoop.streaming.TestGzipInput.testCommandLine
org.apache.hadoop.streaming.TestMultipleCachefiles.unknown
org.apache.hadoop.streaming.TestStreamAggregate.testCommandLine
org.apache.hadoop.streaming.TestStreamDataProtocol.testCommandLine
org.apache.hadoop.streaming.TestStreamReduceNone.testCommandLine
org.apache.hadoop.streaming.TestStreaming.testCommandLine
org.apache.hadoop.streaming.TestSymLink.unknown

Exception (from the first failure):
junit.framework.AssertionFailedError: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:80)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:158)
	at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)
	at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:261)
	at org.apache.hadoop.mapred.JobConf.addInputPath(JobConf.java:173)
	at org.apache.hadoop.streaming.StreamJob.setJobConf(StreamJob.java:673)
	at org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:108)
	at org.apache.hadoop.streaming.TestStreaming.testCommandLine(TestStreaming.java:92)
Caused by: java.lang.reflect.InvocationTargetException
	at java.lang.reflect.Constructor.newInstance(Constructor.java:494)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:78)
	... 23 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.fs.ChecksumFileSystem.<init>(ChecksumFileSystem.java:48)
	at org.apache.hadoop.fs.LocalFileSystem.<init>(LocalFileSystem.java:34)
	... 28 more

	at org.apache.hadoop.streaming.TestStreaming.failTrace(TestStreaming.java:113)
	at org.apache.hadoop.streaming.TestStreaming.testCommandLine(TestStreaming.java:100)

Changes in the failing build:
/lucene/hadoop/branches/branch-0.15/src/test/org/apache/hadoop/fs/TestChecksumFileSystem.java
/lucene/hadoop/branches/branch-0.15/src/java/org/apache/hadoop/fs/ChecksumFileSystem.java
/lucene/hadoop/branches/branch-0.15/CHANGES.txt"
HADOOP-2089,Multiple caheArchive does not work in Hadoop streaming,"Multiple -cacheArchive options in Hadoop streaming does not work. Here is the stack trace:

Exception in thread ""main"" java.lang.RuntimeException: 
            at org.apache.hadoop.streaming.StreamJob.fail(StreamJob.java:528)
            at org.apache.hadoop.streaming.StreamJob.exitUsage(StreamJob.java:469)
            at org.apache.hadoop.streaming.StreamJob.parseArgv(StreamJob.java:203)
            at org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:105)
            at org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:33)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
            at java.lang.reflect.Method.invoke(Method.java:597)
            at org.apache.hadoop.util.RunJar.main(RunJar.java:155)"
HADOOP-2087,Errors for subsequent requests for file creation after original DFSClient goes down..,"task task_200710200555_0005_m_000725_0 started writing a file and the Node went down.. so all following file creation attempts were returned with AlreadyBeingCreatedException
I think the dfs should handle cases wherein, if a dfsclient goes down between file creation, subsequent creates to the same file could be allowed. 

2007-10-20 06:23:51,189 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_200710200555_0005_m_000725_0: Task task_200710200555_0005_m_000725_0 failed to report status for 606 seconds. Killing!
2007-10-20 06:23:51,189 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_200710200555_0005_m_000725_0' from '[tracker_address]:/127.0.0.1:44198'
2007-10-20 06:23:51,209 INFO org.apache.hadoop.mapred.JobInProgress: Choosing normal task tip_200710200555_0005_m_000725
2007-10-20 06:23:51,209 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_200710200555_0005_m_000725_1' to tip tip_200710200555_0005_m_000725, for tracker '[tracker_address]:/127.0.0.1:50914'
2007-10-20 06:28:54,991 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_200710200555_0005_m_000725_1: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /benchmarks/TestDFSIO/io_data/test_io_825 for DFSClient_task_200710200555_0005_m_000725_1 on client 72.30.50.198, because this file is already being created by DFSClient_task_200710200555_0005_m_000725_0 on 72.30.53.224
        at org.apache.hadoop.dfs.FSNamesystem.startFileInternal(FSNamesystem.java:881)
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:806)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:276)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)

"
HADOOP-2086,ability to add dependencies to a job after construction,"The current Job API only allows for dependent jobs to be passed in at object construction time. It would be nice if there was an additional constructor which did not take any depending jobs and then an ""addDependingJob"" method which could be used to add depending jobs to a job at a later point."
HADOOP-2085,"Map-side joins on sorted, equally-partitioned datasets","h3. Motivation

Given a set of sorted datasets keyed with the same class and yielding equal
partitions, it is possible to effect a join of those datasets prior to the
map. This could save costs in re-partitioning, sorting, shuffling, and
writing out data required in the general case.

h3. Interface

The attached code offers the following interface to users of these classes.

|| property || required || value ||
| mapred.join.expr | yes | Join expression to effect over input data |
| mapred.join.keycomparator | no | {{WritableComparator}} class to use for comparing keys |
| mapred.join.define.<ident> | no | Class mapped to identifier in join expression |

The join expression understands the following grammar:
{noformat}
func ::= <ident>([<func>,]*<func>)
func ::= tbl(<class>,""<path>"");
{noformat}

Operations included in this patch are partitioned into one of two types:
join operations emitting tuples and ""multi-filter"" operations emitting a
single value from (but not necessarily included in) a set of input values.
For a given key, each operation will consider the cross product of all
values for all sources at that node.

Identifiers supported by default:

|| identifier || type || description ||
| inner | Join | Full inner join |
| outer | Join | Full outer join |
| override | MultiFilter | For a given key, prefer values from the rightmost source |

A user of this class must set the {{InputFormat}} for the job to
{{CompositeInputFormat}} and define a join expression accepted by the preceding
grammar. For example, both of the following are acceptable:

{noformat}
inner(tbl(org.apache.hadoop.mapred.SequenceFileInputFormat.class,
          ""hdfs://host:8020/foo/bar""),
      tbl(org.apache.hadoop.mapred.SequenceFileInputFormat.class,
          ""hdfs://host:8020/foo/baz""))

outer(override(tbl(org.apache.hadoop.mapred.SequenceFileInputFormat.class,
                   ""hdfs://host:8020/foo/bar""),
               tbl(org.apache.hadoop.mapred.SequenceFileInputFormat.class,
                   ""hdfs://host:8020/foo/baz"")),
      tbl(org.apache.hadoop.mapred/SequenceFileInputFormat.class,
          ""hdfs://host:8020/foo/rab""))
{noformat}

{{CompositeInputFormat}} includes a handful of convenience methods to aid
construction of these verbose statements.

As in the second example, joins may be nested. Users may provide a
comparator class in the {{mapred.join.keycomparator}} property to
specify the ordering of their keys, or accept the default comparator as
returned by {{WritableComparator.get(keyclass)}}.

Users can specify their own join operations, typically by overriding
{{JoinRecordReader}} or {{MultiFilterRecordReader}} and mapping that class
to an identifier in the join expression using the
{{mapred.join.define._ident_}} property, where _ident_ is the identifier
appearing in the join expression. Users may elect to emit- or modify- values
passing through their join operation. Consulting the existing operations for
guidance is recommended. Adding arguments is considerably more complex (and
only partially supported), as one must also add a {{Node}} type to the parse
tree. One is probably better off extending {{RecordReader}} in most cases.

h3. Design

As alluded to above, the design defines inner (Composite) and leaf (Wrapped)
types for the join tree. Delegation satisfies most requirements of the
{{InputFormat}} contract, particularly {{validateInput}} and {{getSplits}}.
Most of the work in this patch concerns {{getRecordReader}}. The
{{CompositeInputFormat}} itself delegates to the parse tree generated by
{{Parser}}.

h4. Hierarchical Joins

Each {{RecordReader}} from the user must be ""wrapped"", since effecting a
join requires the framework to track the head value from each source. Since
the cross product of all values for each composite level of the join is
emitted to its parent, all sources ^1^ must be capable of repeating the
values for the current key. To avoid keeping an excessive number of copies
(one per source per level), each composite requests its children to populate
a {{JoinCollector}} with an iterator over its values. This way, there is
only one copy of the current key for each composite node, the head key-value
pair for each leaf, and storage at each leaf for all the values matching the
current key at the parent collector (if it is currently participating in a
join at the root). Strategies have been employed to avoid excessive copying
when filling a user-provided {{Writable}}, but they have been conservative
(e.g. in {{MultiFilterRecordReader}}, the value emitted is cloned in case
the user modifies the value returned, possibly changing the state of a
{{JoinCollector}} in the tree). For example, if the following sources
contain these key streams:

{noformat}
A: 0  0   1    1     2        ...
B: 1  1   1    1     2        ...
C: 1  6   21   107   ...
D: 6  28  496  8128  33550336 ...
{noformat}

Let _A-D_ be wrapped sources and _x,y_ be composite operations. If the
expression is of the form {{x(A, y(B,C,D))}}, then when the current key at
the root is 1 the tree may look like this:

{noformat}

            x (1, [ I(A), [ I(y) ] ] )
          /   \
         W     y (1, [ I(B), I(C), EMPTY ])
         |   / | \
         |  W  W  W
         |  |  |  D (6, V~6~) => EMPTY
         |  |  C (6, V~6~)    => V~1.1~ @1.1
         |  B (2, V~2~)       => V~1,1~ V~1,2~ V~1,3~ V~1,4~ @1,3
         A (2, V~2~)          => V~1,1~ V~1,2~ @1,2
{noformat}

A {{JoinCollector}} from _x_ will have been created by requesting an
iterator from _A_ and another from _y_. The iterator at _y_ is built by
requesting iterators from _B_, _C_, and _D_. Since _D_ doesn't contain the
key 1, it returns an empty iterator. Since the value to return for a given
join is a {{Writable}} provided by the user, the iterators returned are also
responsible for writing the next value in that stream. For multilevel joins
passing through a subclass of {{JoinRecordReader}}, the value produced will
contain tuples within tuples; iterators for composites delegate to
sub-iterators responsible for filling the value in the tuple at the position
matching their position in the composite. In a sense, the only iterators
that write to a tuple are the {{RecordReader}} s at the leaves. Note that
this also implies that emitted tuples may not contain values from each
source, but they will always have the same capacity.

h4. Writables

{{Writable}} objects- including {{InputSplit}} s and {{TupleWritable}} s-
encode themselves in the following format:

{noformat}
<count><class1><class2>...<classn><obj1><obj2>...<objn>
{noformat}

The inefficiency is regrettable- particularly since this overhead is
incurred for every instance and most often the tuples emitted will be
processed only within the map- but the encoding satisfies the {{Writable}}
contract well enough to be emitted to the reducer, written to disk, etc. It
is hoped that general compression will trim the most egregious waste. It
should be noted that the framework does not actually write out a tuple (i.e.
does not suffer for this deficiency) unless emitting one from
{{MultiFilterRecordReader}} (a rare case in practice, it is hoped).

h4. Extensibility

The join framework is modestly extensible. Practically, users seeking to add
their own identifiers to join expressions are limited to extending
{{JoinRecordReader}} and {{MultiFilterRecordReader}}. There is considerable
latitude within these constraints, as illustrated in
{{OverrideRecordReader}}, where values in child {{RecordReader}} s are
skipped instead of incurring the overhead of building the iterator (that
will inevitably be discarded).^2^ For most cases, the user need only
implement the combine and/or emit methods in their subclass. It is expected
that most will find that the three default operations will suffice.

Adding arguments to expressions is more difficult. One would need to include
a {{Node}} type for the parser, which requires some knowledge of its inner
workings. The model in this area is crude and requires refinement before it
can be ""extensible"" by a reasonable definition.

h3. Performance

I have no numbers.

Notes

1. This isn't strictly true. The ""leftmost"" source will never need to repeat
itself. Adding a pseudo-{{ResettableIterator}} to handle this case would be
a welcome addition.

2. Note that- even if reset- the override will only loop through the values
in the rightmost key, instead of repeating that series a number of times
equal to the cardinality of the cross product of the discarded streams
(regrettably, looking at the code of {{OverrideRecordReader}} is more
illustrative than this explanation).
"
HADOOP-2081,"Configuration getInt, getLong, and getFloat replace invalid numbers with the default value",The current behavior of silently replace invalid numbers in the configuration with the default value leads to hard to diagnose problems. The methods should throw an exception to signal that the input was invalid.
HADOOP-2080,ChecksumFileSystem checksum file size incorrect.,"Periodically, reduce tasks hang. When the log for the task is consulted, you see a stacktrace that looks like this:

2007-10-18 17:02:04,227 WARN org.apache.hadoop.mapred.ReduceTask: java.io.IOException: Insufficient space
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryOutputStream.write(InMemoryFileSystem.java:174)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:39)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:326)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:140)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:122)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.close(ChecksumFileSystem.java:310)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:49)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:64)
	at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:253)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:685)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:637)

The problem stems from a miscalculation of the checksum file created in the InMemoryFileSystem associated with the data being copied from a completed mapper task to the reducer task.

The method used for calculating checksum file size is the following (ChecksumFileSystem:318):

((long)(Math.ceil((float)size/bytesPerSum)) + 1) * 4 + CHECKSUM_VERSION.length;

The issue here is the cast to float.  Floating point numbers have only 24 bits of precision, thus will return short values on any size over 0x1000000.  The fix is to replace this calculation with something that doesn't cast to float.

(((size+1)/bytesPerSum) + 2) * 4 + CHECKSUM_VERSION.length

"
HADOOP-2078,Name-node should be able to close empty files.,"When I try to close an empty file, the name-node throws an exception ""Could not complete write to file"" 
and issues a warning ""NameSystem.completeFile: failed to complete"".
I don't see any reason why empty files should not be allowed.
"
HADOOP-2077,Logging version number (and compiled date) at STARTUP_MSG  ,This will help us figure out which version of hadoop we were running when looking back the logs.
HADOOP-2073,Datanode corruption if machine dies while writing VERSION file,"Yesterday, due to a bad mapreduce job, some of my machines went on OOM killing sprees and killed a bunch of datanodes, among other processes.  Since my monitoring software kept trying to bring up the datanodes, only to have the kernel kill them off again, each machine's datanode was probably killed many times.  A large percentage of these datanodes will not come up now, and write this message to the logs:

2007-10-18 00:23:28,076 ERROR org.apache.hadoop.dfs.DataNode: org.apache.hadoop.dfs.InconsistentFSStateException: Directory /hadoop/dfs/data is in an inconsistent state: file VERSION is invalid.

When I check, /hadoop/dfs/data/current/VERSION is an empty file.  Consequently, I have to delete all the blocks on the datanode and start over.  Since the OOM killing sprees happened simultaneously on several datanodes in my DFS cluster, this could have crippled my dfs cluster.

I checked the hadoop code, and in org.apache.hadoop.dfs.Storage, I see this:
{{{
    /**
     * Write version file.
     * 
     * @throws IOException
     */
    void write() throws IOException {
      corruptPreUpgradeStorage(root);
      write(getVersionFile());
    }

    void write(File to) throws IOException {
      Properties props = new Properties();
      setFields(props, this);
      RandomAccessFile file = new RandomAccessFile(to, ""rws"");
      FileOutputStream out = null;
      try {
        file.setLength(0);
        file.seek(0);
        out = new FileOutputStream(file.getFD());
        props.store(out, null);
      } finally {
        if (out != null) {
          out.close();
        }
        file.close();
      }
    }
}}}

So if the datanode dies after file.setLength(0), but before props.store(out, null), the VERSION file will get trashed in the corrupted state I see.  Maybe it would be better if this method created a temporary file VERSION.tmp, and then copied it to VERSION, then deleted VERSION.tmp?  That way, if VERSION was detected to be corrupt, the datanode could look at VERSION.tmp to recover the data.
"
HADOOP-2072,RawLocalFileStatus is causing Path problems ,"In RawLocalFileStatus of the RawLocalFileSystem class, files were getting changed to URIs then to string which would cause some files to appear as C:/somethingfile://anotherpath.  This is a simple change and it just needs to be converted as toString and opposed to toUri().toString().  The problem area is line 324 of the org.apache.hadoop.fs.RawLocalFileSystem.java file.  I am testing a patch currently and will submit as soon as it passes all current unit tests."
HADOOP-2071,StreamXmlRecordReader throws java.io.IOException: Mark/reset exception in hadoop 0.14,"In hadoop 0.14, using -inputreader StreamXmlRecordReader  for streaming jobs throw 
java.io.IOException: Mark/reset exception in hadoop 0.14
This looks to be related to (https://issues.apache.org/jira/browse/HADOOP-2067).

<stack trace>
Caused by: java.io.IOException: Mark/reset not supported
	at
org.apache.hadoop.dfs.DFSClient$DFSInputStream.reset(DFSClient.java:1353)
	at java.io.FilterInputStream.reset(FilterInputStream.java:200)
	at
org.apache.hadoop.streaming.StreamXmlRecordReader.fastReadUntilMatch(StreamX
mlRecordReader.java:289)
	at
org.apache.hadoop.streaming.StreamXmlRecordReader.readUntilMatchBegin(Stream
XmlRecordReader.java:118)
	at
org.apache.hadoop.streaming.StreamXmlRecordReader.seekNextRecordBoundary(Str
eamXmlRecordReader.java:111)
	at
org.apache.hadoop.streaming.StreamXmlRecordReader.init(StreamXmlRecordReader
.java:73)
	at
org.apache.hadoop.streaming.StreamXmlRecordReader.(StreamXmlRecordReader.jav
a:63)

</stack trace>
"
HADOOP-2070,Test org.apache.hadoop.mapred.pipes.TestPipes.unknown failed,"The test org.apache.hadoop.mapred.pipes.TestPipes.unknown failed on Solaris and Linux.

Error output:
junit.framework.AssertionFailedError: Timeout occurred

Complete test output is available in the console output for the nighlty build #275:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/275/consoleText"
HADOOP-2067,multiple close() failing in Hadoop 0.14,"It looks like multiple close() calls, while reading files from DFS is failing in hadoop 0.14. This was somehow not caught in hadoop 0.13.
The use case was to open a file on DFS like shown below
<code>
 FSDataInputStream
	fSDataInputStream =
	fileSystem.open(new Path(propertyFileName));
      Properties subProperties =
	new Properties();
      subProperties.
	loadFromXML(fSDataInputStream);
      fSDataInputStream.
	close();
</code>

This failed with an IOException
<exception>
EXCEPTIN RAISED, which is java.io.IOException: Stream closed
java.io.IOException: Stream closed
</exception>

The stack trace shows its being closed twice. While this used to work in hadoop 0.13 which used to hide this.
Attached with this JIRA is a text file which has stack trace for both hadoop 0.13 and hadoop 0.14.

How should this be handled from a users point of view? 

Thanks
"
HADOOP-2065,Replication policy for corrupted block ,"Thanks to HADOOP-1955, even if one of the replica is corrupted, the block should get replicated from a good replica relatively fast.

Created this ticket to continue the discussion from http://issues.apache.org/jira/browse/HADOOP-1955#action_12531162.

bq. 2. Delete corrupted source replica
bq. 3. If all replicas are corrupt, stop replication.

For (2), it'll be nice if the namenode can delete the corrupted block if there's a good replica on other nodes.

For (3), I prefer if the namenode can still replicate the block.
Before 0.14, if the file was corrupted, users were still able to pull the data and decide if they want to delete those files. (HADOOP-2063)
In 0.14 and later, we cannot/don't replicate these blocks so they eventually get lost.

To make the matters worse, if the corrupted file is accessed, all the corrupted replicas would be deleted except for one and stay as replication factor of 1 forever.






 "
HADOOP-2063,Command to pull corrupted files,"Before 0.14, dfs -get didn't perform checksum checking.   
Users were able to download the corrupted files to see if they want to delete them.

After 0.14, dfs -get also does the checksumming. 

Requesting a command for no-checksum-get command."
HADOOP-2062,"Standardize long-running, daemon-like, threads in hadoop daemons","There are several long-running, independent, threads in hadoop daemons (atleast in the JobTracker - e.g. ExpireLaunchingTasks, ExpireTrackers, TaskCommitQueue etc.) which need to be alive as long as the daemon itself and hence should be impervious to various errors and exceptions (e.g. HADOOP-2051). 

Currently, each of them seem to be hand-crafted (again, specifically the JobTracker) and different from the other.

I propose we standardize on an implementation of a long-running, impervious, daemon-thread which can be used all over the shop. That thread should be explicitly shut-down by the hadoop daemon and shouldn't be vulnerable to any exceptions/errors.

This mostly likely will look like this:

{noformat}
public abstract class DaemonThread extends Thread {

  public static final Log LOG = LogFactory.getLog(DaemonThread.class);

  {
    setDaemon(true);                              // always a daemon
  }

  public abstract void innerLoop() throws InterruptedException;
  
  public final void run() {
    while (!isInterrupted()) {
      try {
        innerLoop();
      } catch (InterruptedException ie) {
        LOG.warn(getName() + "" interrupted, exiting..."");
      } catch (Throwable t) {
        LOG.error(getName() + "" got an exception: "" + 
                  StringUtils.stringifyException(t));
      }
    }
  }
}

{noformat}

In fact, we could probably hijack org.apache.hadoop.util.Daemon since it isn't used anywhere (Doug is it still used in nutch?) or atleast sub-class that.

Thoughts? Could someone from hdfs/hbase chime in?"
HADOOP-2060,DFSClient should choose a block that is local to the node where the client is running,"
When I chase down the DFSClient code to see how the data locality impact the dfs read throughput,
I realized that DFSClient does not use data locality info (at least not obvious to me) 
when it chooses a block for read from the available replicas.
Here is the relevant code:
{code}
 /**
   * Pick the best node from which to stream the data.
   * Entries in <i>nodes</i> are already in the priority order
   */
  private DatanodeInfo bestNode(DatanodeInfo nodes[], 
                                AbstractMap<DatanodeInfo, DatanodeInfo> deadNodes)
                                throws IOException {
    if (nodes != null) { 
      for (int i = 0; i < nodes.length; i++) {
        if (!deadNodes.containsKey(nodes[i])) {
          return nodes[i];
        }
      }
    }
    throw new IOException(""No live nodes contain current block"");
  }

    private DNAddrPair chooseDataNode(LocatedBlock block)
      throws IOException {
      int failures = 0;
      while (true) {
        DatanodeInfo[] nodes = block.getLocations();
        try {
          DatanodeInfo chosenNode = bestNode(nodes, deadNodes);
          InetSocketAddress targetAddr = DataNode.createSocketAddr(chosenNode.getName());
          return new DNAddrPair(chosenNode, targetAddr);
        } catch (IOException ie) {
          String blockInfo = block.getBlock() + "" file="" + src;
          if (failures >= MAX_BLOCK_ACQUIRE_FAILURES) {
            throw new IOException(""Could not obtain block: "" + blockInfo);
          }
          
          if (nodes == null || nodes.length == 0) {
            LOG.info(""No node available for block: "" + blockInfo);
          }
          LOG.info(""Could not obtain block "" + block.getBlock() + "" from any node:  "" + ie);
          try {
            Thread.sleep(3000);
          } catch (InterruptedException iex) {
          }
          deadNodes.clear(); //2nd option is to remove only nodes[blockId]
          openInfo();
          failures++;
          continue;
        }
      }
    } 
{code}

It seems to pick the first good replica. 
This means that even though some replica is local  to the node where the client runs,
it may actually pick a remote replica. 

Map/reduce tries to schedule a mapper to a node where some copy of the input split data is local to the node.
However, if the DFSClient does not use that info in choosing replica for  read, the mapper may well have to read data 
from the network, even though a local replica is available.



I hope I missed something and misunderstood the code.
Otherwise, this will be a serious problem to performance.

"
HADOOP-2058,Allow adding additional datanodes to MiniDFSCluster,Currently MiniDFSCluster allows first starting namenode and then datanodes. It would be nice that it also allows additional datanodes to a running MiniDFSCluster.
HADOOP-2057,streaming should optionally treat a non-zero exit status of a child process as a failed task,"The exit status of the external processes spawned by streaming tasks is currently logged, but not used to indicate success or failure of the task. While this is reasonable for some UNIX tools (e.g. grep), many programs will indicate failure by a non-zero exit status. (Also, even for custom programs, intentionally indicating the failure of a streaming task is currently rather tricky.)

This could be supported by adding a new job-configuration setting, 'stream.non.zero.exit.is.failure'. If true, a non-zero exit status of a child process would throw an exception in the PipeMapRed, causing task failure. The current behavior would be preserved by using a default setting of false. 

This would allow streaming tasks to easily indicate failure, even if all input has already been consumed."
HADOOP-2055,JobConf should have a setInputPathFilter method,It should be possible to set a PathFilter for the input to avoid taking certain files as input data within the input directories.
HADOOP-2054,Improve memory model for map-side sorts,"{{MapTask#MapOutputBuffer}} uses a plain-jane {{DataOutputBuffer}} which defaults to a buffer of size 32-bytes, and the {{DataOutputBuffer#write}} call doubles the underlying byte-array when it needs more space.

However for maps which output any decent amount of data (e.g. 128MB in examples/Sort.java) this means the buffer grows painfully slowly from 2^6 to 2^28, and each time this results in a new array being created, followed by an array-copy:

{noformat}
    public void write(DataInput in, int len) throws IOException {
      int newcount = count + len;
      if (newcount > buf.length) {
        byte newbuf[] = new byte[Math.max(buf.length << 1, newcount)];
        System.arraycopy(buf, 0, newbuf, 0, count);
        buf = newbuf;
      }
      in.readFully(buf, count, len);
      count = newcount;
    }
{noformat}

I reckon we could do much better in the {{MapTask}}, specifically... 

For e.g. we start with a buffer of size 1/4KB and quadruple, rather than double, upto, say 4/8/16MB. Then we resume doubling (or less).

This means that it quickly ramps up to minimize no. of {{System.arrayCopy}} calls and small-sized buffers to GC; and later start doubling to ensure we don't ramp-up too quickly to minimize memory wastage due to fragmentation.

Of course, this issue is about benchmarking and figuring if all this is worth it, and, if so, what are the right set of trade-offs to make.

Thoughts?"
HADOOP-2052,distcp mapper's status report misleading,"When the mappers of distcp finish, the status page in the web gui reports the data copied.
However, the reported number is far away from the real number, which is very misleading.
For example, a particular mapper task_200710131713_0001_m_000000_0  reported: 

Finished. Bytes copied: 4.3g

However, it does not say which file.
I thought it was for part-00000. But the file size of part-00000
is about 9GB.

It will be much clearer if the status report  say something like:

Finished copy file-xxxx: 4.3g
That way, I can easily check whether the size is correct.

 

"
HADOOP-2051,JobTracker's TaskCommitQueue is vulnerable to non-IOExceptions,"The {{JobTracker#TaskCommitQueue#run}} method only handles {{IOException}}s. Christian Kunz ran into a scenario where a job was stuck with all tasks in {{COMMIT_PENDING}} state and the stack traces showed that the ""Task Commit Thread"" wasn't even around.

The work-around is to model {{TaskCommitQueue#run}} along the lines of other long-running threads in the {{JobTracer}} ({{ExpireLaunchingTasks}}, {{ExpireTrackers}} etc.) to catch, log and ignore any {{Exception}} in a loop."
HADOOP-2049,distcp does not fail if source directory has files with missing blocks,"I copied a directory using distcp (to another directory on the same file system).

There were 9 data blocks missing in the files in the source directory, which caused distcp to print messages like the following:

...
07/10/13 00:09:16 INFO mapred.JobClient:  map 1% reduce 0%
07/10/13 00:09:16 INFO mapred.JobClient: Task Id : task_200710120717_0081_m_000020_0, Status : FAILED
java.io.IOException: Could not obtain block: blk_6787282547149034655 file=/srcdir/file1
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1136)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:988)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1094)
        at java.io.DataInputStream.read(DataInputStream.java:83)
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.copy(CopyFiles.java:289)
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:348)
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:216)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1753)
...

The corresponding tasks failed, but the retries were successful (all files with missing blocks in the source directory were copied as empty files in the target directory).

I think that distcp should fail if it cannot successfully copy all the files (at least when no command-line options are given).

This is critical for us as we intend to use distcp to copy databases from one dfs to another, and if silent failures can happen then we would have to monitor each distcp manually to ensure that it succeeded."
HADOOP-2048,DISTCP mapper should report progress more often,"When I ran DISTCP to copy files from one dfs to another, I noticed that some mappers got killed due to failing to report status for 606 seconds. 
I noticed that the mappers try to make a progress report for every 32MB copied. A better way to ensure progress is to use a time interval since last report.


"
HADOOP-2046,Documentation: improve mapred javadocs,"I'd like to put forward some thoughts on how to structure reasonably detailed documentation for hadoop.

Essentially I think of atleast 3 different profiles to target:
* hadoop-dev, folks who are actively involved improving/fixing hadoop.
* hadoop-user
** mapred application writers and/or folks who directly use hdfs
** hadoop cluster administrators

For this issue, I'd like to first target the latter category (admin and hdfs/mapred user) - where, arguably, is the biggest bang for the buck, right now. 
There is a crying need to get user-level stuff documented, judging by the sheer no. of emails we get on the hadoop lists...

----

*1. Installing/Configuration Guides*

This set of documents caters to folks ranging from someone just playing with hadoop on a single-node to operations teams who administer hadoop on several nodes (thousands). To ensure we cover all bases I'm thinking along the lines of:

* _Download, install and configure hadoop_ on a single-node cluster: including a few comments on how to run examples (word-count) etc.
* *Admin Guide*: Install and configure a real, distributed cluster. 
* *Tune Hadoop*: Separate sections on how to tune hdfs and map-reduce, targeting power admins/users.

I reckon most of this would be done via forrest, with appropriate links to javadoc.

---

*2. User Manual*

This set is geared for people who use hdfs and/or map-reduce per-se. Stuff to document:

* Write a really simple mapred application, just fitting the blocks together i.e. maybe a walk-through of a couple of examples like word-count, sort etc.
* Detailed information on important map-reduce user-interfaces:
*- JobConf
*- JobClient
*- Tool & ToolRunner
*- InputFormat 
*-- InputSplit
*-- RecordReader
*- Mapper
*- Reducer
*- Reporter
*- OutputCollector
*- Writable
*- WritableComparable
*- OutputFormat
*- DistributedCache
* SequenceFile
*- Compression types: NONE, RECORD, BLOCK
* Hadoop Streaming
* Hadoop Pipes

I reckon most of this would land up in the javadocs, specifically package.html and some via forrest.

----

Also, as discussed in HADOOP-1881, it would be quite useful to maintain documentation per-release, even on the hadoop website i.e. we could have a main documentation page link to documentation per-release and to the trunk.

----

Thoughts?"
HADOOP-2045,credits page should have more information,"The hadoop credits page should permit folks to list their organization, timezone, role, etc, as is done for many other projects, e.g.:

http://harmony.apache.org/contributors.html
http://jackrabbit.apache.org/team-list.html
http://db.apache.org/whoweare.html

Thus I propose we add a table to this page, with columns for username, name, organization, timezone and roles.  I don't think we need explicit ""website"" or ""email"" columns.  Folks can make their name a link if they have a website or blog, and email addresses are derivable from Apache username (by adding @apache.org)."
HADOOP-2044,Namenode encounters ClassCastException exceptions for INodeFileUnderConstruction,"A distcp command running on one 400 node cluster shows this exception:

org.apache.hadoop.fs.FSNamesystem: Removing lease [Lease.  Holder: 44 46 53 43 6c 69 65 6e 74 5f 74 61 73 6b 5f 32 30 30 37 31 30 31 31 32 32 35 37 5f 30 30 30 33 5f 6d 5f 30 30 30 30 39 32 5f 30, heldlocks: 0, pendingcreates: 0], leases remaining: 736
org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on /user/xxxx/logs_21/_task_200710112257_0003_m_000027_0/part-00027 file does not exist.
 org.apache.hadoop.fs.FSNamesystem: Removing lease [Lease.  Holder: 44 46 53 43 6c 69 65 6e 74 5f 74 61 73 6b 5f 32 30 30 37 31 30 31 31 32 32 35 37 5f 30 30 30 33 5f 6d 5f 30 30 30 30 32 37 5f 30, heldlocks: 0, pendingcreates: 0], leases remaining: 735
 org.apache.hadoop.fs.FSNamesystem: java.lang.ClassCastException: org.apache.hadoop.dfs.INodeFile cannot be cast to org.apache.hadoop.dfs.INodeFileUnderConstruction
        at org.apache.hadoop.dfs.FSNamesystem.internalReleaseCreate(FSNamesystem.java:1566)
        at org.apache.hadoop.dfs.FSNamesystem.access$100(FSNamesystem.java:51)
        at org.apache.hadoop.dfs.FSNamesystem$Lease.releaseLocks(FSNamesystem.java:1463)
        at org.apache.hadoop.dfs.FSNamesystem$LeaseMonitor.run(FSNamesystem.java:1525)
        at java.lang.Thread.run(Thread.java:619)

"
HADOOP-2034,Reduce tasks' graphs are slightly off-track,"The reduce tasks' graphs are slightly off scale, attaching a screenshot."
HADOOP-2033,In SequenceFile sync doesn't work unless the file is compressed (block or record),DistCp counts on the fact that SequenceFile sync actually puts a sync marker in and it currently doesn't unless the file is being compressed.
HADOOP-2032,distcp split generation does not work correctly,"
With the current implementation, distcp will always assign multiple files to one mapper to copy, no matter how large 
are the files. This is because the CopyFiles class uses a sequencefile to store the list of files to be copied, 
one record per file. CopyFile class correctly generates one split per record in the sequence file. However, 
due to  the way the sequence file record reader works, the minimum unit for splits is the segments between the 
""syncmarks"" in the sequence file. 
This results in the strange behavior that some mappers get zero records (zero files to copy) even though their 
split lengths are non-zero, while other mappers get multiple records (multiple filesto copy) from their split (and beyond
to the next sync mark). 

When CopyFile class creates the sequencefile, it does try to place a sync mark between splitable segments in the sequence file by calling sync() function of the sequence file record writer. 
Unfortunately, the sync() function is a no-op for files that are not block compressed.

Naturally, after I changed the compression type for the sequence file to block compression,
mappers got the correct records from their splits.
So a simple fix is to change the compression tye to CompressionType.BLOCK:

{code}
// create src list
    SequenceFile.Writer writer = SequenceFile.createWriter(
        jobDirectory.getFileSystem(jobConf), jobConf, srcfilelist,
        LongWritable.class, FilePair.class,
        SequenceFile.CompressionType.BLOCK);.
{code}

"
HADOOP-2031,"Lost tasktracker not handled properly leading to tips wrongly being kept as completed, and hence not rescheduled","The TIP.isComplete(taskid) checks for TaskStatus.state being SUCCEEDED and this is used to determine whether this taskid took the TIP to completion. Since, the taskstatus for a KILLED task would have the state as KILLED, this check would fail and hence the TIP would never be marked incomplete. The right solution is to maintain the successful taskid and use that in TIP.isComplete(taskid). This bug got introduced in the last patch for HADOOP-1874. "
HADOOP-2028,distcp fails if log dir not specified and destination not present,"The default location for distcp logs is in the destination directory; when that doesn't exist, distcp exits with an error."
HADOOP-2027,FileSystem should provide byte ranges for file locations,"FileSystem's getFileCacheHints should be replaced with something more useful. I'd suggest replacing getFileCacheHints with a new method:

{code}
BlockLocation[] getFileLocations(Path file, long offset, long range) throws IOException;
{code}

and adding

{code}
class BlockLocation implements Writable {
  String[] getHosts();
  long getOffset();
  long getLength();
}
{code}
"
HADOOP-2026,"Namenode prints out too many log lines for ""Number of transactions""",HADOOP-1942 introduced a log-line that prints the namenode transaction rate. The code was supposed to be printing it out once every minute. But a bug caused it to appear once for every transaction.
HADOOP-2024,Make StatusHttpServer (usefully) subclassable,"hbase puts up webapps modelled on those deployed by dfs and mapreduce.  Currently it does this by copying the bulk of StatusHttpServer down to hbase util as a new class named InfoServer.  StatusHttpServer is copied rather than subclassed because I need access to the currently-private resource loading.

As is, understandably, all webapp-related resources are presumed under the first 'webapps' directory found.  It doesn't allow for the new condition where some resources can be found in hadoop and then others in hbase."
HADOOP-2023,TestLocalDirAllocator fails on Windows,"The following tests failed on Windows. It passed on Linux

org.apache.hadoop.fs.TestLocalDirAllocator.test0
org.apache.hadoop.fs.TestLocalDirAllocator.test1
org.apache.hadoop.fs.TestLocalDirAllocator.test2
org.apache.hadoop.fs.TestLocalDirAllocator.test3

Exception thrown (org.apache.hadoop.fs.TestLocalDirAllocator.test0):

junit.framework.AssertionFailedError
	at org.apache.hadoop.fs.TestLocalDirAllocator.validateTempDirCreation(TestLocalDirAllocator.java:67)
	at org.apache.hadoop.fs.TestLocalDirAllocator.test0(TestLocalDirAllocator.java:84)"
HADOOP-2022,Task times are not saved correctly (bug in hadoop-1874),"In HADOOP-1874, a new taskstatus object is created for successful tasks. However, in the new taskstatus object, the timestamps for the start/end of the various phases are not filled. In the new status object, the timestamps should be set from the old status object. The problem can be easily seen if the taskdetails.jsp is accessed."
HADOOP-2020,Index is out of bound when restarting datanode after 5 hours,"I was told that dfs recovers when datanodes go down and come back after a while, even when some blocks went missing.
As a test I stopped the datanode server on a single-node cluster and restarted after 5 hours.
Dfs did not recover because of the following repeated exception:

2007-10-10 02:42:52,808 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call blockReport(127.0.0.1:50010, [Lorg.apache.hadoop.dfs.Block;@ecb8c4) from 127.0.0.1:49678: error: java.io.IOException: java.lang.AssertionError: Index is out of bound
java.io.IOException: java.lang.AssertionError: Index is out of bound
    at org.apache.hadoop.dfs.BlocksMap$BlockInfo.getNext(BlocksMap.java:77)
    at org.apache.hadoop.dfs.DatanodeDescriptor$BlockIterator.next(DatanodeDescriptor.java:185)
    at org.apache.hadoop.dfs.DatanodeDescriptor$BlockIterator.next(DatanodeDescriptor.java:170)
    at org.apache.hadoop.dfs.DatanodeDescriptor.reportDiff(DatanodeDescriptor.java:325)
    at org.apache.hadoop.dfs.FSNamesystem.processReport(FSNamesystem.java:2111)
    at org.apache.hadoop.dfs.NameNode.blockReport(NameNode.java:621)
    at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:609)
"
HADOOP-2019,DistributedFileCache should support .tgz files in addition to jars and zip files,"Currently the distributed file cache only works with zip and jar archives, which don't work for larger than 2g. We should support .tgz archives also."
HADOOP-2018,Broken pipe SocketException in DataNode$DataXceiver,"I have 2 data-nodes, one of which is trying to replicate blocks to another.
The second data-node throws the following excpetion for every replicated block.
{code}
07/10/09 20:36:39 INFO dfs.DataNode: Received block blk_-8942388986043611634 from /a.d.d.r:43159
07/10/09 20:36:39 WARN dfs.DataNode: Error writing reply back to /a.d.d.r:43159for writing block blk_-8942388986043611634
07/10/09 20:36:39 WARN dfs.DataNode: java.net.SocketException: Broken pipe
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:115)
        at java.io.DataOutputStream.writeShort(DataOutputStream.java:151)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:939)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:763)
        at java.lang.Thread.run(Thread.java:619)
{code}
# It looks like that the first data-node does not expect to receive anything from the second one and closes the connection.
# There should be a space in front of 
{code}
              + ""for writing block "" + block );
{code}
# The port number is misleading in these messages. DataXceivers open sockets on different ports every time, which is
different from the data-node's main port. So we should rather print here the main port in order to be able to recognize
wich data-node the block was sent from. 

Is this related to HADOOP-1908? "
HADOOP-2016,Race condition in removing a KILLED task from tasktracker,"I ran into a situation where a speculative task was killed by the JobTracker and the relevant TaskTracker got the right KillTaskAction, but the tasktracker continued to hold a reference to that task (although the task jvm was killed). The task continued to be in RUNNING state in both the JobTracker and that TaskTracker for ever. I suspect there is some race condition in reading/updating datastructures inside the taskCleanupThread & transmitHeartBeat."
HADOOP-2012,Periodic verification at the Datanode,"Currently on-disk data corruption on data blocks is detected only when it is read by the client or by another datanode.  These errors are detected much earlier if datanode can periodically verify the data checksums for the local blocks.

Some of the issues to consider :

- How should we check the blocks ( no more often than once every couple of weeks ?)
- How do we keep track of when a block was last verfied ( there is a .meta file associcated with each lock ).
- What action to take once a corruption is detected
- Scanning should be done as a very low priority with rest of the datanode disk traffic in mind."
HADOOP-2007,Jobs use incorrect path to job.xml for different users,"We run hadoop/hdfs under a generic username of 'hadoop'.  When submitting a job as user 'fred', the job.xml file gets created correctly as /tmp/*hadoop-fred*/mapred/system/<job_id>/job.xml, but the JobInProgress constructor appears to use the incorrect path of /tmp/*hadoop-hadoop*/mapred/system/<job_id>/job.xml for the copyToLocalFile operation.  This results in an exception because the job.xml file does not exist under /tmp/hadoop-hadoop.

I think the incorrect path gets created on line 133 in JobInProgress.java right before the call to copyToLocalFile.  If I hardcode the correct path (just to test my theory) of ""/tmp/hadoop-fred/mapred/system"" in place of the call to default_conf.getSystemDir(), the job kicks off and runs to completion as the user 'fred' with hadoop/hdfs running as the user 'hadoop'.

Here's a portion of the output when the job is submitted by user 'fred':

[fred@hdm01 ~]$ hadoop --config /usr/local/hadoop/conf jar /usr/local/hadoop/current/hadoop-0.14.1-examples.jar wordcount mytest/input/data output2
07/10/07 17:52:14 INFO mapred.FileInputFormat: Total input paths to process : 62
org.apache.hadoop.ipc.RemoteException: java.io.IOException: /tmp/hadoop-hadoop/mapred/system/job_200710071751_0002/job.xml: No such file or directory
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:138)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:803)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:784)
        at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:134)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1479)


"
HADOOP-2001,Deadlock in jobtracker,"My jobtracker deadlocked; the output from kill -QUIT is:

Found one Java-level deadlock:
=============================
""IPC Server handler 2 on 10001"":
  waiting to lock monitor 0x0813724c (object 0xd5175488, a org.apache.hadoop.mapred.JobInProgress),
  which is held by ""SocketListener0-1""
""SocketListener0-1"":
  waiting to lock monitor 0x081146d4 (object 0xd24d9c50, a org.apache.hadoop.mapred.JobTracker),
  which is held by ""IPC Server handler 2 on 10001""

Java stack information for the threads listed above:
===================================================
""IPC Server handler 2 on 10001"":
        at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:367)
        - waiting to lock <0xd5175488> (a org.apache.hadoop.mapred.JobInProgress)
        at org.apache.hadoop.mapred.JobTracker.updateTaskStatuses(JobTracker.java:1719)
        at org.apache.hadoop.mapred.JobTracker.processHeartbeat(JobTracker.java:1240)
        - locked <0xd24d9c50> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:1116)
        - locked <0xd24d9c50> (a org.apache.hadoop.mapred.JobTracker)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
""SocketListener0-1"":
        at org.apache.hadoop.mapred.JobTracker.finalizeJob(JobTracker.java:907)
        - waiting to lock <0xd24d9c50> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:1059)
        - locked <0xd5175488> (a org.apache.hadoop.mapred.JobInProgress)
        at org.apache.hadoop.mapred.JobInProgress.kill(JobInProgress.java:891)
        - locked <0xd5175488> (a org.apache.hadoop.mapred.JobInProgress)
        at org.apache.hadoop.mapred.jobdetails_jsp._jspService(jobdetails_jsp.java:158)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

Found 1 deadlock."
HADOOP-2000,Re-write NNBench to use MapReduce,"The proposal is to re-write the NNBench benchmark/test to measure Namenode operations using MapReduce. Two buckets of measurements will be done:

1. Transactions per second 
2. Average latency

for these operations
- Create and Close file
- Open file
- Rename file
- Delete file

"
HADOOP-1999,DataNodes can become dead nodes when running 'dfsadmin finalizeUpgrade',"I restarted namenode with -upgrade option, started a few scripts running hadoop command line utility to upload a few files into dfs, and ran at some time

hadoop dfsadmin -finalizeUpgrade.

At this time all the dfs clients I started before got stuck during block transmission."
HADOOP-1997,TestCheckpoint fails on Windows,"The bug is in the test. TestCheckpoint opens edits files in order to verify its length but does not close it, so the
secondary name-node attempt to delete it (in order to move edits.new into it on Win) fails.
But this revealed 3 more bugs.
- FSEditLog.purgeEditLog() in case of delete / rename failure calls FSEditLog.processIOError(), which should have
failed because all FSEditLog streams are closed by that time. It does not because the condition for throwing the
exception is incorrect.
- In fact purgeEditLog() should call directly FSImage.processIOError() because again all streams are closed.
- FSImage.processIOError() should also throw an exception if number of directories is <=1 rather than when it == 1"
HADOOP-1995,Path can not handle a file name that contains a back slash,"When normalizing a path name, Path incorrectly converts a back slash to a path separator even if  the path name is of the unix style. This prohibs a glob from using a back slash to escape a special character. A fix is to make path normalization file system dependent."
HADOOP-1992,Sort validation is taking considerably longer than before,"Sort validation has slowed down compared to previous benchmark runs

Here are the data points:
20 nodes: 0.39hr -> 1.36hr
100 nodes: 0.44hr -> 1.39hr
500 nodes: 0.56hr -> 1.41hr"
HADOOP-1991,Task startup fail when localizeTask directory is full,"We had a job filling up one particular drive (different bug) that resulted in all the subsequent tasks on that node to fail with 
java.io.IOException: Mkdirs failed to create /vol2/mapred-tt/mapred-local/taskTracker/jobcache/job_200710030011_0001/task_200710030011_0001_m_007363_0
	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:1284)
	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:1344)
	at org.apache.hadoop.mapred.TaskTracker.launchTaskForJob(TaskTracker.java:647)
	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:640)
	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:1167)
	at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:832)
	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1203)
	at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1888)

It'll be nice if localization also uses localdirallocator to utilize multiple disks."
HADOOP-1989,Add support for simulated Data Nodes  - helpful for testing and performance benchmarking of the Name Node without having a large cluster,"Proposal is to add an implementation for a Simulated Data Node.
This will 
  - allow one to test certain parts of the system (especially the Name Node, protocols) much more easily and efficiently.
  - allow one to run performance benchmarks on the Name node without having a large cluster.
  - Inject faults for testing (e.g. one can add random faults based probability parameters).

The idea is that the Simulated Data Node will
 - discard any data written to blocks (but remember the blocks and their sizes)
 - generate fixed data on the fly when blocks are read (e.g. block is fixed set of bytes or repeated sequence of strings).


The Simulated Data Node can also be used for fault injection.
The data node can be parameterized with probabilities that allow one to control:
  - Delays on reads and writes, creates, etc
  - IO Exceptions
 - Loss of blocks 
 - Failures"
HADOOP-1986,Add support for a general serialization mechanism for Map Reduce,"Currently Map Reduce programs have to use WritableComparable-Writable key-value pairs. While it's possible to write Writable wrappers for other serialization frameworks (such as Thrift), this is not very convenient: it would be nicer to be able to use arbitrary types directly, without explicit wrapping and unwrapping."
HADOOP-1985,Abstract node to switch mapping into a topology service class used by namenode and jobtracker,"In order to implement switch locality in MapReduce, we need to have switch location in both the namenode and job tracker.  Currently the namenode asks the data nodes for this info and they run a local script to answer this question.  In our environment and others that I know of there is no reason to push this to each node.  It is easier to maintain a centralized script that maps node DNS names to switch strings.

I propose that we build a new class that caches known DNS name to switch mappings and invokes a loadable class or a configurable system call to resolve unknown DNS to switch mappings.  We can then add this to the namenode to support the current block to switch mapping needs and simplify the data nodes.  We can also add this same callout to the job tracker and then implement rack locality logic there without needing to chane the filesystem API or the split planning API.

Not only is this the least intrusive path to building racklocal MR I can ID, it is also future compatible to future infrastructures that may derive topology on the fly, etc, etc..."
HADOOP-1984,some reducer stuck at copy phase and progress extremely slowly,"
In many cases, some reducers got stuck at copy phase, progressing extremely slowly.
The entire cluster seems doing nothing. This causes a very bad long tails of otherwise well tuned map/red jobs.
"
HADOOP-1983,jobs using pipes interface with tasks not using java output format have a good chance of not updating progress and timing out,"When using C++-pipes interface, mappers/reducers not emitting any key-values pairs, but running longer than 'mapred.task.timeout' might timeout even when they send periodical status updates upstream.

The cause of the problem is that all upstream messages are buffered. Progress and status updates should be flushed immediately."
HADOOP-1982,TaskTracker provided java.library.path is lost if set in the job.xml,"if a job 'mapred.child.java.opts' property includes a '-Djava.library.path"" option this overrides the one set by the TaskTracker when setting up the taskrunner.

instead it should append it.
"
HADOOP-1980,'dfsadmin -safemode enter' should prevent the namenode from leaving safemode automatically after startup,"When debugging, I'd like to be able to intentionally keep the FS in a safemode. (For example, when looking at HADOOP-1978).
Also, it'll be nice if the namenode can still update the webUI/report when it hits the dfs.safemode.threshold.pct."
HADOOP-1978,Name-node should remove edits.new during startup rather than renaming it to edits.,"Secondary name-node fails in the middle. The main name-node writes its journal transactions into edits.new at that time.
If the name-node is shut down after that and restarted, then loadFSImage() reads current image file, merges it with the edits
file and with the edits.new file.
Now saveFSImage() saves new image file, creates empty edits file, and then calls rollFSImage(), which particularly renames 
edits.new into edits. This is a mistake, during startup edits.new should be merely removed after merging it with the image.
The purpose of calling rollFSImage() during startups imho is to recover from an unsuccessful checkpoint. So an easy fix
is to empty edits.new before calling rollFSImage the same as edits are emptied, then rollFSImage will rename empty file
to empty which gives us the desired result.
We should fix this bug both in 0.14 and 0.15. I make it a blocker for 0.15."
HADOOP-1973,NPE at JobTracker startup..,"At JT startup a bunch of these were thrown.. and the JT shutdown. 

2007-10-01 07:48:36,501 WARN org.apache.hadoop.mapred.JobTracker: Serious problem, cannot find record of 'previous' heartbeat for '[tracker_address]:/127.0.0.1:40205'; reinitializing the tasktracker
2007-10-01 07:48:36,504 WARN org.apache.hadoop.mapred.JobTracker: Serious problem, cannot find record of 'previous' heartbeat for '[tracker_address]:/127.0.0.1:57935'; reinitializing the tasktracker
2007-10-01 07:48:36,520 WARN org.apache.hadoop.mapred.JobTracker: Serious problem, cannot find record of 'previous' heartbeat for '[tracker_address]:/127.0.0.1:46305'; reinitializing the tasktracker
2007-10-01 07:48:36,523 WARN org.apache.hadoop.mapred.JobTracker: Serious problem, cannot find record of 'previous' heartbeat for '[tracker_address]:/127.0.0.1:55988'; reinitializing the tasktracker
2007-10-01 07:48:36,524 INFO org.apache.hadoop.ipc.Server: IPC Server handler 28 on 50020, call getFilesystemName() from [tracker_ip]:47361: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobTracker.getFilesystemName(JobTracker.java:1458)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
"
HADOOP-1971,Constructing a JobConf without a class leads to a very misleading error message.,"If this line from a typical map/reduce program

        JobConf conf = new JobConf(Flatten.class);

has the argument deleted then I get a message that the input format that I specify cannot be found.  That message leads a naive user on a rat chase.  I only corrected this by bisecting against a working program.

"
HADOOP-1969,"org.apache.hadoop.io.Text uses static ChasetDecoders, but they aren't thread-safe","org.apache.hadoop.io.Text uses static instances Text.DECODER and Text.ENCODER for all encoding and decoding, but these classes are not thread-safe.  Multiple threads calling Text.toString() at the same time can cause the decoders to output jumbled and garbage data.

"
HADOOP-1968,Wildcard input syntax (glob) should support {},"We have users who have organized data by day and would like to select several days in a single input specification.  For example they would like to be able to say:

'/data/2007{0830,0831,0901}/typeX/'

To input 3 days data into map-reduce (or Pig in this case). 

(Also the use of regexp to resolve glob paterns looks like it might introduce some other bugs.  I'd appreciate it if someone took another look at the code to see if there are any file characters that could
be interpreted as regexp ""instructions"")."
HADOOP-1967,"hadoop dfs -ls, -get, -mv command's source/destination URI are inconsistent","While specifying source/destination path for hadoop dfs -ls, -get, -mv, -cp commands, we have some inconsistency related to 'hdfs://' scheme.

Particularly, few of the commands accept both formats
[1] hdfs:///user/lohit/testfile
[2] hdfs://myhost:8020/user/lohit/testfile

and few commands accept only paths, which have authority (host:port)
[2] hdfs://myhost:8020/user/lohit/testfile

below are examples
hadoop dfs -ls  (works for both formats)
{quote}
[lohit@krygw1000 ~]$ hadoop dfs -ls hdfs://kry-nn1:8020/user/lohit/ranges
Found 1 items
/user/lohit/ranges	<r 3>	24	1970-01-01 00:00
[lohit@krygw1000 ~]$ hadoop dfs -ls hdfs:///user/lohit/ranges
Found 1 items
{quote}


hadoop dfs -get (works for only format [2])
{quote}
[lohit@krygw1000 ~]$ hadoop dfs -get hdfs:///user/lohit/ranges .
Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS:
hdfs:/user/lohit/ranges, expected: hdfs://kry-nn1:8020
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:204)
	at
org.apache.hadoop.dfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:108)
	at
org.apache.hadoop.dfs.DistributedFileSystem.getPath(DistributedFileSystem.java:104)
	at
org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:319)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:423)
	at org.apache.hadoop.fs.FsShell.copyToLocal(FsShell.java:177)
	at org.apache.hadoop.fs.FsShell.copyToLocal(FsShell.java:155)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:1233)
	at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:1342)
[lohit@krygw1000 ~]$ hadoop dfs -get hdfs://kry-nn1:8020/user/lohit/ranges .
[lohit@krygw1000 ~]$ ls ./ranges
./ranges
[lohit@krygw1000 ~]$
{quote}

hadoop dfs -mv / -cp command. source path accepts both format [1] and [2], while destination accepts only [2].
{quote}
[lohit@krygw1000 ~]$ hadoop dfs -cp hdfs://kry-nn1:8020/user/lohit/ranges.test2
hdfs:///user/lohit/ranges.test
Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS:
hdfs:/user/lohit/ranges.test, expected: hdfs://kry-nn1:8020
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:204)
	at
org.apache.hadoop.dfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:108)
	at
org.apache.hadoop.dfs.DistributedFileSystem.getPath(DistributedFileSystem.java:104)
	at
org.apache.hadoop.dfs.DistributedFileSystem.exists(DistributedFileSystem.java:162)
	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:269)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:117)
	at org.apache.hadoop.fs.FsShell.copy(FsShell.java:691)
	at org.apache.hadoop.fs.FsShell.copy(FsShell.java:727)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:1260)
	at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:1342)
[lohit@krygw1000 ~]$ hadoop dfs -cp hdfs:///user/lohit/ranges.test2
hdfs://kry-nn1:8020/user/lohit/ranges.test
[lohit@krygw1000 ~]$ 
{quote}

We should have a consistent URI naming convention across all commands. "
HADOOP-1965,Handle map output buffers better,"Today, the map task stops calling the map method while sort/spill is using the (single instance of) map output buffer. One improvement that can be done to improve performance of the map task is to have another buffer for writing the map outputs to, while sort/spill is using the first buffer."
HADOOP-1963,Code contribution of Kosmos Filesystem implementation of Hadoop Filesystem interface,"Kosmos Filesystem (KFS) is an open source implementation targeted towards applications that are required to process large amounts of data.  KFS has been integrated with Hadoop using Hadoop's filesystem interfaces.  This issue is filed with the intent of getting our code, namely, fs/kfs classes, to be included in the next Hadoop release."
HADOOP-1962,code cleanup?,"I would like some code placed in the released streaming, ...

This is a replacement for org.apache.hadoop.streaming.StreamJob.submitAndMonitorJob() :

New code:


  // Based on JobClient
  public void submitAndMonitorJob() throws IOException {

    if (jar_ != null && isLocalHadoop()) {
      // getAbs became required when shell and subvm have different working dirs...
      File wd = new File(""."").getAbsoluteFile();
      StreamUtil.unJar(new File(jar_), wd);
    }

    // ecw - begin
    JobClient.runJob(jobConf_);
    /*
    // if jobConf_ changes must recreate a JobClient
    jc_ = new JobClient(jobConf_);
    boolean error = true;
    running_ = null;
    String lastReport = null;
    try {
      running_ = jc_.submitJob(jobConf_);
      jobId_ = running_.getJobID();

      LOG.info(""getLocalDirs(): "" + Arrays.asList(jobConf_.getLocalDirs()));
      LOG.info(""Running job: "" + jobId_);
      jobInfo();

      while (!running_.isComplete()) {
        try {
          Thread.sleep(1000);
        } catch (InterruptedException e) {
        }
        running_ = jc_.getJob(jobId_);
        String report = null;
        report = "" map "" + Math.round(running_.mapProgress() * 100) + ""%  reduce ""
          + Math.round(running_.reduceProgress() * 100) + ""%"";

        if (!report.equals(lastReport)) {
          LOG.info(report);
          lastReport = report;
        }
      }
      if (!running_.isSuccessful()) {
        jobInfo();
        throw new IOException(""Job not Successful!"");
      }
      LOG.info(""Job complete: "" + jobId_);
      LOG.info(""Output: "" + output_);
      error = false;
    } catch(FileNotFoundException fe){
      LOG.error(""Error launching job , bad input path : "" + fe.getMessage());
    }catch(InvalidJobConfException je){
      LOG.error(""Error launching job , Invalid job conf : "" + je.getMessage());
    }catch(FileAlreadyExistsException fae){
      LOG.error(""Error launching job , Output path already exists : "" 
                + fae.getMessage());
    }catch(IOException ioe){
      LOG.error(""Error Launching job : "" + ioe.getMessage());
    }
    finally {
      if (error && (running_ != null)) {
        LOG.info(""killJob..."");
        running_.killJob();
      }
      jc_.close();
    }
    */
    // ecw - end
  }"
HADOOP-1961,"-get, -copyToLocal fail when  single filename is passed","In 0.14.1 and in trunk, when I try 

% hadoop dfs -get /user/knoguchi/aaa  aaa

get: Failed to rename tmp file to local destination ""aaa"".  Remote source file ""/user/knoguchi/aaa"" is saved to ""/tmp/_copyToLocal_aaa30478"".


This works. 

% hadoop dfs -get /user/knoguchi/aaa  ./aaa

or 

% hadoop dfs -get /user/knoguchi/aaa   /home/knoguchi/aaa

My guess. With change in HADOOP-1292, it now creates a tmp file when -copyToLocal.
When destination path is passed without any directory, tmp file is created under '/tmp'. Otherwise, it uses the same directory as the destination path.

In Java API for File.renameTo, 
http://java.sun.com/javase/6/docs/api/java/io/File.html#renameTo(java.io.File)
it says 
"" The rename operation might not be able to move a file from one filesystem to another"", 

so renameTo call from /tmp/_tmpfile to /home/knoguchi can fail."
HADOOP-1959,Use of File.separator in StatusHttpServer prevents running Junit tests inside eclipse on Windows,"There is exactly one use of File.separator in StatusHttpServer. In every other instance, a literal ""/"" is used.

When running a Junit test inside eclipse on Windows, the the jetty web server fails to start due to the following exception:

java.io.FileNotFoundException: jar:file:/C:/workspace/hadoop-commit/build/hadoop-0.15.0-dev-core.jar!/webapps\dfs

Changing File.separator to ""/"" enables the test to be run, and unit tests run from ant on the command line still work.
"
HADOOP-1955,Corrupted block replication retries for ever,"When replicating corrupted block, receiving side rejects the block due to checksum error. Namenode keeps on retrying (with the same source datanode).
Fsck shows those blocks as under-replicated.


[Namenode log]
{noformat} 
2007-09-27 02:00:05,273 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.heartbeatCheck: lost heartbeat from 99.2.99.111
...
2007-09-27 02:01:02,618 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask 99.9.99.11:9999 to replicate blk_-5925066143536023890 to datanode(s) 99.9.99.37:9999
2007-09-27 02:10:03,843 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_-5925066143536023890
2007-09-27 02:10:08,248 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask 99.9.99.11:9999 to replicate blk_-5925066143536023890 to datanode(s) 99.9.99.35:9999
2007-09-27 02:20:03,848 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_-5925066143536023890
2007-09-27 02:20:08,646 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask 99.9.99.11:9999 to replicate blk_-5925066143536023890 to datanode(s) 99.9.99.19:9999
(repeats)
{noformat} 

[Datanode(sender) 99.9.99.11 log]
{noformat} 
2007-09-27 02:01:04,493 INFO org.apache.hadoop.dfs.DataNode: Starting thread to transfer block blk_-5925066143536023890 to [Lorg.apache.hadoop.dfs.DatanodeInfo;@e58187
2007-09-27 02:01:05,153 WARN org.apache.hadoop.dfs.DataNode: Failed to transfer blk_-5925066143536023890 to 74.6.128.37:50010 got java.net.SocketException: Connection reset
  at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
  at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
  at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
  at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
  at java.io.DataOutputStream.write(DataOutputStream.java:90)
  at org.apache.hadoop.dfs.DataNode.sendBlock(DataNode.java:1231)
  at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1280)
  at java.lang.Thread.run(Thread.java:619)
(repeats)
{noformat} 

[Datanode(one of the receiver) 99.9.99.37 log]
{noformat} 
2007-09-27 02:01:05,150 ERROR org.apache.hadoop.dfs.DataNode: DataXceiver: java.io.IOException: Unexpected checksum mismatch while writing blk_-5925066143536023890 from /74.6.128.33:57605
  at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:902)
  at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:727)
  at java.lang.Thread.run(Thread.java:619)
{noformat} "
HADOOP-1954,start task tracker and stop task tracker scripts,we should have scripts for starting and stopping just the task tracker
HADOOP-1953,the job tracker should wait beteween calls to try and delete the system directory,"In HADOOP-1819, I changed the start up of the JobTracker and inadvertently made the Job Tracker retry safe mode exceptions immediately. It should sleep between the exceptions to slow down the number of exceptions and noise."
HADOOP-1952,Streaming does not handle invalid -inputformat  (typo by users for example),"Hadoop Streaming does not handle invalid inputformat class. For example -inputformat INVALID class would not be thrown as an error. Instead it defaults to StreamInputFormat. If an invalid inputformat is specified, it is good to fail. 

"
HADOOP-1951,"bin/hadoop dfs -ls returns ""No such file or directory"" for empty directory","according to the browseDirectory.jsp 
my <user's temp dir>/mapred/system is an empty directory
according to 
bin/hadoop dfs -ls ""No such file or directory"" 

They can't both be right."
HADOOP-1949,NPE in IPC handler.,"Noticed a few of the following traces during upgrade of a large cluster to 0.14.1 :

{noformat}
2007-09-26 17:21:40,134 WARN org.apache.hadoop.ipc.Server: IPC Server handler 20 on 8020, call 
processUpgradeCommand(org.apache.hadoop.dfs.BlockCrcUpgradeUtils$CrcInfoCommand@2b9c5c9d) 
from 192.0.0.100:40500: output error
java.lang.NullPointerException
        at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:585)
{noformat}


"
HADOOP-1948,Spurious error message during block crc upgrade.,"For blocks with zero size, Namenode prints following warning :

2007-09-26 17:13:04,620 WARN org.apache.hadoop.fs.FSNamesystem: blockCrcInfo(): blk_-3329852458225636700 could not be found in blocks for /user/rangadi/filename .

Fortunately this not harmful. But should be fixed. I think this should go into 0.14.2. Fix is simple."
HADOOP-1946,du should be not called on every heartbeat,"Data-nodes run very slow. The reason is that du is executed on each and every heartbeat.
Looks like the condition in DU.getUsed(), which triggers the refresh of cached values, should be reversed."
HADOOP-1945,pipes examples aren't in the release,"The pipes examples are not copied into the release. They should be included by:

ant -Dcompile.c++=yes package"
HADOOP-1944,Maps which ran on trackers declared 'lost' are being marked as FAILED rather than KILLED,"There seems to be a bug in {{TaskInProgress.taskKilled}} which checks if that taskid was in {{tasksToKill}}, if not, it just marks the taskid as FAILED. This is wrong since tasks can also be KILLED due to *lost task-trackers* and hence won't be found in {{tasksToKill}}.

This clearly leads to a wrongly penalizing user-jobs."
HADOOP-1942,Increase the concurrency of transaction logging to edits log,"For some typical workloads, the throughput of the namenode is bottlenecked by the rate of transactions that are being logged into tghe edits log. In the current code, a batching scheme implies that all transactions do not have to incur a sync of the edits log to disk. However, the existing batch-ing scheme can be improved.

One option is to keep two buffers associated with edits file. Threads write to the primary buffer while holding the FSNamesystem lock. Then the thread release the FSNamesystem lock, acquires a new lock called the syncLock, swaps buffers, and flushes the old buffer to the persistent store. Since the buffers are swapped, new transactions continue to get logged into the new buffer. (Of course, the new transactions cannot complete before this new buffer is sync-ed).

This approach does a better job of batching syncs to disk, thus improving performance.

"
HADOOP-1940,TestDFSUpgradeFromImage doesn't shut down its MiniDFSCluster,"TestDFSUpgradeFromImage doesn't call shutdown() when it's finished with its MiniDFSCluster, so its resources aren't reclaimed before it exits causing subsequent tests to fail."
HADOOP-1939,Need extensive shell interface,"My project, Pig, provides an interactive shell for users where, among other things, they can do some basic DFS operations such as changing firectories, copying files, etc. It would be great if Hadoop could provide a consistent interface to support basic shell operations."
HADOOP-1938,NameNode.create failed ,"
Under heavy load, DFS namenode fails to create file

org.apache.hadoop.ipc.RemoteException: java.io.IOException: Failed to create file /xxx/xxx/_task_0001_r_000001_0/part-00001 on client xxx.xxx.xxx.xxx because there were not enough datanodes available. Found 0 datanodes but MIN_REPLICATION for the cluster is configured to be 1.
	at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:651)
	at org.apache.hadoop.dfs.NameNode.create(NameNode.java:294)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)

The above problem occurred when I ran a well tuned map/reduce program on a hood node cluster.
The program is well tuned in the sense that the map output data are evenly partitioned among 180 reducers.
The shuffling and sorting was completed at about the same time on all the reducers.
The reducers started reduce work at about the same time and were expected to produce about the same amount of output (2GB).
This ""synchronized"" behavior caused  the reducers to try to create output dfs files at about the same time.
The namenode seemed to have difficulty to handle that situation, causing the reducers waiting on file creation for long period of time.
Eeventually, they failed with the above exception.


 "
HADOOP-1935,NullPointerException in internalReleaseCreate,"The exception occurs during abandonFileInProgress().
{code}
07/09/21 13:59:28 ERROR fs.FSNamesystem: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSNamesystem.internalReleaseCreate(FSNamesystem.java:1494)
        at org.apache.hadoop.dfs.FSNamesystem.access$100(FSNamesystem.java:50)
        at org.apache.hadoop.dfs.FSNamesystem$Lease.releaseLocks(FSNamesystem.java:1385)
        at org.apache.hadoop.dfs.FSNamesystem$LeaseMonitor.run(FSNamesystem.java:1447)
        at java.lang.Thread.run(Thread.java:595)
{code}
My guess is that the client sends abandonFileInProgress() after create(), which timed out and was not executed by the name-node.
That is why dir.getFileINode(src) returns null in internalReleaseCreate().
"
HADOOP-1934,"the os.name string on Mac OS contains spaces, which causes the c++ compilation to fail","The os.name contains spaces, which means that the build.platform contains spaces, which leads to mis-parsed commands from ant. By using sed, we can replace the spaces with underscores."
HADOOP-1933,Consider include/exclude files while listing datanodes.,"While listing datanodes (e.g. in webui), consider the nodes listed in include and exclude files.

Proposed Rules :

# If a node is listed in include file, it should be listed
# If a node is listed under exclude file, it should not be listed unless it is not 'dead'.

Dhruba implies only the first rule applies towards the end of HADOOP-1138, i.e., a node should be listed if it exists either of the files. Dhruba, please clarify.
"
HADOOP-1932,Test dfs.TestFileCreation.testFileCreation failed on Windows,"Fails with this assert error:

junit.framework.AssertionFailedError: filestatus.dat should be of size 16384
	at org.apache.hadoop.dfs.TestFileCreation.testFileCreation(TestFileCreation.java:137)"
HADOOP-1930,Too many fetch-failures issue,"A job with 4000 maps on a 1400 node cluster (3 tasks per node allowed) had a lot (150) of 'Too many fetch-failures' map failures.

From the jobtracker log it looks as if it got confused which tasktracker actually ran the task:

(In the following log output, I replaced the corresponding tasktracker nodes with ***node_assigned*** and ***node_fetch_attempt** and they are different)

grep task_200709170247_0018_m_000009_0 hadoop-xxx-jobtracker-node.log.2007-09-19:

2007-09-19 15:52:26,907 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_200709170247_0018_m_000009_0' to tip tip_200709170247_0018_m_000009, for tracker 'tracker_***node_assigned_***:/127.0.0.1:54523'
2007-09-19 15:58:03,111 INFO org.apache.hadoop.mapred.TaskRunner: Saved output of task 'task_200709170247_0018_m_000009_0' to hdfs://location
2007-09-19 15:58:03,111 INFO org.apache.hadoop.mapred.JobInProgress: Task 'task_200709170247_0018_m_000009_0' has completed tip_200709170247_0018_m_000009 successfully.
2007-09-19 15:58:03,111 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_200709170247_0018_m_000009_0' has completed succesfully
2007-09-19 16:21:07,825 INFO org.apache.hadoop.mapred.JobInProgress: Failed fetch notification #1 for task task_200709170247_0018_m_000009_0
2007-09-19 16:23:23,483 INFO org.apache.hadoop.mapred.JobInProgress: Failed fetch notification #2 for task task_200709170247_0018_m_000009_0
2007-09-19 16:25:07,182 INFO org.apache.hadoop.mapred.JobInProgress: Failed fetch notification #3 for task task_200709170247_0018_m_000009_0
2007-09-19 16:25:07,182 INFO org.apache.hadoop.mapred.JobInProgress: Too many fetch-failures for output of task: task_200709170247_0018_m_000009_0 ... killing it
2007-09-19 16:25:07,182 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_200709170247_0018_m_000009_0: Too many fetch-failures
2007-09-19 16:25:07,182 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_200709170247_0018_m_000009_0' has been lost.
2007-09-19 16:25:07,184 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_200709170247_0018_m_000009_0' from 'tracker_***node_fetch_attempt***:/127.0.0.1:48818'
2007-09-19 21:40:00,235 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_200709170247_0018_m_000009_0' from 'tracker_***node_fetch_attempt***:/127.0.0.1:48818'

"
HADOOP-1927,a datanode in a write pipeline should report an error if the next datanode in the pipeline reported an error,Currently a client receives an error only when the first datanode in the pipeline failes to write the block to the local disk. A client receive a success even if rest of the writes in the pipeline  have failed. The problem with the current approach is that  the client is not able to detect if it failed to create the desired number of replicas. 
HADOOP-1926,Design/implement a set of compression benchmarks for the map-reduce framework,"It would be nice to benchmark various compression codecs for use in the hadoop (existing codecs like zlib, lzo and in-future bzip2 etc.) and run these along with our nightlies or weeklies.

Here are some steps:
a) Fix HADOOP-1851 ( Map output compression codec cannot be set independently of job output compression codec)
b) Implement a random-text-writer along the lines of examples/randomwriter to generate large amounts of synthetic textual data for use in sort. One way to do this is to pick a word randomly from {{/usr/share/dict/words}} till we get enough bytes per map. To be safe, we could store an array of Strings of a snap-shot of the words in examples/RandomTextWriter.java.
c) Take a dump of wikipedia (http://download.wikimedia.org/enwiki/) and/or the ebooks from Project Gutenberg (http://www.gutenberg.org/MIRRORS.ALL) and use them as non-synthetic data to run sort/wordcount against.

For both b) and c) we should setup nightly/weekly benchmark runs with different codecs for reduce-outputs and map-outputs (shuffle) and track each.

Thoughts?"
HADOOP-1925,Hadoop Pipes doesn't compile on solaris,"Since the build process runs on a solaris machine, it would be good if the pipes module would be built and tested as part of the build process."
HADOOP-1921,Save the configuration of completed/failed jobs and make them available via the web-ui.,Today we only have the ability to inspect the configuration of currently running jobs via jobdetails.jsp. It would greatly aid in debugging if we could also see it for completed/failed jobs via jobdetails.jsp and also via JobHistory.
HADOOP-1917,Need configuration guides for Hadoop,"We've recently had a spate of questions on the users list regarding features such as rack-awareness, the trash can etc. which are not clearly documented from a user/admins perspective. There is some Javadoc present but most of the ""documentation"" exists either in JIRA or in the default config files themselves.

We should generate top down configuration and use guides for map/reduce and HDFS. These should probably be in forest and accessible from the project website (Javadoc isn't always approachable to our non-programmer audience). Committers should look for user documentation before accepting patches."
HADOOP-1915,adding counters methods using String (as opposed to Enum),"Currently to use the counters from within Map/Reduce code Enums have to be used, the Enum class defines the group and the Enum itself the counter. Internally they are converted to Strings (the class name and the enum toString) and you can retrieve them as strings from the client API.

Using dynamic counters (driven by configuration of the map/reduce) is not easy with the counters Enum based API. For example, currently I have an Enum class with 50 enums and we have to map the cardinality to the counter name on the client. This is cumbersome.

This could be easily improve by adding a String based counter method increment(String group, String counter, long count) to allow use of the counters without Enums.

Internally this method already exists, so the changes are minimal.

"
HADOOP-1914,HDFS should have a NamenodeProtocol to allow  secondary namenodes and rebalancing processes to communicate with a primary namenode,"For the security purpose, it is nice to have a NamenodeProtocol to allow secondary namenodes and rebalacing processes to communicate with a primary namenode. In the first version the NamenodeProtocol is going to support one RPC:
   BlockLocations[] getBlocks(DatanodeID datanode, long size);
This RPC allows a rebalancing process to fetch a partial blocks map from a namenode. It returns a list of blocks on the given datanode and its locations, whose total size is the given size."
HADOOP-1912,Datanode should support block replacement,"This jira Data Node's support for rebalancing (HADOOP-1652). When a balancer decides to move a block B from Source S to Destination D. It also chooses a proxy source PS, which contains a replica of B, to speed up block copy.  The block placement is carried in the following steps:
1. A block copy command is sent to datanode PS in the format of  ""OP_BLOCK_COPY <block_id_of_B> <source S> <destination D>"". It requests PS to copy B to datanode D.
2. PS then transfers block B to datanode D with a block replacement command to D in the format of ""OP_BLOCK_REPLACEMENT <block_id_of_B> <source S> <data_of_B>"". 
3. Datanode D writes the block B to its disk and then sends a name node a blockReceived RPC informing the namenode that a block B is received and please delete a replica of B from source S if there is any excessive replica.
4. The namenode then adds datanode D to block B's map and removes an exesive replicas of B in favor of datanode S.

In addition, each data node has a limited bandwidth for rebalancing. The default value for the bandwidth is 5MB/s. Throttling is done at both source & destination sides. Each data node limits maximum number of concurrent data transfers (including both sending and receiving) for the rebalancing purpose to be 5. In the worst case, each data transfer has a limited bandwidth of 1MB/s. Each sender & receiver has a Throttler. The primary method of the class is ""throttle( int numOfBytes )"". The parameter numOfBytes indicates the total number of bytes that the caller has sent or received since the last throttle is called. The method calculates the caller's I/O rate. If the rate is faster than the bandwidth limit, it sleeps to slow down the data transfer. After it wakes up, it adjusts its bandwidth limit if the number of concurrent data transfers is changed. "
HADOOP-1911,infinite loop in dfs -cat command.,"[knoguchi]$ hadoop dfs -cat fileA
07/09/13 17:36:02 INFO fs.DFSClient: Could not obtain block 0 from any node: 
java.io.IOException: No live nodes contain current block
07/09/13 17:36:20 INFO fs.DFSClient: Could not obtain block 0 from any node: 
java.io.IOException: No live nodes contain current block
[repeats forever]

Setting one of the Debug statement to Warn, it kept on showing 
{noformat} 
 WARN org.apache.hadoop.fs.DFSClient: Failed to connect
to /99.99.999.9 :11111:java.io.IOException: Recorded block size is 7496, but
datanode reports size of 0
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:690)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:771)
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at java.io.DataInputStream.readFully(DataInputStream.java:178)
	at java.io.DataInputStream.readFully(DataInputStream.java:152)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.(ChecksumFileSystem.java:123)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:340)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:259)
	at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:466)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:186)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1707)
{noformat} 


Turns out fileA was corrupted. Fsck showed crc file of 7496 bytes, but when I searched for the blocks on each node, 3 replicas were all size 0.

Not sure how it got corrupted, but it would be nice if the dfs command fail instead of getting into an infinite loop.

"
HADOOP-1910,Extra checks in DFS.create() are not necessary.,"{{DistributedFileSystem.create(path)}} like this :
{code}
  public FSDataOutputStream create(Path f, boolean overwrite,
    int bufferSize, short replication, long blockSize,
    Progressable progress) throws IOException {
    if (exists(f) && !overwrite) {
      throw new IOException(""File already exists:""+f);
    }
    Path parent = f.getParent();
    if (parent != null && !exists(parent) && !mkdirs(parent)) {
      throw new IOException(""Mkdirs failed to create "" + parent);
    }

    return new FSDataOutputStream( dfs.create(getPathName(f), overwrite, 
                                              replication, blockSize, 
                                              progress, bufferSize, ticket));
  }
{code}

This has overhead of 2-3 RPCs to namenode for every create(). The first {{exists()}} is not required because {{overwrite}} flag is passed to Namenode. The second {{exists()}} and {{mkdirs()}} is not required since {{create()}} already does this."
HADOOP-1908,Restructure data node code so that block sending/receiving is seperated from data transfer header handling,"This jira is intended for code sharing. I'd like to have a BlockSender which is resposible for reading a block from a disk and writing it to an output stream, and a BlockReceiver which receives a block from an input stream, writes it to a disk,  and possibly writes it a mirror ouput stream to support pipeline writes. Block sender and receiver code is independent of any block IO protocols so it could be shared by block transfer, block write, block read, and future block replacement introduced by data node rebalancing."
HADOOP-1907,JobClient.runJob kills the job for failed tasks with no diagnostics,"As a follow-up to HADOOP-1892, when a task fails with null taskDiagnostics, runJob throws an exception and kills the whole job."
HADOOP-1906,JobConf should warn about the existance of obsolete mapred-default.xml.,"Since the mapred-default.xml is ignored after HADOOP-785, we should generate a warning when it is present. Otherwise users forget to move the values to hadoop-site.xml and get  confused when things don't work right."
HADOOP-1904,ArrayIndexOutOfBoundException in BlocksMap,"If the name-node receives a duplicate report of a written block from a data-node it does not insert
the block into the block map but mistakenly inserts it into the list of blocks belonging to the data-node,
placing it into the beginning of the list. Since the block is not removed from the list prior to the insertion,
the list itself becomes corrupted.
This patch fixes the problem. I also reorganized the add/removeNode and add/removeBlock methods
in order to avoid similar bugs in the future.
"
HADOOP-1902,du command throws an exception when the directory is not specified,"Running the du command without specifying a directory throws an exception. It runs fine if a directory is specified. Here is the output

mukundm@ucdev28 hadoop]$ bin/hadoop dfs -du
Exception in thread ""main"" java.lang.IllegalArgumentException: Can not create a Path from an empty string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
        at org.apache.hadoop.fs.Path.<init>(Path.java:90)
        at org.apache.hadoop.fs.FsShell.du(FsShell.java:351)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:1040)
        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:1092)

[mukundm@ucdev28 hadoop]$ bin/hadoop dfs -du .
Found 2 items
/user/mukundm/input     28578
/user/mukundm/output    747"
HADOOP-1901,"JobTracker.stopTracker() is not invoked when shutting down JobTracker using bin/stop-{all,mapred}.sh",This can be fixed by adding a  shutdown hook to the JobTracker RunTime that invokes stopTracker().
HADOOP-1900,the heartbeat and task event queries interval should be set dynamically by the JobTracker,"The JobTracker should scale the intervals that the TaskTrackers use to contact it dynamically, based on how the busy it is and the size of the cluster."
HADOOP-1898,locking for the ReflectionUtils.logThreadInfo is too conservative,"When the RPC servers get into trouble with their call queues backing up, they occasionally dump the call stacks. These are very useful for identifying hot spots, but the locking is too conservative and so all of the handlers are blocked while the thread call stacks are dumped."
HADOOP-1897,about.html page is there but not linked. ,"about.html page for http://lucene.apache.org is deployed, but no other page links to it. "
HADOOP-1895,sort fails with OutOfMemoryExceptions,"sort100
> 
> java.lang.OutOfMemoryError: Java heap space at
> java.util.Arrays.copyOf(Arrays.java:2786) at
> java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94) at
> java.io.DataOutputStream.write(DataOutputStream.java:90) at
> org.apache.hadoop.io.BytesWritable.write(BytesWritable.java:137) at
> org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTa
> sk.java:349)
> at
> org.apache.hadoop.mapred.lib.IdentityMapper.map(IdentityMapper
> .java:40)
> at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50) at
> org.apache.hadoop.mapred.MapTask.run(MapTask.java:192) at
> org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1785)
> "
HADOOP-1894,Add fancy graphs for mapred task statuses,I whould like to add graphics for mapred task statuses. 
HADOOP-1892,"In the Job UI, some links don't work",I think some problems got introduced in the Job UI after the patch for HADOOP-1592 got committed. Accessing some of the links (to do with tasks) in the Job UI results in NPE on the JobTracker.
HADOOP-1891,"""."" is converted to an empty path","Path p = new Path(""."");
System.out.println(""path=("" + p.toString() +"")"");

 path =()"
HADOOP-1890,Revert a debug patch.,"
I attached a patch with extra debug code for HADOOP-1774 by mistake. This jira fixes it.
"
HADOOP-1889,Fix path in EC2 scripts for building your own AMI,
HADOOP-1887,ArrayIndexOutOfBoundsException with trunk,"
How to reproduce : 

# Run a DFS cluster (single node is fine). 
# Start writing a file but do not close it : {{cat | bin/hadoop dfs -copyFromLocal - /dir/file}} does that.
# goto /dir on Hadoop webui or do {{bin/hadoop fs -cat /dir/file}}.

This is mostly related to HADOOP-1708."
HADOOP-1886,Undocumented parameters in FilesSystem,"Multiple create methods in public FileSystem class lack documentation for the following 2 parameters.
- long blockSize,
- Progressable progress
"
HADOOP-1885,Race condition in MiniDFSCluster shutdown,"Hudson has been sporadically failing tests that start- or follow tests that start- multiple datanodes in MiniDFSCluster, particularly on Solaris and Windows. The following appears to be at least partially responsible (much credit to Nigel for helping to discern this).

A common error:
{noformat}
java.io.IOException: Cannot remove data directory: /export/home/hudson/hudson/jobs/Hadoop-Nightly/workspace/trunk/build/test/data/dfs/data
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:126)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:80)
	at org.apache.hadoop.dfs.TestFsck.testFsckNonExistent(TestFsck.java:96)
{noformat}

MiniDFSCluster starts multiple DataNodes by calling DataNode::createDataNode, which creates and starts a DataNode thread, assigns the instance to a static member, and returns the Runnable. Of course, each call from MiniDFSCluster overwrites this instance. Since DataNode::shutdown() calls join() on the same Thread, each subsequent join is essentially a noop after the first DataNode finishes. When MiniDFSCluster::shutdown() returns, it may not have released its resources, so the next MiniDFSCluster may fail to start."
HADOOP-1882,Remove extra '*'s from FsShell.limitDecimal(),"
Since HADOOP-1463 I have been seeing extra '*'s on DFS front page etc. This affected everything that uses {{FsShell.limitDecimal()}}. I will attach a patch.
"
HADOOP-1880,SleepJob,This is an example job that sleeps for some defined time at each map and reduce. 
HADOOP-1879,Warnings With JDK1.6.0_02,Received 374 warnings while compiling Hadoop.
HADOOP-1878,Change priority feature in the job details JSP page misses spaces between each priority link,
HADOOP-1877,it is not possible to make a job fail without retries,"
If a job task fails due to no transient reasons (For example, configuration for the job is not correct) Hadoop will retry the failed tasks as many times as retries have been configured. And it will fail again and again.

There should be an JobKillException -thrown by configure, map and reduce methods- that would make Hadoop not to retry the task and kill the job. 
"
HADOOP-1876,Persisting completed jobs status,"Currently the JobTracker keeps information about completed jobs in memory. 

This information is  flushed from the cache when it has outlived (#RETIRE_JOB_INTERVAL) or because the limit of completed jobs in memory has been reach (#MAX_COMPLETE_USER_JOBS_IN_MEMORY). 

Also, if the JobTracker is restarted (due to being recycled or due to a crash) information about completed jobs is lost.

If any of the above scenarios happens before the job information is queried by a hadoop client (normally the job submitter or a monitoring component) there is no way to obtain such information.

A way to avoid this is the JobTracker to persist in DFS the completed jobs information upon job completion. This would be done at the time the job is moved to the completed jobs queue. Then when querying the JobTracker for information about a completed job, if it is not found in the memory queue, a lookup  in DFS would be done to retrieve the completed job information. 

A directory in DFS (under mapred/system) would be used to persist completed job information, for each completed job there would be a directory with the job ID, within that directory all the information about the job: status, jobprofile, counters and completion events.

A configuration property will indicate for how log persisted job information should be kept in DFS. After such period it will be cleaned up automatically.

This improvement would not introduce API changes.
"
HADOOP-1875,multiple dfs.client.buffer.dir directories are not treated as alternatives,"When specifying multiple directories in the value for dfs.client.buffer.dir, jobs fail when the selected directory does not exist or is not writable. Correct behaviour should be to create the directory when it does not exist and fail over to an alternative directory when it is not writable."
HADOOP-1874,lost task trackers -- jobs hang,"This happens on a 1400 node cluster using a recent nightly build patched with HADOOP-1763 (that fixes a previous 'lost task tracker' issue) running a c++-pipes job with 4200 maps and 2800 reduces. The task trackers start to get lost in high numbers at the end of job completion.

Similar non-pipes job do not show the same problem, but is unclear whether it is related to c++-pipes. It could also be dfs overload when reduce tasks close and validate all newly created dfs files. I see dfs client rpc timeout exception. But this alone does not explain the escalation in losing task trackers.

I also noticed that the job tracker becomes rather unresponsive with rpc timeout and call queue overflow exceptions. Job Tracker is running with 60 handlers."
HADOOP-1873,User permissions for Map/Reduce,"HADOOP-1298 and HADOOP-1701 add permissions and pluggable security for DFS files and DFS accesses. Same users permission should work for Map/Reduce jobs as well. 

User persmission should propegate from client to map/reduce tasks and all the file operations should be subject to user permissions. This is transparent to the user (i.e. no changes to user code should be required). "
HADOOP-1869,access times of HDFS files,"HDFS should support some type of statistics that allows an administrator to determine when a file was last accessed. 

Since HDFS does not have quotas yet, it is likely that users keep on accumulating files in their home directories without much regard to the amount of space they are occupying. This causes memory-related problems with the namenode.

Access times are costly to maintain. AFS does not maintain access times. I thind DCE-DFS does maintain access times with a coarse granularity.

One proposal for HDFS would be to implement something like an ""access bit"". 
1. This access-bit is set when a file is accessed. If the access bit is already set, then this call does not result in a transaction.
2. A FileSystem.clearAccessBits() indicates that the access bits of all files need to be cleared.

An administrator can effectively use the above mechanism (maybe a daily cron job) to determine files that are recently used.
"
HADOOP-1866,distcp requires large heapsize when copying many files,"Trying to distcp 1.5 million files with 1G client heapsize,  failed with outofmemory.


Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit
exceeded
       at java.util.regex.Pattern.compile(Pattern.java:1438)
       at java.util.regex.Pattern.<init>(Pattern.java:1130)
       at java.util.regex.Pattern.compile(Pattern.java:846)
       at java.lang.String.replace(String.java:2208)
       at org.apache.hadoop.fs.Path.normalizePath(Path.java:147)
       at org.apache.hadoop.fs.Path.initialize(Path.java:137)
       at org.apache.hadoop.fs.Path.<init>(Path.java:126)
       at org.apache.hadoop.dfs.DfsPath.<init>(DfsPath.java:32)
       at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.listPaths(DistributedFileSystem.java:214)
       at org.apache.hadoop.fs.FileSystem.listPaths(FileSystem.java:483)
       at org.apache.hadoop.fs.FileSystem.listPaths(FileSystem.java:496)
       at org.apache.hadoop.fs.ChecksumFileSystem.listPaths(ChecksumFileSystem.java:539)
       at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.setup(CopyFiles.java:327)
       at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:762)
       at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:808)
       at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)
       at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:818)

It would be nice if distcp doesn't require gigs of heapsize when copying large amount of files.


"
HADOOP-1865,"""org.apache.hadoop.metrics.jvm.EventCounter"" not instantiate error","Hi, 

I got the ""org.apache.hadoop.metrics.jvm.EventCounter"" not instantiate error. This error happens for every hadoop command. But it seems it does not block any operation to success. Don't know if anyone has an idea?
"
HADOOP-1864,Support for big jar file (>2G),"We have huge size binary that need to be distributed onto tasktracker nodes in Hadoop streaming mode. We've tried both -file option and -cacheArchive option. It seems the tasktracker node cannot unjar jar files bigger than 2G. We are considering split our binaries into multiple jars, but with -file, it seems we cannot do it. Also, we would prefer -cacheArchive option for performance issue, but it seems -cacheArchive does not allow more than appearance in the streaming options. Even if -cacheArchive support multiple jars, we still need a way to put the jars into a single directory tree, instead of using multiple symbolic links. 

So, in general, we need a feasible and efficient way to update large size (>2G) binaries for Hadoop streaming. Don't know if there is an existing solution that we either didn't find or took it wrong. Or there should be some extra work to provide a solution?"
HADOOP-1858," "".."" for ls does not check for existence of its parent directory.","'{{bin/hadoop -ls /user/nonexistent/..}}' lists /user even if /user/nonexistent  directory does not exist.
"
HADOOP-1857,Ability to run a script when a task fails to capture stack traces,"This basically is for providing a better user interface for debugging failed
jobs. Today we see stack traces for failed tasks on the job ui if the job
happened to be a Java MR job. For non-Java jobs like Streaming, Pipes, the
diagnostic info on the job UI is not helpful enough to debug what might have
gone wrong. They are usually framework traces and not app traces.
We want to be able to provide a facility, via user-provided scripts, for doing
post-processing on task logs, input, output, etc. There should be some default
scripts like running core dumps under gdb for locating illegal instructions,
the last few lines from stderr, etc.  These outputs could be sent to the
tasktracker and in turn to the jobtracker which would then display it on the
job UI on demand.

"
HADOOP-1855,fsck should verify block placement,fsck currently detects missing and under-replicated blocks. It would be helpful if it can also detect blocks that do not conform to the block placement policy. An administrator can use this tool to verify that blocks are distributed across racks.
HADOOP-1853,multiple -cacheFile option in hadoop streaming does not seem to work ,"Specifying one -cacheFile option in hadoop streaming works. Specifying more than one, gives a parse error. A patch to fix this and a unit test to test the fix has been attached with this bug. "
HADOOP-1851,Map output compression codec cannot be set independently of job output compression codec,"The property ""mapred.output.compression.codec"" is used when setting and getting the map output compression codec in JobConf, thus making it impossible to use a different codec for map outputs and overall job outputs."
HADOOP-1848,Redesign of Eclipse plug-in interface with Hadoop,The current Eclipse plug-in connects to Hadoop via shell scripts remotely executed using SSH and raw string marshaling. This is very inefficient and hard to maintain. The purpose of this issue is to let the plug-in directly use Hadoop's client API.
HADOOP-1846,DatanodeReport should distinguish live datanodes from dead datanodes,"DatanodeReport returns both live and dead datanodes and there is no way to distinguish live nodes from dead nodes. But some applications are interested in only live datanodes.  I propose that instead of haveing one datanodeReport, dfs supports two reports: liveDatanodeReport and deadDatanodeReport."
HADOOP-1843,Remove deprecated code in Configuration/JobConf,Remove Configuration/JobConf apis deprecated by HADOOP-785 in in the 0.15.0 release - in particular: Configuration.add{Default|Final}Resource.
HADOOP-1841,IPC server should write repsonses asynchronously,Hadoop's IPC Server currently writes responses from request handler threads using blocking writes.  Performance and scalability might be improved if responses were written asynchronously.
HADOOP-1840,Task's diagnostic messages are lost sometimes,"HADOOP-1158 (scheduled for 0.15.0) introduced a bug where sometimes the task's diagnostic's messages sent via {{TaskUmbilicalProtocol.reportDiagnosticInfo}} are lost. This is due to them being not being saved properly and being overridden in {{TaskStatus.statusUpdate}}, simple fix is to append the prev  diagnostic message."
HADOOP-1839,Link-ify the Pending/Running/Complete/Killed tasks/task-attempts on jobdetails.jsp,Ensuring that we can quickly view the Pending/Running/Complete/Killed tasks and task-attempts from jobdetails.jsp helps to narrow down and hence makes it easier to debug.
HADOOP-1838,"Files created with an pre-0.15 gets blocksize as zero, causing performance degradation","HADOOP-1656 introduced the support for storing block size persistently as inode metadata. Previously, if the file has only one block then it was not possible to accurately determine the blocksize that the application has requested at file-creation time.

The upgrade of an older layout to the new layout kept the blocksize as zero for single-block files that were upgraded to the new layout. This was done to indicate the DFS really does not know the ""true"" blocksize of this file. This caused map-reduce to determine that a split is 1 byte in length!

"
HADOOP-1837,Insufficient space exception from InMemoryFileSystem after raising fs.inmemory.size.mb,"trying out larger in-memory file system (curious if that helped speed the sort phase). in this run - i had sized it to 500MB. There's plenty of RAM in the machine (8GB) and the tasks are launched with -Xmx2048 option (so there's plenty of heap space as well). However - observing this exception:

2007-09-04 13:47:51,718 INFO org.apache.hadoop.mapred.ReduceTask: task_0002_r_000002_0 Copying task_0002_m_000124_0 output from hadoop004.sf
2p.facebook.com.
2007-09-04 13:47:52,188 WARN org.apache.hadoop.mapred.ReduceTask: task_0002_r_000002_0 copy failed: task_0002_m_000124_0 from hadoop004.sf2p
.facebook.com
2007-09-04 13:47:52,189 WARN org.apache.hadoop.mapred.ReduceTask: java.io.IOException: Insufficient space
        at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryOutputStream.write(InMemoryFileSystem.java:181)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:91)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.close(ChecksumFileSystem.java:416)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:48)
        at org.apache.hadoop.fs.FSDataOutputStream$Buffer.close(FSDataOutputStream.java:72)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:92)
        at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:251)
        at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:680)
        at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:641)

2007-09-04 13:47:52,189 WARN org.apache.hadoop.mapred.ReduceTask: task_0002_r_000002_0 adding host hadoop004.sf2p.facebook.com to penalty bo
x, next contact in 64 seconds

so this ends up slowing stuff down since we backoff on the source host (even though it's not it's fault).  Looking at the code, seems like ReduceTask is trying to write more to InMemoryFileSystem than it should.

"
HADOOP-1832,listTables() returns duplicate tables,"The listTables() method of the HConnection returned by HConnectionManager returns duplicate tables. The number of tables are correct, but the tables returned are identical."
HADOOP-1827,"Reducer.reduce method's OutputCollector is too strict, it shoudn't need the key to be WritableComparable","The output of the {{Reducer}}'s reduce method is *not* sorted, hence the {{OutputCollector}} passed to it shouldn't require the *key* to be {{WritableComparable}}; passing a {{Writable}} should suffice.

Thus

{code: title=Reducer.java}
public interface Reducer<K2 extends WritableComparable, V2 extends Writable, 
                         K3 extends WritableComparable, V3 extends Writable> 
extends JobConfigurable, Closeable {

  void reduce(K2 key, Iterator<V2> values, OutputCollector<K3, V3> output, Reporter reporter) 
  throws IOException;

}
{code}

should, technically, be:

{code: title=Reducer.java}
public interface Reducer<K2 extends WritableComparable, V2 extends Writable, 
                         K3 extends Writable, V3 extends Writable> 
extends JobConfigurable, Closeable {

  void reduce(K2 key, Iterator<V2> values, OutputCollector<K3, V3> output, Reporter reporter) 
  throws IOException;

}
{code}



Pros:
It removes an artificial limitation where it forces applications to emit <{{WritableComparable}}, {{Writable}}> pair, rather than a <{{Writable}}, {{Writable}}> pair, there-by easing some applications (I ran into a few recently... admittedly trivial ones).

Cons:
1. We now need a separate {{Combiner}} interface, since the combiner's {{OutputCollector}} *needs* to be able to sort keys, hence requires a {{WritableComparable}} - same as the {{Mapper}}.
2. We need a separate {{SortableOutputCollector}} (for {{Mapper}}/{{Combiner}}) and a {{NonSortableOutputCollector}} (for {{Reducer}}).
3. Alas! As a consequence of (1) & (2)we cannot use the same class as both a {{Reducer}} and {{Combiner}} anymore, a serious compatibility issue.



The purpose of this issue is two-fold:
1. Spark a discussion among folks, both hadoop-dev & hadoop-users, to figure if this really is a problem i.e. do folks really care about this anomaly in the existing {{Reducer}} interface? Also, is it worth the pain (@see 'Cons') to go fix it.
2. Even if we decide to live with it, this issue could record for posterity why we love hadoop, warts and all. *smile*

Lets discuss...
"
HADOOP-1825,hadoop-daemon.sh script fails if HADOOP_PID_DIR doesn't exist,"If I try to bring up a datanode on a fresh machine, it will fail with this error message:

starting datanode, logging to /b/hadoop/logs/hadoop-me-datanode-example.com.out
/p/share/hadoop/bin/hadoop-daemon.sh: line 99: /b/hadoop/pid/hadoop-me-datanode.pid: No such file or directory
"
HADOOP-1822,Allow SOCKS proxy configuration to remotely access the DFS and submit Jobs,"The purpose of this issue is to introduce a new configuration entry to setup SOCKS proxy for DFS and JobTracker clients.
This enable users to remotely access the DFS and submit Jobs as if they were directly connected to the cluster Hadoop runs on.
"
HADOOP-1819,The JobTracker should ensure that it is running on the right host.,"We had someone start up a private JobTracker with the standard config. Unfortunately, it used the system directory from the config and wiped it clean. I want to add a sanity check that makes sure the JobTracker is being run on the right node before the system directory is wiped."
HADOOP-1818,"MutliFileInputFormat returns ""empty"" MultiFileSplit when number of paths < number of splits",Coming with a patch soon.
HADOOP-1817,MultiFileSplit does not write and read the total length,"So the getLength() method always return 0 inside a running map task.
Coming soon with a patch."
HADOOP-1812,TestIPC and TestRPC should use dynamically allocated ports,TestIPC and TestRPC currently use fixed port numbers for tests.  This can fail if that port is for some reason unavailable.  They should instead let the OS allocate a free port number dynamically.
HADOOP-1810,Incorrect Value type in MRBench (SmallJobs),The value type is incorrect in MRBench.Map causing job failures with ClassCastException
HADOOP-1809,Add link to irc channel #hadoop,We can add a link to #hadoop IRC channel on irc.freenode.org. 
HADOOP-1808,Hudson should run test-contrib even if test-core fails,"Currently, if test-core fails, Hudson does not run test-contrib.

It is entirely possible that a contrib patch does not depend on the failing test-core test. If test-contrib passes even when test-core fails, a patch that only effects contrib files could be committed. At the very least, the test-results and console output would be useful."
HADOOP-1807,DfsTask lacks unit tests,"The HDFS antlib neither has unit tests written for it, nor is it part of the nightly build."
HADOOP-1806,DfsTask no longer compiles,"HADOOP-1436 changed the Tool and Toolbase APIs, but the HDFS antlib was not updated."
HADOOP-1804,TestDFSUpgrade sometimes fails,"The following test sometimes fails:

org.apache.hadoop.dfs.TestDFSUpgrade.testUpgrade
Failing for the past 1 build (since Failed#633)

junit.framework.AssertionFailedError: expected:<1790222743> but was:<3731403302>
	at org.apache.hadoop.dfs.TestDFSUpgrade.checkResult(TestDFSUpgrade.java:74)
	at org.apache.hadoop.dfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:142)

http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/633/testReport/org.apache.hadoop.dfs/TestDFSUpgrade/testUpgrade/"
HADOOP-1803,Generalize making contrib bin content executable in ant package target,"In binary distributions of hadoop, hbase bin content are not executable."
HADOOP-1795,Task.moveTaskOutputs is escaping special characters in output filenames,"after a migration from 0.10.1 to 0.14.0, jobs can't generate output files with special characters in their name, just like '[' or ']' for example, because they are escaped during the {{Task.moveTaskOutputs}} process.

For example, if you try to generate an output file named {{/foo/bar[0]}}, it ends up being named {{/foo/bar%5B0%5B}}.

The culprit is {{Task.getFinalPath()}}, when it does {{relativePath.toString()}}, where I think it should do {{relativePath.getPath()}}."
HADOOP-1788,Increase the buffer size of pipes from 1k to 128k,Currently pipes applications use 1k writes to the socket and it should be larger to increase throughput.
HADOOP-1783,keyToPath in Jets3tFileSystemStore needs to return absolute path,"The keyToPath method probably needs to:

1. take the bucket identifier as a parameter.
2. set the returned Path object's protocol plus authority (bucket). Currently, APIs such as <i>listSubPaths</i> return relative paths (for a directory listing). This in turn breaks map reduce operations if the default file system is set to be something other than S3 (via fs.default.name, for example). 



 "
HADOOP-1782,DFS File Permissions framework,"In [HADOOP-1298|https://issues.apache.org/jira/browse/HADOOP-1298] we want to provide authorization to the DFS. This issue should provide the bases to add permissions information to INodes.
In a short-term implementation, we'll focus on providing POSIX style permissions.
"
HADOOP-1779,Small INodeDirectory enhancement to get all existing INodes components on a path,"This patch introduces {{INode[] INodeDirectory.getExistingPathINodes(String path)}}, modeled over the existing {{getNode()}} / {{getINode()}}.
The purpose of this is to provide a way to retrieve all existing INodes in a path in a single tree scan.
This to allow fast INode's permission (see [HADOOP-1298|https://issues.apache.org/jira/browse/HADOOP-1298]) checking along the path, and without requiring the {{INode.parent}} field which could be removed (see [HADOOP-1687|https://issues.apache.org/jira/browse/HADOOP-1687]).
"
HADOOP-1777,Typo issue in the job details JSP page,"""Runnning"" should have only 2 'n' ;-)"
HADOOP-1775,MapWritable and SortedMapWritable - Writable problems,"When using the Writable interface for MapWritable and SortedMapWritable there are two errors:
- in readFields, if there are a number of entries of a class that is not one of the ""predefined"" classes, the following exception is thrown:
java.lang.IllegalArgumentException: Class <not predefined class name> already registered
- readFields did not set the number of non-predefined classes. Consequently, making a copy of a copy that had entries of non-predefined  classes would fail because the second copy would not receive the mapping from id to class and a NullPointerException would be thrown
"
HADOOP-1774,Remove use of INode.parent in Block CRC upgrade,"HADOOP-1687 proposes various reductions in Namenode memory. One of the 'cuts' is to remove 'parent' field in INodes (8 bytes per file). HADOOP-1743 already deprecates INode.getAbsoluteName() method. 

Block CRC Upgrade code still requires this field (blockID -> fileName -> parent -> INode for .crc file). If we don't want to use parent, we need to build INode-to-parent map at the beginning of the upgrade. Note this would increase memory required during upgrade by 50 bytes for each of non-crc files (50% of files). I would estimate this to be around 5% more memory. "
HADOOP-1772,Hadoop does not run in Cygwin in Windows ,the hostname commands are slightly different in linux and cygwin.  Will work if use $HOSTNAME
HADOOP-1771,streaming hang when IOException in MROutputThread. (NPE),"One streaming task hang and had stderr userlog as follows.

{code}
Exception in thread ""Thread-5"" java.lang.NullPointerException
         at java.lang.Throwable.printStackTrace(Throwable.java:460)
         at org.apache.hadoop.streaming.PipeMapRed$MROutputThread.run(PipeMapRed.java:352)
{code}

In PipeMapRed.java
{code}
351       } catch (IOException io) {
352         io.printStackTrace(log_);
353         outerrThreadsThrowable = io;
{code}

I guess log_ is  Null... Should call logStackTrace.

"
HADOOP-1767,JobClient CLI cleanup and improvement,"Only modifies mapred.JobClient:
 - adds a {{-list}} command: {{hadoop job -list}}, to list currently running jobs from the CLI.
 - moves and update the usage message (outdated) into a separate method
 -- {{-kill}} does not accept anything else but {{<job-id>}}
 -- {{-event}} takes 3 parameters
 - simplified and corrected the arguments processing (unused {{for}} loop, accurate checks)
 - {{-events}} did not work
 - {{JobClient.close()}} is empty; should it be removed?
"
HADOOP-1766,Merging Block and BlockInfo classes on name-node.,"In current implementation
- BlocksMap references BlockInfo
- INode references Block
- BlockInfo contains a reference to the corresponding Block

It would be better to incorporate Block into BlockInfo and reference the latter everywhere in name-node structures.
This saves 24 bytes per block on a 64-bit jvm as stated in HADOOP-1687 (5).
I retained the Block class as a structure for external (client and data-node) communication.
"
HADOOP-1765,IOException on close: Unknown file,"On occasion I get an IOE trying to close a MapFile I've just opened and filled.  Below is the content of the namenode log.  The first thing in it after startup messages is the IOE followed by other messages about '...current leaseholder is trying to recreate file'.  It repeats for ever.  This is TRUNK r567876.

I filed this issue at Raghu's suggestion.  Here's what he said on the list:

.bq Yes, the earler message was also from Namenode log. Before that line there should have been a message that adds a block to this file in the same log. I think you should file a Jira for this. It probably related to HADOOP-999. Raghu. 

{code}
2007-08-22 21:38:49,071 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/208.76.44.139:50010
2007-08-22 21:43:50,216 INFO org.apache.hadoop.fs.FSNamesystem: Roll Edit Log from 208.76.44.139
2007-08-22 21:45:21,459 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call complete(/bfd/hadoop-stack-data/tmp/hbase/compaction.tmp/hregion_hbaserepository,,8918388410463499185/repo/mapfiles/-1/data, DFSClient_1857293290) from 208.76.44.139:52301: error: java.io.IOException: Unknown file: /bfd/hadoop-stack-data/tmp/hbase/compaction.tmp/hregion_hbaserepository,,8918388410463499185/repo/mapfiles/-1/data
java.io.IOException: Unknown file: /bfd/hadoop-stack-data/tmp/hbase/compaction.tmp/hregion_hbaserepository,,8918388410463499185/repo/mapfiles/-1/data
    at org.apache.hadoop.dfs.FSDirectory.addBlocks(FSDirectory.java:561)
    at org.apache.hadoop.dfs.FSNamesystem.completeFileInternal(FSNamesystem.java:1002)
    at org.apache.hadoop.dfs.FSNamesystem.completeFile(FSNamesystem.java:952)
    at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:348)
    at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
2007-08-22 21:45:24,394 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.startFile: failed to create file /bfd/hadoop-stack-data/tmp/hbase/compaction.tmp/hregion_hbaserepository,,8918388410463499185/repo/mapfiles/-1/data for DFSClient_1857293290 on client 208.76.44.139 because current leaseholder is trying to recreate file.
2007-08-22 21:45:24,394 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call create(/bfd/hadoop-stack-data/tmp/hbase/compaction.tmp/hregion_hbaserepository,,8918388410463499185/repo/mapfiles/-1/data, DFSClient_1857293290, true, 3, 67108864) from 208.76.44.139:52312: error: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /bfd/hadoop-stack-data/tmp/hbase/compaction.tmp/hregion_hbaserepository,,8918388410463499185/repo/mapfiles/-1/data for DFSClient_1857293290 on client 208.76.44.139 because current leaseholder is trying to recreate file.
org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /bfd/hadoop-stack-data/tmp/hbase/compaction.tmp/hregion_hbaserepository,,8918388410463499185/repo/mapfiles/-1/data for DFSClient_1857293290 on client 208.76.44.139 because current leaseholder is trying to recreate file.
    at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:740)
    at org.apache.hadoop.dfs.NameNode.create(NameNode.java:307)
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
{code}

Let me try and replicate with DEBUG level enabled.  Also, I think I know how to easily replicate.   Need to do some tests."
HADOOP-1764,Inconsistancy between Mapper/Reducer book keeping,"Refer to HADOOP-1763

This occurs in that scenario once many job trackers are lost, reducers do not know where the map outputs are present. They keep retrying the wrong node causing the reducers to run forever without failures.

Relevant logs:
Reducer output:
2007-08-21 09:47:47,046 INFO org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 Copying task_200708210155_0003_m_002598_0 output from node50
2007-08-21 09:47:53,643 WARN org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 copy failed: task_200708210155_0003_m_002598_0 from node50
2007-08-21 09:47:53,643 WARN org.apache.hadoop.mapred.ReduceTask: java.io.FileNotFoundException: http://wm511750.inktomisearch.com:50060/mapOutput?map=task_200708210155_0003_m_002598_0&reduce=6
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1243)
	at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:207)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:673)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:631)
2007-08-21 09:53:02,327 INFO org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 Copying task_200708210155_0003_m_002598_0 output from node50
2007-08-21 09:53:02,333 WARN org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 copy failed: task_200708210155_0003_m_002598_0 from node50
2007-08-21 09:53:02,333 WARN org.apache.hadoop.mapred.ReduceTask: java.io.FileNotFoundException: http://node50:50060/mapOutput?map=task_200708210155_0003_m_002598_0&reduce=6
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1243)
	at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:207)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:673)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:631)
2007-08-21 09:57:33,899 INFO org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 Copying task_200708210155_0003_m_002598_0 output from node50.inktomisearch.com.
2007-08-21 09:57:33,908 WARN org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 copy failed: task_200708210155_0003_m_002598_0 from node50.inktomisearch.com
2007-08-21 09:57:33,908 WARN org.apache.hadoop.mapred.ReduceTask: java.io.FileNotFoundException: http://node50:50060/mapOutput?map=task_200708210155_0003_m_002598_0&reduce=6
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1243)
	at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:207)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:673)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:631)
2007-08-21 10:00:56,337 INFO org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 Copying task_200708210155_0003_m_002598_1 output from node75.inktomisearch.com.
2007-08-21 10:00:56,342 INFO org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 done copying task_200708210155_0003_m_002598_1 output from node75
2007-08-21 10:02:17,486 INFO org.apache.hadoop.mapred.ReduceTask: task_200708210155_0003_r_000006_2 Ignoring obsolete copy result for Map Task: task_200708210155_0003_m_002598_0 from host: node50

Looking at TIP task_200708210155_0003_m_002598:

task_200708210155_0003_m_002598_0	node50	KILLED	0.00%		21-Aug-2007 09:38:49 	Lost task tracker
task_200708210155_0003_m_002598_1	node75	KILLED	0.00%		21-Aug-2007 11:22:42 	Lost task tracker
task_200708210155_0003_m_002598_2	node55	SUCCEEDED	100.00%	21-Aug-2007 11:22:46	21-Aug-2007 11:27:19 (4mins, 33sec) 	
task_200708210155_0003_m_002598_3	node49	KILLED	100.00%	21-Aug-2007 11:22:48	21-Aug-2007 11:27:48 (4mins, 59sec) 	Already completed TIP


Notes:
1. Even finally the reducer seems to fetch data from the incorrect TaskTracker, it is not checking with the job tracker for the final/correct map output
2. It seems to retry more times and sleeps for longer time (looking at the interval of log messages)
3. An obvious solution may be to go to the job tracker and directly get the correct map output (I was able to get the correct map output from node55 using http, without any errors)"
HADOOP-1763,Too many lost task trackers - Job failures,"Steps to reproduce:
1 .Run a map reduce application running more than 3000 mappers, each running longer than
2. Observe the lost task trackers.

Observations:
1. Most of the lost taskTracker messages correspond to maps that have already completed
2. Based on the logs below the taskTracker is unable to connect to the job tracker and so the jobTracker deletes the job after 20 minutes

One example:
task_200708210155_0003_m_000000_0	node1	KILLED	0.00%		21-Aug-2007 09:39:09 	Lost task tracker   <-- Please note the time

Counters:
Map-Reduce Framework
	Map input records 	28,861
	Map output records 	1,349,114
	Map input bytes 	200,018,562
	Map output bytes 	714,878,712

Node 1 task tracker logs:
2007-08-21 09:08:51,109 INFO org.apache.hadoop.mapred.TaskTracker: Task task_200708210155_0003_m_000000_0 is done. <-- Please note the time
.
.
.
2007-08-21 09:08:52,212 INFO org.mortbay.http.SocketListener: LOW ON THREADS ((40-40+0)<1) on SocketListener0@0.0.0.0:50060
2007-08-21 09:08:52,217 WARN org.mortbay.http.SocketListener: OUT OF THREADS: SocketListener0@0.0.0.0:50060
.
.
.
2007-08-21 09:18:53,877 ERROR org.apache.hadoop.mapred.TaskTracker: Caught exception: java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:472)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
        at org.apache.hadoop.mapred.$Proxy0.heartbeat(Unknown Source)
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:941)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:840)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1227)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1911)
.
.
.
2007-08-21 09:47:45,207 INFO org.apache.hadoop.mapred.TaskTracker: Resending 'status' to 'wm501219' with reponseId '5247
2007-08-21 09:47:46,023 INFO org.apache.hadoop.mapred.TaskTracker: Recieved RenitTrackerAction from JobTracker
2007-08-21 09:47:46,041 INFO org.apache.hadoop.mapred.TaskRunner: task_200708210155_0003_m_000000_0 done; removing files.
2007-08-21 09:47:46,240 INFO org.apache.hadoop.mapred.TaskRunner: task_200708210155_0003_m_002237_0 done; removing files.

Tasktracker is pretty active otherwise:
tracker_wm511293.inktomisearch.com:50050	wm511293.inktomisearch.com	1	6	3

JobTracker logs:
2007-08-21 09:01:11,951 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_200708210155_0003_m_000000_0' to tip tip_200708210155_0003_m_000000, for tracker 'tracker_wm511293.inktomisearch.com:50050'
.
2007-08-21 09:06:27,745 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_200708210155_0003_m_000000_1' to tip tip_200708210155_0003_m_000000, for tracker 'tracker_wm511783.inktomisearch.com:50050'
.
2007-08-21 09:08:51,212 INFO org.apache.hadoop.mapred.JobInProgress: Task 'task_200708210155_0003_m_000000_0' has completed tip_200708210155_0003_m_000000 successfully.
2007-08-21 09:08:51,213 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_200708210155_0003_m_000000_0' has completed succesfully
.
2007-08-21 09:11:27,227 INFO org.apache.hadoop.mapred.TaskInProgress: Already complete TIP tip_200708210155_0003_m_000000 has completed task task_200708210155_0003_m_000000_1
.
2007-08-21 09:39:09,014 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_200708210155_0003_m_000000_0: Lost task tracker
2007-08-21 09:39:09,014 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_200708210155_0003_m_000000_0' has been lost.
.
2007-08-21 09:39:09,348 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_200708210155_0003_m_000000_0' from 'tracker_wm511293.inktomisearch.com:50050'
.
2007-08-21 09:47:20,855 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_200708210155_0003_m_000000_1: Lost task tracker
2007-08-21 09:47:20,855 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'KILLED' for 'task_200708210155_0003_m_000000_1' of TIP 'tip_200708210155_0003_m_000000'



Notes:
1. I do  not see the taskTracker dying during that period
2. Is retry logic not accurate/agressive enough? (did something change recently, this behavior is more evident in 0.15)
3. Inconsistencies with jobTracker logs? Lost task tracker detection bad?
4. TaskTracker:
          CPU usage: 9:10-9:20 50%
                                  9:20-9:40 0%
          Network Usage: 6M incl dfs operations
5. JobTracker
          CPU udage: Avg: 9%
          Network Usage:  Negligible

"
HADOOP-1762,Namenode does not need to store storageID and datanodeID persistently,"
Currently Namenode stores all the storage-ids it generates since the beginning (since last format). It allocates a new storageID everytime a new datanode comes online. It also stores all the known datanode ids since the beginning. 

It would be better if Namenode did not have to keep track of these. I will describe a proposal in the next comment. 

This has implecations regd how Namenode helps administrators identify 'dead datanodes' etc. These issues are addressed in HADOOP-1138."
HADOOP-1759,File name should be represented by a byte array instead of a String,"This patch changes INode.name type to byte[], as outlined in HADOOP-1687 (4).
I submit it as a separate patch since it does not touch a lot of code, just INode.java"
HADOOP-1758,processing escapes in a jute record is quadratic,"The following code appears in hadoop/src/c++/librecordio/csvarchive.cc :


static void replaceAll(std::string s, const char *src, char c)
{
  std::string::size_type pos = 0;
  while (pos != std::string::npos) {
    pos = s.find(src);
    if (pos != std::string::npos) {
      s.replace(pos, strlen(src), 1, c);
    }
  }
}

This is used in the context of replacing jute escapes in the code:


void hadoop::ICsvArchive::deserialize(std::string& t, const char* tag)
{
  t = readUptoTerminator(stream);
  if (t[0] != '\'') {
    throw new IOException(""Errror deserializing string."");
  }
  t.erase(0, 1); /// erase first character
  replaceAll(t, ""%0D"", 0x0D);
  replaceAll(t, ""%0A"", 0x0A);
  replaceAll(t, ""%7D"", 0x7D);
  replaceAll(t, ""%00"", 0x00);
  replaceAll(t, ""%2C"", 0x2C);
  replaceAll(t, ""%25"", 0x25);

}

Since this replaces the entire string for each instance of the escape sequence, practically anything would be better.  I would propose that within deserialize we allocate a char * [since each replacement is smaller than the original], scan for each %, and either do a general hex conversion in place or look for one of the six patterns, and after each replacement move down the unmodified text and scan for the % fom that starting point.

-dk
"
HADOOP-1756,Add toString() methods to some Writable types,"Add missing toString() methods to Writable types that wrap primitive values. Also, add Counters.toString(), which is useful when retrieving counters at the end of the job and displaying them in a UI."
HADOOP-1755,Ability to name streaming jobs for the jobtracker,Added -jobname flag to provide the ability to name jobs in the jobtracker (instead of streamjob****.jar)
HADOOP-1752,"""dfsadmin -upgradeProgress force"" should leave safe mode in order to push the upgrade forward.","I have a cluster (created before hadoop 0.14) on which 40% of data-node blocks were lost. I tried to upgrade it to 0.14.
The distributed upgrade was scheduled correctly on the name-node and all data-nodes. But it never started, since
there was not enough blocks for the name-node to leave safe mode.
I first tried
{code}
bin/hadoop dfsadmin -safemode leave
{code}
But this is prohibited since the distributed upgrade is in progress. I tried 
{code}
bin/hadoop dfsadmin -upgradeProgress force
{code}
But these didn't work because the distributed upgrade does not start until the safe mode conditions are met on the name-node.
The solution would be to set the safe-mode-threshold to 60% if of course I new exactly how many blocks were missing.

The ""force"" command was designed as a way for an administrator to get the upgrade going even if the cluster is not in the perfect shape.
This would let us save at least the data available rather than loosing everything.

I propose to modify the force command so that it would let the cluster start distributed upgrade even if safe-mode is still on."
HADOOP-1750,We should log better if something goes wrong with the process fork,"Currently, if something goes wrong with bash, the user only gets the error code out. In the case of error result, we should copy the stdout and stderr from the bash process to the user logs."
HADOOP-1749,TestDFSUpgrade some times fails with an assert,"From HADOOP-1696 Comment #1:

TestDFSUpgrade has failed in #532 in a different way, see
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/532/testReport/org.apache.hadoop.dfs/TestDFSUpgrade/testUpgrade/

junit.framework.AssertionFailedError: expected:<1790222743> but was:<3731403302>
at org.apache.hadoop.dfs.TestDFSUpgrade.checkResult(TestDFSUpgrade.java:74)
at org.apache.hadoop.dfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:142)

"
HADOOP-1748,Task Trackers fail to launch tasks when they have relative log directories configured,"Task Trackers use the configured log directory to get the locations of the user log files. The Task Tracker does a mkdir, but the task is in a different current directory. Therefore, the directory does not exist and the Task fails."
HADOOP-1745,userlogs not showing up for new jobs,"When I start a new hadoop job, the logs do not show up for a while.  If I check on the filesystem, the file userlogs/$task/stdout is a regular file with size 0.
This was supposed to be fixed in 0.14 by HADOOP-1524."
HADOOP-1744,Small cleanup of DistributedFileSystem and DFSClient (next),"Next step to remove UTF8:
 - DistributedFileSystem and DFSClient
 - completely removed {{UTF8 getPath()}}, replaced with {{String getPathName()}}
 - removed some unused imports (warnings)
 - two test cases, the browse directory JSP, dfs.NamenodeFsck and dfs.StreamFile updated

The patch is ~300 lines.
JUnit tests run successfully.

Thanks"
HADOOP-1743,INode refactoring,"This is one of the patches related to name-node memory optimization HADOOP-1687.
It moves INode into a separate (base) class with two separate derived classes for files and directories.
I also eliminated unnecessary recursions in a couple of places.
And made preparation for further steps outlined in HADOOP-1687."
HADOOP-1742,FSNamesystem.startFile()  javadoc is inconsistent,"FSNamesystem.startFile()  description should be updated. 
It talks about arrays of blocks that are supposed to be returned, but returns void."
HADOOP-1739,ConnectException in TaskTracker Child,"Steps to Reproduce:
I had 11000 mappers and 2700 reducers in a job and most failures correspond to the following logs:

Stderr:
Exception in thread ""main"" java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:193)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:519)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:150)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:530)
	at org.apache.hadoop.ipc.Client.call(Client.java:459)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
	at org.apache.hadoop.mapred.$Proxy0.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:248)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1781)


Syslog:
2007-08-19 18:45:07,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 1 time(s).
2007-08-19 18:45:08,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 2 time(s).
2007-08-19 18:45:09,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 3 time(s).
2007-08-19 18:45:10,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 4 time(s).
2007-08-19 18:45:11,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 5 time(s).
2007-08-19 18:45:12,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 6 time(s).
2007-08-19 18:45:13,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 7 time(s).
2007-08-19 18:45:14,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 8 time(s).
2007-08-19 18:45:15,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 9 time(s).
2007-08-19 18:45:16,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:50051. Already tried 10 time(s)

"
HADOOP-1738,Hadoop-Patch build should also compute the number of existing javadoc warnings in the pre-build stage,"The Hadoop-Patch build does a ""pre-build"" phase to determine the number of javac and findbugs warnings so it can tell if the patch generates new warnings:

########################################################
Pre-building trunk to determine current
number of javac and Findbugs warnings
########################################################

It would be nice if it also computed the number of javadoc warnings so that a new test would not get a -1 because javadoc warnings already exist."
HADOOP-1736,Removing ant jar causes javadoc errors so builds fail,"When the ant jar was removed from lib, builds now experience javadoc errors. For example from Hadoop-Patch #567 console log: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/567/console

  [javadoc] Constructing Javadoc information...
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:23: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.BuildException;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:24: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.DirectoryScanner;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:25: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.Project;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:26: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.Task;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:27: package org.apache.tools.ant.types does not exist
  [javadoc] import org.apache.tools.ant.types.FileSet;
  [javadoc]                                   ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:50: cannot find symbol
  [javadoc] symbol: class Task
  [javadoc] public class RccTask extends Task {
  [javadoc]                              ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:55: cannot find symbol
  [javadoc] symbol  : class FileSet
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   private final ArrayList<FileSet> filesets = new ArrayList<FileSet>();
  [javadoc]                           ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:98: cannot find symbol
  [javadoc] symbol  : class FileSet
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   public void addFileset(FileSet set) {
  [javadoc]                          ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:105: cannot find symbol
  [javadoc] symbol  : class BuildException
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   public void execute() throws BuildException {
  [javadoc]                                ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:124: cannot find symbol
  [javadoc] symbol  : class BuildException
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   private void doCompile(File file) throws BuildException {
  [javadoc]                                            ^
  [javadoc] Standard Doclet version 1.5.0_11
  [javadoc] Building tree for all the packages and classes...
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/docs/api/stylesheet.css...
  [javadoc] 10 warnings"
HADOOP-1735,javadoc warnings in trunk: Complaints about missing ant dependency,"Trunk is throwing javadoc warnings complaining about ant dependency. Its causing patch failures.
{code}
[javadoc] Constructing Javadoc information...
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:23: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.BuildException;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:24: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.DirectoryScanner;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:25: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.Project;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:26: package org.apache.tools.ant does not exist
  [javadoc] import org.apache.tools.ant.Task;
  [javadoc]                             ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:27: package org.apache.tools.ant.types does not exist
  [javadoc] import org.apache.tools.ant.types.FileSet;
  [javadoc]                                   ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:50: cannot find symbol
  [javadoc] symbol: class Task
  [javadoc] public class RccTask extends Task {
  [javadoc]                              ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:55: cannot find symbol
  [javadoc] symbol  : class FileSet
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   private final ArrayList<FileSet> filesets = new ArrayList<FileSet>();
  [javadoc]                           ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:98: cannot find symbol
  [javadoc] symbol  : class FileSet
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   public void addFileset(FileSet set) {
  [javadoc]                          ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:105: cannot find symbol
  [javadoc] symbol  : class BuildException
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   public void execute() throws BuildException {
  [javadoc]                                ^
  [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:124: cannot find symbol
  [javadoc] symbol  : class BuildException
  [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask
  [javadoc]   private void doCompile(File file) throws BuildException {
  [javadoc]                                            ^
  [javadoc] Standard Doclet version 1.5.0_11
  [javadoc] Building tree for all the packages and classes...
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/docs/api/stylesheet.css...
  [javadoc] 10 warnings
{code}"
HADOOP-1733,LocalJobRunner uses old-style job/tip ids,"We should rework LocalJobRunner to use the new style job/tip ids (post HADOOP-1473).

Is this a *blocker*? This isn't a functionality bug, yet ..."
HADOOP-1732,LocalJobRunner uses old-style job/tip ids,"We shoudl rework LocalJobRunner to assign the new style job/tip ids (HADOOP-1473).

Is this a *blocker*? This isn't a functionality bug, yet ..."
HADOOP-1731,contrib jar file names should include hadoop version number,"The file names of contrib jars should include the hadoop version.  That way, when folks copy these around they'll be more easily able to see whether they have compatible versions."
HADOOP-1728,Hadoop should allow the task trackerto to have different mapper/reducer task limits,"

Right now, Hadoop has one configuration variable to set the limit on the number of tasks 
per task tracker. That number applies to both the mappers and reducers. However, in many
cases, it make a lot of sense to set different limits on them, For example, since mappers 
are typically take less resources than reducers, we can set 4 as the limit on mappers
and 2 on reducers. This will improve the cluster utilization and improve the
overall performance.

"
HADOOP-1727,Make ...hbase.io.MapWritable more generic so that it can be included in ...hadoop.io,"The class org.apache.hadoop.hbase.io.MapWritable could be made more generic through the use of ReflectionUtils so that it could support more Map key and value classes. Currently it supports Map<WritableComparable, Writable> only.

When more generalized, submit for consideration to be included in org.apache.hadoop.io
"
HADOOP-1722,Make streaming to handle non-utf8 byte array,"Right now, the streaming framework expects the output sof the steam process (mapper or reducer) are line 
oriented UTF-8 text. This limit makes it impossible to use those programs whose outputs may be non-UTF-8
 (international encoding, or maybe even binary data). Streaming can overcome this limit by introducing a simple
encoding protocol. For example, it can allow the mapper/reducer to hexencode its keys/values, 
the framework decodes them in the Java side.
This way, as long as the mapper/reducer executables follow this encoding protocol, 
they can output arabitary bytearray and the streaming framework can handle them.
"
HADOOP-1719,Improve the utilization of shuffle copier threads,"In the current design, the scheduling of copies is done and the scheduler (the main loop in fetchOutputs) won't schedule anything until it hears back from at least one of the copier threads. Due to this, the main loop won't query the TaskTracker asking for new map locations and may not be using all the copiers effectively. This may not be an issue for small-sized map outputs, where at steady state, the frequency of such notifications is frequent.
Ideally, we should schedule all what we can, and, depending on how busy we currently are, query the tasktracker for more map locations.
"
HADOOP-1718,Test coverage target in build files using clover,"Moving Simon Willnauer clover patch from HADOOP-1496 to a new Jira.  From 1496:

Simon Willnauer - 27/Jun/07 12:18 PM
We have a donated clover license at the ""private"" repository.
It is located right here.
https://svn.apache.org/repos/private/committers/donated-licenses/clover/
You could have a look at the lucene ant files to include clover into the hadoop build managment.
The license is a ""ant - only"" license and can only be used on ""org.apache.*"" packages.

Simon Willnauer - 27/Jun/07 01:56 PM
Clover integration into Hadoop.
I added the clover report task to the build.xml.
We did that in the Lucene project a while ago and I had to do it for work anyway so I added the tasks to the hadoop project as well.
To generate the reports the clover.jar an clover.license from the ""commiter"" repository must be available on the ANT Path.
I had problems with the jar file located in the apache repository so I use the current version from the cenqua website (http://www.cenqua.com/download.jspa - clover for ant-1.3.13)
I created the reports running:
ant -Drun.clover=true clean test generate-clover-reports
"
HADOOP-1717,TestDFSUpgradeFromImage fails on Solaris ,"TestDFSUpgradeFromImage is broken on Solaris so all patch builds will fail until it is fixed.  I believe Raghu is working on a patch which will remove the non-standard tar -z dependency.

From Enis Soztutar:
TestDFSUpgradeFromImage fails for hadoop-patch and hudson-nightly builds on hudson. 
The error thrown is :
{noformat}
java.io.IOException: tar: z: unknown function modifier
	at org.apache.hadoop.fs.Command.run(Command.java:33)
	at org.apache.hadoop.fs.Command.execCommand(Command.java:89)
	at org.apache.hadoop.dfs.TestDFSUpgradeFromImage.setUp(TestDFSUpgradeFromImage.java:75)

Standard Output

2007-08-15 13:22:38,601 INFO  dfs.TestDFSUpgradeFromImage (TestDFSUpgradeFromImage.java:setUp(72)) - Unpacking the tar file /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/test/cache/hadoop-12-dfs-dir.tgz
{noformat}
"
HADOOP-1716,TestPipes.testPipes fails,"I recently started running the unit tests with -Dcompile.c++=yes so that pipes is compiled and it's unit tests are run.

TestPipes.testPipes consistently fails on Linux with 
junit.framework.AssertionFailedError: got exception: java.io.IOException: Job failed!
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:625)
	at org.apache.hadoop.mapred.pipes.Submitter.submitJob(Submitter.java:250)
	at org.apache.hadoop.mapred.pipes.Submitter.main(Submitter.java:404)
	at org.apache.hadoop.mapred.pipes.TestPipes.runNonPipedProgram(TestPipes.java:173)
	at org.apache.hadoop.mapred.pipes.TestPipes.testPipes(TestPipes.java:69)

	at org.apache.hadoop.mapred.pipes.TestPipes.runNonPipedProgram(TestPipes.java:180)
	at org.apache.hadoop.mapred.pipes.TestPipes.testPipes(TestPipes.java:69)

which is perhaps caused by
2007-08-14 02:10:18,831 INFO  mapred.TaskRunner (ReduceTaskRunner.java:close(45)) - task_200708140209_0003_r_000000_0 done; removing files.
2007-08-14 02:10:18,841 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(371)) - Error from task_200708140209_0003_r_000000_0: java.io.IOException: pipe child exception
	at org.apache.hadoop.mapred.pipes.Application.abort(Application.java:130)
	at org.apache.hadoop.mapred.pipes.PipesReducer.close(PipesReducer.java:103)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:328)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1778)
Caused by: java.net.SocketException: Broken pipe
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:115)
	at java.io.DataOutputStream.writeByte(DataOutputStream.java:136)
	at org.apache.hadoop.io.WritableUtils.writeVLong(WritableUtils.java:278)
	at org.apache.hadoop.io.WritableUtils.writeVInt(WritableUtils.java:258)
	at org.apache.hadoop.mapred.pipes.BinaryProtocol.close(BinaryProtocol.java:281)
	at org.apache.hadoop.mapred.pipes.PipesReducer.close(PipesReducer.java:95)
	... 2 more"
HADOOP-1715,TestCopyFiles.testCopyFromLocalToLocal fails on Windows,"3 different exceptions.  Not sure which is relevant:


java.io.FileNotFoundException: C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/data/destdat/five/eight/8850688331351221910
	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:126)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:109)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:266)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:244)
	at org.apache.hadoop.fs.TestCopyFiles.checkFiles(TestCopyFiles.java:135)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:169)

Standard Output:
2007-08-13 19:31:41,191 INFO  ipc.Server (Server.java:run(568)) - IPC Server handler 9 on 1420, call open(/destdat/four/zero/2221298292449454108, 0, 671088640) from 127.0.0.1:1424: error: java.io.IOException: Cannot open filename /destdat/four/zero/2221298292449454108
java.io.IOException: Cannot open filename /destdat/four/zero/2221298292449454108
	at org.apache.hadoop.dfs.NameNode.open(NameNode.java:269)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)

Standard Error:
Copy failed: java.lang.NullPointerException
	at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.setup(CopyFiles.java:321)
	at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:773)
	at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:854)
	at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:166)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
Waiting for the Mini HDFS Cluster to start..."
HADOOP-1714,TestDFSUpgradeFromImage fails on Windows,"2007-08-13 18:48:42,036 INFO  dfs.TestDFSUpgradeFromImage (TestDFSUpgradeFromImage.java:setUp(72)) - Unpacking the tar file C:\hudson\workspace\Hadoop-WindowsTest-0.14\branch-0.14/build/test/cache/hadoop-12-dfs-dir.tgz

java.io.IOException: tar (child): Cannot execute remote shell: No such file or directory
	at org.apache.hadoop.fs.Command.run(Command.java:33)
	at org.apache.hadoop.fs.Command.execCommand(Command.java:89)
	at org.apache.hadoop.dfs.TestDFSUpgradeFromImage.setUp(TestDFSUpgradeFromImage.java:74)"
HADOOP-1712,Unhandled exception in Block CRC upgrade on datanode.,"
One of the un-handled IOException during BlockCRC upgrade results in the upgrade thread to exit with out proper upgrade. 

exception on the datanode :
{noformat}
2007-08-13 22:18:27,324 ERROR org.apache.hadoop.dfs.DataNode: java.io.IOException: Block blk_-6404399692543439055 is not valid.
        at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:492)
        at org.apache.hadoop.dfs.BlockCrcUpgradeObjectDatanode.doUpgrade(BlockCrcUpgrade.java:1431)
        at org.apache.hadoop.dfs.UpgradeObjectDatanode.run(UpgradeObjectDatanode.java:95)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Will also check if there are any more of such misses.
"
HADOOP-1708,make files visible in the namespace as soon as they are created,"In the current DFS implementation, a file appears in the namespace only when the creator closes the file. Also, if the namenode or the client dies before closing the file, the file never appears in the namespace.

This issue will make files appear in the namespace as soon as it is created. Also, it will continue to remain in the namespace even if the creator dies before closing the file.

This is related to HADOOP-89. It is different from HADOOP-89 because it does not attempt to make data visible as soon as it is written. "
HADOOP-1707,Remove the DFS Client disk-based cache,"The DFS client currently uses a staging file on local disk to cache all user-writes to a file. When the staging file accumulates 1 block worth of data, its contents are flushed to a HDFS datanode. These operations occur sequentially.

A simple optimization of allowing the user to write to another staging file while simultaneously uploading the contents of the first staging file to HDFS will improve file-upload performance.
"
HADOOP-1704,Throttling for HDFS Trash purging,"When HDFS Trash is enabled, deletion of a file/directory results in it being moved to the ""Trash"" directory. The ""Trash"" directory is periodically purged by the Namenode. This means that all files/directories that users deleted in the last Trash period, gets ""really"" deleted when the Trash purging occurs. This might cause a burst of file/directory deletions.

The Namenode tracks blocks that belonged to deleted files in a data structure named ""RecentInvalidateSets"". There is a possibility that Trash purging may cause this data structure to bloat, causing undesireable behaviour of the Namenode."
HADOOP-1703,Small cleanup of DistributedFileSystem and DFSClient,"Here is a small code cleanup (276 lines), mainly removing some uses of the deprecated UTF8 class.

* dfs.DistributedFileSystem:
-- removed unused private field: localFs
-- corrected some >80 columns lines
-- replaced some use of  {{UTF8 getPath() { return new UTF8(getPathName()); }}}
 with {{String getPathName()}}

* dfs.DFSClient:
-- removed some UTF8 in favor of String
-- DFSOutputStream: replaced UTF8 with String

* a few other minor updates reflecting the changes from UTF8 to Strings.

All JUnit tests pass successfully.

Thanks,
Christophe T."
HADOOP-1702,Reduce buffer copies when data is written to DFS,"HADOOP-1649 adds extra buffering to improve write performance.  The following diagram shows buffers as pointed by (numbers). Each eatra buffer adds an extra copy since most of our read()/write()s match the io.bytes.per.checksum, which is much smaller than buffer size.

{noformat}
       (1)                 (2)          (3)                 (5)
   +---||----[ CLIENT ]---||----<>-----||---[ DATANODE ]---||--<>-> to Mirror  
   | (buffer)                  (socket)           |  (4)
   |                                              +--||--+
 =====                                                    |
 =====                                                  =====
 (disk)                                                 =====
{noformat}

Currently loops that read and write block data, handle one checksum chunk at a time. By reading multiple chunks at a time, we can remove buffers (1), (2), (3), and (5). 

Similarly some copies can be reduced when clients read data from the DFS.
"
HADOOP-1701,Provide a security framework design,"Only provide a security framework as described below.  A simple implementation will be provided in HADOOP-2229.

h4._Previous Description_
In HADOOP-1298, we want to add user information and permission to the file system.  It requires an authentication service and a user management service.  We should provide a framework and a simple implementation in issue and extend it later.  As discussed in HADOOP-1298, the framework should be extensible and pluggable.

- Extensible: possible to extend the framework to the other parts (e.g. map-reduce) of Hadoop.

- Pluggable: can easily switch security implementations.  Below is a diagram borrowed from Java.

!http://java.sun.com/javase/6/docs/technotes/guides/security/overview/images/3.jpg!

- Implement a Hadoop authentication center (HAC).  In the first step, the mechanism of HAC is very simple, it keeps track a list of usernames (we only support users, will work on other principals later) in HAC and verify username in user login (yeah, no password).  HAC can run inside NameNode or run as a stand alone server.   We will probably use Kerberos to provide more sophisticated authentication service."
HADOOP-1700,Append to files in HDFS,"Request for being able to append to files in HDFS has been raised a couple of times on the list of late.   For one example, see http://www.nabble.com/HDFS%2C-appending-writes-status-tf3848237.html#a10916193.  Other mail describes folks' workarounds because this feature is lacking: e.g. http://www.nabble.com/Loading-data-into-HDFS-tf4200003.html#a12039480 (Later on this thread, Jim Kellerman re-raises the HBase need of this feature).  HADOOP-337 'DFS files should be appendable' makes mention of file append but it was opened early in the life of HDFS when the focus was more on implementing the basics rather than adding new features.  Interest fizzled.  Because HADOOP-337 is also a bit of a grab-bag -- it includes truncation and being able to concurrently read/write -- rather than try and breathe new life into HADOOP-337, instead, here is a new issue focused on file append.  Ultimately, being able to do as the google GFS paper describes -- having multiple concurrent clients making 'Atomic Record Append' to a single file would be sweet but at least for a first cut at this feature, IMO, a single client appending to a single HDFS file letting the application manage the access would be sufficent.

"
HADOOP-1699,Child task debugging on Hadoop,"Recently I discovered debugging a map/reduce task on hadoop can be simplified with couple of lines code in TaskRunner enabling us to run jdb on the child tasks.

TaskRunner.java: 282
        if(conf.getBoolean(""mapred.debug.child.task"", false)) {
            Random r=new Random();
            int debugPort=8000 + r.nextInt(1000);
            vargs.add(""-Xdebug"");
            vargs.add(""-Xrunjdwp:transport=dt_socket,address=""+ debugPort +"",server=y,suspend=n"");
            LOG.info(""Running debug server for task ""+ t.getTaskId() + "" at port"" + debugPort);
        }

This code runs the child tasks with debug enabled after getting a configuration variable.

Connecting to a child task is as simple as running jdb -attach <hostname>:<debugPort> from anywhere. Additionally authentication information could be included in the jobConf. 

I believe it will greatly reduce the development/debug time on hadoop.
"
HADOOP-1698,7500+ reducers/partitions causes job to hang,"Steps to Reproduce:
On the above cluster run any job with #partitions/reducers = 8000+
Observe CPU utilization on any mapper.

Observations:
The output.collect(Key, Value) call takes a huge amount of CPU, causing the job to hang.

This is a result of two issues:
1) Number of partitions beyond 7500 results in a call to sortAndSpillToDisk() on each call to output.collect
2) Call to sortAndSpillToDisk causes creation of a writer object, eventually calling:
 MessageDigest digester = MessageDigest.getInstance(""MD5"");
        digester.update((new UID()+""@""+InetAddress.getLocalHost()).getBytes());
        sync = digester.digest();
A code-block in  SequenceFile.java(652)


Issue #1:
Further investigation reveals the following stack trace whenever the task is suspended.
  [1] java.net.Inet4AddressImpl.lookupAllHostAddr (native method)
  [2] java.net.InetAddress$1.lookupAllHostAddr (InetAddress.java:849)
  [3] java.net.InetAddress.getAddressFromNameService (InetAddress.java:1,183)
  [4] java.net.InetAddress.getLocalHost (InetAddress.java:1,312)
  [5] org.apache.hadoop.io.SequenceFile$Writer.<init> (SequenceFile.java:653)
  [6] org.apache.hadoop.io.SequenceFile$Writer.<init> (SequenceFile.java:622)
  [7] org.apache.hadoop.io.SequenceFile.createWriter (SequenceFile.java:386)
  [8] org.apache.hadoop.io.SequenceFile.createWriter (SequenceFile.java:412)
  [9] org.apache.hadoop.mapred.MapTask$MapOutputBuffer.startPartition (MapTask.java:307)
  [10] org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpillToDisk (MapTask.java:387)
  [11] org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect (MapTask.java:355)
/*My code*/
  [12] mypackage.MyClass$Map.map (MyClass.java:283)
--------------
  [13] org.apache.hadoop.mapred.MapRunner.run (MapRunner.java:46)
  [14] org.apache.hadoop.mapred.MapTask.run (MapTask.java:189)
  [15] org.apache.hadoop.mapred.TaskTracker$Child.main (TaskTracker.java:1,771)

The piece of code causing the problem is (MapTask.java:355)
----------------------------------------------------------
        long totalMem = 0;
        for (int i = 0; i < partitions; i++)
          totalMem += sortImpl[i].getMemoryUtilized();  <---- == 16K (BasicTypeSorterBase.java(88) (startOffsets.length (below)) * BUFFERED_KEY_VAL_OVERHEAD;

        if ((keyValBuffer.getLength() + totalMem) >= maxBufferSize) { <----------------condition is always true if partitions > 7500
          sortAndSpillToDisk();
          keyValBuffer.reset();
          for (int i = 0; i < partitions; i++) {
            sortImpl[i].close();
          }
        }
----------------------------------------------------------

Looking at the variable values in  org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect (MapTask.java:355)
 sortImpl[0] = {
    org.apache.hadoop.mapred.BasicTypeSorterBase.keyValBuffer: instance of org.apache.hadoop.io.DataOutputBuffer(id=1159)
    org.apache.hadoop.mapred.BasicTypeSorterBase.startOffsets: instance of int[1024] (id=1160) <--1K * 16 (previously explained) == 16K
    org.apache.hadoop.mapred.BasicTypeSorterBase.keyLengths: instance of int[1024] (id=1161)
    org.apache.hadoop.mapred.BasicTypeSorterBase.valueLengths: instance of int[1024] (id=1162)
    org.apache.hadoop.mapred.BasicTypeSorterBase.pointers: instance of int[1024] (id=1163)
    org.apache.hadoop.mapred.BasicTypeSorterBase.comparator: instance of org.apache.hadoop.io.MD5Hash$Comparator(id=1164)
    org.apache.hadoop.mapred.BasicTypeSorterBase.count: 0
    org.apache.hadoop.mapred.BasicTypeSorterBase.BUFFERED_KEY_VAL_OVERHEAD: 16
    org.apache.hadoop.mapred.BasicTypeSorterBase.reporter: instance of org.apache.hadoop.mapred.Task$2(id=1165)
}
Computation
maxBufferSize == 120M 
therotical max #of partitions assuming 0 keyValBuffer.getLength() =120M/16K = 7500 partitions

Issue #2: 
digester.update((new UID()+""@""+InetAddress.getLocalHost()).getBytes());
  [1] java.net.Inet4AddressImpl.lookupAllHostAddr (native method)
  [2] java.net.InetAddress$1.lookupAllHostAddr (InetAddress.java:849)
  [3] java.net.InetAddress.getAddressFromNameService (InetAddress.java:1,183)
InetAddress.getLocalHost() call does not cache results, this results in a look up to the host file and DNS(???) bumping up the CPU usage even higher (Observed).

This is a BLOCKER issue and needs immediate attention. 

Notes:
1) Output.collect should not take hit from framework, separate thread to handle spill buffer?
2) InetAddress.getLocalHost result should be cached in a static variable?
3) Spilling logic should not involve #of partitions, needs redesign?
"
HADOOP-1696,NPE in TestDistributedUpgrade,"The TestDistributedUpgrade test fails once a while with the following stack trace:

java.lang.NullPointerException
    at org.apache.hadoop.dfs.FSImage.getDistributedUpgradeState(FSImage.java:1036)
    at org.apache.hadoop.dfs.FSImage.setFields(FSImage.java:411)
    at org.apache.hadoop.dfs.Storage$StorageDirectory.write(Storage.java:169)
    at org.apache.hadoop.dfs.UpgradeUtilities.createVersionFile(UpgradeUtilities.java:306)
    at org.apache.hadoop.dfs.TestDistributedUpgrade.testDistributedUpgrade(TestDistributedUpgrade.java:100)
"
HADOOP-1695,Secondary Namenode halt when SocketTimeoutException at startup,"When we start the namenode and secondary-namenode at the same time, usually primary namenode is busy handling the blockreports.
If secondary namenode fail to connect at startup, it crashes leaving the following exception in the .out file.
I hope it will catch the exception and retry later.

Exception in thread ""main"" java.net.SocketTimeoutException: timed out waiting for rpc response
  at org.apache.hadoop.ipc.Client.call(Client.java:471)
  at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:163)
  at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source)
  at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:247)
  at org.apache.hadoop.dfs.SecondaryNameNode.<init>(SecondaryNameNode.java:96)
  at org.apache.hadoop.dfs.SecondaryNameNode.main(SecondaryNameNode.java:474)
"
HADOOP-1694,lzo compressed input files not properly recognized,"When running the wordcount example with text, gzip and lzo compressed input files, the lzo compressed input files are not properly recognized and are treated as text files.

With an input dir of
{quote}
    /user/hadoopqa/input/part-001.txt
    /user/hadoopqa/input/part-002.txt.gz
    /user/hadoopqa/input/part-003.txt.lzo
{quote}
and running this command
{quote}
    bin/hadoopqa jar hadoop-examples.jar wordcount /user/hadoopqa/input /user/hadoopqa/output
{quote}
I get output that looks like
{quote}
    row     4
    royal   4
    rt$3-ex?ÔøΩ?÷µIStÔøΩ""4D%ÔøΩ9$UÔøΩÔøΩ""ÔøΩ,       1
    ru$ÔøΩÔøΩ#~t""@ÔøΩm*d#\/$ÔøΩÔøΩl.t""XÔøΩÔøΩDi""    1
    rubbÔøΩdÔøΩ&@bT 1
    rubbed  2
{quote}

To lzo compress the file I used lzop:
http://www.lzop.org/download/lzop-1.01-linux_i386.tar.gz
"
HADOOP-1693,Remove LOG members from PendingReplicationBlocks and ReplicationTargetChooser.,"The log member of PendingReplicationBlocks and ReplicationTargetChooser classes is always
set to FSNamesystem.LOG, which is accessible as a static member within the package.
So there is no need to keep an extra reference to it.
This patch fixes the problem."
HADOOP-1692,DfsTask cache interferes with operation,"Some users have experienced problems until they disable the Configuration cache. Since there was no demonstrable need to add it, it should be removed."
HADOOP-1690,No way to programmatically determine hadoop version from binary distribution,"Currently, there's no way (short of starting a hadoop daemon and looking at a web page) to tell what version of Hadoop binaries are installed in a given directory. It'd be great if there were a VERSION file or at least a hadoop-daemon.sh version directive.
"
HADOOP-1689,.sh scripts do not work on Solaris,"the EXPORT commands in the scripts didn't play nicely w/the default shell & Hadoop wouldn't start (""... is not an
identifier"").  I changed the first #! line of bin/hadoop-daemon.sh and bin/hadoop to specify bash, then everything worked: 

#!/bin/bash

ksh is probably another option that will work, but we did not try that"
HADOOP-1687,Name-node memory size estimates and optimization proposal.,"I've done some estimates on how much space our data structures take on the name-node per block, file and directory.

Brief overview of the data structures:
Directory tree (FSDirectory) is built of inodes. Each INode points either to an array of blocks
if it corresponds to a file or to a TreeMap<String, INode> of children INodes if it is a directory.
[Note: this estimates were made before Dhruba replaced the children TreeMap by ArrayList.]
Each block participates also in at least 2 more data structures.
BlocksMap contains a HashMap<Block, BlockInfo> of all blocks mapping a Block into a BlockInfo.
DatanodeDescriptor contains a TreeMap<Block, Block> of all blocks belonging to this data-node.
A block may or may not be contained also in other data-structures, like
{code}
UnderReplicatedBlocks
PendingReplicationBlocks
recentInvalidateSets
excessReplicateMap
{code}
Presence of a block in any of these structures is temporary and therefore I do not count them in my estimates.
The estimates can be viewed as lower bounds.

These are some classes that we are looking at here
{code}
class INode {
   String name;
   INode parent;
   TreeMap<String, INode> children;
   Block blocks[];
   short blockReplication;
   long modificationTime;
}

class Block {
   long blkid;
   long len;
}

class BlockInfo {
   FSDirectory.INode       inode;
   DatanodeDescriptor[]   nodes;
   Block                          block;
}
{code}

The calculations are made for a 64-bit java vm based on that
Reference size = 8 bytes
Object header size = 16 bytes
Array header size = 24 bytes

Commonly used objects:
TreeMap.Entry = 64 bytes. It has 5 reference fields
HashMap.Entry = 48 bytes. It has 3 reference fields
String header = 64 bytes.

The size of a file includes:
# Size of an empty file INode: INode.children = null, INode.blocks is a 0-length array, and file name is empty. (152 bytes)
# A directory entry of the parent INode that points to this file, which is a TreeMap.Entry. (64 bytes)
# file name length times 2, because String represents each name character by 2 bytes.
# Reference to the outer FSDirectory class (8 bytes)

The total: 224 + 2 * fileName.length

The size of a directory includes:
# Size of an empty directory INode: INode.children is an empty TreeMap, INode.blocks = null, and file name is empty. (192 bytes)
# A directory entry of the parent INode that points to this file, which is a TreeMap.Entry. (64 bytes)
# file name length times 2
# Reference to the outer FSDirectory class (8 bytes)

The total: 264 + 2 * fileName.length

The size of a block includes:
# Size of Block. (32 bytes)
# Size of BlockInfo. (64 + 8*replication bytes)
# Reference to the block from INode.blocks (8 bytes)
# HashMap.Entry referencing the block from BlocksMap. (48 bytes)
# References to the block from all DatanodeDescriptors it belongs to.
This is a TreeMap.Entry size times block replication. (64 * replication)

The total: 152 + 72 * replication

Typical object sizes:
Taking into account that typical file name is 10-15 bytes and our default replication is 3 we can say that typical sizes are
File size: 250
Directory size: 290
Block size: 368

||Object||size estimate (bytes)||typical size (bytes)||
|File|224 + 2 * fileName.length|250|
|Directory|264 + 2 * fileName.length|290|
|Block|152 + 72 * replication|368|

One of our clusters has
# Files:  10 600 000
# Dirs:      310 000
# Blocks: 13 300 000

Total Size (estimate): 7,63 GB
Memory used on the name-node (actual reported by jconsole after gc): 9 GB

This means that other data structures like NetworkTopology, heartbeats, datanodeMap, Host2NodesMap,
leases, sortedLeases, and multiple block replication maps occupy ~1.4 GB, which seems to be pretty high
and need to be investigated as well.

Based on the above estimates blocks should be the main focus of the name-node memory reduction effort.
Space used by a block is 50% larger compared to a file, and there is more blocks than files or directories.

Some ideas on how we can reduce the name-node size without substantially changing the data structures.
# INode.children should be an ArrayList instead of a TreeMap. Already done HADOOP-1565. (-48 bytes)
# Factor out the INode class into a separate class (-8 bytes)
# Create base INode class and derive file inode and directory inode classes from the base.
Directory inodes do not need to contain blocks and replication fields (-16 bytes)
File inodes do not need to contain children field (-8 bytes)
# String name should be replaced by a mere byte[]. (-(40 + fileName.length) ~ -50 bytes)
# Eliminate the Block object.
We should move Block fields into into BlockInfo and completely get rid of the Block object. (-16 bytes)
# Block object is referenced at least 5 times in our structures for each physical block.
The number of references should be reduced to just 2. (-24)
# Remove name field from INode. File or directory name is stored in the corresponding directory
entry and does need to be duplicated in the INode (-8 bytes)
# Eliminate INode.parent field. INodes are accessed through the directory tree, and the parent can
be remembered in a local variable while browsing the tree. There is no need to persistently store
the parent reference for each object. (-8 bytes)
# Need to optimize data-node to block map. Currently each DatanodeDescriptor holds a TreeMap of
blocks contained in the node, and we have an overhead of one TreeMap.Entry per block replica.
I expect we can reorganize datanodeMap in a way that it stores only 1 or 2 references per replica
instead of an entire TreeMap.Entry. (-48 * replication)

Note: In general TreeMaps turned out to be very expensive, we should avoid using them if possible.
Or implement a custom map structure, which would avoid using objects for each map entry.


This is what we will have after all the optimizations
||Object||size estimate (bytes)||typical size (bytes)||current typical size (bytes)||
|File|112 + fileName.length|125|250|
|Directory|144 + fileName.length|155|290|
|Block|112 + 24 * replication|184|368|
"
HADOOP-1685,Possible getMapOutput() failures on tasktracker when mapred.reduce.tasks is overriden in job,"The following error occurs many times on a job where I have defined the number of reduce tasks to be less than the default number of reduce tasks defined in my hadoop-site.xml.    Working off my novice understanding of hadoop infrastructure at this point, it appears that the jobTracker is not honoring the mapred.reduce.tasks as defined in the job-conf, and instead is using the default.

Map output lost, rescheduling: getMapOutput(task_0010_m_000002_0,6) failed :
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at java.io.DataInputStream.readLong(DataInputStream.java:399)
	at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:1911)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:747)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:860)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

ie.  
hadoop-site.xml defines mapred.reduce.tasks=7,
In my job I define mapred.reduce.tasks=3

I get many errors looking for:
getMapOutput(task_0010_m_000002_0,3)
getMapOutput(task_0010_m_000002_0,4)
getMapOutput(task_0010_m_000002_0,5)
getMapOutput(task_0010_m_000002_0,6)

This additional error appears to be a side-effect of the actual problem (it stopped happening when I change the job-conf to match default number of reduce tasks):
task_0010_m_000016_0: log4j:ERROR Failed to close the task's log with the exception: java.io.IOException: Bad file descriptor
task_0010_m_000016_0:   at java.io.FileOutputStream.writeBytes(Native Method)
task_0010_m_000016_0:   at java.io.FileOutputStream.write(FileOutputStream.java:260)
task_0010_m_000016_0:   at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
task_0010_m_000016_0:   at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
task_0010_m_000016_0:   at org.apache.hadoop.mapred.TaskLog$Writer.writeIndexRecord(TaskLog.java:251)
task_0010_m_000016_0:   at org.apache.hadoop.mapred.TaskLog$Writer.close(TaskLog.java:235)
task_0010_m_000016_0:   at org.apache.hadoop.mapred.TaskLogAppender.close(TaskLogAppender.java:67)
task_0010_m_000016_0:   at org.apache.log4j.AppenderSkeleton.finalize(AppenderSkeleton.java:124)
task_0010_m_000016_0:   at java.lang.ref.Finalizer.invokeFinalizeMethod(Native Method)
task_0010_m_000016_0:   at java.lang.ref.Finalizer.runFinalizer(Finalizer.java:83)
task_0010_m_000016_0:   at java.lang.ref.Finalizer.access$100(Finalizer.java:14)
task_0010_m_000016_0:   at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:160)
"
HADOOP-1682,Hadoop records can't serialize zero length string,"java.lang.NullPointerException
	at org.apache.hadoop.record.Utils.toBinaryString(Utils.java:307)
	at org.apache.hadoop.record.BinaryRecordOutput.writeString(BinaryRecordOutput.java:95)
	at com.facebook.infrastructure.profiles.users_extended.serialize(users_extended.java:153)
	at org.apache.hadoop.record.Record.serialize(Record.java:58)
	at org.apache.hadoop.record.Record.write(Record.java:72)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:365)
	at com.facebook.infrastructure.profiles.conv_users_extended$MapClass.map(conv_users_extended.java:133)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:186)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1707)

    final int strlen = str.length();
    byte[] bytes = new byte[strlen*4]; // Codepoints expand to 4 bytes max      
  
barfs on trying to allocate zero length array?
"
HADOOP-1680,Improvements to Block CRC upgrade messages,"Nigel has good suggestions regd various log messages during Block CRC upgrade. I will include the suggestions in the next comment.
"
HADOOP-1677,improve semantics of the hadoop dfs command,"HADOOP-230 cover syntax changes.  Capturing here some DFS command line semantic comments between Doug and I:

The goal for DFS should be ""UNIX-like *when* possible"".  The goals of HDFS are primarily to be usable, scalable, reliable, high-performance, and, secondarily, not to be gratuitously incompatible with UNIX.  So, when it's easy to be compatible, we certainly should.  But when UNIX compatibility fights with one of those other goals, it may lose.

Two examples of incompatibilities with UNIX (that will need to be evaluated against the above goal), are:

UNIX: cp foo bar succeeds if bar exists
DFS: dfs -put foo bar fails if bar exists. (same for dfs -cp).

UNIX: mkdir foo fails if foo exists
DFS: dfs -mkdir succeeds silently if foo exists and is a directory (if foo is a file than it fails).

Many more such incompatibilities exist."
HADOOP-1676,java.lang.Exception at org.apache.hadoop.io.compress.zlib.ZlibFactory.getZlibDecompressor(ZlibFactory.java:107),"

In a job running hadoop release 0.13, I saw a lot of following exceptions in stderr:

java.lang.Exception
	at org.apache.hadoop.io.compress.zlib.ZlibFactory.getZlibDecompressor(ZlibFactory.java:107)
	at org.apache.hadoop.io.compress.DefaultCodec.createDecompressor(DefaultCodec.java:80)
	at org.apache.hadoop.io.SequenceFile$Reader.getPooledOrNewDecompressor(SequenceFile.java:1234)
	at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1319)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:1227)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:1216)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:1211)
	at org.apache.hadoop.mapred.SequenceFileRecordReader.(SequenceFileRecordReader.java:40)
	at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:54)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:150)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1707)


The tasks seem to have progressed to complete.

"
HADOOP-1675,Null pointer exception in InMemoryFileSystem,"In a job running release .13, I saw a lot of failures due to the following errors:

2007-08-03 16:13:39,516 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.reserveSpace(InMemoryFileSystem.java:361)
	at org.apache.hadoop.fs.InMemoryFileSystem.reserveSpaceWithCheckSum(InMemoryFileSystem.java:474)
	at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:220)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:680)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:641)


"
HADOOP-1674,bin/hadoop dfsadmin doesnot support [-conf <configuration file>] option,
HADOOP-1671,distcp should set the speculative execution to false,distcp should programmatically set spec exec to false in CopyFiles.java.
HADOOP-1670,Reducer hanging in copyOutput ,"A host with one of the drives 100% full, reducer was hanging, repeating the following warnings.

2007-08-01 15:56:33,386 WARN org.apache.hadoop.mapred.ReduceTask: java.io.IOException: df: `/tmp3/mapred-local': No such file or directory
        at org.apache.hadoop.fs.DF.doDF(DF.java:64)
        at org.apache.hadoop.fs.DF.<init>(DF.java:53)
        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:181)
        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:218)
        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:124)
        at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:228)
        at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:680)
        at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:641)


"
HADOOP-1668,add INCOMPATIBLE CHANGES section to CHANGES.txt for Hadoop 0.14,"HADOOP-1134 and some other Jira's have introduced incompatible changes into Hadoop 0.14.  As per HADOOP-1667 the CHANGES.txt file will include sections for these kind of changes for Hadoop 0.15 and beyond.  As a stop gap for 0.14, the CHANGES.txt file needs an INCOMPATIBLE CHANGES section."
HADOOP-1667,organize CHANGES.txt messages into sections for future releases,"The entries in CHANGES.txt should have the following sections per release:

INCOMPATIBLE CHANGES
NEW FEATURES
OPTIMIZATIONS
BUG FIXES

This should make it easier for folks to read this file."
HADOOP-1666,The FsShell Object cannot be used for multiple fs commands.,"The FsShell object is used to execute an fs command. In its present incantation, this object can be used to invoke only one fs command. Programs that wants to invoke FsShell has to create a FsShell object for each command that it wants to execute."
HADOOP-1665,DFS Trash feature bugs,"In testing the DFS Trash feature, I've run across a couple of bugs.

1) Attempting to remove the same file fails when attempted within the same fs.trash.interval:
    % bin/hadoop dfs -put file /file
    % bin/hadoop dfs -rm /file
    Moved to trash: /file
    % bin/hadoop dfs -put file /file
    % bin/hadoop dfs -rm /file
    rm: Failed to move to trash: /file

2) Removing a file within a directory, followed by removing the directory creates a bizzare hierarchy within /Trash:
    % bin/hadoop dfs -mkdir /dir
    % bin/hadoop dfs -put file /dir/file
    % bin/hadoop dfs -rm /dir/file
    Moved to trash: /dir/file
    % bin/hadoop dfs -rmr /dir
    Moved to trash: /dir
    % bin/hadoop dfs -lsr /Trash
    /Trash/Current  <dir>
    /Trash/Current/dir      <dir>
    /Trash/Current/dir/dir  <dir>     <-- This is weird; potentially related to a rename case not fixed by HADOOP-1623 ???
    /Trash/Current/dir/file <r 3>   10

"
HADOOP-1664,Hadoop DFS upgrade prcoedure,"When upgrading from a July-9  to a July-25 nightly release, we are able to upgrade successfully on a single-node cluster, but failed on a 10 and a 200 node cluster.
As it is not sure whether we made a mistake or not, I file this as an improvement. But going forward it is imperative that there is a safe and well-documented procedure to upgrade dfs without loss of data, including a rollback procedure and listing of operational procedures that are irreversibly destructive (hopefully an empty list)."
HADOOP-1663,streaming returning 0 when submitJob fails with Exception,"In streaming, if I pass an input filename that doesn't exist, 

bash-3.00$ hadoop jar hadoop-streaming.jar -mapper ""/bin/cat"" -reducer ""/bin/cat"" -input ""inputfoo"" -output ""output""
...
07/07/28 07:37:18 ERROR streaming.StreamJob: Error Launching job : Input path doesnt exist : /user/knoguchi/inputfoo
bash-3.00$ echo $?
*0*


=================
Copy&Pasting Milind's comment.

its a hadoop streaming problem. See the code
below:

}catch( IOException ioe){
	LOG.error(""Error Launching job : "" + ioe.getMessage());
      }
      finally {
      if (error && (running_ != null)) {
	LOG.info(""killJob..."");
	running_.killJob();
      }
      jc_.close();

Note that it catches exception, and does not rethrow it !!!
=================
"
HADOOP-1661,TextOutputFormat should ignore NullWritables like null values.,"Currently the TextOutputFormat prints NullWritables, but it would be nicer if it ignored them like it ignores null values."
HADOOP-1660,add support for native library toDistributedCache ,"Currently if a M/R job depends on JNI based component the dynamic library must be available in all the task nodes. This is not possible specially when you have not control on the cluster machines, just using it as a service.

It should be possible to specify using the DistributedCache what are the native libraries a job needs.

For example via a new method 'public void addLibrary(Path libraryPath, JobConf conf)'.

The added libraries would make it to the local FS of the task nodes (same way as cached resources) but instead been part of the classpath they would be copied to a lib directory and that lib directory would be added t the LD_LIBRARY_PATH of the task JVM.

An alternative would be to set the '-Djava.library.path=' task JVM parameter to the lib directory above. However, this would break for libraries that depend on other libraries as the dependent one would not be in the LD_LIBRARY_PATH and the OS would fail to find it as it is not the JVM the one doing the load of the dependent one.

For uncached usage of native libraries, a special directory in the JAR could be used for native libraries. But I'd argue that the DistributedCache enhancement would be enough, and if somebody wants to use a native library s/he should use the DistributedCached. Or a JobConf addLibrary method that uses the DistributedCached under the hood at submission time.

"
HADOOP-1659,job id / job name mix-up,"With the July-25 nightly build the job names in the JobTracker GUI are the same as the job id. On the other hand, in the JobHistory the job names are still correct."
HADOOP-1657,NNBench benchmark hangs with trunk,"
NNBench runs with a small block size (say 20). But uses default value of 512 for io.bytes.per.checksum. Since HADOOP-1134, block size should be a multiple of of bytes.per.checksum. Fix is to set bytes.per.checksum to same as blocksize.

I think following changes to NNBench would help in general (at least the first one) :

 - NNBench does not log these exceptions. I think it should. 
 - It calls create() in an infinite loop as long as create does not succeed. May be we should have an upper limit. Say, max 10000.
"
HADOOP-1656,HDFS does not record the blocksize for a file,The blocksize that a file is created with is not recorded by the Namenode. It is used only by the client when it writes the file. Invoking 'getBlockSize' merely returns the size of the first block. The Namenode should record the blocksize.
HADOOP-1654,IOUtils class,"In the current situation, {{FileUtil}} class includes both file related and io related functionality. This issue intends to separate the two. "
HADOOP-1653,FSDirectory class code cleanup," - lets FSDirectory.INode become a static class, thus sparing one pointer per INode
 - removes an unused constructor for FSDirectory.INode
 - merges identical methods INode.getAbsoluteName() and INode.computeName() and optimizes it using StringBuffer
"
HADOOP-1652,Rebalance data blocks when new data nodes added or data nodes become full,"When a new data node joins hdfs cluster, it does not hold much data. So any map task assigned to the machine most likely does not read local data, thus increasing the use of network bandwidth. On the other hand, when some data nodes become full, new data blocks are placed on only non-full data nodes, thus reducing their read parallelism. 

This jira aims to find an approach to redistribute data blocks when imbalance occurs in the cluster.  An solution should meet the following requirements:
1. It maintains data availablility guranteens in the sense that rebalancing does not reduce the number of replicas that a block has or the number of racks that the block resides.
2. An adminstrator should be able to invoke and interrupt rebalancing from a command line.
3. Rebalancing should be throttled so that rebalancing does not cause a namenode to be too busy to serve any incoming request or saturate the network.
"
HADOOP-1651,Some improvements in progress reporting,"Some improvements that can be done:
1) Progress reporting interval can be made slightly large. It is currently 1 second. Propose to make it 3 seconds to reduce the load on the TaskTracker.
2) Progress reports can potentially be missed. In the loop, if the first attempt at reporting a progress doesn't go through, it is not retried. The next communication will be a 'ping'. 
3) If there is an exception while reporting progress or doing ping, the client should sleep for sometime before retrying.
4) The TaskUmbilicalProtocol client can always stay connected to the server. Currently, the default idle timeout on the IPC client is set to 1000 msec (this means that the client will disconnect if the connection has been idle for 1000 msec). This might lead to unnecessary tearing-down/setting-up of connections for the TaskUmbilicalProtocol and can be avoided by having a high idle timeout for this protocol. The idea behind having the idle timeout was to not hold on to server connections unnecessarily and hence be more scalable when there are 1000s of clients, especially applicable to those protocols involving the JT and the NameNode.  We don't run into scalability issues with TaskUmbilical protocol since it is limited to a few Tasks and the corresponding TaskTracker."
HADOOP-1650,Upgrade Jetty to 6.x,"This is the third attempt at moving to jetty6. Apparently, the jetty-6.1.4 has fixed some of the issues we discovered in jetty during HADOOP-736 and HADOOP-1273. I'd like to keep this issue open for sometime so that we have enough time to test out things."
HADOOP-1649,Performance regression with Block CRCs,"Performance is noticeably affected by Block Level CRCs patch (HADOOP-1134). This is more noticeable on writes (randomriter test etc). 

With random writer, it takes 20-25% on small cluster (20 nodes) and many be 10% on larger cluster. 

There are a few differences in how data is written with 1134. As soon as I can reproduce this, I think it will be easier to fix. "
HADOOP-1648,Add contrib jars to general hadoop CLASSPATH,A mapreduce job that depends on any of the hadoop contrib jars  must bundle the contrib jar into its job jar or copy the contrib jar to the lib dir across the cluster because hadoop contribs are not on the general hadoop CLASSPATH.  It would be an improvement if such as the included hbase mapreduce tasks did not require the running of this extra step.
HADOOP-1647,"DistributedFileSystem.getFileStatus() fails for path ""/""","DistributedFileSystem#getFileStatus throws the following exception when invoked with new Path(""/""). LocalFileSystem does not. The code to produce this error is

{code}
 public static void main(String[] args) throws Exception{
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(conf);
    Path path = new Path(""/"");
    System.out.println(""Path : \"""" + path.toString() + ""\"""");
    System.out.println(fs.isDirectory(path));
    System.out.println(fs.getFileStatus(path).isDir()); 
  }
{code}


for Local configuration the code prints : 
{code}
Path : ""/""
true
true
{code}

For a new formatted dfs with only one file /user/enis/file.txt, Path could not be created. 
{code}
Path : ""/""
false
Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.IllegalArgumentException: Can not create a Path from an empty string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
        at org.apache.hadoop.fs.Path.<init>(Path.java:90)
        at org.apache.hadoop.dfs.DFSFileInfo.<init>(DFSFileInfo.java:59)
        at org.apache.hadoop.dfs.FSDirectory.getFileInfo(FSDirectory.java:729)
        at org.apache.hadoop.dfs.FSNamesystem.getFileInfo(FSNamesystem.java:1301)
        at org.apache.hadoop.dfs.NameNode.getFileInfo(NameNode.java:488)
        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)

        at org.apache.hadoop.ipc.Client.call(Client.java:470)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
        at org.apache.hadoop.dfs.$Proxy0.getFileInfo(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy0.getFileInfo(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient.getFileInfo(DFSClient.java:430)
        at org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:319)
        at org.apache.hadoop.util.TestIsDir.main(TestIsDir.java:38)
{code}"
HADOOP-1645,TestDecommission sometimes fails,"TestDecommission sometimes fails with:

junit.framework.AssertionFailedError: Number of replicas for block0 expected:<3> but was:<2>
        at org.apache.hadoop.dfs.TestDecommission.checkFile(TestDecommission.java:88)
        at org.apache.hadoop.dfs.TestDecommission.testDecommission(TestDecommission.java:285)
"
HADOOP-1643,make the JobSubmissionProtocol package-local instead of public,"Making the JobSubmissionProtocol public was a mistake. All clients should go through JobClient to submit jobs. If there is missing functionality, we should push it into JobClient. Currently any time the protocol changes, it breaks Pig and any other direct users."
HADOOP-1642,Jobs using LocalJobRunner + JobControl fails,"If I run several jobs at the same time using JobControl and the LocalJobRunner i get:
java.io.IOException: Target /tmp/hadoop-johan/mapred/local/localRunner/job_local_1.xml already exists.

It seems like the JobControl class tries to run multiple jobs with the same jobid, causing the exception.


"
HADOOP-1640,TestDecommission fails on Windows,"In the snippet of test log below, the exception happens every ~15 milliseconds for 15 minutes until the test is timed out:

    [junit] Created file decommission.dat with 2 replicas.
    [junit] Block[0] : xxx xxx 
    [junit] Block[1] : xxx xxx 
    [junit] Decommissioning node: 127.0.0.1:50013
    [junit] 2007-07-19 19:12:45,059 INFO  fs.FSNamesystem (FSNamesystem.java:startDecommission(2572)) - Start Decommissioning node 127.0.0.1:50013
    [junit] Name: 127.0.0.1:50013
    [junit] State          : Decommission in progress
    [junit] Total raw bytes: 80030941184 (74.53 GB)
    [junit] Used raw bytes: 33940945746 (31.60 GB)
    [junit] % used: 42.40%
    [junit] Last contact: Thu Jul 19 19:12:44 PDT 2007

    [junit] Waiting for node 127.0.0.1:50013 to change state to DECOMMISSIONED
    [junit] 2007-07-19 19:12:45,199 INFO  http.SocketListener (SocketListener.java:stop(212)) - Stopped SocketListener on 0.0.0.0:3147
    [junit] 2007-07-19 19:12:45,199 INFO  util.Container (Container.java:stop(156)) - Stopped org.mortbay.jetty.servlet.WebApplicationHandler@1d98a
    [junit] 2007-07-19 19:12:45,293 INFO  util.Container (Container.java:stop(156)) - Stopped WebApplicationContext[/,/]
    [junit] 2007-07-19 19:12:45,402 INFO  util.Container (Container.java:stop(156)) - Stopped HttpContext[/logs,/logs]
    [junit] 2007-07-19 19:12:45,481 INFO  util.Container (Container.java:stop(156)) - Stopped HttpContext[/static,/static]
    [junit] 2007-07-19 19:12:45,481 INFO  util.Container (Container.java:stop(156)) - Stopped org.mortbay.jetty.Server@f1916f
    [junit] 2007-07-19 19:12:45,496 INFO  dfs.DataNode (DataNode.java:run(692)) - Exiting DataXceiveServer due to java.net.SocketException: socket closed
    [junit] 2007-07-19 19:12:45,496 WARN  dfs.DataNode (DataNode.java:offerService(568)) - java.io.IOException: java.lang.InterruptedException
    [junit] 	at org.apache.hadoop.fs.DF.doDF(DF.java:71)
    [junit] 	at org.apache.hadoop.fs.DF.getCapacity(DF.java:89)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getCapacity(FSDataset.java:292)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getCapacity(FSDataset.java:379)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getCapacity(FSDataset.java:466)
    [junit] 	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:493)
    [junit] 	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1306)
    [junit] 	at java.lang.Thread.run(Thread.java:595)"
HADOOP-1639,TestSymLink is failing fairly often and is blocking the regression,The regression test is pretty consistently failing. 
HADOOP-1638,Master node unable to bind to DNS hostname,"With a release package of Hadoop 0.13.0 or with latest SVN, the Hadoop contrib/ec2 scripts fail to start Hadoop correctly. After working around issues HADOOP-1634 and HADOOP-1635, and setting up a DynDNS address pointing to the master's IP, the ec2/bin/start-hadoop script completes.

But the cluster is unusable because the namenode and tasktracker have not started successfully. Looking at the namenode log on the master reveals the following error:
{quote}
2007-07-19 16:54:53,156 ERROR org.apache.hadoop.dfs.NameNode: java.net.BindException: Cannot assign requested address
        at sun.nio.ch.Net.bind(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
        at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:186)
        at org.apache.hadoop.ipc.Server.<init>(Server.java:631)
        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:325)
        at org.apache.hadoop.ipc.RPC.getServer(RPC.java:295)
        at org.apache.hadoop.dfs.NameNode.init(NameNode.java:164)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:211)
        at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:803)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:811)
{quote}

The master node refuses to bind to the DynDNS hostname in the generated hadoop-site.xml. Here is the relevant part of the generated file:
{quote}
<property>
  <name>fs.default.name</name>
  <value>blah-ec2.gotdns.org:50001</value>
</property>

<property>
  <name>mapred.job.tracker</name>
  <value>blah-ec2.gotdns.org:50002</value>
</property>
{quote}

I'll attach a patch against hadoop-trunk that fixes the issue for me, but I'm not sure if this issue is something that someone can fix more thoroughly."
HADOOP-1636,constant should be user-configurable: MAX_COMPLETE_USER_JOBS_IN_MEMORY,"In JobTracker.java:   static final int MAX_COMPLETE_USER_JOBS_IN_MEMORY = 100;

This should be configurable."
HADOOP-1635,Keypair Name Hardcoded,"The keypair name is hardcoded as 'gsg-keypair'  on the
""""""
OUTPUT=`ec2-run-instances $AMI_IMAGE -k gsg-keypair`
""""""
line in 'create-hadoop-image'. And again on the 
""""""
RUN_INSTANCES_OUTPUT=`ec2-run-instances $AMI_IMAGE -n $NO_INSTANCES -g $GROUP -k $KEY_NAME -d ""$NO_INSTANCES,$MASTER_HOST"" | grep INSTANCE | awk '{print $2}'`
""""""
line in 'launch-hadoop-cluster'.


The lines should read
""""""
OUTPUT=`ec2-run-instances $AMI_IMAGE -k $KEY_NAME`
""""""
and
""""""
RUN_INSTANCES_OUTPUT=`ec2-run-instances $AMI_IMAGE -n $NO_INSTANCES -g $GROUP -k $KEY_NAME -d ""$NO_INSTANCES,$MASTER_HOST"" | grep INSTANCE | awk '{print $2}'`
""""""
respectively.
"
HADOOP-1634,EC2 launch-hadoop-cluster awk Problem,"With Amazon 'ec2-api-tools-1.2-11797' and Hadoop 0.13.0 the output from ec2-describe-instances is being incorrectly AWK'd on the last few lines of the launch-hadoop-cluster script.

Specifically, on the
""""""
MASTER_EC2_HOST=`ec2-describe-instances | grep INSTANCE | grep running | awk '{if ($7 == 0) print $4}'`
""""""
line, it should be comparing column $8 rather than column $7.

When this command fails, the hostname of the master doesn't get set correctly and the IP lookup finds incorrect results. The last lines of output from the script look something like this:
""""""
Appointing master
Master is . Please set up DNS so blah.gotdns.org points to D.ROOT-SERVERS.NET.
I.ROOT-SERVERS.NET.
F.ROOT-SERVERS.NET.
M.ROOT-SERVERS.NET.
J.ROOT-SERVERS.NET.
C.ROOT-SERVERS.NET.
H.ROOT-SERVERS.NET.
E.ROOT-SERVERS.NET.
A.ROOT-SERVERS.NET.
K.ROOT-SERVERS.NET.
G.ROOT-SERVERS.NET.
B.ROOT-SERVERS.NET.
L.ROOT-SERVERS.NET..
Press return to continue.
"""""""
HADOOP-1632,IllegalArgumentException in fsck,"The easy way to reproduce this bug is to run fsck when only the name-node is up.
The problem is that NetworkTopology.pseudoSortByDistance() is trying to get nextInt() to the length of 
the array of node locations, which is empty in this case, and nextInt() throws an exception when the argument is not positive.

java.lang.IllegalArgumentException: n must be positive
	at java.util.Random.nextInt(Random.java:248)
	at org.apache.hadoop.net.NetworkTopology.pseudoSortByDistance(NetworkTopology.java:642)
	at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:556)
	at org.apache.hadoop.dfs.NameNode.getBlockLocations(NameNode.java:280)
	at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:154)
	at org.apache.hadoop.dfs.NamenodeFsck.fsck(NamenodeFsck.java:123)
	at org.apache.hadoop.dfs.FsckServlet.doGet(FsckServlet.java:49)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-1629,Block CRC Unit Tests: upgrade test,"HADOOP-1286 introduced a distributed upgrade framework.  1 or more unit tests should be developed that start with a zipped up Hadoop 0.12 file system (that is included in Hadoop's src/test directory under version controlled) and attempts to upgrade it to the current version of Hadoop (ie the version that the tests are running against).  The zipped up file system should include some ""interesting"" files, such as:

- zero length files
- file with replication set higher than number of datanodes
- file with no .crc file
- file with corrupt .crc file
- file with multiple blocks (will need to set dfs.block.size to a small value)
- file with multiple checksum blocks
- empty directory
- all of the above again but with a different io.bytes.per.checksum setting

The class that generates the zipped up file system should also be included in this patch."
HADOOP-1628,Block CRC Unit Tests: protocol tests,"HADOOP-1134 introduced a new protocol between DFS clients and DataNodes.  This protocol needs some unit tests that at least cover these cases:

- bad op codes
- bad field lengths
- bad offsets
- bad versions
- bad block ids"
HADOOP-1627,DFSAdmin incorrectly reports cluster data.,"This is what dfsadmin -report currently prints.

{code}
hadoop:504>bin/hadoop dfsadmin -report
Total raw bytes: 1705541296128 (1.55 TB)
Used raw bytes: 1388220184240 (1.26 TB)
% used: 81.39%

Total effective bytes: 8388847 (8.00 MB)
Effective replication multiplier: 165484.0270945459
-------------------------------------------------
Datanodes available: 3

Name: alive.node.1:50077
State          : In Service
Total raw bytes: 60003868672 (55.88 GB)
Used raw bytes: 21500184005 (20.02 GB)
% used: 35.83%
Last contact: Tue Jul 17 19:58:00 PDT 2007


Name: alive.node.2:50077
State          : In Service
Total raw bytes: 1645537427456 (1.49 TB)
Used raw bytes: 1366720000235 (1.24 TB)
% used: 83.05%
Last contact: Tue Jul 17 19:58:01 PDT 2007


Name: dead.node:50077
State          : In Service
Total raw bytes: 0 (0.0 k)
Used raw bytes: 0 (0.0 k)
% used: NaN%
Last contact: Wed Nov 01 12:18:10 PST 2006
{code}

# ""Effective replication multiplier"" is confusing and prints a strange number
# Datanodes available: 3 - should count only live nodes.
# A dead node ""State"" should not be ""In Service"".
# ""% used:"" should not print ""NaN%""
# We should synchronize dfsadmin reporting with the what we report via the web UI.
Field names and numbers should be the same."
HADOOP-1626,DFSAdmin. Help messages are missing for -finalizeUpgrade and -metasave.,DFSAdmin.printHelp() does not print help for the two commands above.
HADOOP-1625,"""could not move files"" exception in DataXceiver","I am running TestDFSIO with the new crcs.
The test fails with the following exception repeated several times

07/07/17 19:15:50 WARN fs.DFSClient: Error while writing.
java.io.EOFException
    at java.io.DataInputStream.readByte(DataInputStream.java:243)
    at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1663)
    at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1735)
    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:49)
    at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:64)
    at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:773)
    at org.apache.hadoop.fs.TestDFSIO.createControlFile(TestDFSIO.java:129)
    at org.apache.hadoop.fs.TestDFSIO.main(TestDFSIO.java:353)

The data-node log also contains an exception.

07/07/17 19:15:50 ERROR dfs.DataNode: DataXCeiver
java.io.IOException: could not move files for blk_-2838788366095905360 from tmp to C:\ndfs\data\current\blk_-2838788366095905360
    at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:93)
    at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:82)
    at org.apache.hadoop.dfs.FSDataset$FSVolume.addBlock(FSDataset.java:327)
    at org.apache.hadoop.dfs.FSDataset.finalizeBlock(FSDataset.java:593)
    at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:962)
    at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:734)
    at java.lang.Thread.run(Thread.java:595)

In debugger I can see that the meta-data file rename fails.
The data-node runs on windows (if it is relevant).
I mark it is as a blocker until the reverse is proven."
HADOOP-1624,Unknown op code exception in DataXceiver.,"There is a missing break statement in DataNode.DataXceiver.run() which sends DataXceiver into an infinite loop printing

Faulty op: 82
07/07/17 17:52:04 ERROR dfs.DataNode: DataXCeiver
java.io.IOException: Unknown opcode 82in data stream
    at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:742)
    at java.lang.Thread.run(Thread.java:595)

Consequence of HADOOP-1134.

Also a printout two lines below seems unnecessary.
And another three lines below
      LOG.error(""DataXCeiver"", t);
should be spelled DataXceiver"
HADOOP-1623,dfs -cp infinite loop creating sub-directories,"% hadoop dfs -ls data 
Found 2 items
/user/knoguchi/data/aaaa        <r 1>   14949
/user/knoguchi/data/bbbb        <r 1>   14949

% hadoop dfs -cp data data

[hangs]

CTRL-\

Exception in thread ""main"" java.lang.NullPointerException
        at java.io.DeleteOnExitHook.add(DeleteOnExitHook.java:33)
        at java.io.File.deleteOnExit(File.java:939)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:1061)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:1025)
        at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:277)
        at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.create(DistributedFileSystem.java:143)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:360)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.<init>(ChecksumFileSystem.java:371)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:360)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:267)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:107)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:101)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:101)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:101)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:101)
        ....

% hadoop dfs -lsr data 
 .
 .
 .
/user/knoguchi/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/aaaa   <r 1>   14949
/user/knoguchi/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/bbbb   <r 1>   14949
/user/knoguchi/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data   <dir>
/user/knoguchi/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/data/aaaa      <r 1>   14949
  .
  . 
  .
[continues]"
HADOOP-1622,Hadoop should provide a way to allow the user to specify jar file(s) the user job depends on,"More likely than not, a user's job may depend on multiple jars.
Right now, when submitting a job through bin/hadoop, there is no way for the user to specify that. 
A walk around for that is to re-package all the dependent jars into a new jar or put the dependent jar files in the lib dir of the new jar.
This walk around causes unnecessary inconvenience to the user. Furthermore, if the user does not own the main function 
(like the case when the user uses Aggregate, or datajoin, streaming), the user has to re-package those system jar files too.
It is much desired that hadoop provides a clean and simple way for the user to specify a list of dependent jar files at the time 
of job submission. Someting like:

bin/hadoop .... --depending_jars j1.jar:j2.jar 
"
HADOOP-1621,Make FileStatus a concrete class,The existing implementaitons of FileStatus may be easily abstracted into a single base class.
HADOOP-1620,FileSystem should have fewer abstract methods,"The FileSystem API has a number of abstract methods that are implemented identically on all but one FileSystem implementation.  These should instead have a default implementation, with one subclass overridding that implementation.  Fewer abstract methods makes it easier to write new FileSystem implementations."
HADOOP-1619,FSInputChecker attempts to seek past EOF,"I'm not sure which class in the stack trace below is responsible for attempting to seek past the end of file. 

2007-07-16 20:31:40,598 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_200707162028_0014_m_000000_0: java.io.IOException: Cannot seek after EOF
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.seek(DFSClient.java:1040)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:37)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:188)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:234)
	at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:353)
	at org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:331)
	at org.apache.hadoop.fs.FSInputChecker.skip(FSInputChecker.java:306)
	at java.io.FilterInputStream.skip(FilterInputStream.java:125)
	at java.io.FilterInputStream.skip(FilterInputStream.java:125)
	at com.yahoo.pig.impl.io.InputStreamPosition.skip(InputStreamPosition.java:55)
	at java.io.BufferedInputStream.skip(BufferedInputStream.java:349)
	at java.io.FilterInputStream.skip(FilterInputStream.java:125)
	at com.yahoo.pig.impl.builtin.RandomSampleLoader.getNext(RandomSampleLoader.java:34)
	at com.yahoo.pig.impl.mapreduceExec.PigInputFormat$PigRecordReader.next(PigInputFormat.java:169)
	at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:171)
	at com.yahoo.pig.impl.mapreduceExec.PigMapReduce.run(PigMapReduce.java:98)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:189)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1771)"
HADOOP-1613,"The dfs webui (dfshealth) shows ""Last Contact"" as a negative number","The dfshealth.jsp displays the number of seconds that have elapse since the namenode got a heartbeat from a datanode. When the number of nodes in the cluster are huge, may of these values are seen to be negative numbers.

The FSNamesystem returns the timestamp when the namenode had last-heard from a datanode. The dfshealth.jsp gets the current system time and then subtracts the timestamp returned by the Namenode. This ordering of retrieving timestamps cause this problem."
HADOOP-1610,Add metrics for failed tasks,"Add metrics for failed tasks - specifically tasks that kill themselves due to communication failure with the tasktracker (exit status 65), and those that are killed due to lack of progress reporting."
HADOOP-1609,Optimize MapTask.MapOutputBuffer.spill() by not deserialize/serialize keys/values but use appendRaw,"In MapTask.MapOutputBuffer.spill() every key and value is read from buffer and then written to file with append(key, value):

{code}
      DataInputBuffer keyIn = new DataInputBuffer();
      DataInputBuffer valIn = new DataInputBuffer();
      DataOutputBuffer valOut = new DataOutputBuffer();
      while (resultIter.next()) {
        keyIn.reset(resultIter.getKey().getData(), 
                    resultIter.getKey().getLength());
        key.readFields(keyIn);
        valOut.reset();
        (resultIter.getValue()).writeUncompressedBytes(valOut);
        valIn.reset(valOut.getData(), valOut.getLength());
        value.readFields(valIn);
        writer.append(key, value);
        reporter.progress();
      }
{code}

When you have complex objects, like nutch's ParseData or Inlinks, this takes time and creates lots of garbage.

I've created a patch, it seems to be working, only tested on 0.13.0.
It's a bit clumsy, since ValueBytes is cast to Un-/CompressedBytes in SequenceFile.Writer.

Thoughts?"
HADOOP-1605,Automatic namenode restart when it encounters an error situation,"The namenode dies when it encounters an unexpected Runtime Exception. Instead, it can catch exceptions, clears up all its internal data structures and restarts. This was attempted in HADOOP-1486 earlier."
HADOOP-1604,admins should be able to finalize namenode upgrades without running the cluster,"Currently a HDFS cluster must be running in order to finalize an upgrade, but if you shut down without finalizing the new software won't start up. I propose a command line option to the namenode that lets you finalize an upgrade with the cluster down.

{code}
% bin/hadoop namenode -finalize
{code}"
HADOOP-1603,Replication gets set to 1 sometimes when Namenode restarted.,"I have seen this with at least 3-4 weeks old trunk. It is not very deterministic but when the cluster restarts all files get their replication reset to 1. It is not deterministic but not very hard  to reproduce. I could reproduce this on a small cluster. I will add more details. 


"
HADOOP-1602,TaskLog.java buffers entire task userlog in memory,"While working on HADOOP-1524, I noticed that TaskLog.Reader.fetchAll() pulls the entire userlog into a giant byte[] and returns the byte[].  This won't work well for large logs.  TaskLog should return an InputStream instead."
HADOOP-1601,GenericWritable should use ReflectionUtils.newInstance to avoid problems with classloaders,"GenericWritable currently uses Class.newInstance and it should use hadoop.utils.ReflectionUtils.newInstance. Furthermore, GenericWritable should be Configurable and should configure the nested objects. This will prevent a lot of classloader issues and allow the objects to get a configuration."
HADOOP-1599,TestCopyFiles with IllegalArgumentException on Windows,"3 of the TestCopyFiles test cases fail on Windows since 
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/152/
Looks like they all failed due to:

Copy failed: java.lang.IllegalArgumentException: Wrong FS: file://C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/data/srcdat, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:204)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:50)
	at org.apache.hadoop.fs.RawLocalFileSystem.exists(RawLocalFileSystem.java:217)
	at org.apache.hadoop.fs.FilterFileSystem.exists(FilterFileSystem.java:156)
	at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:832)
	at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)
	at org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:166)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)"
HADOOP-1597,Distributed upgrade status reporting and post upgrade features.,"This patch introduces 2 additional features to the distributed upgrade framework,
which turned out to be useful for the crc upgrade HADOOP-1134.
1. I introduce a DFSAdmin command
-upgradeProgress status | details | force
which retrieves current upgrade status, or its detailed status, or forces the upgrade to proceed if it is stuck.
2. If a data-node misses an upgrade it can still join the cluster if the respective upgrade object implements
postUpgradeAction() method.

The framework provides a default implementation of the features.
For (1) any upgrade object returns by default a generic UpgradeStatusReport class, which contains
only the version and the current status of the upgrade.
An attempt to force the upgrade will have no effect except that the name-node will log ""forceProceed() is not defined""
For (2) the default implementation leads to a data-node shutdown, which is consistent with the former behavior."
HADOOP-1596,TestSymLink is failing,TestSymLink started failing sometime today.
HADOOP-1595,Add an option to setReplication method to wait for completion of replication ,"Currently setReplication requested by a client returns immediately, without waiting for completion of replication. There are situations where the client would like to know when the replication is actually done.
This option should be available in fs shell and libhdfs (see HADOOP-1551)."
HADOOP-1594,JobTracker WebUI count of complete/pending tasks inconsistent for the case where speculative tasks are run,"In the JobTracker webui, for successful jobs, I sometimes see inconsistency in the count of the number of various task types whenever some speculative tasks (esp. maps) are executed. So for example, if the job has 1000 maps, I'd expect that on a successful job run, the count of the number of pending map tasks to be 0. But sometimes, when there are speculative tasks, the pending count is greater than 0, and the completed tasks count is less than the number of the total tasks that the job originally had."
HADOOP-1593,FsShell should work with paths in non-default FileSystem,"If the default filesystem is, e.g., hdfs://foo:8888/, one should still be able to do 'bin/hadoop fs -ls hdfs://bar:9999/' or 'bin/hadoop fs -ls s3://cutting/foo'.  Currently these generate a filesystem mismatch exception.  This is because FsShell assumes that all paths are in the default FileSystem.  Rather, the default filesystem should only be used for paths that do not specify a FileSystem.  This would easily be accomplished by using Path#getFileSystem()."
HADOOP-1592,Print the diagnostic error messages for FAILED task-attempts to the user console via TaskCompletionEvents,"It would be very helpful to add the diagnostic error messages (via {{TaskInProgress#getDiagnosticInfo(String)}}) to the user-console via {{TaskCompletionEvents}}.

This is particularly useful for debugging batched jobs e.g. nightlies/unit-tests/benchmarks..."
HADOOP-1590,Jobtracker web interface contains several absolute href links instead of relative ones,So a lot of links are broken if the servlet context is not '/'
HADOOP-1588,HClient should unwrap RemoteException and throw the original exception instead,"When an exception is thrown on the server side, the Hadoop IPC bundles it up into a RemoteException on the client side.

If the original exception was a subclass of IOException or RuntimeException, HClient should unwrap the RemoteException and throw the original exception instead.
"
HADOOP-1587,Tasks run by MiniMRCluster don't get sysprops from TestCases,"While it seems a general problem it is surfacing in streaming TestSymLink testcase with patch for HADOOP-1558

The contrib testcases use src/contrib/test/hadoop-site.xml. 
The property 'mapred.system.dir' is this file is defined as with a variable '${contrib.name}'.
The src/build/build-contrib.xml ant file sets the sysproperty 'contrib.name' to the name of the contrib component for the JVM running the testcase.

The problem is that when a testcase uses MiniMRCluster the TaskRunner forks a JVM for the task and in this JVM (which uses the above hadoop-site.xml) the variable 'contrib.name' is undefined.

If I hardcode 'streaming' in the hadoop-site.xml for the TestSymLink the testcase works fine.

"
HADOOP-1586,Progress reporting thread can afford to be slightly lenient towards exceptions other than ConnectException,"Currently, in the loop of Task.startCommunicationThread, MAX_RETRIES (set to three) attempts are made to report progress/ping (TaskUmbilicalProtocol.progress or TaskUmbilicalProtocol.ping). All attempt failures are counted as critical. Here I am proposing a variant - treat only ConnectException exceptions are critical and treat the others as non-critical. The other exception could be the SocketTimeoutException in the case of the two RPCs. 
The reason why I am proposing this is that since HADOOP-1462 went in, I have been seeing quite a few unexpected 65 deaths, and with some logging it appears that they happen, most of the time, due to the SocketTimeoutException in the progress RPC call (before HADOOP-1462, the return value of progress would not be checked). And when the hack described above was put in, things improved considerably. 
One argument that one might make against the above proposal is that the tasktracker could be faulty, when a task is not able to successfully invoke an RPC on it even though it is able to connect. If this is indeed the case, even in the current scheme of things, the only resort is to restart the tasktracker (either manually, or, the JobTracker asks it to reinitialize), and in both the cases, normal behavior of the protocol will ensure that the child task will die (since the reinited tasktracker is going to return false for the progress/ping calls)."
HADOOP-1585,GenericWritable should use generics,GenericWritable should use generics to clarify the requirement that getTypes() must return classes that extend Writable.
HADOOP-1584,Bug in readFields of GenericWritable,"When getTypes() returns more than 127 entries, read of classes with index > 127 will fail.

{code}
  public void readFields(DataInput in) throws IOException {
    type = in.readByte();
    Class clazz = getTypes()[type];
    ...
  }
{code}

{code}
  public void readFields(DataInput in) throws IOException {
    type = in.readByte();
    Class clazz = getTypes()[type & 0xff];
    ...
  }
{code}
"
HADOOP-1582,hdfsRead and hdfsPread should return 0 instead of -1 at end-of-file.,"As a C interface library, libhdfs should rather follow C-library conventions than Java convention, i.e. read functions should return 0 at end-of-file."
HADOOP-1580,provide better error message when subprocesses fail in hadoop streaming,"As described in
http://mail-archives.apache.org/mod_mbox/lucene-hadoop-user/200707.mbox/browser

bugs in the subprocesses started by hadoop streaming currently result in the less-than-intuitive ""broken pipe"" error.  This patch modifies streaming to indicate when the subprocess fails with ""subprocess exited with error code x"" for non-successful error codes.

A patch will follow.
"
HADOOP-1578,Data-nodes should send storage ID to the name-node during registration,"Data-nodes should send their storage IDs to the name-node during registration otherwise
each start results in assigning a new storageID to the data-node.
This was introduced  by HADOOP-1492."
HADOOP-1576,web interface inconsistencies when using speculative execution,"When using speculative execution, web interface can show inconsistent numbers and status.

E.g. both speculative executed tasks as SUCCEEDED in taskDetails.jsp:

Task Attempts	Machine	Status	Progress	Start Time 	Finish Time	Errors	Task Logs	Counters
task_0005_m_000246_0	fwm521057.inktomisearch.com	SUCCEEDED	100.00%	6-Jul-2007 18:27:10	6-Jul-2007 18:31:36 (4mins, 26sec) 
task_0005_m_000246_1	fwm521107.inktomisearch.com	SUCCEEDED	100.00%	6-Jul-2007 18:28:07	6-Jul-2007 18:32:10 (4mins, 3sec)

or in jobDetails.jsp the number of completed tasks does not match the total number of tasks even after the job has finished."
HADOOP-1573,Support for 0 reducers in PIPES,"Under pipes, when specifying number of reducers to zero, all map tasks fail:

java.io.IOException: pipe child exception
	at org.apache.hadoop.mapred.pipes.Application.abort(Application.java:124)
	at org.apache.hadoop.mapred.pipes.PipesMapRunner.run(PipesMapRunner.java:82)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:189)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1763)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.net.SocketInputStream.read(SocketInputStream.java:182)
	at java.io.DataInputStream.readByte(DataInputStream.java:248)
	at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:313)
	at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:335)
	at org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread.run(BinaryProtocol.java:101)

"
HADOOP-1570,Add a per-job configuration knob to control loading of native hadoop libraries ,"As it exists today, native-hadoop libraries are loaded automatically if libhadoop.so is present; however we have sporadically seen issues (HADOOP-1545) since native direct-buffers aren't very well understood. The only way to switch off usage of these is to remove the native libraries which is a maintenence issue for large clusters...

Hence I propose we add a per-job config knob: {{hadoop.native.lib}} (set to {{true}} by default) which can be used to control usage of native libraries even when the libraries are present e.g. we can have hadoop installed with native libraries present and then use this knob to switch off their usage in rare cases we see issues with them; thus aiding maintenence."
HADOOP-1569,distcp should use the Path -> FileSystem interface like the rest of Hadoop,"DistCp should use the standard Path to FileSystem API that the rest of Hadoop uses. By explicitly testing the protocol string, it is much more brittle."
HADOOP-1568,NameNode Schema for HttpFileSystem,"This issue will track the design and implementation of (the first pass of) a servlet on the namenode for querying its filesystem via HTTP. The proposed syntax for queries and responses is as follows.

*Query*
{noformat}GET http://<nn>:<port>/ls.jsp[<?option>[&option]*] HTTP/1.1{noformat}

Where _option_ may be any of the following query parameters:
_path_ : String (default: '/')
_recursive_ : boolean (default: false)
_filter_ : String (default: none)

*Response*
The response will be returned as an XML document in the following format:
{noformat}
<listing path=""..."" recursive=""(yes|no)"" filter=""...""
         time=""yyyy-MM-dd hh:mm:ss UTC"" version=""..."">
  <directory path=""...""/>
  <file path=""..."" modified=""yyyy-MM-dd hh:mm:ss"" blocksize=""...""
        replication=""..."" size=""...""
        dnurl=""http://dn:port/streamFile?...""/>
</listing>
{noformat}"
HADOOP-1567,Build version verification on DataNode should be returned back,"HADOOP-1269 removed build version verification from DataNode.handshake() method.
I think this was done by a mistake.
Build version consistency of the name-node and data-nodes is an important part of version upgrade.
"
HADOOP-1565,DFSScalability: reduce memory usage of namenode,"Experiments have demonstrated that a single file/block needs about 300 to 500 bytes of main memory on a 64-bit Namenode. This puts some limitations on the size of the file system that a single namenode can support. Most of this overhead occurs because a block and/or filename is inserted into multiple TreeMaps and/or HashSets.

Here are a few ideas that can be measured to see if an appreciable reduction of memory usage occurs:

1. Change FSDirectory.children from a TreeMap to an array. Do binary search in this array while looking up children. This saves a TreeMap object for every intermediate node in the directory tree.
2. Change INode from an inner class. This saves on one ""parent object"" reference for each INODE instance. 4 bytes per inode.
3. Keep all DatanodeDescriptors in an array. BlocksMap.nodes[] is currently a 64-bit reference to the DatanodeDescriptor object. Instead, it can be a 'short'. This will probably save about 16 bytes per block.
4. Change DatanodeDescriptor.blocks from a SortedTreeMap to a HashMap? Block report processing CPU cost can increase.

For the records: TreeMap has the following fields:
	Object key;
	Object value;
	Entry left = null;
	Entry right = null;
	Entry parent;
	boolean color = BLACK;

and HashMap object:
	final Object key;
	Object value;
	final int hash;
	Entry next;
"
HADOOP-1564,Write unit tests to detect CRC corruption,The unit tests should have some way to test the case when CRC files are corrupted.
HADOOP-1563,Create FileSystem implementation to read HDFS data via http,"There should be a FileSystem implementation that can read from a Namenode's http interface. This would have a couple of useful abilities:
  1. Copy using distcp between different versions of HDFS.
  2. Use map/reduce inputs from a different version of HDFS. "
HADOOP-1562,Report Java VM metrics,"
It would be useful to have each Java process in Hadoop (JobTracker, TaskTracker, NameNode and DataNode) report some Java VM metrics.  E.g. heap/non-heap memory used/committed, number of garbage collections and percentage of time spent in GC, number of threads that are runnable/blocked/waiting/etc.

"
HADOOP-1558,changes to OutputFormat to work on temporary directory to enable re-running crashed jobs (Issue: 1121),"Add  OutputFormat methods like:

/** Called to initialize output for this job. */
void initialize(JobConf job) throws IOException;

/** Called to finalize output for this job. */
void commit(JobConf job) throws IOException;

In the base implemenation for FileSystem output, initialize() might then create a temporary directory for the job, removing any that already exists, and commit could rename the temporary output directory to the final name. 

The existing checkOutputSpecs() would continue to throw an exception if the final output already exists."
HADOOP-1557,Deletion of excess replicas should prefer to delete corrupted replicas before deleting valid replicas,"Suppose a block has three replicas and two of the replicas are corrupted. If the replication factor of the file is reduced to 2. The filesystem should preferably delete the two corrupted replicas, otherwise it could lead to a corrupted file.

One option would be to make the datanode periodically validate all blocks with their corresponding CRCs. The other option would be to make the setReplication call validate existing replicas before deleting excess replicas."
HADOOP-1556,9 unit test failures: file.out.index already exists,"In my nightly unit testing, 9 unit tests failed June 25 and again July 2, all with this exception:

java.io.IOException: Target build/test/mapred/local/map_0005/file.out.index already exists
	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:246)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:93)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:207)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:476)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:490)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:593)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:190)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:131)

I'm concerned this could be a race conditrion.  The unit tests are:
org.apache.hadoop.fs.TestDFSIO.testIOs
org.apache.hadoop.fs.TestFileSystem.testFs
org.apache.hadoop.mapred.TestAggregates.testAggregates
org.apache.hadoop.mapred.TestFieldSelection.testFieldSelection
org.apache.hadoop.mapred.TestLocalMRNotification.testMR
org.apache.hadoop.mapred.TestMapOutputType.testNoMismatch
org.apache.hadoop.mapred.TestMapRed.testMapred
org.apache.hadoop.mapred.TestMapRed.testCompression
org.apache.hadoop.record.TestRecordMR.testMapred"
HADOOP-1554,"Fix the JobHistory to display things like the number of nodes the job ran on, the number of killed/failed tasks","The JobHistory jsp page doesn't display things like the number of nodes the job ran on (which is important to compare results like total execution time, of the same job, from multiple runs). Currently, the JobHistory doesn't distinguish between failed and killed tasks and classifies both these as failed. These should be distinguished to interpret the history better."
HADOOP-1553,Extensive logging of C++ application can slow down task by an order of magnitude,"We observed that extensive logging (due to some configuration mistake) of a c++ application using the pipes interface can slow down the task by an order of magnitude. During that time disk usage was not high, with no abnormal memory usage, and basically idle CPU."
HADOOP-1552,touchFile support in libhdfs,Sometimes it would be convenient to just create an empty file stamp with a single call to some libhdfs method. There seems to be support for it in java: FileSystem.createNewFile.
HADOOP-1551,libhdfs API is out of sync with Filesystem API,Right now libhdfs only allows to set replication at file creation time. Would be very convenient to have a libhdfs method dedicated to set the replication of a file.
HADOOP-1547,Provide examples for aggregate library,"The abacus examples were not moved to aggregate when it was created. Provide an aggregate example which shows users how to use the aggregate library.

As a part of this issue move TestAggregates to the org.apache.hadoop.mapred.lib.aggregate package."
HADOOP-1546,The DFS WebUI shows an incorrect column for file Creatin Time,
HADOOP-1545,Maps hang when using native compression library to compress map out,This issue is related to HADOOP-1193. I ran the same job. Each map of this job spawns a process which generates output to be cosumed by the map. The new native compression librar seems to hang some of the spawned processes. Some processes print using more than 100% CPU. Non-native library does not produce this problem.
HADOOP-1542,Incorrect task/tip being scheduled (looks like speculative execution),"The change in HADOOP-1440 broke map/reduce by breaking the assumption that Task.getPartition() corresponded to the JobInProgress.map[] order. 

Currently JobInProgress.findNewTask uses Task.getPartition as the index of the map to run. This can be a completely different tip, which will cause incorrect tasks to be run, including duplicates of tasks that are already running."
HADOOP-1541,ipc.Server INFO message shouldn't include an Exception trace,"I see a lot of these in the NN log.  I don't think an INFO message should contain an error message and stack trace.

2007-06-28 00:21:51,057 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020, call getFileInfo(/user/hadoopqa/mapred.loadtest/intermediateouts) from 2.2.2.2:47476: error: java.io.IOException: File does not exist
java.io.IOException: File does not exist
        at org.apache.hadoop.dfs.FSDirectory.getFileInfo(FSDirectory.java:716)
        at org.apache.hadoop.dfs.FSNamesystem.getFileInfo(FSNamesystem.java:1178)
        at org.apache.hadoop.dfs.NameNode.getFileInfo(NameNode.java:479)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
"
HADOOP-1539,distcp overwrites destination files by default,"This is risky. distcp should instead accept a flag that indicates if destination files can be overwritten (overwrite all files, overwrite only if dest timestamp is older than source etc). The default behavior should be to not overwrite any files."
HADOOP-1536,libhdfs tests failing,"Starting today, 2 libhdfs tests are failing on Linux when I run
 ""ant -Dtest.junit.output.format=xml -Dtest.output=yes -Dcompile.native=yes package-libhdfs tar test-core test-libhdfs""

     [exec] Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: java.io.IOException: Failure when trying to obtain lock on /tmp/.testfile.txt.crc
     [exec] 	at org.apache.hadoop.dfs.NameNode.obtainLock(NameNode.java:441)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:470)
     [exec] 	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
     [exec] 	at org.apache.hadoop.dfs.$Proxy0.obtainLock(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:585)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
     [exec] 	at org.apache.hadoop.dfs.$Proxy0.obtainLock(Unknown Source)
     [exec] 	at org.apache.hadoop.dfs.DFSClient.lock(DFSClient.java:478)
     [exec] 	at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.lock(DistributedFileSystem.java:195)
     [exec] 	at org.apache.hadoop.fs.ChecksumFileSystem.lock(ChecksumFileSystem.java:548)
     [exec] Call to org.apache.fs.FileSystem::lock failed!
     [exec] hdfsLock: Failed!

     [exec] Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: java.io.IOException: Failure when trying to release lock on /tmp/.testfile.txt.crc
     [exec] 	at org.apache.hadoop.dfs.NameNode.releaseLock(NameNode.java:453)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:585)
     [exec] 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
     [exec] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
     [exec] 	at org.apache.hadoop.ipc.Client.call(Client.java:470)
     [exec] 	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
     [exec] 	at org.apache.hadoop.dfs.$Proxy0.releaseLock(Unknown Source)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [exec] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
     [exec] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
     [exec] 	at java.lang.reflect.Method.invoke(Method.java:585)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
     [exec] 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
     [exec] 	at org.apache.hadoop.dfs.$Proxy0.releaseLock(Unknown Source)
     [exec] 	at org.apache.hadoop.dfs.DFSClient.release(DFSClient.java:498)
     [exec] 	at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.release(DistributedFileSystem.java:200)
     [exec] 	at org.apache.hadoop.fs.ChecksumFileSystem.release(ChecksumFileSystem.java:561)
     [exec] Call to org.apache.hadoop.fs.FileSystem::release failed!
     [exec] hdfsReleaseLock: Failed!
"
HADOOP-1535,Wrong comparator used to merge files in Reduce phase,"As per the fix for HADOOP-485, we allow users to optionally provide a different comparator to group values when calling the user's Reduce function. Devaraj and I were looking at the code yesterday and we found that in ReduceTask.java, we use the user-supplied comparator to merge the output files from the Map tasks (we use the user-supplied comparator when creating a new SequenceFile.Sorter object). This is incorrect as the comparator used to merge Map output files should be the same as that used to create those files in the Map phase. The user-supplied comparator for grouping values should be used only in the iterator passed to the user's Reduce function (which is done correctly in the code). "
HADOOP-1533,Distcp should log to specified location,Distcp errors are now not logged anywhere (other that reporter.setStatus which gets overwritten). Add support to mention a log URI where the log files should go.
HADOOP-1532,Distcp should support verification modes,"distcp doesnot currently support any verification after copying files. It should support 
1. verify quick (vq) mode - which compares the source and destination CRCs
2. verify long (vl) mode - which in addition to verify quick should read the entire destination file to catch DFS block level errors"
HADOOP-1529,user logs do not seem to be flushed,"The user logs of applications using pipes interface often do not seem to have the user logs flushed. In particular, log messages from the close() method are often missing."
HADOOP-1524,Task Logs userlogs don't show up for a while ,"When I start a task and go to the task logs, nothing shows up for a while.  An examination of TaskLog.Writer and TaskLog.Reader reveals:

1. The TaskLog.Reader relies on the presence of a split.idx to identify the parts of the logs to display.
2. The TaskLog.Writer only updates the split.idx file when it moves on to the next log.

As a result, updates to the log only get pushed when an entire file is done.

Why is there a split.idx file?  It seems that since files are called part-00000, part-00001, etc., the TaskLog.Reader can just look at all files and arrange them by alphabetical order.  The split.idx file also contains file length, but this data is already stored by the filesystem.

If nobody has objections, I'd like to write a patch to eliminate the split.idx file.
"
HADOOP-1520,IndexOutOfBoundsException in FSEditLog.processIOError,"Running NNBench I saw these exceptions in the NameNode logs:

NAMENODE:
2007-06-21 04:02:53,587 INFO org.apache.hadoop.fs.FSNamesystem: Roll FSImage
2007-06-21 04:02:53,594 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020, call complete(/user/hadoopqa/nameNode100Benchmark/output/.214.crc, DFSClient_-1646448212) from 72.30.51.75:56245: error: java.io.IOException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
java.io.IOException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.remove(ArrayList.java:387)
        at org.apache.hadoop.dfs.FSEditLog.processIOError(FSEditLog.java:169)
        at org.apache.hadoop.dfs.FSEditLog.logSync(FSEditLog.java:407)
        at org.apache.hadoop.dfs.FSNamesystem.completeFile(FSNamesystem.java:878)
        at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:346)
        at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)

...

2007-06-21 04:02:53,823 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.completeFile: failed to complete /user/hadoopqa/nameNode100Benchmark/output/.214.crc because dir.getFileBlocks() is non-null and pendingFile is null
2007-06-21 04:02:53,828 INFO org.apache.hadoop.ipc.Server: IPC Server handler 14 on 8020, call complete(/user/hadoopqa/nameNode100Benchmark/output/.214.crc, DFSClient_-1646448212) from 72.30.51.75:56245: error: java.io.IOException: Could not complete write to file /user/hadoopqa/nameNode100Benchmark/output/.214.crc by DFSClient_-1646448212
java.io.IOException: Could not complete write to file /user/hadoopqa/nameNode100Benchmark/output/.214.crc by DFSClient_-1646448212
        at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:352)
        at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)

This last exception occurs over and over again forever (this could be a result of the way NNBench is written)."
HADOOP-1518,Add session id to metric data,"
In the context of Hadoop-On-Demand (HOD), we want to be able to aggregate metrics across the machines in the virtual cluster that corresponds to a HOD session. To enable this, we'd like to tag all the map/reduce related metrics with a session id.  The session id would be a string created by HOD (it might be a Torque job id) and put into the hadoop-site.xml file for the virtual cluster.

The default value for the session id would be the empty string, so that non-HOD usage of the metrics is not affected accept for the addition of a tag which is always the empty string.



"
HADOOP-1517,Three methods in FSNamesystem should not be synchronized.,"Joining my UTF8 patch HADOOP-1283 with synchronization changes introduced by HADOOP-1269
I missed to remove synchronization modifiers in the following three methods:
startFile()
getAdditionalBlock()
allocateBlock()

Fine grained locking does not work if the first two methods are synchronized.
This is the patch that removes them. "
HADOOP-1515,"MultiFileSplit, MultiFileInputFormat","An {{InputSplit}} and {{InputFormat}} implementation for jobs that require to read records from many files. The input is partitioned by files. This can be used for example to implement {{RecordReader}}s which read one record from a file. 




"
HADOOP-1514,Progress reporting not handled for the case where a reducer currently doesn't have anything to fetch,"In some apps (like the randomwriter), the maps take a long time to complete, and there could be cases where a reducer would just be waiting for map(s) to complete. During that time progress reporting is not done."
HADOOP-1513,A likely race condition between the creation of a directory and checking for its existence in the DiskChecker class,"Got this exception in a job run. It looks like the problem is a race condition between the creation of a directory and checking for its existence. Specifically, the line:
if (!dir.exists() && !dir.mkdirs()), doesn't seem safe when invoked by multiple processes at the same time. 

2007-06-21 07:55:33,583 INFO org.apache.hadoop.mapred.MapTask: numReduceTasks: 1
2007-06-21 07:55:33,818 WARN org.apache.hadoop.fs.AllocatorPerContext: org.apache.hadoop.util.DiskChecker$DiskErrorException: can not create directory: /export/crawlspace/kryptonite/ddas/dfs/data/tmp
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:26)
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createPath(LocalDirAllocator.java:211)
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:248)
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:276)
	at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:155)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:1171)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.(DFSClient.java:1136)
	at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:342)
	at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.create(DistributedFileSystem.java:145)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.(ChecksumFileSystem.java:368)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:254)
	at org.apache.hadoop.io.SequenceFile$Writer.(SequenceFile.java:675)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:165)
	at org.apache.hadoop.examples.RandomWriter$Map.map(RandomWriter.java:137)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:189)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1740)

2007-06-21 07:55:33,821 WARN org.apache.hadoop.mapred.TaskTracker: Error running child"
HADOOP-1512,TestTextInputFormat fails on Windows,"Here is the output produced by the test:

Testsuite: org.apache.hadoop.mapred.TestTextInputFormat
Tests run: 5, Failures: 1, Errors: 0, Time elapsed: 8.828 sec
------------- Standard Output ---------------
2007-06-20 18:49:53,064 INFO  mapred.TestTextInputFormat (TestTextInputFormat.java:testFormat(58)) - seed = -1045123620
2007-06-20 18:50:01,830 WARN  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(50)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
------------- ---------------- ---------------

Testcase: testFormat took 8.75 sec
Testcase: testUTF8 took 0 sec
Testcase: testNewLines took 0 sec
Testcase: testGzip took 0.031 sec
Testcase: testGzipEmpty took 0.047 sec
	FAILED
Compressed files of length 0 are not returned from FileInputFormat.getSplits(). expected:<1> but was:<3>
junit.framework.AssertionFailedError: Compressed files of length 0 are not returned from FileInputFormat.getSplits(). expected:<1> but was:<3>
	at org.apache.hadoop.mapred.TestTextInputFormat.testGzipEmpty(TestTextInputFormat.java:248)"
HADOOP-1510,java.io.IOException: Attempt to roll edit log but edits.new exists,"I updated to hadoop-0.13, deleted the dfs/ directory, formatted the namenode, and then brought the namenode up.  Before any datanodes connected, I got this in my namenode log:

2007-06-20 22:16:38,960 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 10000: starting
2007-06-20 22:16:43,226 WARN org.apache.hadoop.dfs.StateChange: DIR* FSDirectory.unprotectedDelete: failed to re
move /hadoop/mapred/system because it does not exist
2007-06-20 22:20:19,345 INFO org.apache.hadoop.fs.FSNamesystem: Roll Edit Log
2007-06-20 22:25:19,440 INFO org.apache.hadoop.fs.FSNamesystem: Roll Edit Log
2007-06-20 22:25:19,442 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 10000, call rollEditLog() fro
m x.x.44.136:35679: error: java.io.IOException: Attempt to roll edit log but edits.new exists
java.io.IOException: Attempt to roll edit log but edits.new exists
        at org.apache.hadoop.dfs.FSEditLog.rollEditLog(FSEditLog.java:467)
        at org.apache.hadoop.dfs.FSNamesystem.rollEditLog(FSNamesystem.java:3239)
        at org.apache.hadoop.dfs.NameNode.rollEditLog(NameNode.java:544)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)
"
HADOOP-1508,ant Task for FsShell operations,"This issue will document the requirements, design and implementation of an ant Task providing FsShell functionality within that framework."
HADOOP-1507,"distcp skipping healthy files with  ""-i"" option","Copied about million files.  8 files were corrupted. (checksum errors)

When I used distcp  ""-i"", (skip read error files), it skipped about 300 files.

I need to check if it was really read errors or if distcp is skipping files more than necessary.

"
HADOOP-1506,distcp not preserving the replication factor and block size of source files,"Myabe not a bug but a feature request.
It would be nice if the source file and the target file have the same replication factor and block size.

 "
HADOOP-1504,terminate-hadoop-cluster may be overzealous,"If folks are using EC2 for things besides Hadoop, then the terminate-hadoop-cluster script (in src/contrib/s3/bin) will kill all instances, not just Hadoop instances.  This could cause loss of work.

http://developer.amazonwebservices.com/connect/thread.jspa?threadID=15699"
HADOOP-1503,Fix for broken build by HADOOP-1498,"Application of HADOOP-1498 broke the build.  A class included in the patch, ImmutableBytesWritable, was not added. "
HADOOP-1501,Block reports from all datanodes arrive at the namenode within a small band of time,"I have a 2000 node cluster and the block report interval is set to 1 hour. Most block report arrive within a few minutes of one another. For example, I have seen block reports from all 2000 nodes arrive within 5 minutes of one another. This causes CPU overload on the namenode, causing dropped calls in Call queue.

My proposal is to make the datanode send a block report as soon as the datanode starts. Then, it waits for a random time between 0 to 1 hour (the configured value) before sending the nect block report. From then on, block reports from that datanode are sent once every 1 hour (the configured value)."
HADOOP-1500,typo's in dfs webui,"browseBlock.jsp:
  - ""Chunk Size ..."" should be ""Chunck size ...""
  - ""... upto ..."" should be ""... up to ...""
  - ""... blocksize ..."" should be ""... block size ...""
  - ""View Next chunk"" should be ""View next chunk""

browseDirectory.jsp:
  - ""BlockSize"" should be ""Block Size""

dfshealth.jsp:
  - Cluster Summary table: the colon (:) should be left justified

All jsp's:
  - 2006 should be 2007"
HADOOP-1499,distcp creating extra directory when copying one file.,"If I try to copy one file by distcp 

    hadoop distcp   hdfs://aaaa:9999/abc/efg hdfs://bbbb:9999/abc/efg 

on the target cluster, the file is  copied to 

/abc/efg/efg

creating one extra depth.

I wasn't sure if this was an expected behavior.
"
HADOOP-1497,Possibility of duplicate blockids if dead-datanodes come back up after corresponding files were deleted,"Suppose a datanode D has a block B that belongs to file F. Suppose the datanode D dies and the namenode replicates those blocks to other datanodes. No, suppose the user deletes file F. The namenode removes all the blocks that belonged to file F. Now, suppose a new file F1 is created and the namenode generates the same blockid B for this new file F1. 

Suppose the old datanode D comes back to life. Now we have a valid corrupted block B on datanode D.

This case is possibly detected by the Client (using CRC). But does HDFS need to handle this scenario better?"
HADOOP-1494,"TaskRunner.java unconditionally sets child vm hadoop.root.logger to INFO,TLA","When TaskRunner.java spins up a task in a child vm, it sets -Dhadoop.root.logger=INFO,TLA.  This seems wrong.  Maybe it would be better as System.getProperty(""hadoop.root.logger"")?"
HADOOP-1493,possible double setting of java.library.path introduced by HADOOP-838,"HADOOP-838 introduced setting java.library.path to run the {{TaskTracker$Child}} from the TaskTracker's library.path, so that the native lib path is passed to the child. However if we also want to set -Djava.library.path from the configuration by ""mapred.child.java.opts"", than the child java process is called with two -Djava.library.path properties, and java seems to ignore the former, rather than use the two. This situation prevents using -Djava.library.path in ""mapred.child.java.opts"". 

"
HADOOP-1492,DataNode version mismatch during handshake() causes NullPointerException.,"If during handshake() DataNode encounters version mismatch with the name-node it sends errorReport()
to the name-node with dnRegistration = null. This causes a NullPointerException on the name-node.
"
HADOOP-1491,"After successful distcp, couple of checksum error files","Tried copying 700,000 files  with distcp. 8 mappers per node.  Single dfs.client.buffer.dir.
Distcp ran on 25 nodes mapreduce.

Couple of tasks failed, but job was successful. 

When checked, 12  files were corrupted. (Checksum error)

This is repeatable.

I'll add more information as we find.



"
HADOOP-1490,Retry frame work retries once more than maxRetries,"
Looks like a typo. RetryPolicies with 'maxRetries' in io.retry retry once more than requested.

I think shoulRetry() check for {{ '(retries  < maxRetries)' }} instead of  {{ '(retries <= maxRetries)' }}. "
HADOOP-1489,Input file get truncated for text files with \r\n,"When input file has \r\n, LineRecordReader uses mark()/reset() to read one byte ahead to check if \r is followed by \n.   This probably caused the BufferedInputStream to issue a small read request (e.g., 127 bytes).   The  ChecksumFileSystem.FSInputChecker.read() code 
{code}
   public int read(byte b[], int off, int len) throws IOException {
     // make sure that it ends at a checksum boundary
     long curPos = getPos();
     long endPos = len+curPos/bytesPerSum*bytesPerSum;
     return readBuffer(b, off, (int)(endPos-curPos));
   }
{code}
tries to truncate ""len"" to checksum boundary.  For DFS, bytesPerSum is 512.  So for small reads, the truncated length become negative (i.e., endPos - curPos is < 0).   The underlying DFS read returns 0 when length is negative.  However, readBuffer changes it to -1 assuming end-of-file has been reached.   This means effectively, the rest of the input file did not get read.  In my case, only 8MB of a 52MB file is actually read.   Two sample stacks are appended.

One related issue, if there are assumptions (such as len >= bytesPerSum) in FSInputChecker's read(), would it be ok to add a check that throws an exception when the assumption is violated?   This assumption is a bit unusal and as code changes (both Hadoop and Java's implementation of BufferedInputStream), the assumption may get violated.  This silently dropping large part of input seems really difficult for people to notice (and debug) when people start to deal with terabytes of data.   Also, I suspect the performance impact for such a check would not be noticed.

bwolen

Here are two sample stacks.  (i have readbuffer throw when it gets 0 bytes, and have inputchecker catches the exception and rethrow both.  This way, I catch the values from both caller and callee (see the callee one starts with ""Caused by"")

-------------------------------------
{code}
java.lang.RuntimeException: end of read()
in=org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker len=127
pos=45223932 res=-999999
       at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:50)
       at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
       at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:116)
       at java.io.FilterInputStream.read(FilterInputStream.java:66)
       at org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:132)
       at org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:124)
       at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:108)
       at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:168)
       at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:44)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:186)
       at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1720)


Caused by: java.lang.RuntimeException: end of read()
datas=org.apache.hadoop.dfs.DFSClient$DFSDataInputStream pos=45223932
len=-381 bytesPerSum=512 eof=false read=0
       at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:200)
       at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:175)
       at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:47)
       ... 11 more
---------------

java.lang.RuntimeException: end of read()  in=org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker len=400 pos=4503 res=-999999
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:50)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:116)
	at java.io.FilterInputStream.read(FilterInputStream.java:66)
	at org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:132)
	at org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:124)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:108)
	at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:168)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:44)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:186)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1720)

Caused by: java.lang.RuntimeException: end of read()  datas=org.apache.hadoop.dfs.DFSClient$DFSDataInputStream pos=4503 len=-7 bytesPerSum=512 eof=false read=0
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:200)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:175)
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:47)
	... 11 more

{code}"
HADOOP-1488,Remove all use of auto-progress threads in map/reduce,"During the last rework of the shuffle, a lot of the stages of the shuffle had auto-progress threads added, leading to system lockups when the shuffle stalls. We need to add Progressables to the sort and fetching interfaces so that if any task gets stuck, it will eventually be killed by the framework."
HADOOP-1486,ReplicationMonitor thread goes away ,"Saw many over/under replicated blocks in fsck output.

.out file showed


Exception in thread ""org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@2785982c"" java.lang.IllegalArgumentException: Unexpected non-existing data node: /99.9.99.0/99.9.99.42:99999
  at org.apache.hadoop.net.NetworkTopology.checkArgument(NetworkTopology.java:379)
  at org.apache.hadoop.net.NetworkTopology.isOnSameRack(NetworkTopology.java:424)
  at org.apache.hadoop.dfs.FSNamesystem$ReplicationTargetChooser.chooseTarget(FSNamesystem.java:2853)
  at org.apache.hadoop.dfs.FSNamesystem$ReplicationTargetChooser.chooseTarget(FSNamesystem.java:2816)
  at org.apache.hadoop.dfs.FSNamesystem.pendingTransfers(FSNamesystem.java:2658)
  at org.apache.hadoop.dfs.FSNamesystem.computeDatanodeWork(FSNamesystem.java:1774)
  at org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor.run(FSNamesystem.java:1723)
  at java.lang.Thread.run(Thread.java:619)

(same as HADOOP-1232)

And, jstack showed no ReplicationMonitor thread."
HADOOP-1485,Metrics should be there for reporting shuffle failures/successes,It would be nice to have metrics for the shuffle phase which reports the failures/successes for the fetches. This would aid in performance tests and in debugging (shuffle).
HADOOP-1484,Kill jobs from web interface,Killing job's is only possible through command line. We need killing job from the web interface. Ideally all the actions that can be done using command line tools should be available in the web interface 
HADOOP-1482,SecondaryNameNode does not roll ports,"The SecondaryNamenode does not roll the port specified as dfs.secondary.info.port. This could cause unit tests to fail if an instance of ScondaryNameNode is already running on the test machine.
"
HADOOP-1481,distcp skips empty directory when copying,
HADOOP-1480,pipes should be able to set user counters,Pipes should expose the user defined counters.
HADOOP-1479,NPE in HStore#get if StoreFile only has keys < than passed key,HStore#get throws a NPE because it doesn't allow MapFile.Reader#getClosest returning null. MapFile.Reader#getClosest returns null if passed a key that is > than all keys contained in the MapFile.
HADOOP-1478,The blockStream of DFSClient.FSInputStream should not be buffered,"Because DFSDataInputStream is buffered, the containing FSInputStream does not need to be buffered to avoid memory overhead and an extra copy."
HADOOP-1475,local filecache disappears,"All our jobs on a 600 node cluster fail. Symptom is that the local filecache disappears.

It might have to do with the fact that lost task trackers get re-initialized when they send a heartbeat again, and purge the local directory completely without updating the filecache.

Side issue is;
why do we get so many lost tasktrackers which then resume the heartbeat (a kind of 'bogus' lost tasktracker)?. We lost tasktrackers:
13 in the 1st hour of the job
18 in the 2nd hour
33 in the 3rd hour
Then the job failed.

E.g. all the tasktrackers lost in the first 2 hours of the job got logged sometime later with a 'Status from unknown Tracker' in the jobtracker log and got reinitialized.

I attach some jobracker log messages showing how the heartbeat of the lost tasktrackers come in late, sometimes less than 1 minute late, sometimes up to 16 minutes. What could be the reason? Do the heartbeats get lost? 



2007-06-07 13:09:08,518 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_070
2007-06-07 13:09:48,919 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_070

2007-06-07 13:39:08,740 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_075
2007-06-07 13:41:50,810 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_075

2007-06-07 14:32:29,093 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_082
2007-06-07 14:35:34,217 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_082

2007-06-07 14:15:48,856 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_085
2007-06-07 14:20:21,337 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_085

2007-06-07 15:25:49,524 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_098
2007-06-07 15:33:56,732 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_098

2007-06-07 14:49:09,203 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_106
2007-06-07 14:54:25,538 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_106

2007-06-07 15:02:29,337 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_108
2007-06-07 15:02:57,558 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_108

2007-06-07 14:19:09,022 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_112
2007-06-07 14:19:15,273 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_112

2007-06-07 14:19:08,881 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_114
2007-06-07 14:30:03,354 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_114

2007-06-07 15:42:29,579 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_116
2007-06-07 15:43:06,422 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_116

2007-06-07 14:55:49,280 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_117
2007-06-07 14:56:38,452 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_117

2007-06-07 15:15:49,461 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_120
2007-06-07 15:31:37,028 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_120

2007-06-07 15:09:09,435 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker tracker_174
2007-06-07 15:18:31,254 WARN org.apache.hadoop.mapred.JobTracker: Status_from_unknown_Tracker : tracker_174


"
HADOOP-1474,"Submittable interface, for the ability to execute and monitor jobs from a java class","Hi,

We wish to add the following interface:
interface Submittable{
           RunningJob submitJob(JobConf jc);
}

Currently there is no clean way to monitor a submitted job programatically, one way would be to call main, and parse the output to figure out Jobid,  and then monitor it using a JobClient.  Currently the only way RunJar can run a Class in a Jar file is using main.invoke(). 

The purpose of this interface is to be able to programatically call the class that extends this interface via another Class (similar to RunJar) and still be able to monitor it like JobClient does.
Essentially, all the functionality within a main class would be encapsulated within this method such as implementing constraints between various user defined job.xml keys.
The purpose of main would be reduced to parsing arguments, setting the JobConf and calling this function.
"
HADOOP-1473,Make jobids unique across jobtracker restarts,"I'll make the job ids unique across JobTracker restarts by adding the startup time of the JobTracker, so if the JobTracker started at 8 Jun 2007 14:50, the first job would be called:

job_200706081450_00001

the second job would be:

job_200706081450_00002

and so on..."
HADOOP-1472,Timed-out tasks are marked as 'KILLED' rather than as 'FAILED' which means the framework doesn't fail a TIP with 4 or more timed-out attempts,"Timed-out tasks (and also tasks which fail with {{FSError}}) are marked as {{KILLED}} rather than as {{FAILED}}. The major issue with this is that post HADOOP-1050 only {{FAILED}} task-attempts are considered to decide if the {{TIP}} has failed, and hence there exists a corner case where a {{TIP}} which has 4 timed-out tasks isn't marked as {{FAILED}} and thus the job keeps running too...

Considering this is a corner-case and is going to entail not-too-insignificant changes to {{TaskTracker}}'s control-flow (ugly as it is right now), I'm proposing to fix this either for 0.13.1 (if need be) or better: 0.14.

Thoughts?"
HADOOP-1471,seekToNewSource() might not work correctly with Checksum failures.,"Patch submitted to HADOOP-893  (by me :( ) seemhave a bug in how it deals with the set {{deadNodes}}. After the patch, the {{seekToNewSource()}} looks like this :

{code}
    public synchronized boolean seekToNewSource(long targetPos) throws IOException {
      boolean markedDead = deadNodes.contains(currentNode);
      deadNodes.add(currentNode);
      DatanodeInfo oldNode = currentNode;
      DatanodeInfo newNode = blockSeekTo(targetPos);
      if (!markedDead) {
        /* remove it from deadNodes. blockSeekTo could have cleared 
         * deadNodes and added currentNode again. Thats ok. */
        deadNodes.remove(oldNode);
      }
      // ...
{code}

I guess with the expectation that caller of this function decides before the call whether to put the node in {{deadNodes}} or not. I am not sure whether this was a bug then or not but it certainly seems to be bug now. i.e. when there is a checksum error with replica1, we try replica2 and if there a checksum error again, then we try replica1 again! 

Note that ChecksumFileSystem.java was created after HADOOP-893 was resolved.

"
HADOOP-1470,Rework FSInputChecker and FSOutputSummer to support checksum code sharing between ChecksumFileSystem and block level crc dfs,"Comment from Doug in HADOOP-1134:
I'd prefer it if the CRC code could be shared with CheckSumFileSystem. In particular, it seems to me that FSInputChecker and FSOutputSummer could be extended to support pluggable sources and sinks for checksums, respectively, and DFSDataInputStream and DFSDataOutputStream could use these. Advantages of this are: (a) single implementation of checksum logic to debug and maintain; (b) keeps checksumming as close to possible to data generation and use. This patch computes checksums after data has been buffered, and validates them before it is buffered. We sometimes use large buffers and would like to guard against in-memory errors. The current checksum code catches a lot of such errors. So we should compute checksums after minimal buffering (just bytesPerChecksum, ideally) and validate them at the last possible moment (e.g., through the use of a small final buffer with a larger buffer behind it). I do not think this will significantly affect performance, and data integrity is a high priority. 
"
HADOOP-1469,Asynchronous table creation,"In some of my code i found it efficient to create a new table without immediately requiring (blocking for) the client to have references to it's HRegions.  Effectively this requires that the client update the table info only as needed.

Can something like the following method to HClient be added?

/**
	 * Creates a new table but does not block and wait for it to come online.
	 * 
	 * @param desc -
	 *            table descriptor for table
	 * 
	 * @throws IllegalArgumentException -
	 *             if the table name is reserved
	 * @throws MasterNotRunningException -
	 *             if master is not running
	 * @throws NoServerForRegionException -
	 *             if root region is not being served
	 * @throws IOException
	 */
	public synchronized void createTableAsync(HTableDescriptor desc)
			throws IOException {
		checkReservedTableName(desc.getName());
		checkMaster();
		try {
			this.master.createTable(desc);

		} catch (RemoteException e) {
			handleRemoteException(e);
		}
	}

which is basically the same as createTable() except without the findServersForTable(desc.getName())  part.

"
HADOOP-1467,Remove redundant counters from WordCount example,The counters in the WordCount example duplicate the automatic counters that the framework defines and should be removed.
HADOOP-1463,dfs.datanode.du.reserved semantics being violated,"Currently namenode reports two statistics back to the client:
1. The total capacity of dfs. This is a sum of all datanode's capacities, each of which is calculated by datanode summing all data directories disk space.
2. The total remaining space of dfs. This is a sum of all datanodes's remaining space. Each datanode's remaining space is calculated by using the following formula: remaining space = unused space - capacity*unusableDiskPercentage - reserved space. So the remaining space shows how much space that the dfs can still use, but it does not show the size of unused space.

Each dfs client caculates the total dfs used space by substracting remaining space from the total capacity. So the used space does not accurately shows the space that dfs is using. However it is a very important number that dfs should provide."
HADOOP-1462,Better progress reporting from a Task,"The Task code that reports progress updates has the following problems:
1. Some RPC calls are blocking. For example, in MapRunner::run(), the call to RecordReader::next() can result in a blocking RPC call to the Task Tracker (TT) to report progress. 
2. Some RPC calls are unnecessary. The Ping thread pings the TT once every second, while we also independently send progress updates every second. We don't, for example, need to ping the TT right after we send the progress update. 
3. In some places, we spawn a thread to send progress updates (in MapOutputBuffer::collect(), for example). If our code gets stuck, the thread will continue sending updates to the TT and we will never be shut down. 

These issues, in some form or another, have been reported in HADOOP-1201 and HADOOP-1431. 

I propose we make the following changes: 

1. In order to make the RPC calls non-blocking, we need a thread that calls TT. This thread, to be created early on, will make sure we make the most appropriate RPCs. It will have access to two flags: a progress flag that indicates that the Task has made progress since the last RPC, and a keep_alive flag that indicates that we need to let the TT know that we're alive. This thread will also handle pings. It's logic will be something like this: 

while (1) {
	if (progress_flag is set) {
		// report progress update
		umbilical::progress(...);
		if (failure), kill task;
		reset progress_flag; 
		reset keep_alive_flag; // calling progress() also indicates that we're alive
	}
	else if (keep_alive_flag is set) {
		// let TT know we're alive
		umbilical::progress(same params as last time);
		if (failure), kill task;
		reset keep_alive_flag;
		break;
	}
	else {
		// see if TT is alive
		umbilical::ping();
		if (failure), kill task;
	}
	sleep (1 sec);
}

2. progress_flag and keep_alive_flag are set by the MapReduce code. Reporter::progress() (in Task.java) sets keep_alive_flag while progress_flag is set whenever Task's taskProgress object has any of its fields changed. 

3. We do away with Task::reportProgress() as this code is now handled in the Progress thread. Wherever this method is called in our MapReduce kernel code, we should replace it either with Reporter::progress() (if the intent was to let TT know that we're alive) or we simply remove that call (if the intent was to transmit progress changes to the TT). 

4. TaskUmbilicalProtocol::progress() should return a boolean, and should return the same values that TaskUmbilicalProtocol::ping() does. This will let the Task know whether its ID is known to the TT. 

5. We no longer need to create a ping thread in TaskTracker::Child. However, we can perhaps create the Progress thread in the same place the Ping thread was created. 

6. We will need to remove code that creates progress threads. This is in MapTask::MapOutputBuffer::collect(), MapTask::MapOutputBuffer::flush(), and ReduceTask::ReduceCopier(), at the least. Instead, we will need to add code that updates the progress or calls Reporter::progress(). Any of these calls simply update flags. so there's not a lot of performance penalty (at worst, updating progress_flag or keep_alive_flag may need to be done within a synchronized block, but even that may not be necessary since the flags are just boolean values). As per HADOOP-1431, these calls can be made through a ReportingComparator, or from within the generic BuferSorter, or perhaps from some place better. 

I may have missed out on some details, but hopefully the overall idea is clear. Comments welcome. "
HADOOP-1461,Corner-case deadlock in TaskTracker,"Thanks to Koji for the attached stack-trace...

Summary:

main()
  -> offerService()
    -> markUnresponsiveTasks (locks the TaskTracker here)
      -> purgeTask() 
        -> removeTaskFromJob (waiting to lock the RunningJob object)

taskCleanup
  -> purgeJob (locks the RunningJob object)
    -> TIP.jobHasFinished()
      -> TIP.cleanup (waiting to lock the TaskTracker)

-*-*-

Clear-case of ordering issues during synchronization... it's a corner-case since it depends on the child-vm getting unresponsive _and_ the cleanup thread kicking in; which is why I'm marking this for 0.14.0 rather than 0.13.0 - what do others think about this?

-*-*-

Two possible solutions to break the deadlock cycle:

a) Make TaskTracker.purgeJob a synchronized method, thus it locks the TaskTracker before locking the RunningJob method.
b) Make the TaskTracker.tasks map a *Collections.synchronizedMap*, thus doing away with the need to lock the TaskTracker in TIP.cleanup

I'd prefer a) since the TaskTracker.tasks is referenced in multiple places in synchronized methods... and hence is a less intrusive change.

-*-*- 

Thoughts?
"
HADOOP-1459,"FileSystem.getFileCacheHints returns IP addresses rather than hostnames, which breaks 'data-locality' in map-reduce","FileSystem.getFileCacheHints via DFSClient.getHints (post HADOOP-894?) returns IP address of the datanodes instead of the hostnames which breaks mapping from task-tracker to datanodes in map-reduce i.e. the system cannot intelligently place maps on datanodes where blocks are present.

I have verified that this affects trunk only, branch-0.13.0 seems ok."
HADOOP-1458,Upgrade from 0.13 (pre HADOOP-1242) to 0.13 (post HADOOP-1242) does not work,"When you convert a 0.13 NameNode storage directory from pre to post HADOOP-1242, the NameNode won't start. You'll get:

{code}
07/06/04 00:10:51 ERROR dfs.NameNode: org.apache.hadoop.dfs.InconsistentFSStateException: Directory /data/hadoop/dfs/name is in an inconsistent state: /data/hadoop/dfs/name/image does not exist.
        at org.apache.hadoop.dfs.FSImage.isConversionNeeded(FSImage.java:439)
        at org.apache.hadoop.dfs.Storage$StorageDirectory.analyzeStorage(Storage.java:263)
        at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:149)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:347)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:251)
        at org.apache.hadoop.dfs.NameNode.init(NameNode.java:173)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:211)
        at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:806)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:814)
{code}

because the ""bad"" image file isn't created later in the startup. Since this is only a non-release issue, I don't know if we need to fix it programatically or can just provide an upgrade script. Thoughts?"
HADOOP-1457,Counters for monitoring task assignments,"For performance reasons, it would be nice to monitor the task assignments to nodes. This is especially true for maps where we want to maximize data-locality-based assignments. Counters could be introduced for that."
HADOOP-1456,TestDecommission fails with assertion  Number of replicas for block1 expected:<3> but was:<2>,"TestDecommission fails sometimes with the following assertion:

junit.framework.AssertionFailedError: Number of replicas for block1 expected:<3> but was:<2>
	at org.apache.hadoop.dfs.TestDecommission.checkFile(TestDecommission.java:89)
	at org.apache.hadoop.dfs.TestDecommission.testDecommission(TestDecommission.java:285)

More logs here: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/231/testReport/org.apache.hadoop.dfs/TestDecommission/testDecommission/

This test creates a file with target replication factor of 3. Then it verifies that the file actually has a replciation of 3. I think the test should set ""dfs.replication.considerLoad"" to false before starting the test so that the namenode ignores the load on datanodes while allocating replicas. This is the approach folowed by TestReplication.
"
HADOOP-1455,Allow any key-value pair on the command line of 'hadoop pipes' to be added to the JobConf,
HADOOP-1454,Reducer doesn't track failed fetches and gets stuck,"As mentioned in HADOOP-1452, but it deserves a separate issue, reducers retry forever to fetch data. At some time this should fail either the reducer or the corresponding mapper. "
HADOOP-1453,exists() not necessary before DFS.open,"
{code:title=DistributedFileSystem.java:131|borderStyle=solid}
public FSDataInputStream open(Path f, int bufferSize) throws IOException {
      if (!exists(f)) {
        throw new FileNotFoundException(f.toString());
      }

      return new DFSClient.DFSDataInputStream(dfs.open(getPath(f)), bufferSize);
 }
{code}

{{exists(f)}} adds extra namenode interaction that is not really required. Open is a critical DFS call.

"
HADOOP-1452,map output transfers of more than 2^31 bytes output are failing,"Symptom:

WARN org.apache.hadoop.mapred.ReduceTask: java.io.IOException: Incomplete map output received for http://<host>:50060/mapOutput?map=task_0026_m_000298_0&reduce=61 (2327458761 instead of 2327347307)
WARN org.apache.hadoop.mapred.ReduceTask: task_0026_r_000061_0 adding host <host> to penalty box, next contact in 263 seconds

Besides failing to fetch data, the reduce will retry forever. This should be limited.

Source of the problem:

in mapred/TaskTracker.java the variable totalRead keeping track what is sent to the reducer should be declared as long:

...
        int totalRead = 0;
        int len = mapOutputIn.read(buffer, 0,
                                   partLength < MAX_BYTES_TO_READ
                                   ? (int)partLength : MAX_BYTES_TO_READ);
        while (len > 0) {
          try {
            outStream.write(buffer, 0, len);
            outStream.flush();
          } catch (IOException ie) {
            isInputException = false;
            throw ie;
          }
          totalRead += len;
          if (totalRead == partLength) break;
          len = mapOutputIn.read(buffer, 0,
                                 (partLength - totalRead) < MAX_BYTES_TO_READ
                                 ? (int)(partLength - totalRead) : MAX_BYTES_TO_READ);
        }
..."
HADOOP-1450,checksums should be closer to data generation and consumption,"ChecksumFileSystem checksums data by inserting a filter between two buffers.  The outermost buffer should be as small as possible, so that, when writing, checksums are computed before the data has spent much time in memory, and, when reading, checksums are validated as close to their time of use as possible.  Currently the outer buffer is the larger, using the bufferSize specified by the user, and the inner is small, so that most reads and writes will bypass it, as an optimization.  Instead, the outer buffer should be made to be bytesPerChecksum, and the inner buffer should be the user-specified buffer size."
HADOOP-1449,Example for contrib/data_join,"DataJoin under ""contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join"" is useful but is missing examples. I plan to add a simple example which can demonstrate a INNER JOIN between two text files based on the values on a column using this framework."
HADOOP-1448,Setting the replication factor of a file too high causes namenode cpu overload,"The replication factor of a file in set to 300 (on a 800 node cluster). Then all mappers try to open this file. For every open call that the namenode receives from each of these 800 clients, it sorts all the replicas of the block(s) based on the distance from the client. This causes CPU usage overload on the namenode.

One proposal is to make the namenode return a non-sorted list of datanodes to the client. Information about each replica also contains the rack on which that replica resides. The client can look at the replicas to determine if there is a copy on the local node. If not, then it can find out if there is a replica on the local rack. If not then it can choose a replica at random.

This proposal is scalable because the sorting and selection of replicas is done by the client rather than the Namenode."
HADOOP-1447,Support for textInputFormat in contrib/data_join,"DataJoinJob under ""contrib/data_join/src/java/org/apache/hadoop/contrib/utils/join"" currently supports only SequenceFileInputFormat. I plan to modify this to support textInputFormat as well which is controlled by a command-line argument."
HADOOP-1446,Metrics from the TaskTracker are updated only when map/reduce tasks start/end/fail,Metrics reporting from the TaskTracker for the maps_running and reduces_running metrics are done only when a task is starting up or finishing up. This is wrong and the metrics reporting should be done periodically during the life of the TaskTracker just as it is done for the JobTracker case.
HADOOP-1444,Block allocation method does not check pendingCreates for duplicate block ids,"The HDFS namenode allocates a new random blockid when requested. It then checks the blocksMap to verify if this blockid is already in use. If this block is is already in use, it generates another random number and above process continues. When a blocksid that does not exist in the blocksMap is found, it stores this blocksid in pendingCreateBlocks and returns the blocksid to the requesting client.

The above check for detecting duplicate blockid should check pendingCreateBlocks as well.

A related problem exists when a file is deleted. Deleting a file causes all its blocks to be deleted from the blocksMap immediately. These blockids move to recentInvalidateSets and are sent out to the corresponding datanodes as part of responses to succeeding heartbeats. So, there is a time window when a block exists in the datanode but not in the blocksMap. At this time, if the blockid-random-number generator generates a blockid that exists in the datanode but not on the blocksMap, then the namenode will fail to detect that this is a duplicate blockid.

"
HADOOP-1443,TestFileCorruption fails with ArrayIndexOutOfBoundsException,"org.apache.hadoop.dfs.TestFileCorruption.testFileCorruption failed once on Windows with this exception:

org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:472)
	at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:436)
	at org.apache.hadoop.dfs.NameNode.getBlockLocations(NameNode.java:272)
	at org.apache.hadoop.dfs.NameNode.open(NameNode.java:259)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:567)

	at org.apache.hadoop.ipc.Client.call(Client.java:471)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
	at org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:590)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.<init>(DFSClient.java:582)
	at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:273)
	at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.open(DistributedFileSystem.java:136)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.<init>(ChecksumFileSystem.java:114)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:340)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:234)
	at org.apache.hadoop.dfs.DFSTestUtil.checkFiles(DFSTestUtil.java:132)
	at org.apache.hadoop.dfs.TestFileCorruption.testFileCorruption(TestFileCorruption.java:66)
"
HADOOP-1442,Zero-byte input files are not included in InputSplit,"The default FileInputFormat::getSplits method does not include zero-byte input files in FileSplit[]. Applications that assume the minimum number of maps to be at least number of input files can get bitten by this, as well as  applications that use -reducer NONE to produce number of partitions that are equal to input partitions."
HADOOP-1440,JobClient should not sort input-splits,"Currently, the JobClient sorts the InputSplits returned by InputFormat in descending order, so that the map tasks corresponding to larger input-splits are scheduled first for execution than smaller ones. However, this causes problems in applications that produce data-sets partitioned similarly to the input partition with -reducer NONE.

With -reducer NONE, map task i produces part-i. Howver, in the typical applications that use -reducer NONE it should produce a partition that has the same index as the input parrtition.

(Of course, this requires that each partition should be fed in its entirety to a map, rather than splitting it into blocks, but that is a separate issue.)

Thus, sorting input splits should be either controllable via a configuration variable, or the FileInputFormat should sort the splits and JobClient should honor the order of splits."
HADOOP-1439,Add endRow parameter to HClient#obtainScanner,"Currently the HClient#obtainScanner looks like this:

{code}
public synchronized HScannerInterface obtainScanner(Text[] columns, Text startRow) throws IOException;
{code}

Add an overload that allows specification of endRow:

{code}
public synchronized HScannerInterface obtainScanner(Text[] columns, Text startRow, Text endRow) throws IOException;
{code}

Use Case: Table contains the whole web.  Client just wants to scan google's pages.  Currently, client could cut off the scanner as soon as the row key leaves the google domain but cleaner if {{HScannerInterface#next()}} returns false



"
HADOOP-1438,Grammatical / wording / copy edits for Hadoop Distributed File System: Architecture and Design white paper,"Overall, the white paper is quite good, but I think it could use some revisions to enhance its clarity and readability.  I will post a patch of the Forrest file in src/docs/src/documentation/content/xdocs/hdfs_design.xml"
HADOOP-1437,Eclipse plugin for developing and executing MapReduce programs on Hadoop,"An Eclipse plugin for developing and executing MapReduce programs on remote Hadoop servers.  Automatically provides templates for creating Map/Reduce classes, transparently bundles the classes into JAR files and sends them to a remote server for execution.  Allows the user to easily view status of Hadoop jobs and browse/upload/delete files from the Hadoop DFS within the Eclipse IDE."
HADOOP-1436,Redesign Tool and ToolBase API and releted functionality,"With the discussion from HADOOP-1425, we need better abstraction and better tool runner utilities. 

1. Classes do not need to extend ToolBase 
2. functions for parsing general HadoopCommands (-fs, -conf, -jt) should be public
3. We need a ToolRunner, or similar functionality 
4. Also we need each class (implementing Tool) to be runnable (main method)
5. CLI objects can be passed to run method of the Tool class (arguable)"
HADOOP-1435,FileSystem.globPaths should not create a Path from an empty string,"Got the following error when tried to list a directory in a home directory using a relative path name

hadoop dfs -ls in_jar

Exception in thread ""main"" java.lang.IllegalArgumentException: Can not create a Path from an empty string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
        at org.apache.hadoop.fs.Path.<init>(Path.java:90)
        at org.apache.hadoop.fs.FileSystem.globPaths(FileSystem.java:561)
        at org.apache.hadoop.fs.FileSystem.globPaths(FileSystem.java:540)
        at org.apache.hadoop.fs.FsShell.ls(FsShell.java:314)
        at org.apache.hadoop.fs.FsShell.doall(FsShell.java:842)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:1016)
        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:1092)
"
HADOOP-1434,Let users add compression types,"This is probably a special case, but we're considering serving data from the generated sequence files to avoid having to convert to other file format.

However, using block compression means we'd have to read up to almost one mb (default) of data to find the data. Our records are so small that compressing
them using records compression increases the size of the file compared to no compression. 

I'd like to make a modified version of the BlockCompressWriter that ends a block depending on features of the key appended.
There's currently no easy way of adding this without modifying SequenceFile directly."
HADOOP-1433,Add job priority,"As more and more developers start using our cluster we run into problems where big low priority jobs block smaller high priority ones.

A simple way of specifying the job priority in the JobConf and perhaps even change it during runtime via the jobtracker web ui would help a lot."
HADOOP-1431,Map tasks can't timeout for failing to call progress,"Currently the map task runner creates a thread that calls progress every second to keep the system from killing the map if the sort takes too long. This is the wrong approach, because it will cause stuck tasks to not be killed. The right solution is to have the sort call progress as it actually makes progress. This is part of what is going on in HADOOP-1374. A map gets stuck at 100% progress, but not done."
HADOOP-1429,RPC Server won't go quietly,"Trying to do a controlled shutdown of hbase, the RPC Server spews the following ugly output:

unknown-208-76-47-46:~/Documents/checkouts/hadoop-trunk stack$ ./src/contrib/hbase/bin/hbase master stop
07/05/24 12:53:47 INFO ipc.Server: Stopping server on 60000
07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 0 on 60000 caught: java.lang.InterruptedException
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:541)
07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 0 on 60000: exiting
unknown-208-76-47-46:~/Documents/checkouts/hadoop-trunk stack$ 07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 1 on 600
00 caught: java.lang.InterruptedException
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:541)
07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 2 on 60000 caught: java.lang.InterruptedException
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:541)
...

You get the same noise when if run the TestIPC unit test."
HADOOP-1428,ChecksumFileSystem : some operations implicitly not supported.,"
It looks like {{skip()}} would skip bytes on data stream. That assures next read is going to be a checksum error. If skip() is not supported, ideally it should throw an exception with clear message.

Also positional reads ({{read(long position, buf)}}) don't seem to checksum the data. It might be ok, but it is not obvious to users.
"
HADOOP-1427,Typo in GzipCodec.createInputStream - bufferSize,"Theres a typo (I think) in GzipCodec:

{code}
       Decompressor decompressor =
         new ZlibDecompressor(ZlibDecompressor.CompressionHeader.AUTODETECT_GZIP_ZLIB,
            64*1-24);
{code}
gives a buffersize of 40, should be:
{code}
       Decompressor decompressor =
         new ZlibDecompressor(ZlibDecompressor.CompressionHeader.AUTODETECT_GZIP_ZLIB,
            64*1024);
{code}
"
HADOOP-1425,Rework the various programs in 'examples' to extend ToolBase ,Ensuring all 'examples' extend ToolBase will make it easy to tweak various config params (via -D switches for e.g.) while running the programs... 
HADOOP-1419,FindBugs : Dodgy : in dfs,"
Fix some of the FindBugs finds in DFS.
"
HADOOP-1417,Exclude some Findbugs detectors,"Exclude these detectors from Findbugs:
 - May expose internal representation by returning reference to mutable object
 - May expose internal representation by incorporating reference to mutable object
 - Comparator doesn't implement Serializable"
HADOOP-1414,Findbugs - Bad Practice,"Fix most issues categorized as ""Bad Practice"" by the findbugs tool."
HADOOP-1413,A new example to do tile placements using distributed dancing links,"During a Yahoo Hack Day last year, I wrote a distributed pentomino solver that used map/reduce and Knuth's dancing link algorithm to solve pentomino tile placement problems. As a side benefit, I also wrote a sudoku solver using the dancing link library, but the sudoku solver is so fast that there is no need to distribute it. Anyways, I think it makes an interesting example of how to do some interesting cpu-heavy distribution in Hadoop."
HADOOP-1412,"FindBugs: Dodgy bugs in fs, filecache, io, and util packages","Fix dodgy bugs reported in fs, filecache, io, and util packages"
HADOOP-1411,AlreadyBeingCreatedException from task retries,"HADOOP-1407 indicates 2 bugs: a mapred bug which will be fixed as part of 1407, and a DFSClient bug that will be fixed here.

Note that the test run in 1407 was without speculation."
HADOOP-1410,"target ""clean"" in Makefile is very dangerous","clean:
	$(RM) $(LIBHDFS_BUILD_DIR)/* 

People can execute ""make clean"" and forget to export LIBHDFS_BUILD_DIR. "
HADOOP-1408,"fix warning about cast of Map<String, Map<String, JobInfo>> in jobhistory.jsp",There is a warning where one of the attributes is cast to a map of the proper form. The fix is defining a wrapping class so that the cast is safe.
HADOOP-1407,Failed tasks not killing job,"Some test runs on May 10 and then since May 14 contain failed tasks (all 4 executions fail) but the job does not fail.  Given these dates, my suspicion is that this could be related to HADOOP-1350 or turning on speculative execution in my testing.

Some JobTracker log snippets for tip_0005_m_002705:

2007-05-10 21:21:16,820 INFO org.apache.hadoop.mapred.JobInProgress: Choosing cached task tip_0005_m_002705
2007-05-10 21:21:16,820 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_0005_m_002705_0' to tip tip_0005_m_002705, for tracker 'tracker_2982.com:50050'
...
2007-05-10 21:22:12,542 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0005_m_002705_0: java.io.FileNotFoundException: /e/c/k/hadoopqa/dfs/data500/tmp/client-5285738463038775723 (No such file or directory)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:106)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1322)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1415)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:48)
        at org.apache.hadoop.fs.FSDataOutputStream$Buffer.close(FSDataOutputStream.java:72)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:92)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.close(ChecksumFileSystem.java:414)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:48)
        at org.apache.hadoop.fs.FSDataOutputStream$Buffer.close(FSDataOutputStream.java:72)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:92)
        at org.apache.hadoop.fs.TestDFSIO$WriteMapper.doIO(TestDFSIO.java:207)
        at org.apache.hadoop.fs.IOMapperBase.map(IOMapperBase.java:123)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:187)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)
...
2007-05-10 21:22:15,700 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0005_m_002705_0' has been lost.
2007-05-10 21:22:15,702 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_0' from 'tracker_2982.com:50050'
2007-05-10 21:22:15,710 INFO org.apache.hadoop.mapred.JobInProgress: Choosing normal task tip_0005_m_002705
2007-05-10 21:22:15,710 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_0005_m_002705_1' to tip tip_0005_m_002705, for tracker 'tracker_2617.com:50050'
...
2007-05-10 21:22:22,665 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0005_m_002705_1: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /benchmarks/TestDFSIO/io_data/test_io_3705 for DFSClient_task_0005_m_002705_1 on client 72.30.50.13, because this file is already being created by DFSClient_task_0005_m_002705_0 on 1.2.3.4
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:606)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:294)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)

        at org.apache.hadoop.ipc.Client.call(Client.java:471)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
        at org.apache.hadoop.dfs.$Proxy1.create(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy1.create(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateNewBlock(DFSClient.java:1172)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:1114)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1321)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1273)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:124)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:124)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:91)
        at org.apache.hadoop.fs.TestDFSIO$WriteMapper.doIO(TestDFSIO.java:207)
        at org.apache.hadoop.fs.IOMapperBase.map(IOMapperBase.java:123)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:187)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)
...
2007-05-10 21:22:23,604 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0005_m_002705_1' has been lost.
2007-05-10 21:22:23,605 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_1' from 'tracker_2617.com:50050'
2007-05-10 21:22:23,607 INFO org.apache.hadoop.mapred.JobInProgress: Choosing normal task tip_0005_m_002705
2007-05-10 21:22:23,608 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_0005_m_002705_2' to tip tip_0005_m_002705, for tracker 'tracker_2552.com:50050'
...
2007-05-10 21:22:29,280 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0005_m_002705_2: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /benchmarks/TestDFSIO/io_data/test_io_3705 for DFSClient_task_0005_m_002705_2 on client 72.30.52.8, because this file is already being created by DFSClient_task_0005_m_002705_0 on1.2.3.4
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:606)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:294)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)

        at org.apache.hadoop.ipc.Client.call(Client.java:471)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
        at org.apache.hadoop.dfs.$Proxy1.create(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy1.create(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateNewBlock(DFSClient.java:1172)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:1114)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1321)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1273)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:124)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:124)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:91)
        at org.apache.hadoop.fs.TestDFSIO$WriteMapper.doIO(TestDFSIO.java:207)
        at org.apache.hadoop.fs.IOMapperBase.map(IOMapperBase.java:123)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:187)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)
...
2007-05-10 21:22:30,044 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0005_m_002705_2' has been lost.
2007-05-10 21:22:30,045 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_2' from 'tracker_2552.com:50050'
2007-05-10 21:22:30,065 INFO org.apache.hadoop.mapred.JobInProgress: Choosing normal task tip_0005_m_002705
2007-05-10 21:22:30,066 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_0005_m_002705_3' to tip tip_0005_m_002705, for tracker 'tracker_2618.com:50050'
...
2007-05-10 21:22:36,099 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0005_m_002705_3: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /benchmarks/TestDFSIO/io_data/test_io_3705 for DFSClient_task_0005_m_002705_3 on client 72.30.50.14, because this file is already being created by DFSClient_task_0005_m_002705_0 on 1.2.3.4
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:606)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:294)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)

        at org.apache.hadoop.ipc.Client.call(Client.java:471)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
        at org.apache.hadoop.dfs.$Proxy1.create(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy1.create(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateNewBlock(DFSClient.java:1172)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:1114)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1321)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1273)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:124)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.FilterOutputStream.flush(FilterOutputStream.java:123)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:124)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:91)
        at org.apache.hadoop.fs.TestDFSIO$WriteMapper.doIO(TestDFSIO.java:207)
        at org.apache.hadoop.fs.IOMapperBase.map(IOMapperBase.java:123)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:187)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)

2007-05-10 21:22:36,099 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0005_m_002705_3' has been lost.
2007-05-10 21:22:36,100 INFO org.apache.hadoop.mapred.TaskInProgress: TaskInProgress tip_0005_m_002705 has failed 4 times.
2007-05-10 21:22:36,101 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_3' from 'tracker_2618.com:50050'
...
2007-05-10 21:28:29,456 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_2' from 'tracker_2552.com:50050'
...
2007-05-10 21:28:30,757 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_3' from 'tracker_2618.com:50050'
...
2007-05-10 21:28:33,782 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_1' from 'tracker_2617.com:50050'
...
2007-05-10 21:28:35,884 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0005_m_002705_0' from 'tracker_2982.com:50050'
"
HADOOP-1406,Metrics based on Map-Reduce Counters are not cleaned up,"When map-reduce jobs are finished, the metrics corresponding to their counters are not cleaned up.  This is a memory leak, but worse it means that an ever-increasing amount of metric data is sent to the metrics system (if one is enabled).

The fix is for JobInProgress to clean up the metrics it created when the job is complete.

"
HADOOP-1400,JobClient rpc times out getting job status,"JobClient rpc times out getting job status and logs an INFO exception message.  JobClient seems to recover, however, so I think the exception should be logged at DEBUG or the message indicate that JobClient will retry.  Here's an example log:

07/05/20 20:45:38 INFO mapred.JobClient: Running job: job_0029
07/05/20 20:45:39 INFO mapred.JobClient:  map 0% reduce 0%
07/05/20 20:45:43 INFO mapred.JobClient:  map 2% reduce 0%
07/05/20 20:45:44 INFO mapred.JobClient:  map 7% reduce 0%
07/05/20 20:45:45 INFO mapred.JobClient:  map 17% reduce 0%
07/05/20 20:45:46 INFO mapred.JobClient:  map 23% reduce 0%
07/05/20 20:45:47 INFO mapred.JobClient:  map 36% reduce 0%
07/05/20 20:45:48 INFO mapred.JobClient:  map 60% reduce 0%
07/05/20 20:45:49 INFO mapred.JobClient:  map 71% reduce 0%
07/05/20 20:45:50 INFO mapred.JobClient:  map 73% reduce 0%
07/05/20 20:45:51 INFO mapred.JobClient:  map 75% reduce 0%
07/05/20 20:45:52 INFO mapred.JobClient:  map 79% reduce 0%
07/05/20 20:45:53 INFO mapred.JobClient:  map 84% reduce 0%
07/05/20 20:45:54 INFO mapred.JobClient:  map 97% reduce 0%
07/05/20 20:45:57 INFO mapred.JobClient:  map 99% reduce 0%
07/05/20 20:45:58 INFO mapred.JobClient:  map 100% reduce 1%
07/05/20 20:46:00 INFO mapred.JobClient:  map 100% reduce 2%
07/05/20 20:46:08 INFO mapred.JobClient:  map 100% reduce 3%
07/05/20 20:46:11 INFO mapred.JobClient:  map 100% reduce 4%
07/05/20 20:46:18 INFO mapred.JobClient:  map 100% reduce 5%
07/05/20 20:46:21 INFO mapred.JobClient:  map 100% reduce 6%
07/05/20 20:46:28 INFO mapred.JobClient:  map 100% reduce 7%
07/05/20 20:46:31 INFO mapred.JobClient:  map 100% reduce 8%
07/05/20 20:46:37 INFO mapred.JobClient:  map 100% reduce 9%
07/05/20 20:46:39 INFO mapred.JobClient:  map 100% reduce 10%
07/05/20 20:46:47 INFO mapred.JobClient:  map 100% reduce 11%
07/05/20 20:46:50 INFO mapred.JobClient:  map 100% reduce 12%
07/05/20 20:46:57 INFO mapred.JobClient:  map 100% reduce 13%
07/05/20 20:46:59 INFO mapred.JobClient:  map 100% reduce 14%
07/05/20 20:47:07 INFO mapred.JobClient:  map 100% reduce 15%
07/05/20 20:47:09 INFO mapred.JobClient:  map 100% reduce 16%
07/05/20 20:47:17 INFO mapred.JobClient:  map 100% reduce 17%
07/05/20 20:47:19 INFO mapred.JobClient:  map 100% reduce 18%
07/05/20 20:47:28 INFO mapred.JobClient:  map 100% reduce 19%
07/05/20 20:47:29 INFO mapred.JobClient:  map 100% reduce 20%
07/05/20 20:47:35 INFO mapred.JobClient:  map 100% reduce 21%
07/05/20 20:47:39 INFO mapred.JobClient:  map 100% reduce 22%
07/05/20 20:47:44 INFO mapred.JobClient:  map 100% reduce 23%
07/05/20 20:47:49 INFO mapred.JobClient:  map 100% reduce 24%
07/05/20 20:47:58 INFO mapred.JobClient:  map 100% reduce 25%
07/05/20 20:47:59 INFO mapred.JobClient:  map 100% reduce 26%
07/05/20 20:48:05 INFO mapred.JobClient:  map 100% reduce 27%
07/05/20 20:48:09 INFO mapred.JobClient:  map 100% reduce 28%
07/05/20 20:48:14 INFO mapred.JobClient:  map 100% reduce 29%
07/05/20 20:48:19 INFO mapred.JobClient:  map 100% reduce 30%
07/05/20 20:48:24 INFO mapred.JobClient:  map 100% reduce 31%
07/05/20 20:48:29 INFO mapred.JobClient:  map 100% reduce 32%
07/05/20 20:48:31 INFO mapred.JobClient:  map 100% reduce 35%
07/05/20 20:49:32 INFO mapred.JobClient: Communication problem with server: java.net.SocketTimeoutException: timed out waiting for rpc response
	at org.apache.hadoop.ipc.Client.call(Client.java:473)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)
	at $Proxy1.getJobStatus(Unknown Source)
	at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:491)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:556)
	at org.apache.hadoop.mapred.MRBench.runJobInSequence(MRBench.java:188)
	at org.apache.hadoop.mapred.MRBench.main(MRBench.java:280)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:69)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:140)
	at org.apache.hadoop.test.AllTestDriver.main(AllTestDriver.java:64)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)

07/05/20 20:49:35 INFO mapred.JobClient:  map 100% reduce 97%
07/05/20 20:49:37 INFO mapred.JobClient:  map 100% reduce 98%
07/05/20 20:49:40 INFO mapred.JobClient:  map 100% reduce 99%
07/05/20 20:49:42 INFO mapred.JobClient:  map 100% reduce 100%
07/05/20 20:49:43 INFO mapred.JobClient: Job complete: job_0029"
HADOOP-1399,Provide the ability to cache column data in memory,"Bigtable allows column families to be served out of memory to reduce disk accesses.

Issues with this are:
- need region server row/value caching and block caching
- maintaining read cache coherency while writes are on-going."
HADOOP-1396,FileNotFound exception on DFS block,"Got a couple of exceptions of the form illustrated below. This was for a randomwriter run (and every node in the cluster has multiple disks).

java.io.FileNotFoundException: /tmp/dfs/data/tmp/client-8395631522349067878 (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:106)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1323)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1274)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.write(DFSClient.java:1256)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.write(ChecksumFileSystem.java:402)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:775)
	at org.apache.hadoop.examples.RandomWriter$Map.map(RandomWriter.java:158)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:187)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)

So it seems like the bug reported in HADOOP-758 still exists."
HADOOP-1394,FindBugs : Performance : in dfs,"Fix performance issues that findBugs reports in DFS.
"
HADOOP-1393,using Math.abs(Random.getInt()) does not guarantee a positive number,Findbugs suggested that we fix the potential for Integer.MIN_VALUE to slip through Math.abs.
HADOOP-1392,"FindBugs : Fix some correctness bugs reported in DFS, FS, etc.","
Fix some correctness bugs reported by FindBugs in DFS, FS, IO, NET, etc packages."
HADOOP-1390,"Inconsistent Synchronization cleanup for {Configuration, TaskLog, MapTask, Server}.java","findbugs reported inconsistent synchronization for the files {Configuration, TaskLog, MapTask, Server}.java. "
HADOOP-1388,Possible Null Pointer Dereference in taskdetails.jsp,Possible null pointer dereference of TaskStatus[]
HADOOP-1387,FindBugs -> Performance ,Fix the 'Performance' bugs shown up by findbugs.
HADOOP-1386,The constructor of Path should not take an empty string as a parameter,"DFSShell should pass a user's home directory instead of an empty string for any operations on a user's home directory.

This bug caused an accidental deletion of a user's directory with valuable data on our dfs cluster. Therefore, I marke it as a blocker to release 0.13"
HADOOP-1385,MD5Hash has a bad hash function,"The MD5Hash class has a really bad hash function, that will cause most most md5s to hash to 0xFFFFFFxx leaving only the low order byte as meaningful. The problem comes from the automatic sign extension when promoting from byte to int."
HADOOP-1382,Using org.apache.hadoop.fs.LocalDirAllocator for jobcache/jobid/tasktip,"When one drive filled up, we had tasks fail with 
   java.io.IOException: Mkdirs failed to create
   /hadoop/mapred/local/taskTracker/jobcache/job_0963/task_0963_m_000756_3

It'll be nice if it can retry or choose the drive with sufficient space."
HADOOP-1379,Integrate Findbugs into nightly build process,"I think we should integrate Findbugs (http://findbugs.sourceforge.net) into our nightly test runs at http://lucene.zones.apache.org:8080/hudson/ and/or our patch process builds.

Findbugs uses static analysis to look for bugs in Java code.  It is licensed under Lesser GNU Public License so the build target will need to be optional, similar to the checkstyle target. "
HADOOP-1377,Creation time and modification time for hadoop files and directories,"This issue will document the requirements, design and implementation of creation times and modification times of hadoop files and directories.

My proposal is to have support two additional attributes for each file and directory in HDFS. The ""creation time"" is the time when the file/directory was created. It is a 8 byte integer stored in each FSDirectory.INode. The ""modification time"" is the time when the last modification occured to the file/directory. It is an 8 byte integer stored in the FSDirectory.INode. These two fields are stored in in the FSEdits and FSImage as part of the transaction that created the file/directory.

My current proposal is to not support ""access time"" for a file/directory. It is costly to implement and current applications might not need it.

In the current implementation, the ""modification time"" for a file will be same as its creation time because HDFS files are currently unmodifiable. Setting file attributes (e.g. setting the replication factor) of a file does not modify the ""modification time"" of that file. The ""modification time"" for a directory is either its creation time or the time when the most recent file-delete or file-create occured in that directory.

A new command named ""hadoop dfs -lsl"" will display the creation time and modification time of the files/directories that it lists. The output of the existing command ""hadoop dfs -ls"" will not be affected.

The ClientProtocol will change because DFSFileInfo will have two additional fields: the creation time and modification time of the file that it represents. This information can be retrieved by clients thorugh the ClientProtocol.getListings() method. The FileSystem public API will have two additional methods: getCreationTime and getModificationTime().

The datanodes are completely transparent to this design and implementation and requires no change."
HADOOP-1376,RandomWriter should be tweaked to generate input data for terasort,We need input data to run the terasort benchmark. The RandomWriter can do that - it needs to generate - 10(power)10 number of 100 byte records (10-byte keys and 90-byte values).
HADOOP-1373,checkPath() throws IllegalArgumentException,"This was introduced recently in one of the patches committed around 05/15 or 05/14.
I am running TestDFSIO on a two node cluster. Here is the exception I get

07/05/15 19:14:53 INFO mapred.TaskInProgress: Error from task_0001_m_000007_0: java.lang.IllegalArgumentException: Wrong FS: hdfs://MY-HOST:7017/benchmarks/TestDFSIO/io_control/in_file_test_io_7, expected: hdfs://my-host:7017
    at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:230)
    at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.getPath(DistributedFileSystem.java:110)
    at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.exists(DistributedFileSystem.java:170)
    at org.apache.hadoop.fs.FilterFileSystem.exists(FilterFileSystem.java:168)
    at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:335)
    at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1162)
    at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1156)
    at org.apache.hadoop.mapred.SequenceFileRecordReader.<init>(SequenceFileRecordReader.java:40)
    at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:54)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:149)
    at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)

I confess, my config on one of the machines specifies name-node ""MY-HOST:7017"" and on the other one ""my-host:7017"".
But that was acceptable before and should stay that way in the future afaiu."
HADOOP-1372,DFS Clients should start using the org.apache.hadoop.fs.LocalDirAllocator,"I encountered this exception during one of the randomwriter runs. I think this situation can be improved by using org.apache.hadoop.fs.LocalDirAllocator that has been written to handle these kind of problems. I set the fix version as 0.14 but wonder whether it makes sense to have it in 0.13 itself (since the amount of code change would not be much).

java.io.FileNotFoundException: /local/dfs/data/tmp/client-1299146109450372217 (Read-only file system)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.(FileOutputStream.java:179)
	at java.io.FileOutputStream.(FileOutputStream.java:131)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1356)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1273)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.write(DFSClient.java:1255)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.write(ChecksumFileSystem.java:402)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:775)
	at org.apache.hadoop.examples.RandomWriter$Map.map(RandomWriter.java:152)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:187)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)"
HADOOP-1369,Inconsistent synchronization of TaskTracker fields,"TaskTracker.java

Inconsistent synchronization of TaskTracker.mapTotal; locked 62% of time
Inconsistent synchronization of TaskTracker.reduceTotal; locked 83% of time
Inconsistent synchronization of TaskTracker.runningTasks; locked 77% of time"
HADOOP-1368,Inconsistent synchronization of 3 fields in JobInProgress.java,"JobInProgress.java

Inconsistent synchronization of JobInProgress.runningReduceTasks; locked 75% of time
line 531

Inconsistent synchronization of JobInProgress.status; locked 88% of time
line 255

Inconsistent synchronization of JobInProgress.tasksInited; locked 60% of time
line 490

"
HADOOP-1367,Inconsistent synchronization of NetworkTopology.distFrom; locked 50% of time,"org.apache.hadoop.net.NetworkTopology.java line 556

The distFrom field of this class appears to be accessed inconsistently with respect to synchronization."
HADOOP-1366,Inconsistent synchronization of Server$Listener.acceptChannel; locked 50% of time,"Server.java line 290

The acceptChannel field of this class appears to be accessed inconsistently with respect to synchronization."
HADOOP-1365,Inconsistent synchronization of Client$Connection.shouldCloseConnection; locked 66% of time,"Client.java line 244

The shouldCloseConnection field of this class appears to be accessed inconsistently with respect to synchronization."
HADOOP-1364,Inconsistent synchronization of SequenceFile$Reader.noBufferedValues; locked 66% of time,"SequenceFile.java line 1600

The noBufferedValues field appears to be accessed inconsistently with respect to synchronization."
HADOOP-1363,waitForCompletion() calls Thread.sleep() with a lock held,"JobClient.java line 143

Method holds NetworkedJob object lock while sleeping for 5 seconds."
HADOOP-1361,seek calls in 3 io classes ignore result of skipBytes(int),"UTF8.java line 113
Text.java line 233
WritableUtils.java line 51

These method calls ignore the return values of java.io.InputStream.skip() which may skip fewer bytes than requested.
"
HADOOP-1360,Possible null pointer dereference of thisAuthority in FileSystem.checkPath(Path),"FileSystem.java line 227

    if (!(this.getUri().getScheme().equals(uri.getScheme()) &&
          (thisAuthority == null && thatAuthority == null)
          || thisAuthority.equals(thatAuthority)))

I'm not convinced that this couldn't produce a NPE on thisAuthority.  This logic should be simplified to ensure it's correctness"
HADOOP-1359,Variable dereferenced then later checked for null,"NamenodeFsck.java
line 174 dereferences the variable 'locs'
line 191 checks if 'locs' is null
"
HADOOP-1358,seek call ignores result of skipBytes(int),"DFSClient.java line 929

This method call ignores the return value of java.io.InputStream.skip() which may skip fewer bytes than requested."
HADOOP-1357,"Call to equals() comparing different types in CopyFiles.cleanup(Configuration, JobConf, String, String)","CopyFiles.java line 369

The call to equals is comparing a Path and a String, which will never be equal."
HADOOP-1356,"ValueHistogram.addNextValue(Object) ignores return value of String.substring(int, int)","ValueHistogram.java line 54

The String.substring method doesn't update the object it operates on.  Instead, it returns a new String object.  This return value is ignored, which must be a bug. "
HADOOP-1355,Possible null pointer dereference in TaskLogAppender.append(LoggingEvent),"TaskLogAppender.java line 49

this.layout is checked for null on line 43, then it is dereferenced on line 49"
HADOOP-1354,Null pointer dereference of paths in FsShell.dus(String),"FsShell.java line 372
has a guaranteed NPE.

Perhaps && was supposed to be ||"
HADOOP-1353,Null pointer dereference of nodeInfo in FSNamesystem.removeDatanode(DatanodeID),"FSNamesystem.java line 1816
Guaranteed NPE."
HADOOP-1352,JobTracker consistently throws ArrayIndexOutOfBoundsException for some jobs accessed via the job-history,"This is the exception from the jobtracker when I attempted to access the via the job history. It consistently threw this exception for that particular job whereas for other jobs in the history it worked fine..

2007-05-13 18:51:07,343 WARN /: /analysejobhistory.jsp?jobid=job_0002&jobTrackerId=1179059464110:
java.lang.ArrayIndexOutOfBoundsException: 79647
        at org.apache.hadoop.mapred.analysejobhistory_jsp._jspService(analysejobhistory_jsp.java:107)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-1351,Want to kill a particular task or attempt,"It would be convenient to be able to kill a particular task or attempt from the command line. It would look like:

bin/hadoop job -kill-task tip_0001_m_000000
bin/hadoop job -kill-attempt task_0001_m_000000_0

This would allow the user to tell the system to stop a particular task or attempt without having to restart a task tracker."
HADOOP-1350,Shuffle started taking a very long time after the HADOOP-1176 fix,Shuffle phase started taking a very long time after the fix for HADOOP-1176.
HADOOP-1348,Configuration XML bug: comments inside values,"The logic of reading the data of the value causes (very) unexpected behaviour:

value = ((Text)field.getFirstChild()).getData();

When commenting out a part of the value only the part till the comment will be processed.  "
HADOOP-1346,Provide alternatives to merge sort while sorting map output values in memory,"Currently we have only do a merge sort while sorting map output. We can have alternatives to this for the user to choose from, in particular, QuickSort could provide a good improvement."
HADOOP-1345,Checksum object does not get restored to the old state in retries when handle ChecksumException,"In ChecksumFile.FSInputChecker, when a ChecksumException occurs, it tries to recover from the error by reading a different replica. However, the current code does not restore the Checksum object's old state. This causes a read not able to recover from ChecksumException although there are non-corrupted replicas available if the read follows a seek to a position which is not at the checksum chunk boundary . "
HADOOP-1344,getJobName not accessible from JobClient,There isn't any way to access the job name from the JobClient.
HADOOP-1343,"Deprecate the Configuration.set(String,Object) method and make Configuration Iterable","When I was removing the Configuration methods for putting objects into Configurations, I missed one of them and the associated callers. I also extend Configuration to implement Iterable<Map.Entry<String,String>> so that you can iterate through the configuration."
HADOOP-1342,A configurable limit on the number of unique values should be set on the UniqueValueCount and ValueHistogram aggregators,"
In the current implementation, the uniq number of values may increase unbounded, causing out of memory eventually.
"
HADOOP-1340,md5 file in filecache should inherit replication factor from the file it belongs to.,"Currently Distributed FileCache generates for every file an attached md5 file, with default replication. Instead, the replication factor should be inherited from the file it belongs to."
HADOOP-1336,turn on speculative execution by defaul,"Now that speculative execution is working again, we should enable it by default."
HADOOP-1335,C++ reducers under hadoop-pipes are not started when there are no key-value pairs to be reduced,"Probably originally thought as a feature, Hadoop-Pipes does not start  a C++ reducer application when there are no key-values to be processed. Unfortunately, this does not allow to have certain common tasks performed by every reducer independent of key-value pairs. This is a requirement for our applications."
HADOOP-1332,"Sporadic unit test failures (TestMiniMRClasspath, TestMiniMRLocalFS, TestMiniMRDFSCaching)","Since April 22 I've been seeing sporadic failures of these tests on Windows:
 - TestMiniMRClasspath
 - TestMiniMRLocalFS
 - TestMiniMRDFSCaching
The change log since then can be viewed starting at build 66:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/66/

The tests fail because they timeout.  
They timeout because one of the task trackers doesn't go idle.
One of the task trackers doesn't go idle because a map fails and has to be killed.

Reordered and annotated tests logs (my comments are in parenthesis):

    (map 0 executes and logs a 'done' and two 'completed' messages):
    [junit] 2007-05-03 21:19:40,516 INFO  mapred.JobInProgress (JobInProgress.java:findNewTask(653)) - Choosing cached task tip_0001_m_000000
    [junit] 2007-05-03 21:19:40,516 INFO  mapred.JobTracker (JobTracker.java:createTaskEntry(758)) - Adding task 'task_0001_m_000000_0' to tip tip_0001_m_000000, for tracker 'tracker_task000.com:2893'
    [junit] 2007-05-03 21:19:40,516 INFO  mapred.TaskTracker (TaskTracker.java:startNewTask(1071)) - LaunchTaskAction: task_0001_m_000000_0
    [junit] 2007-05-03 21:19:45,655 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1284)) - task_0001_m_000000_0 1.0% hdfs://localhost:2882/testing/ext/input/part-0:0+10
    [junit] 2007-05-03 21:19:46,201 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1284)) - task_0001_m_000000_0 1.0% hdfs://localhost:2882/testing/ext/input/part-0:0+10
    [junit] 2007-05-03 21:19:46,201 INFO  mapred.TaskTracker (TaskTracker.java:reportDone(1334)) - Task task_0001_m_000000_0 is done.
    [junit] 2007-05-03 21:19:46,357 INFO  mapred.JobInProgress (JobInProgress.java:completedTask(734)) - Task 'task_0001_m_000000_0' has completed tip_0001_m_000000 successfully.
    [junit] 2007-05-03 21:19:46,357 INFO  mapred.TaskInProgress (TaskInProgress.java:completedTask(475)) - Task 'task_0001_m_000000_0' has completed.

    (map 2 executes and logs a 'done' message but no 'completed' messages. It is eventually killed):
    [junit] 2007-05-03 21:19:40,594 INFO  mapred.JobInProgress (JobInProgress.java:findNewTask(653)) - Choosing cached task tip_0001_m_000002
    [junit] 2007-05-03 21:19:40,594 INFO  mapred.JobTracker (JobTracker.java:createTaskEntry(758)) - Adding task 'task_0001_m_000002_0' to tip tip_0001_m_000002, for tracker 'tracker_task002.com:2902'
    [junit] 2007-05-03 21:19:40,594 INFO  mapred.TaskTracker (TaskTracker.java:startNewTask(1071)) - LaunchTaskAction: task_0001_m_000002_0
    [junit] 2007-05-03 21:19:46,295 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1284)) - task_0001_m_000002_0 0.0% hdfs://localhost:2882/testing/ext/input/part-0:20+10
    [junit] 2007-05-03 21:19:46,310 INFO  mapred.TaskTracker (TaskTracker.java:reportDone(1334)) - Task task_0001_m_000002_0 is done.
    ...
    [junit] 2007-05-03 21:29:52,957 INFO  mapred.TaskTracker (TaskTracker.java:markUnresponsiveTasks(909)) - task_0001_m_000002_0: Task failed to report status for 606 seconds. Killing.
    (long thread dump)
    (shutting down MiniMRCluster)
    [junit] Waiting for task tracker tracker_task002.com:2902 to be idle.
    [junit] Waiting for task tracker tracker_task002.com:2902 to be idle.
    [junit] Waiting for task tracker tracker_task002.com:2902 to be idle.
    [junit] Waiting for task tracker tracker_task002.com:2902 to be idle.
    [junit] Waiting for task tracker tracker_task002.com:2902 to be idle.
    [junit] Waiting for task tracker tracker_task002.com:2902 to be idle.
    ... (repeated until the test times out)"
HADOOP-1331,Multiple entries for 'dfs.client.buffer.dir',"If the (DFS) client host has multiple drives, I'd like the different dfs  -put calls to utilize these drives. 

Also, 
 - It might be helpful when we have multiple reducers writing to dfs. 
 - If we want datanode/tasktracker to skip dead drive, we probably need this?


"
HADOOP-1328,Hadoop Streaming needs to provide a way for the stream plugin to update global counters,"Sometimes, the stream plugin may want to create/update its own global counters.
There is no way to do so currently.
One possible way to enable that is to use the stderr output of the stream process.
The stream process can emit the global counter update information (such as GLOCAL_COUNTER COUNTER_NAME NUM)
to the stderr of the process. The Stderr handling thread can call the global counter API whenever it encounters the global counter update information line.
"
HADOOP-1327,Doc on Streaming,
HADOOP-1326,Return the RunningJob from JobClient.runJob,It would be nice if the JobClient.runJob returned a value that lets you get the job's name and inquire about its status.
HADOOP-1324,FSError encountered by one running task should not be fatal to other tasks on that node,"Currently, if one task encounters a FSError, it reports that to the TaskTracker and the TaskTracker reinitializes itself and effectively loses state of all the other running tasks too. This can probably be improved especially after the fix for HADOOP-1252. The TaskTracker should probably avoid reinitializing itself and instead get blacklisted for that job. Other tasks should be allowed to continue as long as they can (complete successfully, or, fail either due to disk problems or otherwise)."
HADOOP-1323,Disk problems encountered by a running task are not correctly reported to the user,"Currently, if a running task encounters a disk problem, it report FSError to the TaskTracker. The TaskTracker then reinitializes itself and the state of the running tasks are lost. The JobTracker stops getting status reports about those tasks from that TaskTracker, and fails the tasks with a ""Lost TaskTracker"" diagnostic message. This error reporting should be improved to something like ""The node running the task encountered disk problems""."
HADOOP-1322,Tasktracker blacklist leads to hung jobs in single-node cluster,"Post HADOOP-1278, adding _the_ tasktracker to the blacklist in single-node clusters leads to a situation where the entire cluster is 'flaky' and no trackers are available to execute tasks. 

The straight-forward fix to check before adding the tracker to the blacklist."
HADOOP-1320,Rewrite 'random-writer' to use '-reducer NONE',"Post HADOOP-1216 the 'random-writer' is archaic in the sense that it uses custom code to write data to dfs, this is now supported by the framework and doesn't make sense to reinvent the wheel here."
HADOOP-1319,"NPE in TaskLog.getTaskLogDir, as called from tasklog.jsp","Calling TaskCompletionEvent.getTaskTrackerHttp() gives me an url that looks like http://tasktracker.host:50060/tasklog.jsp?plaintext=true&taskid=task_0264_m_000083_0&all=true.  If I try to access that URL, I get a Jetty 500 error.  In my tasktracker logs, I then see:

2007-05-02 21:32:16,107 WARN /: /tasklog.jsp?taskid=task_0261_m_000000_0&all=true&
plaintext=true: 
java.lang.NullPointerException
        at org.apache.hadoop.mapred.TaskLog.getTaskLogDir(TaskLog.java:49)
        at org.apache.hadoop.mapred.TaskLog.access$000(TaskLog.java:33)
        at org.apache.hadoop.mapred.TaskLog$Reader.<init>(TaskLog.java:313)
        at org.apache.hadoop.mapred.tasklog_jsp.printTaskLog(tasklog_jsp.java:26)
        at org.apache.hadoop.mapred.tasklog_jsp._jspService(tasklog_jsp.java:232)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplication
Handler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567
)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationCo
ntext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:24
4)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-1318,Do not fail completed maps on lost tasktrackers if '-reducer NONE' is specified,"If '-reducer NONE' is specified the map outputs directly go to hdfs, hence there is no reason to fail completed maps on 'lost' tasktrackers."
HADOOP-1315,Hadoop Streaming code clean up,"

Remove dead/half baked code in streaming
Code re-factor/clean up
 "
HADOOP-1313,JobInProgress should be public (or implement a public interface),"I'm trying to get programmatic access to hadoop job/task status through the JobTracker api.

I notice that JobTracker returns a JobInProgress object in several public methods (runningJobs, getJob).  However, JobInProgress is a package-access class.  So, oddly, I can get JobTracker.getJob(), but I can't store the result as a JobInProgress (I suppose I could store it as an Object, but then I couldn't upcast it back).  

The JobInProgress object gives me useful information about jobs, so I don't think making runningJobs/getJob not public is a good idea.  I get the idea from HADOOP-28 that JobInProgress is not public because nobody wants to maintain compatibility in this class across hadoop versions.  

So it would probably be best if we created public interfaces that JobInProgress and TaskInProgress implement.  I only care about the accessors, so maybe from JobInProgress we could expose (getProfile, getStatus, get*Time, {finished,desired,running}{Maps,Reduces}, getMapTasks, getCounters) and from TaskInProgress (isRunning, isComplete, isFailed, isMapTask, numTaskFailures, numKilledTasks, getProgress, getCounters).

Any thoughts?"
HADOOP-1312,heartbeat monitor thread goes away,"The heartbeat monitor thread encounters a ConcurrentModificationException while iterating over the ""heartbeats"" data structure. This occurs when the namenode was getting restarted. There are actuallt two bugs here:

1. The Heartbeat Monitor thread needs to catch Exceptions and continue, instead of exiting.
2. The heartbeats data structures is protected by the heartbeats lock. The registerDatanode() method invokes removeDatanode() without acquiring the heartbeats monitor lock. This causes the ConcurrentModificationException.

"
HADOOP-1311,"Bug in BytesWritable.set(byte[] newData, int offset, int length) ","Current implementation:

  public void set(byte[] newData, int offset, int length) {
    setSize(0);
    setSize(length);
    System.arraycopy(newData, 0, bytes, 0, size);
  }


Correct implementation:
  public void set(byte[] newData, int offset, int length) {
    setSize(0);
    setSize(length);
    System.arraycopy(newData, offset, bytes, 0, size);
  }

please fix.
"
HADOOP-1310,Fix unchecked warnings in aggregate code,Compiling trunk results in 4 unchecked warnings (5 under Java 6). I believe these were introduced by HADOOP-1290 (Move contrib/abacus into mapred/lib/aggregate).
HADOOP-1308,Tighten generic Class restrictions in JobConf.java,JobConf methods have lots of Class<?> parameters.  We should get javac to typecheck these.
HADOOP-1306,DFS Scalability: Reduce the number of getAdditionalBlock RPCs on the namenode,"One of the most-frequently-invoked RPCs in the namenode is the addBlock() RPC. The DFSClient uses this RPC to allocate one more block for a file that it is currently operating upon. The scalability of the namenode will improve if we can decrease the number of addBlock() RPCs. One idea that we want to discuss here is to make addBlock() return more than one block. This proposal came out of a discussion I had with Ben Reed. 

Let's say that addBlock() returns n blocks for the file. The namenode already tracks these blocks using the pendingCreates data structure. The client guarantees that these n blocks will be used in order. The client also guarantees that if it cannot use a block (dues to whatever reason), it will inform the namenode using the abandonBlock() RPC. These RPCs are already supported.

Another possible optimization : since the namenode has to allocate n blocks for a file, should it use the same set of datanodes for this set of blocks? My proposal is that if n is a small number (e.g. 3), it is prudent to allocate the same set of datanodes to host all replicas for this set of blocks. This will reduce the CPU spent in chooseTargets().

"
HADOOP-1305,extra space in mapred.local.dir config results in FileNotFoundException in reduce copyOutput,"When mis-configured with extra space in mapred.local.dir,  TaskTracker comes up and run tasks but always fail when reduce tries to pull the map output.
It would be nice if this error is caught earlier.

example (in hadoop-.0.12.3)

<value>[space]/hadoop/mapred/local</value>

will result in 

at the mapper side.
2007-04-30 16:35:23,260 WARN org.apache.hadoop.mapred.TaskTracker: getMapOutput(task_0001_m_000000_0,0) failed :
java.io.FileNotFoundException: /hadoop/mapred/local/task_0001_m_000000_0/file.out.index
  at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:332)
  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:245)
  at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:1637)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
  at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
  at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
  at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
  at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
  at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
  at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
  at org.mortbay.http.HttpServer.service(HttpServer.java:954)
  at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
  at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
  at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
  at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
  at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
  at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

at the reducer side.
2007-04-30 16:35:23,271 WARN org.apache.hadoop.mapred.TaskRunner: java.io.FileNotFoundException: http://aaa.bbb.com:11111/mapOutput?map=task_0001_m_000000_0&reduce=0
  at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1243)
  at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:200)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.copyOutput(ReduceTaskRunner.java:311)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.run(ReduceTaskRunner.java:274)

"
HADOOP-1304,MAX_TASK_FAILURES should be configurable,"After a couple of weeks of failed attempts I was able to finish a large job only after I changed MAX_TASK_FAILURES to a higher value. In light of HADOOP-1144 (allowing a certain amount of task failures without failing the job) it would be even better if this value could be configured separately for mappers and reducers, because often a success of a job requires the success of all reducers but not of all mappers."
HADOOP-1302,Remove deprecated contrib/abacus code,"With HADOOP-1290, contrib/abacus has moved to the core.  So, in the next release, we should completely remove the now-deprecated contrib/abacus code."
HADOOP-1301,resource management proviosioning for Hadoop,"The Hadoop On Demand (HOD) project addresses the provisioning and managing of MapReduce instances on cluster resources. With HOD, the MapReduce user interacts with the cluster solely through a self-service interface and the JT, TT info ports. The user never needs to log into the cluster or even have an account on the cluster for that matter. HOD allocates nodes, provisions MapReduce (and optionally HDFS) on the cluster and when the user is done with MapReduce jobs, cleanly shuts down MapReduce and de-allocates the nodes (i.e., re-introducing them to the pool of available resources in the cluster).

Using HOD, a cluster can be shared among different users in a fair and efficient manner. HOD is not a replacement or re-implementation of a traditional resource manager. HOD is implemented using the resource manager paradigm and at present is envisioned supporting Torque and Condor out of the box. It also supports ""static"" resources, i.e., a dedicated set of resources not using a resource manager.

HOD is also self provisioning and, thus, can be used on systems such as EC2 or a campus cluster not already running MapReduce software or a resouce manager. Figure 1 depicts a cluster using HOD. As the figure shows, the user never logs into the cluster itself. The user's jobs run as the 'hod' user (a configurable unix id).

The user interacts with MapReduce and the cluster using the hod shell, hodsh. Once in the hodsh, the user can allocate/de-allocate nodes and automatically run JT, TTs, NN, DNs on those nodes without knowing the specifics of which nodes are running which or logging into any of those boxes. HOD transparently masks failures by allocating nodes to replace failed nodes. Once the user has allocated nodes, she can run /bin/MapReduce my1.jar and then /bin/MapReduce my2.jar ... from within the hod shell which automatically generates the configuration file for the MapReduce script. When done, the user will exit the shell.

The hod shell has an automatic timeout so that users cannot hog resources they aren't using. The timeout applies only when there is no MapReduce job running. In addition, hod also has the option of tracking and enforcing user/group resource limits.

Optionally, HOD can run dedicated log and directory services in the cluster. The log services are a central repository for collecting and retrieving Hadoop logs for any given job. The directory service provides an easy way to inspect what's running in the cluster or for the end user and html interfacing for getting to their JT and TT info ports. "
HADOOP-1300,deletion of excess replicas does not take into account 'rack-locality',"One rack went down today, resulting in one missing block/file.
Looking at the log, this block was originally over-replicated. 
3 replicas on one rack and 1 replica on another.

Namenode decided to delete the latter, leaving 3 replicas on the same rack.

It'll be nice if the deletion is also rack-aware."
HADOOP-1299,"Once RPC.stopClient has been called, RPC can not be used again","Calls to RPC.stopClient render RPC subsequently unusable -- at least until a reload of RPC class so static initializer has a chance to run again.

I am trying to write unit tests for a little cluster built atop hadoop RPC class.  Intent is to spin up the little cluster before each test is run.  Post unit test, the cluster is torn down.  Part of the takedown includes invocation of RPC.stopClient to clean up any outstanding connections, etc.

I've found that when the second unit test runs, RPC Client is horked.  The 'running' flag is false so Connections can't work and 'Connection Culler' is not running."
HADOOP-1298,adding user info to file,"I'm working on adding a permissions model to hadoop's DFS. The first step is this change, which associates user info with files. Following this I'll assoicate permissions info, then block methods based on that user info, then authorization of the user info. 

So, right now i've implemented adding user info to files. I'm looking for feedback before I clean this up and make it offical. 

I wasn't sure what release, i'm working off trunk. "
HADOOP-1297,datanode sending block reports to namenode once every second,"The namenode is requesting a block to be deleted. The datanode tries this operation and encounters an error because the block is not in the blockMap. The processCommand() method raises an exception. The code is such that the variable lastBlockReport is not set if processCommand() raises an exception. This means that the datanode immediately send another block report to the namenode. The eats up quite a bit of CPU on namenode.

In short, the above condition causes the datanode to send blockReports almost once every second!

I propose that we do the following:

1. in Datanode.offerService, replace the following piece of code

DatanodeCommand cmd = namenode.blockReport(dnRegistration,
data.getBlockReport());
processCommand(cmd);
lastBlockReport = now;

with

DatanodeCommand cmd = namenode.blockReport(dnRegistration,
data.getBlockReport());
lastBlockReport = now;
processCommand(cmd);

2. In FSDataSet.invalidate:
a) continue to process all blocks in invalidBlks[] even if one in the middle encounters a problem.
b) if getFile() returns null, still invoke volumeMap.get() and print whether we found the block in
volumes or not. The volumeMap is used to generate the blockReport and this might help in debugging.
["
HADOOP-1296,Improve interface to FileSystem.getFileCacheHints,"The FileSystem interface provides a very limited interface for finding the location of the data. The current method looks like:

String[][] getFileCacheHints(Path file, long start, long len) throws IOException

which returns a list of ""block info"" where the block info consists of a list host names. Because the hints don't include the information about where the block boundaries are, map/reduce is required to call the name node for each split. I'd propose that we fix the naming a bit and make it:

public class BlockInfo extends Writable {
  public long getStart();
  public String[] getHosts();
}

BlockInfo[] getFileHints(Path file, long start, long len) throws IOException;

So that map/reduce can query about the entire file and get the locations in a single call."
HADOOP-1294,Fix unchecked warnings in main Hadoop code under Java 6.,"Reported by Tahir Hashmi:

I get the following warning while building:

    [javac]
/home/tahir/Desktop/Trunk/trunk/src/test/org/apache/hadoop/dfs/ClusterTestDFS.java:439:
warning: [unchecked] unchecked call to
getConstructor(java.lang.Class<?>...) as a member of the raw type
java.lang.Class
    [javac]         randomDataGeneratorCtor = clazz.getConstructor(new
Class[]{Long.TYPE});
    [javac]                                                       ^
    [javac] Note: Some input files use or override a deprecated API."
HADOOP-1293,stderr from streaming skipped after first 20 lines.  ,"It prints to stderr when this condition is true

> if (num < 20 || (now-lastStderrReport > reporterErrDelay_))

Either first 20 lines or certain interval has passed since the last stderr output.

Output is logged  to syslog but can be overwritten by streaming infrastructure outputs.
"
HADOOP-1292,dfs -copyToLocal should guarantee file is complete,"We should copy to a temporary file, maybe _tmp.<realname>, and then rename the file when the copy is complete.  Restarting a copy should reuse the _tmp file, just checksumming it.  Then ^Cing a copy will do the right thing.

Original suggestion:

On Apr 23, 2007, at 2:38 AM, Richard Kasperski wrote:

I'd like to have a guarantee that a file copy is both completed and that the file is whole. In the past I've done this  by copying the file to a temporary name tmp.<realname> and then moving it to <realname> once I have the file copy is complete. This has the following very nice properties; If the <realname> exists then the file copy is complete and I'm not looking at a partial copy of the file. I believe that the copy to the cluster has both of these properties in that the file doesn't appear in a DFS directory until the whole file has been copied. The copy from the cluster to a local file system does not have these guarantees and it would be very nice if it did. There are two scenarios under what I wish to use this. First is that if I ctrl-c the 'hadoop dfs -copyToLocal' I know what parts are complete and what parts aren't. Second I can run a background compressor to compress the files as they are copied.
"
HADOOP-1291,Split files in HDFS into appropriately named packages,"HDFS should move into a set of packages under org.apache.hadoop.dfs, or more appropriately org.apache.hadoop.fs.dfs.
"
HADOOP-1290,Move Hadoop Abacus to hadoop.mapred.lib,"
Owen and I discussed this issue and we both felt that it is appropriate to move Hadoop Abacus to the hadoop main framework.
Any comments/thoughts/concerns/objections?

"
HADOOP-1289,Just launched tasks aren't getting cleaned up on job completion (success/killed),"When a job is killed, if there are tasks which haven't been launched yet (just scheduled on a tasktracker) it leads to a situation where the non-launched task hangs around for a long time on the tasktrackers taking up valuable 'slots' preventing the scheduling of _legitimate_ tasks on them.

This is due to a race-condition:
a) Job is killed:
    JobInProgress.kill
    -> JobInProgress.garbageCollect
    -> JobInProgress.finalizeJob
    -> JobTracker.removeJobTasks
    -> JobTracker.removeTaskEntry - this removes the taskid -> TIP entry in JobTracker.taskIdToTIPMap
b) ExpireLaunchingTasks thread detects the task has timedout, however since the taskid -> TIP map is missing it cannot 'fail' the task - thus leaving it orphaned on the tasktracker.

One possible solution is to broadcast a KillJobAction to all trackers in the cluster (or only those trackers which ever ran a task of this Job) ensuring the right cleanup... clearly this is a hairy situation.

Thoughts?
"
HADOOP-1286,Distributed cluster upgrade,"Some data layout changes in HDFS require more than just a version upgrade introduced in HADOOP-702,
because the cluster can function properly only when all components have upgraded, and the components
need to communicate to each other and exchange data before they can perform the upgrade.
The CRC upgrade discussed in HADOOP-1134 is one of such examples. Future enhancements like
implementation of appends can change block meta-data and may require distributed upgrades.

Distributed upgrade (DU) starts with a version upgrade (VU) so that at any time one could rollback
all changes and start over.
When VU is finished the name-node enters safe mode and persistently records that the DU have been started.
It will also need to write a record when DU is finished. This is necessary to report unfinished upgrades in case
of failure or for monitoring.

The actual upgrade code from version vO to vN should be implemented in a separate UpgradeObject class,
which implements interface Upgradeable.
We create a new UpgradeObject for each pair of versions vO to vN that require a DU.
We keep a (hard coded) table that determines which UpgradeObject(s) are applicable for the version pairs.
Something like:
|| Old version || New version || class names ||
| vO1 | vN1 | NameUpgradeObject1, DataUpgradeObject1 |
| vO2 | vN2 | NameUpgradeObject2, DataUpgradeObject2 |
where vO1 < vN1 < vO2 < vN2 ...
Now, if we need to upgrade from version version vX to version vY, we look for all pairs <vOi, vNi>
in the table such that vX < vOi < vNi < vY and perform corresponding DUs one after another as they appear in the table.

Each DU can and most probably should contain multiple UpgradeObjects.
I'd define one object for the name-node and one for the data-nodes.
The upgrade objects (in the same row) can communicate to each other either via existing protocols or using
temporary protocols defined exclusively for this particular upgrade objects.
I envision that some DUs will need to use old  (vO) protocols to exchange the pre-upgrade data,
and new (vN) protocols to reoport the upgraded data.

UpgradeObjects should be able to bypass safe mode restrictions, be able to +modify+ name-node data.
"
HADOOP-1285,ChecksumFileSystem : Can't read when io.file.buffer.size < bytePerChecksum,"
Looks like ChecksumFileSystem fails to read a file when bytesPerChecksum is larger than io.file.buffer.size. Default for bytesPerChecksum  and buffer size are 512 and 4096, so default config might not see the problem.

I noticed this problem when I was testing block level CRCs with different configs.

How to reproduce with latest trunk:
Copy a text  file larger than 512 bytes to dfs : bin/hadoop fs -copyFromLocal ~/tmp/x.txt x.txt
then set io.file.buffer.size to something smaller than 512 (say 53). Now try to read the file :

 bin/hadoop dfs -cat x.txt

This will print only the first 53 characters.

The following code or comment at  ChecksumFileSystem.java:163 seems suspect. But not sure if more changes are required:
{code}
    public int read(byte b[], int off, int len) throws IOException {
      // make sure that it ends at a checksum boundary
      long curPos = getPos();
      long endPos = len+curPos/bytesPerSum*bytesPerSum;
      return readBuffer(b, off, (int)(endPos-curPos));
    }
{code}
"
HADOOP-1284,clean up the protocol between stream mapper/reducer and the framework,"Right now, the protocol between stream mapper/reducer and the framework is very inflexible.
The mapper/reducer generates line oriented output. The framework picks up line by line, and split 
each line into a key/value pair. By default, the substring up to the first tab char is the key, and the 
substring after the first tab char is the value.

However, in many cases, the application wants some control over how the pair is split. 
Here, I'd like to introduce the following configuration variables for that:

1. ""streaming.output.field.separator"": the value will be the tab key, by default. 
But the user can specify a different one (e.g. ':', or ', ', etc.)
A map output line can be considered as a list of fields separated by the separator.

2. ""streaming.num.fields.for.mapout.key"":  the number of the first fields will be used the map output key  
(and for sorting in the reduce side). 
The default value is 1.
The rest of the fields will be used as the value.  For example, I can specify the first 5 fields as my mapout key.

3. ""streaming.num.fields.for.partitioning"": Sometimes, I want to use fewer fields for partitioning to 
achieve ""primary/secondary"" composite 
key effect as proposed in HADOOP485. The default value is 1. 
For example, I can set ""streaming.num.fields.for.partitioning"" to 3 
and ""streaming.num.fields.for.mapout.key"" to 5. 
This effectively amounts to saying that fields 4 and 5 are my secondary key.

With the above default values, it is compatible with the current behavior 
while introducing a new desirable feature in a clean way.

Thoughts?


"
HADOOP-1283,Eliminate internal UTF8 to String and vice versa conversions in the name-node.,"We have internal conversions of those two types inside name-node code. One example:
NameNode.complete(String src, String clientName)
then it calls
FSNamesystem.completeFile(new UTF8(src), new UTF8(clientName));
which in turn finally calls
FSDirectory.addNode(path.toString(), newNode )
and in another place
FSDirectory.getNode(src.toString())

So we have several conversions of the same parameter back and forth during computation.
We should keep the parameter type consistent within different methods.

The question is, which type should be used: String or Text.
From previous discussions I remember that Text is more efficient in space and time for non ASCII
data. Here we mostly deal with file names and network addresses, which are ASCII.
Does it make sense to use Text in this case?

UTF8 is also used as a key in two maps: pendingCreates and leases.
This should be replaced too."
HADOOP-1281,Speculative map tasks aren't getting killed although the TIP completed,The speculative map tasks run to completion although the TIP succeeded since the other task completed elsewhere.
HADOOP-1280,Speculative tasks which are killed are being marked as 'FAILED',"It isn't a big problem once we get HADOOP-1050, however it's still confusing for users who need to double check if the task was a speculative one which got 'killed' or was a genuine failure."
HADOOP-1279,list of completed jobs purges jobs based on submission not on completion age,"The JobTracker keeps at most the last 100 completed jobs.

One this limit is exceeded and there is a new completed job the old submitted job in completed state is removed regardless if there are jobs that have finished much earlier in the completed list.

The job should be added at completion time, not a submission time to avoid this problem.
"
HADOOP-1278,Fix the per-job tasktracker 'blacklist',"Today whenever a tracker is 'lost' all the jobs which ever ran on it are considered as failures and added to the blacklist, which automatically ensures that the particular TT is *never* considered for allocating new tasks unless *all* tasktrackers are on the list. This results in an ugly situation where a majority of nodes in the cluster are on the blacklist and hence idle, while the other TTs are maxed out.

The proposal is two-fold:
a) Don't count *all* tasks which ever ran on the TT, we can count it as a 'single' task failure - which means that each 'lost' tracker results in a loss of 20% of the '5 failures == blacklisted'  quota.
b) Stop adding nodes to the blacklist when a certain percentage of the cluster, say 25%, are already on the blacklist - adding more than that would just delay the inevitable i.e. there is something horrendously wrong with the cluster - we might as well fail the job early and noisily.

Thoughts?"
HADOOP-1276,TaskTracker expiry interval is not configurable,"The tasktracker expiry interval is hardcoded to 10 mins with the MRConstants.TASKTRACKER_EXPIRY_INTERVAL constant.

For small clusters, running small jobs, this interval is too high.

Making it configurable it would require:

 * Introducing a 'tasktracker.expiry.interval' property with default value of 10 mins.
 * Load the property in the JobTracker
 * Change the 5 usages of the constant by a property in the JobTracker
 * Remove the constant from MRConstants
 * add default value to the hadoop-default.xml file
"
HADOOP-1275,job notification property in hadoop-default.xml is misspelled,"Property name 'job.end.retry.attempts' is misspelled in hadoop-default.xml as 

  <name>job.end.retry.attempt</name>

"
HADOOP-1274,Configuring different number of mappers and reducers per TaskTracker,"Depending on the application, it sometimes make sense to have more mappers than reducers assigned to each node. 
(I'm assuming user either has a dedicated cluster or use HOD.)
"
HADOOP-1273,Upgrade Jetty to 6.x,We should try again to upgrade to Jetty 6 (previous attempt was HADOOP-565 and HADOOP-736) because it should substantially help the cpu usage on the TaskTracker. See http://www.infoq.com/news/jetty-6-release for a discussion of the improvements to Jetty 6.
HADOOP-1272,Extract InnerClasses from FSNamesystem into separate classes,"This will make the code cleaner. Also, it leads itself to a cleaner and easily understandable finer-grain locking model."
HADOOP-1271,The StreamBaseRecordReader is unable to log record data that's not UTF-8 ,"Streaming code at times can work on data that's not UTF-8, for instance image/audio data. The StreamBaseRecordReader tries to log some status info, which contains part of the record, using the LOG.info() call. This call throws this exception :

java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:111)
	at org.apache.hadoop.mapred.TaskLog$Writer.write(TaskLog.java:217)
	at org.apache.hadoop.mapred.TaskLogAppender.append(TaskLogAppender.java:51)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65)
	at org.apache.log4j.Category.callAppenders(Category.java:203)
	at org.apache.log4j.Category.forcedLog(Category.java:388)
	at org.apache.log4j.Category.log(Category.java:853)
	at org.apache.commons.logging.impl.Log4JLogger.info(Log4JLogger.java:133)
	at org.apache.hadoop.streaming.StreamBaseRecordReader.numRecStats(StreamBaseRecordReader.java:114)
	at org.apache.hadoop.streaming.StreamXmlRecordReader.next(StreamXmlRecordReader.java:99)
	at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)
"
HADOOP-1270,Randomize the fetch of map outputs,"HADOOP-248 did away with random probing of maps for locating map outputs and instead we now rely on TaskCompletionEvents for the same. 

However we lost out on the benefit that the randomization in probing resulted in an added benefit where the map's jetty isn't overloaded with requests for the outputs. We have now a situation where a map completes, the JT is notified, *all* the reduces get the TaskCompletionEvent and pretty much swamp the poor map's jetty and this repeats for each map.

I propose we make a minor change where we collect a set of TaskCompletionEvents and randomize the list before firing the fetches. Should help fix this mass-hysteria at the map's jetty.

Thoughts?"
HADOOP-1269,DFS Scalability: namenode throughput impacted becuase of global FSNamesystem lock,"I have been running a 2000 node cluster and measuring namenode performance. There are quite a few ""Calls dropped"" messages in the namenode log. The namenode machine has 4 CPUs and each CPU is about 30% busy. Profiling the namenode shows that the methods the consume CPU the most are addStoredBlock() and getAdditionalBlock(). The first method in invoked when a datanode confirms the presence of a newly created block. The second method in invoked when a DFSClient request a new block for a file.

I am attaching two files that were generated by the profiler. serverThreads40.html captures the scenario when the namenode had 40 server handler threads. serverThreads1.html is with 1 server handler thread (with a max_queue_size of 4000).

In the case when there are 40 handler threads, the total elapsed time taken by  FSNamesystem.getAdditionalBlock() is 1957 seconds whereas the methods that that it invokes (chooseTarget) takes only about 97 seconds. FSNamesystem.getAdditionalBlock is blocked on the global FSNamesystem lock for all those 1860 seconds.

My proposal is to implement a finer grain locking model in the namenode. The FSNamesystem has a few important data structures, e.g. blocksMap, datanodeMap, leases, neededReplication, pendingCreates, heartbeats, etc. Many of these data structures already have their own lock. My proposal is to have a lock for each one of these data structures. The individual lock will protect the integrity of the contents of the data structure that it protects. The global FSNamesystem lock is still needed to maintain consistency across different data structures.

If we implement the above proposal, both addStoredBlock() and getAdditionalBlock() does not need to hold the global FSNamesystem lock. startFile() and closeFile() still needs to acquire the global FSNamesystem lock because it needs to ensure consistency across multiple data structures.










"
HADOOP-1266,Remove DatanodeDescriptor dependency from NetworkTopology,"NetworkTopology should operate only with the Node interface.
The actual class DatanodeDescriptor, which implements Node should not be used inside.
We should avoid introducing unnecessary dependencies.
This is the typical case when everything can and should be defined using the interface.
This becomes a requirement if we want to measure distances between other than data-nodes
types of nodes like TaskTrackers, clients.
"
HADOOP-1265,TaskTracker won't bind to localhost,"Connecting to the TaskTracker with the default configuration fails with the firewall settings as mentioned in the Environment.  This means the job seems to start, but then will hang with all tasks at 0%

Also, setting mapred.tasktracker.dns.interface to 'lo' has no effect.  I would expect this bound the TaskTracker in such way that local connections would be made.

To make it work, I have to explicitly poke a hole in the firewall:
# hadoop
iptables -A INPUT --protocol tcp --destination-port 50050 -j ACCEPT
iptables -A OUTPUT --protocol tcp --destination-port 50050 -j ACCEPT

While in practise a Hadoop will often run on a cluster (so the firewall has to be opened anyway), I don't think this should be the default behaviour, because it is highly confusing."
HADOOP-1264,OutOfMemoryError while running sort example,"The problem is reproducible, and I get this stacktrace:

java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:604)
        at org.apache.hadoop.examples.Sort.main(Sort.java:115)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:143)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:40)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)


java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2786)
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.io.BytesWritable.write(BytesWritable.java:138)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:318)
        at org.apache.hadoop.mapred.lib.IdentityMapper.map(IdentityMapper.java:39)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)"
HADOOP-1263,"retry logic when dfs exist or open fails temporarily, e.g because of timeout","Sometimes, when many (e.g. 1000+) map jobs start at about the same time and require supporting files from filecache, it happens that some map tasks fail because of rpc timeouts. With only the default number of 10 handlers on the namenode, the probability is high that the whole job fails (see Hadoop-1182). It is much better with a higher number of handlers, but some map tasks still fail.

This could be avoided if rpc clients did retry when encountering a timeout before throwing an exception.

Examples of exceptions:

java.net.SocketTimeoutException: timed out waiting for rpc response
at org.apache.hadoop.ipc.Client.call(Client.java:473)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:163)
at org.apache.hadoop.dfs.$Proxy1.exists(Unknown Source)
at org.apache.hadoop.dfs.DFSClient.exists(DFSClient.java:320)
at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.exists(DistributedFileSystem.java:170)
at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.open(DistributedFileSystem.java:125)
at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.(ChecksumFileSystem.java:110)
at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:330)
at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:245)
at org.apache.hadoop.filecache.DistributedCache.createMD5(DistributedCache.java:327)
at org.apache.hadoop.filecache.DistributedCache.ifExistsAndFresh(DistributedCache.java:253)
at org.apache.hadoop.filecache.DistributedCache.localizeCache(DistributedCache.java:169)
at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:86)
at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:117)

java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:473)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:163)
        at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:511)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.<init>(DFSClient.java:498)
        at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:207)
        at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.open(DistributedFileSystem.java:129)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.<init>(ChecksumFileSystem.java:110)
        at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:330)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:245)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:82)
        at org.apache.hadoop.fs.ChecksumFileSystem.copyToLocalFile(ChecksumFileSystem.java:577)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:766)
        at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:370)
        at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:877)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:545)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:913)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1603)

"
HADOOP-1262,file corruption detected because dfs client does not use replica blocks for checksum file,"A block of a crc file was corrupted. This caused the DFS client to detect a CRc corruption. The client tried all the three replicas of the data file. It did not try any replicas of the CRC file. This caused the client to abort the read request with a bad-CRC message.

07/04/16 20:42:26 INFO fs.FileSystem: Found checksum error in data stream at block=blk_6205660483922449140 on datanode=xx:50010
07/04/16 20:42:26 INFO fs.FileSystem: Found checksum error in checksum stream at block=blk_-3722915954820866561 on datanode=yy:50010

07/04/16 20:42:26 INFO fs.FileSystem: Found checksum error in data stream at block=blk_6205660483922449140 on datanode=zz:50010
07/04/16 20:42:26 INFO fs.FileSystem: Found checksum error in checksum stream at block=blk_-3722915954820866561 on datanode=yy:50010

07/04/16 20:42:26 INFO fs.FileSystem: Found checksum error in data stream at block=blk_6205660483922449140 on datanode=xx.:50010
07/04/16 20:42:26 INFO fs.FileSystem: Found checksum error in checksum stream at block=blk_-3722915954820866561 on datanode=yy:50010"
HADOOP-1261,Restart of the same data-node should not generate edits log records.,"Currently during registration of a data-node with the storage id that has already been registered
the name-node logs 2 records: <add node1> <remove node2>.
If the same node has been restarted these two records are redundant, lead to unnecessary
increase of the edits file and result in a slow-down of the name-node startup.
"
HADOOP-1260,need code review guidelines,"Following the improvements to the Hadoop release process (HADOOP-1161), it would be helpful to better document what a code review should entail.  I've posted a proposal here (not yet linked to from anywhere):

http://wiki.apache.org/lucene-hadoop/CodeReviewChecklist

Thoughts?"
HADOOP-1259,DFS should enforce block size is a multiple of io.bytes.per.checksum ,"
DFSClient currently does not enforce that dfs.block.size is a multiple io.bytes.per.checksum. This not really problem currently but can future upgrades like HADOOP-1134 (see one of the comments http://issues.apache.org/jira/browse/HADOOP-1134#action_12488542 there). 

I propose DFSClient should fail loudly and ask the user politely to change the config to meet this conidtion. Of course we will change the documentation for dfs.block.size also.
"
HADOOP-1258,TestCheckpoint test case doesn't wait for MiniDFSCluster to be active,TestCheckpoint is missing a waitForCluster call in one place.
HADOOP-1256,Dfs image loading and edits loading creates multiple instances of DatanodeDescriptor for the same datanode,This leads to multiple instances of DatanodeDescriptors for the same datanode stored in Host2DatanodeMap.
HADOOP-1255,Name-node falls into infinite loop trying to remove a dead node.,"Under certain conditions the name-node fall into infinite loop in heartbeatCheck().
It's rather hard to reproduce. I'm running one node cluster: 1 name-node, 1 data-node.
The data-node dies, and 10 minutes later I get

07/04/12 10:40:34 INFO net.NetworkTopology: Removing a node: /default-rack/0.0.0.0:50077
07/04/12 10:44:35 INFO dfs.StateChange: BLOCK* NameSystem.heartbeatCheck: lost heartbeat from 0.0.0.0:50077
...................................................
07/04/12 10:45:17 INFO net.NetworkTopology: Removing a node: /default-rack/0.0.0.0:50077
07/04/12 10:47:44 INFO dfs.StateChange: BLOCK* NameSystem.heartbeatCheck: lost heartbeat from 0.0.0.0:50077

Here is what I see in the debugger:
FSNamesystem.heartbeats contains 2 identical (same instance) DatanodeDescriptor entries, both have
DatanodeDescriptor.isAlive = false. The heartbeatCheck() correctly detects that there is a dead node in
the list, but removeDatanode() does not delete the node from the heartbeats because it is dead.
"
HADOOP-1254,TestCheckpoint fails intermittently,"TestCheckpoint started intermittently failing last night:

http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/55/testReport/org.apache.hadoop.dfs/TestCheckpoint/testCheckpoint/

This is probably caused by one of the changes introduced yesterday:

http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/55/


"
HADOOP-1253,ConcurrentModificationException and NPE in JobControl,"toArrayList(Hashtable jobs) in JobControl.java isn't synchronized and it resulted in a ConcurrentModificationException.
checkRunningState in Job.java can throw a NullPointerException if running = jc.getJob(this.mapredJobID) fails, leaving running null."
HADOOP-1252,Disk problems should be handled better by the MR framework,"The MR framework should recover from Disk Failure problems without causing jobs to hang. Note that this issue is about a short-term solution to solving the problem. For example, by looking at the code and improving the exception handling (to better detect faulty disks and missing files). The long term approach might be to have a FS layer that takes care of failed disks and makes it transparent to the tasks. That will be a separate issue by itself.
Some of the issues that have been reported are HADOOP-1087 and a comment by Koji on HADOOP-1200 (not sure whether those are all). Please add to this issue as much details as possible on disk failures leading to hung jobs (details like relevant exception traces, way to reproduce, etc.)."
HADOOP-1251,A method to get the InputSplit from a Mapper,I need a way to get the InputSplit from a Mapper. This is effectively a stop gap until we get context objects and can fix this much better. *smile* I introduce a static method in the TaskTracker named getInputSplit() that returns the InputSplit.
HADOOP-1250,Remove the MustangFile class from streaming and promote the chmod into FileUtils,I need a chmod utility and streaming has one that is hidden away. Reimplement it in FileUtils and remove the MustangFile class.
HADOOP-1249,Unit test for detecting corruption of replicas in DFS,DFS has this important feature that the DFSClient detects corrupted replicas of a block. We need a unit test that ensures that DFSClient is directed to access a good copy of a block and ignores corrupted replicas. The existing unit test named TestFileCorruption does not test this feature.
HADOOP-1247,Make Hadoop Abacus work with Hadoop Streaming,"It will be nice if Hadoop streaming can use Hadoop Abacus.

This requires to make some changes on StreamJob and StreamMapRed classes so that the streaming job uses the Abacus reducer/combiner classes, and streaming mapper generates intermediate data conforming to Hadoop Abacus protocol.
"
HADOOP-1245,"value for mapred.tasktracker.tasks.maximum taken from jobtracker, not tasktracker","I want to create a cluster with machines with different numbers of CPUs.  Consequently, each machine should have a different value for mapred.tasktracker.tasks.maximum, since my map tasks are CPU bound.

When a new job starts up, the jobtracker uses its (single) value for mapred.tasktracker.tasks.maximum to assign tasks.  This means that each tasktracker gets the same number of tasks, regardless of how I configured that particular machine.

The jobtracker should not consult its config for the value of mapred.tasktracker.tasks.maximum.  It should assign tasks (or allow tasktrackers to request tasks) according to each tasktracker's value of mapred.tasktracker.tasks.maximum.

Originally, I thought the behavior was slightly different, so this issue contained this text:
After the first task finishes on each tasktracker, the tasktracker will request new tasks from the jobtracker according to the tasktracker's value for mapred.tasktracker.tasks.maximum.  So after the first round of map tasks is done, the cluster reverts to a mode that works well for heterogeneous clusters.
"
HADOOP-1244,stop-dfs.sh incorrectly specifies slaves file for stopping datanode,bin/stop-dfs.sh incorrectly specifies a slaves file in a random location for stopping datanode.  This overrides the value set explicitly in HADOOP_CONF_DIR.
HADOOP-1243,ClientProtocol.versionID should be 11,"HADOOP-1133 added a new ClientProtocol method metaSave() but did not increment the protocol version.
Just the comment. We should fix it before the next release.
    /*
     * 11: metasave() added
     */
    public static final long versionID = 10L; "
HADOOP-1242,dfs upgrade/downgrade problems,"I ran my test cluster on 0.13 and then tried to run it under 0.12. When I downgraded, the namenode would not come up and the message said I needed to format the filesystem. I ignored that and tried to restart on 0.13, now the datanode will not come up with:

2007-04-10 11:25:37,448 ERROR org.apache.hadoop.dfs.DataNode: org.apache.hadoop.
dfs.InconsistentFSStateException: Directory /local/owen/hadoop/dfs/d
ata is in an inconsistent state: Old layout block directory /local/owen/hadoop/dfs/data/data is missing
        at org.apache.hadoop.dfs.DataStorage.isConversionNeeded(DataStorage.java
:170)
        at org.apache.hadoop.dfs.Storage$StorageDirectory.analyzeStorage(Storage
.java:264)
        at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.j
ava:83)
        at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:230)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:199)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1175)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1119)
        at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:1140)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1299)
"
HADOOP-1241,Null PointerException in processReport when namenode is restarted,"INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020 call error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSNamesystem.shouldNodeShutdown(FSNamesystem.java:3341)
        at org.apache.hadoop.dfs.FSNamesystem.processReport(FSNamesystem.java:2075)
        at org.apache.hadoop.dfs.NameNode.blockReport(NameNode.java:661)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:339)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)
INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from xx.xx.xx.xx:50010 storage DS-355824822
INFO org.apache.hadoop.dfs.NameNode: BLOCK* NameSystem.registerDatanode: node from name: xx.xx.xx.xx:50010
INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/xx.xx.xx.xx:50010
INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/xx.xx.xx.xx:50010
"
HADOOP-1239,Classes in src/test/testjar need package name,The three classes in src/test/testjar should have a package name. Eclipse complains because they don't.
HADOOP-1238,maps_running metric is only updated at the end of the task,"The maps_running and reduces_running metrics aren't getting into ganglia correctly - they always appear to be underestimated.  A quick look at the TaskTracker code shows that these metrics are only updated when a task is finished.  Of course, once a task is finished, the reporting will not note the completed task (since it's not running anymore)."
HADOOP-1236,hadoop dfs -rmr dir should ask the user to confirm before deleting the dir,"
The current behavior is very dangerous.
Imagine somebody type in 

hadoop dfs -rmr /

and then accidentally hit the return key.
"
HADOOP-1234,map tasks fail because they do not find application in file cache,"Using C++ map-reduce interface, a lot of tasks fail because they do not find the application in the file cache.
Might be a race condition between multiple tasks on the same node.

java.io.IOException: Cannot run program ""<path>/mapred/local/taskTracker/archive/<namenodehost>/<path>/<application>"": java.io.IOException: error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
	at org.apache.hadoop.mapred.pipes.Application.runClient(Application.java:131)
	at org.apache.hadoop.mapred.pipes.Application.(Application.java:59)
	at org.apache.hadoop.mapred.pipes.PipesMapRunner.run(PipesMapRunner.java:44)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:176)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1459)
Caused by: java.io.IOException: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.(UNIXProcess.java:148)
	at java.lang.ProcessImpl.start(ProcessImpl.java:65)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
	... 5 more
"
HADOOP-1233,repeated job failures because of too many 'lost tasktrackers',"Several attempts to run large jobs (100,000+ map taks, 1000+ reducers) on a 1000 node cluster failed, mainly because of too many lost and eventually blacklisted tasktrackers.


Jobtracker log:
2007-04-09 00:16:55,180 INFO org.apache.hadoop.mapred.JobInProgress: TaskTracker at 'tracker_<host>' turned 'flaky'
2007-04-09 00:16:55,180 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0100_m_068220_0' from 'tracker_<host>:50050'
2007-04-09 00:16:55,294 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0100_m_000177_0: Lost task tracker
2007-04-09 00:16:55,294 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0100_m_000177_0' has been lost.
2007-04-09 00:16:55,294 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0100_m_001426_0: Lost task tracker
2007-04-09 00:16:55,294 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0100_m_001426_0' has been lost.
...

Checking a few  tasktracker logs, they did not show any exceptions at the same time, but started to get a lot of communication errors some time later (20 seconds up to a few minutes).
e.g.:
2007-04-09 00:17:16,788 WARN org.apache.hadoop.ipc.Server: handler output error
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:126)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:578)
"
HADOOP-1232,Datanode did not get removed from blockMap when a datanode was down,"After  a datanode shuted down, the following exception was thrown when a job tried to open a file with blocks on the data node. It looks that the datanode was removed from NetworkTopology but not from the blockMap.

org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.IllegalArgumentException: Unexpected non-existing data node: /xxx/yyy:50010
        at org.apache.hadoop.net.NetworkTopology.checkArgument(NetworkTopology.java:379)
        at org.apache.hadoop.net.NetworkTopology.getDistance(NetworkTopology.java:396)
        at org.apache.hadoop.dfs.FSNamesystem$ReplicationTargetChooser$1.compare(FSNamesystem.java:3161)
        at org.apache.hadoop.dfs.FSNamesystem$ReplicationTargetChooser$1.compare(FSNamesystem.java:3160)
        at java.util.Arrays.mergeSort(Arrays.java:1270)
        at java.util.Arrays.sort(Arrays.java:1210)
        at java.util.Collections.sort(Collections.java:159)
        at org.apache.hadoop.dfs.FSNamesystem$ReplicationTargetChooser.sortByDistance(FSNamesystem.java:3159)
        at org.apache.hadoop.dfs.FSNamesystem.open(FSNamesystem.java:549)
        at org.apache.hadoop.dfs.NameNode.open(NameNode.java:250)
        at sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)
 
        at org.apache.hadoop.ipc.Client.call(Client.java:471)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:163)
        at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:511)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:498)
        at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:207)
        at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.open(DistributedFileSystem.java:129)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.(ChecksumFileSystem.java:110)
        at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:330)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:245)
        at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:54)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:139)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)
"
HADOOP-1231,Add generics to Mapper and Reducer interfaces,"By making the input and output types of the Mapper and Reducers generic, we can get the information from the classes and not require the user to set them in the configuration."
HADOOP-1230,"Replace parameters with context objects in Mapper, Reducer, Partitioner, InputFormat, and OutputFormat classes","This is a big change, but it will future-proof our API's. To maintain backwards compatibility, I'd suggest that we move over to a new package name (org.apache.hadoop.mapreduce) and deprecate the old interfaces and package. Basically, it will replace:

package org.apache.hadoop.mapred;
public interface Mapper extends JobConfigurable, Closeable {
  void map(WritableComparable key, Writable value, OutputCollector output, Reporter reporter) throws IOException;
}

with:

package org.apache.hadoop.mapreduce;
public interface Mapper extends Closable {
  void map(MapContext context) throws IOException;
}

where MapContext has the methods like getKey(), getValue(), collect(Key, Value), progress(), etc."
HADOOP-1228,Eclipse project files,"I've created Eclipse project files for Hadoop (to be attached). I've found them very useful for exploring Hadoop and running the unit tests.

The project files can be included in the source repository to make it easy to import Hadoop into Eclipse.

A few features:

- Eclipse automatically calls the Ant build to generate some of the necessary source files
- Single unit tests can be run from inside Eclipse
- Basic Java code style formatter settings for the Hadoop conventions (still needs some work)

The following VM arguments must be specified in the run configuration to get unit tests to run:

-Xms256m -Xmx256m -Dtest.build.data=${project_loc}\build\test\data

Some of the unit tests don't run yet, possibly due to some missing VM flags, the fact that I'm running Windows, or some other reason(s).

TODO:

- Specify native library location(s) once I investigate building of Hadoop's native library
- Get all the unit tests to run"
HADOOP-1226,makeQualified should return an instance of a DfsPath when passed  a DfsPath ,"Currently, it returns a instance of a Path.



"
HADOOP-1224,"""Browse the filesystem"" link pointing to a dead data-node","On the NameNode status web page ""Browse the filesystem"" link can point to a dead data-node.
The reason for that is that FSNamesystem.randomDataNode() selects a random node from the
list of all nodes rather then selecting among alive nodes only."
HADOOP-1220,block not found in blockMap,"From Hadoop-973  debug message, we had datanode constantly printing out the following message.

...
2007-04-02 23:59:50,122 WARN org.apache.hadoop.dfs.DataNode: java.io.IOException: Unexpected error trying to delete block blk_-3400783150525166031. Block not found in blockMap.
  at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:596)
  at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:460)
  at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1053)
  at java.lang.Thread.run(Thread.java:619)

2007-04-02 23:59:50,433 INFO org.apache.hadoop.dfs.DataNode: Served block blk_-8672111663356339464 to /72.30.127.164
2007-04-02 23:59:52,993 WARN org.apache.hadoop.dfs.DataNode: java.io.IOException: Unexpected error trying to delete block blk_-3400783150525166031. Block not found in blockMap.
  at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:596)
  at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:460)
  at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1053)
  at java.lang.Thread.run(Thread.java:619)
....

 Ten minutes before these logs, there was 
2007-04-02 20:42:49,648 INFO org.apache.hadoop.dfs.DataNode: Deleting block blk_-3400783150525166031

There was a  file for that block in the directory.
/../../hadoop/dfs/data/data/subdir17/blk_-3400783150525166031

"
HADOOP-1219,Spurious progress messages should be discarded after a task is done,"Noticed that the Map task's ""last"" progress message sometimes goes through even after the fix for HADOOP-1191. In reality, this *could* happen since we don't check for thread.isInterrupted() at each step in the path of the client RPC call. So the situation described in HADOOP-1191 remains. The effect of this is that the TaskTracker finally kills the task for not sending progress report for the configured number of seconds (if the process happens to be alive then)."
HADOOP-1218,In TaskTracker the access to RunningJob object is not synchronized in one place,"In the method addTaskToJob, the access to RunningJob is not synchronized (leading to potential race conditions)."
HADOOP-1217,Specify a junit test timeout in build.xml files,"To enable a more stable patch process, I'd like to be able to timeout a junit test when it takes too long to complete."
HADOOP-1216,Hadoop should support reduce none option,"
This has been a highly desired feature in streaming world and was asked occationally in the non-streaming side.
Streaming implemented a working (hacking) solution. But it also generates discrepency between hadoop 
streaming/non-streaming model. It would be nice if Hadoop offers such a feature 
that works both streaming and non-streaming. Owen and I discussed this a bit and here is the 
general idea for further discussions/suggestions:

1. Allows the user to specify reducer=none in jobconf. 
2. The user still can specify output format and output directory
3. Each mapper will generate an output file in the specified directory. The naming convention can still be like part-xxxxxxxx
where xxxxxxxx is the map task number.
4. The mapoutput collector of a mapper task will be a record writer on the 
5. The mapper will call output.collect() to write the output, thus the same mapper class can be 
used, regardless reducer none is set or not.

When reducer is set to none for a job, there will be no mapoutput files writen on to local file system at all, 
and no data shuffling between mappers and reducers. As a mapper of fact, the framework may choose 
not to create reducers at all.
 "
HADOOP-1215,Streaming should allow to specify a partitioner,
HADOOP-1214,the first step for streaming clean up,"
This is the first step for streaming clean up.

This step will mainly replace various streaming classes related inputformat/output format, record readers, etc. with hadoop's counterparts.

This step will maintain backward compatibility

"
HADOOP-1213,When RPC call fails then log call message detail,"When an RPC call fails, it would be helpful to have more information about the call.  Currently, this is all I get.

    [junit] 2007-04-05 06:19:19,518 WARN  ipc.Server (Server.java:run(594)) - handler output error
    [junit] java.nio.channels.ClosedChannelException
    [junit] 	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:125)
    [junit] 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:294)
    [junit] 	at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)
    [junit] 	at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)
    [junit] 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
    [junit] 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
    [junit] 	at java.io.DataOutputStream.flush(DataOutputStream.java:106)
    [junit] 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:592)"
HADOOP-1211,Remove deprecated constructor and unused static members in DataNode class,"HADOOP-702 deprecated one DataNode constructor, which is only used in MiniDFSCluster. It should be removed.
Static member
dataNodeList
dataNodeThreadList
seem to be redundant now. Removing them will also require to remove static method
shutdownAll()
It is public now, but is also used only by MiniDFSCluster.
The alternative to static shutdownAll() is to call shutdown() on the DataNode instance."
HADOOP-1210,Log counters in job history,"It would be useful if the value of the global counters were logged to the job history, perhaps even individually for each task after completion."
HADOOP-1209,Record I/O should support comments in the DDL,The Record I/O record description language should support comments.
HADOOP-1207,hdfs -rm should NOT fail if one of the files to remove is missing,"It looks like hdfs -rm with wild card argument first builds the list of
files to delete, and then deletes them one by one.
(This is OK so far).

If one of the files on the list gets deleted by someone else before the
command reaches it, the command fails.
This seems a wrong behavior to me. I'd expect the command report the problem
and continue deleting the rest of the files.

"
HADOOP-1206,UpgradeUtilities doesn't roll ports properly,UpgradeUtilities doesn't properly set NameNode and DataNode ports to 0
HADOOP-1205,The open method of FSNamesystem should be synchronized,"Method open accesses a shared data structure blockMap. If synchronization access is not enforced, open might return inconsistent results."
HADOOP-1204,Re-factor InputFormat/RecordReader related classes,"
This Jira is the first small step to unify the code related to the inputformat/record readers for streaming 
with the Hadoop main framework.

This Jira does a few things to clean up the related parts in the Hadoop main framework.

1. Add a constructor 
       public LineRecordReader(Configuration job, FileSplit split)
to LineRecordReader. This makes the constructors of both SequenceFileRecordReader and LineRecordReader
have the same signature. This facilitates to have a factory class to create various record readers when 
we bring in the class readers classes for hadoop streaming to the main framework.

2. Implementded next() method using the following newly added protected method to LineRecordReader class:

     protected long readLine() throws IOException {
         return LineRecordReader.readLine(in, buffer);
     }

    This allows the user to easily overwrite the readLine logic to use different line breaker (e.g. treat '\r' as part of data, not line breaker).

3. Rename class InputFormatBase to FileInputFormat to better reflect the functionality of the class.
To keep backward compatible, still keep InputFormatBase class, but make it deprecated shallow class simply inheriting FileInputFormat .

4. Change TextInputFormat and SequenceFileFormat to extend FileInputFormat.
"
HADOOP-1203,UpgradeUtilities should use MiniDFSCluster to start and stop NameNode/DataNodes,UpgradeUtilities used by DFS tests should use MiniDFSCluster to start and stop NameNode/DataNodes.
HADOOP-1201,Progress reporting can be improved for both Map/Reduce tasks,"Both the map and reduce tasks do progress reporting in separate threads. However, in the ReduceTask, after the sort phase, the progress reporting happens inline with the reducer invocations. This slows down the Reduce phase since RPC is involved for every progress report. The better thing to do would be to do progress reporting for all phases in separate threads and have the tasks just update the progress fields.
One proposal is to extract out the reporting stuff that is there in MapTask/ReduceTask and put it in the Task superclass as a new class, and have methods in the new class that control what/when progress is reported. Thoughts?"
HADOOP-1200,Datanode should periodically do a disk check,HADOOP-1170 removed the disk checking feature. But this is a needed feature for maintaining a large cluster. I agree that checking the disk on every I/O is too costly. A nicer approach is to have a thread that periodically do a disk check. It then automatically decommissions itself when any error occurs.
HADOOP-1198,ipc.client.timeout of 2000ms for test cases seems too small; causes too many timeouts and leads to hung test cases,We should increase the timeout slightly... what do other think? 5000ms? 10000ms?
HADOOP-1197,"The misleading Configuration.set(String, Object) should be removed","I think that the confusing methods in Configuration:
  set(String, Object)
  getObject(String)
  get(String, Object)
should be deprecated. Users expect them to work and in a distributed environment, there is almost no way to make them work correctly. If some user really needs the functionality, we should implement it right and require that the objects be Writable and serialize them into the Configuration.

Thoughts?"
HADOOP-1196,StreamXmlRecordReader throws java.io.IOException: Resetting to invalid mark,"I encounter this error consistently whenever I have over 500 maps in my job.  I read XML feeds with about 200,000 documents in each, parse and index the document. I get the following exception.

java.io.IOException: Resetting to invalid mark
	at java.io.BufferedInputStream.reset(BufferedInputStream.java:416)
	at java.io.FilterInputStream.reset(FilterInputStream.java:200)
	at org.apache.hadoop.streaming.StreamXmlRecordReader.fastReadUntilMatch(StreamXmlRecordReader.java:291)
	at org.apache.hadoop.streaming.StreamXmlRecordReader.readUntilMatchBegin(StreamXmlRecordReader.java:120)
	at org.apache.hadoop.streaming.StreamXmlRecordReader.seekNextRecordBoundary(StreamXmlRecordReader.java:113)
	at org.apache.hadoop.streaming.StreamXmlRecordReader.init(StreamXmlRecordReader.java:75)
	at org.apache.hadoop.streaming.StreamXmlRecordReader.(StreamXmlRecordReader.java:65)
	at com.gale.searchng.workflow.fetcher.DocFetcherParser$XmlInputFormat.getRecordReader(DocFetcherParser.java:231)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:139)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)
"
HADOOP-1195,NullPointerException in FSNamesystem due to getDatanode() return value is not checked,"FSNamesystem.getDatanode( nodeID ) returns null if the node is not found.
There are several places where we do not check wether the returned node is null, e.g.
FSNamesystem.processReport()
FSNamesystem.setDatanodeDead()
FSNamesystem.invalidateBlock()
FSNamesystem.verifyNodeRegistration()

I'm getting the following NPE:

07/04/02 17:31:50 WARN dfs.DataNode: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException
    at org.apache.hadoop.dfs.FSNamesystem.shouldNodeShutdown(FSNamesystem.java:3306)
    at org.apache.hadoop.dfs.FSNamesystem.processReport(FSNamesystem.java:2012)
    at org.apache.hadoop.dfs.NameNode.blockReport(NameNode.java:654)
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:585)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:580)

    at org.apache.hadoop.ipc.Client.call(Client.java:473)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:164)
    at org.apache.hadoop.dfs.$Proxy1.blockReport(Unknown Source)
    at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:460)
    at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1095)
    at org.apache.hadoop.dfs.MiniDFSCluster$DataNodeRunner.run(MiniDFSCluster.java:160)
    at java.lang.Thread.run(Thread.java:595)
"
HADOOP-1194,map output should not do block level compression,"
If the user sets to compress the map output, the compression style should be record level, not block level, since using block level compression for map outputs causes performance degragation significantly.
"
HADOOP-1193,Map/reduce job gets OutOfMemoryException when set map out to be compressed,One of my jobs quickly fails with the OutOfMemoryException when I set the map out to be compressed. But it worked fine with release 0.10.
HADOOP-1192,Du command takes a noticable longer time to execute on a large dfs than the 0.11 release,"After installing 0.12.2, we notice that the du command takes a much longer time to execute than the previous release. Another problem is that dus prints a negative value."
HADOOP-1191,MapTask should wait for the status reporting thread to die before invoking the TaskUmbilicalProtocol.done(taskid),"Currently, the status reporting thread is sent an interrupt and immediately after that TaskUmbilicalProtocol.done() is invoked. A better thing to do is to wait for the thread to die before invoking done() otherwise it is possible that a status message just makes it through and then the Task is put in RUNNING state at the TaskTracker. This results in inconsistency about the runstate of a task at the TaskTracker's end."
HADOOP-1190,Fix unchecked warnings,Fix all the unchecked warnings that occur when compiling Hadoop.
HADOOP-1189,Still seeing some unexpected 'No space left on device' exceptions,"
One of the datanodes has one full partition (disk) out of four. Expected behaviour is that datanode should skip this partition and use only the other three. HADOOP-990 fixed some bugs related to this. It seems to work ok but some exceptions are still seeping through. In one case there 33 of these out 1200+ blocks written to this node. Not sure what caused this. I will submit a patch to the prints a more useful message throw the original exception.

Two unlikely reasons I can think of are 2% reserve space (8GB in this case) is not enough or client some how still says block size is zero in some cases. Better error message should help here.

If you see small number of these exceptions compared to number of blocks written, for now you don't need change anything.
"
HADOOP-1188,processIOError() should update fstime file,"Name-node can have multiple directories to store the name space image and edits.
During startup the name-node selects the latest image and reads it in memory.
fstime stores the time of the latest checkpoint.
If one of the directories becomes inaccessible during normal operation the name-node
excludes it from the list, and never writes anything into it until restarted.
Now if the the name-node restarts after excluding one of the directories, and if that
excluded directory contained the latest image, then all modifications of the name space
recorded in edits files in other directories starting from the moment the directory was
excluded will be lost.
We should update fstime in all remaining good directories after the exclusion."
HADOOP-1187,DFS Scalability: avoid scanning entire list of datanodes in getAdditionalBlocks,"A new block allocations for a file scans the list of all known datanodes to find if the client that is a also a cluster node. If so, then it tries to allocate a replica locally. This check consumes plenty of CPU, especially if the number of datanodes in a cluster is large.

An optimization: if the client is also a cluster node, then cache a reference to the corresponding DatanodeDescriptor from the entry in pendingCreate. The method getAdditionalBlock() uses the cached DatanodeDescriptor and thus avoids scanning the entire list of datanodes."
HADOOP-1186,deadlock in Abstract Metrics Context,"There appears to be a lock-inversion deadlock in AbstractMetricsContext.

When using ganglia metrics, sometimes the jobtracker will start timing out requests.  The logs then reveal:

2007-03-30 13:59:50,942 WARN org.apache.hadoop.ipc.Server: Call queue overflow discarding oldest call heartbeat(org.apache.hadoop.mapred.Task
TrackerStatus@1c19919, false, true, 407) from 10.255.62.129:50215

A kill -QUIT dump shows:

""IPC Server handler 6 on 10001"" daemon prio=1 tid=0x08515c08 nid=0x526a waiting for monitor entry [0x4e6f4000..0x4e6f4f40]
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.createRecord(AbstractMetricsContext.java:192)
        - waiting to lock <0x5a562c98> (a org.apache.hadoop.metrics.ganglia.GangliaContext)
        at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:130)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1384)
        - locked <0x5a446330> (a org.apache.hadoop.mapred.JobTracker)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)
...
""Timer-0"" prio=1 tid=0x08664040 nid=0x5274 waiting for monitor entry [0x4e36d000..0x4e36df40]
        at org.apache.hadoop.mapred.JobTracker.getRunningJobs(JobTracker.java:944)
        - waiting to lock <0x5a446330> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobTracker$JobTrackerMetrics.doUpdates(JobTracker.java:429)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:275)
        - locked <0x5a562c98> (a org.apache.hadoop.metrics.ganglia.GangliaContext)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:48)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:242)
        at java.util.TimerThread.mainLoop(Unknown Source)
        at java.util.TimerThread.run(Unknown Source)
"
HADOOP-1185,dynamically change log levels,"I would like to switch on the debug log level of the namenode (or other components) without restarting it. This is needed to analyze a production system.

Can somebody please advice on how to set the loglevel dyncamically on a running namenode? I was thinking of enhancing dfsadmin to make an RPC to the namenode to set a specified logging level. But the apache common logging APi does not export an API to change logging levels."
HADOOP-1184,Decommission fails if a block that needs replication has only one replica,"If the only replica of a block resides on a node being decommissioned, then the decommission command does not complete. The blocks do not get added to neededReplication because neededReplications.update() believes that the number of current replicas is zero.
"
HADOOP-1183,MapTask completion not recorded properly at the Reducer's end,"A couple of reducers were continuously trying to fetch map outputs from a lost tasktracker. Although the tasks running on that lost TT successfully reexecuted elsewhere, the Reducers' tasktrackers didn't correctly note those events."
HADOOP-1182,DFS Scalability issue with filecache in large clusters,"When using filecache to distribute supporting files for map/reduce applications in a 1000 node cluster, many map tasks fail  because of timeouts. There was no such problem using a 200 node cluster for the same applications with comparable input data. Either the whole job fails because of too many map failures, or even worse, some map tasks hang indefinitely.


java.net.SocketTimeoutException: timed out waiting for rpc response
	at org.apache.hadoop.ipc.Client.call(Client.java:473)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:163)
	at org.apache.hadoop.dfs.$Proxy1.exists(Unknown Source)
	at org.apache.hadoop.dfs.DFSClient.exists(DFSClient.java:320)
	at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.exists(DistributedFileSystem.java:170)
	at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.open(DistributedFileSystem.java:125)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.(ChecksumFileSystem.java:110)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:330)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:245)
	at org.apache.hadoop.filecache.DistributedCache.createMD5(DistributedCache.java:327)
	at org.apache.hadoop.filecache.DistributedCache.ifExistsAndFresh(DistributedCache.java:253)
	at org.apache.hadoop.filecache.DistributedCache.localizeCache(DistributedCache.java:169)
	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:86)
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:117)"
HADOOP-1181,userlogs reader,"My jobs output lots of logging.  I want to be able to quickly parse the logs across the cluster for anomalies.  org.apache.hadoop.tool.Logalyzer looks promising at first but it does not know how to deal with the userlog format  and it wants to first copy all logs local.  Digging, there does not seem to currently be a reader for hadoop userlog format.  TaskLog$Reader is not generally accessible and it too expects logs to be on the local filesystem (The latter is of little good if I want to run the analysis as a mapreduce job)."
HADOOP-1180,NNbench test should be able to test the checksumfilesystem as well as the raw filesystem,The NNbench test should have the option of testing a file system with checksums turned on and with checksums turned off. The original behaviour of nnbench test was to test hdfs without checksums.
HADOOP-1179,task Tracker should be restarted if its jetty http server cannot serve get-map-output files,"
Due to some errors (mem leak?), the jetty http server throws outOfMemory exception when serving get-map-output requests:

2007-03-28 20:42:39,608 WARN org.mortbay.jetty.servlet.ServletHandler: Error for
 /mapOutput?map=task_0334_m_013127_0&reduce=591
2007-03-28 20:46:42,788 WARN org.mortbay.jetty.servlet.ServletHandler: Error for
 /mapOutput?map=task_0334_m_013127_0&reduce=591
2007-03-28 20:49:38,064 WARN org.mortbay.jetty.servlet.ServletHandler: Error for
java.lang.OutOfMemoryError
        at java.io.FileInputStream.readBytes(Native Method)
        at java.io.FileInputStream.read(FileInputStream.java:199)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(R
awLocalFileSystem.java:119)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInput
Stream.java:41)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at java.io.DataInputStream.read(DataInputStream.java:132)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(Che
cksumFileSystem.java:182)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumF
ileSystem.java:167)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInput
Stream.java:41)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:399)
        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTrack
er.java:1643)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427
)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicati
onHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:5
67)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplication
Context.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:
244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

In this case, the task tracker cannot send out the map outut files on that machine, rendering it useless.
Moreover, all the reduces depending on those map output files are just stuck there.
If the task tracker reports fail to the job tracker, the map/reduce job can recover.
If the task tracker restarted, it can continue to join the cluster as a new mamber.

"
HADOOP-1178,NullPointer Exception in org.apache.hadoop.dfs.NameNode.isDir on namenode restart,"On a namenode restart, I sometimes get the following exception. The problem is that the RPC server is initialized before the namenode data structures are initialized. This means that an RPC starts getting processed by the namenode before its data structures are consistent. The fix is to first initialize the namesystem and then start the RPC server.


NFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 50000 call error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.NameNode.isDir(NameNode.java:438)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:339)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:564)
"
HADOOP-1177,Lack of logging of exceptions in MapOutputLocation.getFile,The current implementation of MapOutputLocation.getFile effectively catches all IOExceptions and returns null. This hides what is going wrong and makes it very hard to diagnose.
HADOOP-1176,Reduce hang on huge map output,"I ran a job with one map and one reduce. The map succeeded, producing 3G output. The reducer hang on fetching the map output. The log shows the following error message:

WARN org.apache.hadoop.mapred.TaskTracker: getMapOutput(task_0401_m_000000_0,0) failed :
java.net.SocketException: Connection reset
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at org.mortbay.http.ChunkingOutputStream.bypassWrite(ChunkingOutputStream.java:151)
        at org.mortbay.http.BufferedOutputStream.write(BufferedOutputStream.java:139)
        at org.mortbay.http.HttpOutputStream.write(HttpOutputStream.java:423)
        at org.mortbay.jetty.servlet.ServletOut.write(ServletOut.java:54)
        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:1664)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

WARN /: /mapOutput?map=task_0401_m_000000_0&reduce=0:
java.lang.IllegalStateException: Committed
        at org.mortbay.jetty.servlet.ServletHttpResponse.resetBuffer(ServletHttpResponse.java:212)
        at org.mortbay.jetty.servlet.ServletHttpResponse.sendError(ServletHttpResponse.java:375)
        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:1694)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-1175,the user output/logs are broken from the web ui,The links from the web/ui for getting the task outputs don't work any more. The data is available by using the backdoors and accessing the data directly.
HADOOP-1172,Reduce job failed due to error in logging,"Here is the stack trace:

java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
	at org.apache.hadoop.mapred.TaskLog$Writer.writeIndexRecord(TaskLog.java:251)
	at org.apache.hadoop.mapred.TaskLog$Writer.close(TaskLog.java:235)
	at org.apache.hadoop.mapred.TaskRunner.runChild(TaskRunner.java:406)
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:281)

Fail to log should not fail the task. Especially when closing the logwriter. At that time, the mapper was actually complete."
HADOOP-1170,Very high CPU usage on data nodes because of FSDataset.checkDataDir() on every connect,"While investigating performance issues in our Hadoop DFS/MapReduce cluster I saw very high CPU usage by DataNode processes.

Stack trace showed following on most of the data nodes:
""org.apache.hadoop.dfs.DataNode$DataXceiveServer@528acf6e"" daemon prio=1 tid=0x00002aaacb5b7bd0 nid=0x5940 runnable [0x000000004166a000..0x000000004166ac00]
        at java.io.UnixFileSystem.checkAccess(Native Method)
        at java.io.File.canRead(File.java:660)
        at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:34)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:164)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:168)
        at org.apache.hadoop.dfs.FSDataset$FSVolume.checkDirs(FSDataset.java:258)
        at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.checkDirs(FSDataset.java:339)
        - locked <0x00002aaab6fb8960> (a org.apache.hadoop.dfs.FSDataset$FSVolumeSet)
        at org.apache.hadoop.dfs.FSDataset.checkDataDir(FSDataset.java:544)
        at org.apache.hadoop.dfs.DataNode$DataXceiveServer.run(DataNode.java:535)
        at java.lang.Thread.run(Thread.java:595)

I understand that it would take a while to check the entire data directory - as we have some 180,000 blocks/files in there. But what really bothers me that from the code I see that this check is executed for every client connection to the DataNode - which also means for every task executed in the cluster. Once I commented out the check and restarted datanodes - the performance went up and CPU usage went down to reasonable level.
"
HADOOP-1169,CopyFiles skips src files of s3 urls,"When given a source file of items to copy, the CopyFiles utility tries each of its supported schemes in turn looking for matching entries in the passed file.  Of the 4 schemes -- file, hdfs, http, and s3 -- we never make it to the s3 test.  We skip out early.  Also, even if we did make it to the search-for-s3-URLs code, the list of s3 srcPaths would always be empty because of a copy/paste error."
HADOOP-1167,InMemoryFileSystem uses synchronizedtMaps with maps that are locked anyways,The InMemoryFileSystem uses synchronizedMaps and then guards each access to them by locking the FileSystem. There is also insufficient synchronization in create.
HADOOP-1166,Pull the NullOutputFormat into the lib package,It is convenient to have an OutputFormat that just ignores any outputs.
HADOOP-1165,Code for toString in code generated by Record I/O Compiler can be generic,The generated toString method for every record is identical (it calls csv serialization on the record.) It can be moved to the Record class from which every generated record inherits.
HADOOP-1164,TestReplicationPolicy doesn't use port 0 for the NameNode,"The TestReplicationPolicy test doesn't use mini-dfs and specifies a fixed port, so it can crash with address in use:

java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
        at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:184)
        at org.apache.hadoop.ipc.Server.<init>(Server.java:622)
        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:323)
        at org.apache.hadoop.ipc.RPC.getServer(RPC.java:293)
        at org.apache.hadoop.dfs.NameNode.init(NameNode.java:181)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:207)
        at org.apache.hadoop.dfs.TestReplicationPolicy.<clinit>(TestReplicationPolicy.java:36)
"
HADOOP-1163,Ganglia metrics reporting is misconfigured,"In hadoop-metrics.properties, I set mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext.

If I then get the gmond xml feed from the gmond server, I get this:

<METRIC NAME=""load_one"" VAL=""1.04"" TYPE=""float"" UNITS="""" TN=""28"" TMAX=""70"" DMAX=""0"" SLOPE=""both"" SOURCE=""gmond""/>
...
<METRIC NAME=""datanode.myhostname.bytes_read"" VAL=""657927"" TYPE=""int32"" UNITS="""" TN=""5696"" TMAX=""60"" DMAX=""0"" SLOPE=""both"" SOURCE=""gmetric""/>

Because the bytes_read metric has the datanode.hostname prefix, it will not aggregate with metrics from other hosts properly.
"
HADOOP-1162,Record IO: seariliizing a byte buffer to CSV fails if buffer contains bytes less than 16.,The unit test only tests that an empty byte buffer can be serialized and deserialized.  If you put in some bytes in the range 0..15 the test will fail.
HADOOP-1161,need improved release process,"Hadoop's release process needs improvement.  We should better ensure that releases are stable, not releasing versions that have not been proven stable on large clusters, and we should better observe Apache's release procedures.  Once agreed on, this process should be documented in http://wiki.apache.org/lucene-hadoop/HowToRelease.

Here's a proposal:

. candidate release builds should be placed in lucene.apache.org/hadoop/dev/dist
. candidate artifacts should be accompanied by a md5 and pgp signatures
. a 72-hour vote for the release artifact should be called on hadoop-dev.
. 3 binding +1 votes and a majority are required
. if the vote passes, the release can then posted to www.apache.org/dist/lucene/hadoop for mirroring

This would bring us into accord with Apache's requirements, and better permit large-cluster validation.

We should also build consensus for a release before we commence this process.  Perhaps we should aim for releases every two months instead of every month.  We should perhaps develop more elaborate branching and merging conventions around releases.  Currently we mostly lock-out changes intended for release X+1 from trunk until release X is complete, which can be awkward.  How can we better manage that?


"
HADOOP-1160,DistributedFileSystem doesn't close the RawDistributedFileSystem on close.,"The DistributedFileSystem doesn't have a close method, and thus doesn't close the RawDistributedFileSystem. This will cause files that are being written to not be aborted correctly."
HADOOP-1159,Reducers hang when map output file has a checksum error,"Two reduces hung in our sort benchmark. They always fail to get map outputs from node X due to checksum error when the map outputs are read at that node resulting in a NullPointerException on node X. This leads to constant failures on the two fetching reduces.

2007-03-26 00:02:57,082 WARN org.apache.hadoop.fs.FileSystem: Moving bad file /e/c/k/hqa/tb/tmp/mapred/local2/task_0002_m_022488_0/file.out to /e/c/bad_files/file.out.542279301
2007-03-26 00:02:57,083 INFO org.apache.hadoop.fs.FSInputChecker: Found checksum error: org.apache.hadoop.fs.ChecksumException: Checksum error: /e/c/k/hqa/tb/tmp/mapred/local2/task_0002_m_022488_0/file.out at 106484224
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.verifySum(ChecksumFileSystem.java:254)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:211)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:167)
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at java.io.DataInputStream.read(DataInputStream.java:132)
	at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:1659)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

2007-03-26 00:02:57,083 WARN /: /mapOutput?map=task_0002_m_022488_0&reduce=1542: 
java.lang.NullPointerException"
HADOOP-1158,"JobTracker should collect statistics of failed map output fetches, and take decisions to reexecute map tasks and/or restart the (possibly faulty) Jetty server on the TaskTracker","The JobTracker should keep a track (with feedback from Reducers) of how many times a fetch for a particular map output failed. If this exceeds a certain threshold, then that map should be declared as lost, and should be reexecuted elsewhere. Based on the number of such complaints from Reducers, the JobTracker can blacklist the TaskTracker. This will make the framework reliable - it will take care of (faulty) TaskTrackers that sometimes always fail to serve up map outputs (for which exceptions are not properly raised/handled, for e.g., if the exception/problem happens in the Jetty server).
"
HADOOP-1157,a dfs block went missing,"I was told that this incident of a missing block might generate enough interest for somebody to look at the logs (available on request). The block belonged to a 13GB file, 3-way replicated with a block size of 64MB.
I do not know whether this was related to HADOOP-1135 or not."
HADOOP-1156,NullPointerException in MiniDFSCluster,"This was committed with HADOOP-1063
I'm getting a NullPointerException in MiniDFSCluster.DataNodeRunner.run()
        node.shutdown();
because node is null. Should it be just
        shutdown(); "
HADOOP-1154,streaming hang. (PipeMapRed$MROutputThread gone),"One streaming reducer ('cat')  hang, 

 '/bin/cat' reducer waiting in 
    0 S _____  24587 24557  0  75   0 -   631 pipe_w Mar22 ?        00:00:02 /bin/cat

strace showed that it's waiting in write.
$ strace -p 24587
Process 24587 attached - interrupt to quit
write(1, ""________  ""..., 138 <unfinished ...>
Process 24587 detached

When I looked at the jstack of ReduceTask process, I couldn't find the PipeMapRed$MROutputThread.



"
HADOOP-1153,DataNode and FSNamesystem don't shutdown cleanly,The DataNode and FSNamesystem don't interrup their threads when shutting down.  This causes threads to stay around which is a problem if tests are starting and stopping these servers many times in the same process.
HADOOP-1152,Reduce task hang failing in MapOutputCopier.copyOutput,"We had couple of reduce tasks hang repeating the output below.

2007-03-22 23:57:16,296 WARN org.apache.hadoop.mapred.TaskRunner: java.io.IOException: Path /hadoop/mapred/local/task_0026_r_000307_0/map_7854.out already exists
  at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.rename(InMemoryFileSystem.java:246)
  at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:471)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.copyOutput(ReduceTaskRunner.java:336)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.run(ReduceTaskRunner.java:274)

2007-03-22 23:57:16,296 WARN org.apache.hadoop.mapred.TaskRunner: task_0026_r_000307_0 adding host ______  to penalty box, next contact in 192 seconds

===============================
Before the above output, there was 

2007-03-22 18:15:24,274 ERROR org.apache.hadoop.mapred.TaskRunner: Map output copy failure: java.lang.NullPointerException
  at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:416)
  at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getLength(InMemoryFileSystem.java:286)
  at org.apache.hadoop.fs.FilterFileSystem.getLength(FilterFileSystem.java:178)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.copyOutput(ReduceTaskRunner.java:340)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.run(ReduceTaskRunner.java:274)

"
HADOOP-1151,streaming PipeMapRed prints system info to stderr,"In streaming, user task's stderr is filled with following output from the streaming infrastructure.

R/W/S=4316801/4316308/0 in:73166=4316801/59 [rec/s] out:73157=4316308/59 [rec/s]
R/W/S=4316901/4316424/0 in:73167=4316901/59 [rec/s] out:73159=4316425/59 [rec/s]
R/W/S=4317001/4316470/0 in:73169=4317001/59 [rec/s] out:73160=4316470/59 [rec/s]
R/W/S=4317101/4316723/0 in:73171=4317101/59 [rec/s] out:73164=4316724/59 [rec/s]
R/W/S=4317201/4316772/0 in:73172=4317201/59 [rec/s] out:73165=4316773/59 [rec/s]
R/W/S=4317301/4316905/0 in:73174=4317301/59 [rec/s] out:73167=4316905/59 [rec/s]
R/W/S=4317401/4316946/0 in:73176=4317401/59 [rec/s] out:73168=4316946/59 [rec/s]

This can hit the size limit and delete users' actual stderr messages."
HADOOP-1150,Streaming -reducer and -mapper no longer have defaults,
HADOOP-1149,DFS Scalability: high cpu usage in addStoredBlock,I have seen addStoredBlock() consume lots of CPU. One possible cause is that it invokes proccessOverReplicatedBlock. The logic to find and purge over-replicated blocks can be done much less often.
HADOOP-1148,re-indent all code,We should re-indent all code to consistently use 2-spaces per level.  This will not invalidate outstanding patches: one can use the '-l' option to ignore whitespace differences in patches.
HADOOP-1147,remove all @author tags from source,"We should remove @author tags from the source code.  We give contributors credit in at least three places (Jira, subversion and CHANGES.txt).  Many files have been substantially re-written by a range of contributors and their @author tags are no longer accurate.  Also, @author tags imply individual ownership, when we should rather strive for community ownership."
HADOOP-1146,"""Reduce input records"" counter name is misleading","It has been pointed out that the counter name ""reduce input records"" is misleading; this number should be called ""reduce input keys"" or ""reduce input groups"".  It could also be useful to have the actual number of reduce input records, which should be the same as the number of map output records.
"
HADOOP-1145,XmlRecordInput class should be public,
HADOOP-1144,Hadoop should allow a configurable percentage of failed map tasks before declaring a job failed.,"In our environment it can occur that some map tasks will fail repeatedly because of corrupt input data, which sometimes is non-critical as long as the amount is limited. In this case it is annoying that the whole Hadoop job fails and cannot be restarted till the corrupt data are identified and eliminated from the input. It would be extremely helpful if the job configuration would allow to indicate how many map tasks are allowed to fail."
HADOOP-1140,Deadlock bug involving the o.a.h.metrics package,"Hi David,

Our nightly benchmarks are occasionally failing (2 to 4 of them per night) due to this deadlock in the JT that looks to be caused by Simon.  Do you have time to fix this in the morning?

Thanks,
Nige



Found one Java-level deadlock:
=============================
""expireLaunchingTasks"":
  waiting to lock monitor 0x08141b44 (object 0x57eafdd0, a org.apache.hadoop.mapred.JobTracker),
  which is held by ""IPC Server handler 8 on 50020""
""IPC Server handler 8 on 50020"":
  waiting to lock monitor 0x08141630 (object 0x57de46b8, a com.yahoo.simon.hadoop.metrics.SimonContext),
  which is held by ""Timer-0""
""Timer-0"":
  waiting to lock monitor 0x08141b44 (object 0x57eafdd0, a org.apache.hadoop.mapred.JobTracker),
  which is held by ""IPC Server handler 8 on 50020""

Java stack information for the threads listed above:
===================================================
""expireLaunchingTasks"":
        at org.apache.hadoop.mapred.JobTracker$ExpireLaunchingTasks.run(JobTracker.java:152)
        - waiting to lock <0x57eafdd0> (a org.apache.hadoop.mapred.JobTracker)
        at java.lang.Thread.run(Thread.java:619)
""IPC Server handler 8 on 50020"":
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.createRecord(AbstractMetricsContext.java:192)
        - waiting to lock <0x57de46b8> (a com.yahoo.simon.hadoop.metrics.SimonContext)
        at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:130)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1383)
        - locked <0x57eafdd0> (a org.apache.hadoop.mapred.JobTracker)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)
""Timer-0"":
        at org.apache.hadoop.mapred.JobTracker.getRunningJobs(JobTracker.java:943)
        - waiting to lock <0x57eafdd0> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobTracker$JobTrackerMetrics.doUpdates(JobTracker.java:429)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:275)
        - locked <0x57de46b8> (a com.yahoo.simon.hadoop.metrics.SimonContext)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:48)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:242)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)

Found 1 deadlock. "
HADOOP-1139,All block trasitions should be logged at log level INFO,The namenode records block trasitions in its log file. It is seen that some of the block transition messages were being logged at debug level. These should be done at INFO level.
HADOOP-1138,Datanodes that are dead for a long long time should not show up in the UI,"Proposal 1:
If a include files is used, then show all nodes (dead/alive) that are listed in the includes file. If there isn't an include file, then display only nodes that have pinged this instance of the namenode.

Proposal2:
A config variable specifies the time duration. The namenode, on a restart, purges all datanodes that have not pinged for that time duration. The default value of this config variable can be 1 week. "
HADOOP-1137,StatusHttpServer assumes that resources for /static are in files,"StatusHttpServer uses ClassLoader.getResource() to find the webapps, but then assumes it is a file URL and extracts the filename. This requires the webapps resources to be in files even though they can be loaded from the classpath. If the webapps resources are not in files, but packaged in a jar file for example, things will not work.

The fix is extremely simple. The String returned from getWebAppsPath() is passed to Jetty which is then later converted back into a URL before it is used. We just need to return the URL as a string. (Since it is a URL we should not use the File.separator)."
HADOOP-1136,exception in UnderReplicatedBlocks:add when ther are more replicas of a block than required,"I was running a random writer followed by a sort when I saw this condition:

Exception in thread ""org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@187814"" java.lang.ArrayIndexOutOfBoundsException: 3
        at org.apache.hadoop.dfs.FSNamesystem$UnderReplicatedBlocks.add(FSNamesystem.java:447)
        at org.apache.hadoop.dfs.FSNamesystem$UnderReplicatedBlocks.add(FSNamesystem.java:464)
        at org.apache.hadoop.dfs.FSNamesystem.processPendingReplications(FSNamesystem.java:1891)
        at org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor.run(FSNamesystem.java:1795)
        at java.lang.Thread.run(Thread.java:619)

processPendingReplications is invoking neededReplications.add().  The add method then invokes add(block, 3, 4). This add method calls getPriority and that returns 3. Now, the call to priorityQueues[3].add() throws an OutOfBoundsException."
HADOOP-1135,A block report processing may incorrectly cause the namenode to delete blocks ,"When a block report arrives at the namenode, the namenode goes through all the blocks on that datanode. If a block is not valid it is marked for deletion. The blocks-to-be-deleted are sent to the datanode as a response to the next heartbeat RPC. The namenode sends only 100 blocks-to-be-deleted at a time. This was introduced as part of hadoop-994. The bug is that if the number of blocks-to-be-deleted exceeds 100, then that namenode marks all the remaining blocks in the block report for deletion."
HADOOP-1134,Block level CRCs in HDFS,"
Currently CRCs are handled at FileSystem level and are transparent to core HDFS. See recent improvement HADOOP-928 ( that can add checksums to a given filesystem ) regd more about it. Though this served us well there a few disadvantages :

1) This doubles namespace in HDFS ( or other filesystem implementations ). In many cases, it nearly doubles the number of blocks. Taking namenode out of CRCs would nearly double namespace performance both in terms of CPU and memory.

2) Since CRCs are transparent to HDFS, it can not actively detect corrupted blocks. With block level CRCs, Datanode can periodically verify the checksums and report corruptions to namnode such that name replicas can be created.

We propose to have CRCs maintained for all HDFS data in much the same way as in GFS. I will update the jira with detailed requirements and design. This will include same guarantees provided by current implementation and will include a upgrade of current data.



 "
HADOOP-1133,Tools to analyze and debug namenode on a production cluster,"I would like to have a tool that dumps namenode data structures to a log file. This is especially useful in debugging a production cluster where attaching a debugger to the namenode is not a feasible option.

"
HADOOP-1132,Job submission RPC more likely to timeout and fail,"For the past week, Small Jobs Benchmark has been failing sporadically when run on small, medium and large size clusters.  I run it with 30 iterations.  It usually fails somewhere between the 12th and 20th iteration with a timeout trying to submit the job.  Perhaps the JT is busier now doing counter related work which causes this to timeout.  Here's the client side exception:

07/03/19 13:57:41 INFO mapred.MRBench: Running job 13: input=/MRBench/mr_output/output_-1811371677
07/03/19 13:57:41 INFO mapred.InputFormatBase: Total input paths to process : 1
java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:473)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:163)
        at $Proxy1.submitJob(Unknown Source)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:376)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:543)
        at org.apache.hadoop.mapred.MRBench.runJobInSequence(MRBench.java:188)
        at org.apache.hadoop.mapred.MRBench.main(MRBench.java:280)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:143)
        at org.apache.hadoop.test.AllTestDriver.main(AllTestDriver.java:64)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
"
HADOOP-1131,Add a closeAll() static method to FileSystem,"The closeAll method should close all ""cached"" FileSystem (FileSystems present in the CACHE variable).

This can be very usefull when an access to the HDFS is needed in a webapp : when the web application context is unloaded from the container, the application should only call the closeAll method to stop DFSClients (and especially the LeaseChecker thread )."
HADOOP-1130,Remove unused ClientFinalizer in DFSClient,"The ClientFinalizer shutdown hook is not used.

This can lead to severe memory leaks if you use the DFSClient in a dynamic class loading context (such as in a webapp) as the DFSClient.ClientFinalizer class is retained in the memory by the shutdown hook. It retains also the current ClassLoader and thus all classes loaded by the ClassLoader. (Threads put in the shutdown hook are never garbage collected)."
HADOOP-1129,The DFSClient hides IOExceptions in flush,"The DFS client uses a couple of classes that are extensions of FilterOutputStream that should override close, because the default close catches and ignores IOExceptions from flush."
HADOOP-1128,Missing progress information in map tasks,"Long-running map tasks don't update properly their progress - the propgress percentage stays at 0% only to jump suddenly at the end of the task to 100%. The reason, discovered by Espen Amble Kolstad, is that there's a missing cast to float in SequenceFileRecordReader and in LineRecordReader."
HADOOP-1127,Speculative Execution and output of Reduce tasks,"We've recently seen instances where jobs run with 'speculative execution' tend to be quite unstable and fail with *AlreadyBeingCreatedException* noticed at the NameNode. Also potentially we could have hairy situations where a failed Reduce tasks's output could clash with a successful task's (same tip) output.

As it exists, speculative execution relies on the PhasedFileSystem which creates a temp output file and then on task-completion that file is 'moved' to its final position via a call to PhasedFileSystem.commit from ReduceTask.run(). This has lead to issues such as the above.

Proposal:

Basically the idea is to due this uniformly for all Reduce tasks i.e. all reducers create temp files and then have a serialized 'commit' done by the JobTracker which moves the temp file to it's final position. 

We create the temp file in the job's output directory itself:
<output_dir>/_<taskid> (emphasis on the leading '_')

On task completion we'll add that temp file's path to the TaskStatus and then the JobTracker moves that file to it's final position.

Thoughts?"
HADOOP-1126,Optimize CPU usage when cluster restarts,"When the namenode restarts, all datanodes report their blocks to the namenode. The namenode inserts these blocks into neededReplication(). When safemode exists, pendingTransfers removes them from neededReplication. This is non-optimal because pendingTransfer consumes plenty of CPU just after exiting safe-mode.

The problem is that neededReplications.remove(block) does not remove a block if oldExpectedReplicas is exactly equal to oldReplicas."
HADOOP-1124,ChecksumFileSystem does not handle ChecksumError correctly,"When handle ChecksumError, the checksumed file system tries to recover by rereading from a different replica.

I have three comments:
1. One bug in the code is that when retrying, the object that computes checksum does not get restored to the old state.
2. The code also assumes that the first byte read and the byte being read when ChecksumError occurs are in the same block. 
3. It would be more efficient if we roll back to the first byte in the chunk that's being checksumed instead of rolling back to the first byte that was read."
HADOOP-1123,LocalFileSystem gets a NullPointerException when tries to recover from ChecksumError,"NullPointerException occurs when run a large sort
java.lang.NullPointerException
	at org.apache.hadoop.fs.FSDataInputStream$Buffer.seek(FSDataInputStream.java:74)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:121)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:221)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:167)
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:93)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at org.apache.hadoop.io.SequenceFile$Reader.nextRawKey(SequenceFile.java:1616)
	at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:2567)
	at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2353)
	at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:180)
	at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:149)
	at org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:41)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:313)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)

"
HADOOP-1122,Divide-by-zero exception in chooseTarget,"
2007-03-15 20:18:02,542 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020 call error: java.io.IOException: java.lang.ArithmeticException: / by zero
java.io.IOException: java.lang.ArithmeticException: / by zero
    at org.apache.hadoop.dfs.FSNamesystem$ReplicationTargetChooser.chooseTarget(FSNamesystem.java:2809)
    at org.apache.hadoop.dfs.FSNamesystem$ReplicationTargetChooser.chooseTarget(FSNamesystem.java:2773)
    at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:724)
    at org.apache.hadoop.dfs.NameNode.create(NameNode.java:283)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:585)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)
"
HADOOP-1121,Recovering running/scheduled jobs after JobTracker failure,"Currently all running/scheduled jobs are kept in memory in the JobTracker. If the JobTracker goes down all the running/scheduled jobs have to be resubmitted.

Proposal:

(1) On job submission the JobTracker would save the job configuration (job.xml) in a jobs DFS directory using the jobID as name.
(2) On job completion (success, failure, klll) it would delete the job configuration from the jobs DFS directory.
(3) On JobTracker failure the jobs DFS directory will have all running/scheduled jobs at failure time.
(4) On startup the JobTracker would check the jobs DFS directory for job config files. if there is none it means no failure happened on last stop, there is nothing to be done. If there are job config files in the jobs DFS directory continue with the following recovery steps.

(A) rename all job config files to $JOB_CONFIG_FILE.recover.
(B) for each $JOB_CONFIG_FILE.recover: delete the output directory if it exists, schedule the job using the original job ID, delete the $JOB_CONFIG_FILE.recover (as a new $JOB_CONFIG_FILE will be there per scheduling (per step #1).
(C) when B is completed start accepting new job submissions.

Other details:

A configuration flag would enable/disable the above behavior, if switched off (default behavior) nothing of the above happens.
A startup flag could switch off job recovery for systems with the recover set to ON.
Changes to the job ID generation should be put in place to avoid Job ID collision with jobs IDs from previous failed runs, for example appending a JT startup timestamp to the job IDs would do.

Further improvements on top of this one:

This mechanism would allow having a JobTracker node in standby to be started in case of main JobTracker failure. The standby JobTracker would be started on main JobTracker failure. Making things a little more comprehensive they backup JobTrackers could be running in warm mode and hearbeats and ping calls among them would activate a warm stand by JobTracker as new main JobTracker. Together with an enhancement in the JobClient (keeping a list of backup JobTracker URLs) would enable client fallback to backup JobTrackers.

State about partially run jobs could be kept, tasks completed/in-progress/pending. This would enable to recover jobs half way instead restarting them. 
"
HADOOP-1120,Contribute some code helping implement map/reduce apps for joining data from multiple sources,"
With the current Hadoop, it is a bit hard for the user to implement data joining apps. 
HADOOP-475/485 attempt to provide some support for data joining jobs, but it seems to be had to implement.

This Jira rather calls for a application level support. 
The idea is to provide a generic map/reduce classes implementing data join jobs, 
and allows the user to extend those classes to add their special logic. 

In particular, the user needs to define a mapper class 
that extends DataJoinMapperBase class  to implement methods for the
following functionalities:

1. Compute the source tag of input values 
2. Compute the map output value object 
3. Compute the map output key object
 
The source tag will be used by the reducer to determine from which source
(which table in SQL terminology) a value comes. Computing the map output
value object amounts to performing projecting/filtering work in a SQL
statement (through the select/where clauses). Computing the map output key
amounts to choosing the join key. This class provides the appropriate plugin
points for the user defined subclasses to implement the appropriate logic.

The the user needs to define a reducer class 
that extends DataJoinReduceBase class  to implement the following:

    protected abstract TaggedMapOutput combine(Object[] tags, Object[] values);
 
The above method is expected to produce one output value from an array of
records of different sources. The user code can also perform filtering here.
It can return null if it decides to the records do not meet certain conditions.

That is pretty much the user need to do in order to create a map/reduce job to join data 
from different sources.


"
HADOOP-1119,DataNode receives an unknown opcode,"I'm seeing 3 instance of *each* of these exceptions across various logs when running RandomWriter.  The times seem to be well correlated.

JT Log:
2007-03-14 06:23:29,312 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0001_m_000051_0: java.io.FileNotFoundException: /e/c/k/hqa/dfs/data500/tmp/client-8331829540309757259 (No such file or directory)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:106)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1284)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1236)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.write(DFSClient.java:1218)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.write(ChecksumFileSystem.java:391)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:771)
        at org.apache.hadoop.examples.RandomWriter$Map.map(RandomWriter.java:164)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)

DN Log:
2007-03-14 06:23:26,166 ERROR org.apache.hadoop.dfs.DataNode: DataXCeiver
java.io.IOException: Unknown opcode for incoming data stream
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:605)

NN Log:
2007-03-14 06:23:45,935 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.startFile: failed to create file /u/hqa/sort/input/part-51 for DFSClient_task_0001_m_000051_1 on client 72.3.5.5 because pendingCreates is non-null.
2007-03-14 06:23:45,938 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020 call error: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /u/hqa/sort/input/part-51 for DFSClient_task_0001_m_000051_1 on client 72.3.5.5 because pendingCreates is non-null.
org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /u/hqa/sort/input/part-51 for DFSClient_task_0001_m_000051_1 on client 72.3.5.5 because pendingCreates is non-null.
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:701)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:283)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)
"
HADOOP-1118,NullPointerException in DistributedFileSystem$RawDistributedFileSystem.reportChecksumFailure,"I saw one NullPointerException in the JT log during a large sort run:

2007-03-14 08:36:10,210 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0002_m_037315_1: java.lang.NullPointerException
        at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.reportChecksumFailure(DistributedFileSystem.java:326)
        at org.apache.hadoop.dfs.DistributedFileSystem.reportChecksumFailure(DistributedFileSystem.java:405)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.verifySum(ChecksumFileSystem.java:253)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:211)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:167)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readFully(DataInputStream.java:152)
        at org.apache.hadoop.io.SequenceFile$Reader.sync(SequenceFile.java:1712)
        at org.apache.hadoop.mapred.SequenceFileRecordReader.<init>(SequenceFileRecordReader.java:45)
        at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:55)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:139)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)
"
HADOOP-1117,DFS Scalability: When the namenode is restarted it consumes 80% CPU,"When the namenode is restarted, the datanodes register and each block is inserted into neededReplication. When the namenode exists, safemode it sees starts processing neededReplication. It picks up a block from neededReplication, sees that it has already has the required number of replicas, and continues to the next block in neededReplication. The blocks remain in neededReplication permanentlyhe namenode worker thread to scans this huge list of blocks once every 3 seconds. This consumes plenty of CPU on the namenode."
HADOOP-1116,"Add maxmemory=""256m"" in the junit call of build-contrib.xml","When testing some very custom code, TestSymlink failed due to an OutOfMemoryError."
HADOOP-1115,copyToLocal doesn't copy directories,"Rohan reported this and I can also reproduce it:

> I am having problems with -copyToLocal in Hadoop [trunk]
> I have injected some data into the Distributed file system of Hadoop [trunk]
> When I try to do -copyToLocal on a file it works fine but when I try to do -copyToLocal on a directory I get the following error:
> 
> *[user@bob01 nutch]$ bin/hadoop fs -copyToLocal crawldb rohan/
> copyToLocal: Target rohan/data already exists*
>
> Same thing happens when i use -get or -cp option"
HADOOP-1114,bin/hadoop script clobbers CLASSPATH,"The bin/hadoop script distributed with hadoop clobbers the user's CLASSPATH.  This prevents ad-hoc appending to the CLASSPATH.
"
HADOOP-1113,namenode slowdown when orphan block(s) left in neededReplication,"There were about 200 files that had some under-replicated blocks. A ""dfs -setrep 4"" followed by a ""dfs -setrep 3"" was done on these files. Most of the replications took place but the namenode CPU usage got stuck at 99%. The cluster has about 450 datanodes.

The stack trace of the namenode, we saw that there is always one thread of the following type:

IPC Server handler 3 on 8020"" daemon prio=1 tid=0x0000002d941c7d30 nid=0x2d52 runnable [0x0000000042072000..0x0000000042072eb0]
	at org.apache.hadoop.dfs.FSDirectory.getFileByBlock(FSDirectory.java:745)
	- waiting to lock <0x0000002aa212f030> (a org.apache.hadoop.dfs.FSDirectory$INode)
	at org.apache.hadoop.dfs.FSNamesystem.pendingTransfers(FSNamesystem.java:2155)
	- locked <0x0000002aa210f6b8> (a java.util.TreeSet)
	- locked <0x0000002aa21401a0> (a org.apache.hadoop.dfs.FSNamesystem)
	at org.apache.hadoop.dfs.NameNode.sendHeartbeat(NameNode.java:521)
	at sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)

Also, the namenode is currently not doing any replication requests (as seen from the namenode log). A new ""setrep"" command immediately took place. 

My belief is that there is a block(s) that is permanently stuck in neededReplication. This causes all heartbeats requests to do lots of additional processing. thus leading to higher CPU usage. One possibility is that all datanodes that host the replicas of the block in neededReplication are down.

"
HADOOP-1112,Race condition in Hadoop metrics,"AbstractMetricsContext has non-synchronized accesses to the HashMap bufferedData.
"
HADOOP-1111,Job completion notification to a job configured URL,"Currently clients have to poll the JobTracker to find if a job has completed or not.

When invoking Hadoop from other systems is desirable to have a notification mechanism on job completion. 

The notification approach simplifies the client waiting for completion and removes load from the JobTracker as polling can be avoided. 

Proposed solution:

When the JobTracker processes the completion of a job (success and failure)  if the job configuration has a jobEnd.notificationUrl property it will make a HTTP GET request to the specified URL.

The jobEnd.notificationUrl property may include 2 variables in it '${jobId}' and '${jobStatus}'. if they are present, they will be replaced with tehe job ID and status of the job and the URL will be invoked.

Two additional properties, 'jobEnd.retries' and 'jobEnd.retryInterval', will indicate retry behavior.

Not to delay the JobTracker processing while doing notifications, a ConsumerProducer Queue will be used to queue up job notification upon completion.

A daemon thread will consume job notifications from the above Queue and will make the URL invocation. 

On notification failure, the job notification is  queue up again on the notification queue.

The queue will be a java.util.concurrent.DelayQueue. This will make job notifications (on retries) to be avaiable on the consumer side only when the retry time is up.

The changes will be done in the JobTracker and in the LocalJobRunner.

"
HADOOP-1110,"JobTracker WebUI  ""Map input records"" a little off.",Seems like each mapper is counting one extra record.
HADOOP-1109,"Streaming, NPE when reading sequencefile ","When using StreamSequenceRecordReader, I get

java.lang.NullPointerException
	at org.apache.hadoop.streaming.StreamInputFormat.getRecordReader(StreamInputFormat.java:127)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:139)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)"
HADOOP-1108,Checksumed file system should  retry reading if a different replica is found when handle ChecksumException,Currently there is bug in the code where a checksumed file system throws an exception if a different replica is found but retry otherwise when handle ChecksumException.
HADOOP-1107,Calling listPaths() on a dfs:// path doesn't always preserve the dfs relationship,"If you create a Path object from a URI string (ie, dfs://namenode:port/filename), and your default filesystem *isn't* that same dfs namenode, when you call listPaths() on that Path object, the objects you get back are DfsPath() objects which haven't been constructed with the right details to connect to the same host.

I tried this on a debug machine that hadn't been configured to use our DFS configuration. The listPaths() completes, populating with the actual contents of the Path in DFS - but the Path objects no longer refer to the dfs://namenode:port/ that they need to.

This appears to be a problem with DistributedFileSystem.listPaths(), and/or DfsPath(). When you create a DfsPath(), it takes a DfsInfo() object, which isn't derived from any object which remembers the name or port of the namenode."
HADOOP-1106,Number of DataNodes less than target replication causes NameNode WARN message every millisecond,"When I startup a 4 DataNode cluster that already contains files with a target replication of 3, but 2 of the DataNodes fail to come online (ie. there are fewer DataNodes then the target replication), I get the following output in the NameNode log every millisecond or less, continued forever:

2007-03-09 22:45:22,848 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,849 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,850 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,851 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,852 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,852 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,853 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,853 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
2007-03-09 22:45:22,854 WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2
"
HADOOP-1105,"Reducers don't make ""progress"" while iterating through values","Reduces make progress when they go to a new key, but not when they read the next value, which could cause reduces to time out when they have a lot of values for the same key."
HADOOP-1104,Reduce uses the umbilical.progress directly rather than the reporter,"Every time the reduce gets the next key, it calls the umbilical progress method directly, rather than using the reporter object that limits the calls to 1/second."
HADOOP-1102,Setting speculative execution to false via setSpeculativeExecution(false) doesn't seem to have effect,Noticed that RandomWriter job has speculative tasks although speculative execution is explicitly set to false in the program.
HADOOP-1101,Add more statistics in the web-ui to do with tasks,"Currently, the web-ui does not display all the statistics needed to analyze performance of a job in the MR framework. Things like the average time the maps took to finish is missing. Similarly, the min/max/average for shuffle is missing. It would also be nice to add the statistics for major transitions like the time when the last map finished, the time when the last shuffle/copy finished, etc. "
HADOOP-1099,NullPointerException in JobInProgress.getTaskInProgress,"Saw this in the JobTracker log when a user was trying to reload a page for a running job that since finished.

2007-03-09 00:23:44,029 WARN /: /taskdetails.jsp?jobid=job_0003&taskid=tip_0003_m_001379:
java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobInProgress.getTaskInProgress(JobInProgress.java:916)
        at org.apache.hadoop.mapred.JobTracker.getTip(JobTracker.java:1551)
        at org.apache.hadoop.mapred.JobTracker.getTaskStatuses(JobTracker.java:1526)
        at org.apache.hadoop.mapred.taskdetails_jsp._jspService(taskdetails_jsp.java:59)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-1098,output blocks lost when speculative execution is used,"The Sort benchmark completes successfully for me on the latest trunk (0.12.1 candidate) with speculation turned on.  Validation of the Sort benchmark output, however, is failing.  I see one sort output file (part-00375) that is way smaller than all the rest.  In fact, it is exactly 1 block long.

dfs ls output:
...
/user/hadoopqa/sortBenchmark100/output/part-00373       <r 3>   2971688212
/user/hadoopqa/sortBenchmark100/output/part-00374       <r 3>   2973451660
/user/hadoopqa/sortBenchmark100/output/part-00375       <r 3>   134217728
/user/hadoopqa/sortBenchmark100/output/part-00376       <r 3>   2972933208
/user/hadoopqa/sortBenchmark100/output/part-00377       <r 3>   2972309956
...


During the Sort Benchmark, I see 9 AlreadyBeingCreatedExceptions in the NameNode log for this file (and more of these exceptions for other files too).  I also include here the 1 PendingReplicationMonitor WARN message from the NameNode log in case it's relevant:
...
2007-03-08 21:56:31,747 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_-849195508701590166
...
2007-03-08 22:04:35,471 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.startFile: failed to create file /user/hadoopqa/sortBenchmark100/output/part-00375 for DFSClient_task_0002_r_000375_1 on client 72.30.38.16 because pendingCreates is non-null.
2007-03-08 22:04:35,476 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020 call error: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /user/hadoopqa/sortBenchmark100/output/part-00375 for DFSClient_task_0002_r_000375_1 on client 72.30.38.16 because pendingCreates is non-null.
org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /user/hadoopqa/sortBenchmark100/output/part-00375 for DFSClient_task_0002_r_000375_1 on client 72.30.38.16 because pendingCreates is non-null.
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:701)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:283)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)
...


During sort validation, I get this exception in the JobTracker log:
2007-03-08 22:51:32,017 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0003_m_001379_0: java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:180)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:57)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:91)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1525)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1436)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1482)
        at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:72)
        at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)


I also saw this in the DataNode log during sort validation, but it could be unrelated:
2007-03-09 01:04:41,323 WARN org.apache.hadoop.dfs.DataNode: java.io.IOException: Unexpected error trying to delete block blk_-5047673597270588432. Block not found in blockMap.
	at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:596)
	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:429)
	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1053)
	at java.lang.Thread.run(Thread.java:619)

Since the default for 0.12.1 will be for speculative execution to be turned off, I am assigning this to 0.13.0"
HADOOP-1097,Bug in XML serialization of strings in record I/O,XML serialization of strings in record I/O has a bug whicha makes the XmlInputArchive crash with SAXParseExcpetion (generated XML contains invalid Unicode characters http://www.w3.org/TR/REC-xml/#charsets).
HADOOP-1096,Rename InputArchive and OutputArchive and make them public,"Currently hadoop.record.RecordReader and RecordWriter act as factories for various InputArchive and OutputArchive recently. In the original design, this was done in order to have tight control over various serialization formats. This has proven to be counterproductive. For wider usage of record I/O one should be able to use their own serialization formats. The proposed changes make it possible. They are as follows: 

1. Eliminate current record.RecordReader and record.RecordWriter. 

2. rename InputArchive as RecordInput, and OutputArchive as RecordOutput. 

3. rename various archives accordingly. e..g. BinaryInputArchive -> BinaryRecordInput etc."
HADOOP-1094,Optimize readFields and write methods in record I/O,"Optimize generated write() and readFields() methods, so that they do not have to create BinaryOutputArchive or BinaryInputArchive every time these methods are called on a record.  These will be done via static ThreadLocal variable in Binary Archives."
HADOOP-1093,NNBench generates millions of NotReplicatedYetException in Namenode log,"Running NNBench on latest trunk (0.12.1 candidate) on a few hundred nodes yielded 2.3 million of these exceptions in the NN log:

   2007-03-08 09:23:03,053 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 8020 call error:
   org.apache.hadoop.dfs.NotReplicatedYetException: Not replicated yet
        at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:803)
        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:309)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)

I run NNBench to create files with block size set to 1 and replication set to 1.  NNBench then writes 1 byte to the file.  Minimum replication for the cluster is the default, ie 1.  If it encounters an exception while trying to do either the create or write operations, it loops and tries again.  Multiply this by 1000 files per node and a few hundred nodes.
"
HADOOP-1092,NullPointerException in HeartbeatMonitor thread,"Running NNBench with latest trunk, I saw this NPE in the Namenode log:

   Exception in thread ""org.apache.hadoop.dfs.FSNamesystem$HeartbeatMonitor@182bcde"" java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSNamesystem.countContainingNodes(FSNamesystem.java:2506)
        at org.apache.hadoop.dfs.FSNamesystem.access$000(FSNamesystem.java:52)
        at org.apache.hadoop.dfs.FSNamesystem$UnderReplicationBlocks.update(FSNamesystem.java:456)
        at org.apache.hadoop.dfs.FSNamesystem.removeStoredBlock(FSNamesystem.java:2242)
        at org.apache.hadoop.dfs.FSNamesystem.removeDatanode(FSNamesystem.java:1876)
        at org.apache.hadoop.dfs.FSNamesystem.heartbeatCheck(FSNamesystem.java:1958)
        at org.apache.hadoop.dfs.FSNamesystem$HeartbeatMonitor.run(FSNamesystem.java:1698)
        at java.lang.Thread.run(Thread.java:619)"
HADOOP-1091,  NPE from Simon in JT stdout ,"
Seen in JT std out:

Exception in thread ""Timer-0"" java.lang.NullPointerException
        at com.yahoo.simon.hadoop.metrics.Client.computeBlurbSize(Unknown Source)
        at com.yahoo.simon.hadoop.metrics.Client.sendBlurb(Unknown Source)
        at com.yahoo.simon.hadoop.metrics.SimonContext.emitRecord(Unknown Source)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:295)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:48)
        at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:242)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462) 

There are (at least) two bugs:

   (1) o.a.h.metrics.MetricsRecord.setTag(String name, String value) will
   cause this problem to happen (later) if value==null.  It should treat a
   null value as equivalent to an empty string.
   (2) o.a.h.mapred.Counters.getGroupNames returns a collection which may
   be getting updated concurrently.  It needs to make a copy.
"
HADOOP-1090,"In SortValidator, the check for whether a file belongs to sort-input or sort-output dir is weak","In SortValidator, Maps invoke the method called deduceInputFile in the configure method. The deduceInputFile is supposed to return whether the input file belongs to the sort-input directory or the sort-output directory. However, the check that deduceInputFile does - inputFile.toString().startsWith(inputPaths[0].toString()) - is not totally correct. The check will always returns true for inputPaths like /user/foo/smallInput/<filenames>, /user/foo/smallInput-sorted/<filenames>. This finally causes the SortValidator to declare the sort output as incorrect."
HADOOP-1089,The c++ version of write and read v-int don't agree with the java versions,"The serialization of vints is inconsistent between C++ and Java. Since the Java has been fixed recently, I'll move the C++ implementation to match the current Java implementation."
HADOOP-1088,Csv and Xml serialization for buffers do not work for byte value of -1,"If the buffer contains a byte with value -1, the CSV and XML formats do not serialize correctly. Patch follows."
HADOOP-1087,Reducer hangs pulling from incorrect file.out.index path. (when one of the mapred.local.dir is not accessible but becomes available later at reduce time),"
2007-03-07 23:14:23,431 WARN org.apache.hadoop.mapred.TaskRunner: java.io.IOException: Server returned HTTP response code: 500 for URL: http://____:____/mapOutput?map=task_7810_m_000897_0&reduce=397
  at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1149)
  at org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:121)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.copyOutput(ReduceTaskRunner.java:236)
  at org.apache.hadoop.mapred.ReduceTaskRunner$MapOutputCopier.run(ReduceTaskRunner.java:199)
2007-03-07 23:14:23,431 WARN org.apache.hadoop.mapred.TaskRunner: task_7810_r_000397_0 adding host ____.com to penalty box, next contact in 279 seconds

This happened when one of the drives was full and not accessible at map time.

and one mapper

    public void mergeParts() throws IOException {
      ...
      Path finalIndexFile = mapOutputFile.getOutputIndexFile(getTaskId());

failed on the first hash entry in mapred.local.dir and used the second entry

Afterwards, first dir entry became available and when reducer tried to pull through,
    public static class MapOutputServlet extends HttpServlet {
      ...
      Path indexFileName = conf.getLocalPath(mapId+""/file.out.index"");

it used the first entry.

As a result, directory was empty and reducer kept on trying to pull from the incorrect path and hang.

(wasn't sure if this is a duplicate of HADOOP-895 since it is not reproducible unless I get disk failure.)"
HADOOP-1086,spurious data corruption detected because bad crc file,"A bunch of files were  moved from one directory to another but their crc files did not get moved appropriately. Following this move, an application  tries to access these files but gets a crc error. Here is a snippet from the namenode logs:

unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.254.crc to /user/xxx/foo/.254.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.255.crc to /user/xxx/foo/.255.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.256.crc to /user/xxx/foo/.256.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.257.crc to /user/xxx/foo/.257.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.262.crc to /user/xxx/foo/.262.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.265.crc to /user/xxx/foo/.265.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.268.crc to /user/xxx/foo/.268.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.270.crc to /user/xxx/foo/.270.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.271.crc to /user/xxx/foo/.271.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/.274.crc to /user/xxx/foo/.274.crc because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/275 to /user/xxx/foo/275 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/276 to /user/xxx/foo/276 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/277 to /user/xxx/foo/277 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/278 to /user/xxx/foo/278 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/279 to /user/xxx/foo/279 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/280 to /user/xxx/foo/280 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/281 to /user/xxx/foo/281 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/282 to /user/xxx/foo/282 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/283 to /user/xxx/foo/283 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/284 to /user/xxx/foo/284 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/285 to /user/xxx/foo/285 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/286 to /user/xxx/foo/286 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/287 to /user/xxx/foo/287 because destination exists
unprotectedDelete: failed to remove /hadoop/mapred/kry2/system/.job_2745.crc because it does not exist
unprotectedDelete: failed to remove /hadoop/mapred/kry2/system/job_2745 because it does not exist
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/288 to /user/xxx/foo/288 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/289 to /user/xxx/foo/289 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/290 to /user/xxx/foo/290 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/291 to /user/xxx/foo/291 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/292 to /user/xxx/foo/292 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/293 to /user/xxx/foo/293 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/294 to /user/xxx/foo/294 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/295 to /user/xxx/foo/295 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/296 to /user/xxx/foo/296 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/297 to /user/xxx/foo/297 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/298 to /user/xxx/foo/298 because destination exists
unprotectedRenameTo: failed to rename /user/xxx/foo-fix/299 to /user/xxx/foo/299 because destination exists
"
HADOOP-1085,Remove 'port rolling' from Mini{DFS|MR}Cluster,"The rolling of ports in these 2 clusters lead to a lot of timing issues and failed test cases; as witnessed in our patch process.

The way around is to let the OS pick the port for the NameNode/JobTracker and let the the DataNode/TaskTracker query them for the port to connect to and then use that port."
HADOOP-1084,"updating a hdfs file, doesn't cause the distributed file cache to update itself","If I delete and upload a new version of a file /user/owen/foo to HDFS and start my job with hdfs://user/owen/foo as a cached file, it will use the previous contents."
HADOOP-1083,Replication not occuring after cluster restart when datanodes missing,"When a cluster is restarted, but one or more datanodes do not start, the blocks on those missing datanodes are not replicated."
HADOOP-1082,NullpointerException in ChecksumFileSystem$FSInputChecker.seek,"java.lang.NullPointerException
    at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.seek(ChecksumFileSystem.java:138)
    at org.apache.hadoop.fs.FSDataInputStream$PositionCache.seek(FSDataInputStream.java:47)
    at org.apache.hadoop.fs.FSDataInputStream$Buffer.seek(FSDataInputStream.java:82)
    at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:121)
    at org.apache.hadoop.streaming.StreamXmlRecordReader.init(StreamXmlRecordReader.java:73)
    at org.apache.hadoop.streaming.StreamXmlRecordReader.(StreamXmlRecordReader.java:65)
    at com.gale.searchng.workflow.fetcher.FetcherParser$XmlInputFormat.getRecordReader(FetcherParser.java:279)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:139)
    at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)"
HADOOP-1081,JAVA_PLATFORM with spaces (i.e. Mac OS X-ppc-32) breaks bin/hadoop script,"Thus says Brian Whitman in NUTCH-432:

""In some later nightly in the past few weeks (not sure when) the bin/nutch script stopped working on my Macs with
Exception in thread ""main"" java.lang.NoClassDefFoundError: OS
On any command. I tracked it down to the JAVA_PLATFORM env variable that is used to try to find a native hadoop library. The line
JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} org.apache.hadoop.util.PlatformName`
in bin/nutch returns ""Mac OS X-ppc-32"", which then appears as
-Djava.library.path=/Users/bwhitman/Desktop/nn/lib/native/Mac OS X-ppc-32
in the java command line to start a nutch tool.
Not sure the best way to fix this, but I manually put
JAVA_PLATFORM='MacOSX/PPC'
and the error went away. ""

The same problem occurs in bin/hadoop.

I propose the following fix:

  JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} org.apache.hadoop.util.PlatformName | sed -e 's/ /_/g'`

The alternative would be to fix this in PlatformName, but then we may want to get the real platform name in some other places. We could also add a cmd-line switch to PlatformName."
HADOOP-1080,Cygwin path translation should occur earlier in bin/hadoop,"When native Linux libraries are present, and bin/hadoop is running under Cygwin, the part of the script that sets up java.library.path uses un-translated CLASSPATH. This leads to the (in)famous message:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName

The fix is to perform the Cygwin translation of the CLASSPATH earlier, before checking for the native libs."
HADOOP-1077,Race condition in fetching map outputs (might lead to hung reduces),"Sometimes when a map task is lost while the map-output fetch is happening from the TT for that task, and the lost map has successfully executed on some other node, the event for that successful execution is lost at the fetching TT. The fetching TT might eventually fail to fetch the output for the lost task, but then since the event for the new run of the lost map might also have been lost, the fetching TT might hang.

This ""hung"" problem was discovered while working on HADOOP-1060."
HADOOP-1076,Periodic checkpointing cannot resume if the secondary name-node fails.,"If secondary name-node fails during checkpointing then the primary node will have 2 edits file.
""edits"" - is the one which current checkpoint is to be based upon.
""edits.new"" - is where new name space edits are currently logged.
The problem is that the primary node cannot do checkpointing until ""edits.new"" file is in place.
That is, even if the secondary name-node is restarted periodic checkpointing is not going to be resumed.
In fact the primary node will be throwing an exception complaining about the existing ""edits.new""
There is only one way to get rid of the edits.new file - to restart the primary name-node.
So in a way if secondary name-node fails then you should restart the whole cluster.

Here is a rather simple modification to the current approach, which we discussed with Dhruba.
When secondary node requests to rollEditLog() the primary node should roll the edit log only if
it has not been already rolled. Otherwise the existing ""edits"" file will be used for checkpointing
and the primary node will keep accumulating new edits in the ""edits.new"".
In order to make it work the primary node should also ignore any rollFSImage() requests when it
already started to perform one. Otherwise the new image can become corrupted if two secondary
nodes request to rollFSImage() at the same time.

2. Also, after the periodic checkpointing patch HADOOP-227 I see pieces of unusable code.
I noticed one data member SecondaryNameNode.localName and at least 4 methods in FSEditLog
that are not used anywhere. We should remove them and others alike if found.
Supporting unusable code is such a waist of time."
HADOOP-1075,Ensure that all test-cases using MiniDFSCluster & MiniMRCluster do not start the datanodes/tasktrackers before namenode/jobtracker due to port-rolling problems.,The MiniDFSCluster & MiniMRCluster seem to fail with problems arising from port-rolling in our patch-process; stopping the test-cases from starting the NN/JT before the DNs/TTs should help as a short-term fix.
HADOOP-1074,fsck reports different number of files with differing dfs.data.dir directories,"I run randomwriter twice on a formatted Namenode, each time only varying the number of directory elements in dfs.data.dir property.

When dfs.data.dir has 1 element, fsck report the following:

................................................................................Status: HEALTHY
 Total size:    47214240 B
 Total blocks:  760 (avg. block size 62124 B)
 Total dirs:    6
 Total files:   80
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Target replication factor:     3
 Real replication factor:       3.0
The filesystem under path '/' is HEALTHY

When dfs.data.dir has more than one entry, fsck reports the following:
............................................................Status: HEALTHY
 Total size:    35410680 B
 Total blocks:  570 (avg. block size 62124 B)
 Total dirs:    6
 Total files:   60
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Target replication factor:     3
 Real replication factor:       3.0
The filesystem under path '/' is HEALTHY
"
HADOOP-1073,DFS Scalability: high CPU usage in choosing replication targets and file open,"I have a test cluster that has about 1600 data nodes. randomWriter fails to run because of map tasks fail with ""connection timeout"" message. The namenode quickly gets to 100% CPU usage. 

The positives first:
1. Datanodes continue to heartbeat and there are no cascading failures.
2. chooseRandom() does not use much CPU and is very lightweight.


An analysis of the namenode shows the following:

1. High CPU usage in FSNamesystem.getPipeline().
2. Moderate CPU usage in FSNamesystem.sortByDistance().

The first one is used by chooseTarget() to sort a list of target-datanodes based on their distances from the writer. The second one is used by an open() call to arrange the list of datanodes so that the datanode that is closest to the reader is first in the list.

I have two proposals to address this problem. Please comment.

Proposal 1: Optimize getDistance()
--------------
In the current implementation, each datanode has a network path associated with it. For example ""/default-rack/74.6.138.207:50010"". The method getDistance() splits the network-pathname (using ""/"") and then does string-compares to determine the nearest common ancestor of two given nodes. One optimization would be to avoid string splits and comparisions while determining distance between two nodes.

Instead, we can maintain the ""height"" at which a node is located in the network topology tree. The root node being at heigth 0. Also, from each InnerNode we maintain a direct reference to the parent node. If the two nodes are at the same height, send each node to its parent until we reach a common parent.  Thus the distance between the two nodes is 2x where x is the distance to the common parent.  If the nodes are at different depths to begin with, then repeatedly send the node at a greater height to its parent until the nodes are at the same height, and then continue as before.

Also, the calls to check checkArgument() from getDistance() may be removed. 
Also, the call to getPipeline() may be done outside the global FSNamesystem lock.


Proposal 2: Distribute the workload to the DFSClient
---------------
The namenode downloads the network topology to a dfsclient. The dfsclient caches it in memory. When a new block needs to be allocated, the namenode sends a list of unsorted datanodes to the client. The client sorts them based on the cached network topology map. Similarly, when a file is opened, the namenode sends the list of unsorted blocks that comprise this file. The dfsclient sorts them and uses them appropriately. The topology map can be compacted into maybe a 1Mb buffer for a 10000 node system.

If the network topology is very big, then another option would be to have a set of toppology servers (that has a cached copy of the network topology) and the dfsclient contacts one of them to sort its list of target datanodes.




"
HADOOP-1071,RPC$VersionMismatch exception is not fatal to JobTracker,"The following exception should be fatal to the JT, but isn't.  Instead, this exception is repeated every 1 second forever.

2007-03-06 23:07:35,947 WARN org.apache.hadoop.mapred.JobTracker: Error starting tracker: org.apache.hadoop.ipc.RPC$VersionMismatch: Protocol org.apache.hadoop.dfs.ClientProtocol v
ersion mismatch. (client = 7, server = 10)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:253)
        at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:106)
        at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:65)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:160)
        at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:119)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:91)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:546)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:76)
        at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:1600)
"
HADOOP-1070,Number of racks and datanode double temporarily when upgrading from 0.10.1 to 0.11.2,"When upgrading from Hadoop 0.10.1 to 0.11.2, I see the number of racks and datanode double after the 2nd startup of the Namenode.  After the 3rd startup, they correct themselves:

Namenode Log After 1st Startup:
2007-03-06 18:27:27,045 INFO org.apache.hadoop.dfs.StateChange: STATE* Network topology has 1 racks and 4 datanodes

Namenode Log After 2nd Startup:
2007-03-06 18:27:43,201 INFO org.apache.hadoop.dfs.StateChange: STATE* Network topology has 2 racks and 8 datanodes

Namenode Log After 3rd Startup:
2007-03-06 18:28:09,730 INFO org.apache.hadoop.dfs.StateChange: STATE* Network topology has 1 racks and 4 datanodes
"
HADOOP-1069,Rename Hadoop record I/O to Jute,"jute was the original name of the hadoop record i/o component. IMHO, it is easier to pronounce, easier to remember and has already stuck among its users. This renaming should be done while there isn't a large codebase using jute, otherwise it will be very difficult later. rcc will be renamed jrc (jute record compiler)."
HADOOP-1068,Improve error message for 0 datanode case,"A user got the error message:

org.apache.hadoop.ipc.RemoteException: java.io.IOException: failed to create
file /user/root/rand-input/part000000 on client localhost.localdomain
because target-length is 0, below MIN_REPLICATION (1)

A better error message for 0 targets would be something like:
The cluster has no datanodes for creating file /user/root/rand-input/part000000.
"
HADOOP-1067,Compile fails if Checkstyle jar is present in lib directory,"HADOOP-1051 added a checkstyle target. However, the compile target fails if the checkstyle jar is present in the lib directory since it includes an earlier version of Commons CLI which Hadoop doesn't compile against.

The simplest solution is to exclude the checkstyle jar from the classpath."
HADOOP-1066,http://lucene.apache.org/hadoop/ front page is not user-friendly,"One of our tech writers has been looking at the state of Hadoop documentation, and suggests reworking the Hadoop front page to be more than just a release page (as it seems to be now).

Especially in comparison with other other apache project pages, ours is uninviting to users unfamiliar with the premise of the project. She's suggesting updating the page to include more meta-information about the site, in addition to most of the links and news items currently on the page.

"
HADOOP-1064,dfsclient logging messages should have appropriate log levels,"The log messages printed out by DFSClient may pollute the applications' log. Thus, all log messages from the DFSClient should have appropriate log levels so that extraneous messages do not appear in the application log.

Also, an application should be able to switch off all messages that are at informational, warning and debug level."
HADOOP-1063,MiniDFSCluster exists a race condition that lead to data node resources are not properly released,"In MiniDFSCluster, there is a possibility that a data node gets shutted down before it is constructed. This leads to the situation that the data node's resources are not properly released. In Cygwin I observe that the data node directory is still kept locked after a miniDFSCluster is shut down."
HADOOP-1062,Checksum error in InMemoryFileSystem,"Getting the following error in the tasktracker log on 2 attempts:
2007-03-05 14:59:50,320 WARN  mapred.TaskRunner - task_0001_r_000005_0 Intermediate Merge of the inmemory files threw an exception: org.apache.hadoop.fs.ChecksumException: Checksum error: /trank/n
utch-0.9-dev/filesystem/mapred/local/task_0001_r_000005_0/map_2.out at 16776192
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.verifySum(ChecksumFileSystem.java:250)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:207)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:163)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:57)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:91)
        at org.apache.hadoop.io.SequenceFile$Reader.readBuffer(SequenceFile.java:1300)
        at org.apache.hadoop.io.SequenceFile$Reader.seekToCurrentValue(SequenceFile.java:1363)
        at org.apache.hadoop.io.SequenceFile$Reader.nextRawValue(SequenceFile.java:1656)
        at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawValue(SequenceFile.java:2579)
        at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2351)
        at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2226)
        at org.apache.hadoop.mapred.ReduceTaskRunner$InMemFSMergeThread.run(ReduceTaskRunner.java:820)

When I changed fs.inmemory.size.mb to 0 (was 75 - default) the reduce completes successfully.
Could it be related to HADOOP-1027 or HADOOP-1014?

- Espen"
HADOOP-1061,S3 listSubPaths bug,"I had problem with the -ls command in s3 file system. It was returning inconsistence number of ""Found Items"" if you rerun it different times and more importantly it returns recursive results (depth 1) for some folders. 

I looked into the code, the problem is caused by jets3t library. The inconsistency problem will be solved if we use :
S3Object[] objects = s3Service.listObjects(bucket, prefix, PATH_DELIMITER);

instead of 

S3Object[] objects = s3Service.listObjects(bucket, prefix, PATH_DELIMITER , 0);

in listSubPaths of Jets3tFileSystemStore class (line 227)! This change will let GET REST request to have a ""max-key"" paramter with default value of 1000! It seems s3 GET request is sensetive to this paramater! 

But, the recursive problem is because the GET  request doesn't execute the delimiter constraint correctly. The response contains all the keys with the given prefix but they don't stop at the path_delimiter. You can simply test this by making couple folder on hadoop s3 filesystem and run -ls. I followed the generated GET request and it looks all fine but it is not executed correctly at the s3 server side.I still don't know why the response doesn't stop at the path_delimiter. 

Possible casue: Jets3t library does URL encoding, why do we need to do URL encoding in Jets3tFileSystemStore class!?

example:

Original path is   /user/root/folder  and it will be encoded to %2Fuser%2Froot%2Ffolder is Jets3tFileSystemStore class. Then, Jets3t will reencode this to make the REST request. And it will be rewritten as %252Fuser%252Froot%252Ffolder, so the the generated folder on the S3 will be %2Fuser%2Froot%2Ffolder after decoding at the amazon side. Wouldn't be better to skip the encoding part on Hadoop. This strange structure might be the reason that the s3 doesn't stop at the path_delimiter. 

"
HADOOP-1060,IndexOutOfBoundsException in JobInProgress.updateTaskStatus leads to hung jobs,"When the JobTracker detects that a TaskTracker is 'lost' and tries to fail the incomplete tasks and the completed map tasks it fails with:
2007-03-03 00:38:24,056 ERROR org.apache.hadoop.mapred.JobTracker: Tracker Expiry Thread got exception: java.lang.IndexOutOfBoundsException: Index: 310, Size: 307
        at java.util.ArrayList.RangeCheck(ArrayList.java:546)
        at java.util.ArrayList.get(ArrayList.java:321)
        at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:342)
        at org.apache.hadoop.mapred.JobInProgress.failedTask(JobInProgress.java:862)
        at org.apache.hadoop.mapred.JobTracker.lostTaskTracker(JobTracker.java:1637)
        at org.apache.hadoop.mapred.JobTracker$ExpireTrackers.run(JobTracker.java:269)
        at java.lang.Thread.run(Thread.java:595)

Another instance of same exception:
2007-03-05 07:44:42,869 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 50020 call error: java.io.IOException: java.lang.IndexOutOfBoundsException: Index: 12341
215, Size: 83189
java.io.IOException: java.lang.IndexOutOfBoundsException: Index: 12341215, Size: 83189
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:342)
        at org.apache.hadoop.mapred.JobTracker.updateTaskStatuses(JobTracker.java:1611)
        at org.apache.hadoop.mapred.JobTracker.processHeartbeat(JobTracker.java:1163)
        at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:1037)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559)

This means that the tasks aren't updated correctly and the JT just assumes the task is running and never restarts the task... thereby leading to a hung job.
"
HADOOP-1058,ClassCastException in TextInputFormat.getRecordReader,"Running TestMiniMRLocalFS on Linux, I saw this exception during the 3rd job that it runs: 

    [junit] 2007-02-28 21:10:14,739 INFO  mapred.TaskRunner (MapTaskRunner.java:close(46)) - task_0003_m_000001_0 done; removing files.
    [junit] 2007-02-28 21:10:14,746 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(328)) - Error from task_0003_m_000001_0: java.lang.ClassCastException: org.apache.hadoop.mapred.TestMiniMRLocalFS$MyInputFormat$MySplit
    [junit] 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:46)
    [junit] 	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:145)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1444)

    [junit] 2007-02-28 21:10:14,747 INFO  mapred.TaskInProgress (TaskInProgress.java:failedSubTask(380)) - Task 'task_0003_m_000001_0' has been lost.
    [junit] 2007-02-28 21:10:14,748 INFO  mapred.JobTracker (JobTracker.java:removeMarkedTasks(802)) - Removed completed task 'task_0003_m_000001_0' from 'tracker_foo.com:60042'
    [junit] 2007-02-28 21:10:15,145 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1056)) - task_0003_r_000000_0 0.11111112% reduce > copy (1 of 3 at 0.00 MB/s) > 
    [junit] 2007-02-28 21:10:15,373 INFO  mapred.JobClient (JobClient.java:runJob(584)) - Task Id : task_0003_m_000001_0, Status : FAILED
    [junit] task_0003_m_000001_0: 2007-02-28 21:10:11,670 WARN  mapred.TaskTracker (TaskTracker.java:main(1449)) - Error running child
    [junit] task_0003_m_000001_0: java.lang.ClassCastException: org.apache.hadoop.mapred.TestMiniMRLocalFS$MyInputFormat$MySplit
    [junit] task_0003_m_000001_0: 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:46)
    [junit] task_0003_m_000001_0: 	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:145)
    [junit] task_0003_m_000001_0: 	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1444)
"
HADOOP-1057,IOException: job.xml already exists,"Running TestMiniMRLocalFS on Linux, I saw this exception during the 2nd job that it runs:

    [junit] 2007-02-28 21:09:50,076 INFO  mapred.JobInProgress (JobInProgress.java:findNewTask(573)) - Choosing normal task tip_0002_r_000000
    [junit] 2007-02-28 21:09:50,079 INFO  mapred.JobTracker (JobTracker.java:createTaskEntry(709)) - Adding task 'task_0002_r_000000_0' to tip tip_0002_r_000000, for tracker 'tracker_foo.com:60042'
    [junit] 2007-02-28 21:09:50,080 INFO  mapred.TaskTracker (TaskTracker.java:startNewTask(849)) - LaunchTaskAction: task_0002_r_000000_0
    [junit] 2007-02-28 21:09:50,089 WARN  mapred.TaskTracker (TaskTracker.java:startNewTask(866)) - Error initializing task_0002_r_000000_0:
    [junit] java.io.IOException: Target /foo/trunk/build/test/mapred/local/60044_0/taskTracker/jobcache/job_0002/job.xml already exists
    [junit] 	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:226)
    [junit] 	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:70)
    [junit] 	at org.apache.hadoop.fs.LocalFileSystem.copyToLocalFile(LocalFileSystem.java:55)
    [junit] 	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:766)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:351)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:862)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:530)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:898)
    [junit] 	at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:130)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-02-28 21:09:51,115 INFO  mapred.TaskTracker (TaskTracker.java:reportProgress(1056)) - task_0002_m_000000_0 1.0% file:/foo/trunk/build/test/data/wc/input/part-0:0+47
    [junit] 2007-02-28 21:09:51,117 INFO  mapred.TaskTracker (TaskTracker.java:reportDone(1106)) - Task task_0002_m_000000_0 is done.
    [junit] 2007-02-28 21:09:51,203 INFO  mapred.JobInProgress (JobInProgress.java:completedTask(616)) - Task 'task_0002_m_000000_0' has completed tip_0002_m_000000 successfully.
    [junit] 2007-02-28 21:09:51,203 INFO  mapred.TaskInProgress (TaskInProgress.java:completedTask(411)) - Task 'task_0002_m_000000_0' has completed.
    [junit] 2007-02-28 21:09:51,847 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(328)) - Error from task_0002_r_000000_0: Error initializing task_0002_r_000000_0:
    [junit] java.io.IOException: Target /foo/trunk/build/test/mapred/local/60044_0/taskTracker/jobcache/job_0002/job.xml already exists
    [junit] 	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:226)
    [junit] 	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:70)
    [junit] 	at org.apache.hadoop.fs.LocalFileSystem.copyToLocalFile(LocalFileSystem.java:55)
    [junit] 	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:766)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:351)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:862)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:530)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:898)
    [junit] 	at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:130)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-02-28 21:09:51,848 INFO  mapred.TaskInProgress (TaskInProgress.java:failedSubTask(380)) - Task 'task_0002_r_000000_0' has been lost."
HADOOP-1056,Decommission only recognizes IP addesses in hosts and exclude files on refresh.,"HADOOP-442 and HADOOP-985 collided and caused this bug.  Because DatanodeID.getHost() now returns an IP address, when we reread the hosts and exclude files on refresh, we won't recognize any of the hostnames in the files as matching the what getHost returns.  "
HADOOP-1055,Decommission/exclude only recognizes IP addresses in hosts and exclude files,
HADOOP-1054,Add more then one input file per map?,"I've got a problem with mapreduce overhead when it comes to small input files.

Roughly 100 mb comes in to the dfs every few hours. Then afterwards data related to that batch might be added on for another few weeks.
The problem is that this data is roughly 4-5 kbytes per file. So for every reasonably big file we might have 4-5 small ones.

As far as I understand it each small file will get assigned a task of it's own. This causes performance issues since the overhead of such small
files is pretty big.

Would it be possible to have hadoop assign multiple files to a map task up until a configurable limit?"
HADOOP-1053,Make Record I/O functionally modular from the rest of Hadoop,"This issue has been created to separate one proposal originally included in HADOOP-941, for which no consensus could be reached. For earlier discussion about the issue, please see HADOOP-941.

I will summarize the proposal here.  We need to provide a way for some users who want to use record I/O framework outside of Hadoop."
HADOOP-1052,Map tasks not getting killed,"-----------Steps to reproduce---------

1. Create and compile a map reduce application with 
public void map(WritableComparable key,
                    Writable value,
                    OutputCollector output, Reporter reporter) throws IOException {

     reporter.setStatus(""Started"");
    Thread.sleep(3600000);
    report.setStatus(""Should have been killed""); 

}

2. Run the task with the default hadoop configuration: 

<property>
  <name>mapred.task.timeout</name>
  <value>1200000</value>
  <description>The number of milliseconds before a task will be
  terminated if it neither reads an input, writes an output, nor
  updates its status string.
  </description>
</property>

----------------Result------------------
The task does not get killed, and the job is successful

----------------Expected Result------------------
The map tasks get killed and the job should fail.

-Srikanth

  
"
HADOOP-1051,Add checkstyle target to ant build file,"As discussed in HADOOP-948, add a target to allow people to run style checks on the codebase."
HADOOP-1050,Do not count lost tasktracker against the job,Equating a task failure to the one caused by a lost tasktracker is unfair on the job; in extreme cases it leads to a job being killed since a TIP could fail.
HADOOP-1049,race condition in setting up ipc connections,"While running svn head, I get:

[junit] 2007-02-27 19:11:17,707 INFO  ipc.Client (Client.java:run(281)) - java.lang.NullPointerException
    [junit] 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:251)

There is a race condition between when the threads are created above and when the IO streams are set up below."
HADOOP-1048,Sort500 failing since counters patch went in,"Nigel wrote:
  We have a problem
  Looks like counters patch is ""breaking"" things
  Sort500 has been failing since the patch went in
  Looking at the JT logs, there are 0 ""Call queue overflow"" message before the patch went in and 15000+ after the patch went in
  Looks like the counters stuff is overwhelming the JT 

"
HADOOP-1047,TestReplication fails because DFS does not guarantee all the replicas are placed when a file is closed,TestReplication asserts that the number of replicas that a file has is equal to its replication factor immediately after a file is closed. But DFS only guarantees  the minimun number of replicas is placed when a file is closed. This leads to occasional failure of TestReplication.
HADOOP-1046,Datanode should periodically clean up /tmp from partially received (and not completed) block files,"Cluster is set up with tasktrackers running on the same machines as datanodes. Tasks create heavy load in terms of local CPU/RAM/diskIO. I noticed a lot of the following messages from the datanodes in such situations:

2007-02-15 05:30:53,298 WARN  dfs.DataNode - Failed to transfer blk_-4590782726923911824 to xxx.xxx.xxx/10.10.16.109:50010
java.net.SocketException: Connection reset 
....
java.io.IOException: Block blk_71053993347675204 has already been started (though not completed), and thus cannot be created. 

My reading of the code in DataNode.DataXceiver.writeBlock() and FSDataset.writeToBlock() + FSDataset.java:459 suggests the following scenario: there is no cleanup of temporary files in /tmp that are used to store the incomplete blocks being transferred. If the datanode is CPU-starved and drops the connection while creating this temp file, the source datanode will attempt to transfer it again - but there is already a file under this name in /tmp, because when the connection was dropped the target datanode didn't bother to cleanup.

I also see that this section is unchanged in trunk/.

The solution to this would be to check the age of the physical file in the /tmp dir, in FSDataset.java:436 - if it's older than a few hours or so, we should delete it and proceed as if there were no ongoing create op for this block."
HADOOP-1044,TestDecommission fails because it attempts to transfer block to a dead datanode,"There are two iterations in TestDecommission.  After the first iteration, one datanode will be shut down because it was decommissioned.  In the second iteration, while decommissioning the node, if it attempts to transfer blocks to the shut down node, the test will fail.

http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/29/console 

"
HADOOP-1043,Optimize the shuffle phase (increase the parallelism),"In the current shuffle code, only one map output location node is accessed from any Reduce at any given point of time. For example, if a particular node, say machine1.foo.com ran 300 maps, the reducer would fetch just one output from there at a time. machine1.foo.com will be inserted into a Set datastructure (uniqueHosts) and until it gets removed from there, no other map output will be fetched from that machine. The fact that only one map output is fetched at a time from any particular host seems fine, but the logic for removing a node from uniqueHosts is such that there could be a lot of delay before a node gets deleted from the Set datastructure (even after the map output has been fetched from that node). This probably leads to suboptimal performance since it reduces the parallelism in fetching."
HADOOP-1042,Improve the handling of failed map output fetches,"Currently, whenever fetch of a map output fails the corresponding MapOutputLocation is added to a List datastructure for later retrial. But, if the failure was due to a lost task, the entry that was added is not deleted. For such cases, unnecessary retrials will happen. This situation should be prevented."
HADOOP-1041,Counter names are ugly,"Having the complete class name in the counter names makes them unique, but they are ugly to present to non-developers. It would be nice to have some way to have a nicer string presented to the user. Currently, the Enum is converted to a name like:

key.getDeclaringClass().getName() + ""#"" + key.toString()

which gives counter names like ""org.apache.hadoop.examples.RandomWriter$Counters#BYTES_WRITTEN""

which is unique, but not very user friendly. Perhaps, we should strip off the class name for presenting to the users, which would allow them to make nice names. In particular, you could define an enum type that overloaded toString to print a nice user friendly string.

Thoughts?"
HADOOP-1040,"Improvement of RandomWriter example to use custom InputFormat, OutputFormat, and Counters","It would be good if the RandomWriter example used custom InputFormat and OutputFormats rather than creating temporary files in DFS. It is not only faster, it provides a better example to users of how to handle programs that don't have any input. (It still uses FileSplits rather than custom InputSplits, since the FileSplits would do want I need.)

I also added Counters to the example to count records and bytes written."
HADOOP-1039,Reduce the time taken by TestCheckpoint,TestCheckpoint starts and kills MiniDFSCluster about 7 times. Each restart of the MiniDFScluster incurs about 15 seconds of wait time. This increases the total time needed to run TestCheckpoint.
HADOOP-1037,bin/slaves.sh not compatible with /bin/dash,"Ubuntu now uses /bin/dash for scripts that specify /bin/sh, but bin/slaves.sh currently fails with /bin/dash, requiring instead /bin/bash.  We should either declare it to use /bin/bash or fix it to be compatible.

The error I see is:

/home/cutting/src/hadoop/trunk/bin/slaves.sh: 45: Syntax error: Bad substitution
"
HADOOP-1036,task gets lost during assignment,"I ran a unit test (TestMRClassPath) that had a problem (likely in task initialization) that cause one of the maps to get ""lost"". The job tracker had the task as ""assigned"" but the task tracker did not know about it. It did not time out even after 30+ minutes."
HADOOP-1035,StackOverflowError in FSDataSet,"[hadoop.org.apache.hadoop.dfs.DataNode] DataXCeiver
java.lang.StackOverflowError
at java.nio.ByteBuffer.wrap([BII)Ljava.nio.ByteBuffer;(Unknown Source)
at java.nio.ByteBuffer.wrap([B)Ljava.nio.ByteBuffer;(Unknown Source)
at java.lang.StringCoding$CharsetSE.encode([CII)[B(Unknown Source)
at java.lang.StringCoding.encode(Ljava.lang.String;[CII)[B(Unknown Source)
at java.lang.String.getBytes(Ljava.lang.String;)[B(Unknown Source)
at java.io.UnixFileSystem.rename0(Ljava.io.File;Ljava.io.File;)Z(Native Method)
at java.io.UnixFileSystem.rename(UnixFileSystem.java:265)
at java.io.File.renameTo(File.java:1192)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:89)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:105)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)
at org.apache.hadoop.dfs.FSDataset$FSDir.addBlock(FSDataset.java:95)

I do not have the end of the stacktrace, but it is sure it happens in DataNode.DataXceiver.writeBlock().

This error occurs after applying the patch provided in HADOOP-1034 that permits to see such exceptions in log files.
"
HADOOP-1034,RuntimeException and Error not catched in DataNode.DataXceiver.run(),"Only IOException is catched and logged (in warn). 

Every Throwable should be logged in error.

Eg: a RuntimeException occurs in the writeBlock() method. The exception will not be logged, but simply ignored. The socket is closed silently and nothing and an non understandable exception will be thrown in the DFSClient sending the block...."
HADOOP-1032,Support for caching Job JARs ,"Often jobs need to be rerun number of times.. like a job that reads from crawled data time and again.. so having to upload job jars to every node is cumbersome. We need a caching mechanism to boost performance. Here are the features for job specific caching of jars/conf files.. 

 - Ability to resubmit jobs with jars without having to propagate same jar to all nodes.
    The idea is to keep a store(path mentioned by user in job.xml?) local to the task node so as to speed up task initiation on tasktrackers. Assumes that the jar does not change during an MR task.

- An independent DFS store to upload jars to (Distributed File Cache?).. that does not cleanup between jobs.
    This might need user level configuration to indicate to the jobclient to upload files to DFSCache instead of the DFS. https://issues.apache.org/jira/browse/HADOOP-288 facilitates this. Our local cache can be client to the DFS Cache.

- A standard cache mechanism that checks for changes in the local store and picks from dfs if found dirty.
   This does away with versioning. The DFSCache supports a md5 checksum check, we can use that.

Anything else? Suggestions? Thoughts?"
HADOOP-1030,"in unit tests, set ipc timeout in one place","The unit test code currently decreases ipc.client.timeout in three different places in order to speed the execution of unit tests (since daemon startup and shutdown time is related to this value).  The value for unit tests should be set in only a single place.  Also, the value used when testing should perhaps be increased from one second to two seconds, to make tests more reliable."
HADOOP-1029,streaming doesn't work with multiple maps,"StreamInputFormat doesn't 'seek' to the right offset for the split, therefore leading to a situation where every map reads the same input-split and processes it; also the StreamLineRecordReader has a bug where it doesn't clear out the contents of previous keys/values and that data is appended to each subsequent key/value."
HADOOP-1028,Servers should log startup and shutdown messages,"JobTracker, TaskTracker, NameNode, and DataNode should log when they startup and when they shutdown."
HADOOP-1027,Fix the RAM FileSystem/Merge problems (reported in HADOOP-1014),"1) Merge algorithm implementation does not delete empty segments (sequence files with no key/val data) in cases where single level merges don't happen on those segments (due to the check ""numberOfSegmentsRemaining <= factor"" returning true). This affected the in-mem merge in a subtle way :-
For the in-mem merge, the merge-spill-file is given the same name as the name of the 0th entry file in the ramfs. If this file was an empty file, then it would not get deleted from the ramfs, and if the subsequent merge on ramfs chose the same name for the merge-spill-file, it would overwrite the previously created spill. This led to the inconsistent output sizes.

2) The InMemoryFileSystem has a ""close"" method which is not protected (only method where pathToFileAttribs map is modified without first locking the InMemoryFileSystem instance) and that quite likely leads to ConcurrentModificationException if some thread calls InMemoryFileSystem.close (due to some exception) and some other thread is in the process of doing InMemoryFileSystem.getFiles(). However, this problem will not affect the correctness of the merge process (anyway the task is going to fail) and the more important thing is that some other exception happened (like insufficient disk space and so map outputs could not be written) which may not be related to the merge process at all.

3) The number of outputs that is merged at once in RAM should be limited. This is to prevent OutOfMemory errors. Consider a case where there are 10s of thousands of maps and all maps generate empty outputs. Given the default size of the RAM FS as 75 MB, we can possibly accomodate lots of map outputs in RAM without doing any merge but it also results in the various other data structures exploding in size. We have to do a trade off here especially because the inmem-merging is done in the TaskTracker process which already is under a good amount of memory pressure."
HADOOP-1025,remove dead code in Server.java,"There's some dead code in Server.java.  The callDequeued field is no longer used, yet it is populated, synchronized on and notified."
HADOOP-1023,better links to mailing list archives,"The archive links on http://lucene.apache.org/hadoop/mailing_lists.html only point to files, not to websites where the archives can be browsed and searched. I suggest to use these links (additionally or instead of the existing ones):

users:
http://mail-archives.apache.org/mod_mbox/lucene-hadoop-user/
http://www.mail-archive.com/hadoop-user%40lucene.apache.org/

dev:
http://mail-archives.apache.org/mod_mbox/lucene-hadoop-dev/
http://www.mail-archive.com/hadoop-dev%40lucene.apache.org/

commits:
http://mail-archives.apache.org/mod_mbox/lucene-hadoop-commits/
http://www.mail-archive.com/hadoop-commits%40lucene.apache.org/

mail-archives.apache.org seems to be the official archive, mail-archive.com has a search feature.
"
HADOOP-1021,TestMiniMRLocalFS and TestMiniMRCaching broken on Windows,"After fixing HADOOP-761, TestMiniMRLocalFS and TestMiniMRCaching and now broken on Windows.  I belive that fixing them is dependent on fixing HADOOP-1020.

There are multiple problems in a utility class used by these 2 tests, MRCaching.

1) the ""dfs"" uri's should be ""hdfs"" for TestMiniMRCaching
2) the ""dfs"" uri's should be ""file"" for TestMiniMRLocalFS
3) the cache directory should be different for each test"
HADOOP-1020,Path class on Windows seems broken,"Executing this code:

Path file = new Path(""file:///"", ""C:/trunk/build/test/data"");   or   Path file = new Path(""C:/trunk/build/test/data"");
FileSystem fs = file.getFileSystem(conf);
fs.mkdirs(file))

produces this exception.  It looks like it's defaulting to the DistributedFileSystem.

2007-02-14 11:36:31,700 INFO  mapred.TaskInProgress (TaskInProgress.java:updateStatus(296)) - Error from task_0001_m_000000_0: java.lang.IllegalArgumentException: Pathname /C:/trunk/build/test/data from C:/trunk/build/test/data is not a valid DFS filename.
        at org.apache.hadoop.dfs.DistributedFileSystem.getPath(DistributedFileSystem.java:111)
        at org.apache.hadoop.dfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:211)
        at org.apache.hadoop.mapred.MRCaching$MapClass.configure(MRCaching.java:68)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:50)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:70)
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:50)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:70)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:178)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1396)

"
HADOOP-1018,"Single lost heartbeat leads to a ""Lost task tracker""","Under heavy load, task tracker may lose the heartbeat response from the JobTracker. Task tracker tries to resend the last heartbeat message, which job tracker treats as ""duplicate"" response and ignores. Since task tracker tries to resend the same heartbeat message, with the same id, over and over again, no ""valid"" messages reach the job tracker, so after a while it considers the task tracker to be lost. Task tracker cannot recover from this state and needs to be restarted.

Looking at Hadoop trunk/ I believe this problem still may occur - in JobTracker.java.heartbeat():992 JobTracker should not ignore duplicate messages but acknowledge them without processing. This would cause the task tracker to sync back it's last heartbeat id with the last hearbeat id remembered in the job tracker."
HADOOP-1017,Optimization: Reduce Overhead from ReflectionUtils.newInstance,"I found that a significant amount of time on my project was being spent in creating constructors for each row of data. I dramatically optimized this performance by creating a simple WeakHashMap to cache constructors by class. For example, in a sample job I find that ReflectionUtils.newInstance takes 200 ms (2% of total) with the cache enabled, but it uses 900 ms (6% of total) without the cache.

"
HADOOP-1016,navigation on wiki front page should be above the fold,"The Hadoop wiki's front page currently has a long textual description of the project above the navigational links.  The front page is a starting point not just for project newbies, but also for experienced developers.  Thus one should be able to navigate the site quickly from it.  I propose that we have a short introductory paragraph at the top of the page, the move the longer introduction either: to a separate ""about"" page; or underneath the navigational links.  I have a slight preference for a separate ""about"" page.  Thoughts?"
HADOOP-1015,slaves are not recognized by name,"After upgrading from nutch 0.8.1 (has Hadoop 0.4.0) to nutch 0.9.0 (with hadoop 0.10.1), the datanodes where starting with bin/start-all.sh but did not appear in the Hadoop Map/Reduce Administration screen. Only the datanode where the namenode is also running appeared. I was using local dns names which worked fine with hadoop 0.4.0. Now I use ip addresses which give no problem."
HADOOP-1014,map/reduce is corrupting data between map and reduce,"It appears that a random data corruption is happening between the map and the reduce. This looks to be a blocker until it is resolved. There were two relevant messages on hadoop-dev:

from Mike Smith:

The map/reduce jobs are not consistent in hadoop 0.11 release and trunk both
when you rerun the same job. I have observed this inconsistency of the map
output in different jobs. A simple test to double check is to use hadoop
0.11 with nutch trunk.

from Albert Chern:

I am having the same problem with my own map reduce jobs.  I have a job
which requires two pieces of data per key, and just as a sanity check I make
sure that it gets both in the reducer, but sometimes it doesn't.  What's
even stranger is, the same tasks that complain about missing key/value pairs
will maybe fail two or three times, but then succeed on a subsequent try,
which leads me to believe that the bug has to do with randomization (I'm not
sure, but I think the map outputs are shuffled?).

All of my code works perfectly with 0.9, so I went back and just compared
the sizes of the outputs.  For some jobs, the outputs from 0.11 were
consistently 4 bytes larger, probably due to changes in SequenceFile.  But
for others, the output sizes were all over the place.  Some partitions were
empty, some were correct, and some were missing data.  There seems to be
something seriously wrong with 0.11, so I suggest you use 0.9.  I've been
trying to pinpoint the bug but its random nature is really annoying.
"
HADOOP-1012,OutOfMemoryError in reduce,"I'm seeing OutOfMemoryErrors from a reduce in each of DFSIO Benchmark and RandomWriter.  No stack traces are given.  Snipets from the TaskTracker logs are below.  I believe I first saw this on February 3rd during tests that I run weekly.

=====
DFSIO
=====
...
2007-02-10 18:25:20,201 INFO org.apache.hadoop.mapred.TaskRunner: task_0005_r_000000_0 Copying of all map outputs complete. Initiating the last merge on the remaining files in ramfs://mapoutput9105104
2007-02-10 18:25:20,771 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:21,773 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:23,280 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:24,607 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:25,960 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:27,105 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:28,982 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:29,984 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:31,481 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:33,379 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:34,478 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:35,656 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:36,758 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:42,593 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:43,600 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:46,573 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:48,791 INFO org.apache.hadoop.mapred.TaskTracker: task_0005_r_000000_0 0.33333334% reduce > copy (9000 of 9000 at 0.00 MB/s)
2007-02-10 18:25:49,828 WARN org.apache.hadoop.mapred.TaskRunner: Merge of the inmemory files threw an exception: java.lang.OutOfMemoryError: Java heap space
...

============
RandomWriter
============
...
2007-02-11 03:58:00,887 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000000_3 Copying of all map outputs complete. Initiating the last merge on the remaining files in ramfs://mapoutput6576294
2007-02-11 03:58:01,681 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:02,921 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:03,923 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:05,375 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:06,742 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:08,818 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:09,821 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:11,406 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:13,277 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:14,280 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:15,282 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:16,284 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:18,401 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:19,403 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:20,636 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:37,860 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000000_3 0.33333334% reduce > copy (8890 of 8890 at 0.00 MB/s)
2007-02-11 03:58:37,898 WARN org.apache.hadoop.mapred.TaskRunner: task_0001_r_000000_3 Child Error
java.lang.OutOfMemoryError: Java heap space
..."
HADOOP-1011,ConcurrentModificationException in JobHistory,"Using the jobtracker web ui to look at job history.  Loading multiple history URLs concurrently in differnt browser windows yielded this exception in the jobtracker log:

2007-02-12 21:49:00,574 WARN /: /taskdetailshistory.jsp?jobid=job_0001&jobTrackerId=1171157609392&taskid=tip_0001_m_002660:
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:787)
        at java.util.HashMap$EntryIterator.next(HashMap.java:829)
        at java.util.HashMap$EntryIterator.next(HashMap.java:827)
        at java.util.HashMap.putAll(HashMap.java:515)
        at org.apache.hadoop.mapred.JobHistory$KeyValuePair.set(JobHistory.java:263)
        at org.apache.hadoop.mapred.JobHistory$KeyValuePair.handle(JobHistory.java:270)
        at org.apache.hadoop.mapred.DefaultJobHistoryParser$JobTasksParseListener.handle(DefaultJobHistoryParser.java:98)
        at org.apache.hadoop.mapred.JobHistory.parseLine(JobHistory.java:150)
        at org.apache.hadoop.mapred.JobHistory.parseHistory(JobHistory.java:125)
        at org.apache.hadoop.mapred.DefaultJobHistoryParser.parseJobTasks(DefaultJobHistoryParser.java:38)
        at org.apache.hadoop.mapred.loadhistory_jsp._jspService(loadhistory_jsp.java:83)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.Dispatcher.dispatch(Dispatcher.java:275)
        at org.mortbay.jetty.servlet.Dispatcher.include(Dispatcher.java:161)
        at org.apache.jasper.runtime.JspRuntimeLibrary.include(JspRuntimeLibrary.java:966)
        at org.apache.hadoop.mapred.taskdetailshistory_jsp._jspService(taskdetailshistory_jsp.java:73)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-1010,getReordReader methof of InputFormat class should handle null reporter argument,"
In some cases, I need to create a record reader object in the config method of mappers. 
At that time, reporter is not available yet. And logically, the reporter should not be a required for getRecordReader anyway.
"
HADOOP-1009,Infinite loop in chooseTarget,"I am seeing the namenode hangs with 99% CPU usage. The stack trace invariably shows the following stack trace.


 - java.util.Random.next(int) @bci=35, line=141 (Compiled frame; information may be imprecise)
 - java.util.Random.nextInt(int) @bci=40, line=255 (Compiled frame)
 - org.apache.hadoop.dfs.FSNamesystem$Replicator.chooseRandom(int, org.apache.hadoop.dfs.DatanodeDescriptor[], java.util.List, long, int, java.util.List) @bci=7, line=2986 (Interpreted frame)
 - org.apache.hadoop.dfs.FSNamesystem$Replicator.chooseTarget(int, org.apache.hadoop.dfs.DatanodeDescriptor, org.apache.hadoop.dfs.DatanodeDescriptor[], java.util.List, long, int, java.util.List) @bci=211, line=2810 (Interpreted frame)
 - org.apache.hadoop.dfs.FSNamesystem$Replicator.chooseTarget(int, org.apache.hadoop.dfs.DatanodeDescriptor, java.util.List, java.util.List, long) @bci=141, line=2764 (Interpreted frame)
 - org.apache.hadoop.dfs.FSNamesystem$Replicator.chooseTarget(int, org.apache.hadoop.dfs.DatanodeDescriptor, java.util.List, long) @bci=25, line=2716 (Interpreted frame)
 - org.apache.hadoop.dfs.FSNamesystem.startFile(org.apache.hadoop.io.UTF8, org.apache.hadoop.io.UTF8, org.apache.hadoop.io.UTF8, boolean, short, long) @bci=579, line=709 (Interpreted frame)
 - org.apache.hadoop.dfs.NameNode.create(java.lang.String, java.lang.String, java.lang.String, boolean, short, long) @bci=97, line=270 (Interpreted frame)
 - sun.reflect.GeneratedMethodAccessor5.invoke(java.lang.Object, java.lang.Object[]) @bci=238 (Interpreted frame)
 - sun.reflect.DelegatingMethodAccessorImpl.invoke(java.lang.Object, java.lang.Object[]) @bci=6, line=25 (Interpreted frame)
 - java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) @bci=111, line=585 (Interpreted frame)
 - org.apache.hadoop.ipc.RPC$Server.call(org.apache.hadoop.io.Writable) @bci=64, line=337 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler.run() @bci=351, line=538 (Interpreted frame)
"
HADOOP-1008,NPE in org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue,"I think this is the same issue as HADOOP-917

java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:2392)
	at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:2087)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:498)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:191)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1372)"
HADOOP-1007,"Names used for map, reduce, and shuffle metrics should be unique","The names used for map, reduce, and shuffle metrics currently overlap:

Map metrics:
  input_bytes 
  input_records 
  output_bytes 
  output_records 

Recduce metrics:
   input_records 
   output_records 

Shuffle metrics:
   input_bytes 

I propose that the metric names be unique:
  map_input_bytes 
  map_input_records 
  map_output_bytes 
  map_output_records 
  shuffle_input_bytes 
  reduce_input_records 
  reduce_output_records
"
HADOOP-1006,"The ""-local"" option does work properly with test programs","The test programs take a param ""-local"" to force the local file system. This does not work in cases where the file(of type Path) is asked for it's FS. This affects teh testsequencefile program and cud affect other test programs as well. "
HADOOP-1005,"Percentage of reducer progress reaches more than 100%, even though the tasks are still in sorting phase",
HADOOP-1003,Proposal to batch commits to edits log.,"
Right now most expensive namenode operations are that require commits to edits log. e.g. creating a file, deleting, renaming etc. Most of the time is spent in fsync() of edits file (multiple fsync() calls in the case of multiple image directories). During this time whole namesystem is under lock and even non-mutating operations like open() are blocked.

On a local filesystem, each fsync could take in the order of milliseconds. My understanding is that guarantee namenode provides is that edits log is synced before replying to the client. Without any changes to  current locking structure, I was thinking of the following for batching multiple edits : 

     a) a facility in RPC Server to postpone responding to a particular call (communication with ThreadLocals may be). This is strictly not required but without it, number operations batched would be limited to number of IPC threads.
     b) Another Server thread that waits for pending commits to be synced and replies back to clients. 
     c)  fsync manager that periodically syncs the edit log and informs waiting RPCs. The sync thread can dynamically decide to wait longer or shorter based on the load so that we don't increase the latency when namenode is lightly loaded. Event simple policy of 'sync if there are any mutations' will also work but that might reduce the hard disk life.
 
All the synchronization between these threads is a bit complicated but it can be stable. My main concern is whether the guarantee we are providing enough for namenode operation. I think it is enough.  

In terms of throughput, number of creates a namenode can do should be on the same range as number of opens it can do.

 "
HADOOP-1001,the output of the map is not type checked against the specified types,The output of the map is not checked against the types specified in the JobConf leading to hard to diagnose bugs.
HADOOP-1000,Loggers in the Task framework should not write the the Tasks stderr,Tasks should have a log4j config such that events generated by the framework are not printed to the user's Tasks stdout and stderr.
HADOOP-999,DFS Client should create file when the user creates the file,"Currently, the DFS client code does not create the file on the namenode until the first block is flushed. This leads to difficult to diagnose error conditions when there is a conflicting file creation."
HADOOP-997,Implement S3 retry mechanism for failed block transfers,"HADOOP-882 improves S3FileSystem so that when certain communications problems with S3 occur the operation is retried. However, the retry mechanism cannot handle a block transfer failure, since blocks may be very large and we don't want to buffer them in memory. This improvement is to write a wrapper (using java.lang.reflect.Proxy if possible - see discussion in HADOOP-882) that can retry block transfers."
HADOOP-995,DataNode started with illegal startup options should print usage and stop,"Now that we have data-node startup options we should enforce them and not start the data-node when the options are illegal.
Currently the data-node just ignores illegal option.
If I start my data-node with --rack option without the argument the data-node prints the usage and starts anyway.
If I provide an unspecified option, say -xxx, then even the usage message is not printed.
I see an inconsistency here between the usage information and the data-node behavior."
HADOOP-994,DFS Scalability : a BlockReport that returns large number of blocks-to-be-deleted cause datanode to lost connectivity to namenode,"The Datanode periodically invokes a block report RPC to the Namenode. This RPC returns the number of blocks that are to be invalidated by the Datanode. The Datanode then starts to delete all the corresponding files. This block deletion is done by the heartbeat thread in the Datanode. If the number of files to be deleted is large, the Datanode stops sending heartbeats for this entire duration. The Namenode declares the Datanode as ""dead"" and starts replicating its blocks.

In my observed case, the block report returns 1669 blocks that were to be invalidated. The Datanode was running on a RAID5 ext3 filesystem and 4 active tasks were running on it. The deletion of  these 1669 files took about 30 minutes, Wow! The average disk service time during this period was less than 10 ms. The Datanode was using about 30% CPU during this time. "
HADOOP-993,Namenode does not need to store any data node info persistently.,"
Namenode does not need to serialize datanode info. It will map datanode to storageID when datanode register."
HADOOP-992,The mini/mr cluster for testing always uses the local file system rather than the namenode that was passed in,"The mini map/reduce cluster that is used for testing was incorrectly setting the attribute for the filesystem, causing all of the tests to run against the local file system instead of the mini dfs cluster. This patch fixes that and also adds a new method to the mini mr cluster to generate an appropriate JobConf to submit against that cluster. That makes sure that all of the appropriate fields are set."
HADOOP-991,dfs -setrep won't replicate the blocks if the first_block's replication factor == requested replication factor,"% hadoop fsck /user/aaa/bbb -files -blocks
/user/aaa/bbb _________ , 2 block(s):  OK
0. -11111111111111111 repl=3
1. 222222222222222222 repl=2  

%  hadoop dfs -setrep 3 /user/aaa/bbb

would not replicate the block 1.


"
HADOOP-990,Datanode doesn't retry when write to one (full)drive fail,"When one drive is 99.9% full and datanode choose that drive to write, it fails with 

2007-02-07 18:16:56,574 WARN org.apache.hadoop.dfs.DataNode: DataXCeiver
org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: No space left on device
 at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:801)
 at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:563)
 at java.lang.Thread.run(Thread.java:595) 

Combined with HADOOP-940, these failed blocks stay under-replicated.
"
HADOOP-989,Namenode does not need to use Block objects,"
Currently Block object has Block ID and Block length. There are various maps in Namenode that map from or to Block objects but its length is rarely used and essentially a nice-to-have member. We could track only BlockIds in Namenode and if length is required we could store it in rest of the block related data (see HADOOP 988). 

This will reduce memory and will make block look ups faster. 
"
HADOOP-988,Namenode should use single map for block to its meta data.,"
This is a follow up from HADOOP-803. Currently there two maps which have similar functionality :

   1) blockMap : maps block to list of datanodes  that contain the block
   2) activeBlocks : maps block to INode that it blongs to.

Apart from simplifying, it saves 32 bytes per block and 24 bytes by avoid extra block object we currently have for files that exist before Namenode starts (see HADOOP-803).

We could combine these two into something like block to { containingNodes, INode, etc }. 

Another option is to get Move INode and list of dataNodes into Block object.

Another option that requires bigger change is not use Block object but just 64 bit BlockId. Then the map would be BlockId to all the block related info. I will file another Jira regd not using Block object in NameNode.

"
HADOOP-987,pendingTransfer does not remove a block from neededReplications correctly after targets are chosen,"As described in the subject, a block does not get removed from the neededReplications queue after targets are choosen in pendingTransfer."
HADOOP-985,Namenode should identify DataNodes as ip:port instead of hostname:port,"
Right now NameNode keeps track of DataNodes with ""hostname:port"". One proposal is to keep track of datanodes with ""ip:port"". There are various concerns expressed regd hostnames and ip. Please add your experiences here so that we have better idea on what we should fix etc.

How should be calculate datanode ip: 

            1) Just like how we calculate hostname currently with ""dfs.datanode.dns.interface"" and ""dfs.datanode.dns.nameserver"". So if interface specified wrong, it could report ip like 127.0.0.1 which might or might not be intended.

            2) Namenode can use the remove socket address when the datanode registers. Not sure how easy it to get this address in RPC or if this is desirable.

            3) Namenode could just resolve the hostname when a datanode registers. It could print of a warning if the resolved ip and reported ip don't match.

One advantage of using IPs is that DFSClient does not need to resolve them when it connects to datanode. This could save few milliseconds for each block. Also, DFSClient should check all its ips to see if a given ip is local or not.

As far I see namenode does not resolve any DNS in normal operations since it does not actively contact datanodes. In that sense not sure if this have any change in Namenode performance.

Thoughts?

"
HADOOP-984,Exception while retrieving a map output index file is not reported to JobTracker,"The index file of a map output reported a checksum error and was moved to ""bad_file"" dir.  Reduces, however, continued to attempt retrieval of the file forever.  It seem the checksum exception did *NOT* result in the map's TaskTracker reporting to the JobTracker that the map output was lost.  This was likely introduced by HADOOP-331.

TaskTracker log:

2007-02-06 09:06:19,200 WARN org.apache.hadoop.dfs.DistributedFileSystem: Moving bad file /foo/testbase/tmp/mapred/local2/task_0002_m_011745_0/file.out.index to /foo/bad_files/file.out.index.-48633003
2007-02-06 09:06:19,203 INFO org.apache.hadoop.fs.DataInputStream: Found checksum error: org.apache.hadoop.fs.ChecksumException: Checksum error: /foo/testbase/tmp/mapred/local2/task_0002_m_011745_0/file.out.index at 363792
        at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:167)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:125)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:218)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:235)
        at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:281)
        at org.apache.hadoop.fs.FSDataInputStream$Buffer.seek(FSDataInputStream.java:268)
        at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:331)
        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:1533)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HADOOP-983,"Adding -ll & -llr options to 'hadoop dfs', listing # replicas, block size, # of blocks and size","An 'ls' output currently is:

-----
noidea:~ tucu$ hadoop dfs -ls a 100000-5kb IN
Found 7 items
/user/tucu/a/output1-m-00000    <r 1>   330750
/user/tucu/a/output1-m-00001    <r 1>   351471
/user/tucu/a/output2-m-00000    <r 1>   523350
/user/tucu/a/output2-m-00001    <r 1>   506893
/user/tucu/a/output3-m-00000    <r 1>   684390
/user/tucu/a/output3-m-00001    <r 1>   687714
/user/tucu/a/part-00000 <r 1>   5235319
Found 2 items
/user/tucu/100000-5kb/part-00000        <r 1>   257950067
/user/tucu/100000-5kb/part-00001        <r 1>   257938957
Found 1 items
/user/tucu/IN/a.txt     <r 1>   2
-----

The new 'll' output is:

-----
noidea:~ tucu$ hadoop dfs -ll a 100000-5kb IN
Found 7 items
hdfs://localhost:9000/user/tucu/a:
output1-m-00000                <r  1> <bs  -   > <b    1>  322.99 k 
output1-m-00001                <r  1> <bs  -   > <b    1>  343.23 k 
output2-m-00000                <r  1> <bs  -   > <b    1>  511.08 k 
output2-m-00001                <r  1> <bs  -   > <b    1>  495.01 k 
output3-m-00000                <r  1> <bs  -   > <b    1>  668.34 k 
output3-m-00001                <r  1> <bs  -   > <b    1>  671.59 k 
part-00000                     <r  1> <bs  -   > <b    1>    4.99 MB
Found 2 items
hdfs://localhost:9000/user/tucu/100000-5kb:
part-00000                     <r  1> <bs 64 MB> <b    4>  246.00 MB
part-00001                     <r  1> <bs 64 MB> <b    4>  245.98 MB
Found 1 items
hdfs://localhost:9000/user/tucu/IN:
a.txt                          <r  1> <bs  -   > <b    1>       2   
-----

Where <bs ##> is the block size ( - if the file is 1 block or less) and <b ###> is the number of blocks the file has. 

The size is converted to the closest unit and the path of the file is display header style.

"
HADOOP-982,A couple setter functions and toString method for BytesWritable.,I wrote a couple of setters and a workable toString for BytesWritable.
HADOOP-978,AlreadyBeingCreatedException detail message could contain more useful info,"When I get this exception, it would be REALLY helpful if the detail message could include the hostname of the host that is already creating the file.

org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /user/foo/bar_02 for DFSClient_task_0001_m_001777_1 on client myhost.com becausependingCreates is non-null.
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:661)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:248)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)
"
HADOOP-977,The output from the user's task should be tagged and sent to the resepective console streams.,"Currently, the JobClient prints the combined stdout & stderr to the console. It would be good if the output were tagged with the task id and sent to the stdout or stderr matching where it was sent by the task. So if your Mapper (task_0001_m_000123_0) writes ""foobar"" to stderr, the submitting user would see ""task_0001_m_000123_0: foobar"" on their stderr. Clearly this depends on HADOOP-975."
HADOOP-976,SequenceFile.Metadata class should be public,"
The SequenceFile.metadata is currently package private. However, it is needed by public methods.
Thus the class should be public.

"
HADOOP-975,Separation of user tasks' stdout and stderr streams,Today both the stdout and stderr of the task's vm are sent to the same file is ${hadoop.log.dir}/userlogs; it would be very useful to separate them out for both viewing via the tasklog.jsp and to send them back independently for HADOOP-977.
HADOOP-973,NPE in FSDataset during heavy Namenode load,"Running namenode delete benchmark, I saw many of these NPE's traces in a datanode log:

...
2007-02-04 16:17:53,744 INFO org.apache.hadoop.dfs.DataNode: Deleting block blk_-949112377379974621
2007-02-04 16:17:53,744 INFO org.apache.hadoop.dfs.DataNode: Deleting block blk_748220495971269869
2007-02-04 16:17:54,619 INFO org.apache.hadoop.dfs.DataNode: Deleting block blk_658746956471887871
2007-02-04 16:17:56,752 ERROR org.apache.hadoop.dfs.DataNode: Exception: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:519)
        at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:410)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1033)
        at java.lang.Thread.run(Thread.java:595)
2007-02-04 16:18:01,754 INFO org.apache.hadoop.dfs.DataNode: using BLOCKREPORT_INTERVAL of 3329089msec
2007-02-04 16:18:02,577 INFO org.apache.hadoop.dfs.DataNode: Deleting block blk_1657305102826276927
2007-02-04 16:18:02,577 INFO org.apache.hadoop.dfs.DataNode: Deleting block blk_589129256174830133
..."
HADOOP-972,Improve the rack-aware replica placement performance,"This issue aims to improve the rack-aware replica placement performance. A major idea is to avoid constructing lists of possible targets for random selection in chooseTarget, which currently needs interating all DatanodeDescriptors. I plan to change the NetworkTopology data structure as follow:
1. each InnerNode stores its childrens as a list;
2. each InnerNode adds a new field numberOfLeaves the total number of leaves (i.e. data nodes) in its subtree. 
NetworkTopology will support two new methods:
1. DatanodeDescriptor chooseRandom( String scope): it randomly choose one leave from scope.
2. DatanodeDescriptor chooseRandomExclude(String excludedScope): it randomly choose one leave from ~scope

In addition, Issue 971 will also help improve the performance of the rack-aware DFS patch."
HADOOP-971,DFS Scalabilty: Improve name node performance by adding a hostname to datanodes map,"A name node currently maintains a datanode map which maps a storage id to a dataNodeDescriptor. When the name node needs to get a datanodeDescriptor by its name (hostname:port#) or by its host name, it has to iterate through the data node set, which is very inefficent. I'd like to add an additional map that maps a host name to an array list of DatanodeDescriptors that contains all the data nodes running on the host. Since most of time a node runs only one data node, the intial size of the array list is set to be 1."
HADOOP-969,deadlock in job tracker RetireJobs,"The JobTracker deadlocks because RetireJobs grabs locks in the wrong order. The call stacks look like:

""IPC Server handler 5 on 50020"":
       at org.apache.hadoop.mapred.JobTracker.getNewTaskForTaskTracker(JobTracker.java:1108)
       - waiting to lock <0x74487a80> (a java.util.Vector)
       - locked <0x744874b0> (a org.apache.hadoop.mapred.JobTracker)
       at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:992)
       - locked <0x744874b0> (a org.apache.hadoop.mapred.JobTracker)
       at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:585)
       at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)
""retireJobs"":
       at org.apache.hadoop.mapred.JobTracker.removeJobTasks(JobTracker.java:782)
       - waiting to lock <0x744874b0> (a org.apache.hadoop.mapred.JobTracker)
       at org.apache.hadoop.mapred.JobTracker.access$300(JobTracker.java:42)
       at org.apache.hadoop.mapred.JobTracker$RetireJobs.run(JobTracker.java:312)
       - locked <0x74487bb0> (a java.util.ArrayList)
       - locked <0x74487a80> (a java.util.Vector)
       - locked <0x74487a58> (a java.util.TreeMap)
       at java.lang.Thread.run(Thread.java:595)

Found 1 deadlock.

"
HADOOP-968,Reduce shuffle and merge should be done a child JVM,"The Reduce's shuffle and initial merge is done in the TaskTracker's JVM. It would be better to have it run in the Task's child JVM. The advantages are:
  1. The class path and environment would be set up correctly.
  2. User code doesn't need to be loaded into the TaskTracker.
  3. Lower memory usage and contention in the TaskTracker."
HADOOP-967,flip boolean to have rpc clients send a header,This is the continuation of HADOOP-667. I forgot to flip it for 0.10. *smile*
HADOOP-966,Allow logging of Tasks to their own directories,This patch allow the logging from TaskTracker$Child instances (Tasks) to be logged to their own directories.  Currently under the hadoop.log.dir there is a userlogs directory created under which their are task directories to hold error output from tasks.  This feature sets the hadoop.log.dir of each child task being created to that task directory under userlogs.  This way each task can have its logging go to its own directory instead of all being in the main hadoop log file.
HADOOP-965,Isolation Runner looking for job.jar in wrong directory,"The IsolationTaskRunner is looking for the job.jar file in the task directory similar to this:

/d01/dennis/hadoop/mapred/local/taskTracker/jobcache/job_0001/task_0001_m_000000_0/job.jar

This actual file is:

/d01/dennis/hadoop/mapred/local/taskTracker/jobcache/job_0001/job.jar

I have changed the IsolationTaskRunner to look in the correct directory for the jar file."
HADOOP-964,ClassNotFoundException in ReduceTaskRunner,"In the ReduceTaskRunner constructor lin 339 a sorter is created that attempts to get the map output key and value classes from the configuration object.  This is before the TaskTracker$Child process is spawned off into into own separate JVM so here the classpath for the configuration is the classpath that started the TaskTracker.  The current hadoop script includes the hadoop jars, meaning that any hadoop writable type will be found, but it doesn't include nutch jars  so any nutch writable type or any other writable type will not be found and will throw a ClassNotFoundException.

I don't think it is a good idea to have a dependecy on specific Nutch jars in the Hadoop script but it is a good idea to allow jars to be included if they are in specific locations, such as the HADOOP_HOME where the nutch jar resides.  I have attached a patch that adds any jars in the HADOOP_HOME directory to the hadoop classpath.  This fixes the issues with getting ClassNotFoundExceptions inside of Nutch processes."
HADOOP-963,improve the stack trace returned by RPC client,"Currently, the RemoteException thrown from calls to RPCs include the stack trace from the RPC thread rather than the user's thread. "
HADOOP-962,Hadoop EC2 scripts are not executable,"The build script needs to make the scripts under src/contrib/bin executable when packaging them up in the tar file.

Also worth making the small changes mentioned by Doug in HADOOP-884 (adding a README and a template config file) at the same time."
HADOOP-961,a cli tool to get the event logs from a job,Here is a little tool to list the events for a given job. The output can be used to find where each task ran.
HADOOP-959,TestCheckpoint fails on Windows,"The new unit test that tests periodic checkpointing fails on Windows. The problem is that the method FSEditLog.purgeEditLog() uses renameTo() to atomically move edits.new to edits

File.renameTo() fails on Windows is not atomic: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4017593

I propose that we change this code to first try the renameTo(). If it fails, then it deletes edits and then renames edits.new to edits. 

 "
HADOOP-955,"Metrics.report() metricValue parameter type should be float, not long","org.apache.hadoop.metrics.Metrics.report() takes a long metricValue parameter that gets mapped by Java in the method implementation to MetricsRecord.setMetric(String,float) method.  This should be made explicit in the Metrics.report() method parameters by changing the type of metricValue to float."
HADOOP-954,Metrics should offer complete set of static report methods or none at all,org.apache.hadoop.metrics.Metrics currently has one report method.  I should either have report methods for all underlying MetricsRecord or no report methods at all.
HADOOP-952,Create a public (shared) Hadoop EC2 AMI,"HADOOP-884 makes it easy to run Hadoop on an EC2 cluster, but building an AMI (Abstract Machine Image) can take a little while. Amazon EC2 supports shared AMIs (http://developer.amazonwebservices.com/connect/entry.jspa?entryID=530&ref=featured), so we could provide publically available AMIs for each Hadoop release."
HADOOP-951, java.util.ConcurrentModificationException  in FSNamesystem.chooseTargets,"2007-01-26 01:14:37,509 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8020 call error: java.io.IOException: java.util.ConcurrentModificationException
java.io.IOException: java.util.ConcurrentModificationException
  at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:449)
  at java.util.AbstractList$Itr.next(AbstractList.java:420)
  at org.apache.hadoop.dfs.FSNamesystem.chooseTargets(FSNamesystem.java:2282)
  at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:484)
  at org.apache.hadoop.dfs.NameNode.create(NameNode.java:238)
  at sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:585)
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)

Not sure if it's related, but this exception happend when namenode was replicating many blocks.
"
HADOOP-949,reduce hangs after applying HADOOP-248 on 20 node cluster,"After HADOOP-248 was committed, my benchmarks started to hang.  Running TestDFSIO on 20 nodes, the job's single reduce is hanging.  Annodated logs from the task tracker running the reduce are included in the next comment.  I have reverted the patch using:
   cd $trunk_dir
   svn merge -r 500410:500397 .
and reran some benchmarks.  They all pass."
HADOOP-948,Coding style issues ,"I would like to recommend some mainly stylistic changes in the recent fix of http://issues.apache.org/jira/browse/HADOOP-886.  The file in question is CodeFactory.java, and the reasons for the changes are:

   * It is generally preferable to avoid multiple return statements.
   * It is nearly always preferable to use curly braces and a newline after an if (condition).
   * There's no benefit to doing the hash lookup twice in the common case (by calling contains and then get).

(1) and (2) are commonly found in Java coding style guidelines as they make the code more readable. 

I'll attach the fix shortly.
"
HADOOP-947,isReplicationInProgress() is very heavyweight,"Suppose a decommission is in progress. The namenode receives a heartbeat from the being-decommissioned node and then invokes isReplicationInProgress() to determine if the decommissioned is complete. This method is very heavyweight and chews up plenty of CPU.

One option to fix this issue is to keep a counter to indicate how many blocks are pending replication.  This could also help in showing a progress-status-indicator to display how far the decommissioning process is complete.
"
HADOOP-945,Heavy map tasks should be assigned first,"In many cases I noticed that a few  heavy map tasks finished really late since they were started late too. When the jobtracker chooses map tasks for task trackers, it should takes the input data size of the tasks into account, and try to assign the ones with large input first.
"
HADOOP-943,fsck to show the filename of the corrupted file,"When fsck / shows some corrupted file/block information, 

*******************************
CORRUPT FILES:  1
MISSING BLOCKS: 1
MISSING SIZE:   ___ B
********************************

it doesn't show the filename.  

I can run fsck /  -files  again, but this would take a lot longer than the plain fsck. 




"
HADOOP-942,"dfsadmin -report returns deadnode as ""In Service""","dfsadmin -report returned a deadnode entry as follows.

Name: hostA.zzz.com:12345
State          : In Service
Total raw bytes: 0 (0.0 k)
Used raw bytes: 0 (0.0 k)
% used: NaN%

""in service"" is confusing for a node that's not running a datanode.
"
HADOOP-941,Enhancements to Hadoop record I/O - Part 1,"Hadoop record I/O can be used effectively outside of Hadoop. It would increase its utility if developers can use it without having to import hadoop classes, or having to depend on Hadoop jars. Following changes to the current translator and runtime are proposed.

Proposed Changes:

1. Use java.lang.String as a native type for ustring (instead of Text.)
2. Provide a Buffer class as a native Java type for buffer (instead of BytesWritable), so that later BytesWritable could be implemented as following DDL:
module org.apache.hadoop.io {
  record BytesWritable {
    buffer value;
  }
}
3. Member names in generated classes should not have prefixes 'm' before their names. In the above example, the private member name would be 'value' not 'mvalue' as it is done now.
4. Convert getters and setters to have CamelCase. e.g. in the above example the getter will be:
  public Buffer getValue();
5. Generate clone() methods for records in Java i.e. the generated classes should implement Cloneable.
6. Make generated Java codes for maps and vectors use Java generics.

These are the proposed user-visible changes. Internally, the translator will be restructured so that it is easier to plug-in translators for different targets.
"
HADOOP-940,pendingReplications of FSNamesystem is not informative,"Currently when a neededReplication block is scheduled to be replicated, it is put to the pendingReplications queue. When it is no longer under replicated, it is pulled out of the pendingReplications queue. But the queue does not provide any information like how many targets have been choosen or who those targets are. PendingReplications are not used when deciding if a block is under replication. This may cause a block to be over replications or inaccurate estimate of its replication priority.

For example, when a block has 1 replicas but it's replication factor is 2, a data node is choosen to replicate this block and the block is put in the pendingReplications queue. If the block's replication factor is changed to be 3 before the block replication notification, which is the next block report, comes in, the block will be put into neededReplictions queue again under the assumption that it needs to choose 2 targets instead of 1. So the block will end up with 4 replicas.

I propose that we change pendingReplications to be a map from a block to the choosen data nodes. Data nodes in both pendingReplications and blockMap are used when deciding the total number of replicas that a block has. When the name node is notified that the block is replicated in a choosen data node, the data node is moved from pendingReplications to blockMap.

Each choosen target is also associated with a timer indicating how long it expects to receive the block replication notification. PendingReplications queue needs to be periodically scanned to remove those data nodes whose timer is expired."
HADOOP-939,No-sort optimization,"There should be a way to tell the mapred framework that the output of the map() phase will already be sorted.  The Reduce phase can just merge the intermediate files together without sorting.

"
HADOOP-937,data node re-registration,"A name node should ask a data node to re-register when
1) the name node restarts; or
2) the name node receives a heartbeat from an unregistered data node; or
3) the name node recieves a heartbeat from a data node whose heart beat is lost.

In response to the re-registration request, the data node sends a register request followed by a block report."
HADOOP-936,More updates to metric names to conform to HADOOP-887,"In fixing HADOOP-890 I missed 3 metrics in TaskTracker.java

          metricsRecord.setMetric(""tasks-completed"", ++totalTasksCompleted);
          metricsRecord.setMetric(""maps-running"", mapTotal);
          metricsRecord.setMetric(""reduce-running"", reduceTotal);"
HADOOP-935,Abacus should not delete the output dir,"
The current behavior of the Abacus tries to delete the output dir specified in the user's job.
This is not desirable.
Abacus should fail if the output dir already exists.
"
HADOOP-934,TaskTracker sends duplicate status when updating task metrics throws exception,"When updating the TaskTracker metrics repeatedly throws an exception, the TaskTracker repeatedly sends status events to the JobTracker.  This seems to get the JobTracker into a funny state.

See annotated logs below

TaskTracker log:
2007-01-21 23:51:03,555 INFO org.apache.hadoop.mapred.TaskTracker: LaunchTaskAction: task_0001_m_000000_0
2007-01-21 23:51:05,669 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_m_000000_0 0.0% writing -1539638179018996759@438272/1858392
2007-01-21 23:51:06,722 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_m_000000_0 0.0% wrote 7041109562936259497
2007-01-21 23:51:07,784 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_m_000000_0 0.0% wrote -3327068643288265619
2007-01-21 23:51:08,819 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_m_000000_0 0.0% wrote -53923196326936926
2007-01-21 23:51:09,322 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_m_000000_0 1.0% wrote -3407225283463353195
2007-01-21 23:51:09,323 INFO org.apache.hadoop.mapred.TaskTracker: Task task_0001_m_000000_0 is done.
2007-01-21 23:51:09,391 ERROR org.apache.hadoop.mapred.TaskTracker: Caught exception: org.apache.hadoop.metrics.MetricsException:
        at foo.Bar.setMetric(Unknown Source)
        at org.apache.hadoop.mapred.TaskTracker$TaskTrackerMetrics.completeTask(TaskTracker.java:135)
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:604)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:495)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:852)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1494)
2007-01-21 23:51:14,057 INFO org.apache.hadoop.mapred.TaskTracker: Resending 'status' to 'jobtrackerhost' with reponseId '21
2007-01-21 23:51:14,060 ERROR org.apache.hadoop.mapred.TaskTracker: Caught exception: org.apache.hadoop.metrics.MetricsException:
        at foo.Bar.setMetric(Unknown Source)
        at org.apache.hadoop.mapred.TaskTracker$TaskTrackerMetrics.completeTask(TaskTracker.java:135)
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:604)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:495)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:852)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1494)
2007-01-21 23:51:14,060 INFO org.apache.hadoop.mapred.TaskTracker: Resending 'status' to 'jobtrackerhost' with reponseId '22
2007-01-21 23:51:14,063 ERROR org.apache.hadoop.mapred.TaskTracker: Caught exception: org.apache.hadoop.metrics.MetricsException:
        at foo.Bar.setMetric(Unknown Source)
        at org.apache.hadoop.mapred.TaskTracker$TaskTrackerMetrics.completeTask(TaskTracker.java:135)
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:604)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:495)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:852)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1494)
...
[these messages continue forever with each status sent to jobtrackerhost having an increasing responseId]

JobTracker log:
2007-01-21 23:51:03,500 INFO org.apache.hadoop.mapred.JobInProgress: Choosing normal task tip_0001_m_000000
2007-01-21 23:51:03,511 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_0001_m_000000_0' to tip tip_0001_m_000000, for tracker 'tracker:50050'
2007-01-21 23:51:03,569 INFO org.apache.hadoop.mapred.JobInProgress: Choosing normal task tip_0001_r_000000
2007-01-21 23:51:03,577 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_0001_r_000000_0' to tip tip_0001_r_000000, for tracker 'tracker2:50050'
2007-01-21 23:51:09,386 INFO org.apache.hadoop.mapred.JobInProgress: Task 'task_0001_m_000000_0' has completed tip_0001_m_000000 successfully.
2007-01-21 23:51:09,389 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0001_m_000000_0' has completed.
2007-01-21 23:51:14,059 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'SUCCEEDED' for 'task_0001_m_000000_0' of TIP 'tip_0001_m_000000'
2007-01-21 23:51:14,062 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'SUCCEEDED' for 'task_0001_m_000000_0' of TIP 'tip_0001_m_000000'
2007-01-21 23:51:14,064 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'SUCCEEDED' for 'task_0001_m_000000_0' of TIP 'tip_0001_m_000000'
2007-01-21 23:51:14,066 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'SUCCEEDED' for 'task_0001_m_000000_0' of TIP 'tip_0001_m_000000'
... [there are hundreds of these identical messages]
2007-01-21 23:51:16,441 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'SUCCEEDED' for 'task_0001_m_000000_0' of TIP 'tip_0001_m_000000'
2007-01-21 23:51:16,442 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'SUCCEEDED' for 'task_0001_m_000000_0' of TIP 'tip_0001_m_000000'
2007-01-21 23:51:16,444 INFO org.apache.hadoop.mapred.JobInProgress: Task 'task_0001_r_000000_0' has completed tip_0001_r_000000 successfully.
2007-01-21 23:51:16,446 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0001_r_000000_0' has completed.
2007-01-21 23:51:16,456 INFO org.apache.hadoop.mapred.JobInProgress: Job job_0001 has completed successfully.
2007-01-21 23:51:16,459 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0001_r_000000_0' from 'tracker2:50050'
2007-01-21 23:51:16,460 WARN org.apache.hadoop.mapred.TaskInProgress: Recieved duplicate status update of 'SUCCEEDED' for 'task_0001_m_000000_0' of TIP 'tip_0001_m_000000'
2007-01-21 23:51:16,460 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'task_0001_m_000000_0' from 'tracker:50050'
2007-01-21 23:51:16,462 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid task_0001_m_000000_0
2007-01-21 23:51:16,463 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid task_0001_m_000000_0
2007-01-21 23:51:16,465 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid task_0001_m_000000_0
...
[these messages continue forever]"
HADOOP-933,Application defined InputSplits do not work,"If an application defines its own InputSplit, the task tracker chokes when it cannot deserialize the InputSplit when it deserializes MapTasks it receives from the JobTracker. This is because the TaskTracker does not resolve classes from the job jar file. The attached patch delays resolution of the InputSplit until it is running in the context of the child process where it can resolve the InputSplit class."
HADOOP-932,File locking interface and implementation should be remvoed.,"HADOOP-726 was filed on the same issue. HADOOP-726 only deprecates file locking interface. We should remove all the code related these locks in hadoop.
"
HADOOP-931,Make writes to S3FileSystem world visible only on completion,"Currently files written to S3 are visible to other processes as soon as the first block has been written. This is different to DFS which only makes files world visible after the stream writing to the file has closed (see FSNamesystem.completeFile).

We could implement this by having a piece of inode metadata that indicates the visibility of the file."
HADOOP-930,Add support for reading regular (non-block-based) files from S3 in S3FileSystem,"People often have input data on S3 that they want to use for a Map Reduce job and the current S3FileSystem implementation cannot read it since it assumes a block-based format.

We would add the following metadata to files written by S3FileSystem: an indication that it is block oriented (""S3FileSystem.type=block"") and a filesystem version number (""S3FileSystem.version=1.0""). Regular S3 files would not have the type metadata so S3FileSystem would not try to interpret them as inodes.

An extension to write regular files to S3 would not be covered by this change - we could do this as a separate piece of work (we still need to decide whether to introduce another scheme - e.g. rename block-based S3 to ""s3fs"" and call regular S3 ""s3"" - or whether to just use a configuration property to control block-based vs. regular writes).
"
HADOOP-929,PhasedFileSystem should implement get/set configuration ,PhasedFileSystem doesnt channel get/set configuration to base file system
HADOOP-928,make checksums optional per FileSystem,"Checksumming is currently built into the base FileSystem class.  It should instead be optional, with each FileSystem implementation electing whether to use the Hadoop-provided checksum system, or to disable it, or to implement its own custom checksum system.

To implement this, a ChecksumFileSystem implementation can be provided that wraps another FileSystem implementation, implementing checksums as in Hadoop's current mandatory implementation (i.e., as a separate crc file per file that's elided from directory listings).  The 'raw' FileSystem methods would be removed.  FSDataInputStream and FSDataOutputStream would be made interfaces.
"
HADOOP-927,MapReduce is Broken for User-Defined Classes,"Checked out trunk today and all of my jobs with a user-defined class as a map output value class fail with a ClassNotFoundException.  Here's the stack trace:

java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: test.StringWrapper
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:344)
	at org.apache.hadoop.mapred.JobConf.getMapOutputValueClass(JobConf.java:406)
	at org.apache.hadoop.mapred.ReduceTaskRunner.(ReduceTaskRunner.java:339)
	at org.apache.hadoop.mapred.ReduceTask.createRunner(ReduceTask.java:91)
	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:983)
	at org.apache.hadoop.mapred.TaskTracker.launchTaskForJob(TaskTracker.java:366)
	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:359)
	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:823)
	at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:510)
	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:852)
	at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1494)

I haven't looked deeply into it, but I would guess that the problem is in the constructor of ReduceTaskRunner where it attempts to create a SequenceFile.Sorter for the map output value class.  It's probably done in the main TaskTracker JVM process, so external libraries are not on its classpath."
HADOOP-924,Map task is not getting rescheduled although the corresponding TT got lost,"I encountered this ""job hung"" situation during one of the sort runs. Two tasks assigned to a TT were never rescheduled although the TT was lost and this led to the job getting stuck forever. This TT was assigned lots of tasks and everyone got rescheduled except these two. Here are the relevant log messages (below the JT logs has been split into two parts to bring out the sequence of events) for one of the tasks.

JT log:
---------
2007-01-24 10:53:09,564 INFO org.apache.hadoop.mapred.JobInProgress: Choosing normal task tip_0001_m_020699
2007-01-24 10:53:09,564 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'task_0001_m_020699_0' to tip tip_0001_m_020699, for tracker 'foo.com:7020'

TT log:
---------
2007-01-24 10:53:09,564 INFO org.apache.hadoop.mapred.TaskTracker: LaunchTaskAction: task_0001_m_020699_0
2007-01-24 10:53:12,180 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_m_020699_0 0.0% hdfs://foo:50000/user/ddas/somedir/part002444:134217728+134217728

JT log:
---------
2007-01-24 11:05:32,409 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker 'foo.com:7020'

Looks like there is some race condition. Since only two out of the many tasks never got rescheduled,  could mean that the JT was somehow unaware of the state of this two tasks after it assigned them to the (soon-to-be-lost) TT (did they get added to the relevant tables properly?)."
HADOOP-923,DFS Scalability: datanode heartbeat timeouts cause cascading timeouts of other datanodes,"The datanode sends a heartbeat to the namenode every 3 seconds. The namenode processes the heartbeat and sends  a list of block-to-be-replicated and blocks-to-be-deleted as part of the heartbeat response.

At times when a couple of datanodes fail, the heartbeat processing on the namenode becomes pretty heavyweight. It acquires the global FSNamesystem lock, traverses the neededReplication structure, generates a list of blocks to be replicated and responds to the heartbeat message. Determining the list of blocks-to-be-replciated is pretty heavyweight, takes plenty of CPU and blocks processing of other heartbeats because of the global FSNamesystem lock.

It would improve scalability a lot if heartbeat processing does not require the FSNamesystem lock. In fact, the pre-existing ""heartbeat"" lock already exists for this purpose. 

I propose that the Heartbeat message be separate from the ""retrieve blocks-to-replicate and blocks-to-delete"" messages. The datanode can continue to heartbeat once every 3 seconds while it can afford to ""retrieve blocks-to-replicate"" at a much coarser interval. Heartbeat processing on the namenode will be fast because it does not require the global FSNamesystem lock. Moreover, a datanode failure will not aggrevate the heartbeat processing time on the namenode.
 
"
HADOOP-922,Optimize small reads and seeks,"A seek on a DFSInputStream causes causes the next read to re-open the socket connection to the datanode and fetch the remainder of the block all over again. This is not optimal.

A small read followed by a small positive seek could re-utilize the data already fetched from the datanode as part of the previous read. "
HADOOP-921,tail of file not checked for checksum errors,"The checksum is only verified every bytesPerSum (512) bytes.  For the last file size % bytesPerSum bytes in a file, the checksum is not checked. "
HADOOP-920,MapFileOutputFormat and SequenceFileOutputFormat use incorrect key/value classes in map/reduce tasks,"Let's assume a job uses different key/value class for the output of map tasks and for the final output of reduce tasks.

When executing map tasks classes returned from JobConf.getMapOutputKeyClass() / getMapOutputValueClass() should be used, and when executing reduce tasks classes returned from JobConf.gtOutputKeyClass() / getOutputValueClass() should be used.

Currently both map and reduce tasks will use getMapOutputKeyClass/getMapOutputValueClass when using MapFileOutputFormat, or they will always use getOutputKeyClassgetOutputValueClass when using SequenceFileOutputFormat. This causes exceptions, because Mapper / Reducer implementations will output different key/value classes than expected."
HADOOP-918,Examples of Abacus using Python plugins,"
Provide an example to use Abacus with  Python plugin "
HADOOP-917,NPE in org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue,"After nutch started using hadoop 0.10.1 the following Exception started to appear:

java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:2158)
	at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:1892)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:498)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:191)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1367)

Anyone know the cure?"
HADOOP-916,HADOOP-908 patch causes javadoc warnings,"  [javadoc] /home/ndaley/hadoop/912/src/contrib/abacus/src/java/org/apache/hadoop/abacus/JobBase.java:108: warning - @return tag has no arguments.
  [javadoc] /home/ndaley/hadoop/912/src/contrib/abacus/src/java/org/apache/hadoop/abacus/UserDefinedValueAggregatorDescriptor.java:90: warning - @param argument ""valiput"" is not aparameter name.
  [javadoc] /home/ndaley/hadoop/912/src/contrib/abacus/src/java/org/apache/hadoop/abacus/ValueAggregatorBaseDescriptor.java:115: warning - @param argument ""valiput"" is not a parameter name.
  [javadoc] /home/ndaley/hadoop/912/src/contrib/abacus/src/java/org/apache/hadoop/abacus/ValueAggregatorCombiner.java:49: warning - @param argument ""key:"" is not a parameter name.
  [javadoc] /home/ndaley/hadoop/912/src/contrib/abacus/src/java/org/apache/hadoop/abacus/ValueAggregatorDescriptor.java:57: warning - @param argument ""valiput"" is not a parameter name.
  [javadoc] /home/ndaley/hadoop/912/src/contrib/abacus/src/java/org/apache/hadoop/abacus/ValueHistogram.java:48: warning - @param argument ""the"" is not a parameter name.
"
HADOOP-914,Replace use of UTF8 in ObjectWritable,"- Replace use of UTF8 with Text
- Make format a little less verbose, before ObjectWritable containing Text ""text"" would be stored like:
  <UTF8:org.apache.hadoop.io.Text><UTF8:org.apache.hadoop.io.Text><Text:text>
  now it is stored as
  <Text:org.apache.hadoop.io.Text><Text:text>
-As changes are incompatible they are introduced in a new class 'WrappingWritable', ObjectWritable is deprecated
-Add junit test"
HADOOP-913,dynamically loading C++  mapper/reducer classes in map/reduce jobs,"
It is highly desirable for the current map/reduce framework to be able to call functions in c++ (or other languages).

I am proposing a generic entension to the current framework to achieve the above goal. 
The extension is an application level solution, similar to 
HadoopStreaming in spirit, thus does not have impact on Hadoop core.
I will maintain the native map/reduce execution model. 

The basic idea is to use socket/rpc to go through the language barrier.
In particular, we can implement a generic mapper/reducer  class in Java as a proxy for calling functions in other language.
The configure function of the class will create a process that will open a user specified shared lirary act as an RPC server.
The map function of the class will just invoke an RPC call  the key/value pair. 
Such an RPC call is expected to return a list of key/value pairs. The map function then can emit the outputs.
The below is a sketch for the generic class:

        public class MapRedCPPAdapter implements Mapper, Reducer {
                String sharedLibraryName;
                RPCProxy theServer;
                
                ...

                public void configure(JobConf job) {
                        sharedLibraryName = job.get(""shared.lib.name"");
                        theServer = createServer(sharedLibraryName );
               }
               public void close() {
                        theServer.stop();
               }
               public void map(key, value, output, repoter) {
                        ArrayList pairs = invokeRemoteMap(theServer, key, value);
                        emit(pairs)
               }
               public void reduce (key, values, output, reporter) {
                        ArrayList pairs = invokeRemoteReduce(theServer, key, value);
                        emit(pairs)
               }
         }

The cons of this approach include are the overhead associated with 
RPC calls and creating an additional process per mapper/reducer task.
The pros are thhat the extension is clean, generic, simple. It is applicable to other foreign languages too.


"
HADOOP-912,TestMiniMRWithDFS fails sporadically,"TestMiniMRWithDFS fails sporadically with the following error:

junit.framework.AssertionFailedError: Spurious directory task_0001_m_000008_0 found in C:\hudson\workspace\Hadoop-WindowsSmokeTest\trunk\build\test\mapred\local\50068_0
	at org.apache.hadoop.mapred.TestMiniMRWithDFS.checkTaskDirectories(TestMiniMRWithDFS.java:128)
	at org.apache.hadoop.mapred.TestMiniMRWithDFS.testWithDFS(TestMiniMRWithDFS.java:163)

This tests checks that all mapred.local.dirs have been cleaned up after running a mapred job on MiniMRCluster.  Effectively, this tests waits for TaskTracker.isIdle() to return true before checking the dirs have been cleaned up.  I believe that HADOOP-639 reordered the cleanup code so that isIdle() will return true before the directory cleanup is complete."
HADOOP-911,Multithreading issue with libhdfs library,Multithreaded applications using libhdfs sometimes run into IllegalMonitorStateException or plainly lock up (strace shows a thread being in a loop of calling sched_yield). It probably has to do with the fact that libhdfs does not ensure proper allocation of hashtable entries that map a threadId to JNI interface pointer.
HADOOP-910,Reduces can do merges for the on-disk map output files in parallel with their copying,"Proposal to extend the parallel in-memory-merge/copying, that is being done as part of HADOOP-830, to the on-disk files.

Today, the Reduces dump the map output files to disk and the final merge happens only after all the map outputs have been collected. It might make sense to parallelize this part. That is, whenever a Reduce has collected io.sort.factor number of segments on disk, it initiates a merge of those and creates one big segment. If the rate of copying is faster than the merge, we can probably have multiple threads doing parallel merges of independent sets of io.sort.factor number of segments. If the rate of copying is not as fast as merge, we stand to gain a lot - at the end of copying of all the map outputs, we will be left with a small number of segments for the final merge (which hopefully will feed the reduce directly (via the RawKeyValueIterator) without having to hit the disk for writing additional output segments).
If the disk bandwidth is higher than the network bandwidth, we have a good story, I guess, to do such a thing."
HADOOP-909,"dfs ""du"" shows that the size of a subdirectory is 0","dfs ""du"" is implemented by sending a listPaths request to the namenode to get the size of each file/subdir under the directory. At the namenode side, the size of subdir was calculated by recursively going through the whole subtree with the subdir as the root. But starting from the release 0.10.0, the size of subdir is no longer gets calculated. So dfs ""du"" shows its size as 0.

The problem is that both ""du"" and ""list"" send the same request ""listPaths"" to the  namenode. The previous implmentation made list very expensive, but the current implementation makes du not working."
HADOOP-908,"Hadoop Abacus, a package for performing simple counting/aggregation","Hadoop Abacus package is a specialization of map/reduce framework, 
specilizing for performing various counting and aggregations. 
It offers similar functionalities to Google's SawZall. 

Generally speaking, in order to implement an application using Map/Reduce model, 
the developer needs to implement Map and Reduce functions (and possibly Combine function). 
However, for a lot of applications related to counting and statistics computing, 
these functions have very similar characteristics. 
Abacus abstracts out the general patterns and provides a package implementing those patterns. 
In particular, the package provides a generic mapper class, a reducer class and a combiner class, 
and a set of built-in value aggregators. It also provides a generic utility class, ValueAggregatorJob
for creating Abacus jobs.

To create an Abacus job, the user just needs to implement one plugin class that 
is responsible for specifying what aggregators to use and what values are for which aggregators. 
The mapper will call this class in the runtime to generate aggregation ids and values.
The generic  combiner and reducer will aggregate the values associated with the same 
aggregation ids accordingly. Thus, it is much easier to create and run an Abacus job than 
a normal map/reduce job. Since a  built-in generic combiner is always used, the execution is very efficient.




"
HADOOP-906,unit test failures: job.xml already exists,"In my last 30 test runs on REL4 and Windows, I've seen this exception 6 times on Windows and 2 times on REL4 in the TestMiniMRDFSCaching or TestMiniMRClasspath logs:

    [junit] java.io.IOException: Target C:/trunk/build/test/mapred/local/50064_1/taskTracker/jobcache/job_0001/job.xml already exists
    [junit] 	at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:245)
    [junit] 	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:80)
    [junit] 	at org.apache.hadoop.fs.LocalFileSystem.copyToLocalFile(LocalFileSystem.java:349)
    [junit] 	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:842)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:330)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:823)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:510)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:852)
    [junit] 	at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:141)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

In additon to the above exception, TestMiniMRClasspath also logged this exception further down in its log:

    [junit] 2007-01-05 12:23:24,027 INFO  mapred.JobInProgress (JobInProgress.java:completedTask(531)) - Job job_0001 has completed successfully.
    [junit] 2007-01-05 12:23:24,058 INFO  mapred.TaskTracker (TaskTracker.java:purgeJob(638)) - Received 'KillJobAction' for job: job_0001
    [junit] 2007-01-05 12:23:24,058 INFO  mapred.TaskTracker (TaskTracker.java:cleanup(1167)) - Error cleaning up task runner: java.lang.NullPointerException
    [junit] 	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.cleanup(TaskTracker.java:1161)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:1079)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.purgeJob(TaskTracker.java:650)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker.access$100(TaskTracker.java:51)
    [junit] 	at org.apache.hadoop.mapred.TaskTracker$1.run(TaskTracker.java:151)
    [junit] 	at java.lang.Thread.run(Thread.java:595)"
HADOOP-905,Code to qualify inputDirs doesn't affect path validation,"This code, at JobClient:306, doesn't seem to validate the fully qualified inputDirs, since inputDirs is a newly created arrray:

        Path[] inputDirs = job.getInputPaths();
 
        // make sure directories are fully qualified before checking them
        for(int i=0; i < inputDirs.length; ++i) {
          if (inputDirs[i].toUri().getScheme() == null) {
            inputDirs[i] = userFileSys.makeQualified(inputDirs[i]);
          }
        }

        // input paths should exist. 
        job.getInputFormat().validateInput(job);

"
HADOOP-903,TestPread occasionally fails with exception trace mentioned below,"Impossible situation: could not find target position 8192 
java.io.IOException: Impossible situation: could not find target position 8192 
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:873) 
        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:61) 
        at org.apache.hadoop.fs.FSDataInputStream$Checker.readFully(FSDataInputStream.java:155) 
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.readFully(FSDataInputStream.java:212) 
        at org.apache.hadoop.fs.FSDataInputStream$Buffer.readFully(FSDataInputStream.java:265) 
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:315) 
        at org.apache.hadoop.dfs.TestPread.pReadFile(TestPread.java:92) 
        at org.apache.hadoop.dfs.TestPread.testPreadDFS(TestPread.java:119)"
HADOOP-902,NPE in DFSOutputStream.closeBackupStream(),"After HADOOP-757 is checked in, Devraj noticed following NPE:

java.lang.NullPointerException
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.closeBackupStream(DFSClient.java:972)
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1219)
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1181)
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.write(DFSClient.java:1163)
at org.apache.hadoop.fs.FSDataOutputStream$Summer.write(FSDataOutputStream.java:85)
at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:114)
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
at java.io.DataOutputStream.flush(DataOutputStream.java:106)
at java.io.FilterOutputStream.close(FilterOutputStream.java:140)
at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:558)
at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:72)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:331)
at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1367)

I will submit a patch after confirming its ok for backupStream to be null at this stage.
"
HADOOP-901,Make S3FileSystem do recursive renames,"From Mike Smith:

I went through the S3FileSystem.java codes and fixed the renameRaw() method.
Now, it iterates through the folders recursively and rename those. Also, in
the case of existing destination folder, it moves the src folder under the
dst folder.

Here is the piece code that should be replaced in S3FileSystem.java.
renameRaw() method should be replaced by the following methods:


@Override
 public boolean renameRaw(Path src, Path dst) throws IOException {

  Path absoluteDst = makeAbsolute(dst);
  Path absoluteSrc = makeAbsolute(src);

  INode inode = store.getINode(absoluteDst);
  // checking to see of dst folder exist. In this case moves the
  // src folder under the existing path.
  if (inode != null && inode.isDirectory()) {
   Path newDst = new Path(absoluteDst.toString
()+""/""+absoluteSrc.getName());
   return renameRaw(src,newDst,src);
  } else {
  // if the dst folder does not exist, then the dst folder will be created.

  return renameRaw(src,dst,src);
  }
 }

 // recursively goes through all the subfolders and rename those.
 public boolean renameRaw(Path src, Path dst,Path orgSrc) throws
IOException {
    Path absoluteSrc = makeAbsolute(src);
    Path newDst = new Path(src.toString().replaceFirst(orgSrc.toString(),
dst.toString()));
    Path absoluteDst = makeAbsolute(newDst);
    LOG.info(absoluteSrc.toString());
    INode inode = store.getINode (absoluteSrc);
    if (inode == null) {
      return false;
    }
    if (inode.isFile()) {
     store.storeINode(makeAbsolute(absoluteDst), inode);
    } else {
      store.storeINode (makeAbsolute(absoluteDst), inode);
      Path[] contents = listPathsRaw(absoluteSrc);
      if (contents == null) {
        return false;
      }
      for (Path p : contents) {
        if (! renameRaw(p,dst,orgSrc)) {
          return false;
        }

      }
    }
    store.deleteINode(absoluteSrc);
    return true;
}"
HADOOP-900,non-default filesystems do not work for map-reduce inputs,Currently only input paths on the default file system work correctly.
HADOOP-899,Removal of deprecated code (in v0.10.0) from trunk breaks libhdfs,"Some of the code removed as part of HADOOP-781 caused libhdfs to break, since the use of deprecated methods was not discontinued there."
HADOOP-898,namenode generates infinite stream of null pointers,"My namenode is generating a constant stream of NullPointerExceptions. The log looks like:

2007-01-17 19:51:27,461 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3
on 50000 call error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSNamesystem.addStoredBlock(FSNamesystem.java:1621)
        at org.apache.hadoop.dfs.FSNamesystem.processReport(FSNamesystem.java:1563)
        at org.apache.hadoop.dfs.NameNode.blockReport(NameNode.java:573)
        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:589)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)
"
HADOOP-897,Need a simpler way to specify arbitrary options to java compiler while building Hadoop,"Currently, if one has to specify arbitrary command-line options, such as ""-Xlint:unchecked"", to javac, one has to edit build.xml. There should be a property, javac.args, that can be specified on the ant command-line, as follows:

ant compile -Djavac.args=""-Xlint:unchecked""

Patch forthcoming."
HADOOP-895,Reduce hang on small set of SequenceFiles (reproducible),"I'm running into a problem where a Map/Reduce job hangs in the reduce phase.  It happens on a simple program to dump the keys of a small set of SequenceFiles.  It can be reproduced as follows:

1. Pull a vanilla hadoop-0.10.1 release and untar it
2. Apply the attached patch
3. Modify conf/hadoop-env.sh to have the correct JAVA_HOME
4. Run the following script:

#!/bin/sh

ant zvents
./bin/hadoop namenode -format
mkdir fs-hadoop/dfs/tmp
mkdir fs-hadoop/dfs/data
mkdir -p fs-hadoop/mapred/local
./bin/start-all.sh
sleep 5
./bin/hadoop dfs -mkdir /heritrix
./bin/hadoop dfs -mkdir /heritrix/crawls
./bin/hadoop dfs -mkdir /heritrix/crawls/test-4-20070116061502956
./bin/hadoop dfs -copyFromLocal data/IAH-20070116061532-00000-doug-judds-computer.local /heritrix/crawls/test-4-20070116061502956
./bin/hadoop dfs -copyFromLocal data/IAH-20070116061539-00001-doug-judds-computer.local /heritrix/crawls/test-4-20070116061502956
./bin/hadoop dfs -copyFromLocal data/IAH-20070116061539-00002-doug-judds-computer.local /heritrix/crawls/test-4-20070116061502956
./bin/hadoop com.zvents.hadoop.DumpUrls /heritrix/crawls/test-4-20070116061502956 /foo

- Doug
"
HADOOP-894,dfs client protocol should allow asking for parts of the block map,"I think that the HDFS client protocol should change like:

/** The meta-data about a file that was opened. */
class OpenFileInfo {
  /** the info for the first block */
  public LocatedBlockInfo getBlockInfo();
  public long getBlockSize();
  public long getLength();
}

interface ClientProtocol extends VersionedProtocol {
  public OpenFileInfo open(String name) throws IOException;
  /** get block info for any range of blocks */
  public LocatedBlockInfo[] getBlockInfo(String name, int blockOffset, int blockLength) throws IOException;
}

so that the client can decide how much block info to request and when. Currently, when the file is opened or an error occurs, the entire block list is requested and sent."
HADOOP-893,dead datanode set should be maintained in the file handle or file system for hdfs,Currently each call to read is creating a new set of dead datanodes. It seems like it would be more useful to keep a set of dead datanodes at either the file handle or file system level since dead datanodes are probably not quite that transient.
HADOOP-890,Update tag and metric names to conform to HADOOP-887,"In preparation for HADOOP-887, update the current tag and metric names that use currently use '-'.  Convert all '-' to '_'."
HADOOP-889,DFS unit tests have duplicate code,"A number of the DFS-related unit tests have a bunch of copied code.  These include TestRestartDFS, TestFileCorruption, TestFsck and TestCopyFiles.  Even within each test there is a lot of duplicated code.  Maintaining these as APIs evolve is arduous.  They should instead use a common base class."
HADOOP-886,thousands of TimerThreads created by metrics API,"When running the smallJobsBenchmark with 180 maps and hadoop metrics logging to a file 
(ie hadoop-metrics.properties file contains 
     dfs.class=org.apache.hadoop.metrics.file.FileContext
     mapred.class=org.apache.hadoop.metrics.file.FileContext)
then I get this error:

org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:574)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:517)
	at org.apache.hadoop.ipc.Client.call(Client.java:452)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:164)
	at org.apache.hadoop.dfs.$Proxy0.isDir(Unknown Source)
	at org.apache.hadoop.dfs.DFSClient.isDirectory(DFSClient.java:325)
	at org.apache.hadoop.dfs.DistributedFileSystem.isDirectory(DistributedFileSystem.java:167)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:82)
	at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:222)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:842)
	at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:86)
	at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1338)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:258)

Using jconsole, I see that 2000+ of these threads were created:

Name: Timer-101
State: TIMED_WAITING on java.util.TaskQueue@1501026
Total blocked: 0  Total waited: 5
Stack trace: 
java.lang.Object.wait(Native Method)
java.util.TimerThread.mainLoop(Timer.java:509)
java.util.TimerThread.run(Timer.java:462)

The only use of the java.util.Timer API is in org.apache.hadoop.metrics.spi.AbstractMetricsContext.
"
HADOOP-885,Reduce CPU usage on namenode: gettimeofday,"On a 900 node idle cluster, the namenode spends about  20% of CPU. Most of this CPU is spent processing pure heartbeats. No jobs are running on this cluster and all nodes are alive and acting well.

Of the total namenode CPU usage, about 12% is in usermode and about 70% is in kernel mode! The question that natually arises is why is heartbeat processing taking so much time in kernel mode?

An strace of namenode reveals that a 20 second period has about 52000 syscalls with the following breakup:

gettimeofday  :       18000 calls
accept             :          2655 calls
close               :          2655 calls
shutdown       :          2655 calls
fcntl                  :          7965 calls
read                 :          7965 calls
futex                 :          5295 calls
poll                   :          4894 calls

A code inspection reveals that the code is doing multiple (about 5) calls to System.currentTimeMillis() in processing a single request in the RPC.java and Server.java classes. This might mean that there is a possibility of optimization.



"
HADOOP-884,Create scripts to run Hadoop on Amazon EC2,"It is already possible to run Hadoop on Amazon EC2 (http://wiki.apache.org/lucene-hadoop/AmazonEC2), however it is a rather involved, largely manual process. By writing scripts to automate (as far as is possible) image creation and cluster launch it will make it much easier to use Hadoop on EC2."
HADOOP-882,S3FileSystem should retry if there is a communication problem with S3,File system operations currently fail if there is a communication problem (IOException) with S3. All operations that communicate with S3 should retry a fixed number of times before failing.
HADOOP-881,job history web/ui does not count task failures correctly,"The archived job history displays the count of tips that failed, which is almost always 0, instead of the number of task attempts that failed, which is a lot more interesting."
HADOOP-880,Recursive delete for an S3 directory does not actually delete files or subdirectories,"Here is the bug report from Michael Stack:

Here I'm listing a BUCKET directory that was copied up using 'hadoop
fs', then rmr'ing it and then listing again:

stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs
s3://ID:SECRET@BUCKET -ls /fromfile
Found 2 items
/fromfile/diff.txt      <r 1>   591
/fromfile/x.js  <r 1>   2477
stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs
s3://ID:SECRET@BUCKET -rmr /fromfile
Deleted /fromfile
stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs
s3://ID:SECRET@BUCKET -ls /fromfile
Found 0 items

The '0 items' is odd because, now, listing my BUCKET using a tool other
than 'hadoop fs' (i.e. hanzo webs python scripts):

stack@bregeon:~/checkouts/hadoop.trunk$ s3ls BUCKET
%2F
%2Ffromfile%2F.diff.txt.crc
%2Ffromfile%2F.x.js.crc
%2Ffromfile%2Fdiff.txt
%2Ffromfile%2Fx.js
block_-5013142890590722396
block_5832002498000415319
block_6889488315428893905
block_9120115089645350905

Its all still there still.  I can subsequently do the likes of the
following:

stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs
s3://ID:SECRET@BUCKET -rmr /fromfile/diff.txt

... and the delete will succeed and looking at the bucket with alternate
tools shows that it has actually been remove, and so on up the hierarchy."
HADOOP-879,SequenceFileInputFormat can no longer read from data produced by MapFileOutputFormat,"The call to validateInput() happens before listPaths(). listPaths() will allow MapFile inputs, but validateInput() fails on directories."
HADOOP-878,reducer NONE does not work with multiple maps,"If you execute more than one maps with -reducer None output data seems to get lost. The number of lines output with identity reducer is greater than the number of lines output using reducer NONE.

"
HADOOP-877,we should automate checks of the output of the sort example program,"Since we are using the sort example program to do smoke tests on new versions of Hadoop, it would be nice to have some checks of the output. The checks that I've considered:
  1. count the number of records on input & output
  2. compute the md5 of each key/value and xor across all of the rows
  3. use a map/reduce job to merge the input and output directories and make sure that each key/value appears on both input and output"
HADOOP-876,need a setting for random writer that generates compressable data and compressess it.,It would help debugging and testing if the random writer example had a switch to generate compressible data and write it compressed.
HADOOP-875,memory model is not accurate enough for map side sorts,I configured a sort (IdentityMapper) with large compressed values with the map output buffer (io.sort.mb) set to 300mb and the child jvm heap size set to 900mb and some of the maps were running out of memory deterministically. I suspect that some part of the data path is not handling large values well and is consuming large amounts of ram.
HADOOP-874,merge code is really slow,"I had a case where the map output buffer size (io.sort.mb) was set too low and caused a spill and merge. Fixing the configuration caused it to not spill until it was finished. With the spill it took 9.5 minutes per a map. Without the spill it took 45 seconds. Therefore, I assume it was taking ~9 minutes to do the 2 file merge. That is really slow. The input files to the merge were two 25 mb sequence files (default codec (java), block compressed)
"
HADOOP-873,native libraries aren't loaded unless the user specifies the java.library.path in the child jvm options,"The TaskRunner adds the setting of the java.library.path after the class name, which makes it an argument to the main of the TaskTracker.Child. It should be added to the command line before the class name."
HADOOP-872,map output sorter doesn't compress the outputs before the sort,"The map outputs are not compressed for the in memory sort, even if mapred.compress.map.output is set. This can significantly increase the in memory size of the buffered outputs."
HADOOP-871,"java.library.path is wrongly initialized by bin/hadoop when only pre-built libs are present, but custom-built ones aren't","bin/hadoop assumes presence of custom-libs and doesn't check it - resulting in wrong java.library.path.

Straight-forward fix is to check and add the ':' only if custom-libs are present."
HADOOP-870,Store creation date for the files produced by Map Reduce,"Being able to know when a file has been created it very useful.
While full support of files dates in DFS is a big task and does not seem to be going to happen soon, it still would be good to intorduce a partial solution for Map Reduce.
Why don't we put the date into the file names?
Instead of 
  part-00000
the name would be
  part-00000-07-01-08-11:35:20
or something like this.

This may make life so much easier....    "
HADOOP-868,Fix the merge method on Maps to limit the number of open files,"The mergeParts method should not hardcode the factor for the merge and instead rely on the configured default. Also, there is an unnecessary open call made in the method."
HADOOP-867,job client should generate input fragments before the job is submitted,"The JobClient should generate the input fragments and write them to dfs next to the job.xml parallel to the job.xml. This would offload the JobTracker, get rid of the job state where the job isn't initialized yet, and fix the problem with the class path for job initialization."
HADOOP-866,dfs -get should remove existing crc file if -crc is not specified,"When we added -crc option to dfs -get (aka dfs -copyToLocal) the absence of this command-line option implies not copying the crc file associated with hdfs file. However, if the checksum file already exists, it will not correspond with the newly copied data file, and opening it would cause checksum failure. The solution is to remove any existing crc file corresponding to the data file if -crc option is not given for -get. Patch forthcoming."
HADOOP-865,Files written to S3 but never closed can't be deleted,"I've been playing with the S3 integration. My first attempts to use it are actually as a drop-in replacement for a backup job, streaming data offsite by piping the backup job output to a ""hadoop dfs -put - targetfile"".

If enough errors occur posting to S3 (this happened easily last Thursday, during an S3 growth issue), the write can eventually fail. At that point, there are both blocks and a partial INode written into S3. Doing a ""hadoop dfs -ls filename"" shows the file, it has a non-zero size, etc. However, trying to ""hadoop dfs -rm filename"" a failed-written file results in the response ""rm: No such file or directory."""
HADOOP-864,bin/hadoop jar throws file creation exception for temp files,"bin/hadoop throw file creation exception if hadoop.tmp.dir doesn't exist. 
$ bin/hadoop jar build/hadoop-streaming.jar 
Exception in thread ""main"" java.io.IOException: No such file or directory
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.checkAndCreate(File.java:1345)
        at java.io.File.createTempFile(File.java:1434)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:109)

it should create the hadoop.tmp.dir before trying File.createTempFile"
HADOOP-863,MapTask prints info log message when the progress-reporting thread starts,One LOG.info in MapTask.java (in the run method of the progress-reporting thread) should be changed to LOG.debug for maintaining consistency with what is logged at debug/info levels.
HADOOP-862,Add handling of s3 to CopyFile tool,CopyFile is a useful tool for doing bulk copies.  It doesn't have handling for the recently added s3 filesystem.
HADOOP-858,clean up smallJobsBenchmark and move to src/test/org/apache/hadoop/mapred,"Finish off the missing piece of HADOOP-371 (move src/contrib/smallJobsBenchmark to src/test/org/apache/hadoop/mapred) and cleanup or consolidate documentation, scripts and dead code.
"
HADOOP-857,IOException when running map reduce on S3 filesystem,"Setting fs.default.name to be an S3 path causes the following exception:

java.io.IOException: Cannot create file /tmp/hadoop-tom/mapred/system/submit_habo0/job.jar since parent directory does not exist.
        at org.apache.hadoop.fs.s3.S3FileSystem.createRaw(S3FileSystem.java:150)
        at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:58)
        at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:47)
        at org.apache.hadoop.fs.FSDataOutputStream.<init>(FSDataOutputStream.java:148)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:369)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:276)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:94)
        at org.apache.hadoop.fs.s3.S3FileSystem.copyFromLocalFile(S3FileSystem.java:290)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:294)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:371)
        at org.apache.hadoop.examples.Grep.main(Grep.java:69)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:143)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:41)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:151)
"
HADOOP-856,fsck reports a non-existant DFS path as healthy,"I would have expected the following to generate an error message:

-bash-3.00$ bin/hadoop fsck non-existant
Status: HEALTHY
 Total size:    0 B
 Total blocks:  0
 Total dirs:    0
 Total files:   0
 Over-replicated blocks:        0
 Under-replicated blocks:       0
 Target replication factor:     3
 Real replication factor:       0.0

The filesystem under path 'non-existant' is HEALTHY
"
HADOOP-855,HDFS should repair corrupted files,"While reading if we discover a mismatch between a block and checksum, we want to report this back to the namenode to delete the corrupted block or crc.

To implement this, we need to do the following:
DFSInputStream
1. move DFSInputStream out of DFSClient
2. add member variable to keep track of current datanode (the chosen node)

DistributedFileSystem
1. change reportChecksumFailure parameter crc from int to FSInputStream (needed to be able to delete it). 
2. determine specific block and datanode from DFSInputStream passed to reportChecksumFailure  
3. call namenode to delete block/crc vis DFSClient

ClientProtocol
1. add method to ask namenode to delete certain blocks on specifc datanode.

Namenode
1. add ability to delete certain blocks on specific datanode"
HADOOP-853,Move site directories to docs directories,"The site documentation needs to be included in the release (HADOOP-371) as a top-level docs directory.  Before doing so, we should move, in Subversion, the top-level site directory and src/site to a top-level docs directory and src/docs, respectively."
HADOOP-852,want ant task for record definitions,"An ant task which invoked the record compiler would be good to have.  Then, in build.xml, we can use this task to scan the src tree and generate java files for all defined records.  Generated classes should also be included in the javadoc."
HADOOP-851,Implement the LzoCodec with support for the lzo compression algorithms,"lzo is clearly one the best compression libraries out there: ... http://compression.ca/act/act-summary.html

It should be a good value-add for hadoop...
"
HADOOP-850,Add Writable implementations for variable-length integer types.,"Currently Hadoop supports only three basic integer-like types: ByteWritable, IntWritable and LongWritable. They provide a fixed tradeoff between their value range and on-disk space consumption. But it is sometimes useful to be able to store integer values with broader allowed range, but less space consumption when possible.

This is especially useful when storing very long series of  values, combined with delta encoding.

Lucene already implements variable-length encoding for positive int and long. I propose to add similar Writable implementations, which use the same encoding methods."
HADOOP-849,randomwriter fails with 'java.lang.OutOfMemoryError: Java heap space' in the 'reduce' task,"Reproducible, tried to increase the child jvm's heapsize via 
<property>
  <name>mapred.child.java.opts</name>
  <value>-Xmx512m</value>
</property>

without any difference, still fails.

Need to investigate further."
HADOOP-848,randomwriter generates too many errors of type: org.apache.hadoop.dfs.AlreadyBeingCreatedException,"randomwriter with speculative execution turned 'on' fails with too many errors:

2006-12-25 06:50:38,880 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0002_m_002175_1: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /randomwriter/input/part000522 for DFSClient_task_0002_m_002101_1 on client XXX because pendingCreates is non-null.
2006-12-25 06:50:48,203 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0002_m_002213_1: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /randomwriter/input/part000623 for DFSClient_task_0002_m_002213_1 on client XXX because pendingCreates is non-null.
2006-12-25 06:50:49,917 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0002_m_002211_1: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /randomwriter/input/part000621 for DFSClient_task_0002_m_002211_1 on client XXX because pendingCreates is non-null.
2006-12-25 06:50:52,812 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0002_m_002247_1: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file /randomwriter/input/part000654 for DFSClient_task_0002_m_002247_1 on client XXX because pendingCreates is non-null.

I suspect this is due to the the fact that randomwriter doesn't use the PhasedFileSystem.

-*-*-

Unrelated note: can we add the 'examples' *Component* to this project here on jira? I've put in 'mapred' for this issue for now, please feel free to correct me. Thanks!"
HADOOP-846,Progress report is not sent during the intermediate sorts in the map phase,"Have seen tasks getting lost at the TaskTracker's end due to MapTask's progress not getting reported for a long time (the configured timeout). The progress report is currently not sent in the intermediate sort phases in the MapTask. But, if for some reason, the sort takes a long time, the TaskTracker might kill the task."
HADOOP-845,DFS -get and DFS -cat on a zip file generate different output,dfs -get file.gz localfile.gz and dfs -cat file.gz  > /tmp/file.gz generate different output. dfs -get geenreates corect output but dfs -cat does not geenerate the right ouput. 
HADOOP-844,Metrics messages are sent on a fixed-delay schedule instead of a fixed-rate schedule,"This potentially affects all metrics implementation packages which use a server that expects data to be sent at a particular rate.  It means that with counter metrics you will see a spurious periodic dip in the metric, because the rate at which metrics are sent is not  quite keeping up with what the server expects.

The fix is trivial.  In org.apache.hadoop.metrics.spi.AbstractMetricsContext, the call to Timer.schedule should be changes to Timer.scheduleAtFixedRate (which takes the same arguments).

"
HADOOP-843,JobClient gets rpc timeout exception after running lots of jobs,"While testing HADOOP-815 with smallJobsBenchmark (running ~750 jobs with 300 maps & 2 reduces each) I ran into this:

06/12/20 22:35:22 INFO conf.Configuration: parsing file:/export/crawlspace/kryptonite/arunc/hadoop/hadoop-0.9.3-dev/conf/hadoop-default.xml
06/12/20 22:35:22 INFO conf.Configuration: parsing file:/export/crawlspace/kryptonite/arunc/hadoop/hadoop-0.9.3-dev/conf/mapred-default.xml
06/12/20 22:35:22 INFO mapred.MultiJobRunner: Running job, Input : /mapred/benchmark/input Output : /mapred/benchmark/temp/multiMapRedOutput_-2104398834
06/12/20 22:35:39 INFO mapred.JobClient: Running job: job_0746
06/12/20 22:35:41 INFO mapred.JobClient:  map 0% reduce 0%
06/12/20 22:36:07 INFO mapred.JobClient:  map 1% reduce 0%
06/12/20 22:36:08 INFO mapred.JobClient:  map 2% reduce 0%
06/12/20 22:36:09 INFO mapred.JobClient:  map 3% reduce 0%
06/12/20 22:36:10 INFO mapred.JobClient:  map 4% reduce 0%
06/12/20 22:37:11 INFO mapred.JobClient: Communication problem with server: java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:467)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:164)
        at $Proxy1.getJobStatus(Unknown Source)
        at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:345)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:382)
        at org.apache.hadoop.benchmarks.mapred.MultiJobRunner.runJobInSequence(MultiJobRunner.java:169)
        at org.apache.hadoop.benchmarks.mapred.MultiJobRunner.run(MultiJobRunner.java:277)
        at org.apache.hadoop.benchmarks.mapred.MultiJobRunner.main(MultiJobRunner.java:401)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:143)
        at org.apache.hadoop.benchmarks.mapred.BenchmarkRunner.main(BenchmarkRunner.java:17)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:149)
06/12/20 22:38:46 INFO mapred.JobClient: Communication problem with server: java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:467)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:164)
        at $Proxy1.getJobStatus(Unknown Source)
        at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:345)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:382)
        at org.apache.hadoop.benchmarks.mapred.MultiJobRunner.runJobInSequence(MultiJobRunner.java:169)
        at org.apache.hadoop.benchmarks.mapred.MultiJobRunner.run(MultiJobRunner.java:277)
        at org.apache.hadoop.benchmarks.mapred.MultiJobRunner.main(MultiJobRunner.java:401)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:143)
        at org.apache.hadoop.benchmarks.mapred.BenchmarkRunner.main(BenchmarkRunner.java:17)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:149)

06/12/20 22:38:47 INFO mapred.JobClient:  map 99% reduce 1%
06/12/20 22:38:52 INFO mapred.JobClient:  map 100% reduce 2%
06/12/20 22:39:03 INFO mapred.JobClient:  map 100% reduce 17%
06/12/20 22:39:06 INFO mapred.JobClient:  map 100% reduce 51%
06/12/20 22:39:12 INFO mapred.JobClient:  map 100% reduce 66%
06/12/20 22:39:15 INFO mapred.JobClient:  map 100% reduce 100%
06/12/20 22:39:16 INFO mapred.JobClient: Job complete: job_0746

The job completes successfully though...

Thoughts?"
HADOOP-842,change the open method in ClientProtocol to take an additional argument: clientMachine,"The open method returns a list of blocks that belong to a file and, for each block, a list of datanodes that contain the block.To improve reading efficiency, I'd like to return the list of datanodes in the order of its distance to the client machine. So I suggest that we make the open method to take an additional argument clientMachine which is in the format of hostname:port#."
HADOOP-841,native hadoop libraries don't build properly with 64-bit OS and a 32-bit jvm,"src/native/lib/Makefile.am doesn't pass along the appropriate -m32/-m64 flag while using libtool to create the shared-object which results in this problem.

Simple fix:
-AM_LDFLAGS = @JNI_LDFLAGS@
+AM_LDFLAGS = @JNI_LDFLAGS@ -m$(JVM_DATA_MODEL)
"
HADOOP-840,the task tracker is getting blocked by long deletes of local files,The task tracker is getting blocked in the main heartbeat loop by doing task cleanups in the main thread. We need to queue up the cleanup actions and work on them offline. This should make our task trackers not miss heartbeats and thus get lost.
HADOOP-838,TaskRunner.run() doesn't pass along the 'java.library.path' to the child (task) jvm,Since TaskRunner.run doesn't pass along java.library.path of the parent (TT) to the child (task) the native library path (if needed) is missing and hence it cannot load the native library. The fix is to do a System.getProperty and add it to the child jvm's arguments.
HADOOP-837,RunJar should unpack jar files into hadoop.tmp.dir,RunJar currently unpacks a jar file into the default system temp directory. RunJar sometimes can not proceed because the disk space is running out. It would be better to unpack the jar file into a configurable temp directory like hadoop.tmp.dir.
HADOOP-836,unit tests fail on windows (/C:/cygwin/... is invalid),"Under windows, I get the following exception from some of the unit tests:

java.io.IOException: Invalid file name: /C:/cygwin/home/hadoopwin/owen/hadoop/build/test/mapre
d/local/50062_0/taskTracker/jobcache/job_0001/task_0001_m_000002_0/.split.dta.crc
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:416)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:238)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)

"
HADOOP-835,conf not set for the default Codec when initializing a Reader for a record-compressed sequence file,"Because the conf field of a default Codec does not get set when initializing a reader for a record-compressed sequence file, a NullPointerException is thrown when attempting to create an input stream for the default Codec as shown in the following stack trace:

java.lang.NullPointerException
	at org.apache.hadoop.io.compress.DefaultCodec.createInputStream(DefaultCodec.java:59)
	at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1004)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:927)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:918)
	at org.apache.hadoop.mapred.SequenceFileRecordReader.(SequenceFileRecordReader.java:39)
	at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:55)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:180)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1388)
"
HADOOP-833,need documentation of native build requirements,"We should document the requirements for building the native libraries.

A link to a wiki page should be added to src/java/overview.html.  The wiki page should provide instructions and requirements for building the native code on various platforms.  Ideally Fedora/Redhat, Ubuntu, Windows and Mac instructions should be provided.
"
HADOOP-831,A command for locking the JobTracker to stop receiving new job requests.,"When I want to restart or upgrade the JobTracker, it'll be useful to have a feature to lock the JobTracker so that it won't accept any more jobs.  This way, I can wait for the current running jobs to finish and then safely restart the MR cluster."
HADOOP-830,Improve the performance of the Merge phase,"This issue is about trying to improve the performance of the merge phase (especially on the reduces). Currently, all the map outputs are copied to disk and then the merge phase starts (just to make a note - sorting happens on the maps).

The first optimization that I plan to implement is to do in-memory merging of the map outputs. There are two buffers maintained - 
1) a scratch buffer for writing map outputs (directly off the socket). This is a first-come-first-serve buffer (as opposed to strategies like best fit). The map output copier copies the map output off the socket and puts it here (assuming there is sufficient space in the buffer).
2) a merge buffer - when the scratch buffer cannot accomodate any more map output, the roles of the buffers are switched - that is, the scratch buffer becomes the merge buffer and the merge buffer becomes the scratch buffer. We avoid copying by doing this switch of roles. The copier threads can continue writing data from the socket buffer to the current scratch buffer (access to the scratch buffer is synchronized). 

Both the above buffers are of equal sizes configured to have default values of 100M.

Before switching roles, a check is done to see whether the merge buffer is in use (merge algorithm is working on the data there). We wait till the merge buffer is free. The hope is that while merging we are reading key/value data from an in-memory buffer and it will be really fast and so we won't see client timeouts on the server serving the map output. However, if they really timeout, the client sees an exception, and resubmits the request to the server.

With the above we are doing copying/merging in parallel.

The merge happens and then a spill to disk happens. At the end of the in-memory merge of all the map outputs, we will end up with ~100M files on disk that we will need to merge. Also, the in-memory merge gets triggered when the in-memory scratch buffer has been idle too long (like 30 secs), or, the number of outputs copied so far is equal to the number of maps in the job, whichever is earlier. We can proceed with the regular merge for these on-disk-files and maybe we can do some optimizations there too (haven't put much thought there).

If the map output can never be copied to the buffer (because the map output is let's say 200M), then that is directly spilled to disk.

To implement the above, I am planning to extend the FileSystem class to provide an InMemoryFileSystem class that will ease the integration of the in-memory scratch/merge with the existing APIs (like SequenceFile, MapOutputCopier) since all them work with the abstractions of FileSystem and Input/Output streams.

Comments?"
HADOOP-829,Separate the datanode contents that is written to the fsimage vs the contents used in over-the-wire communication,"In the existing implementation, the Namenode has a data structure called DatanodeDescriptor. It is used to serialize contents of the Datanode while writing it to the fsimage. The same serialization method is used to send a Datanode object to the Datanode and/or the Client. This introduces the shortcoming that one cannot introduce non-persistent fields in the DatanodeDescriptor.

One solution is to separate out the following two functionality into two separate classes:
1. The fields from a Datanode that are part of the ClientProtocol and/or DatanodeProcotol.
2. The fields from a Datanode that are stored in the fsImage."
HADOOP-827,turn off speculative execution by default,"Currently, speculative execution reduces reliability. I want to turn it off by default until it works well."
HADOOP-826,job tracker WI doesnt display shuffle finish time correctly.,It displays current time instead of finish time. 
HADOOP-825,"If the default file system is set using the new uri syntax, the namenode will not start","The namenode tries to parse the string manually by looking for the first "":"" and translating the rest of the string into an integer. This is great, if it is ""<host>:<port>"", but not if it is ""hdfs://<host>:<port>/"""
HADOOP-824,DFSShell should become FSShell,"DFSShell is entirely based on the public FileSystem API.  It would be useful to have a generic shell for the FileSystem API.  Thus I propose that DFSShell be renamed org.apache.hadoop.fs.FsShell.
"
HADOOP-823,DataNode will not start up if any directories from dfs.data.dir are missing,"Minor bug - the list of good directories is generated, but the original list of directories is still used."
HADOOP-822,"Web UI should display the split info (filename, start/end position) of each map task",
HADOOP-821,"Web UI should display the split info (filename, start/end position) of each map task",
HADOOP-819,LineRecordWriter should not always insert tab char between key and value,"
With the current implementation of LineRecordWriter in TextOutputFormat, the client cannot pass null key/or value to the write function, and a tab char is always inserted between  the key and value. This works fine most time. However, in some 
cases, one just does not want to have the extra tab char. A common example is that, if I need to implement a utility similar 
to the unix sort with some fields in the lines as the sort key, I can have my map to extract the sort key from each line and pass the whole line as the value. The reducer just outputs the values and ignore the keys. However, if I use TextOutputFormat, my output will have an extra tab key in each of the lines, which is annoying. 

A simple solution is that let the write function of LineRecordWriter accept null key argument, and write out the value only if the key is null. "
HADOOP-818,ant clean test-contrib doesn't work,"When you run ""ant clean test-contrib"" it fails because the test-contrib counts on build/test directories having been created already."
HADOOP-816,Allow the sort benchmark to set a buffersize for the map buffer,"Discovered that framework merges are the hotspots where most time is spent in the sort benchmark. With HADOOP-331, the Map phase could potentially do a merge of the spills (this merge was not done pre-HADOOP-331), and then there is one compulsory merge on each reduce. It may be good to avoid the merge in the Map phase, if possible."
HADOOP-815,Investigate and fix the extremely large memory-footprint of JobTracker,"The JobTracker's memory footprint seems excessively large, especially when many jobs are submitted.

Here is the 'top' output of a JobTracker which has scheduled ~1k jobs thus far:

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                     
31877 arunc     19   0 2362m 261m  13m S 14.0 12.9  24:48.08 java  

Clearly VIRTual memory of 2364Mb v/s 261Mb of RESident memory is symptomatic of this issue..."
HADOOP-814,Increase dfs scalability by optimizing locking on namenode.,"The current dfs namenode encounters locking bottlenecks when the number of datanodes is large. The namenode uses a single global lock to protect access to data structures. One key area is heartbeat processing. The lower the cost of processing a heartbeat, more the number of nodes HDFS can support.  A simple change to this current locking model can increase the scalability. Here are the details:

Case 1: Currently we have three locks, the global lock (on FSNamesystem), the heartbeat lock and the datanodeMap lock. The following function is called when a heartbeat is received by the Namenode

public synchronized FSNamesystem. gotHeartbeat() { ........ (A)
    synchronized (heartbeat) {                                        ........ (B)
      synchronized (datanodeMap) {                               ......... (C)
   ...
     }
}

In the above piece of code, statement (A) acquires the global-FSNamesystem-lock. This synchronization can be safely removed (remove updateStats too). This means that a heartbeat from the datanode can be processed without holding the FSnamesystem-global-lock.

Case 2: A following thread called the heartbeatCheck thread periodically traverses all known Datanodes to determine if any of them has timed out. It is of the following form:

void FSNamesystem.heartbeatCheck() {
            synchronized (this) {                                        ........... (D)
                        synchronized (heartbeats) {                .............(E) 
}

This thread acquires the global-FSNamesystem lock in Statement (D). This statement (D) can be removed. Instead the loop can check to see if any nodes are dead. If a dead node is found, only then it acquires the FSNamesystem-global-lock.

It is possible that fixing the above two cases will cause HDFS to scale to higher number of nodes.

 "
HADOOP-813,map tasks lost during sort,"On a 500 node cluster, I had a bunch of map tasks get ""lost"" because they failed to report progress for 10 minutes. They appear to be in the sort stage at the end of the map. I hypothesize that the patch for HADOOP-331 does not update the map's progress during the sort/merge. If the sort/merge takes more than 10 minutes, the task is lost."
HADOOP-812,Make DfsPath a public class,I would like to get the filesize of a particular HDFS file from an external program. This functionality is exported through the DfsPath class. I would like to make this class a public class.
HADOOP-811,Patch to support multi-threaded MapRunnable,"The MapRunner calls Mapper.map in a serialized fashion.

This is suitable for CPU/memory bound operations.

However, when doing IO bound operations this serialization affects the throughput significantly.

In order to support IO bound operations more efficiently I've implemented a multithreaded MapRunnable.

Following is the implementation of this MapRunnable, MultithreadedMapRunner.

I've only had to modify on method in the existing code (in the Task class) to avoid data corruption in the reporter.

Index: Task.java
===================================================================
--- Task.java   (revision 485492)
+++ Task.java   (working copy)
@@ -153,9 +153,11 @@
   public Reporter getReporter(final TaskUmbilicalProtocol umbilical,
                               final Progress progress) throws IOException {
     return new Reporter() {
-        public void setStatus(String status) throws IOException {
-          progress.setStatus(status);
-          progress();
+        public void setStatus(String status) throws IOException {
+          synchronized (this) {
+            progress.setStatus(status);
+            progress();
+          }
         }
         public void progress() throws IOException {
             reportProgress(umbilical);

-----------------------------------------------------------
MultithreadedMapRunner.java

package org.apache.hadoop.mapred;

import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.Writable;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import java.io.IOException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;

/**
 * Multithreaded implementation for @link org.apache.hadoop.mapred.MapRunnable.
 * <p>
 * It can be used instead of the default implementation,
 * @link org.apache.hadoop.mapred.MapRunner, when the Map operation is not CPU
 * bound in order to improve throughput.
 * <p>
 * Map implementations using this MapRunnable must be thread-safe.
 * <p>
 * The Map-Reduce job has to be configured to use this MapRunnable class (using
 * the <b>mapred.map.runner.class</b> property) and
 * the number of thread the thread-pool can use (using the
 * <b>mapred.map.multithreadedrunner.threads</b> property).
 * <p>
 *
 * @author Alejandro Abdelnur
 */
public class MultithreadedMapRunner implements MapRunnable {
  private static final Log LOG =
      LogFactory.getLog(MultithreadedMapRunner.class.getName());

  private JobConf job;
  private Mapper mapper;
  private ExecutorService executorService;
  private IOException ioException;

  public void configure(JobConf job) {
    int numberOfThreads =
        job.getInt(""mapred.map.multithreadedrunner.threads"", 10);
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Configuring job "" + job.getJobName() +
          "" to use "" + numberOfThreads + "" threads"" );
    }

    this.job = job;
    this.mapper = (Mapper)ReflectionUtils.newInstance(job.getMapperClass(),
                                                      job);

    // Creating a threadpool of the configured size to execute the Mapper
    // map method in parallel.
    executorService = Executors.newFixedThreadPool(numberOfThreads);
  }

  public void run(RecordReader input, OutputCollector output,
                  Reporter reporter)
    throws IOException {
    try {
      // allocate key & value instances these objects will not be reused
      // because execution of Mapper.map is not serialized.
      WritableComparable key = input.createKey();
      Writable value = input.createValue();

      while (input.next(key, value)) {

        // Run Mapper.map execution asynchronously in a separate thread.
        // If threads are not available from the thread-pool this method
        // will block until there is a thread available.
        executorService.execute(
            new MTMapperRunable(key, value, output, reporter));

        // Checking if a Mapper.map within a Runnable has generated an
        // IOException. If so we rethrow it to force an abort of the Map
        // operation thus keeping the semantics of the default
        // implementation.
        if (ioException != null) {
          throw ioException;
        }

        // Allocate new key & value instances as mapper is running in parallel
        key = input.createKey();
        value = input.createValue();
      }

      if (LOG.isDebugEnabled()) {
        LOG.debug(""Finished dispatching all Mappper.map calls, job ""
            + job.getJobName());
      }

      // Graceful shutdown of the Threadpool, it will let all scheduled
      // Runnables to end.
      executorService.shutdown();

      try {

        // Now waiting for all Runnables to end.
        while (!executorService.awaitTermination(100, TimeUnit.MILLISECONDS)) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(""Awaiting all running Mappper.map calls to finish, job ""
                + job.getJobName());
          }

          // Checking if a Mapper.map within a Runnable has generated an
          // IOException. If so we rethrow it to force an abort of the Map
          // operation thus keeping the semantics of the default
          // implementation.
          // NOTE: while Mapper.map dispatching has concluded there are still
          // map calls in progress.
          if (ioException != null) {
            throw ioException;
          }
        }

        // Checking if a Mapper.map within a Runnable has generated an
        // IOException. If so we rethrow it to force an abort of the Map
        // operation thus keeping the semantics of the default
        // implementation.
        // NOTE: it could be that a map call has had an exception after the
        // call for awaitTermination() returing true. And edge case but it
        // could happen.
        if (ioException != null) {
          throw ioException;
        }
      }
      catch (IOException ioEx) {
        // Forcing a shutdown of all thread of the threadpool and rethrowing
        // the IOException
        executorService.shutdownNow();
        throw ioEx;
      }
      catch (InterruptedException iEx) {
        throw new IOException(iEx.getMessage());
      }

    } finally {
        mapper.close();
    }
  }


  /**
   * Runnable to execute a single Mapper.map call from a forked thread.
   */
  private class MTMapperRunable implements Runnable {
    private WritableComparable key;
    private Writable value;
    private OutputCollector output;
    private Reporter reporter;

    /**
     * Collecting all required parameters to execute a Mapper.map call.
     * <p>
     *
     * @param key
     * @param value
     * @param output
     * @param reporter
     */
    public MTMapperRunable(WritableComparable key, Writable value,
        OutputCollector output, Reporter reporter) {
      this.key = key;
      this.value = value;
      this.output = output;
      this.reporter = reporter;
    }

    /**
     * Executes a Mapper.map call with the given Mapper and parameters.
     * <p>
     * This method is called from the thread-pool thread.
     *
     */
    public void run() {
      try {
        // map pair to output
        MultithreadedMapRunner.this.mapper.map(key, value, output, reporter);
      }
      catch (IOException ex) {
        // If there is an IOException during the call it is set in an instance
        // variable of the MultithreadedMapRunner from where it will be
        // rethrown.
        synchronized (MultithreadedMapRunner.this) {
          if (MultithreadedMapRunner.this.ioException == null) {
            MultithreadedMapRunner.this.ioException = ex;
          }
        }
      }
    }
  }

}


"
HADOOP-810,The non-existence of the job.xml file causes a LOT of tasks of the job fail,"
For some reason, the job.xml file disappears from one of the nodes. That causes the following exception:

java.lang.RuntimeException: java.lang.RuntimeException:  taskTracker/jobcache/job_0039/job.xml not found
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:551)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:472)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:453)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:201)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:286)
	at org.apache.hadoop.mapred.JobConf.getKeepFailedTaskFiles(JobConf.java:212)
	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.setJobConf(TaskTracker.java:874)
	at org.apache.hadoop.mapred.TaskTracker.launchTaskForJob(TaskTracker.java:313)
	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:307)
	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:691)
	at org.apache.hadoop.mapred.TaskTracker.checkForNewTasks(TaskTracker.java:533)
	at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:448)
	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:720)
	at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1374)


That is fine by itself.
The problem is the task tracker keeps accepting tasks of the same job, and keeps failing them!
The correct behavior is that it either tries to get the job.xml from the job tracker, or refuses accepting any new tasks 
of the same job.

"
HADOOP-809,Streaming should have a built in identity mapper,"Currently, in order to merge 2 inputs , or to feed the output of type-changing reduce to another reduce, one needs to use 
-mapper /bin/cat

Streaming already has identity reducer implemented.
Adding identity mapper will make it more conceptually clean, more efficient, and hopefully more reliable."
HADOOP-807,splitting DFS version number,"In the current code, there is a enum named DFS_CURRENT_VERSION. It is being used for three different purposes:

1. It is stored in the fsimage and represents the format of the fsimage.
2. It is stored in the ""storage"" file by the Datanodes. Thus, it represents the format of data on Datanodes.
3. It is used as a version of the Datanode Protocol.

The current implementation makes it difficult to change the fsimage format without affecting the Datanode Protocol. My proposal is to introduce three new constants, one for each of the functionality described above.

In the current code, we have:
    public static final int DFS_CURRENT_VERSION = -3;

Instead, we can have:
    public static final int DFS_FSIMAGE_CURRENT_VERSION = -3;
    public static final int DFS_DATANODEPROTOCOL_CURRENT_VERSION = -3;
    public static final int DFS_DATASTORAGE_CURRENT_VERSION = -3;

The first one is associated with the format of the fsimage/edits; the second one is associated with the communication procotol between the namenode and the datanode; the third one is associated with the format of the datastore on the datanodes (e.g. directory fanout).

Please comment."
HADOOP-806,NameNode WebUI : Include link to each of datanodes,"Minor improvement to Namenode webui.

Datanode in node table on webui should be a link to datanode's frontpage. Note that ""Browse the filesystem"" link pics a datanode randomly.

"
HADOOP-805,JobClient should print the Task's stdout and stderr to the clients console,"The job client should be configurable to print stdout/stderr for Tasks to the user's console.

JobClient:

public static enum TaskStatusFilter { NONE, FAILED, SUCCEEDED};

public void setTaskOutputFilter(TaskStatusFilter newValue);
public TaskStatusFilter getTaskOutputFilter();

"
HADOOP-804,"Cut down on the ""mumbling"" in the Task process' stdout/stderr","Currently there is a lot of useless mumbling that Tasks do when running that needs to be removed. For example, here is a list of the output from a map:

06/12/07 16:45:48 INFO mapred.TaskTracker: Child starting
06/12/07 16:45:48 INFO conf.Configuration: parsing jar:file:/local/owen/hadoop-0.8.1-dev/hadoop-0.8.1-dev.jar!/hadoop-default.xml
06/12/07 16:45:48 INFO conf.Configuration: parsing file:/local/owen/conf/mapred-default.xml
06/12/07 16:45:48 INFO ipc.Client: org.apache.hadoop.io.ObjectWritableConnection culler maxidletime= 1000ms
06/12/07 16:45:48 INFO ipc.Client: org.apache.hadoop.io.ObjectWritable Connection Culler: starting
06/12/07 16:45:48 INFO conf.Configuration: parsing jar:file:/local/owen/hadoop-0.8.1-dev/hadoop-0.8.1-dev.jar!/hadoop-default.xml
06/12/07 16:45:48 INFO conf.Configuration: parsing file:/local/owen/conf/mapred-default.xml
06/12/07 16:45:48 INFO conf.Configuration: parsing /local/owen/run/tasktracker/taskTracker/jobcache/job_0002/task_0002_m_000000_0/job.xml
06/12/07 16:45:49 INFO conf.Configuration: parsing jar:file:/local/owen/hadoop-0.8.1-dev/hadoop-0.8.1-dev.jar!/hadoop-default.xml
06/12/07 16:45:49 INFO conf.Configuration: parsing file:/local/owen/conf/mapred-default.xml
06/12/07 16:45:49 INFO mapred.MapTask: opened part-0.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-1.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-2.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-3.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-4.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-5.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-6.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-7.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-8.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-9.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-10.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-11.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-12.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-13.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-14.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-15.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-16.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-17.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-18.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-19.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-20.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-21.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-22.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-23.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-24.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-25.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-26.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-27.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-28.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-29.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-30.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-31.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-32.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-33.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-34.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-35.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-36.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-37.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-38.out
06/12/07 16:45:49 INFO mapred.MapTask: opened part-39.out

Furthermore, it seems like the default logging level for Task subprocesses should be set to WARN."
HADOOP-803,Reducing memory consumption on Namenode : Part 1,"
There appears to be some places in Namenode that allow reducing memory consumption without intrusive code or feature changes. This bug is an initial attempt making those changes. Please include your thoughts as well. 

One change I am planning to make : 

Currently one copy of each block exists for each of the replicas and one copy for blockMap. I think they are all supposed to be same.
"
HADOOP-802,mapred.speculative.execution description in hadoop-defauls.xml is not complete,"<property>
  <name>mapred.speculative.execution</name>
  <value>true</value>
  <description>If true, then multiple instances of some map tasks may
  be executed in parallel.</description>
</property>

This property also controls speculative execution of reduces, however the description field doesn't indicate that."
HADOOP-801,job tracker should keep a log of task completion and failure,"The JobTracker should track a list of task completion events in JobInProgress.

So JobClientProtocol & InterTrackerProtocol should get a new method:

  TaskCompletionEvent[] getTaskCompletionEvents(String jobid, int fromEventId) throws IOException;

TaskCompletionEvent should have:
  int getEventId();
  String getTaskTrackerHttp();
  String getTaskId();
  static public enum Status {FAILED, SUCCEEDED};
  Status getTaskStatus();
}

The events will be stored in a List<TaskCompletionEvent> and the eventId is the position in the list.

These event logs will allow JobClient to display task output to the user as well as provide the start of the fix for HADOOP-248."
HADOOP-800,More improvements to DFS browsing WI,"
(a) the TITLE of web pages is
     HDFS: <directory name>
or
     HDFS: <file_name>
for directories and files, correspondingly.
trimmed from the right, if too long.
(This helps finding browser windows, tabs, navigating history, and creating bookmarks)

(b) instead of or in addition to ""go to parent directory"" link, each element of the path displayed at the top of the page should  link to the corresponding directory.  
This also saves a line in display.

(c) the file browsing page should not automatically refresh.

(d) it would be nice to use the whole window rather than the text box for viewing.
Wrapping is often harmful.  
(These may be a radio-button options option saved in a cookie)
"
HADOOP-799,NullPointerException in FSDataset,"Running the smallJobsBenchmark this morning, I got this NPE in one of the datanode logs:

2006-12-07 10:58:09,541 INFO org.apache.hadoop.dfs.DataNode: Deleting block blk_-4859335730964457601
2006-12-07 10:58:21,548 ERROR org.apache.hadoop.dfs.DataNode: Exception: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:519)
        at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:386)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1007)
        at java.lang.Thread.run(Thread.java:595)
"
HADOOP-797,Streaming throws out a nullpointer exception if the mapper/reducer is not specified correctly.,
HADOOP-796,Node failing tasks and failed tasks should be more easily accessible through jobtracker history.,"JobTracker history should provide quick access to nodes that failed tasks and the tasks that failed. Currently its not very easy to dig through details. 
"
HADOOP-794,JobTracker crashes with ArithmeticException,"The sort benchmark on 20 nodes failed for me with an ArithmeticException.  Sort.java calculates the number of maps and reduces dynamically based on the cluster size.  I'm guessing at the time it did the calculation, the jobtracker had heard from 0 task trackers (which seems odd since I wait 1 minute after starting the jobtracker before starting Sort).

From the job output:
Running on 0 nodes to sort from /sortBenchmark20/input into /sortBenchmark20/output with 0 reduces.

From the JobTracker log:
2006-12-07 07:10:06,709 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:
java.lang.ArithmeticException: / by zero
        at org.apache.hadoop.mapred.InputFormatBase.getSplits(InputFormatBase.java:130)
        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:125)
        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:326)
        at java.lang.Thread.run(Thread.java:595)

The jobtacker should check for illegal inputs like this and not fall over.
"
HADOOP-793,SequenceFile.Reader does not set Configuration on DefaultCodec in init(),init() in SequenceFile.Reader should call setConf(conf) on the newly created DefaultCodec
HADOOP-792,Invalid dfs -mv can trash your entire dfs,"If the target path of the dfs -mv command exists within the source path, the dfs becomes corrupt. For example:

% hadoop dfs -mkdir target
% hadoop dfs -mv / target

I'm not certain whether this is reproducible in the current trunk, but I'd bet that it is.

This problem successfully circumvented my own patch to make dfs -rm a little safer (see my email c.2006-08-30 to nutch-dev for details). I had been deleting old crawl directories from the DFS by copying their names and pasting them into my command buffer. At one point, I paused to do something else, copied some other text (which unfortunately began with a Java comment and included carriage returns), then went back to removing the crawl directories. I must not have pressed hard enough on the ""c"" key when I did my next copy, since when I pasted into the command buffer, hadoop immediately began executing a dfs -rm / command. No problem - I'm protected, because my patched dfs command is just going to try to move / to /trash (and fail), right?

Wrong! Even though hadoop isn't really capable of such a move, it apparently tries hard enough to corrupt the namenode's DB.

Thankfully, I ran into this problem at a relatively opportune time, when the contents of my dfs had little value."
HADOOP-791,deadlock issue in taskstracker.,"the stack trace--
""main"":
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.createStatus(TaskTracker.java:880)
        - waiting to lock <0xea101658> (a org.apache.hadoop.mapred.TaskTracker$TaskInProgress)
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:489)
        - locked <0x75505f00> (a org.apache.hadoop.mapred.TaskTracker)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:442)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:720)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1374)
""taskCleanup"":
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.cleanup(TaskTracker.java:1072)
        - waiting to lock <0x75505f00> (a org.apache.hadoop.mapred.TaskTracker)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:1013)
        - locked <0xea101658> (a org.apache.hadoop.mapred.TaskTracker$TaskInProgress)
        at org.apache.hadoop.mapred.TaskTracker$1.run(TaskTracker.java:144)
        at java.lang.Thread.run(Thread.java:595)
Found 1 deadlock.

The jobhasfinished method and transmitHeart beat lock the tasktracker and tip in a different order. Also , before emitting HeartBeat we should be updating the status and removing entries from runningtasks. Currently this is done after the heartbeat."
HADOOP-790,Jobtracker should be able to return the list of tasktrackers/info ports via http get in a xml format,
HADOOP-788,Streaming should use a subclass of TextInputFormat for reading text inputs.,"Currently streaming uses a lot of custom code for processing text inputs. 

I propose:

 1. Move class LineRecordReader  out of TextInputFormat.
 2. Make class StreamLineRecordReader extend LineRecordReader.
 3. StreamLineRecordReader uses LineRecordReader.next to read the lines and splits them on tab to generate a Text/Text key/value pair.

This will remove a lot of code from streaming and give it automatic support for the compression codecs that the ""base"" part of Hadoop enjoys. In particular, if the native zlib code is used, it will remove the 2gb limit on compressed files."
HADOOP-787,job tracker should limit the number of completed/failed jobs in RAM,"The job tracker currently runs out of memory when a user runs a lot of small jobs back to back. To bound the amount of RAM that the job tracker consumes recording completed/failed jobs, I think that the JobTracker should limit the number of ""remembered"" jobs per a user to jobtracker.max.remembered.task.per.user (default 100)."
HADOOP-786,PhasedFileSystem should use debug level log for ignored exception.,"For ignored exception while file close, if the file was already closed by user, phased file system should log at debug log level. Its to reduce unnecessary lines from log. "
HADOOP-785,Divide the server and client configurations,"The configuration system is easy to misconfigure and I think we need to strongly divide the server from client configs. 

An example of the problem was a configuration where the task tracker has a hadoop-site.xml that set mapred.reduce.tasks to 1. Therefore, the job tracker had the right number of reduces, but the map task thought there was a single reduce. This lead to a hard to find diagnose failure.

Therefore, I propose separating out the configuration types as:

class Configuration;
// reads site-default.xml, hadoop-default.xml

class ServerConf extends Configuration;
// reads hadoop-server.xml, $super

class DfsServerConf extends ServerConf;
// reads dfs-server.xml, $super

class MapRedServerConf extends ServerConf;
// reads mapred-server.xml, $super

class ClientConf extends Configuration;
// reads hadoop-client.xml, $super

class JobConf extends ClientConf;
// reads job.xml, $super

Note in particular, that nothing corresponds to hadoop-site.xml, which overrides both client and server configs. Furthermore, the properties from the *-default.xml files should never be saved into the job.xml."
HADOOP-784,In caching if localizing the cache throws out an error sometimes the task failure is not updated,if localize cache throws out an exception the release cache throws out a null pointer exception which leads to a situation where the task has failed but it is not reported back to the tasktracker. The tasktrakcer still thinks the tasks are running and this failure is never communicated to the tasktracker.
HADOOP-783,Hadoop dfs -put and -get accept '-' to indicate stdin/stdout,
HADOOP-782,"TaskTracker.java:killOverflowingTasks & TaskTracker.java:markUnresponsiveTasks only put the tip in tasksToCleanup queue, they don't update the runningJobs","Both killOverflowingTasks and markUnresponsiveTasks only put the tip in 'tasksToCleanup' queue, they should also call 'removeTaskFromJob' to update the 'runningJobs' map to reflect the fact that the task is no longer running. Thoughts?
"
HADOOP-781,Remove from trunk things deprecated in 0.10 branch.,
HADOOP-780,ReduceTask.ValueIterator should apply job configuration to Configurable values,"ReduceTask.ValueIterator should apply job configuration to Configurable values. This is similar to the way it's done in ipc.Client.Connection.run().

Some classes in Nutch (such as FetcherOutput and ParseData) rely on this behavior, and if it's not set an NPE is thrown."
HADOOP-779,Hadoop streaming does not work with gzipped input,"When input files are gzipped, StreamLineRecordReader does not take the corect OutputStream to fetch the next record. Instead of using a GzipOutputStream, it uses a FSOutputStream. So input files are read as uncompressed plain text."
HADOOP-778,Path.toString() should retain trailing '/' if passed in constructor,"Does it break something if Path retains trailing '/' if passed in constructor and returns it back in toString() ? 
Looking at Path.toString() JobClient wont have a way of knowing which Paths were intended to be directories and not files, in the context of globbed regex ( HADOOP-619 ). 
"
HADOOP-777,the tasktracker hostname is not fully qualified,The hostname of the tasktracker that is passed around in map reduce in all the UI's is not fully qualified. IT would be nice to have all the url's as fully qualified url's.
HADOOP-775,If the tasktracker kills a task it cleans up the job directory as well,If the tasktracker kills a task it will clean up the job directory as well. So if other tasks of the same job will later fail saying there is no job.xml or job.jar
HADOOP-774,Datanodes fails to heartbeat when a directory with a large number of blocks is deleted,"If a user removes a few files that are huge, it causes the namenode to send BlockInvalidate command to the relevant Datanodes. The Datanode process the blockInvalidate command as part of its heartbeat thread. If the number of blocks to be invalidated is huge, the datanode takes a long time to process it. This causes the datanode to not send new heartbeats to the namenode. The namenode declares the datanode as dead!

1. One option is to process the blockInvalidate as a separate thread from the heartbeat thread in the Datanode. 
2. Another option would be to constrain the namenode to send a max (e.g. 500) blocks per blockInvalidate message.
"
HADOOP-773,In Streaming output block boundaries cut records,I see blocks that start with an incomplete record.
HADOOP-771,Namenode should return error when trying to delete non-empty directory,"Currently, the namenode.delete() method allows recursive deletion of a directory. That is, even a non-empty directory could be deleted using namenode.delete(). To avoid costly programmer errors, the namenode should not remove the non-empty directories in this method. Recursively deleting directory should either be performed with listPaths() followed by a delete() for every path, or with a specific namenode method such as deleteRecursive().
"
HADOOP-770,"When JobTracker gets restarted, Job Tracker History doesn't show the jobs that were running. (incomplete jobs)","When JobTracker fail or get killed manually,  last running jobs won't show up in the Job Track History."
HADOOP-769,Error in crc file after a FileSystem.rename operation.,"I am not sure whats going wrong here but after a file rename operation on a bunch of files, I am losing crc files in the destination directory. It causes exceptions when doing dfs -get or any other operation on the file. 
I added some prints and it seems renameRaw() is called on checksum file but its not created in target path. There are no errors in namenode logs also. 

I will debug this further and more details if i find any. 

FileSystem.java:386 
-------
      Path checkFile = getChecksumFile(src);
        if (exists(checkFile)) { //try to rename checksum
          if(isDirectory(dst)) {
            renameRaw(checkFile, dst);
          } else {
            renameRaw(checkFile, getChecksumFile(dst));                    <=== This is executed. 
          }
        }
"
HADOOP-765,Hadoop Streaming should (optionally) sort on secondary key,"This is related to HADOOP-485

As described in HADOOP-485 and HADOOP-686,  many algorithms need the values to come in specific order.  
(The most prominent is JOIN : in MapReduce implementation of JOIN, the value has to indicate which ""table"" the record comes from.  It is very useful to have records from the smaller ""table"" to come first.)

(a) once HADOOP-485 is implemented, it should be propagated to Streaming so that sorting by secondary is done without writing any code, but just with specifying a parameter.

(b) alternatively, as Hadoop Streaming records are lines of text with key(s) separated from the value by a tab, a simple hack of running a sort on the MERGED input of reduce will work fine.   This may be quite efficient and easy way to implement this important feature without relying on  HADOOP-485.   
"
HADOOP-764,The memory consumption of processReport() in the namenode can be reduced,"The FSNamesystem.processReport() method converts the blocklist for a datanode into an array by calling node.getBlocks(). Although this memory allocation is transient, it could possibly require the garbage-collector to work that much harder. 

The method Block.getBlocks() should be deprecated. Code that currently uses this method should instead iterate over the Collection."
HADOOP-763,NameNode benchmark using mapred is insufficient,"The current namenode benchmark (org.apache.hadoop.examples.NNBench) uses map/reduce to distribute a load on the namenode.  For the purposes of loading the namenode, this model gives insufficient control over job start and failure recovery.  I propose the namenode benchmark be re-written to use slaves.sh directly to execute the namenode benchmark.  The benchmark should also give finer control over the operations executed and the timings reported."
HADOOP-761,Unit tests should cleanup created files in /tmp. It causes tests to fail if more than one users run tests on same machine.,"TestMiniMRLocalFS  test cases creates /tmp/wc/input/, which is not cleaned up, any other user running test on same machine simultaneously or later will see test failures. 
It should either use a temp directory in the build/test or use java's File.createTempFile method for temporary data and then clean it up. "
HADOOP-760,HDFS edits log file corrupted can lead to a major loss of data.,"In one of our test system, our HDFS gets corrupted after the edits log file has been corrupted (i can tell how).

When we restarted the HDFS, the namenode refusses to started with a exception in hadoop-namenode-xxx.out.

Unfortunately, a rm mistake has been done, and I was not able to save somewhere this exception. 

But it was an ArrayIndexOutOfBoundException somewhere in a UTF8 method called from FSEditLog.loadFSEdits.

The result : the namenode was unable to start, the only way to get it fixed was the removing of the edits log file.

As it was on a test machine we do not have any backup, so all files created in the hdfs since the last start of the namenode were lost.

Is there a way to periodically commit changes to the hdfs in fsimage instead of keeping a huge logfile ? (eg every 10 minutes or so.)

Even if the namenode files are rsync'ed, what can be done in that particular case ? (if we periodically rsync the fsimage and its corrupted edits file).

This issue affects the 0.6.1 HDFS version. After looking at the hadoop trunk code, I am not able to says if this can be happening anymore... (I would say yes because of the use of UTF8 class in the same way as in 0.6.1)


"
HADOOP-758,FileNotFound on DFS block file,"While run the sort benchmark a reduce failed with:

java.io.FileNotFoundException: /tmp/hadoop-oom/dfs/tmp/tmp/client-4362164194084664090 (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:106)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1156)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1244)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.fs.FSDataOutputStream$Summer.close(FSDataOutputStream.java:98)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:515)
	at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:71)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:310)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1271)
"
HADOOP-757,"""Bad File Descriptor"" in closing DFS file","Running the sort benchmark, I had a reduce fail with a DFS error:

java.io.IOException: Bad file descriptor
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flushData(DFSClient.java:1128)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1114)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1241)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.fs.FSDataOutputStream$Summer.close(FSDataOutputStream.java:99)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:515)
	at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:71)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:310)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1271)
"
HADOOP-756,new dfsadmin command to wait until safe mode is exited,I would like to have a dfsadmin command that waits until dfs leaves safemode. I want to be able to have my start up scripts wait until dfs is up before starting my jobtracker.
HADOOP-754,Problem with the patch for Hadoop-741,"This is related to Hadoop-753. There seems to be a problem with the patch for Hadoop-741 also. The sort benchmark fails to run with this patch and without this patch (and the Hadoop-723 patch) the benchmark runs to completion (and in the last run without these patches in, the sort benchmark ran with 0 failures on a 355 node cluster).
Maybe these patches makes some other bugs surface."
HADOOP-753,Problem with the patch for Hadoop-723,
HADOOP-752,Possible locking issues in HDFS Namenode,"I have been investigating the cause of random Namenode memory corruptions/memory overflows, etc. Please comment.

 1. The functions datanodeReport() and DFSNodesStatus() do not acquire the global lock.
   This can race with another thread invoking registerDatanode(). registerDatanode()
   can remove a datanode (thru wipeDatanode()) while the datanodeReport thread is
   traversing the list of datanodes. This can cause exceptions to occur.

 2. The blocksMap is protected by the global lock. The setReplication() call does not acquire
   the global lock when it calls proccessOverReplicatedBlock(). This can cause corruption in blockMap.


"
HADOOP-751,Namenode constantly using up 100% CPU,"Trying to figure out the exact cause, but CPU load of the namenode is constantly 100% without any file exchanges.

.log file of the namenode showing the following exception about every 20 seconds.

2006-11-27 16:02:07,199 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020 call error: java.io.IOException: java.util.NoSuchElementException
java.io.IOException: java.util.NoSuchElementException
  at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1029)
  at java.util.TreeMap$KeyIterator.next(TreeMap.java:1058)
  at java.util.AbstractCollection.toArray(AbstractCollection.java:176)
  at org.apache.hadoop.dfs.DatanodeDescriptor.getBlocks(DatanodeDescriptor.java:96)
  at org.apache.hadoop.dfs.FSNamesystem.processReport(FSNamesystem.java:1446)
  at org.apache.hadoop.dfs.NameNode.blockReport(NameNode.java:506)
  at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:585)
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:387)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:518)



On some of the datanodes, .log file is showing

2006-11-27 16:03:57,542 WARN org.apache.hadoop.dfs.DataNode: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.util.NoSuchElementException
  at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1029)
  at java.util.TreeMap$KeyIterator.next(TreeMap.java:1058)
  at java.util.AbstractCollection.toArray(AbstractCollection.java:176)
  at org.apache.hadoop.dfs.DatanodeDescriptor.getBlocks(DatanodeDescriptor.java:96)
  at org.apache.hadoop.dfs.FSNamesystem.processReport(FSNamesystem.java:1446)
  at org.apache.hadoop.dfs.NameNode.blockReport(NameNode.java:506)
  at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:585)
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:387)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:518)

  at org.apache.hadoop.ipc.Client$Connection.run(Client.java:248)
"
HADOOP-750,race condition on stalled map output fetches,"I've seen reduces getting killed because of a race condition in the ReduceTaskRunner.  In the logs it looks like:

2006-11-27 08:40:44,795 WARN org.apache.hadoop.mapred.TaskRunner: Map output copy stalled on http://kry2296.inktomisearch.com:7030/mapOutput?map=task_0001_m_015626_0
...
2006-11-27 09:16:41,361 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000658_0 Need 52 map output(s)
2006-11-27 09:16:41,361 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000658_0 Got 39 known map output location(s); scheduling...
2006-11-27 09:16:41,361 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000658_0 Scheduled 0 of 39 known outputs (0 slow hosts and 39 dup hosts)
...
2006-11-27 09:16:47,071 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000658_0 0.3328575% reduce > copy (28679 of 28720 at 0.76 MB/s) >
...
2006-11-27 09:16:47,338 INFO org.apache.hadoop.mapred.TaskRunner: task_0001_r_000658_0 done copying task_0001_m_015462_0 output from node1
...
2006-11-27 09:36:51,398 INFO org.apache.hadoop.mapred.TaskTracker: task_0001_r_000658_0: Task failed to report status for 1204 seconds. Killing.

Basically, the handling of the stall has a race condition that leaves the fetcher in a bad state. At the end of the fetch, all of the tasks finish and their results never get handled. When the thread times out, all of the map output copiers are waiting for things to fetch and the prepare thread is waiting for results."
HADOOP-749,The jobfailures.jsp gets a NullPointerException after a task tracker has been lost,"The job tracker gets a NullPointerException:

2006-11-27 06:59:25,148 WARN /: /jobfailures.jsp?jobid=job_0001&kind=map:
java.lang.NullPointerException
        at org.apache.hadoop.mapred.jobfailures_jsp.printFailedAttempts(jobfailu
res_jsp.java:57)
        at org.apache.hadoop.mapred.jobfailures_jsp.printFailures(jobfailures_js
p.java:103)
        at org.apache.hadoop.mapred.jobfailures_jsp._jspService(jobfailures_jsp.
java:168)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427
)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicati
onHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:5
67)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplication
Context.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:
244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

when there is a lost task tracker. Since the task tracker was lost, when the jsp tries to get the task tracker object for that task, it gets a null value. The jsp needs to protect itself from nulls.
"
HADOOP-748,the delete in localfilesystem traverses symlinks to delete files which is not expected,the fullyDelete in FileUtil follows symlinks and deletes all the files in the symlinks. This is unexpected behaviour. Creates issues with caching as well.
HADOOP-747,RecordIO compiler does not produce correct Java code when buffer is used as key or value in map,"When buffer type is used as a key type or value type in map, the record IO translator does not produce correct serialization of deserialization code. Patch forthcoming."
HADOOP-746,CRC computation and reading should move into a nested FileSystem,"Currently FileSystem provides both an interface and a mechanism for computing and checking crc files. I propose splitting the crc code into a nestable FileSystem that like the PhasedFileSystem has a backing FileSystem. Once the Paths are converted to URI, this is fairly natural to express. To use crc files, your uris will look like:

crc://hdfs:%2f%2fhost1:8020/ which is a crc FileSystem with an underlying file system of hdfs://host1:8020

This will allow users to use crc files where they make sense for their application/cluster and get rid of the ""raw"" methods."
HADOOP-745,NameNode throws FileNotFoundException: Parent path does not exist on startup,"(with credit to Christian)

The following exception is reported by the namenode upon startup:

2006-11-22 12:18:17,713 ERROR org.apache.hadoop.dfs.NameNode:
java.io.FileNotFoundException: Parent path does not exist: /foo/bar
        at org.apache.hadoop.dfs.FSDirectory$INode.addNode(FSDirectory.java:186)
        at org.apache.hadoop.dfs.FSDirectory.unprotectedMkdir(FSDirectory.java:731)
        at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:254)
        at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:191)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:321)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:230)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:145)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:137)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:585)

Perhaps the 'mkdirs' section

               INode inserted = unprotectedMkdir(cur);
               if (inserted != null) {
                   NameNode.stateChangeLog.debug(""DIR* FSDirectory.mkdirs: ""
                        +""created directory ""+cur );
                   fsImage.getEditLog().logMkDir( inserted );

needs to be synchronized or the unprotectedMkdir method needs to be changed
to allow creating missing parent directories.
"
HADOOP-744,The site docs are not included in the release tar file,"The top-level site directory should be included somewhere in the release, perhaps inside the docs directory."
HADOOP-741,Fix average progress calculation in speculative execution of maps + fix pending issues in speculative execution of reduces,"This issue addresses following  
1. Avg progress in speculative execution of map tasks should not divide / maps.length again in JonInProgress.java:331
2. Normal tasks should get priority over speculative tasks in reduce. JobInProgress.findNewTask should not break if a spec task is found. 
3. Delete directories used by PhasedFileSystem irrespective of whether speculative execution is enable to allow tasks to use PhasedFileSystem independently. "
HADOOP-740,JobTracker does not clean up task entries at job completion,"The JobTracker never calls removeTaskEntry on tasks that don't fail. This causes the JobTracker to use more and more heap space as jobs run. The fix is to:

removeTaskEntry on reduces in completeTask
removeTaskEntry on everything when a job is killed or completes"
HADOOP-739,TestIPC occassionally fails with BindException,"org.apache.hadoop.ipc.TestIPC.testParallel occassionally fails with

java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:161)
	at org.apache.hadoop.ipc.Server.start(Server.java:607)
	at org.apache.hadoop.ipc.TestIPC.testParallel(TestIPC.java:174)
	at org.apache.hadoop.ipc.TestIPC.testParallel(TestIPC.java:164)

My guess is that port 1234 used by TestIPC.testSerial hasn't been cleaned up by the time TestIPC.testParallel tries to use it.  The failure rate of this test seems around 5%.
"
HADOOP-738,dfs get or copyToLocal should not copy crc file,"Currently, when we -get or -copyToLocal a directory from DFS, all the files including crc files are also copied. When we -put or -copyFromLocal again, since the crc files already exist on DFS, this put fails. The solution is not to copy checksum files when copying to local. Patch is forthcoming."
HADOOP-737,TaskTracker's job cleanup loop should check for finished job before deleting local directories,"TaskTracker  uses jobClient.pollForTaskWithClosedJob() to find tasks which should be closed. This mechanism doesnt pass the information on whether the job is really finished or the task is being killed for some other reason( speculative instance succeeded). Since Tasktracker doesnt know this state it assumes job is finished and deletes local job dir, causing any subsequent tasks on the same task tracker for same job to fail with job.xml not found exception as reported in HADOOP-546 and possibly in HADOOP-543. This causes my patch for HADOOP-76 to fail for a large number of reduce tasks in some cases.
 
Same causes extra exceptions in logs while a job is being killed, the first task that gets closed will delete local directories and any other tasks (if any) which are about to get launched will throw this exception. In this case it is less significant is as the job is killed anyways and only logs get extra exceptions. 

Possible solutions : 
1. Add an extra method in InetTrackerProtocol for checking for job status before deleting local directory. 
2. Set TaskTracker.RunningJob.localized to false once the local directory is deleted so that new tasks don't look for it there. 

There is clearly a race condition in this and logs may still get the exception while shutdown but in normal cases it would work. 

Comments ? 
"
HADOOP-736,Roll back Jetty6.0.1 to Jetty5.1.4,"There seems to be a problem with the current version of Jetty (6.0.1) in the case where we are using it for serving map outputs. It becomes very slow after serving a few outputs and doesn't recover from that state. Tried with the current release of Jetty - 6.1.0(pre1), but the problem seems to be there even with that release. So roll back to the previous Jetty version (5.1.4) seems to be right thing to do for now. This is the URL for the Jira issue on Jetty's performance issue: 
http://jira.codehaus.org/browse/JETTY-177"
HADOOP-735,"The underlying data structure, ByteArrayOutputStream,  for buffer type of Hadoop record is inappropriate","With ByteArrayOutputStream as the underlying data structure for a buffer, the user is forced to convert it into a byte [] object in order to do any operations other than sequence append on the buffer. The convertion will create a new copy of bytes. That will cause huge performance problem. 

It seems BytesWritable is a better replacement.
"
HADOOP-734,"Link to the FAQ in the ""Documentation"" section of the hadoop website","We should place commonly discussed topics on the wiki FAQ.
This patch provides a link to the wiki FAQ from hadoop web site.
"
HADOOP-733,dfs shell has inconsistent exit codes,"After upgrading to 0.8.0, some of my script applications stopped to work properly, seemingly because of hadoop dfs utility returning 0 exit code when it should not (kind of revival of hadoop-488, with a different cause).

dfs -cat  and dfs -rm always return exit code 0, even for non-existing files. The former can be traced back to the fact that DFSShell's 'run' method calls a 'doall' method without passing on the exit code ('doall' catches its own exceptions and returns an exit code). The latter occurs because the return code of the DFSClient delete method is only used in DFS Shell to print different messages without affecting exit code.

There might be more inconsistent behavior of the dfs shell. Hadoop dfs command line should return 0 signaling success exactly when the corresponding unix command returns 0 (or at least it should be related to success whatever this means in a documented manner).

I also would recommend to use a kind of regression test to prevent that this gets broken again."
HADOOP-732,SequenceFile's header should allow to store metadata in the form of key/value pairs,"
The sequence file currently stores a fixed list of metadata attributes, such as key/value class names, 
compression method, etc.  To make sequence file more self descriptable, it should allow to store a list of key/value pairs.  One particular attribute of interest is to indicate whether the key/value classes are actually hadoop record classes, 
if so, store the DDls for the records. This way, we may create tools to extract DDl from a sequence file and 
then generate necessary classes. It also make it possible to provide an interpretive version of Hadoop record. 
This way, even in the situation where Hadoop or the application does not have the necessary classes, 
a sequence file of Hadoop records can be read and deserialized ""interpretively"".

"
HADOOP-731,"Sometimes when a dfs file is accessed and one copy has a checksum error the I/O command fails, even if another copy is alright.","for a particular file [alas, the file no longer exists -- I had to progress]  

    $dfs -cp foo bar        

and

    $dfs -get foo local

failed on a checksum error.  The dfs browser's download function retrieved the file, so either that function doesn't check, or more likely the download function got a different copy.

When a checksum fails on one copy of a file that is redundantly stored, I would prefer that dfs try a different copy, mark the bad one as not existing [which should induce a fresh copy being made from one of the good copies eventually], and make the call continue to work and deliver bytes.

Ideally, if all copies have checksum errors but it's possible to piece together a good copy I would like that to be done.

-dk
"
HADOOP-730,Local file system uses copy to implement rename,"There is a variable LocalFileSystem.useCopyForRename that is set to true. When true, the local file system will implement rename as a copy followed by a delete. This is likely a performance problem. Is there a reason that useCopyForRename is set?"
HADOOP-729,packageNativeHadoop.sh has non-standard sh code,"packageNativeHadoop.sh uses the shell check ""-e"" which fails on Solaris; this caused a nightly build to fail."
HADOOP-728,Map-reduce task does not produce correct results when -reducer NONE is specified through streaming,"a) a file is create for the output instead of a directory.
b) there is no way to understand what is going on from the client output

I can produce an example for you, if you like -- but the behavior is consistent, so $HSTREAM -mapper /bin/cat -reducer NONE should show the problem
~
"
HADOOP-727,Hadoop should include a general purpose distributed lock manager,"Related to HADOOP-726. Currently, there is no good way for distributed apps using HDFS or Map/Reduce to synchronize their operations. HDFS locking doesn't work very well and lock management is an unnecessary burden on the namenode. In addition, it depending on the filesystem for locking could make applications less portable. Hadoop should implement a general purpose distributed lock management service."
HADOOP-726,HDFS locking mechanisms should be simplified or removed,"HDFS includes a locking mechanism that allows clients to lock files and directories in the filesystem. This requires that the Namenode, in addition to all other management of filesystem state also has to act like a lock manager on behalf of clients. Rather than burden the Namenode with this Hadoop should include a distinct general purpose distributed lock manager.

In the interim, the locking functionality on the filesystem should be simplified to permit only file locks or removed altogether. At this point, removing or simplifying the functionality is not likely to break client code, removing it later will be much harder. In any case, locks in HDFS don't work very well, see HADOOP-656. Also, directory locks are overkill most UNIX fses don't support it."
HADOOP-725,chooseTargets method in FSNamesystem is very inefficient,"Currently the chooseTargets method (that selects datanodes for block-placement) takes in excess of 20% of cpu on a namenode. This is the most time-consuming namenode method, according to the profiler. This inefficiency has already contributed to cascading crash in DFS earlier. As datanodes went down, new locations needed to be found for the blocks on dead datanodes, and since this was done inside a synchronized method, it locked the whole namesystem for several minutes, which caused more datanode failures, when the namenode marked them dead because no heartbeat could be processed during that interval. This has been detailed in HADOOP-572.

The patch I am about to upload reduces the time taken in the chooseTarget method to be proportional to nReplicas per block, instead of the current implementation, which is proportional to (nDataNodes * nReplicas). Also, when a number of datanodes crash, their blocks are put on the pendingReplications list one datanode at a time in a synchronized section. (Currently, the syncchronized section processes ALL the dead datanodes, thus locking the namesystem for a considerable amount of time.) Also, this patch will add a unit test to check replication.
"
HADOOP-724,"bin/hadoop:111 uses java directly, it should use JAVA_HOME","JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} java org.apache.hadoop.util.PlatformName`

should use JAVA_HOME instead of java."
HADOOP-723,Race condition exists in the method MapOutputLocation.getFile,"There seems to be a race condition in the way the Reduces copy the map output files from the Maps. If a copier is blocked in the connect method (in the beginning of the method MapOutputLocation.getFile) to a Jetty on a Map, and the MapCopyLeaseChecker detects that the copier was idle for too long, it will go ahead and issue a interrupt (read 'kill') to this thread and create a new Copier thread. However, the copier, currently blocked trying to connect to Jetty on a Map, doesn't actually get killed until the connect timeout expires and as soon as the connect comes out (with an IOException), it will delete the map output file which actually could have been (successfully) created by the new Copier thread. This leads to the Sort phase for that reducer failing with a FileNotFoundException.
One simple way to fix this is to not delete the file if the file was not created within this getFile method."
HADOOP-722,native-hadoop deficiencies,"Deficiencies in native-hadoop (HADOOP-538):

a) packageNativeHadoop.sh assumes prebuilt linux libraries have been checked into svn (lib/native/Linux-i382-32)
  This assumption led to latest failure of nightly-build on Nov 14.

b) Some of the scripts introduced by the patch need to have 'svn:executable' property set to '*'
src/native/configure
src/native/config/config.guess
src/native/config/config.sub
src/native/config/depcomp
src/native/config/install-sh
src/native/config/missing
"
HADOOP-721,jobconf.jsp shouldn't find the jobconf.xsl via http,"The jobconf.jsp gets the jobconf.xsl via http, but it is a local file. The file should be on the class path and found via findResource."
HADOOP-720,"Write a white paper on Hadoop File System Architecture, Design and Features","Write a white paper on Hadoop File System Architecture, Design and Features."
HADOOP-719,Integration of Hadoop with batch schedulers,"Hadoop On Demand (HOD) is an integration of Hadoop with batch schedulers like Condor/torque/sun grid etc. Hadoop On Demand or HOD hereafter is a system that populates a Hadoop instance using a shared batch scheduler. HOD will find a requested number of nodes and start up Hadoop daemons on them. Users map reduce jobs can then run on the hadoop instance. After the job is done, HOD gives back the  nodes to the shared batch scheduler. A group of users will use HOD to acquire Hadoop instances of varying sizes and the batch scheduler will schedule requests in a way that important jobs gain more importance/resources and finish fast. Here are a list of requirements for HOD and batch schedulers:

Key Requirements :

--- Should allocate the specified minimum number of nodes for a job 
   Many batch jobs can finish in time, only when enough resources are allocated. Therefore batch scheduler should allocate the asked number of nodes for a given job when the job starts. This is simple form of what's known as gang    scheduling.

  Often the minimum nodes are not available right away, especially if the job asked for a large number. The batch scheduler should support advance reservation for important jobs so that the wait time can be determined. In advance   reservation, a reservation is created on earliest future point when the preoccupied nodes become available. When nodes are currently idle but booked by future reservations, batch scheduler is ok to give them to other jobs to increase system utilization, but only when doing so does not delay existing reservations.

--- run short urgent job without costing too much loss to long job. Especially, should not kill job tracker of long job. 
  Some jobs, mostly short ones, are time sensitive and need urgent treatment. Often, large portion of cluster nodes will be occupied by long running jobs. Batch scheduler should be able to preempt long jobs and run urgent jobs. Then, urgent jobs will finish quickly and long jobs can re-gain the nodes afterward. 

When preemption happens, HOD should minimize the loss to long jobs. Especially, it should not kill job tracker of long job.

--- be able to dial up, at run time, share of resources for more important projects.
  Viewed at high level, a given cluster is shared by multiple projects. A project consists of a number of jobs submitted by a group of users.Batch scheduler should allow important projects to have more resources. This should be tunable at run time as what projects deem more important may change over time. 

--- prevent malicious abuse of the system. 
  A shared cluster environment can be put in jeopardy if malicious or erroneous job code does: 
 -- hold unneeded resources for a long period 
 -- use privileges for unworthy work 
  Such abuse can easily cause under-utilization or starvation of other jobs. Batch scheduler should allow  setting up policies for preventing resource abuse by: 
 -- limit privileges to legitimate uses asking for proper amount 
 -- throttle peak use of resources per player 
 -- monitor and reduce starvation 

--- The behavior should be simple and predictable 
   When status of the system is queried, we should be able to determine what factors caused it to reach current status and what could be the future behavior with or without our tuning on the system. 

--- be portable to major resource managers 
   HOD design should be portable so that in future we are able to plugin other resource manager. 

Some of the key requirements are implemented by the batch schedulers. The others need to be implemented by HOD."
HADOOP-718,JobTracker history should link to failed task attemps for tasks that succeeded eventually.,Jobtracker history should show a link to task attempts that failed but the task later succeeded. These could be killed speculative instances or actual task failures. currently if a task succeeds then job details page doesnt show those tasks as having failed attempts. 
HADOOP-717,"When there are few reducers, sorting should be done by mappers","If I understand correctly, currently, sort happens on the reducer side.
So if few hundred mappers produce few (or many) Gig of data, and there is just ONE reduce to consume it, copying and sorting takes forever.

It may make sense to have a special case optimization for a single reducer.  (E.g. ""when there is only reducer and many mappers, sort is done by the mappers, and reducer does only a merge"")

Or to have some smarter policy that makes sure that sorting uses as many CPUs as it makes sense.   If  the map step has produced data on all the nodes of the cluster, it makes sense to use all the nodes for sorting.
"
HADOOP-716,Javadoc warning in SequenceFile.java,Javadoc gives warning for a particular line in SequenceFile.java
HADOOP-715,build.xml sets up wrong 'hadoop.log.dir' property for 'ant test',"build.xml - line nos. 329-331
      <sysproperty key=""hadoop.log.dir"" value=""${hadoop.log.dir}""/>
      <sysproperty key=""test.src.dir"" value=""${test.src.dir}""/>
      <sysproperty key=""hadoop.log.dir"" value="".""/> 
wrongly overrides 'hadoop.log.dir' causing it to be setup incorrectly."
HADOOP-714,Namenode format does not work when hadoop is installed for first time,"The namenode can not be formatted for the first time installation. There is a bug is the main function of org.apache.hadoop.dfs.NameNode class. The code will skip formatting when the directory for name does not exist. There is a possible fix for the bug

			if (argv.length == 1 && argv[0].equals(""-format"")) {
				boolean aborted = false;
				File[] dirs = getDirs(conf);

				for (int idx = 0; idx < dirs.length; idx++) {
					if (dirs[idx].exists()) {
						System.err.print(""Re-format filesystem in "" + dirs[idx]
								+ "" ? (Y or N) "");
						if (!(System.in.read() == 'Y')) {
							System.err
									.println(""Format aborted in "" + dirs[idx]);
							aborted = true;
						} else {
							System.out.println(""do format""
									+ dirs[idx].getAbsolutePath());
							format(dirs[idx]);
							System.err.println(""Formatted "" + dirs[idx]);
						}
						System.in.read(); // discard the enter-key
					} else {
						System.out.println(""do format""
								+ dirs[idx].getAbsolutePath());
						format(dirs[idx]);
						System.err.println(""Formatted "" + dirs[idx]);
					}
				}

				System.exit(aborted ? 1 : 0);
			}
"
HADOOP-713,dfs list operation is too expensive,"A list request to dfs returns an array of DFSFileInfo. A DFSFileInfo of a directory contains a field called contentsLen, indicating its size  which gets computed at the namenode side by resursively going through its subdirs. At the same time, the whole dfs directory tree is locked.

The list operation is used a lot by DFSClient for listing a directory, getting a file's size and # of replicas, and getting the size of dfs. Only the last operation needs the field contentsLen to be computed.

To reduce its cost, we can add a flag to the list request. ContentsLen is computed If the flag is set. By default, the flag is false."
HADOOP-712,Record-IO XML serialization is broken for control characters,Record I/O does not serialize control characters in XML. Patch is forthcoming.
HADOOP-711,TestSymLink fails when ant is not executed from HADOOP_HOME,"When I exec 'ant -f somePath/build.xml test'
org.apache.hadoop.streaming.TestSymLink fails with the following exception:

junit.framework.AssertionFailedError: java.io.FileNotFoundException: build/test/dfs/tmp/tmp/client-2526974511105483818 (No such file or directory)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:179)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:131)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:907)
	at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:278)
	at org.apache.hadoop.dfs.DistributedFileSystem.createRaw(DistributedFileSystem.java:106)
	at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:58)
	at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:47)
	at org.apache.hadoop.fs.FSDataOutputStream.<init>(FSDataOutputStream.java:148)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:263)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:170)
	at org.apache.hadoop.streaming.TestSymLink.testSymLink(Unknown Source)

	at org.apache.hadoop.streaming.TestSymLink.failTrace(Unknown Source)
	at org.apache.hadoop.streaming.TestSymLink.testSymLink(Unknown Source)"
HADOOP-710,block size isn't honored,"When writing a buffer to an FSOutputStream using write(buffer,offset,length), the block size isn't honored when the block size is less than the buffer size."
HADOOP-709,streaming job with Control characters in the command causes runtime exception in the job tracker,"I run a streaming job with a Ctrl-A character in the command line option as follows:

$HSTREAMING -mapper ""/bin/sort -t'^A'"" -input ""input/*"" -output output

It causes a exeception as listed below. If i run the same command without the Ctrl-A character the command completes successfully.

Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.RuntimeException: org.xml.sax.SAXParseException: Character reference ""&#1"" is an invalid XML character.
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:551)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:472)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:453)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:201)
        at org.apache.hadoop.mapred.JobConf.getUser(JobConf.java:175)
        at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:88)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1016)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:514)

        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:248)

"
HADOOP-708,test-libhdfs.sh does not properly capture and return error status,The BUILD_STATUS variable in test-libhdfs.sh incorrectly attempts to capture the return value of running hdfs_test.
HADOOP-705,IOException: job.xml already exists,"I'm seeing this exception in the JobTracker log.  It's caused the same map to fail enough that the job aborted.

2006-11-09 17:28:37,594 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0004_m_000588_2: Error 
initializing task_0004_m_000588_2:
java.io.IOException: Target /foobar/tmp/mapred/local/taskTracker/jobcache/job_0004/job.xml already exists
        at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:215)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:61)
        at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:192)
        at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:279)
        at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:691)
        at org.apache.hadoop.mapred.TaskTracker.checkForNewTasks(TaskTracker.java:533)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:448)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:720)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1374)
"
HADOOP-704,Reduce hangs at 33%,"I have a MR job that is hanging when the reduce reaches 33%.

Both the map and reduce are no-ops.  The single reducer is continuously trying to retrieve output from a TaskTracker that seems to have a crashed ""Acceptor 50060"" thread.  (Note the thread crash does not seem to be logged anywhere).  The thread dump of the TaskTracker is as follows:

""org.apache.hadoop.dfs.DFSClient$LeaseChecker@1329642"" daemon prio=1 tid=0x085abd68 nid=0x5b37 waiting on condition [0x4e979000..0x4e979f30]
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.dfs.DFSClient$LeaseChecker.run(DFSClient.java:462)
        at java.lang.Thread.run(Thread.java:595)

""org.apache.hadoop.io.ObjectWritable Connection Culler"" daemon prio=1 tid=0x0809fe18 nid=0x5b34 waiting on condition [0x4f1e5000..0x4f1e5eb0]
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ipc.Client$ConnectionCuller.run(Client.java:388)

""IPC Server handler 1 on 50050"" daemon prio=1 tid=0x085b5d30 nid=0x57f8 in Object.wait() [0x4eafd000..0x4eafd130]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x5553ee10> (a java.util.LinkedList)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:490)
        - locked <0x5553ee10> (a java.util.LinkedList)

""IPC Server handler 0 on 50050"" daemon prio=1 tid=0x085b57b0 nid=0x57f7 in Object.wait() [0x4eb7e000..0x4eb7e1b0]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x5553ee10> (a java.util.LinkedList)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:490)
        - locked <0x5553ee10> (a java.util.LinkedList)

""IPC Server listener on 50050"" daemon prio=1 tid=0x083884d8 nid=0x57f6 runnable [0x4ebfe000..0x4ebff030]
        at sun.nio.ch.PollArrayWrapper.poll0(Native Method)
        at sun.nio.ch.PollArrayWrapper.poll(PollArrayWrapper.java:100)
        at sun.nio.ch.PollSelectorImpl.doSelect(PollSelectorImpl.java:56)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x5553f3e8> (a sun.nio.ch.Util$1)
        - locked <0x5553f3d8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x5553f150> (a sun.nio.ch.PollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:224)

""btpool0-1 - Invalidator - /"" prio=1 tid=0x08239ac0 nid=0x57f2 waiting on condition [0x4edfe000..0x4edfef30]
        at java.lang.Thread.sleep(Native Method)
        at org.mortbay.jetty.servlet.AbstractSessionManager$SessionScavenger.run(AbstractSessionManager.java:933)
        at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:475)

""taskCleanup"" daemon prio=1 tid=0x0810fd60 nid=0x57ed in Object.wait() [0x4f6c0000..0x4f6c0e30]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x554dc650> (a java.util.ArrayList)
        at java.lang.Object.wait(Object.java:474)
        at org.apache.hadoop.mapred.TaskTracker$BlockingQueue.take(TaskTracker.java:783)
        - locked <0x554dc650> (a java.util.ArrayList)
        at org.apache.hadoop.mapred.TaskTracker$1.run(TaskTracker.java:143)
        at java.lang.Thread.run(Thread.java:595)

""Low Memory Detector"" daemon prio=1 tid=0x509a54a8 nid=0x57ea runnable [0x00000000..0x00000000]

""CompilerThread1"" daemon prio=1 tid=0x509a40c0 nid=0x57e9 waiting on condition [0x00000000..0x506793d8]

""CompilerThread0"" daemon prio=1 tid=0x509a3138 nid=0x57e8 waiting on condition [0x00000000..0x506fa258]

""AdapterThread"" daemon prio=1 tid=0x509a2170 nid=0x57e7 waiting on condition [0x00000000..0x00000000]

""Signal Dispatcher"" daemon prio=1 tid=0x509a13e0 nid=0x57e6 runnable [0x00000000..0x00000000]

""Finalizer"" daemon prio=1 tid=0x50998880 nid=0x57e5 in Object.wait() [0x5087d000..0x5087dfb0]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x554dca70> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)
        - locked <0x554dca70> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=1 tid=0x509983b8 nid=0x57e4 in Object.wait() [0x508fe000..0x508fee30]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x554c4450> (a java.lang.ref.Reference$Lock)
        at java.lang.Object.wait(Object.java:474)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
        - locked <0x554c4450> (a java.lang.ref.Reference$Lock)

""main"" prio=1 tid=0x0805e608 nid=0x57d2 in Object.wait() [0xdfffc000..0xdfffcd08]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x554dc7b0> (a [I)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:436)
        - locked <0x554dc7b0> (a [I)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:720)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1374)

""VM Thread"" prio=1 tid=0x50996028 nid=0x57e3 runnable

""GC task thread#0 (ParallelGC)"" prio=1 tid=0x08078cc8 nid=0x57df runnable

""GC task thread#1 (ParallelGC)"" prio=1 tid=0x080798d0 nid=0x57e0 runnable

""GC task thread#2 (ParallelGC)"" prio=1 tid=0x0807a4c0 nid=0x57e1 runnable

""GC task thread#3 (ParallelGC)"" prio=1 tid=0x0807b0b0 nid=0x57e2 runnable

""VM Periodic Task Thread"" prio=1 tid=0x509a6a10 nid=0x57eb waiting on condition
"
HADOOP-703,streaming should have a jnuit test that catches any api changes in current working directories,the current working directores were changes in 0.7.2 which broke applications that ran on streaming. We should have a junit test that catches these api changes.
HADOOP-702,DFS Upgrade Proposal,"Currently the DFS cluster upgrade procedure is manual.
http://wiki.apache.org/lucene-hadoop/Hadoop_Upgrade
It is rather complicated and does not guarantee data recoverability in case of software errors or administrator mistakes.
This is a description of utilities that make the upgrade process almost automatic and minimize chance of loosing or corrupting data.
Please see the attached html file for details."
HADOOP-700,bin/hadoop includes in classpath all jar files in HADOOP_HOME,"The hadoop script includes all jars from HADOOP_HOME in the classpath.  This means that all the daemons get the examples and tests included in their class paths.  If a change is made to an example or test, all the daemons have to be restarted to see the affect of the change.  This doesn't seem desirable."
HADOOP-699,"""Browse the filesystem"" link on Name Node redirects to wrong port on DataNode","""Browse the filesystem"" link on NameNode front page does not work since it redirects to Datanode:65535/ . 
 "
HADOOP-698,"When DFS client fails to read from a datanode, the failed datanode is not excluded from target reselection","In the method read(byte buf[ ], int off, int len) of DFSInputStream, when read fails,  it calls ""blockSeekTo"" to reselect a datanode. However, the failed datanode does not feed back to blockSeekTo. The datanode selection algorithm works as follows:
* If the machine that the client is running on has a local copy, return the local machine;
* Otherwise, randomly pick up one location.

When the failed data node info does not feed back to target reselection, this leads to two flaws:
1. When a client fails to read from the local copy, for example, because of the checksum error, the local machine will always be chosen in retries.
2. Random selection may still return the same failed node.

"
HADOOP-697,Duplicate data node name calculation in Datanode constructor,"A data node name is calculated twice when a DataNode object is constructed. One is computed at line 158 in DataNode.java using  the statement:

  InetAddress.getLocalHost().getHostName().

The value is passed the private DataNode constructor but is never used. The name is recomputed at lines 189-192 using the statement:

 DNS.getDefaultHost(
   conf.get(""dfs.datanode.dns.interface"", ""default""), 
   conf.get(""dfs.datanode.dns.nameserver"", ""default""))"".
"
HADOOP-696,TestTextInputFormat fails on some platforms due to non-determinism in format.getSplits(),"TestTextInputFormat depends on format.getSplits() returning splits in the order they were created. On a local filesystem getSplits() depends on File.list(), the order produced by which is non-deterministic. This causes the test to fail on some platforms."
HADOOP-695,Unexpected NPE from the next method of StreamLineRecordReader fails map/reduce jobs,"For some reason that I do not have much idea, if input of a map/reduce job is gzipped, an unexpected null pointer may be returned from UTF8ByteArrayUtils.readline in the next method of StreamLineRecordReader. However the pointer is read before a null pointer check is performed. Thus a NPE may be thrown and fail the job."
HADOOP-694,jobtracker expireluanching tasks throws out Nullpointer exceptions,This happens when a tasktracker has been expired and this thread tries to call failedTask. The tasktrackerStatus becomes null and this thread throws out nullpointerexceptions.
HADOOP-692,Rack-aware Replica Placement,"This issue assumes that HDFS runs on a cluster of computers that spread across many racks. Communication between two nodes on different racks needs to go through switches. Bandwidth in/out of a rack may be less than the total bandwidth of machines in the rack. The purpose of rack-aware replica placement is to improve data reliability, availability, and network bandwidth utilization. The basic idea is that each data node determines to which rack it belongs at the startup time and notifies the name node of the rack id upon registration. The name node maintains a rackid-to-datanode map and tries to place replicas across racks.
"
HADOOP-690,NPE in jobcontrol,"While running a few jobs I got this exception:

Exception in thread ""Thread-10"" java.lang.NullPointerException
        at org.apache.hadoop.mapred.jobcontrol.Job.checkRunningState(Job.java:246)
        at org.apache.hadoop.mapred.jobcontrol.Job.checkState(Job.java:264)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.checkRunningJobs(JobControl.java:211)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:280)

It looks like this was just when all the jobs finished."
HADOOP-689,hadoop should provide a common way to wrap instances with different types into one type,"When two sequence files, which have same Key type but different Value types, are mapped out to reduce, multiple Value types is not allowed. In this case, we need a way to wrap instances with different types into one class type to reduce.

In current code, ObjectWritable is a sole choice. but it costs too many space, because the class declaration will be appended into output file as a string for every Key-value pair."
HADOOP-688,move dfs administrative interfaces to a separate command,"The dfs adminsitrative commands (e.g. safemode, report, etc) should be separate from the dfs user commands. The proposal is to remove the following two commands:

bin/hadoop dfs -report
bin/hadoop dfs -safemode

The above two features will be available through a new command:

bin/hadoop dfsadmin -report
bin/hadoop dfsadmin -safemode

This is done so that there is a clean distinction between cluster administrative commands and users command."
HADOOP-687,Upgrade to Jetty 6 does not patch bin/hadoop,"The upgrade to jetty 6 does not fix 'bin/hadoop' to include new jars. The directory 'lib/hadoop/jetty-ext' has been replaced by 'lib/hadoop/jsp-2.0' however 'bin/hadoop' does not include the jars in the new directory in CLASSPATH. As a result, tasktrackers and namenodes fail to launch.

"
HADOOP-686,job.setOutputValueComparatorClass(theClass) should be supported,"if the input of Reduce phase is :

K2, V3
K2, V2
K1, V5
K1, V3
K1, V4

in the current hadoop, the reduce output could be:
K1, (V5, V3, V4)
K2, (V3, V2)

But I hope hadoop supports job.setOutputValueComparatorClass(theClass), so that i can make values are in order, and the output could be:
K1, (V3, V4, V5) 
K2, (V2, V3)

This feature is very important, I think. Without it, we have to take the sorting by ourselves, and have to worry about the possibility that the values are too large to fit into memory. Then the codes becomes too hard to read. That is the reason why i think this feature is so important, and should be done in the hadoop framework.

"
HADOOP-685,DataNode appears to require DNS name resolution as opposed to direct ip mapping,"DataNode appears to require DNS resolution of nodes via the class org.apache.hadoop.net.DNS as opposed being able to use a specified ip.

as an example, i was not able to set up more then one instance of dfs datanodes on one box using loopback w/ varying ports since DataNode
resolved the ip of 127.0.0.1 to be ""foo.bar"" which was then mapped to the dhcp allocated ip of 192.168.0.***, which was not addressable by the
rest of the dfs cluster (namely namenode).

while this example is trivial one should be able to use the very same process yet change only the ip's of the nodes and have things work as
expected.

it would be nice to not always require nds resolution."
HADOOP-683,bin/hadoop.sh doesn't work for /bin/dash (eg ubuntu 6.10b),"bin/hadoop.sh has a conditional which doesn't work with /bin/dash which ubuntu 6.10b symlinks to /bin/sh.

here's a trivial patch that works for me:

Index: bin/hadoop-daemon.sh
===================================================================
--- bin/hadoop-daemon.sh        (revision 468719)
+++ bin/hadoop-daemon.sh        (working copy)
@@ -56,7 +56,7 @@
 pid=$HADOOP_PID_DIR/hadoop-$HADOOP_IDENT_STRING-$command.pid
 
 # Set default scheduling priority
-if [ ""$HADOOP_NICENESS"" == """" ]; then
+if [ ""$HADOOP_NICENESS"" = """" ]; then
     export HADOOP_NICENESS=0
 fi
"
HADOOP-682,hadoop namenode -format doesnt work anymore if target directory doesnt exist,"Not sure if this was intended, but hadoop namenode -format doesnt work anymore if the target directory doesnt exist due to the check in the NameNode.main(). It used to work earlier 

        if (argv.length == 1 && argv[0].equals(""-format"")) {
          boolean aborted = false;
          File[] dirs = getDirs(conf);
          for (int idx = 0; idx < dirs.length; idx++) {
            if (dirs[idx].exists()) {                     <<<====== if dir doesnt exist it won't format
              System.err.print(""Re-format filesystem in "" + dirs[idx] +"" ? (Y or N) "");
              if (!(System.in.read() == 'Y')) {
                System.err.println(""Format aborted in ""+ dirs[idx]);
                aborted = true;
              } else {
                format(dirs[idx]);
                System.err.println(""Formatted ""+dirs[idx]);
              }
              System.in.read(); // discard the enter-key
            }
          }
          System.exit(aborted ? 1 : 0);
        }


"
HADOOP-681,Adminstrative hook to pull live nodes out of a HDFS cluster,"Introduction
------------
An administrator sometimes needs to bring down a datanode for scheduled maintenance. It would be nice if HDFS can be informed about this event. On receipt of this event, HDFS can take steps so that HDFS data is not lost when the node goes down at a later time.

Architecture
-----------
In the existing architecture, a datanode can be in one of two states: dead or alive. A datanode is alive if its heartbeats are being processed by the namenode. Otherwise that datanode is in dead state. We extend the architecture to introduce the concept of a tranquil state for a datanode.
A datanode is in tranquil state if:
    - it cannot be a target for replicating any blocks
    - any block replica that it currently contains does not count towards the target-replication-factor of that block

Thus, a node that is in tranquil state can be brought down without impacting the guarantees provided by HDFS.

The tranquil state is not persisted across namenode restarts. If the namenode restarts then that datanode will go back to being in the dead or alive state.

The datanode is completely transparent to the fact that it has been labeled as being in tranquil state. It can continue to heartbeat and serve read requests for datablocks.

DFSShell Design
-----------------------
We extend the DFS Shell utility to specify a list of nodes to the namenode.
    hadoop dfs -tranquil {set|clear|get} datanodename1 [,datanodename2]

The DFSShell utility sends this list to the namenode. This DFSShell command invoked with the ""set"" option completes when the list is transferred to the namenode. This command is non-blocking; it returns before the datanode is actually in the tranquil state. The client can then query the state by re-issuing the command with the ""get"" option. This option will indicate whether the datanode is in tranquil state or is ""being tranquiled"". The ""clear"" option is used to transition a tranquil datanode to the alive state. The ""clear"" option is a no-op if the datanode is not in the ""tranquil"" state.

ClientProtocol Design
--------------------
The ClientProtocol is the protocol exported by the namenode for its client.
This protocol is extended to incorporate three new methods:
   ClientProtocol.setTranquil(String[] datanodes)
   ClientProtocol.getTranquil(String datanode)
   ClientProtocol.clearTranquil(String[] datanodes)

The ProtocolVersion is incremented to prevent conversations between imcompatible clients and servers. An old DFSShell cannot talk to the new NameNode and vice-versa.

NameNode Design
-------------------------
The namenode does the bulk of the work for supporting this new feature.

The DatanodeInfo object has a new private member named ""state"". It also has three new member functions:
    datanodeInfo.tranquilStarted(): start the process of tranquilization
    datanodeInfo.tranquilCompleted(): node is not in tranquil state
    datanodeInfo.clearTranquil() : remove tranquilization from node

The namenode exposes a new API to set and clear tranquil states for a datanode. On receipt of a ""set tranquil"" command, it invokes datanodeInfo.tranquilStarted().

The FSNamesystem.chooseTarget() method skips over datanodes that are marked as being in the ""tranquil"" state. This ensures that tranquil-datanodes are never chosen as targets of replication. The namenode does *not* record
this operation in either the FsImage or the EditLogs.

The namenode puts all the blocks from a being-tranquiled node into the neededReplication data structure. Necessary code changes are made to ensure that these blocks get replicated by the regular replication method. As of now, the regular replication code does not distinguish between these blocks and the blocks that are replication candidates because some other datanode might have died. It might be prudent to give different (lower?) weightage to this type of replication requests, but that exercise is deferred to a later date. In this design, replication requests generated because of a node going to a tranquil state are not distinguished from replication requests generated by a datanode going to the dead state.

The DatanodeInfo object has another new private member named ""pendingTranquilCount"". This field stores the remaining number of blocks that still remain to be replicated. This field is valid only if the node is in the ets being-tranquiled state.  On receipt of every 'n' heartbeats from the being-tranquiled datanode, the namenode calculates the amount of data that is still remaining to be replicated and updates the ""pendingTranquilCount"". in the DatanodeInfo.When all the replications complete, the datanode is marked as tranquiled. The number 'n' is selected in such a way that the average heartbeat processing time does not increase appreciably.

It is possible that the namenode might stop receving heartbeats from a datanode that is being-tranquiled. In this case,   the tranquil flag of the datanode gets cleared. It transitions to the dead state and the normal processing for alive-to-dead transition occurs here.

Web Interface
-------------------
The dfshealth.jsp displays the live nodes, dead nodes, being-tranquiled and tranquil nodes. For nodes in the being-tranquiled state, it displays the percentage of tranquilization completed till now.

Issues
--------
1. If a request for tranquilization starts getting processed and there aren't enough space available in DFS to complete the necessary replication, then that node might remain in the being-tranquiled state for a long long time. This is not necessarily a bad thing but is there a better option?

2. We have opted for not storing cluster configuration information in the persistent image of the file system. (The tranquil state of a datanode may be lost if the namenode restarts).
 "
HADOOP-680,checksum errors reading map output,"we get many exceptions complaining about crc errors reading map output by reducers:

org.apache.hadoop.fs.ChecksumException: Checksum error: /hadoop/mapred/local/task_0189_r_000448_3/map_9683.out at 978944
	at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:136)
	at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:112)
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:187)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:313)
	at java.io.DataInputStream.readFully(DataInputStream.java:176)
	at java.io.DataInputStream.readFully(DataInputStream.java:152)
	at org.apache.hadoop.io.SequenceFile$UncompressedBytes.reset(SequenceFile.java:274)
	at org.apache.hadoop.io.SequenceFile$UncompressedBytes.access$700(SequenceFile.java:261)
	at org.apache.hadoop.io.SequenceFile$Reader.nextRaw(SequenceFile.java:1338)
	at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:1600)
	at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:1538)
	at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:1511)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:245)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1210)"
HADOOP-678, Investigate direct buffer leaks and fix them.,"
""direct memory"" leaks are suspected in NameNode running out of memory. At this point I don't have much more information.

Please add any info you have if you have looked in to NameNode memory. I am taking a look at Server.java in hadoop.ipc.
"
HADOOP-677,RPC should send a fixed header and version at the start of connection,"There have been problems with http clients connecting to the RPC servers, which causes the RPC to try and allocate huge buffers and get OutOfMemoryExceptions. I want to have a fixed prefix that is sent first that identifies it as an RPC client. To make the system compatible as much as possible, I'd make the servers accept both forms for a Hadoop release and then change the clients to send the prefix, and finally make the servers only accept the new form."
HADOOP-676,JobClient should print user friendly messages for standard errors,The exceptions for non-existent input dirs or already existing output dirs should generate user-friendly error messages. 
HADOOP-675,streaming should check the command line arguments for validity,Streaming should check the command line arguments using the commons cli parser.
HADOOP-674,Hadoop records should provide comvenient APIs for serialization/deserialization,"I found the following APIs are very convenient to use and should be  part of the generated class from Jute IDL :

public BytesWritable serialize(String format);

public void deserialize(BytesWitable data, String format);

public static MyRecordJT deserialize(BytesWitable data, String format);

"
HADOOP-673,the task execution environment should have a current working directory that is task specific,"The tasks should be run in a work directory that is specific to a single task. In particular, I'd suggest using the <local>/jobcache/<jobid>/<taskid> as the current working directory."
HADOOP-672,dfs shell enhancements," A few enhancements to  dfsshell:

Bring the wiki page up to date with the latest functionality if  that's not done already.
Add a -help option to the command.
Make the syntax line more helpful in identifying which argument to each command needs to
be a directory, a file  or a glob.
"
HADOOP-671,Distributed cache creates unnecessary symlinks if asked for creating symlinks,DistributedCache should check if the symlink has already been created and then only create the symlink. 
HADOOP-670,Generic types for FSNamesystem,"I introduced generic types for FSNamesystem class.
I tried to keep interfaces in declarations of the member as general as possible
in order to have flexibility to change types of instances in the future.
I tried to contain changes within one java file.

I also replaced Vector types by ArrayList. Vectors are synchronized,
which we don't need since we already have top level synchronization in FSNamesystem."
HADOOP-669,Upgrade to trunk causes data loss.,"Current hadoop trunk after applying the multiple namenode image backup patch (patch for hadoop-90) can cause data loss. I have fixed the bug. Patch is forthcoming.
"
HADOOP-668,improvement to DFS browsing WI,"improvement to DFS browsing WI:

1.  Increase the default ""Chunk Size to view"" to 32K bytes.  Currently 
in most cases, I always need to add another digit to the size and get 
the page redisplayed.

2. Add date to the display.  This may an ""approximate"" date -- e.g. the 
date of any block of the file.

3. Remove the directory name from the file name in the file list

4. Let the links in the file list point to the file names, not to the 
file data.  That is, if the files in the directory that is displayed in 
the window have changed, clicking on the file name will still get the 
user to the file, rather than saying ""bad link"" (or smth. like that).  
If you still prefer to link to the file data (there is certain merit to 
this), at least make the link to the removed file say
""sorry, this file has been deleted, here is the link to the current 
version""  or
""sorry, this file has been deleted""

5. replication factor and size for directories is currently meaningless.
use these fields for something meaningful, like total size of directory,
number of files, creation date etc."
HADOOP-667,Path configuration properties should not be comma separated,"A few configuration properties allow multiple directory paths separated by comma's (,).  Since comma is a valid character for a directory name, it should not be used as a path separator.

At a minimum, this applies to these properties:
     mapred.local.dir
     dfs.name.dir

[ I also wonder how robust the implementation is against paths that contain spaces. ]"
HADOOP-666,hadoop dfs  does not complain about unknown option,"No error message displayed when wrong arguments were fed to ""hadoop dfs"" as shown by the snippet at the end of this description.


hadoop dfs -rmr/user/arkady/doremi /user/arkady/fuchun /user/arkady/fuchun_h /user/arkady/log /user/arkady/t /user/arkady/t1
06/10/31 20:11:27 INFO conf.Configuration: parsing jar:file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/tools/hadoop/kryptonite/hadoop-0.7.1.jar!/hadoop-default.xml
06/10/31 20:11:27 INFO conf.Configuration: parsing file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/tools/hadoop/kryptonite/conf/kryptonite2/hadoop-site.xml
06/10/31 20:11:27 INFO ipc.Client: org.apache.hadoop.io.ObjectWritable ConnectionCuller maxidletime=1000ms: starting

nlpdev2:/home/arkady/ngrams> which hadoop
/usr/releng/tools/hadoop/kryptonite/bin/hadoop"
HADOOP-665, hadoop -rmr  does NOT process multiple arguments and does not complain,"hadoop dfs -rmr /user/arkady/fuchun /user/arkady/fuchun_h /user/arkady/log /user/arkady/t /user/arkady/t1
06/10/31 20:15:31 INFO conf.Configuration: parsing jar:file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/
tools/hadoop/kryptonite/hadoop-0.7.1.jar!/hadoop-default.xml
06/10/31 20:15:31 INFO conf.Configuration: parsing file:/nfs/ystools/vol/ystools/releng/build/Linux_2.6_rh4_i686/tools/
hadoop/kryptonite/conf/kryptonite2/hadoop-site.xml
06/10/31 20:15:31 INFO ipc.Client: org.apache.hadoop.io.ObjectWritable
ConnectionCuller maxidletime=1000ms: starting Deleted /user/arkady/fuchun
"
HADOOP-664,build doesn't fail if libhdfs test(s) fail,The build should fail if the C++ test(s) fail.
HADOOP-663,ant test is failing,ant test is failing for contrib
HADOOP-662,"dfs -ls sometime prints ""Found xxx items""; sometimes it does not print that string","I get a different response from dfs -ls depending on whether or not the 
ls contained a directory or a wildcard. The wild card misses the 
response that tells me how many files.

It did indeed cause a problem with my scripts. It was easy to filter 
out. If it's a desired action to do it sometimes and others then that's 
it's ok. It doesn't seem like this is really the intention so I pointed 
it out. My script is now happy with both forms. I do really on the fact 
that the 3 column of the output is the file size. Perhaps I shouldn't be 
doing this but I am writing some automated scripts in python to drive 
hadoop so it is useful to check file sizes. If you were to provide a 
format string that would allow me to place the items in a specific 
format then you would be free to change it, whenever, however, you wanted.
-Richard
"
HADOOP-661,JobConf for a job should be viewable from the web/ui,"It would be very useful to be able to see the JobConf for a job from the web/ui. I would add a link from the job detail page to an xml dump of the JobConf which uses a variant of the configuration.xsl, which I would add to the webapps/static directory, to display it."
HADOOP-660,Format of junit output should be configurable,It would be helpful if the junit output type (text or xml) could be configured by a property. 
HADOOP-659,Boost the priority of re-replicating blocks that are far from their replication target,"I see two types of replications that should be accelerated compared to all others.
1. Blocks that have only one remaining copy (but are required to have higher replication).
2. Blocks that have less than 1/3 of their replicas in place.
The latter occurs when map/reduce sets replication of certain files to 10, and we want
it happen fast to achieve better performance on the tasks.

So I think we should distinguish two major groups of under-replicated blocks:
first-priority (having only 1 copy or less than 1/3 of required replicas), and the rest.
The name-node places first-priority blocks into the beginning of the neededReplication
list, and the rest are placed at the end. That way the first-priority blocks will be replicated
first and then the others.
"
HADOOP-658,source headers must conform to new Apache guidelines,"All Apache releases after November 1st must conform to the new header guidelines, documented at:

http://www.apache.org/legal/src-headers.html
"
HADOOP-657,Free temporary space should be modelled better,"Currently, there is a configurable size that must be free for a task tracker to accept a new task. However, that isn't a very good model of what the task is likely to take. I'd like to propose:

Map tasks:  totalInputSize * conf.getFloat(""map.output.growth.factor"", 1.0) / numMaps
Reduce tasks: totalInputSize * 2 * conf.getFloat(""map.output.growth.factor"", 1.0) / numReduces

where totalInputSize is the size of all the maps inputs for the given job.

To start a new task, 
  newTaskAllocation + (sum over running tasks of (1.0 - done) * allocation) >= 
       free disk * conf.getFloat(""mapred.max.scratch.allocation"", 0.90);

So in English, we will model the expected sizes of tasks and only task tasks that should leave us a 10% margin. With:
map.output.growth.factor -- the relative size of the transient data relative to the map inputs
mapred.max.scratch.allocation -- the maximum amount of our disk we want to allocate to tasks."
HADOOP-656,dfs locking doesn't notify the application when a lock is lost,"DFS locks may be lost for failing to renew the lease on time, but the application is not notified about the loss of the lock and may therefore perform operations assuming it has the lock, even though the lock has been given to another process. I propose that DFS operations check to see if that client has lost a lock since the last check and if so throw a LostLockException."
HADOOP-655,remove deprecations,"Code deprecated in the 0.6 release and earlier should be removed before 0.8 is released.

Can folks think of exceptions?  Probably UTF8.  Any others?"
HADOOP-654,jobs fail with some hardware/system failures on a small number of nodes,"occasionally, such as when the OS is out of some resource, a node fails only partly. The node is up and running, the task tracker is running and sending heartbeats, but every task fails because the tasktracker can't fork tasks or something.
In these cases, that task tracker keeps getting assigned tasks to execute, and they all fail.
A couple of nodes like that and jobs start failing badly.

The job tracker should avoid assigning tasks to tasktrackers that are misbehaving.

simple approach: avoid tasktrackers that report many more failures than average (say 3X). Simply use the info sent by the TT.
better but harder: track TT failures over time and:
 1. avoid those that exhibit a high failure *rate*
 2. tell them to shut down"
HADOOP-653,Hadoop Record csv serialization should not convert Text into String,
HADOOP-652,Not all Datastructures are updated when a block is deleted,"
Currently when a block is deleted, DataNode just deletes the physical file and updates its map. We need to update more things. For e.g. numBlocks in FSDir is not decremented.. effect of this would be that we will create more subdirectories than necessary. It might not show up badly yet since numBlocks gets correct value when the dataNode restarts. I have to see what else needs to be updated.

"
HADOOP-651,fsck does not handle arguments -blocks and -locations correctly,"fsck does not pass arguments correctly to the servlet. As a result, only the last argument (except for the path) is passed to the servlet. Patch is forthcoming."
HADOOP-649,Jobs without any map and reduce operations seems to be lost after their execution,"When a job does not provide any file splits (because there is no data present), the job successes but then it is lost by the job tracker. Is this normal ?"
HADOOP-647,Map outputs can't have a different type of compression from the reduce outputs,"Right now there is only a single knob to control the compression type for sequence files. Sorting and merging is faster with record compression, but the files are smaller with block compression. I'd like to introduce a mapOutputCompressionType that lets the application control how the map outputs are compressed."
HADOOP-646,name node server does not load large (> 2^31 bytes) edits file,"FileInputStream.available() returns negative values when reading a large file (> 2^31 bytes) -- this is a known (unresolved) java bug:
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6402006

Consequence: a large edits file is not loaded and deleted without any warnings. The system reverts back to the old fsimage.

This happens in jdk1.6 as well, i.e. the bug has not yet been fixed.

In addition, when finally I was able to load my big cron-backed-up edits file (6.5 GB)  with a kludgy work-around, the blocks did not exist anymore in the data node servers, probably deleted from the previous attempts when the name node server did not know about the changed situation. 

Moral till this is fixed or worked-around: don't wait too long to restart the name node server. Otherwise this is a way to lose the entire dfs."
HADOOP-645,Map-reduce task does not finish correctly when -reducer NONE is specified,"Map-reduce task does not finish correctly when -reducer NONE is specified, The NONE option means that the reducer should not be generating any output. Using this option causes an exception in the task tracker:

java.lang.IllegalArgumentException: URI is not hierarchical
TaskRunner: at java.io.File.<init>(File.java:335)
TaskRunner: at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:583)
TaskRunner: at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:96)
TaskRunner: at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:49)
TaskRunner: at org.apache.hadoop.mapred.MapTask.run(MapTask.java:213)
TaskRunner: at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1240)
TaskRunner: sideEffectURI_ file:output length 11
"
HADOOP-643,failure closing block of file,"I've been getting ""failure closing block of file"" on random files.
Both datanode and tasktracker running on node7. No problems with pinging.
Guess it got stuck after the NPE in DataNode.

Job cannot start because of:

java.io.IOException: failure closing block of file /home/hadoop/mapred/system/submit_99u9cd/.job.jar.crc to node node7:50010
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:1199)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1163)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1241)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
        at org.apache.hadoop.fs.FSDataOutputStream$Summer.close(FSDataOutputStream.java:96)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
        at org.apache.hadoop.fs.FileUtil.copyContent(FileUtil.java:205)
        at org.apache.hadoop.fs.FileUtil.copyContent(FileUtil.java:190)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:77)
        at org.apache.hadoop.dfs.DistributedFileSystem.copyFromLocalFile(DistributedFileSystem.java:186)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:289)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:314)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:248)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:280)
        at java.lang.Thread.run(Thread.java:595)
Caused by: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)
        at java.io.DataInputStream.readFully(DataInputStream.java:176)
        at java.io.DataInputStream.readLong(DataInputStream.java:380)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:1193)


Cxception in datanode.out on node7:
Exception in thread ""org.apache.hadoop.dfs.DataNode$DataXceiveServer@1c86be5"" java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:162)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:162)
        at org.apache.hadoop.dfs.FSDataset$FSVolume.checkDirs(FSDataset.java:238)
        at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.checkDirs(FSDataset.java:326)
        at org.apache.hadoop.dfs.FSDataset.checkDataDir(FSDataset.java:522)
        at org.apache.hadoop.dfs.DataNode$DataXceiveServer.run(DataNode.java:480)
        at java.lang.Thread.run(Thread.java:595)
"
HADOOP-642,Explicit timeout for ipc.Client,"This bug contributed to the crash discussed in HADOOP-572.
ipc.Client is trying to establish connection with its server with an infinite timeout.
For an unknown to me reason infinity equals 3 minutes in this case.
I guess it is configured somewhere in the native socket implementation.
With this timeout data-nodes had only 3 chances to send heartbeats during the 10
minute expiration interval. And with a very busy name-node this makes their
chances to be accepted close to 0.

I included an explicit call of Socket.connect() with a timeout set to 1 min, which is
our default for all connections.
Modified a log message to include information that turned out to be useful for debugging.
Removed unnecessary imports.
"
HADOOP-641,Name-node should demand a block report from resurrected data-nodes.,"1. This bug contributed to the crash discussed in HADOOP-572.
The problem is that when the name-node is busy, and is not able to process all requests from its clients,
it can consider one of data-nodes dead and discard its blocks sending them into the neededRelications list.
When it finally gets the heartbeat from this data-node it resurrects the node, but not the data-node blocks,
and hence continues to replicate them.
Of course, eventually the name-node will receive the block report from this data-node, but it could take up
to 1 hour. During this time it proceeds with unnecessary block replications, which could be avoided if the
data-node sent its block report right after the resurrection.

I modified code so that the name-node requests block report if it receives a heartbeat from a dead data-node.
I introduced a new command type in the BlockCommand class.
I replaced multiple boolean indicators of the command types by one enum field.
I changed the DatanodeProtocol version.

2. This patch also includes a fix for the data-node registration. If a data-nodes times out during registration
it silently exits, which is hard to notice with a large number of nodes. This patch places registration in a loop,
so that it could retry.
"
HADOOP-639,"task cleanup messages can get lost, causing task trackers to keep tasks forever","If the pollForTaskWithClosedJob call from a job tracker to a task tracker times out when a job completes, the tasks are never cleaned up. This can cause the mini m/r cluster to hang on shutdown, but also is a resource leak."
HADOOP-638,TaskTracker missing synchronization around tasks variable access,TaskTracker has one access to the tasks variable (line 449) that is not synchronized.  It should be.
HADOOP-637,ipc.Server has memory leak -- serious issue for namenode server,"In my environment (running a lot of batch processes each of which reads, creates, and deletes a lof of  files in dfs) the namenode server can run out of memory rather quickly (in a few hours on a 150 node cluster). The netbeans profiler shows an increasing number of direct byte buffers not garbage collected. The documentation on java.nio.ByteBuffer indicates that their allocation might (and obviously does) happen outside the normal gc-collected heap, and, therefore, it is required that direct byte buffers should only be used for long-lived objects.

ipc.Server seems to use a 4KB direct byte buffer for every connection, but, worse, for every RPC call. If I replace the latter ones with non-direct byte buffers, the memory footprint of the namenode server increases only slowly, but even then it is just a matter of time (since I started it 24 hours ago, it leaked by about 300-400MB). If the performance increase by using direct buffers is a requirement, I would suggest to use a static pool.

Although my environment abuses the namenode server in unusual manner, I would imagine that the memory footprint of the namenode server creeps up slowly everywhere"
HADOOP-636,MapFile constructor should accept Progressible,"MapFile's constructor should accept a Progressible and pass it down to the underlying SequenceFiles.  This permits DFS to signal task progress while writing blocks, and can keep reduce tasks from timing out when block writes are slow."
HADOOP-635,"hadoop dfs copy, move commands should accept multiple source files as arguments","hadoop dfs copy, move commands should accept multiple source files as arguments"
HADOOP-634,Test files missing copyright headers,"The tests are missing apache headers:

src/test/org/apache/hadoop/io/compress/TestCodec.java
src/test/org/apache/hadoop/io/TestBytesWritable.java
src/test/org/apache/hadoop/fs/TestLocalFileSystem.java
src/test/org/apache/hadoop/fs/TestGlobPaths.java
src/test/org/apache/hadoop/fs/AccumulatingReducer.java
src/test/org/apache/hadoop/fs/IOMapperBase.java
src/test/org/apache/hadoop/mapred/EmptyInputFormat.java
src/test/org/apache/hadoop/dfs/TestSeekBug.java
src/test/org/apache/hadoop/dfs/TestDFSMkdirs.java
src/test/org/apache/hadoop/dfs/TestLocalDFS.java
src/test/org/apache/hadoop/dfs/MiniDFSCluster.java
src/test/org/apache/hadoop/dfs/TestPread.java
src/test/org/apache/hadoop/dfs/TestDFSShellGenericOptions.java
src/test/org/apache/hadoop/record/test/RecString.java
src/test/org/apache/hadoop/record/test/RecBuffer.java
src/test/org/apache/hadoop/record/test/RecInt.java
src/test/org/apache/hadoop/record/test/RecRecord0.java
src/test/org/apache/hadoop/record/test/RecRecord1.java
"
HADOOP-633,if the jobinit thread gets killed the jobtracker keeps running without doing anything.,"The jobtracker failed with OutOfMemory while uinjarring the jar file and this kileld the jobinit thread. The Jobtracker still kept running while doing nothing though accepting job submissions, but the jobs just sat in the queue."
HADOOP-632,"dfs -report lists ""missing"" datanodes","""dfs -report"" lists all nodes that have ever registered. It should only list the ""alive"" nodes."
HADOOP-630,The web/ui for the JobTracker doesn't come up until after the namenode is out of safe mode,"The http status server for the job tracker isn't initialized until after the working directory is cleared, meaning that it can't happen until after the name node leaves safe mode. "
HADOOP-629,none of the rpc servers check the protcol name for validity,"All of the Hadoop RPC servers either ignore the protocol name or do:

if (protocol.equals(Prot1.class.getName()) {
  return Prot1.versionId;
} else {
  return Prot2.versionId;
}

A much better structure would be:

if (Prot1.class.getName().equals(protocol)) {
  return Prot1.versionId;
} else if (Prot2.class.getName().equals(protocol)) {
  return Prot2.versionId;
} else {
  throw new VersionMismatchException(""Expected protocol Prot1 or Prot2 and received: "" + protocol);
}"
HADOOP-628,hadoop hdfs -cat   replaces some characters with question marks.,"Should not the effect of

hadoop hdfs -get path local-file
and
hadoop hdfs -cat path >local-file

be the same?

Try to do this with a (hdfs) file that contains non-ascii characters and do a diff."
HADOOP-627,MiniMRCluster missing synchronization,"org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner contains (at least) 2 instance variables that are read by another thread: isInitialized and isDead.  These should be declared volatile or proper synchronization should be used for their access.
"
HADOOP-626,NNBench example comments are incorrect and code contains cut-and-paste error,"The class and main method comments don't reflect the actual behavior.  In addition, the Mapper implementation contains an unnecessary randomizeBytes call when reading back from the created file."
HADOOP-625,add a little servlet to display the server's thread call stacks,"Add a trivial servlet to each jetty server that provides the stacks for each thread from

http://<host>:<port>/stacks"
HADOOP-624,fix warning about pathSpec should start with '/' or '*' : mapOutput,Every task tracker start gets a complaint that the mapOutput servlet has a bad path.
HADOOP-622,Users should be able to change the environment in which there maps/reduces run.,"This would be useful with caching. So you would be avble to say, cache file X and then should be able to change the environment variable like PATH/LD_LIBRARY_PATH to include the local path hwere the file was cached. "
HADOOP-621,"When a dfs -cat command is killed by the user, the correspondig hadoop process does not get aborted","if I use hadoop dfs -cat  in a pipeline, and kill the consumer of -cat output, the hadoop process stays alive.
"
HADOOP-620,replication factor should be calucalated based on actual dfs block sizes at the NameNode.,"Currently 'dfs -report' calculates replication facto like the following :
     (totalCapacity - totalDiskRemaining) / (totalSize of dfs files in Name space).

Problem with this is that this includes disk space used by non-dfs files (e.g. map reduce jobs) on data node. On my single node test, I get replication factor of 100 since I have a 1 GB dfs file with out replication and there is 99GB of unrelated data on the same volume.

ideally name should calculate it with : (total size of all the blocks known to it) / (total size of files in Name space).

Initial proposal to keep 'total size of all the blocks' update is to track it in datanode descriptor and update it when namenode receives block reports from the datanode ( and subtract when the datanode is removed).


"
HADOOP-619,Unify Map-Reduce and Streaming to take the same globbed input specification,"Right now streaming input is specified very differently from other map-reduce input.  It would be good if these two apps could take much more similar input specs.

In particular -input in streaming expects a file or glob pattern while MR takes a directory.  It would be cool if both could take a glob patern of files and if both took a directory by default (with some patern excluded to allow logs, metadata and other framework output to be safely stored).

We want to be sure that MR input is backward compatible over this change.  I propose that a single file should be accepted as an input or a single directory.  Globs should only match directories if the paterns is '/' terminated, to avoid massive inputs specified by mistake.

Thoughts?"
HADOOP-618,JobProfile and JobSubmissionProtocol should be public,"Are there any reasons why they are not public?

Making them public adds a lot of flexibility for the client to interact with the Hadoop.


"
HADOOP-614,Sporadic TestEmptyJobWithDFS failure due to NPE is JobTracker.submitJob(),"org.apache.hadoop.mapred.TestEmptyJobWithDFS has failed a couple of times (low reproducibility) with the following exception:

2006-10-17 21:48:24,875 INFO  ipc.Server (Server.java:run(516)) - Server handler 2 on 50050 call error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1020)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:385)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:514)

Complete test log attached."
HADOOP-613,The final merge on the reduces should feed the reduce directly,The final merge on the input to the reduce should use the iterators from HADOOP-611 directly rather than using a round trip to disk.
HADOOP-612,DFs copyFromLocal fails with NullPointerException for a single file,"DFS copyFromLocal fails with NullPointerException when copying a single file to DFS. Copying a directory works. 

  public File getFile(String dirsProp, String path)
    throws IOException {
    String[] dirs = getStrings(dirsProp);                     <===== returns null for a single file
    int hashCode = path.hashCode();
    for (int i = 0; i < dirs.length; i++) {  // try each local dir                     <==== Throws NullPointerException
      int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;
      File file = new File(dirs[index], path);
      File dir = file.getParentFile();
      if (dir.exists() || dir.mkdirs()) {
        return file;
      }
    }
    throw new IOException(""No valid local directories in property: ""+dirsProp);
  }



Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:397)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:913)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:903)
        at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:276)
        at org.apache.hadoop.dfs.DistributedFileSystem.createRaw(DistributedFileSystem.java:104)
        at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:56)
        at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:45)
        at org.apache.hadoop.fs.FSDataOutputStream.<init>(FSDataOutputStream.java:146)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:271)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:178)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:77)
        at org.apache.hadoop.dfs.DistributedFileSystem.copyFromLocalFile(DistributedFileSystem.java:186)
        at org.apache.hadoop.dfs.DFSShell.copyFromLocal(DFSShell.java:45)
        at org.apache.hadoop.dfs.DFSShell.run(DFSShell.java:516)
        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)
        at org.apache.hadoop.dfs.DFSShell.main(DFSShell.java:570)
"
HADOOP-611,SequenceFile.Sorter should have a merge method that returns an iterator,"SequenceFile.Sorter should get a new merge method that returns an iterator over the keys/values.

The current merge method should become a simple method that gets the iterator and writes the records out to a file."
HADOOP-610,Task Tracker offerService does not adequately protect from exceptions,"The TaskTracker's offerService loop doesn't handle exceptions, such as time outs well and will reset the task tracker. I believe this is the cause of most of the lost task trackers. The scenario looks like:

  1. an rpc timeout in offerService
  2. the task tracker cleans up (which takes 30 minutes with the task tracker locked up)
  3. the task tracker is declared lost for not providing its heartbeat"
HADOOP-609,Mini map/reduce cluster should use multiple temp directories,Currently the mini map/reduce cluster only tests the task trackers with a single working directory. That caused us to miss a bug that broke 0.7.1. At least some of the mini m/r tests should run task trackers with two or threee working directories.
HADOOP-607,Classpath of tasks is not set correctly (bug happens for multiple mapred local dirs only),Tasks fail because the classpath for the tasks is not set correctly. The current working directory of the tasks is not set to the directory where the job.jar is unjarred.
HADOOP-606,DFSShell should print messages to standard error,"DFSShell prints many of its usage and error messages to System.out.  Only command output (ls, cat, report, etc.) should be written to System.out, everything else should be sent to System.err."
HADOOP-605,The jobtracker UI does not show some jobs,"we have seen this in streaming, not sure if it happens in hadoop java programs. The jobclient gets a jobid from the jobtracker and then fails to launch the job since hte job is missing a mapper (""No -mapper specified"" in streaming). The job is never submitted to the jobtracker and the jobtracker does not have any history of this job. Streaming should inform the jobtracker why the job failed and this should show up in the job UI."
HADOOP-604,DataNodes get NullPointerException and become unresponsive on 50010,"I wonder whether handling of 'children' in FSDataset is thread-safe.

Exception in thread ""org.apache.hadoop.dfs.DataNode$DataXceiveServer@19b04e2"" java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:162)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:162)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:162)
        at org.apache.hadoop.dfs.FSDataset$FSDir.checkDirTree(FSDataset.java:162)
        at org.apache.hadoop.dfs.FSDataset$FSVolume.checkDirs(FSDataset.java:238)
        at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.checkDirs(FSDataset.java:326)
        at org.apache.hadoop.dfs.FSDataset.checkDataDir(FSDataset.java:522)
        at org.apache.hadoop.dfs.DataNode$DataXceiveServer.run(DataNode.java:472)
        at java.lang.Thread.run(Thread.java:595)
"
HADOOP-603,Extend SequenceFile to provide MapFile function by storing index at the end of the file,"MapFile increases the load on the name node as two files are created to provide a index file format. If SequenceFile were extended by storing the index at the end of the file, 1/2 of the files currently created for a map/reduce operation would be needed, reducing the load on the name node.

Perhaps this is why Google implemented SSTable files in this manner. (SSTable files are functionally identical to Hadoop MapFiles) (see the paper on BigTable - section 4 ""Building Blocks"" http://labs.google.com/papers/bigtable.html)"
HADOOP-602,Remove Lucene dependency,"Hadoop uses just a single class from Lucene, namely the PriorityQueue. I propose to remove this dependency by copying this class into org.apache.hadoop.util.PriorityQueue and removing lucene-core.jar from dependencies.

Removing this dependency makes it easier to run map-reduce applications that use different versions of Lucene."
HADOOP-600,Race condition in JobTracker updating the task tracker's status while declaring it lost,There was a case where the JobTracker lost track of a set of tasks that were on a task tracker. It appears to be a race condition because the ExpireTrackers thread doesn't lock the JobTracker while updating the state. The fix would be to build a list of dead task trackers and then lock the job tracker while updating their status.
HADOOP-599,DFS Always Reports 0 Bytes Used,"Ever since 0.6 our DFS cluster has been reporting that it is empty, even though it's not.  We seem to have resolved the issue by changing line 296 of org.apache.hadoop.dfs.FSDataset from:

Long remaining = new Long(volumes[idx].getCapacity();

to:

Long remaining = new Long(volumes[idx].getAvailable());"
HADOOP-598,rpc timeout in Task.done kills task,"When tasks are done and try to signal the task tracker that they are done, a single rpc timeout will kill the task."
HADOOP-597,transmission errors to the reduce will cause map output to be considered lost,"When the mapOutput servlet gets an IOException from reading the map output file, it correctly declares the map output lost. However, an IOException writing to the socket should NOT cause the map output to be declared lost. Unfortunately, the current implementation always declares the map output lost. When the map output is lost, the map is re-run on a different computer."
HADOOP-596,TaskTracker taskstatus's phase doesnt get updated on phase transition causing wrong values displayed in WI,"On task phase transition from shuffle > sort > reduce, the new phase doesnt get updated in task tracker's cached TaskStatus  causing wrong values displayed in WI. "
HADOOP-595,Browsing content of filesystem over HTTP not works,"When I click ""Browse the filesystem"" in my browser I get following:

An error occurred while loading http://localhost.localdomain:50070/nn_browsedfscontent.jsp:
Could not connect to host localhost.localdomain (port 65535).

In HADOOP 0.6.2 it works properly."
HADOOP-594,Safemode default threshold should be 0.999,"We want all blocks reported before we leave safemode, except for just a few that might be missing or corrupted.
So it's effectively 1.0 but not quite."
HADOOP-593,NullPointerException in JobTracker's ExireTaskTracker thread,"The JobTracker's thread that finds dead task trackers is getting a NullPointerException:

Exception in thread ""Thread-19"" java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobInProgress.failedTask(JobInProgress.java:614)
        at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:291)
        at org.apache.hadoop.mapred.JobInProgress.failedTask(JobInProgress.java:674)
        at org.apache.hadoop.mapred.JobTracker.lostTaskTracker(JobTracker.java:1204)
        at org.apache.hadoop.mapred.JobTracker$ExpireTrackers.run(JobTracker.java:225)
        at java.lang.Thread.run(Thread.java:595)

Which kills the thread and prevents any other task trackers from being killed."
HADOOP-592,NullPointerException in toString for RPC connection objects,"If the socket is closed when the log message is generated, the toString method gets a NullPointerException.

The exception is caught and logged, but happens often enough that administrators will worry about it. It also loses useful debugging information."
HADOOP-591,Reducer sort should even out the pass factors in merging different pass,"
When multiple pass merging is needed during sort, the current sort implementation in SequenceFile class uses a simple ""greedy"" way to select pass factors, resulting uneven pass factor in different passes. For example, if the factor pass is 100 (the default), and there are 101 segments to be merged. The current implementation will first merge the first 100 segments into one and then merge the big output file with the last segment with pass factor 2. It will be better off to use pass factors 11 in the first pass and pass factor 10 in the second pass.

"
HADOOP-590,Reducer's pass merger should utilize temporary directories on different disks,"
The current implementation of pass merge of SequenceFile class uses the same temp directory for the in/out files of the pass merger class, even though when multiple temp dirs are available. Thus, it cannot fully utlize the advantage of multiple disks during sort.

"
HADOOP-588,JobTracker History bug - kill() ed tasks are logged wrongly as finished.,"JobInProgress.kill() logs jobs as finished in jobtracker history, whereas they should be logged as failed. "
HADOOP-587,SequenceFile sort should use quicksort instead of merge sort for sorting runs.,"
I noticed that Hadoop uses mergesort for sorting the runs in the sequence file SortPass class. 
I think quicksort should be faster and should be used instead. 
I also noticed that Java's Arrays.sort() static functions for non-primitive element types also use merge sort. 

Are there any subtle reasons why not to use quicksort?

If no, we should implement out quicksort. Otherwise, we should use Java's sort, unless we believe our mergesort is better.

"
HADOOP-586,"Job Name should not ben empty, if its is not given bu user, Hadoop should use original Jar name as the job name.",JobTracker UI and JobTracker history UI have job name column empty for jobs that don't have a name. It would be good to have some identification of the job there even if user doesn't give it a name. Name of the user jar file is a good candidate. 
HADOOP-585,hadoop command should have an option to print it's version information,
HADOOP-583,DFSClient should use debug level to log missing data-node.,"When DFSClient cannot read a block replica it logs a message saying
""Failed to connect to <target> ..."" + exception
and then tries other replicas. If the client succeeds with another replica this
is only record that the log will contain. This seems to be confusing for users.
So I propose to use debug logging level here.
I also stringified exceptions.
"
HADOOP-581,Datanode's offerService does not handle rpc timeouts,"The Datanode's offerService routine needs to protect itself (with log & ignore) from IOExceptions (especially RPC timeouts) in the offer service method.

Currently, if a heartbeat times out, the Datanode will ""reset"" itself, which means that the full block report must be sent again."
HADOOP-578,Failed tasks should not be put at the end of the job tracker's queue,"This functionality was basically a workaround for other problems, which have been fixed. The impact of putting them at the end of the work queue is that if you have 100,000 maps, and the Mapper fails deterministically, you'll run 300,000+ attempts before your job is killed."
HADOOP-577,Job tracker should collect detailed information about tasks and return it to the client,"Mapreduce driver should get detailed information about task execution -- at least at the end of the job so that it can be repoted to the user (stored in a ""normal"" or HDFS file, printed to the stdout, whatever).
This information should include, at least:
-- task execution start and end
-- the number of record read and written
-- the number of butes read and written
for each task  (for each attempt)

It would also be nice to have this infromation while the job is running, updated at least when a task is started or ended.
However, this is just ""nice to have"", while the final report is quite essential.
"
HADOOP-576,Enhance streaming to use the new caching feature,"Design proposal to expose filecache access to Hadoop streaming.

The main difference with the pure-Java filecache code is:
1. As part of job launch (in hadoopStreaming client) we validate presence of
cached archives/files in DFS.
2. As part of Task initialization, a symbolic link to cached files/unarchived
directories is created in the Task working directory.


C1. New command-line options (example)
-cachearchive dfs:/user/me/big.zip#big_1 
-cachefile dfs:/user/other/big.zip#big_2 
-cachearchive dfs:/user/me/bang.zip

This maps to API calls to static methods:
DistributedCache.addCacheArchive(URI uri, Configuration conf)
DistributedCache.addCacheFile(URI uri, Configuration conf)
This is done in class StreamJob methods parseArgv() and setJobConf().
The code should be similar to the way ""-file"" is handled.

One difference is that we now require a FileSystem instance to VALIDATE the DFS
paths in -cachefile and -cachearchive. The FileSystem instance should not be
accessed before the filesystem is set by this: setUserJobConfProps(true);

If FileSystem instance is ""local"" and there are -cachearchive/-cachefile
options , then fail: this is not supported.

Else this should return true:
fs_.isFile(Path) for each -cachearchive/-cachefile option.
Only in verbose mode: show the isFile status of each option.
In any verbosity mode: show the first failed isFile() status and abort using
method StreamJob.fail().


C2. Task initialization
The symlinks are called:
Workingdir/big_1 (points to directory: /cache/user/me/big_zip)
Workingdir/big_2 (points to file: /cache/user/other/big.zip)
Workingdir/bang.zip (points to directory /cache/user/me/bang_zip)


This will require hadoopStreaming to create symbolic links.
Hadoop should have code to do this in a portable way.
Although this may not be supported on non-Unix platforms. 
Cross-platform support is harder than for hard-links. 
Cygwin soft links are not a solution: they only work for applications compiled with
cygwin1.dll)
Symbolic links make JUnit tests less portable.
So maybe the test should run as part of ant target test-unix. (in contrib/streaming/build.xml)


The parameters after -cachearchive and -cachefile have the following
properties:

A. you can optionally give a name to your symlink (after #)
B. the default name is the leaf name (big.zip, big.zip, bang.zip)
C. if the same leaf name appears more than once you MUST give a name. Otherwise
streaming client aborts and complains. For example with this, Streaming client
should complain:
-cachearchive dfs:/user/me/big.zip 
-cachefile dfs:/user/other/big.zip
This complains because multiple occurrences of ""big.zip"" are not disambiguated
with #big_1, #big_2.
Ideally the Streaming client error message should then generate an example on
how to fix the parameters:
-cachearchive dfs:/user/me/big.zip#1
-cachefile dfs:/user/other/big.zip#2

---------

hadoop-Client note:
Currently argv parsing is position-independant. i.e. changing the order of
arguments never impacts the behaviour of hadoopStreaming. It would be good to
keep this behaviour.


URI notes:
scheme is ""dfs:"" for consistency with current state of Hadoop code.
However there is a proposal to change the scheme to ""hdfs:""

Using a URI fragment to give a local name to the resource is unusual. The main
constraint is that the URI should remain parsable by java.net.URI(String). And
encoding attributes in the fragment is standard (like CGI parameters in an HTTP
GET request) (fragment is #big2 in dfs:/user/other/big.zip#big_2)
"
HADOOP-574,want FileSystem implementation for Amazon S3,"An S3-based Hadoop FileSystem would make a great addition to Hadoop.

It would facillitate use of Hadoop on Amazon's EC2 computing grid, as discussed here:

http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00318.html

This is related to HADOOP-571, which would make Hadoop's FileSystem considerably easier to extend.

"
HADOOP-572,Chain reaction in a big cluster caused by simultaneous failure of only a few data-nodes.,"I've observed a cluster crash caused by simultaneous failure of only 3 data-nodes.
The crash is reproducable. In order to reproduce it you need a rather large cluster.
To simplify calculations I'll consider a 600 node cluster as an example.
The cluster should also contain a substantial amount of data.
We will need at least 3 data-nodes containing 10,000+ blocks each.
Now suppose that these 3 data-nodes fail at the same time, and the name-node
started replicating all missing blocks belonging to the nodes.
The name-node can replicate 50 blocks per second on average based on experimental data.
Meaning, it will take more than 10 minutes, which is the heartbeat expiration interval,
to replicates all 30,000+ blocks.

With the 3 second heartbeat interval there are 600 / 3 = 200 heartbeats hitting the name-node every second.
Under heavy replication load the name-node accepts about 50 heartbeats per second.
So at most 3/4 of all heartbeats remain unserved.

Each node SHOULD send 200 heartbeats during the 10 minute interval, and every time the probability
of the heartbeat being unserved is 3/4 or less.
So the probability of failing of all 200 heartbeats is (3/4) ** 200 = 0 from the practical standpoint.

IN FACT since current implementation sets the rpc timeout to 1 minute, a failed heartbeat takes
1 minute and 8 seconds to complete, and under this circumstances each data-node can send only
9 heartbeats during the 10 minute interval. Thus, the probability of failing of all 9 of them is 0.075,
which means that we will loose 45 nodes out of 600 at the end of the 10 minute interval.
From this point the name-node will be constantly replicating blocks and loosing more nodes, and
becomes effectively dysfunctional.

A map-reduce framework running on top of it makes things deteriorate even faster, because failing
tasks and jobs are trying to remove files and re-create them again increasing the overall load on
the name-node.

I see at least 2 problems that contribute to the chain reaction described above.
1. A heartbeat failure takes too long (1'8"").
2. Name-node synchronized operations should be fine-grained.
"
HADOOP-571,Path should use URI syntax,"The following changes are proposed:
1. Add a factory/registry of FileSystem implementations.  Given a protocol, hostname and port, it should be possible to get a FileSystem implementation.
2. Path's constructor should accept URI-formatted strings & a configuration.
3. A new Path method should be added: FileSystem.getFileSystem().  This returns the filesystem named in the path or the default configured filesystem.
4. Most methods which currently take FileSystem and Path parameters can be changed to take only Path.
5. Many FileSystem operations (create, open, delete, list, etc.) can become convenience methods on Path.
6. A URLStreamHandler can be defined in terms of the FileSystem API, so that URLs for any protocol with a registered FileSystem implementation can be accessed with a java.net.URL, permitting FileSystem implementations to be used on the classpath, etc.

It is tempting to try to replace Path with java.net.URL, but URL's methods are insufficient for mapreduce.  We require directory listings, random access, location hints, etc., which are not supported by existing URLStreamHandler implementations.  But we can expose all FileSystem implementations for access with java.net.URL.

(From a brainstorm with Owen.)"
HADOOP-570,"Map tasks may fail due to out of memory, if the number of reducers are moderately big","
Map tasks may fail due to out of memory, if the number of reducers are moderately big. 
In my case, I set child task heap size to 1GB, turned on compression for the mapoutput files. 
The average size of input records is about 30K (I don't know the variation though). 
A lot of map tasks failed due to out of memory when the number of reducers was at 400 and higher.
The number of reducers can be somewhat higher (as high as 800) if the compression for the mapoutput files was off).
This problem will impose a hard limit on the scalability of map/reduce clusters.

One possible solution to this problem is to let the mapper to write out single map output file, 
and then to perform sort/partition as a separate phrase. 
his will also make it unnecessary for  the reducers to perform sort on individual portions from mappers. 
Rather, the reducers should just perform merge operations on the map output files directly. 
This may even allow the possibility of dynamically collect some statistics of  the map outputs and 
use the stats to drive the partition on the mapper side, and obtain the optimal merge plan on the reducer side!
 "
HADOOP-569,Hadoop should allow the user to dynamically change the number of times to re-try failed tasks before declaring the job fail,"
Hadoop has a built-in mechanism to fail a job if some tasks failed more than 3 times. This mechanism works fine in most scenarios. However, in some other cases, it is highly desirable for the user to change (increase) that number. My current running job demonstrates such a scenario: The job has run more than 2.5 days. It is close to complete (90+%). Everything indicates that it will finish eventually in a day, except for one potential danger: some of the tasks are in their 3rd try! 
It will be extremely helpful if I can change the maximun number of tries to 6 instead of 4!
"
HADOOP-568,small jobs benchmark fails: task is UNASSIGNED,"the small jobs benchmark consistently failes when executed a sufficient number of iterations.
It's just a matter of time before one of the jobs gets stuck, with one map task (out of a 100) simply doesn't get executed.
Looking at the map list, the task is UNASSIGNED.
Restarting the affected task tracker releases the job."
HADOOP-567,The build script should record the Hadoop version into the build,"It would be good to compile the Hadoop version, subversion revision, and compilation date into the hadoop.jar file. 

The web/ui would display the version for each of the server home page.

I'd also add ""bin/hadoop version"" to print the version information."
HADOOP-566,"hadoop-daemons.sh fails with ""no such file or directory"" when used from a relative symlinked path","The new shell scripts fail with a relative directory:

% current/bin/hadoop-daemons.sh start datanode
node1: current/bin/..: No such file or directory
node2: current/bin/..: No such file or directory

The problem is that HADOOP_HOME is set to a relative directory and hadoop-daemons does a cd, breaking the other scripts."
HADOOP-565,Upgrade Jetty to 6.x,"Now that Jetty 6.x has been released as stable, we should upgrade."
HADOOP-564,we should use hdfs:// in all API URIs,"Minor nit, but it seems that we should choose a protocol name that is likely not to conflict with other distributed FS projects.  HDFS seems less likely to.  

Right now this will be trivial to change.  Just wanted to socialize this before doing the search and replace.  

PS right now the dfs: usage is not consistent and has crept into a couple of new features the y! team is working on (caching of files and distcp)."
HADOOP-563,DFS client should try to re-new lease if it gets a lease expiration exception when it adds a block to a file,"In the current DFS client implementation, there is one thread responsible for renewing leases. If for whatever reason, that thread runs behind, the lease may get expired. That causes the client gets a lease expiration exception when writing a block. The consequence of that is very devastating: the client can no longer write to the file, and all the partial results up to that point are gone! This is especially costly for some map reduce jobs where a reducer may take hours or even days to sort the intermediate results before the actual reducing work can start.

The problem will be solved if the flush method of  DFS client can renew lease on demand. That is, it should try to re-new lease  when it catches a lease expiration exception. That way,  even when under heavy load and the lease renewing thread runs behind, the reducer  task (or what ever tasks use that client) can preceed.  That will be a huge saving in some cases (where sorting intermediate results take a long time to finish). We can set a limit on the number of retries, and may even make it configurable (or changeable at runtime). 

The namenode can use a different expiration time that is much higher than the current 1 minute lease expiration time for cleaning  up the abandoned unclosed files.

 "
HADOOP-562,Jobs fail if mapred.local.dir have several entries in it,"Since HADOOP-288 got commited jobs (sometimes) fail if you have a comma seperated list of directories as your mapred.local.dir

The jobs fail with ClassNotFoundExceptions, I'm guessing the job.jar is put on one of the disks but something fails when reading/finding it again."
HADOOP-561,one replica of a file should be written locally if possible,"one replica of a file should be written locally if possible. That's currently not the case.
Copying a 1GB file using hadoop dfs -cp running on one of the cluster nodes, all the blocks were written to remote nodes, as seen by fsck -files -blocks -locations on the newly created file.

as long as there is sufficient space locally, a local copy has significant performance benefits."
HADOOP-560,"tasks should have a ""killed"" state",Tasks should have a state that reflects the fact that they were killed instead of failed to help the user understand what happened to the task.
HADOOP-559,Support file patterns in dfs commands,"Currently DFSShell does not support file patterns in dfs file names. It's quite inconvenient for users when they need to manipulate multiple files.

This issue is intended to extend DFSShell to interprete file patterns. A file pattern contains special special pattern matching characters:

?: matches any character
*: matches zero or more characters
[abc]: matches a character from character set {a, b, c}
[a-c]: matches a character from character range: {a...c}
[^a]: matches a character not in character set or range {a}
\c:  escape any special meaning of character c
 

"
HADOOP-558,Adding versioning to Writable classes,"We currently don't have a way to support versioned Writables, which among other problems means that it is very difficult to change the serialization of any types. (A recent example is that we can't change any of the Writables that currently use UTF8.writeString to use Text.writeString with out breaking everything.) Just changing the file version doesn't work because you can't read the old versions.

Therefore, I propose adding a new interface:

public interface VersionMapWritable extends Writable {
  /**
   * Destructively read into this object from in based on the version map.
   * @param versionMap a map from each class to its version in the DataInput (version 0 classes are omitted)
   */
  public void readFields(DataInput in, Map<Class<VersionMapWritable>, int> versionMap) throws IOException;

  /**
   * Classes with non-zero versions should register themselves in static blocks.
   */
  public static void registerCurrentVersion(Class<VersionMapWritable> class, int version) {...}

  /**
   * If a version map includes the parent type, always include the child type as well.
   */
  public static void addDependence(Class<VersionMapWritable> parent, Class<VersionMapWritable> child) { ... }

  /**
   * Build a version map for a given list of classes, including any dependent types.
   */
  public static Map<Class<VersionMapWritable>, int> 
              buildVersionMap(Set<Class<VersionMapWritable>> classes) {...}

  /**
   * Add the types in the parameter/result types to the list of classes.
   * Recursively adds the field types for any new types that are added to the set.
   */
  public static void addMethodTypes(Set<Class<VersionMapWritable>> result, 
                     Class<VersionedProtocol> protocol) {...}

  /**
   * Add the non-transient fields to the list of classes.
   */
  public static void addFieldTypes(Set<Class<VersionMapWritable>> result, Class<Writable> writable) {...}

  public static Map<Class<VersionMapWritable>, int> readVersionMap(DataInput in) throws IOException { ... }

  public static void writeVersionMap(DataOutput out, 
                  Map<Class<VersionMapWritable> versionMap) throws IOException {...}
}

VersionedWritable, which stored a version byte within each object, will be depriciated."
HADOOP-556,"Streaming should send keep-alive signals to Reporter every 10 seconds, not every 100 records","Streaming should send keep-alive signals to Reporter every 10 seconds, not every 100 records.
A first version of this was already implemented but was not satisfactory.

Now we check whether 10sec have passed:

-after reading a *line* of Application's stderr 
 (so please use as your stderr heartbeat: cerr << "".\n"", not cerr << "".""  )

-and after outputting a record in mapper / combiner / reducer.

If 10 sec have passed then we set Reporter status.

Effects:

-the reporter status changes more often and provides useful feedback in the Web UI or in another client.
-a Task will not time out after 10 minutes just because it outputs records slowly. 

No artificial heartbeat is introduced in this proposal.
The streaming Application still has to show activity (either on stdout or on stderr)







"
HADOOP-555,Tasks should inherit some of the server's environment,"When the task trackers start up the Task process, they clear the environment variables. It would be better if there was a SERVER config that listed the names of the environment variables that would be copied from the server's enviroment.

So the task tracker's config would have:

<property><name>mapred.task.environment</name>
<value>PATH,LD_LIBRARY_PATH</value></property>"
HADOOP-554,hadoop dfs command line doesn't exit with status code on error,"In DFSClient, the code looks like:

---- >8 ----

        int exitCode = -1;
        ...
        try {
            if (""-put"".equals(cmd) || ""-copyFromLocal"".equals(cmd)) {
                copyFromLocal(new Path(argv[i++]), argv[i++]);
            ...
            exitCode = 0;;
        } catch (IOException e ) {
          System.err.println( cmd.substring(1) + "": "" + e.getLocalizedMessage() );  
        } finally {
            fs.close();
        }
        return exitCode;

---- 8< ----

Point 1: Few, if any of the functions called throw an exception. Instead they System.err.println and return.
Point 2: exitCode, regardless if there is an exception, is always 0.

At minimum, it would be best if SOME status code were returned, so that any scripts calling hadoop dfs would know to parse the output.

At best, there'd be a well-documented table of exit codes so that parsing stderr wouldn't be required.
"
HADOOP-553,DataNode and NameNode main() should catch and report exceptions.,"Right now we do not know what happened during startup failure,
because there no records in the logs.
I propose to change main() so that it would catch exceptions and 
log them rather then throwing them into std out.
May be the same should be done for the JobTracker.
The TaskTracker.main() catches at least the IOException."
HADOOP-552,getMapOutput doesn't reliably detect errors and throw to the caller,"getMapOutput does not reliabily detect errors in transmission, which usually ends up with an EOFException in the reduce's sort:

java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:178)
	at java.io.DataInputStream.readFully(DataInputStream.java:152)
	at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:952)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:937)
	at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:928)
	at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:1594)
	at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:1523)
	at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:1496)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:240)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1173)
"
HADOOP-551,reduce the number of lines printed to the console during execution,"In the patch for HADOOP-423, I made the web ui and the job client cli format the percentage done consistently as 0.00 - 100.00%. This has the side effect of making the job client print a lot more lines to the user's console (up to 20k!)."
HADOOP-550,Text constructure can throw exception,"I finally got back around to moving my working code to using Text objects.

And, once again, switching to Text (from UTF8) means my jobs are failing. This time, its better defined - constructing a Text from a string extracted from Real World data makes the Text object constructor throw a CharacterCodingException. This may be legit - I don't actually understand UTF well enough to understand what's wrong with the supplied string. I'm assembling a series of strings, some of which are user-supplied, and something causes the Text constructor to barf.

However, this is still completely unacceptable. If I need to stuff textual data someplace - I need the container to *do* it. If user-supplied inputs can't be stored as a ""UTF"" aware text value, then another container needs to be brought into existence. Sure, I can use a BytesWritable, but, as its name implies - Text should handle ""text"". If Text is supposed to == ""StringWritable"", then, well, it doesn't, yet.

I admit to being a few weeks' back in the bleeding edge at this point, so maybe my particluar Text bug has been fixed, though the only fixes to Text I see are adopting it into more of the internals of Hadoop. This argument goes double in that case - if we're using Text objects internally, it should really be a totally solid object - construct one from a String, get one back, but _never_  throw a content-related Exception. Or, if Text is not the right object because its data-sensitive, then I argue we shouldn't use it in any case where data might kill it - internal, or anywhere else (by default).

Please, don't remove UTF8, for now."
HADOOP-549,NullPointerException in TaskReport's serialization code,"TaskReport[] rep = jc.getMapTaskReports(jobId);
I get de/serialization problems:


This change suggested by Owen fixes it:
-    new ObjectWritable(diagnostics).write(out);
+    WritableUtils.writeStringArray(out, diagnostics);

-    ObjectWritable wrapper = new ObjectWritable();
-    wrapper.readFields(in);
-    diagnostics = (String[])wrapper.get();
+    diagnostics = WritableUtils.readStringArray(in);

java.lang.NullPointerException
at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:174)
at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:59)
at org.apache.hadoop.mapred.TaskReport.readFields(TaskReport.java:64)
at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:225)
at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:163)
at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:210)
at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:59)
at org.apache.hadoop.ipc.Client$Connection.run(Client.java:256)
2006-09-19 08:44:31,059 WARN  ipc.Server (Server.java:run(493)) - handler output error
java.nio.channels.ClosedChannelException
"
HADOOP-548,add a switch to allow unit tests to show output,"I'd like a switch so that I can have the unit tests show their output to the console.

ant test   // current behavior
ant -Dtest.output=yes  test // show output to console"
HADOOP-547,"ReduceTaskRunner can miss sending hearbeats if no map output copy finishes within ""mapred.task.timeout""","In ReduceTaskRunner, main loop sending heartbeats waits on copyResults, which releases only if a copy thread finishes copying. This can cause good reduce tasks which are copying data to fail, if no map task output was copied within ""mapred.task.timeout"". 

ReduceTaskRunner.java:490
        try {
          copyResults.wait();                      <=========== Calls unconditional wait. 
        } catch (InterruptedException e) { }

wait() should be with a timeout, possibly taskTimeout/2 after which it should send a hearbeat and go back to wait. "
HADOOP-546,Task tracker doesnt generate job.xml in jobcache for some tasks ( possibly for only rescheduled tasks),"For many of the rescheduled tasks task tracker doesn't generate job.xml causing the task to fail. The exception appears in Job tracker logs and not in task tracker's logs. 

possibly related to Hadoop-543

2006-09-19 01:10:37,064 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0001_m_022869_3: java.io.FileNotFoundException: .../hadoop/mapred/local/taskTracker/jobcache/job_0001/task_0001_m_022869_3/job.xml (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:179)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:131)
        at org.apache.hadoop.fs.LocalFileSystem$LocalFSFileOutputStream.<init>(LocalFileSystem.java:133)
        at org.apache.hadoop.fs.LocalFileSystem.createRaw(LocalFileSystem.java:172)
        at org.apache.hadoop.fs.LocalFileSystem.createRaw(LocalFileSystem.java:180)
        at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:56)
        at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:45)
        at org.apache.hadoop.fs.FSDataOutputStream.<init>(FSDataOutputStream.java:146)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:270)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:177)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:810)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:858)
        at org.apache.hadoop.mapred.TaskTracker.launchTaskForJob(TaskTracker.java:282)
        at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:275)
        at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:669)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:486)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:689)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1290)
"
HADOOP-545,Unused parameter in hadoop-default.xml,"found a parameter in haddop-default.xml that seems not to be used anymore in the java code : mapred.task.tracker.output.port.

I've done a grep ""mapred.task.tracker.output.port"" **/*.java (thanks zsh) in the whole source tree and the only result was :
src/test/org/apache/hadoop/mapred/MiniMRCluster.java: jc.setInt(""mapred.task.tracker.output.port"", taskTrackerPort++);

I think it should be removed from hadoop-default.xml
"
HADOOP-544,"Replace the job, tip and task ids with objects.","I think that it is silly to have tools parsing the strings that the framework builds for task ids. I propose:

class JobId implements Writable {
   public int getJobId() {...}
}

class TaskId implements Writable {
  public JobId getJobId(); 
  public boolean isMap() { ... }
  public int getTaskId() { ... }
}

class TaskAttemptId implements Writable {
  public TaskId getTaskId();
  public int getAttemptId();
}

each of the classes will have a toString() method that generates the current string."
HADOOP-543,Error to open job files,"I was running a faily large job on Hadoop release 0.6.2.
The job failed because a lot of map tasks failed with following exceptions:

org.apache.hadoop.ipc.RemoteException: java.io.IOException: 
Cannot open filename .../mapred/system/submit_zaretf/job.xml at org.apache.hadoop.dfs.NameNode.open(NameNode.java:178) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:333) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:473) at org.apache.hadoop.ipc.Client$Connection.run(Client.java:245) 

java.lang.RuntimeException: java.lang.RuntimeException: ../mapred/local/taskTracker/task_0031_m_046467_0/job.xml not found at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:549) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:476) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:457) at org.apache.hadoop.conf.Configuration.set(Configuration.java:204) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:724) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:779) at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:574) at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:394) at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:603) at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1191) Caused by: java.lang.RuntimeException: ../mapred/local/taskTracker/task_0031_m_046467_0/job.xml not found at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:516) ... 9 more 

org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot open filename ../mapred/system/submit_zaretf/job.jar at org.apache.hadoop.dfs.NameNode.open(NameNode.java:178) at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:585) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:333) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:473) at org.apache.hadoop.ipc.Client$Connection.run(Client.java:245) "
HADOOP-542,"on-the-fly merge sort, HADOOP-540, reformat","A large patch for streaming. Changes:

Support for on-the-fly merge sort of multiple map input files.
This supposes that the inputs are already sorted.

Support for reducer-NONE side-effects to a single local output with DFS inputs.
This can be used to do an on-the-fly merge-sort of remote sorted files.
(Compare to DFSShell -getmerge which does *catenation* of remote sorted files)
The single output can be a regular file, a named pipe or a socket.
URI Syntax: -mapsideoutput [file:/C:/win|file:/unix/|socket://host:port]

Add an optional JUnit test for on-the-fly merge-sort.
It requires Unix tools. It also works with cygwin.

If it has been more than 10 secs since last time we did this:
call reporter.setStatus() when consuming a stderr line from the Application.
Calling setStatus with reducer-NONE was already done as part of HADOOP-413.
So overall this resolves HADOOP-540.

Reformat streaming code to conform to Hadoop conventions 
 (indent 2 spaces, opening bracket on same-line) 
"
HADOOP-540,Streaming should send keep-alive signals to Reporter,"The goal is to avoid the 10-minutes Task timeout with certain usages of hadoopStreaming.

This proposal does not introduce an artificial keep-alive heartbeat: 
it still requires the external Application to have activity.

Streaming should set Reporter status in more places:

1. output to side-effect files should (periodically) set reporter status.

2. Application's stderr data should (periodically) set reporter status.

"
HADOOP-539,Namenode WI shows incorrect cluster remaining size,"dfshealth.jsp shows remaining cluster size equals to cluster capacity, and all nodes show at 0% used. the actual cluster contains few hundred GBs of data and the data is visible through command line or file browsing. "
HADOOP-538,Implement a nio's 'direct buffer' based wrapper over zlib to improve performance of java.util.zip.{De|In}flater as a 'custom codec',"There has been more than one instance where java.util.zip's {De|In}flater classes perform unreliably, a simple wrapper over zlib-1.2.3 (latest stable) using java.nio.ByteBuffer (i.e. direct buffers) should go a long way in alleviating these woes."
HADOOP-537,clean-libhdfs target of build.xml does not work on windows,"It produces the following:

BUILD FAILED
Hadoop\build.xml:496: Execute failed: java.io.IOException: CreateProcess: make clean error=2

Besides, I would propose to have clean-* target for every compile-* target in build.xml.
Some people probably don't build libhdfs or contrib, so why should they clean it."
HADOOP-536,Broke ant test on windows,One of my junit tests breaks on windows because of a bug in the junit test code. 
HADOOP-534,Jobconf should set the default output value class to be Text,"When input files are in the text format, the input value class is set to be Text. But the output value class is by default set to be UTF8. This causes the Grep example to fail."
HADOOP-533,TestDFSShellGenericOptions creates a sub-directory in conf/,"TestDFSShellGenericOptions creates a subdirectory of conf/.  This is not removed.

Generally, the build & test process should not create or modify files outside of the build/ directory.  So the best fix would be to create this directory somewhere under build/, perhaps build/test.  Then it will be naturally removed by 'ant clean', will not show up in 'svn stat', etc."
HADOOP-532,Writable underrun in sort example,"When running the sort benchmark, I get consistent failures of this sort:

java.lang.RuntimeException: java.io.IOException: org.apache.hadoop.io.BytesWritable@43d748ad read 2048 bytes, should read 2052 at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:150) at org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:271) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1066) Caused by: java.io.IOException: org.apache.hadoop.io.BytesWritable@43d748ad read 2048 bytes, should read 2052 at org.apache.hadoop.io.SequenceFile$Reader.getCurrentValue(SequenceFile.java:1163) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1239) at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:181) at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:147) ... 3 more"
HADOOP-531,Need to sort on more than the primary key,There are many tasks where I need to have finer control over the ordering in the reduce than a sort on a single key provides. Most of these situations arise when a merge two sources of data and am attaching a single instance of one source to multiple instances of a second source. I know that I can read all the the records with a single key. It's possible that there might be many millions of these making memory demands that cannot be satisfied.
HADOOP-530,Error message does not expose mismached key or value class name correctly in Sequence file,"When a passed-in key or value is not of the expected class, the value of the key or value rather than its class name is printed to an error message."
HADOOP-529,SequenceFile fails task with NullPointerException in codec initialization,"Sequence file fails in following block of code in 0.6 hadoop ( Sep 13th trunk ) - 
      if (decompress) {
        valInFilter = this.codec.createInputStream(valBuffer);   ---> throws NullPointerException

this.codec is apparently not initialized in 
      if (version >= CUSTOM_COMPRESS_VERSION && this.decompress) {    
        try {
          this.codec = (CompressionCodec)  <---- Init code. 

and is used later. It should probably check for version also before using the this.codec. 



2006-09-13 07:37:55,010 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_0003_m_001601_0: java.lang.NullPointerException
        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1001)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:933)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:924)
        at org.apache.hadoop.mapred.SequenceFileRecordReader.<init>(SequenceFileRecordReader.java:37)
        at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:53)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1066)
"
HADOOP-528,Some hadoop distcp task fails with dfs.client.buffer.dir not found error while the property exists.,"On some nodes hadoop distcp fails with folowing error. The property dfs.client.buffer.dir is defined in hadoop-site.xml and the corresponding directory is also created on the node by framework. Same configuration works on all other nodes. 

It could be a timing issue where someone is trying to read the dir before its created on local disk, I am not sure of the cause. 

java.io.IOException: No valid local directories in property: dfs.client.buffer.dir at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:405) at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:780) at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.(DFSClient.java:770) at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:276) at org.apache.hadoop.dfs.DistributedFileSystem.createRaw(DistributedFileSystem.java:104) at org.apache.hadoop.fs.FSDataOutputStream$Summer.(FSDataOutputStream.java:56) at org.apache.hadoop.fs.FSDataOutputStream$Summer.(FSDataOutputStream.java:45) at org.apache.hadoop.fs.FSDataOutputStream.(FSDataOutputStream.java:146) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:270) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:177) at org.apache.hadoop.util.CopyFiles$DFSCopyFilesMapper.copy(CopyFiles.java:186) at org.apache.hadoop.util.CopyFiles$DFSCopyFilesMapper.map(CopyFiles.java:391) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:210) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1066)"
HADOOP-527,Allow to specify the bind address for all hadoop servers,Hadoop servers sockets are always binded on 0.0.0.0 ; this patch allow to specify a bind address.
HADOOP-526,datanode lock message causes NullPointerException,"If you try to bring up a second datanode in the same directory, you get a NullPointerException.

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.dfs.DataStorage.lock(DataStorage.java:132)
        at org.apache.hadoop.dfs.DataStorage.<init>(DataStorage.java:72)
        at org.apache.hadoop.dfs.DataStorage.<init>(DataStorage.java:50)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:194)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:155)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1050)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:984)
        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:1013)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1066)
"
HADOOP-525,Need raw comparators for hadoop record types,"Raw comparators are not generated for types that are generated with the Hadoop record framework. This could have a substantial performance impact when using hadoop record generated types in Map/Reduce. The record i/o framework should auto-generate raw comparators for types.

Comparison for hadoop record i/o types is defined to be member wise comparison of objects. A possible implementation could only deserialize one member from each object at a time, compare them and either return or move on to the next member if the values are equal."
HADOOP-524,Contrib documentation does not appear in Javadoc,Documentation for contrib modules like streaming and the small jobs benchmark does not appear in the Javadoc.
HADOOP-523,TextInputformat .isSplittable() fails with NullPointerException with hadoop 0.6.1,"With hadoop 0.6.1, TextInputFormat fails with NullPointerException, causing Job with fail. 

TextInputFormat.configure() is not called to initialize compressionCodecs and following method throws NullPointerException.

  protected boolean isSplitable(FileSystem fs, Path file) {
    return compressionCodecs.getCodec(file) == null;
  }


java.lang.NullPointerException
        at org.apache.hadoop.mapred.TextInputFormat.isSplitable(TextInputFormat.java:37)
        at org.apache.hadoop.mapred.InputFormatBase.getSplits(InputFormatBase.java:149)
        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:129)
        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:314)
        at java.lang.Thread.run(Thread.java:613)
"
HADOOP-522,MapFile should support block compression,"MapFile is layered on SequenceFile and permits random-access to sorted data files (typically reduce output) through a parallel index file.  This is used widely in Nutch (e.g. at search time for displaying cached pages, incoming links, etc).  Such sorted data should benefit from block compression, but the current MapFile API does not support specification of block compression.  Also, even if it did, the semantics of SequenceFile methods like seek() and getPosition() are changed under block compression so that MapFile may not work."
HADOOP-521,classloader problem for clients,"HADOOP-419 bit again, after updating with hadoop-0.6.0. Although resolved, there was one instance left in io.ObjectWritable.java still using Thread.currentThread().getContextClassLoader() instead of conf.getClassByName()
I got exceptions like:

java.lang.RuntimeException: readObject can't find class
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:223)
        at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:59)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:256)
Caused by: java.lang.ClassNotFoundException: org/apache/hadoop/io/ObjectWritable$NullInstance
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:242)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:219)
        ... 2 more
"
HADOOP-520,libhdfs returns success even when writing to a file fails,"hdfsWrite prints out an error message when write fails, but still returns number of bytes requested. "
HADOOP-519,HDFS File API should be extended to include positional read,"HDFS Input streams should support positional read. Positional read (such as the pread syscall on linux) allows reading for a specified offset without affecting the current file offset. Since the underlying file state is not touched, pread can be used efficiently in multi-threaded programs.

Here is how I plan to implement it.

Provide PositionedReadable interface, with the following methods:

int read(long position, byte[] buffer, int offset, int length);
void readFully(long position, byte[] buffer, int offset, int length);
void readFully(long position, byte[] buffer);

Abstract class FSInputStream would provide default implementation of the above methods using getPos(), seek() and read() methods. The default implementation is inefficient in multi-threaded programs since it locks the object while seeking, reading, and restoring to old state.

DFSClient.DFSInputStream, which extends FSInputStream will provide an efficient non-synchronized implementation for above calls.

In addition, FSDataInputStream, which is a wrapper around FSInputStream, will provide wrapper methods for above read methods as well.

Patch forthcoming early next week."
HADOOP-518,hadoop dfs -cp foo/bar/bad-file mumble/new-file copies a file with a bad checksum,"I have a file that reliably generates a checksum error when it's read, whether by a map/reduce job as input or by a ""dfs -get"" command.

However...

if I do a ""dfs -cp"" from the file with the bad checksum the copy can be read in its entirety without a checksum error.

I would consider it reasonable for the command to fail, or for the new file to be created but to also have a checksum error in the same place, but this behavior is unsettling.

-dk
"
HADOOP-517,readLine function of UTF8ByteArrayUtils does not handle end of line correctly,"The readLine function does not read ahead looking for '\n' after it reads '\r'. Therefore it incorrectly generates two lines, wth the second one empty in the window platform."
HADOOP-514,namenode heartbeat interval should be configurable,"The namenode heartbeat interval is currently a constant in FSConstants.   It should be configurable instead.  Things which currently rely on the heartbeat occuring fairly frequently should be disassociated.    
"
HADOOP-513,IllegalStateException is thrown by TaskTracker,"I am running TestDFSIO on a one node cluster and getting IllegalStateException for each map task run as listed below.
The same exceptions are flooding logs for TestMiniMRWithDFS on my nightly build for quite a while now.
Does anybody know what that is?
My understanding is that if it is ""illegal"", then tests should fail.
If not then it should be caught and ignored INTERNALLY.

06/09/06 14:54:50 INFO mapred.TaskTracker: task_0002_r_000000_0 0.25% reduce > copy (3 of 4 at 0.00 MB/s) >
06/09/06 14:54:50 INFO mapred.TaskRunner: task_0002_r_000000_0 Need 1 map output(s)
06/09/06 14:54:50 INFO mapred.TaskRunner: task_0002_r_000000_0 Got 1 known map output location(s); scheduling...
06/09/06 14:54:50 INFO mapred.TaskRunner: task_0002_r_000000_0 Scheduled 1 of 1 known outputs (0 slow hosts and 0 dup hosts)
06/09/06 14:54:50 INFO mapred.TaskRunner: task_0002_r_000000_0 Copying task_0002_m_000003_0 output from ENO.
06/09/06 14:54:50 WARN /: /getMapOutput.jsp?map=task_0002_m_000003_0&reduce=0:
java.lang.IllegalStateException
    at org.mortbay.jetty.servlet.ServletHttpResponse.getWriter(ServletHttpResponse.java:561)
    at org.apache.jasper.runtime.JspWriterImpl.initOut(JspWriterImpl.java:122)
    at org.apache.jasper.runtime.JspWriterImpl.flushBuffer(JspWriterImpl.java:115)
    at org.apache.jasper.runtime.PageContextImpl.release(PageContextImpl.java:190)
    at org.apache.jasper.runtime.JspFactoryImpl.internalReleasePageContext(JspFactoryImpl.java:115)
    at org.apache.jasper.runtime.JspFactoryImpl.releasePageContext(JspFactoryImpl.java:75)
    at org.apache.hadoop.mapred.getMapOutput_jsp._jspService(getMapOutput_jsp.java:100)
    at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
    at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
    at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
    at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
    at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
    at org.mortbay.http.HttpServer.service(HttpServer.java:954)
    at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
    at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
    at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
    at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
    at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
    at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
06/09/06 14:54:50 INFO mapred.TaskRunner: task_0002_r_000000_0 done copying task_0002_m_000003_0 output from ENO."
HADOOP-512,Write and integrate HDFS webdav provider,"Write and integrate HDFS webdav provider:
Wider context: http://issues.apache.org/jira/browse/HADOOP-496

This patch will include third-party code in the form of a webdav.jar library:
http://issues.apache.org/jira/browse/HADOOP-505

The new code will live in package:
org.apache.hadoop.dfs.webdav

The code will build as part of the regular ant compile-core.

The Web UI in the NameNode and DataNode will run a servlet rooted at /dav:

For ex:
http://namenode:50070/dav/user/michel/streamingtest/in/gz/
http://datanode2:50075/dav/user/michel/streamingtest/in/plain/



"
HADOOP-511,mapred.reduce.tasks not used,"After some testing with nutch I found problem with ArithmeticException in partition function because numReduceTasks came in 0. After seting mapred.reduce.tasks in hadoop-site.xml to same value as default everything works.

This bug disappear in SVN version of hadoop

I this known issue. Please check also NUTCH-361 for detail explanation."
HADOOP-509,initTasks method in the JobInProgress class does not correctly configure the classloader with the custom map/reduce jar,"In this part of code :

        JobConf jd = new JobConf(localJobFile);
        FileSystem fs = FileSystem.get(conf);
        String ifClassName = jd.get(""mapred.input.format.class"");
        InputFormat inputFormat;
        if (ifClassName != null && localJarFile != null) {
          try {
            ClassLoader loader =
              new URLClassLoader(new URL[]{ localFs.pathToFile(localJarFile).toURL() });
            Class inputFormatClass = Class.forName(ifClassName, true, loader);
            inputFormat = (InputFormat)inputFormatClass.newInstance();
          } catch (Exception e) {
            throw new IOException(e.toString());
          }
        } else {
          inputFormat = jd.getInputFormat();
        }

I think that the URLClassLoader should be added the URLs to all the jars contained in the lib directory of the mapred jar. Or maybe, the jar should be unjarred and the classpath should be updated as well as it is done into the TaskRunner class :

      String jar = conf.getJar();
      if (jar != null) {                      // if jar exists, it into workDir
        RunJar.unJar(new File(jar), workDir);
        File[] libs = new File(workDir, ""lib"").listFiles();
        if (libs != null) {
          for (int i = 0; i < libs.length; i++) {
            classPath.append(sep);            // add libs from jar to classpath
            classPath.append(libs[i]);
          }
        }
        classPath.append(sep);
        classPath.append(new File(workDir, ""classes""));
        classPath.append(sep);
        classPath.append(workDir);
      }

Actually, it is not possible to load a custom InputFormat class. Any thoughts ?

PS : This issue comes following a discussion on the issue https://issues.apache.org/jira/browse/HADOOP-366

"
HADOOP-508,random seeks using FSDataInputStream can become invalid such that reads return invalid data,"Some of my applications using Hadoop DFS  receive wrong data after certain random seeks. After some investigation I believe (without looking at source code of java.io.BufferedInputStream) that it basically boils down to the fact that the method 
read(byte[] b, int off, int len), when called with an external buffer larger than the internal buffer, reads into the external buffer directly without using the internal buffer anymore, but without invalidating the internal buffer by setting the variable 'count' to 0 such that a subsequent seek to an offset which is closer to the 'position' of the Positioncache than the internal buffersize will put the current position into the internal buffer containing outdated data from somewhere else."
HADOOP-507,Runtime exception in org.apache.hadoop.io.WritableFactories.newInstance when trying to startup namenode/datanode,"Here's the logs:

arun@neo ~/dev/java/latest-hadoop/trunk $ cat /home/arun/dev/java/hadoop-0.4.0/build/libhdfs/tests/logs/hadoop-arun-namenode-neo.out
2006-09-05 22:18:39,756 INFO  conf.Configuration (Configuration.java:loadResource(496)) - parsing file:/home/arun/dev/java/hadoop-0.4.0/src/c++/libhdfs/tests/conf/hadoop-default.xml
2006-09-05 22:18:39,804 INFO  conf.Configuration (Configuration.java:loadResource(496)) - parsing file:/home/arun/dev/java/hadoop-0.4.0/src/c++/libhdfs/tests/conf/hadoop-site.xml
2006-09-05 22:18:39,918 INFO  util.Credential (FileResource.java:<clinit>(60)) - Checking Resource aliases
2006-09-05 22:18:39,935 INFO  http.HttpServer (HttpServer.java:doStart(729)) - Version Jetty/5.1.4
2006-09-05 22:18:40,366 INFO  util.Container (Container.java:start(74)) - Started org.mortbay.jetty.servlet.WebApplicationHandler@1171b26
2006-09-05 22:18:40,478 INFO  util.Container (Container.java:start(74)) - Started WebApplicationContext[/,/]
2006-09-05 22:18:40,478 INFO  util.Container (Container.java:start(74)) - Started HttpContext[/logs,/logs]
2006-09-05 22:18:40,479 INFO  util.Container (Container.java:start(74)) - Started HttpContext[/static,/static]
2006-09-05 22:18:40,485 INFO  http.SocketListener (SocketListener.java:start(204)) - Started SocketListener on 0.0.0.0:50070
2006-09-05 22:18:40,487 INFO  util.Container (Container.java:start(74)) - Started org.mortbay.jetty.Server@1b09468
Exception in thread ""main"" java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)
        at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:134)
        at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:157)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:317)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:199)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:132)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:123)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:543)
Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""
        at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)
        at java.lang.Class.newInstance0(Class.java:344)
        at java.lang.Class.newInstance(Class.java:303)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)
        ... 8 more

Steps to reproduce:
1. Start namenode/datanode
2. Run hdfs_test program (part of libhdfs)
3. Stop namenode/datanode
4. goto step 1
"
HADOOP-506,"job tracker hangs on to dead task trackers ""forever""","I see cases where a task tracker gets disconnected from the job tracker and disconnects, and then appears twice in the job tracker's list, with one instance being alive and well, and the other's 'time since last heartbeat' increasing monotonically.
that all makes sense.
What doesn't make sense, is that the old instances never expire. It's been over 400000 seoncds since the last heartbeat. And the cluster reports having more nodes up and running than its size (350 nodes in a 320 node cluster).

there should be some reasonable timout for these expired task trackers, somewhere between 10 minutes and an hour."
HADOOP-505,Patch for external project it.could.webdav,"Context: 
http://issues.apache.org/jira/browse/HADOOP-496
http://could.it/main/a-simple-approach-to-webdav.html

This patch applies to an external project that Hadoop would use as a library.
You can obtain the baseline code:
svn co --revision {""1 Sep 2006 00:00:00 GMT""} http://could.it/repo/webdav/head/
Then apply the attached patch.

Patch details below.
---------------------
Changes in the patch are:

Filehandle leaks prevented it from working on Windows. (in.close() in 3 places)
Filehandle leaks would eventually crash server on Linux.

Now it passes the litmus compliance tests, in both original and HDFS mode.
Mostly had to refine the HTTP status codes.

Decoupled COPY and MOVE
This allows an implementation of MOVE other than COPY-and-delete

There is no new ""abstraction code"" to support a distributed filesystem backend.
Only: one change to make a java.io.File reference work when it represents a 
non-Windows path on Windows: (unixAbsolute = rootPath.startsWith(""/""))


The factory code via servlet properties was extended to allow loading by name:
repositoryClass=""it.could.webdav.DAVRepository""
repositoryClass=""it.could.webdav.XMLRepository""
repositoryClass=""org.apache.hadoop.dfs.webdav.HDFSRepository""

4 classes are subclassed for the alternative implementation:
A few private fields were made protected.
Some constructor logic moved to protected init() method-s so it can be overriden.
"
HADOOP-501,"toString(resources, sb) in Configuration.java throws a ClassCastException since resources can be loaded from an URL","Object obj = i.next();
if (obj instanceof Path) {
        sb.append((Path)obj);
} else {
        sb.append((String)obj);
}

If obj is an URL -> ClassCastException.

Moreover, I think the test before appending the resource to the StringBuffer is not really necessary since the method take an Object as argument. Why not simply have :

Object obj = i.next();
sb.append(obj);

I have attached a patch to fix this."
HADOOP-500,Datanode should scan blocks continuously to detect bad blocks / CRC errors,This is a spin-off of the discussion in HADOOP-95
HADOOP-499,Avoid the use of Strings to improve the  performance of hadoop streaming,"In hadoop streaming, a record is represented as a String for  I/O and is encoded as UTF8 for map/reduce. A record has to be converted between String and UTF8 back and forth multiple times and  this wastes CPU time. "
HADOOP-498,fsck should execute faster,"on a good size dfs, with a few 100000 blocks, fsck takes around 30 minutes, while dfs -du takes only a couple of seconds.
fsck should take seconds, rather than minutes. It should be a namenode operation, not a client program."
HADOOP-497,DataNodes and TaskTrackers should be able to report hostnames and ips relative to customizable network interfaces and nameservers,"This patch allows for network configuration parameters to be aded to the hadoop-site.xml file. These parameters specify a network interface name and an optional nameserver hostname which DataNodes and TaskTrackers consult to resolve  their hostnames from the IP bound to the specified network interface.

This is useful when machines that are part of different physical or logical network need to participate in hadoop clusters as client nodes. The hostname and IP reported by InetAddress.getLocalHost() are not necessarily the ones that will allow the JobTracker and NameNode to reach the clients, as well as not necessarily the ones through which the DFS clients can reach the DataNodes.

The configuration parameters are
 - cluster.report.nif
 - cluster.report.ns

nif: takes the name of a network interface, like en0, en1 (on macs), eth0, etc...
ns: the host name of a DNS server to use when resolving the IP bound to the specified nif

These parameters are set by default to the value ""default"" which will replicate the current behavior of reporting InetAddress.getLocalHost().getHostName() and getHostAddress()

As part of the patch, a new library dnsjava was added along with its license information (BSD license). The list of affected files is:

src
 org.apache.hadoop.dfs.DataNode 
 org.apache.hadoop.mapred.taskTracker 
 org.apache.hadoop.util.NetworkUtils
conf
 hadoop-default.xml
lib
 dnsjava-2.0.2.jar
 dnsjava-2.0.2.LICENSE.txt
"
HADOOP-495,distcp defeciencies and bugs,"distcp as currently implemented has several defeciencies and bugs which I encountered when trying to use it to import logs from HTTP servers into my local DFS cluster. In general, it is user unfriendly and does not do comprehensible error reporting. 
Here's a list of things that can be improved:

1) There isn't a man page that explains the various command line options. We should have one.
2) Malformed URLs cause a NullPointerException to be thrown with no error message stating what went wrong
3) Relative paths for the local filesystem are not handled at all
4) The schema used for HDFS URLs is dfs:// it ought to be hdfs://, 'dfs' is far to general an acronym to use in URLs
5) If a copy to the local filesystem is specified with a relative path, for instance
    ./bin/hadoop distcp dfs://localhost:8020/foo.txt foo.txt
then the job runs successfully but the file is nowhere to be seen. It looks like this gets copied to the map/reduce jobs
current working directory
6) If a copy to a dfs is specified and the namenode cannot be resolved, the job fails with an IOException, no comprehensible error message is printed
7) If an HTTP URI has a query component, it is disregarded when constructing the destination file name, for instance, if one specifies the following two URLs to be copied in a file list
  http://myhost.mydomain.com/files.cgi?n=/logs/foo.txt
  http://myhost.myfomain.com/files.cgi?n=/logs/bar.txt

a single file called 'files.cgi' is created and is overwritten by one or both source files, it's not clear which. The destination
path name should be constructed in the way that 'wget' does it, using the filename+query part of the URL, escaping characters as necessary.

8) It looks like if a list of URLs is specified in a file distcp runs a separate map reduce job for each entry in the file, why?
Seems like one could do a straight copy for local files since the task needs to run locally, followed by a single MR job that
copies HDFS and http URLs

"
HADOOP-494,hadoop utility returning exit code 0 always,"After switching from 0.4.0 to 0.5.0 my scripts using the hadoop utility (mainly hadoop dfs) did not work anymore because the exit code is 0 regardless whether it is successfull or failing.
This is obviously related to Hadoop-488, but I believe that after the fix in doMain the main routine in DFSShell.java needs to call System.exit based on return code of doMain or exception thrown."
HADOOP-493,hadoop dfs -report information is incorrect,"The information presented by hadoop dfs -report is incorrect.
In the following example, the total raw bytes are correct, the used raw bytes are incorrect and should actually be on the order of 138TB.
The total effective bytes is correct, and the actual replication factor is really a healthy 3.

yarnon@g1010:build:942>hadoop dfs  2> /dev/null -report | head
Total raw bytes: 240761579888640 (224226.69 Gb)
Used raw bytes: 35934229005360 (33466.35 Gb)
% used: 14.92%

Total effective bytes: 47093545108964 (43859.28 Gb)
Effective replication multiplier: 0.7630393703047027
-------------------------------------------------


In the following example again the total raw bytes and total effective bytes are correct, but the used raw bytes, % used and effective replication are a tad off the mark.

hadoop  dfs 2> /dev/null -report | head
Total raw bytes: 236476179599360 (220235.60 Gb)
Used raw bytes: -22028236104431 (-2.15 k)
% used: -9.31%

Total effective bytes: 55609049637201 (51789.96 Gb)
Effective replication multiplier: -0.39612682194976206
-------------------------------------------------
"
HADOOP-492,Global counters,"It would be nice to have map / reduce job keep aggregated counts for arbitrary events occuring in its tasks -- the numer of records processed, the numer of exceptions of a specific type, the number of sentences in passive voice, whatever the jobs finds useful.

This can be implemented by tasks periodically sending <name, value> pairs to the jobtracker (in some implementations such messages are piggy-backed on the heartbeats), so that the job tracker stores all the latests values from each task and aggregates them on a request.  It should also make the aggregated values available at the job end.  The value for a task would be flushed when the task fails.

#491 and #490 may be related to this one."
HADOOP-491,streaming jobs should allow programs that don't do any IO for a long time,"The jobtracker relies on task to send heartbeats  to know the tasks are still alive.
There is a 600 seconds timeout preset.
hadoop streaming also uses input to or output from the program it spawns to indicate progress, sending appropriate heartbeats.
Some spawned programs spend longer that 600 seconds without any output while being perfectly healthy.

It would be good to enhance the interface between hadoop streaming and the programs it spawns to track a healthy program in the absense of output.

There are certain dangers with this protocol: e.g. a task can run a separate thread that does nothing but send ""i'm alive"" message.   This would be a user bug to abuse the API in such way.  "
HADOOP-489,Seperating user logs from system logs in map reduce,"Currently the user logs are a part of system logs in mapreduce. Anything logged by the user is logged into the tasktracker log files. This create two issues-
1) The system log files get cluttered with user output. If the user outputs a large amount of logs, the system logs need to be cleaned up pretty often.
2) For the user, it is difficult to get to each of the machines and look for the logs his/her job might have generated.

I am proposing three solutions to the problem. All of them have issues with it -

Solution 1.
Output the user logs on the user screen as part of the job submission process. 

Merits- 
This will prevent users from printing large amount of logs and the user can get runtime feedback on what is wrong with his/her job.

Issues - 
This proposal will use the framework bandwidth while running jobs for the user. The user logs will need to pass from the tasks to the tasktrackers, from the tasktrackers to the jobtrackers and then from the jobtrackers to the jobclient using a lot of framework bandwidth if the user is printing out too much data.

Solution 2.
Output the user logs onto a dfs directory and then concatenate these files. Each task can create a file for the output in the log direcotyr for a given user and jobid.

Issues -
This will create a huge amount of small files in DFS which later can be concatenated into a single file. Also there is this issue that who would concatenate these files into a single file? This could be done by the framework (jobtracker) as part of the cleanup for the jobs - might stress the jobtracker.
 
Solution 3.
Put the user logs into a seperate user log file in the log directory on each tasktrackers. We can provide some tools to query these local log files. We could have commands like for jobid j and for taskid t get me the user log output. These tools could run as a seperate map reduce program with each map grepping the user log files and a single recude aggregating these logs in to a single dfs file.

Issues-
This does sound like more work for the user. Also, the output might not be complete since a tasktracker might have went down after it ran the job. 

Any thoughts?"
HADOOP-488,Change ToolBase.doMain() to return a status code,"I propose to change the signature of ToolBase.doMain from this:

    public final void doMain(Configuration conf, String[] args) throws Exception;

to this:

    public final int doMain(Configuration conf, String[] args) throws Exception;

and then change main() methods of all classes that use ToolBase.doMain to call System.exit(code), where ""code"" is this return value from doMain(). All command-line tools, which still call main() methods of other tools, should be converted to use doMain() and return exit codes properly.

The main reason for this change is that returning varying exit codes is required for proper operation of shell scripts, especially differing between code == 0 (normal exit) and code != 0 (error exit).

Additionally, current implementation of doMain() already gets the return code, it just silently discards it (ToolBase.java:184)."
HADOOP-487,misspelt DFS host name gives null pointer exception in getProtocolVersion,"I'm trying to construct a DistributedFileSystem but I have the wrong hostname.

It doesn't work.  It throws an exception.  Fair enough.  

However, it throws a null pointer exception inside $Proxy0.getProtocolVersion, the code that checks whether we have the right protocol version, rather than giving us any exception we could understand or taking an exception in code to which we have sources so we have a way of understanding the problem.

-dk
"
HADOOP-486,adding username to jobstatus,"Adding a new field to jobstatus that has the username, so that remotely doing jobclient.jobsToComplete() has the username with the jobstatuses."
HADOOP-485,allow a different comparator for grouping keys in calls to reduce,"Some algorithms require that the values to the reduce be sorted in a particular order, but extending the key with the additional fields causes  them to be handled by different calls to reduce. (The user then collects the values until they detect a ""real"" key change and then processes them.)

It would be much easier if the framework let you define a second comparator that did the grouping of values for reduces. So your reduce inputs look like:

A1, V1
A2, V2
A3, V3
B1, V4
B2, V5

instead of getting calls to reduce that look like:

reduce(A1, {V1}); reduce(A2, {V2}); reduce(A3, {V3}); reduce(B1, {V4}); reduce(B2, {V5});

you could define the grouping comparator to just compare the letters and end up with:

reduce(A1, {V1,V2,V3}); reduce(B1, {V4,V5});

which is the desired outcome. Note that this assumes that the ""extra"" part of the key is just for sorting because the reduce will only see the first representative of each equivalence class."
HADOOP-484,Additional splilts for last reduces?,"Often last few reduces take very long.  

Would it make sense, if hardware is available, to resplit their inputs into smaller chunks and to run multiple task instead?
"
HADOOP-483,Document libhdfs and fix some OS specific stuff in Makefile,Document libhdfs (api docs and user manual) and fix some OS specific stuff (paths to utilities like rm) in the Makefile and TestDFSCIO.java
HADOOP-482,unit test hangs when a cluster is running on the same machine,"Instead of hanging, it should report an error."
HADOOP-481,Hadoop mapred metrics should include per job input/output statistics rather than per-task statistics,"Currently hadoop reports metrics such as input bytes, input records, etc on per-task basis. Accurate aggregation of these metrics is required at the job-level and reporting should be done on a per-job basis."
HADOOP-478,Streaming should deliver stderr to the user.,"Streaming should collect stderr output of -mapper and - reduces steps and put it into the place accessible by the user.
E.g. it can go into
/user/name/stderr/job-id/task-id.txt

This is indpendent from MapReduce framework catching stderr of ""normal"" Java MapReduce jobs.  
A Java programmer has many ways to handle the problem on her own (at least until the framework matures and provides more help).
Unlike her, a user of Streaming has no access to files.  
Therefore it is an immediate requirement for Streaming to provide its own support for delivering this critical data to the user."
HADOOP-477,Streaming should execute Unix commands and scripts in well known languages without user specifying the path,"
If the executables for -mapper or -reducer are well-known (grep, cat, awk), Streaming should make sure that the executable is found.
If a script  for -mapper or -reducer are in a well-known language (.pl, .py), Streaming should  execute it  with the correct language processor.

Reason:
many jobs get started from machines with a different environment from that on the cluster.  
On another hand, different clusters may have different environments.  
Also, a user may have no access to the cluster machines.
Because of this, a user may be unable to specify correct paths for standard commands, and correct language processors for scripts.

Implementation:
Stream may tailr the commands by prepending the path, or the name of language processor.  
Another solution is to make sure that the commands are executed in a ""meaningful"" environment (with good $PATH, and other variables Unix users are accustomed to count upon).

Once again, Streaming is user facing tool -- it is not a library or a hackable example that the users are to modify for their needs.  So it should work out of the box."
HADOOP-476,Streaming should check for correctness of the task,"Currently, if anythin is wrong with streaming job, it dies without any explanation.

Before creating and running actual MapReduce job, Streaming should check if:

-- the executables (or scripts) for -mapper and -reducer are available and have right permissions
-- the input fragments exist
"
HADOOP-475,The value iterator to reduce function should be clonable,"In the current framework, when the user implements the reduce method of Reducer class, 
the user can only iterate through the value iterator once. 
This makes it hard for the user to perform join-like operations with in the reduce method. 
To address problem, one approach is to make the input value iterator clonable. Then the user can iterate the values in different ways.
If the iterator can be reset, then the user can perform nested iterations over the data, thus 
carry out join-likeoperations.

The user code in reduce method would be something like:

                  iterator1 = values.clone();
                  iterator2 = values.clone();
                 while (iterator1.hasNext()) {
                      val1 = iterator1.next();
                      iterator2.reset();
                      while (iterator2.hasNext()) {
                           val2 = iterator.next();
                           do something vased on val1 and val2
                           .......................
                      }
                 }

One possible optimization is that if the values are sorted based on a secondary key, 
the reset function can take a secondary key as an argument and reset the iterator to the begining
position of the secondary key. It will be very helpful if there is a utility that returns a list of iterators,
one per secondary key value, from the given iterator:

                          TreeMap getIteratorsBasedOnSecondaryKey(iterator);

Each entry in the returned map object is a pair of <secondary key, iterator for the values with the same secondary key>.

  "
HADOOP-474,support compressed text files as input and output,"I'd like TextInputFomat and TextOutputFormat to automatically compress and uncompress text files when they are read and written. Furthermore, I'd like to be able to use custom compressors as defined in HADOOP-441. Therefore, I propose:

Adding a map of compression codecs in the server config files:

io.compression.codecs = ""<suffix>=<codec class>,...""

so the default would be something like:

<property>
  <name>io.compression.codecs</name>
  <value>.gz=org.apache.hadoop.io.GZipCodec,.Z=org.apache.hadoop.io.ZipCodec</value>
  <description>A list of file suffixes and the codecs for them.</description>
</property>

note that the suffix can include multiple ""."" so you could support suffixes like "".tar.gz"", but they are just treated as literals against the end of the filename.

If the TextInputFormat is dealing with such a file, it:
  1. makes a single split
  2. decompresses automatically

On the output side, if mapred.output.compress is true, then TextOutputFormat would use a new property mapred.output.compression.codec that would define the codec to use to compress the outputs,  defaulting to gzip. "
HADOOP-473,TextInputFormat does not correctly handle all line endings,"The current TextInputFormat readLine method calls break on either a single '\r' or '\n' character.  This causes windows formatted text files '\r' '\n' to leave a trailing '\n' character and the next time the readLine method is called on the same input stream it returns a blank string.  The patch attached corrects this issue by looking for either single or double character line endings and positions the input stream to the next line.  It correctly handles windows, mac, and unix line endings."
HADOOP-471,JobTracker should assign unique jobids across restarts,"JobTracker assigns jobids starting from _001 every time it restarts. It should not reuse Ids for a large number of jobs ( 10,000 ?)  even if it is restarted. It will make it easy to track and analyze jobs in history by jobids . 
"
HADOOP-470,Some improvements in the DFS content browsing UI,"Some improvement requests from Yoram:

1. directory browsing: the size, replication and block size fields are unused, and indeed the replication field contains random junk. It would be useful to use these fields to represent the size of the folder (recursive, like du -s), and possibly the number of files in the folder.

2. since file sizes are typically very large, introducing a comma thousands separator will make them more readable. 

3. For a particular file I have the list of blocks that make it up. It would be useful to see the block placement information - which datanodes are holding that block. That's arguably more relevant than the block contents when clicking on the block.

4. a nit - 2048 may be too small a chunk size by default. The overhead of getting the first byte is so high (redirect, connect, handshake etc.) that you may as well get 10-20k as your first shot."
HADOOP-469,Portability of hadoop shell scripts for deployment,"Hadoop shell scripts are based on /bin/bash, which is a ""standard"" shell only on GNU/Linux. On other Unix systems like FreeBSD however the ""standard"" shell is /bin/sh. The attached patch addresses these compatiblity issues."
HADOOP-468,Setting scheduling priority in hadoop-env.sh,"It is not possible to set the scheduling priority in the Hadoop configuration. Attached is a patch for starting the Hadoop daemons through nice with the configured priority (can be configured in conf/hadoop-env.sh, for instance).

I would have included a patch for conf/hadoop-env.sh as well, but it seems to be ignored?"
HADOOP-467,"Building test cases broken on java-1.4, TextTest.java has string creation bug.","*Test Files
--- src/test/org/apache/hadoop/io/TestText.java (revision 432549)
--- src/test/org/apache/hadoop/mapred/MiniMRCluster.java        (revision 432549)
--- src/test/org/apache/hadoop/mapred/TestSequenceFileInputFilter.java  (revision 432549)
--- src/test/org/apache/hadoop/dfs/TestDFSShellGenericOptions.java      (revision 432549)

Broken on java-1.4
and a bug in static method getTestString(int len).
This function generates a character with code point equal or smaller then 0xFFFF, and converts this into array of chars of size 2(which should be one) and appened it buffer.
So resulting into a a null character tobe inserted into string after a no null character.
 ----
Find Patch attached."
HADOOP-465,Jobtracker doesn't always spread reduce tasks evenly if (mapred.tasktracker.tasks.maximum > 1),"I note that (at least for Nutch 0.8 Generator.Selector.reduce) if mapred.reduce.tasks is the same as the number of tasktrackers, and mapred.tasktracker.tasks.maximum is left at the default of 2, I typically have no reduce tasks running on a few of my tasktrackers, and two reduce tasks running on the same number of other tasktrackers.

It seems like the jobtracker should assign reduce tasks to tasktrackers in a round robin fashion, so that the distribution will be spread as evenly as possible. The current implementation would seem to waste at least some time if one or more slave machines have to execute two reduce tasks simultaneously while other tasktrackers sit idle, with the amount of wasted time depending on how dependent the reduce tasks were on the slave machine's resources.

I first thought that perhaps the jobtracker was ""overloading"" the tasktrackers that had already finished their map tasks (and avoiding those that were still mapping). However, as I understand it, the reduce tasks are all launched at the beginning of the job so that they are all ready and waiting for map output data when it first appears."
HADOOP-464,Troubleshooting message for RunJar (bin/hadoop jar),"Many users get this exception when using bin/hadoop
There are various reasons:
HADOOP_CONF_DIR or HADOOP_HOME misconfigured. 
""hadoop jar"" parameter incorrect

So the patch displays the path of the jar it is trying to open if this fails: 

Problem while opening hadoop job jar: /home/build/hadoop-streaming.jar
Exception in thread ""main"" java.util.zip.ZipException: error in opening zip file
        at java.util.zip.ZipFile.open(Native Method)
        at java.util.zip.ZipFile.<init>(ZipFile.java:203)
        at java.util.jar.JarFile.<init>(JarFile.java:132)
        at java.util.jar.JarFile.<init>(JarFile.java:70)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:80)
"
HADOOP-463,variable expansion in Configuration,"Add variable expansion to Configuration class.
=================

This is necessary for shared, client-side configurations:

A Job submitter (an HDFS client) requires:
<name>dfs.data.dir</name><value>/tmp/${user.name}/dfs</value>

A local-mode mapreduce requires:
<name>mapred.temp.dir</name><value>/tmp/${user.name}/mapred/tmp</value>

Why this is necessary :
=================

Currently we use shared directories like:
<name>dfs.data.dir</name><value>/tmp/dfs</value>
This superficially seems to work.
After all, different JVM clients create their own private subdirectory map_xxxx., so they will not conflict.

What really happens:

1. /tmp/ is world-writable, as it's supposed to.
2. Hadoop will create missing subdirectories. 
This is Java so that for ex. /tmp/system is created as writable only by the JVM process user
3. This is a shared client machine so next user's JVM will find /tmp/system owned by somebody else. Creating a directory within /tmp/system fails

Implementation of var expansion
=============
in class Configuration, 
The Properties really store things like put(""banner"", ""hello ${user.name}"");
In public String get(String name): postprocess the returned value:
Use a regexp to find the pattern ${xxxx}
Lookup xxxx as a system property
If found, replace ${xxxx} by the system property value.
Else leave as-is. An unexpanded ${xxxx} is a hint that the variable name is invalid.


Other workarounds 
===============
The other proposed workarounds are not as elegant as variable expansion.

Workaround 1: 
have an installation script which does:
mkdir /tmp/dfs
chmod uga+rw /tmp/dfs
repeat for ALL configured subdirectories at ANY nesting level
keep the script in sync with changes to hadoop XML configuration files.
Support the script on non-Unix platform
Make sure the installtion script runs before Hadoop runs for the first time.
If users change the permissions/delete any of the shared directories, it breaks again.

Workaround 2: 
do the chmod operations from within the Hadoop code.
In pure java 1.4, 1.5 this is not possible.
It requires the Hadoop client process to have chmod privilege (rather than just mkdir privilege)
It requires to special-case directory creation code.


"
HADOOP-462,DFSShell throws out arrayoutofbounds exceptions if the number of arguments is not right,"All the DFSShell commands do not check for the number of arguments. In DFSShell.run(), the args length is not checked before calling copyfromlocal, movefromlocal, copytolocal, and some others as well. This throws out arrrayoutofbounds exception in case lesser number of arguments is specified."
HADOOP-461,"Build broken forjava version ""1.4.2_09""","Build is broken on java 1.4.2_09.

Method java.lang.Boolean.parseBoolean(String val) is  available since Java 5
http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Boolean.html#parseBoolean(java.lang.String)"
HADOOP-460,Small jobs benchmark fails with current Hadoop due to UTF8 -> Text ClassCastException,"Small jobs benchmark uses UTF8 as Map input keys, it fails with ClassCastException ( UTF8 / Text ) with the latest trunk. "
HADOOP-459,libhdfs leaks memory when writing to files,hdfsWrite leaks memory when called repeatedly. The same probably applies to repeated reads using hdfsRead
HADOOP-458,libhdfs corrupts memory,"in getJNIEnv() not enough space for optHadoopClassPath is allocated, corrupting the memory."
HADOOP-457,Unable to create file in dfs in code run on machine not in the cluster.,"I've got a tiny hadoop cluster and one machine that's not in the cluster that rsync the latest version off the namenode/jobtracker machine. The machine that's not in the cluster is then responsible for starting a few jobs here and there.

However, the latest trunk is unable to create files in the dfs from my code.
I'm using a simple new OutputStreamWriter(fs.create(file))); and the file creation looks like it's going ok but then this gets thrown:

org.apache.hadoop.ipc.RemoteException: java.io.IOException: failed to create file /user/hadoop/submissions/1150000000/1152000000/1152440000/.1155824015.negsub.crc on client ""servernotincluster"" because target-length is 0, below MIN_REPLICATION (1)
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:410)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:202)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:332)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:468)

        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:159)


If I add that machine to the conf/slaves it works fine. Note that it's trying to create that file on the server that's not in the cluster."
HADOOP-456,Checkpointing and logging of data node descriptors,"This is a partial implementation of the DFS cluster startup process laid out in HADOOP-306.
The description of the patch is here:
http://issues.apache.org/jira/browse/HADOOP-306#action_12427634
"
HADOOP-455,Text class should support the DEL character,"The DEL character (aka 0x7F) is valid UTF-8, but the Text class does not support it."
HADOOP-454,hadoop du optionally behave like unix's du -s,"In Unix, the user has the option to specify a -s flag:
     -s      Display an entry for each specified file.  (Equivalent to -d 0)

Instead of providing data for the subdirectories of the arguments, du prints the total usage of each argument, without printing subdirectories.

This is very convenient for those looking to find how much space is used by a particular branch, without requring them to add up all the sub-usages.
"
HADOOP-453,bug in Text.setCapacity( int len ),"There seems to be a bug in o.a.h.i.Text class in methd setCapacity( int len ) attached patch demonstrates the issue with unit test and
fixes the problem."
HADOOP-451,Add a Split interface,"The InputFormat interface has a method:

FileSplit[] getSplits();

This should change to:

Split[] getSplits();

The Split interface would look like:

public interface Split extends Writable {

  /** Returns a list of hosts that contain this split.
       This is only used to optimize task placement, so this may be empty. */
  String[] getLocations(FileSystem fs);

  /** The relative, estimated cost of operating on this.  Typically the size of the data in the split.
       Used to prioritize tasks in a job (high-cost tasks are run first).  */
   long getCost();
}"
HADOOP-450,Remove the need for users to specify the types of the inputs,"Currently, the application specifies the types of the input keys and values and the RecordReader checks them for consistency. It would make more sense to have the RecordReader define the types of keys that it will produce. Therefore, I propose that we add two new methods to RecordReader:

WritableComparable createKey();
Writable createValue();

Note that I propose adding them to the RecordReader rather than the InputFormat, so that they can specific to a particular input split."
HADOOP-448,DistributedFileSystem uses the wrong user.name to set the working directory.,"DistributedFileSystem initializes the working directory to be new Path(""/user"", System.getProperty(""user.name"")); rather than new Path(""/user"", conf.get(""user.name"")); the initialization would have to be moved to the constructor to access conf.

If relative paths are used, the remote tasks and JobTracker will use the uid they are running with rather than the id of the user that submitted the job."
HADOOP-447,DistributedFileSystem.getBlockSize(Path) does not resolve absolute path,getBlockSize() does not check for an absolute path like the rest of the DistributeFileSystem API does. Consequently getBlockSize(Path) does not work with relative paths.
HADOOP-445,Parallel data/socket writing for DFSOutputStream,"Currently, as DFS clients output blocks they write the entire block to disk before starting to transmit to the datanode. By writing to disk the client is able to retry a block write if the datanode files in the middle of a block transfer. Writing to disk and then to the datanode adds latency. Hopefully, the common case is that block transfers to datanodes are successful. This patch writes to the datanode and the disk in parallel. If the write to the datanode fails, it falls back to current behavior.

In my tests of transmits of 237M and 946M datasets using -copyFromLocal I'm seeing a 20-25% improvement in throughput."
HADOOP-444,"In streaming with a NONE reducer, you get duplicate files if a mapper fails, is restarted, and succeeds next time.","When the dust settled after a streaming run, the directory ended up looking like this:

  /user/dking/<project-name>/K-HTML-UTF8-2006-08-09-rescued-abstracted/task_0026_m_007384_0	<r 3>	10563406
  /user/dking/<project-name>/K-HTML-UTF8-2006-08-09-rescued-abstracted/task_0026_m_007384_1	<r 3>	10563406

Future processing will receive duplicated data.

-dk
"
HADOOP-442,"slaves file should include an 'exclude' section, to prevent ""bad"" datanodes and tasktrackers from disrupting  a cluster","I recently had a few nodes go bad, such that they were inaccessible to ssh, but were still running their java processes.
tasks that executed on them were failing, causing jobs to fail.
I couldn't stop the java processes, because of the ssh issue, so I was helpless until I could actually power down these nodes.
restarting the cluster doesn't help, even when removing the bad nodes from the slaves file - they just reconnect and are accepted.
while we plan to avoid tasks from launching on the same nodes over and over, what I'd like is to be able to prevent rogue processes from connecting to the masters.
Ideally, the slaves file will contain an 'exclude' section, which will list nodes that shouldn't be accessed, and should be ignored if they try to connect. That would also help in configuring the slaves file for a large cluster - I'd list the full range of machines in the cluster, then list the ones that are down in the 'exclude' section"
HADOOP-441,SequenceFile should support 'custom compressors',"SequenceFiles should support 'custom compressors' which can be specified by the user on creation of the file. 

Readily available packages for gzip and zip (java.util.zip) are among obvious choices to support. Of course there will be hooks so that other compressors can be added in future as long as there is a way to construct (input/output) streams on top of the compressor/decompressor.

The 'classname' of the 'custom compressor/decompressor' could be stored in the header of the SequenceFile which can then be used by SequenceFile.Reader to figure out the appropriate 'decompressor'. Thus I propose we add constructors to SequenceFile.Writer which take in the 'classname' of the compressor's input/output stream classes (e.g. DeflaterOutputStream/InflaterInputStream or GZIPOutputStream/GZIPInputStream), which acts as the hook for future compressors/decompressors.
"
HADOOP-440,"In streaming, error messages issued in stream mappers or reducers don't go anywhere",I would like such error messages to go to the machines' logs at least.
HADOOP-439,Streaming does not work for text data if the records don't fit in a short UTF8 [2^16/3 characters],"The streaming code internally reads the input data into a UTF8 .  This causes truncated data to be shipped to the mapper when the input exceeds about 21000 characters, with no notice to the user except possibly in individual tasks' machines' logs, which people would not normally read for apparently successful jobs."
HADOOP-438,DFS pathname limitation.,"I was trying to create a deep hierarchy of directories using DFS mkdirs().
When the path to the leaf directory became long (~20000) DFS was still able to create
directories with these names, but UTF8 started truncating long strings resulting in
incorrect logging of namespace edits. That later crashed the namenode during restart,
when it was trying to reproduce file creation logged in the edits file with truncated names.
UTF8 is deprecated now so we will have to replace it with Text.
With UTF8  we should enforce a pathname limit of 0xffff/3 = 21845
With Text it is going to be larger. Not sure what the exact number is.
"
HADOOP-437,support gzip input files,"To specify that line-oriented input is in gzip format:
   -jobconf stream.recordreader.compression=gzip

Effect: 
1. FileSplit are forced to be whole files
2. A GZIPInputStream filter is applied in the RecordReader


"
HADOOP-436,Concluding that the Map task failed may not be always right in getMapOutput.jsp,"In the getMapOutput.jsp, tracker.mapOutputLost is called irrespective of whether the sender of the map output threw an exception or the receiver died (thereby closing the socket causing the jsp to throw an exception). Invoking the method causes the corresponding map task to fail and causes it to be rescheduled but it may be the case that the problem is at the reciever's end. Need to do a better check of where the exception was thrown."
HADOOP-435,Encapsulating startup scripts and jars in a single Jar file.,"Currently, hadoop is a set of scripts, configurations, and jar files. It makes it a pain to install on compute and datanodes. It also makes it a pain to setup clients so that they can use hadoop. Everytime things are updated the pain begins again.

I suggest that we should be able to build a single Jar file that has a Main-Class defined with the configuration built in so that we can distribute that one file to nodes and clients on updates. One nice thing that I haven't done would be to make the jarfile downloadable from the JobTracker webpage so that clients can easily submit the jobs.

I currently use such a setup on my small cluster. To start the job tracker I used ""java -jar hadoop.jar -l /tmp/log jobtracker"" to submit a job I use ""java -jar hadoop.jar jar wordcount.jar"". I used the client on my linux and Mac OSX machines and I'll I need installed in java and the hadoop.jar file.

hadoop.jar helps with logfiles and configurations. The default of pulling the config files from the jar file can be overridden by specifying a config directory so that you can easily have machine specific configs and still have the same hadoop.jar on all machines.

Here are the available commands from hadoop.jar:
USAGE: hadoop [-l logdir] command
  User commands:
    dfs          run a DFS admin client
    jar          run a JAR file
    job          manipulate MapReduce jobs
    fsck         run a DFS filesystem check utility
  Runtime startup commands:
    datanode     run a DFS datanode
    jobtracker   run the MapReduce job Tracker node
    namenode     run the DFS namenode (namenode -format formats the FS)
    tasktracker  run a MapReduce task Tracker node
  HadoopLoader commands:
    buildJar     builds the HadoopLoader jar file
    conf         dump hadoop configuration

Note, I don't have the classes for hadoop streaming built into this Jar file, but if I had that would also be an option (it checks for needed classes before displaying an option). It makes it very easy for users that just write scripts to use hadoop straight from their machines.

I'm also attaching the start.sh and stop.sh scripts that I use. These are the only scripts I use to startup the daemons. They are very simple and the start.sh script uses the config file to figure out whether or not to start the jobtracker and the nameserver.

The attached patch adds the HadoopIt patch, modifies the Configuration class to find the config files correctly, and modifies the build to make a fully contained hadoop.jar. To update the configuration in a hadoop.jar you simply use ""zip hadoop.jar hadoop-site.xml""."
HADOOP-434,Use Hadoop scripts to run smallJobsBenchmark to avoid classpath issues.,"Small job benchmark uses its own scripts to set classpath, it should use hadoop scripts with ProgramDriver to run the benchmark. "
HADOOP-433,Better access to the RecordReader,"The record reader has access to the FileSplit which can in turn have information that is useful to the Mapper. For example, Map processing may vary according to file name or attributes associated with a file. Unfortunately, even using a MapRunner you only have access to the progress wrapper of the RecordReader. To get access to the real record reader I had to use a thread local variable which I set in RecordReader.getNext(). It would be much nicer if you could get a reference to the real RecordReader from the RecordReader passed to MapRunner."
HADOOP-432,"support undelete, snapshots, or other mechanism to recover lost files","currently, once you delete a file it's gone forever.
most file systems allow some form of recovery of deleted files.
a simple solution would be an 'undelete' command.
a more comprehensive solution would include snapshots, manual and automatic, with scheduling options."
HADOOP-431,"default behaviour of dfsShell -rm should resemble 'rm -i', not 'rm -rf'","when using hadoop dfs -rm <path> the behaviour is like that of rm -rf.
That's very dangerous.
A better behaviour would require a confirmation and allow removal of files only by default. Optional flags would allow recursive and forced deleted."
HADOOP-430,http server should die if the Datanode fails.,"HADOOP-375 changed the order in which a data node registers and starts its http server.
Current version first starts the http server, then registers. As a result if the registration fails,
which can happen e.g. if the datanode has illegal storage id (=does not belong to the cluster),
the http server keeps running, which makes the data node seem running too.
I think 2 things need to be done.
1) Datanode registration should precede the http server startup.
2) The http server should shutdown when the datanode fails (in general).
I am attaching a patch for the first item.
Does anybody know how the latter can be done in the version of jetty we are on now?
"
HADOOP-428,Condor and Hadoop Map Reduce integration,"The issue is about using/enhancing Condor's features for Hadoop's Map Reduce framework. Some of the early thoughts in this respect:
* One should be able to submit a MR job that takes advantage of Condor's features like node reservation according to a job's requirements, monitoring of jobs, etc.
* JobTracker and TaskTrackers work as Master/Workers in the Condor environment. One should be able to simply start a MR cluster and the cluster goes down when the job is done.
* The classads can have an attribute for input file block locations that will be an input to Condor's scheduling decisions.
* Condor's features of monitoring jobs can be leveraged to reschedule failed TaskTrackers. Checkpointing of JobTrackers can also probably be done so that if the JobTracker job dies for some reason, the failed jobs can be restarted to start from the point where the JobTracker was last checkpointed at (assuming the input data has not changed).
* User priorities, job priorities should also be handled. If nodes are currently in use due to a job being run by one user, and another user of the same priority submits a new job, it gets queued and opportunistically the job of the second user is scheduled - for e.g., one master and 1 worker to start with and then 2 workers and so on... If the second user is of a higher priority, then the first user's job is completely suspended.
Please add your thoughts on this topic."
HADOOP-427,DatanodeInfo class should be used instead of DatanodeDescriptor in the jsp & related files,"DatanodeDescriptor should be replaced by DatanodeInfo since DatanodeInfo is a public class and meant to be used by others. Specifically, DatanodeDescriptor is referenced in dfshealth.jsp & JspHelper.java files. Change those references to DatanodeInfo."
HADOOP-426,streaming unit tests failing on SunOS,"This is causing nightly builds to fail, since these run on a Solaris box.

java.lang.RuntimeException: Operating system SunOS not supported by this class
        at org.apache.hadoop.streaming.Environment.<init>(Environment.java:47)
        at org.apache.hadoop.streaming.StreamJob.init(StreamJob.java:68)
        at org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:55)
        at org.apache.hadoop.streaming.TestStreaming.testCommandLine(Unknown Sour"
HADOOP-425,a python word count example that runs under jython,I translated the word count example into python and convert it into a jar using jython. It provides a nice example of writing Hadoop map/reduce programs in python.
HADOOP-424,mapreduce jobs fail when no split is returned via inputFormat.getSplits,"I'm using a MapReduce job to process some data logged and timestamped into files.
When the job runs, it does not process the whole data, but filters only the data that has been logged since the last job run.

However, when no new data has been logged, the job fails because the getSplits method of InputFormat returns no split. Thus the number of map tasks is 0. This is not intercepted, and the job fails at reduce step because it seems it does not find any data to process:

java.io.FileNotFoundException: /local/home/hadoop/var/mapred/local/task_0030_r_000000_3/all.2 at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:121) at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:47) at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:221) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:150) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:259) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:253) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:241) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1013)

What should be Hadoop's behaviour in such a case?

IMHO, the job should be considered as successful. Indeed, this is not a job failure, but just a lack of input data. WDYT?

"
HADOOP-423,file paths are not normalized,"Paths containing  '.' or '..' are not normalized -- the corresponding directories actually seem to be created. This is inconsistent and unexpected behavior, and  is particularly painful when running from a working directory, using relative paths that start with ..."
HADOOP-422,ant test is failing with TestLocalDFS and TestMiniMRWithDFS failing,"org.apache.hadoop.ipc.RPC$VersionMismatch: Protocol org.apache.hadoop.dfs.ClientProtocol version mismatch. (client = 1, server = 2)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:250)
        at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:103)
        at org.apache.hadoop.dfs.DistributedFileSystem.<init>(DistributedFileSystem.java:47)
        at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:101)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:86)
        at org.apache.hadoop.dfs.MiniDFSCluster.getFileSystem(MiniDFSCluster.java:146)
        at org.apache.hadoop.dfs.TestLocalDFS.testWorkingDirectory(TestLocalDFS.java:42)

This is the error I am getting when running ant test on TestLocalDFS.
Its the same error (version mismatch) for TestMiniMRWithDFS."
HADOOP-421,replace String in hadoop record io with the new Text class,The record io in Java is currently using String and should be using the new Text class.
HADOOP-420,Accumulate bytes & records statistics at the job level via haoop.metrics.,"We'd like to accumulate the statistics (and get graphs) at the job level rather than the job level, because it will provide a lot more useful information. This likely implies that they need to be rolled up in the TaskStatus reported up to the JobTracker."
HADOOP-419,libdfs doesn't work with application threads,"Using libdfs from C++ using threads I get a null pointer for the ThreadClassLoader, which throws an exception."
HADOOP-418,hadoopStreaming test jobconf -> env.var. mapping,"The subprocess loads environment variables and tests for expected JobConf properties.

"
HADOOP-417,Replace Jetty5 with Jetty6,"Possibly a solution for HADOOP-401?
"
HADOOP-415,DFSNodesStatus() should sort data nodes.,"HADOOP-392 introduced a new TreeMap member (datanodeMapByName) in the name node.
It is used solely for reporting the data nodes in the UI sorted by their names.
I think it is quite inefficient both time and space-wise to support an excessive
data structure just for that.
Suppose the UI will also require sorting by last heartbeat and/or by the available space....
I think DFSNodesStatus() should just sort the original datanodeMap before returning the list.
"
HADOOP-413,streaming: replace class UTF8 with class Text,"need to support long lines in hadoopStreaming.
So replace class UTF8 with class Text or with line-oriented binary I/O
"
HADOOP-412,provide an input format that fetches a subset of sequence file records,Sometimes a map/red job only wants to work on a subset of input data for the needs of its apllication or at the debugging phase. It would be convenient if an input format transparently handles this. It should provide an API that allows a programmer to specify a filtering criteria.
HADOOP-411,junit test for HADOOP-59: support generic command-line options,This issue will proivde Junit test cases for HADOOP-59.
HADOOP-410,Using HashMap instead of TreeMap for some maps in Namenode yields 17% performance improvement,"For blocksMap in FSNameSystem and activeBlocks map in FSDirectory, if we use HashMap instead of TreeMap, it yields 17% performance improvement (without significant difference in memory consumption).
I am attaching a patch, alongwith a namenode benchmark."
HADOOP-409,expose JobConf properties as environment variables,"expose JobConf properties as environment variables (for the benefit of hadoopStreaming apps)

This patch depends on  HADOOP-345
"
HADOOP-408,Unit tests take too long to run.,"Several of the unit tests take over a minute to run.  I suspect most of this time is waiting for daemons to stop.  Perhaps we can restructure daemon shutdown in the test frameworks so that all daemons are first asked to shutdown, and then wait for the actual shutdown of all daemons in the TestCase's tearDown() method.  Also, we might try to combine some unit tests that require the same daemons under a single TestCase caller, so that they share setup/tearDown costs.  Or we could try to simply make daemon shutdowns faster."
HADOOP-407,the SequenceFile sorter should take a Progressable that should be used in map/reduce sort,"Currently, when the framework is sorting it creates a side thread that sends fake progress reports back to the task tracker once a second. This can cause problems if there is a problem in the sort getting stuck. It would be far better to have the sort take a Progressable that can report that progress is being made in the sort."
HADOOP-406,Tasks launched by tasktracker in separate JVM can't generate log output,"Child JVM's don't have access to logging config system properties. When the child JVM gets launched, it doesn't inherit the Java system properties hadoop.log.dir and hadoop.log.file (which are actually based on the Bash environment variables $HADOOP_LOG_DIR and $HADOOP_LOGFILE). This means that you get no log messages from the actual map/reduce tasks that are executing.

Stefan Groschupf reported this problem a while back:

-------------------------------------------------------------------------
To: hadoop-dev@lucene.apache.org
From: Stefan Groschupf <sg@media-style.com>
Subject: tasks can't log bug?
Date: Tue, 25 Jul 2006 19:26:17 -0700
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Hadoop developers,

I'm confused about the way logging works within map or reduce tasks.
Since tasks are launched in a new JVM  the java system properties ""hadoop.log.dir"" and ""hadoop.log.file"" are not passed to the new JVM.
This prevents the child process from logging properly. In fact you get:

 java.io.FileNotFoundException: / (Is a directory)
  at java.io.FileOutputStream.openAppend(Native Method)
  at java.io.FileOutputStream.<init>(FileOutputStream.java:177)
  at java.io.FileOutputStream.<init>(FileOutputStream.java:102)
  at org.apache.log4j.FileAppender.setFile(FileAppender.java:289)
  at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:165)
  at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163)
  at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256)
  at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132)
  at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96)
  at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654)
  at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612)
  at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.j
2006-07-25 15:59:07,553 INFO  mapred.TaskTracker (TaskTracker.java:main(993)) - Child
  at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415)
  at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441)
  at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:4
  at org.apache.log4j.LogManager.<clinit>(LogManager.java:122)
  at org.apache.log4j.Logger.getLogger(Logger.java:104)
  at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)
  at org.apache.commons.logging.impl.Log4JLogger.<init>(Log4JLogger.java:65)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImp
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcc
  at java.lang.reflect.Constructor.newInstance(Constructor.java:494)
  at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529
  at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235
  at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370)
  at org.apache.hadoop.mapred.TaskTracker.<clinit>(TaskTracker.java:44)
  at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:993)

We see several ways to solve this problem. First retrieve the properties ""hadoop.log.dir"" and ""hadoop.log.file"" from the mother JVM and then pass them to the child JVM as within the args parameter.
Second would be to  access the environment variables ""$HADOOP_LOG_DIR"" and ""$HADOOP_LOGFILE"" using System.getEnv (java 1.5).
Third there would be a more general solution. Taskrunner would resolve any environment variables it found in ""mapred.child.java.opts"" by lookup the value using System.getEnv().
Eg:
unix:
export MAX_MEMORY = 200
hadoop-site.xml:
<name>mapred.child.java.opts</name>
<value>-Xmx${MAX_MEMORY}</value>
"
HADOOP-405,Duplicate browseDirectory.jsp,"I found 2 identical files
src\webapps\datanode\browseDirectory.jsp
and
src\webapps\dfs\browseDirectory.jsp

And there is a line in build.xml that makes the copy
    <copy file=""${src.webapps}/datanode/browseDirectory.jsp"" todir=""${src.webapps}/dfs/""/>

That does not seem right. If one of them is a copy of another, then only one should be in svn.
Another thing is that while copying files to the build/ directory is OK, copying files around within
the source directories makes things complicated. Is there a way to avoid that?"
HADOOP-404,Regression tests are not working.,"TestMiniMRLocalFS gets stuck on running the reduce. THe reduce never seems to return. The test kept on running for more than an hour and the reduce never returned. I tried ant test with revision 427236
 which is just before the patch for HADOOP-362, and the tests seemed to work fine. I have not yet diagnozed the problem."
HADOOP-403,close method in a Mapper should be provided with OutputCollector and a Reporter,"For mappers with side-effects, or mappers that work as aggregators (i.e. no output on individual key-value pairs, but an aggregate output at the end of all key-value pairs), output should be performed in the close method. For this purpose, we need to supply output collector and reporter to the close method of Mapper. This involves interface change, though. Thoughts ?"
HADOOP-402,TaskTracker should shutdown when it receives the VersionMismatch exception.,"Like Datanodes TaskTracker should shutdown when it receives the VersionMismatch exception rather than
infinitely retrying to re-connect. The result will be the same.
"
HADOOP-400,the job tracker re-runs failed tasks on the same node,"The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it.

I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster. Thus, for ""normal"" clusters with more than 4 TaskTrackers, you will be guaranteed that it will run on 4 different TaskTrackers. For small clusters, it will run on every TaskTracker in the cluster at least once.

Does that sound reasonable to everyone?"
HADOOP-399,the javadoc currently generates lot of warnings about bad fields,"When building the ""tar"" target, I get the following warnings:

  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DFSck.java:475: warning - @return tag has no arguments.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java:46: warning - Tag @link: reference not found: DatanodeRegistration
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/dfs/DatanodeProtocol.java:46: warning - Tag @see: reference not found: FSNamesystem#registerDatanode(DatanodeRegistration)
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/BytesWritable.java:63: warning - @return tag has no arguments.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/SequenceFile.java:252: warning - Tag @link: can't find Reader(FileSystem,Path,Configuration) in org.apache.hadoop.io.SequenceFile.Reader
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/SequenceFile.java:81: warning - Tag @link: can't find Writer(FileSystem,Path,Class,Class) in org.apache.hadoop.io.SequenceFile.Writer
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/Text.java:104: warning - @returns is an unknown tag.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/Text.java:413: warning - @param argument ""utf8:"" is not a parameter name.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/io/WritableUtils.java:338: warning - @param argument ""i:"" is not a parameter name.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/java/org/apache/hadoop/mapred/JobConf.java:77: warning - @param argument ""conf"" is not a parameter name.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/ExampleDriver.java:29: warning - Tag @author cannot be used in method documentation.  It can only be used in the following types of documentation: overview, package, class/interface.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/ExampleDriver.java:29: warning - @date is an unknown tag.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/PiBenchmark.java:62: warning - @param argument ""value"" is not a parameter name.
  [javadoc] /home/oom/work/eclipse/hadoop-trunk/src/examples/org/apache/hadoop/examples/PiBenchmark.java:106: warning - @ is an unknown tag.
"
HADOOP-396,Writable DatanodeID,"This patch implements Owen's comments to HADOOP-321. Namely,
- Namenode.sendHeartbeat does not create a throwaway DatanodeDescriptor. DatanodeID is sufficient in this case.
- DatanodeID is made Writable, and it also implements equals() and hashCode().
- DatanodeInfo is updated to call base class methods in its implementation of Writable interface.
- I changed the way strings are written/read using UTF8.writeString and UTF8.readString.
- Some comments are updated.
"
HADOOP-395,infoPort field should be a DatanodeID member,"I have a couple of comments on HADOOP-375. Sorry missed that discussion when the patch was under construction.
1) As it is done now infoPort is a member of two classes DatanodeInfo and DatanodeRegistration, which in fact have a
common base DatanodeID. It seems more logical to place infoPort into DatanodeID. This will let us handle the port
assignments inside the constructors rather than outside, and will prevent from unsafe ""dynamic casts"" like the one
found in FSNamesystem.gotHeartbeat( DatanodeID nodeID, ... )
            nodeinfo.infoPort = ((DatanodeRegistration)nodeID).infoPort;
2) Also, should we make infoPort short rather than integer?
Since it is a part of a heartbeat message we might want keeping it small.
3) Member getters should start with get<MemberName>
- public int infoPort() { return infoPort; }
+ public int getInfoPort() { return infoPort; }
"
HADOOP-394,MiniDFSCluster shudown order,"I propose to reverse the order of shutdown in MiniDFSCluster.
Currently it shutdowns the name node first and then the data node,
which causes the data node throwing unnecessary Exceptions in unit test."
HADOOP-393,The validateUTF function of class Text throws MalformedInputException for valid UTF8 code containing ascii chars,The validateUTF function does not handle ascii chars right therefore causing MIE exception to be thrown.
HADOOP-392,Improve the UI for DFS content browsing,"A couple of improvements requested:

* change the title of the namenode page to include namenode
    Hadoop Administration
    -> Hadoop NameNode (<host>:<port>)

* Move all the info except the per node info from the dfshealth.jsp page to the home page.  No reason to make folks go through that page to get summary info or to find local logs.

* It would  be good if the health status information were sorted by node name, rather than the (random) order that -report returns

* Looking at an xml file copied to the dfs, can't view/tail it in the browser. Can download the file and view it locally.

* Viewing a java source file, the newlines don't come out right. Again, downloading and viewing locally is fine.

* When clicking on a file, it would be good to view its contents (or some subset of it) immediately, with the advanced options appearing at the bottom (block locations [just choose one], number of blocks view/tail/download/chunk size).

* Fix the filename problem (browser related) [e.g. /user/yarnon/build.xml gets translated to -user-yarnon-build.xml]


"
HADOOP-391,test-contrib with spaces in classpath (Windows),"On Windows, ant test-contrib (streaming) with fail if classpath contains spaces.
-    vargs.add(System.getProperty(""java.class.path""));
+    vargs.add(""\"""" + System.getProperty(""java.class.path"") + ""\"""");
"
HADOOP-390,compile-core-test target depends on compile-examples,"Compiling with compile-core-test target returns the following error:
    [javac] hadoop\src\test\org\apache\hadoop\mapred\TestMiniMRWithDFS.java:28: package org.apache.hadoop.examples does not exist
    [javac] import org.apache.hadoop.examples.WordCount;
    [javac]                                   ^
This target should depend on the compile-examples target."
HADOOP-389,MiniMapReduce tests get stuck because of some timing issues with initialization of tasktrackers.,The MiniMapReduce tests sometimes calls shutdown before the tasktrackers have been initialized. This makes the TestMiniMRBringup run for ever. 
HADOOP-388,"the hadoop-daemons.sh fails with ""no such file or directory"" when used from a relative path","The new shell scripts fail with a relative directory:

% current/bin/hadoop-daemons.sh start datanode
node1: current/bin/..: No such file or directory
node2: current/bin/..: No such file or directory

The problem is that HADOOP_HOME is set to a relative directory and hadoop-daemons does a cd, breaking the other scripts."
HADOOP-387,LocalJobRunner assigns duplicate mapid's,"While hunting down nutch issue NUTCH-266 i discovered that id's are generated with following fragment of code:

    private String newId() {
      return Integer.toString(Math.abs(new Random().nextInt()),36);
    }

and the related Javadoc:
""
public Random()

    Creates a new random number generator. Its seed is initialized to a value based on the current time:

         public Random() { this(System.currentTimeMillis()); }

    Two Random objects created within the same millisecond will have the same sequence of random numbers.

""

it appears that in this case there are more than one Random pobject generated at the same millisecond and id's are
no longer unique.
"
HADOOP-386,Periodically move blocks from full nodes to those with space,"I'm still having *a lot* of problems with some nodes filling up quickly and others hardly being touched, mostly because of the hardware being
very different.

As someone suggested, there should be a thread that periodically checks the dfs for nodes with little or no free space and schedules blocks
to be moved off that node."
HADOOP-385,rcc does not generate correct Java code for the field of a record type,"If  a field of a Jute record  is also a Jute record and the target language is Java, rcc does not generate correct field name, correct code for deserializing the field. The access modifier ""private"" of the validation method causes problem for validating nested records.

In addition, rcc can not handle the case when a comment started with ""//"" also contains the string ""//""."
HADOOP-384,improved error messages for file checksum errors,Improves the messages on a couple of the failures we've been seeing to try and get enough information to identifiy the problem.
HADOOP-383,unit tests fail on windows,"When I run the ""ant test"" target under Windows, I get the following exception in the logs for the streaming unit tests. The unit tests need to run under Windows, if it is going to be a supported platform. So we either need to remove the streaming unit tests or make them work.

Testsuite: org.apache.hadoop.streaming.TestStreaming
Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.172 sec

Testcase: testCommandLine took 0.172 sec
        FAILED
java.lang.RuntimeException: Operating system Windows XP not supported by this class
        at org.apache.hadoop.streaming.Environment.<init>(Environment.java:47)
        at org.apache.hadoop.streaming.StreamJob.init(StreamJob.java:68)
        at org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:55)
        at org.apache.hadoop.streaming.TestStreaming.testCommandLine(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at junit.framework.TestCase.runTest(TestCase.java:154)
        at junit.framework.TestCase.runBare(TestCase.java:127)
        at junit.framework.TestResult$1.protect(TestResult.java:106)
        at junit.framework.TestResult.runProtected(TestResult.java:124)
        at junit.framework.TestResult.run(TestResult.java:109)
        at junit.framework.TestCase.run(TestCase.java:118)
        at junit.framework.TestSuite.runTest(TestSuite.java:208)
        at junit.framework.TestSuite.run(TestSuite.java:203)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:2
97)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.jav
a:672)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:
567)

"
HADOOP-382,add a unit test for multiple datanodes in a machine,"recently we saw a bug present itself only when multiple data nodes are started on a single machine.
A unit test that starts multiple data nodes would expose such bugs before they happen in a real installation."
HADOOP-381,keeping files for tasks that match regex on task id,"For debugging map/reduce jobs, if a single task is producing bad results, but *not* failing, it is hard to debug the problem. This patch lets you set a pattern for task ids that will keep their files from being deleted when the task and job complete. This allows the developer to run the task in the IsolationRunner under the debugger."
HADOOP-380,The reduce tasks poll for mapoutputs in a loop,The Reduce tasks poll for the mapoutputs in a loop. The polling thread should be sleeping for 5 seconds before polling again but there is a bug in updating the timestamps which make the reduce task poll in a loop without sleeping.
HADOOP-377,Configuration does not handle URL,"Current Configuration allows:

* <String> pointing to a resource in the classpath
* <Path> local path on the file system

The attached patch handles java.net.URL.  We use it to load hadoop-client.xml from a JAR.

Thanks in advance!"
HADOOP-376,Datanode does not scan for an open http port,The DataNode does not scan for an open http port. Only the singleton servers are allowed to have required port assignments.
HADOOP-375,Introduce a way for datanodes to register their HTTP info ports with the NameNode,"If we have multiple datanodes within a single machine the Jetty servers (other than the first one) won't be able to bind to the fixed HTTP port. So, one solution is to have the datanodes pick a free port (starting from a configured port value) and then inform namenode about it so that the namenode can then do redirects, etc.

Johan Oskarson reported this problem. 

If a computer have a second dfs data dir in the config it doesn't start properly because of:

Exception in thread ""main"" java.io.IOException: Problem starting http server
        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:182)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:170)
        at org.apache.hadoop.dfs.DataNode.makeInstanceForDir(DataNode.java:1045)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:999)
        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:1015)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1066)
Caused by: org.mortbay.util.MultiException[java.net.BindException: Address already in use]
        at org.mortbay.http.HttpServer.doStart(HttpServer.java:731)
        at org.mortbay.util.Container.start(Container.java:72)
        at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:159)
        ... 5 more
"
HADOOP-374,native support for gzipped text files,"in many cases it is convenient to store text files in dfs as gzip compressed files.
It would be good to have built in support for processing these files in a mapreduce job.

The getSplits implementation should return a single split per input file, ignoring the numSplits parameter.
One can probably subclass InputFormatBase, and the getSplits method can simply call listPaths() 
and then construct and return a single split per path returned.

The code for reading would look something like (courtesy of Vijay Murthy):

   public RecordReader getRecordReader(FileSystem fs, FileSplit split,
                                       JobConf job, Reporter reporter)
     throws IOException {
     final BufferedReader in =
       new BufferedReader(new InputStreamReader
         (new GZIPInputStream(fs.open(split.getPath()))));
     return new RecordReader() {
         long position;
         public synchronized boolean next(Writable key, Writable value)
           throws IOException {
           String line = in.readLine();
           if (line != null) {
             position += line.length();
             ((UTF8)value).set(line);
             return true;
           }
           return false;
         }
         public synchronized long getPos() throws IOException {
           return position;
         }
        public synchronized void close() throws IOException {
           in.close();
         }
       };
   }

"
HADOOP-373,Some calls to mkdirs do not check return value,"Some calls to mkdirs do not check if mkdirs failed.  

Arun noticed this while looking at HADOOP-281.  Since this issue is actually a separate issue from the mkdirs implementation issue of HADOOP-281, I created a new bug for it.   "
HADOOP-372,should allow to specify different inputformat classes for different input dirs for Map/Reduce jobs,"Right now, the user can specify multiple input directories for a map reduce job. 
However, the files under all the directories are assumed to be in the same format, 
with the same key/value classes. This proves to be  a serious limit in many situations. 
Here is an example. Suppose I have three simple tables: 
one has URLs and their rank values (page ranks), 
another has URLs and their classification values, 
and the third one has the URL meta data such as crawl status, last crawl time, etc. 
Suppose now I need a job to generate a list of URLs to be crawled next. 
The decision depends on the info in all the three tables.
Right now, there is no easy way to accomplish this.

However, this job can be done if the framework allows to specify different inputformats for different input dirs.
Suppose my three tables are in the following directory respectively: rankTable, classificationTable. and metaDataTable. 
If we extend JobConf class with the following method (as Owen suggested to me):
    addInputPath(aPath, anInputFormatClass, anInputKeyClass, anInputValueClass)
Then I can specify my job as follows:
    addInputPath(rankTable, SequenceFileInputFormat.class, UTF8.class, DoubleWritable.class)
    addInputPath(classificationTable, TextInputFormat.class, UTF8,class, UTF8.class)
    addInputPath(metaDataTable, SequenceFileInputFormat.class, UTF8.class, MyRecord.class)
If an input directory is added through the current API, it will have the same meaning as it is now. 
Thus this extension will not affect any applications that do not need this new feature.

It is relatively easy for the M/R framework to create an appropriate record reader for a map task based on the above information.
And that is the only change needed for supporting this extension.





"
HADOOP-371,ant tar should package contrib jars,"From Hadoop-356:
>I note that the contrib packages are not included in distributions (the ""tar"" target).  
>They probably should be.  Michel, would you like to modify ""tar"" to include the contrib code?  
>This should be done in a separate bug.

OK.
This packaging is done in target deploy-contrib.
So I can just add a dependency to the top-level tar target:
<!-- Make release tarball(s)                                               -->
<target name=""tar"" depends=""package, deploy-contrib"">


"
HADOOP-369,Added ability to copy all part-files into one output file,"Since we use the hadoop output in non-hadoop applications it's nice to be able to merge the part-files into one output file on the local filesystem.
So I've added a dfsshell feature that streams from all files in a directory to one output file."
HADOOP-368,"DistributedFSCheck should cleanup, seek, and report missing files.",
HADOOP-367,Static blocks do not automatically run when a class is loaded in Java 5.0,"There seems to be a change that happened between 1.4 and 1.5 with respect to static initializers. I can't find this documented, but I can reproduce with a very simple program. Basically, a static initializer is not called unless a static member/method of the class is accessed or an instance is created. This is actually what the JLS says, but until 1.5 the static initializers ran when the class was loaded. Note that this behavior only occurs when running with the 1.5 JRE AND compiling for 1.5.

For many Writables this isn't an issue, so the fallback behavior of the WritableFactory works, but Block is package private, so loadEdits fails when called from org.apache.hadoop.io.ArrayWritable.readFields() yielding the following trace:

Caused by: java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)
        at org.apache.hadoop.dfs.FSDirectory.loadFSEdits(FSDirectory.java:532)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:470)
        at org.apache.hadoop.dfs.FSDirectory.<init>(FSDirectory.java:307)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:177)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:91)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:84)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:491)


"
HADOOP-366,Should be able to specify more than one jar into a JobConf file,"A job should be able to specify more than one jar file into its JobConf file because sometimes custom Map and Reduce classes or just InputFormat classes uses objects coming from other jar files. For now, we have to build a unique jar to make Hadoop mapreduce operations works."
HADOOP-365,datanode crashes on startup with ClassCastException,"When I bring up a DataNode it gets a ClassCastException:

Exception in thread ""main"" java.lang.ClassCastException: java.util.HashMap$Entry
        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:907)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:954)

This was introduced by the patch for HADOOP-354."
HADOOP-364,rpc versioning broke out-of-order server launches,The change to check the RPCs broke the ability to bring up the datanodes before the namenode and the tasktracker before the jobtracker.
HADOOP-362,tasks can get lost when reporting task completion to the JobTracker has an error,"Basically, the JobTracker used to lose some updates about successful map tasks and it would assume that the tasks are still running (the old progress report is what it used to display in the web page). Now this would cause the reduces to also wait for the map output and they would never receive the output. This would cause the job to appear as if it was hung.
 
The following piece of code sends the status of tasks to the JobTracker:
 
            synchronized (this) {
                for (Iterator it = runningTasks.values().iterator();
                     it.hasNext(); ) {
                    TaskInProgress tip = (TaskInProgress) it.next();
                    TaskStatus status = tip.createStatus();
                    taskReports.add(status);
                    if (status.getRunState() != TaskStatus.RUNNING) {
                        if (tip.getTask().isMapTask()) {
                            mapTotal--;
                        } else {
                            reduceTotal--;
                        }
                        it.remove();
                    }
                }
            }
 
            //
            // Xmit the heartbeat
            //
           
            TaskTrackerStatus status =
              new TaskTrackerStatus(taskTrackerName, localHostname,
                                    httpPort, taskReports,
                                    failures);
            int resultCode = jobClient.emitHeartbeat(status, justStarted);
 
 
Notice that the completed TIPs are removed from runningTasks data structure. Now, if the emitHeartBeat threw an exception (if it could not communicate with the JobTracker till the IPC timeout expires) then this update is lost. And the next time it sends the hearbeat this completed task's status is missing and hence the JobTracker doesn't know about this completed task. So, one solution to this is to remove the completed TIPs from runningTasks after emitHeartbeat returns. Here is how the new code would look like:
 
 
            synchronized (this) {
                for (Iterator it = runningTasks.values().iterator();
                     it.hasNext(); ) {
                    TaskInProgress tip = (TaskInProgress) it.next();
                    TaskStatus status = tip.createStatus();
                    taskReports.add(status);
                }
            }
 
            //
            // Xmit the heartbeat
            //
 
            TaskTrackerStatus status =
              new TaskTrackerStatus(taskTrackerName, localHostname,
                                    httpPort, taskReports,
                                    failures);
            int resultCode = jobClient.emitHeartbeat(status, justStarted);
            synchronized (this) {
                for (Iterator it = runningTasks.values().iterator();
                     it.hasNext(); ) {
                    TaskInProgress tip = (TaskInProgress) it.next();
                    if (tip.runstate != TaskStatus.RUNNING) {
                        if (tip.getTask().isMapTask()) {
                            mapTotal--;
                        } else {
                            reduceTotal--;
                        }
                        it.remove();
                    }
                }
            }
 "
HADOOP-361,junit with pure-Java hadoopStreaming combiner; remove CRLF in some files,"junit with pure-Java hadoopStreaming combiner
remove CRLF in some files

When this is committed, the hadoopStreaming tests should be pure-Java.
This implies that hadoopStreaming can safely be built nightly without plaftorm dependencies.

After I check that this is the case I will update the build.xml in this issue:
HADOOP-356  	 Build and test hadoopStreaming nightly

"
HADOOP-360,hadoop-daemon starts but does not stop servers under cygWin,"Latest changes HADOOP-352 to the hadoop scripts made them incompatible with cygwin.
The servers start fine, but when  I try to stop them the script reports there is nothing to stop. 
Which in turn messes up consequent server starts."
HADOOP-359,add optional compression of map outputs,"Currently, there is no way to request that the transient data be compressed."
HADOOP-358,NPE in Path.equals,"An NPE is raised in Path.equals when testing the method with two unequal pathes and with the first one having no drive.

This is due to operator precedence: && has a higher priority level than ?:

See http://java.sun.com/docs/books/tutorial/java/nutsandbolts/expressions.html

See attached patch (just added some parenthesis and a testcase)."
HADOOP-357,hadoop doesn't handle 0 reduces,"We have cases where we want to use maps for submitting parallel jobs and don't need any reduces. Currently, you are required to have a single reducer. It would be nicer if the framework would let you specify that you don't need any reducers."
HADOOP-356,Build and test hadoopStreaming nightly,"hadoopStreaming was originally built and tested separately.

It makes sense to keep building the hadoopStreaming code as a separate jar file:
this is all client code that can be uploaded in the Job jar.
this allows quick turnaround on production clusters 
(hadoopStreaming code updates do not require to bring down the MapReduce system.
If necessary users can use their own modified versions of the hadoopStreaming jar but still run on the production cluster.)

On the other hand it makes sense to build this code nightly.
Many recent changes broke either compilation or correctness.
All the problems would have been caught if the hadoopStreaming compilation and tests were run nighlty.

Conclusion:
the updated top-level build.xml adds the following dependencies:

1. Target compile calls target compile in contrib/streaming/build.xml
2. Target test calls target test in contrib/streaming/build.xml








"
HADOOP-355,"hadoopStreaming: fix APIs, -reduce NONE, StreamSequenceRecordReader","This patch fixes outstanding problems with streaming:
fix APIs
option -reduce NONE
support StreamSequenceRecordReader as an input
"
HADOOP-354,All daemons should have public methods to start and stop them,"To get tomcat working shutdown working properly I needed to make the NameNode stop() method public as well as the DataNode shutdown() method public. (will attach patch file).

Furthermore I noticed that the RPC object has a static client reference that you have no way of stopping, so I added one. (will attach patch)"
HADOOP-352,Portability of hadoop shell scripts for deployment,"Hadoop shell scripts are based on /bin/bash, which is a ""standard"" shell only on GNU/Linux.  On other Unix systems like FreeBSD however the ""standard"" shell is /bin/sh.  The attached patch addresses these compatiblity issues.  Note that Solaris support is not yet tested.

Also, the best way to set the HADOOP_HOME variable upon ssh connection in a portable way is to set it in .ssh/environment.  The bash startup script "".bashrc"" is not an option on systems where ""bash"" is not installed."
HADOOP-351,Remove Jetty dependency,"Somewhat related to HADOOP-349, it would be nice to not have a Jetty dependency for those of us embedding Hadoop within our own web applications. In particular the Server object is using SocketChannelOutputStream from the Jetty code base. It seems to me that for an object like this it would be better to simply have a Hadoop version of a blocking output stream on an nio channel if necessary vs. using the Jetty version requiring a Hadoop user's to package yet another jar file (and all the complications that are associated with that)."
HADOOP-350,"In standalone mode, 'org.apache.commons.cli cannot be resolved'","Standalone works fine in 0.4.0 but if I use TRUNK, 'Last Changed Date: 2006-06-28 14:27:33 -0700 (Wed, 28 Jun 2006)', and pass it my fat job jar, I get following exception.

06/07/06 13:18:02 INFO conf.Configuration: parsing file:/tmp/hadoop-unjar29923/nutch-site.xml
06/07/06 13:18:02 INFO conf.Configuration: parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-site.xml
Exception in thread ""main"" java.lang.Error: Unresolved compilation problems:   
        The import org.apache.commons.cli cannot be resolved
        The import org.apache.commons.cli cannot be resolved
        The import org.apache.commons.cli cannot be resolved
        The import org.apache.commons.cli cannot be resolved       
        The import org.apache.commons.cli cannot be resolved
        The import org.apache.commons.cli cannot be resolved
        The import org.apache.commons.cli cannot be resolved
        The import org.apache.commons.cli cannot be resolved
        The import org.apache.commons.cli cannot be resolved
        Options cannot be resolved to a type
        Option cannot be resolved to a type
        OptionBuilder cannot be resolved
        Option cannot be resolved to a type
        OptionBuilder cannot be resolved
        Option cannot be resolved to a type
        OptionBuilder cannot be resolved
        Option cannot be resolved to a type
        OptionBuilder cannot be resolved
        Options cannot be resolved to a type
        Options cannot be resolved to a type
        CommandLine cannot be resolved to a type
        Options cannot be resolved to a type
        The method buildGeneralOptions() is undefined for the type ToolBase
        CommandLineParser cannot be resolved to a type
        GnuParser cannot be resolved to a type
        CommandLine cannot be resolved to a type
        ParseException cannot be resolved to a type
        e cannot be resolved
        HelpFormatter cannot be resolved to a type
        HelpFormatter cannot be resolved to a type

        at org.apache.hadoop.util.ToolBase.<init>(ToolBase.java:21)
        at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:178)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:320)
        at org.archive.access.nutch.ImportArcs.importArcs(ImportArcs.java:576)
        at org.archive.access.nutch.Nutchwax.doImport(Nutchwax.java:159)
        at org.archive.access.nutch.Nutchwax.doAll(Nutchwax.java:144)
        at org.archive.access.nutch.Nutchwax.doJob(Nutchwax.java:368)
        at org.archive.access.nutch.Nutchwax.main(Nutchwax.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:128)

I took a bit of a look.  Odd is that command-cli.jar is in hadoop/lib dir so should be found no problem (and we seem to be loading other stuff out of the uncompressed job jar fine -- see mention of nutch-site.xml above).   I tried adding the command-cli to my job jar.  I printed out all that RunJar -- see below -- was adding to its CLASSPATH and saw mention of command-cli but still no workee.

file:/tmp/hadoop-unjar29923/lib/archive-commons-1.8.0.jar
file:/tmp/hadoop-unjar29923/lib/commons-codec-1.3.jar
file:/tmp/hadoop-unjar29923/lib/commons-httpclient-3.0-rc3.jar
file:/tmp/hadoop-unjar29923/lib/commons-logging-1.0.4.jar
file:/tmp/hadoop-unjar29923/lib/dsi.unimi.it-1.2.0.jar
file:/tmp/hadoop-unjar29923/lib/commons-lang-2.1.jar
file:/tmp/hadoop-unjar29923/lib/commons-cli-2.0-SNAPSHOT.jar
file:/tmp/hadoop-unjar29923/lib/lucene-core-1.9.1.jar
file:/tmp/hadoop-unjar29923/lib/lucene-misc-1.9.1.jar
file:/tmp/hadoop-unjar29923/lib/jakarta-oro-2.0.7.jar
file:/tmp/hadoop-unjar29923/lib/xerces-2_6_2-apis.jar
file:/tmp/hadoop-unjar29923/lib/xerces-2_6_2.jar
file:/tmp/hadoop-unjar29923/lib/concurrent-1.3.4.jar
06/07/06 13:18:01 INFO conf.Configuration: parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-default.xml 

Any ideas?

Thanks."
HADOOP-349,Allow info server to be turned off/on by conf file,"Since I am using hadoop within my own servlet which is not Jetty, it would be nice to not have to need Jetty to run my servlet. As such I propose adding an if statement/donf setting at FSNamesystem:171 as such         

if(conf.getBoolean(""dfs.info.active"",true)){
            this.infoPort = conf.getInt(""dfs.info.port"", 50070);
            this.infoServer = new StatusHttpServer(""dfs"", infoPort, false);
            this.infoServer.start();
}"
HADOOP-348,fs.default.name default not working,"in NameNode:84 and FSNamesystem:176 there is a line:

InetSocketAddress addr = DataNode.createSocketAddr(conf.get(""fs.default.name"", ""local""));

This won't ever work as createSocketAddr first checks to make sure there is a : in the string to get the server name and port. It is not a super simple fix of just putting in a :50000 (or something like that) because the NameNode constructor allows you to pass in a port that would need to match the port the in the conf setting. I think the method public NameNode(File dir, int port, Configuration conf) should simply be deprecated."
HADOOP-347,Implement HDFS content browsing interface,"Implement HDFS content browsing interface over HTTP. Clients would connect to the NameNode and this would send a redirect to a random DataNode. The DataNode, via dfs client, would proxy to namenode for metadata browsing and to other datanodes for content. One can also view the local blocks on any DataNode. Head, Tail will be provided as shorthands for viewing the first block and the last block of a file. 
For full file viewing, the data displayed per HTTP request will be a block with a PREV/NEXT link. The block size for viewing can be a configurable parameter (the user sets it via the web browser) to the HTTP server (e.g., 256 KB can be the default block size for viewing files)."
HADOOP-345,JobConf access to name-values,"class JobConf (or its base class Configuration) 
should be extended to enable enumeration of all its key-value pairs.
( more precisely: the Properties returned by Configuration.getProps() )

This will be useful to ""export"" all JobConf properties to environment variables.
We use env.vars to expose some Hadoop context to non-Java MapReduce applications.

Note that the typed properties are also represented as Strings 
(getInt, getStrings, getClass, etc.)
So a single enumeration exposes everything as (untyped) environment variables.

The proposed escaping rules from JobConf properties to env.var are:

1. values are left as-is.
2. keys are escaped as follows:
[A-Za-z0-9] --> unchanged.
all other chars --> underscore.

For example
set(""mapred.input.key.class"", ""com.example.MyKey"")
becomes env.var:
export mapred_input_key_class=com.example.MyKey

Justification:
1. Environment variables are case-sensitive. (Although uppercase is the preferred convention)
  So no need to uppercase everything.
2. Some characters are forbidden in env.vars, or at least not shell-friendly:
For example period, colon are problematic.
3. The Hadoop conventions are already hierarchical and provide some namespace protection.
   This means we don't need an additional prefix as protection.
   For example all exported environment variables will start with ""mapred."" , ""dfs."" , ""ipc."" etc.
  This means they will not conflict with standard environemnt variables like PATH, USER, etc.
  And they will not conflict with standard hadoop env.vars because those are upper-case. (like HADOOP_CONF_DIR)








"
HADOOP-344,TaskTracker passes incorrect file path to DF under cygwin,"The path that is passed is OS dependent. The File abstraction should be used in order to make it universal.
I'm attaching a patch that does that.
We might want to change the path parameter for DF. Making it File rather than String should prevent us from 
bugs like that in the future."
HADOOP-343,"In case of dead task tracker, the copy mapouts try copying all mapoutputs from this tasktracker","In case of a dead task tracker, the reduces which do not have the updated map out locations try copygin files from this node and since there are failures on copying, this leads to backoff and slowing down of the copy pahse."
HADOOP-342,Design/Implement a tool to support archival and analysis of logfiles.,"Requirements:

  a) Create a tool support archival of logfiles (from diverse sources) in hadoop's dfs.
  b) The tool should also support analysis of the logfiles via grep/sort primitives. The tool should allow for fairly generic pattern 'grep's and let users 'sort' the matching lines (from grep) on 'columns' of their choice.

  E.g. from hadoop logs: Look for all log-lines with 'FATAL' and sort them based on timestamps (column x)  and then on column y (column x, followed by column y).


Design/Implementation:

  a) Log Archival

    Archival of logs from diverse sources can be accomplished using the *distcp* tool (HADOOP-341).
  
  b) Log analysis

    The idea is to enable users of the tool to perform analysis of logs via grep/sort primitives.

    This can be accomplished via a relatively simple Map-Reduce task where the map does the *grep* for the given pattern via RegexMapper and then the implicit *sort* (reducer) is used with a custom Comparator which performs the user-specified comparision (columns). 

    The sort/grep specs can be fairly powerful by letting the user of the tool use java's in-built regex patterns (java.util.regex).
"
HADOOP-341,Enhance distcp to handle *http* as a 'source protocol'.,"Requirements:

  Presently distcp recursively copies a directory from one dfs to another i.e. both source and destination of of the *dfs* protocol.
  Enhance it to handle *http* as the source protocol i.e. support copying files from arbitrary http-based sources into the dfs.

Design:
  
  Follow distcp's current design: one map task per file which needs to be copied.

  Caveat: distcp handles *recursive* copying by listing sub-directories; this is not as feasible with a http-based source since things like 'fancy-indexing' might not be enabled on the web-server (for all sub-locations recursively too), and even if it is enabled it will mean tedious parsing of the html served to glean the sub-directories etc. Hence the idea is to support an input file (via a -f option) which contains a list of the http-based urls which represent multiple source files.
"
HADOOP-340,Using wildcards in config pathnames,"In our cluster there's machines with very different disk setups
I've solved this by not rsyncing hadoop-site.xml, but as you probably understand this means new settings will not get copied properly.

I'd like to be able to use wildcards in the dfs.data.dir path for example:
<property>
  <name>dfs.data.dir</name>
    <value>/home/hadoop/disk*/dfs/data</value>
</property>

then every disk mounted in that directory would be used"
HADOOP-339,making improvements to the jobclients to get information on currenlyl running jobs and the jobqueue,adding a few methods to the joblcient and jobsubmission protocol to support queries to the jobtracker for currently running jobs
HADOOP-337,DFS files should be appendable,"
Actually two related issues

1. One should be able to open an existing DFS file, to seek to a position and truncate the rest, and to append starting at the end (or where trancation happens) .
2. One should be able to read the writen data of a DFS file while other is writing/appending to the file
"
HADOOP-336,"The task tracker should track disk space used, and have a configurable cap","We've been having problems where the task tracker is destabilizing HDFS by filling disks and then releasing them.  Various tasks can also interfere with each other this way.

We should have a configurable max working space for a task (and allow the setting of lower per task limits).  The task tracker should track use against this limit and terminate a job if it overruns it."
HADOOP-335,factor out the namespace image/transaction log writing,Factor the checkpoint/transaction log handling code out of FSNameSystem into its own class.
HADOOP-334,Redesign the dfs namespace datastructures to be copy on write,The namespace datastructures should be copy on write so that the namespace does not need to be completely locked down from user changes while the checkpoint is being made.
HADOOP-333,we should have some checks that the sort benchmark generates correct outputs,"We should implement some checks of the input versus output of the sort benchmark to get some correctness guarantees:

1. the number of records
2. the number of bytes
3. the output records are in fact sorted
4. the xor of the md5 of each record's key/value pair"
HADOOP-332,Implement remote replication of dfs namespace images and transaction logs,"The namespace information (image and transaction logs) needs to be replicated on hosts other than the namenode to prevent data loss if a namenode crashes. In the short term we will add a new protocol to the datanodes to receive and store the namespace information.

In the long term, it would be nice to have read-only namenodes that could receive the information and serve it up for users that want to read data, but do not need to write to the namespace."
HADOOP-331,map outputs should be written to a single output file with an index,"The current strategy of writing a file per target map is consuming a lot of unused buffer space (causing out of memory crashes) and puts a lot of burden on the FS (many opens, inodes used, etc).  

I propose that we write a single file containing all output and also write an index file IDing which byte range in the file goes to each reduce.  This will remove the issue of buffer waste, address scaling issues with number of open files and generally set us up better for scaling.  It will also have advantages with very small inputs, since the buffer cache will reduce the number of seeks needed and the data serving node can open a single file and just keep it open rather than needing to do directory and open ops on every request.

The only issue I see is that in cases where the task output is substantiallyu larger than its input, we may need to spill multiple times.  In this case, we can do a merge after all spills are complete (or during the final spill).
"
HADOOP-330,Raw SequenceFile Input/Output formats,We'd like a raw input/output format for Map/Reduce that allows jobs to get the raw bytes for keys/values. This would allow things like the IdentityMap to be much faster because the values would not be decompressed/compressed or serialized/deserialized.
HADOOP-329,ClassCastException in DFSClient,"I'm getting the following message back to my launching application:

Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.ClassCastException: org.apache.hadoop.dfs.DatanodeInfo cannot be cast to java.lang.Comparable
        at java.util.TreeMap.getEntry(TreeMap.java:325)
        at java.util.TreeMap.containsKey(TreeMap.java:209)
        at java.util.TreeSet.contains(TreeSet.java:217)
        at org.apache.hadoop.dfs.DFSClient.bestNode(DFSClient.java:373)
        at org.apache.hadoop.dfs.DFSClient.access$100(DFSClient.java:42)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:520)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:638)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:167)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)
        at java.io.DataInputStream.readFully(DataInputStream.java:174)
        at java.io.DataInputStream.readFully(DataInputStream.java:150)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:55)
        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:237)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:72)
        at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:182)
        at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:83)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:935)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:589)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:469)

        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:159)
"
HADOOP-328,add a -i option to distcp to ignore read errors of the input files,"Add an option ""-i"" to ignore problems reading files and just copy what can be copied."
HADOOP-327,ToolBase calls System.exit,The new ToolBase class calls System.exit when the main routine finishes. That will break if the application uses threads that need to finish before the jvm exits. The normal semantics is that the program doesn't finish execution until all of the non-daemon threads exit (including the main one) and System.exit should never be called except for critical errors.
HADOOP-326,cleanup of dead field (map ouput port),"The TaskTrackerStatus retains a field that records the port of the map outuput server, even though that server no longer exists in the current code base."
HADOOP-325,ClassNotFoundException under jvm 1.6,"We have been having problems with classes that are returned by RPC methods not being loaded/initialized correctly. The work around has been to put in the servers, code of the form:

static { new FooBar(); }  // to resolve the ClassNotFoundException for class FooBar.

When I tried running under java 1.6, that stopped working because one of the classes had to be instantiated from a package that didn't have visibility to create an instance. So I tracked the problem down to how the classes were being loaded via reflection."
HADOOP-324,"""IOException: No space left on device"" is handled incorrectly","When a data node disk is almost full the name node still assigns blocks to the data node.
By the time the data node actually tries to write that data to disk the disk may become full.
Current implementation forces the data node to shutdown after that.
The expected behavior is to report the block write failure and continue.

The Exception looks as follows:

java.io.IOException: No space left on device
at java.io.FileOutputStream.writeBytes(Native Method)
at java.io.FileOutputStream.write(FileOutputStream.java:260)
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
at java.io.DataOutputStream.write(DataOutputStream.java:90)
at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:623)
at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:410)
at java.lang.Thread.run(Thread.java:595)
2006-06-26 08:26:04,751 INFO org.apache.hadoop.dfs.DataNode: Finishing DataNode in: /tmp/hadoop/dfs/data/data

"
HADOOP-322,Need a job control utility to submit and monitor a group of jobs which have DAG dependency,"
In my applications, some jobs depend on the outputs of other jobs. Therefore, job dependency forms a DAG. A job is ready to run if and only if it does not have any dependency or all the jobs it depends are finished successfully. To help schedule and monitor a group of jobs like that, I am thinking of implementing a utility that:
	- accept jobs with dependency specification
      - monitor job status
      - submit jobs when they are ready

With such a utility, the application can construct its jobs, specify their dependency and then hand the jobs to the utility class. The utility takes care of the details of job submission.

I'll post my design skech for comments/suggestion.
Eventually, I'll submit a patch for the utility.



"
HADOOP-321,DatanodeInfo refactoring,"I'm trying to refactor some name node classes, which seem to be similar.
So DatanodeInfo is a public api now for purely external ( to name node) use.
The name node class that stores information about data nodes including the
set of its blocks is called DatanodeDescriptor.
The DatanodeReport is removed since it was a variation of DatanodeInfo.
Previously DatanodeInfo and DatanodeDescriptor were the same class, and
DatanodeReport was used for reporting node statistics only.
This is a preparation step for HADOOP-306.
"
HADOOP-320,bin/hadoop dfs -mv does not mv  source's checksum file if source is a file,None of the rename operation of DistributedSystem or DFSClient checks if the source is a file or not. Need also to rename the checksum file accordingly if the source is a file.
HADOOP-319,"FileSystem ""close"" does not remove the closed fs from the fs map","The close methods of both DistributedFileSystem and LocalFileSystem do not remove the closed file system from the fs map in FileSystem. As a result, a subsequent call to FileSystem.getNamed may return the handle to the closed file system and receive errors if perform any operation on the fs handle."
HADOOP-318,Progress in writing a DFS file does not count towards Job progress and can make the task timeout,"When a task writes to DFS file, depending on how busy the cluster is, it can timeout after 10 minutes by default, because the progress towards writing a DFS file does not count as progress of the task. The solution (patch is forthcoming) is to provide a way to callback reporter to report task progress from DFSOutputStream."
HADOOP-317,"""connection was forcibly closed"" Exception in RPC on Windows","I see a lot of exceptions caused by RPC in the nightly build on Windows.
The most often is thrown by RPC on the namenode, saying

06/06/21 19:28:36 INFO ipc.Server: Server listener on port 7017: readAndProcess threw exception java.io.IOException: An existing connection was forcibly closed by the remote host. Count of bytes read: 0
java.io.IOException: An existing connection was forcibly closed by the remote host
    at sun.nio.ch.SocketDispatcher.read0(Native Method)
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25)
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)
    at sun.nio.ch.IOUtil.read(IOUtil.java:200)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:207)
    at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:374)
    at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:289)
    at org.apache.hadoop.ipc.Server$Listener.run(Server.java:210)

I am not sure how serious that is, since the tests do not fail, and the name node does not crash.
Besides the RPC we should probably also check that the unit tests actually fail if they need to.
"
HADOOP-316,job tracker has a deadlock,"The JobTracker has a deadlock in the ExpireLaunchingTasks stuff.

In particular, it locks the JobTracker and launchingTasks inconsistently.

This deadlocks has been observed in the wild and causes the JobTracker to stop responding to rpc and http (other than the root page)."
HADOOP-315,bobo Exception in TestRPC,"We are getting a bobo exceptions in TestRPC.
It looks like the test should fail in this case after throwing bobo.
By the way, does anybody know whether ""bobo"" means anything?

error: java.io.IOException: bobo
java.io.IOException: bobo
    at org.apache.hadoop.ipc.TestRPC$TestImpl.error(TestRPC.java:84)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:585)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:467)"
HADOOP-314,remove the append phase in sorting the reduce inputs,This patch creates a new interface to SequenceFile.sort that allows both a list of files to be passed as input and specifying whether the inputs should be deleted. The ReduceTask is changed to pass the list of map inputs and to delete them as they are sorted.
HADOOP-313,A stand alone driver for individual tasks,"This is a tool to reproduce problems and to run unit tests involving either a map or reduce task.
You just give it a reduce directory on the command line.

Usage: java org.apache.hadoop.mapred.StandaloneReduceTask <taskdir> [<limitmaps>]
taskdir name encodes: task_<jobid>_r_<partition>_<attempt>
taskdir contains job.xml and one or more input files named: map_<dddd>.out
You should run with the same -Xmx option as the TaskTracker child JVM

"
HADOOP-312,Connections should not be cached,"Servers and clients (client include datanodes, tasktrackers, DFSClients & tasks) should not cache connections or maybe cache them for very short periods of time. Clients should set up & tear down connections to the servers everytime they need to contact the servers (including the heartbeats). If connection is cached, then reuse the existing connection for a few subsequent transactions until the connection expires. The heartbeat interval should be more so that many more clients (order of  tens of thousands) can be accomodated within 1 heartbeat interval."
HADOOP-311,dfs client timeout on read kills task,"If a DFS client reads a file and times out, it immediately throws an exception to the client, which will kill the task."
HADOOP-309,NullPointerException in StatusHttpServer,"The NullPointerException happens in the constructor of StatusHttpServer because the System property ""hadoop.log.dir"" is undefined.
I see it trying to start the Namenode directly without using any scripts.

Exception in thread ""main"" java.lang.NullPointerException
	at org.mortbay.util.Resource.newResource(Resource.java:99)
	at org.mortbay.http.ResourceCache.setResourceBase(ResourceCache.java:140)
	at org.mortbay.http.HttpContext.setResourceBase(HttpContext.java:2207)
	at org.apache.hadoop.mapred.StatusHttpServer.<init>(StatusHttpServer.java:66)
	at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:172)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:97)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:88)
	at org.apache.hadoop.dfs.NameNode.main(NameNode.java:496)

In general I think the sources should not rely on the system properties and environment variables defined in hadoop scripts.
"
HADOOP-308,Task Tracker does not handle the case of read only local dir  case correctly,"In case that the local dir is not writable on a node, the tasks on the  node will fail as expected, with an exception like:

(Read-only file system) at java.io.FileOutputStream.open(Native Method) 
at java.io.FileOutputStream.(FileOutputStream.java:179) 
at java.io.FileOutputStream.(FileOutputStream.java:131) 
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.(DFSClient.java:723) 
at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:241) 
at org.apache.hadoop.dfs.DistributedFileSystem.createRaw(DistributedFileSystem.java:96) 
at org.apache.hadoop.fs.FSDataOutputStream$Summer.(FSDataOutputStream.java:44) 
at org.apache.hadoop.fs.FSDataOutputStream.(FSDataOutputStream.java:134) 
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:224) 
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:176) 
....
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:265) 
at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:847)

However, the task tracker will continue accept new tasks and continue to fail.
The runloop of tasktracker should detect such a problem and exits.

"
HADOOP-307,Many small jobs benchmark for MapReduce,"A benchmark that runs many small MapReduce tasks in sequence. A single map reduce implementation is used, it is invoked multiple times with input as the output from previous run. The input to first Map is a TextInputFormat ( a text file with few hundred KBs). Input records are passed to output without much processing. The idea is to benchmark the time taken by initialization of Mapper and Reducer. An initial prototyping on a single machine with 20 MR tasks in sequence took ~47 seconds per task. Looking for suggestions on what else can be included in the benchmark. 

"
HADOOP-306,Safe mode and name node startup procedures,"This is a proposal to improve DFS cluster startup process.
The data node startup procedures were described and implemented in HADOOP-124.
I'm trying to extend them to the name node here.
The main idea is to introduce safe mode, which can be entered manually for administration
purposes, or automatically when a configurable threshold of active data nodes is breached,
or at startup when the node stays in safe mode until the minimal limit of active
nodes is reached.

This are high level requirements intended to improve the name node and cluster reliability.
    = The name node safe mode means that the name node is not changing the state of the
       file system. Meta data is read-only, and block replication / removal is not taking place.
    = In safe mode the name node accepts data node registrations and
       processes their block reports.
    = The name node always starts in safe mode and stays safe until the majority
        (a configurable parameter: safemode.threshold) of data nodes (or blocks?)
        is reported.
    = The name node can also fall into safe mode when the number of non-active
        (heartbeats stopped coming in) data nodes becomes critical.
    = The startup ""silent period"", when the name node is in safe mode and is
        not issuing any block requests to the data nodes, is initially set to a
        configurable value safemode.timeout.increment. By the end of the timeout
        the name node checks the safemode.threshold and decides whether to switch
        to the normal mode or to stay in safe. If the normal mode criteria is not
        met, then the silent period is extended by incrementing the safemode timeout.
    = The name node stays in safe mode not longer than a configurable value of
        safemode.timeout.max, in which case it logs missing data nodes and shuts
        itself down.
    = When the name node switches to normal mode it checks whether all required
        data nodes have actually registered, based on the list of active data storages
        from the last session. Then it logs missing nodes, if any, and starts
        replicating and/or deleting blocks as required.
    = A historical list of data storages (nodes) ever registered with the cluster is
        persistently stored in the image and log files. The list is used in two ways:
        a) at startup to verify whether all nodes have registered, and to report
        missing nodes;
        b) at runtime if a data node registers with a new storage id the
        name node verifies that no new blocks are reported from that storage,
        which would prevent us from accidentally connecting data nodes from a
        different cluster.
    = The name node should have an option to run in safe mode. Starting with
        that option would mean it never leaves safe mode.
        This is useful for testing the cluster.
    = Data nodes that can not connect to the name node for a long time (configurable)
        should shut down themselves."
HADOOP-305,tasktracker waits for 10 seconds for asking for a task.,the tasktracker should ask for a job as soon as a job running on it is finished. 
HADOOP-304,UnregisteredDatanodeException message correction,
HADOOP-302,class Text (replacement for class UTF8) was: HADOOP-136,"Just to verify, which length-encoding scheme are we using for class Text (aka LargeUTF8) 

a) The ""UTF-8/Lucene"" scheme? (highest bit of each byte is an extension bit, which I think is what Doug is describing in his last comment) or 
b) the record-IO scheme in o.a.h.record.Utils.java:readInt 

Either way, note that: 

1. UTF8.java and its successor Text.java need to read the length in two ways: 
  1a. consume 1+ bytes from a DataInput and 
  1b. parse the length within a byte array at a given offset 
(1.b is used for the ""WritableComparator optimized for UTF8 keys"" ). 

o.a.h.record.Utils only supports the DataInput mode. 
It is not clear to me what is the best way to extend this Utils code when you need to support both reading modes 

2 Methods like UTF8's WritableComparator are to be low overhead, in partic. there should be no Object allocation. 
For the byte array case, the varlen-reader utility needs to be extended to return both: 
 the decoded length and the length of the encoded length. 
 (so that the caller can do offset += encodedlength) 
    
3. A String length does not need (small) negative integers. 

4. One advantage of a) is that it is standard (or at least well-known and natural) and there are no magic constants (like -120, -121 -124) "
HADOOP-301,the randomwriter example will clobber the output file,"The randomwriter will automatically clobber the output directory, which is dangerous given the lack of permissions in  dfs."
HADOOP-299,maps from second jobs will not run until the first job finishes completely,"Because of the logic in the JobTracker's pollForNewTask, second jobs will rarely start running maps until the first job finishes completely. The JobTracker leaves room to re-run failed maps from the first job and it reserves the total number of maps for the first job. Thus, if you have more maps in the first job than your cluster capacity, none of the second job maps will ever run.

I propose setting the reserve to 1% of the first job's maps."
HADOOP-298,nicer reports of progress for distcp,The unformatted number of bytes in distcp is difficult to read.
HADOOP-296,Do not assign blocks to a datanode with < x mb free,"We're running a smallish cluster with very different machines, some with only 60 gb harddrives
This creates a problem when inserting files into the dfs, these machines run out of space quickly and then they cannot run any map reduce operations

A solution would be to not assign any new blocks once the space is below a certain user configurable threshold
This free space could then be used by the map reduce operations instead (if that's on the same disk)"
HADOOP-295,jobs don't get executed in parallel,"we expect tasks to be assigned to nodes up to their configured capacity, with separate slots for map and reduce tasks.
we're observing jobs queueing up, with the map slots free, as exemplified by this copy-paste (converted to text) of the JT WI page.

Cluster Summary
Maps Reduces Tasks/Node Nodes 
0           205             2                 156 
--------------------------------------------------------------------------------
Running Jobs  
Jobid        User           Name          Map % complete Map total Maps completed Reduce % complete Reduce total Reduces completed 
job_0088 runping  DocUpdater   100.00%                  3597                3597                  99.92%                       256                        253 
job_0089 murthij   Term Count        0.00%                  243                        0                     0.00%                        200                            0 
job_0091 zshao     sort                       0.00%                  444                        0                     0.00%                        100                           0 


--------------------------------------------------------------------------------
"
HADOOP-294,dfs client error retries aren't happening (already being created and not replicated yet),The conditions for catching the dfs error conditions that need to be retried were broken in a previous patch.
HADOOP-293,map reduce job fail without reporting a reason,"Often I see in the WI reports of tasks failing without information reported as to the reason of the failure.
It makes analysis and fixing the problem much harder.
The reason for the failure should always be reported in the WI."
HADOOP-292,hadoop dfs commands should not output superfluous data to stdout,"running a command such as hadoop dfs -ls /data
produces output such as the following:
06/06/08 17:42:32 INFO conf.Configuration: parsing jar:file: /hadoop/hadoop-0.4-dev/hadoop-0.4-dev.jar!/hadoop-default.xml
06/06/08 17:42:32 INFO conf.Configuration: parsing file:hadoop/hadoop-site.xml
06/06/08 17:42:32 INFO dfs.DistributedFileSystem: No FS indicated, using default:kry1200:8020
06/06/08 17:42:32 INFO ipc.Client: Client connection to 172.30.111.134:8020: starting
Found 2 items
/data/a <dir>
/data/b     <dir>

the first few lines shouldn't be there.
it's especially annoying when running -cat into a file or into some post processing program, but in general, the output should be clean."
HADOOP-291,Hadoop Log Archiver/Analyzer utility,"Overview of the log archiver/analyzer utility...

1. Input
  The tool takes as input a list of directory URLs, each url could also we associated with a file-pattern to specify what pattern of files in that directory are to be used.
  e.g. http://g1015:50030/logs/hadoop-sameer-jobtracker-*
         file:///export/crawlspace/sanjay/hadoop/trunk/run/logs/haddop-sanjay-namenode-* (local disk on the machine on which the job was submitted)

2. The tool supports 2 main functions:

  a) Archival
    Archive the logs in the DFS in the following hierarchy:
   /users/<username>/log-archive/YYYY/mm/dd/HHMMSS.log by default 
   Or a user-specified directory and then: 
   <input-dir>/YYYY/mm/dd/HHMMSS.log

  b) Processing with simple sort/grep primitives
    Archive the logs as above and then grep for lines with given pattern (e.g. INFO) and then sort with spec e.g. <logger><level><date>. (Note: This is proposed with current log4j based logging in mind... do we need anything more generic?). The sort/grep specs are user-provided; along with directory URLs.

3. Thoughts on implementation...

  a) Archival
    Current idea is to put a .jsp page (src/webapps) on each of the nodes; which then does a *copyFromLocal* of the log-file into the DFS. The jobtracker will fire n map-tasks which only hit the jsp page as per the directory URLs. The reduce-task is a no-op and only collects statistics on failures (if any).

  b) Processing with sort/grep
    Here, the tool first archives the files as above and then another set of map-reduce tasks will do the sort/grep on the files in DFS with given specs.


                                                                                          - * - * - 

 Suggestions/corrections welcome...

thanks,
Arun"
HADOOP-290,Fix Datanode transfer thread logging,"Under the current datanode the logging of ""Starting thread to transfer block"" doesn't print out the hosts that it is transferring to, it prints out a java array toString.  This patch fixes this and prints out the hostnames and ports for all hosts we are tranferring to."
HADOOP-289,Datanodes need to catch SocketTimeoutException and UnregisteredDatanodeException,"- Datanode needs to catch SocketTimeoutException when registering otherwise it goes down
the same way as when the namenode is not available (HADOOP-282).
- UnregisteredDatanodeException need to be caught for all non-registering requests. The data
node should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.
"
HADOOP-288,RFC: Efficient file caching,"RFC: Efficient file caching 
(on Hadoop Task nodes, for benefit of MapReduce Tasks)
------------------------------------------------------

We will start implementing this soon. Please provide feedback and improvements to this plan.

The header ""Options:"" indicates places where simple choices must be made.


Problem:
-------
o MapReduce tasks require access to additional out-of-band data (""dictionaries"")

This out-of-band data is:

o in addition to the map/reduce inputs.
o large (1GB+)
o broadcast (same data is required on all the Task nodes)
o changes ""infrequently"", in particular:
oo it is always constant for all the Tasks in a Job. 
oo it is often constant for a month at a time 
oo it may be shared across team members
o sometimes used by pure-Java MapReduce programs
o sometimes used by non-Java MapReduce programs (using Hadoop-Streaming)
o (future) used by programs that use HDFS and Task-trackers but not MapReduce.

Existing Solutions to the problem:
---------------------------------
These solutions are not good enough. The present proposal is to do Sol 1 with caching.

Sol 1: Pure Hadoop: package the out-of-band data in the MapReduce Job jar file.
Sol 2: Non  Hadoop: for each task node run rsync from single source for data.
Sol 3: Non  Hadoop: use BitTorrent, etc.

Sol.1 is correct but slow for many reasons:
 The Job submitter must recreate a large jar(tar) file for every Job.
  (The jar contains both changing programs and stable dictionaries)
 The large Jar file must be propagated from the client to HDFS with 
 a large replication factor. 
 At the beginning of every Task, the Task tracker gets the job jar from HDFS 
 and unjars it in the working directory. This can dominate task execution time.
 
Sol.2 has nice properties but also some problems.
 It does not scale well with large clusters (many concurrent rsync read requests i.e. single-source broadcast)
 It assumes that Hadoop users can upload data using rsync to the cluster nodes. As a policy, this is not allowed.
 It requires rsync.
 
Sol.3 alleviates the rsync scalability problems but 
      It is a dependency on an external system. 
      We want something simpler and more tightly integrated with Hadoop.
      

Staging (uploading) out-of-band data:
------------------------------------
The out-of-band data will often originate on the local filesystem of a user machine 
 (i.e. a MapReduce job submitter)
Nevertheless it makes sense to use HDFS to store the original out-of-band data because:
o HDFS has (wide) replication. This enables scalable broadcast later.
o HDFS is an available channel to move data from clients to all task machines.
o HDFS is convenient as a shared location among Hadoop team members.


Accessing (downloading) out-of-band data:
----------------------------------------
The non-Java MapReduce programs do not have or want[1] APIs for HDFS.
Instead these programs just want to access out-of-band data as 
 local files at predefined paths.
([1] Existing programs should be reusable with no changes. 
 This is often possible bec. communication is over stdin/stdout.)



Job's jar file as a special case:
--------------------------------
One use case is to allow users to make the job jar itself cachable.

This is only useful in cases where NOTHING changes when a job is resubmitted
 (no MapRed code changes and no changes in shipped data)
This situation might occur with an 'extractor' job (gets data from an external source: like Nutch crawler)

Currently the Hadoop mapred-jar mechanism works in this way:
 the job jar data is unjarred in the ""working directory"" of the Task 
 the jar contains both MapRed java code (added to classpath)



Cache synchronization:
---------------------

The efficient implementation of the out-of-band data distribution
is mostly a cache synchronization problem.
A list of the various aspects where choices must be made follows.


Cache key:
---------
How do you test that the cached copy is out-of-date?

Options: 
1. the archive/file timestamp 
2. the MD5 of the archive/file content

Comparing source and destination Timestamps is problematic bec. it assumes synchronized clocks.
Also there is no last-modif metadata in HDFS (for good reasons, like scalability of metadata ops)

Timestamps stored with the source ('last-propagate-time') do 
 not require synchronized clocks, only locally monotonic time. 
(and the worse which can happen at daylight-savings switch is a missed update or an extra-update)

The cache code could store a copy of the local timestamp 
in the same way that it caches the value of the content hash along with the source data.
 


Cachable unit:
-------------
Options: individual files or archives or both.

Note:
At the API level, directories will be processed recursively 
(and the local FS directories will parallel HDFS directories)
So bulk operations are always possible using directories.
The question here is whether to handle archives as an additional bulk mechanism.


Archives are special because:
o unarchiving occurs transparently as part of the cache sync
o The cache key is computed on the archive and preserved although 
  the archive itself is not preserved.
Supported archive format will be: tar (maybe tgz or compressed jar)
Archive detection test: by filename extension "".tar"" or "".jar""

Suppose we don't handle archives as special files:
Pros:
 o less code, no discussion about which archive formats are supported
 o fine for large dictionary files. And when files are not large, user may as well
   put them in the Job jar as usual.
 o user code could always check and unarchive specific cached files
   (as a side-effect of MapRed task initialization)
Cons:
 o handling small files may be inefficient 
  (multiple HDFS operations, multiple hash computation, 
   one 'metadata' hash file along with each small file)
 o It will not be possible to handle the Job's jar file as a special case of caching 



Cache isolation: 
---------------
In some cases it may be a problem if the cached HDFS files are updated while a Job is in progress:
The file may become unavailable for a short period of time and some tasks fail.
The file may change (atomically) and different tasks use a different version.

This isolation problem is not addressed in this proposal.
Standard solutions to the isolation problem are:

o Assume that Jobs and interfering cache updates won't occur concurrently.

o Put a version number in the HDFS file paths and refer to a hard-coded version in the Job code.

o Before running the MapRed job, run a non-distributed application that tests
  what is the latest available version of the out-of-band data. 
  Then make this version available to the MapRed job.
  Two ways to do this. 
  o either set a job property just-in-time:
    addCachePathPair(""/mydata/v1234/"", ""localcache/mydata_latest""); 
    (see Job Configuration for meaning of this)
  o or publish the decision as an HDFS file containing the version.
    then rely on user code to read the version, and manually populate the cache:
    Cache.syncCache(""/hdfs/path/fileordir"", ""relative/local/fileordir"");
    (see MapReduce API for meaning of this)


Cache synchronization stages:
----------------------------
There are two stages: Client-to-HDFS and HDFS-to-TaskTracker

o Client-to-HDFS stage.
Options: A simple option is to not do anything here, i.e. rely on the user.

This is a reasonable option given previous remarks on the role of HDFS:
 HDFS is a staging/publishing area and a natural shared location.
In particular this means that the system need not track 
where the client files come from.


o HDFS-to-TaskTracker:
Client-to-HDFS synchronization (if done at all) should happen before this.
Then HDFS-to-TaskTracker synchronization must happen right before 
the data is needed on a node.



MapReduce cache API:
-------------------
Options:

1. No change in MapReduce framework code:
require the user to put this logic in map() (or reduce) function:

 in MyMapper constructor (or in map() on first record) user is asked to add:
 
    Cache.syncCache(""/hdfs/path/fileordir"", ""relative/local/fileordir"");
    Cache.syncCache(""...""); //etc.
  
-----

2. Put this logic in MapReduce framework and use Job properties to
   communicate the list of pairs (hdfs path; local path)
 
Directories are processed recursively.
If archives are treated specially then they are unarchived on destination.

 
MapReduce Job Configuration:
---------------------------
Options:

with No change in MapReduce framework code (see above)
 no special Job configuration: 
   it is up to the MapRed writer to configure and run the cache operations.

---
with Logic in MapReduce framework (see above)
 some simple Job configuration

JobConf.addCachePathPair(String, String)
JobConf.addCachePathPair(""/hdfs/path/fileordir"", ""relative/local/fileordir"");

"
HADOOP-287,Speed up SequenceFile sort with memory reduction,I replaced the merge sort with a quick sort and it yielded approx 30% improvement in sort time. It also reduced the memory requirement for sorting because the sort is done in place.
HADOOP-286,copyFromLocal throws LeaseExpiredException,"
Loading local files to dfs through hadoop dfs -copyFromLocal failed due to the following exception:

copyFromLocal: org.apache.hadoop.dfs.LeaseExpiredException: No lease on output_crawled.1.txt
        at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:414)
        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:190)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)
"
HADOOP-285,Data nodes cannot re-join the cluster once connection is lost,"A data node looses connection to a name node and then tries to offerService() again.
HADOOP-270 changes force it to start dataXceiveServer, which is already started and in this case
throws IllegalThreadStateException, which goes on in a loop, and never reaches the heartbeat section.
So the data node never re-joins the cluster, while from the out side it looks it's still running.
This is another reason why we see missing data, and don't see failed data nodes.
"
HADOOP-284,dfs timeout on open,"In my sort benchmark, I've started seeing hundred's of maps dying with timeout exceptions in the dfs.open() code:

java.net.SocketTimeoutException: timed out waiting for rpc response at org.apache.hadoop.ipc.Client.call(Client.java:305) at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:153) at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:457) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:444) at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:207) at org.apache.hadoop.dfs.DistributedFileSystem.openRaw(DistributedFileSystem.java:90) at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:46) at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:228) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:72) at org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:182) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:535) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:584) at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:395) at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:308) at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:424) at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:961)
"
HADOOP-283,Counting of running maps/reduces in tasktrackerstatus,"The functions countreducetasks and countmaptasks currently return the total number of maps/reduces on the tasktracker. They should only return the number of running maps/reduces. Because they return the total count, the jobtracker is not able to schedule tasks on these tasktrackers in the current cycle, when a task is finished and the tasktracker is runnning max number of maps/reduces."
HADOOP-282,the datanode crashes if it starts before the namenode,"If the datanode tries to register before the namenode is offering service, it crashes with a uncaught exception.

java.net.ConnectE
xception: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:507)
        at java.net.Socket.connect(Socket.java:457)
        at java.net.Socket.<init>(Socket.java:365)
        at java.net.Socket.<init>(Socket.java:207)
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:112)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:351)
        at org.apache.hadoop.ipc.Client.call(Client.java:289)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)
        at org.apache.hadoop.dfs.$Proxy0.register(Unknown Source)
        at org.apache.hadoop.dfs.DataNode.register(DataNode.java:176)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:109)
        at org.apache.hadoop.dfs.DataNode.makeInstanceForDir(DataNode.java:892)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:846)
        at org.apache.hadoop.dfs.DataNode.runAndWait(DataNode.java:862)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:917)"
HADOOP-281,dfs.FSDirectory.mkdirs can create sub-directories of a file!,"dfs.FSDirectory.mkdirs will merrily adds children to a directory tree node without checking whether it represents a directory. So it is possible to create a subdirectories of a file.

"
HADOOP-280,AllTestDriver has incorrect class name for DistributedFSCheck test,It has TestDFSIO instead.
HADOOP-279,running without the hadoop script causes warnings about log4j not being configured correctly,"Anyone who runs hadoop programs without the bin/hadoop script will get a message about log4j not being configured correctly. The problem is that the config file uses the system property ""hadoop.root.logger"", which is set in the script."
HADOOP-278,a missing map/reduce input directory does not produce a user-visible error message,"If map/reduce's input directory is in DFS and does not exist, the user has to find the problem in the jobtracker logs rather than either the launching program or the webapp."
HADOOP-277,Race condition in Configuration.getLocalPath(),"(attached: a patch to fix the problem, and a logfile showing the problem occuring twice)

There is a race condition in Configuration.java:

       Path file = new Path(dirs[index], path);
       Path dir = file.getParent();
       if (fs.exists(dir) || fs.mkdirs(dir)) {
         return file;

If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation:
 ""returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise""

That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists.

This was really happening. We use four temporary directories, and we had reducers failing all over the place with  bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below.

Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist.

060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05.
060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04.
...
060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out
...
060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out
...
060605 142531 task_0001_r_000009_1 0.31808624% reduce > append > /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out
...
060605 142725 task_0001_r_000009_1 java.io.FileNotFoundException: /data2/tmp/mapred/local/task_0001_r_000009_1/map_15.out


"
HADOOP-276,No appenders could be found for logger,"When you start the servers with an old configuration directory without the properties files, you get messages about:

log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration).
log4j:WARN Please initialize the log4j system properly.

The problem is that the property files are not included in the jar file."
HADOOP-275,log4j changes for hadoopStreaming,"A patch for hadoopStreaming. 
Use log4j 
Still Using short UTF8 strings

"
HADOOP-274,The new logging framework puts application logs into server directory in hadoop.log,"The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications).

Thoughts?"
HADOOP-273,Add a interactive shell for admistrative access to the DFS,"Implement a shell that allows the user to work in the dfs like on the local fs.

cd /user/
cd test
put text.txt someText.txt
cat someText.txt
rm someText.txt
"
HADOOP-272,bin/hadoop dfs -rm <dir> crashes in log4j code,"When I run ""bin/hadoop dfs -rm out-dir"" I get the following error messages:

log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory)
        at java.io.FileOutputStream.openAppend(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:177)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:102)
        at org.apache.log4j.FileAppender.setFile(FileAppender.java:289)
        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163)
        at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215)
        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256)
        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132)
        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612)
        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:122)
        at org.apache.log4j.Logger.getLogger(Logger.java:104)
        at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229)
        at org.apache.commons.logging.impl.Log4JLogger.<init>(Log4JLogger.java:65)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:494)
        at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529)
        at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235)
        at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370)
        at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:54)
        at org.apache.hadoop.dfs.DFSShell.main(DFSShell.java:307)
log4j:ERROR Either File or DatePattern options are not set for appender [DRFA].
Delete failed
"
HADOOP-271,add links to task tracker http server from task details and failure pages,Make the machine name in the list of task attempts and failures into links to the correct task tracker http server.
HADOOP-270,possible deadlock when shut down a datanode thread,"The DataNode class provides a method ""shutdown"" that can be used to notify a data node thread to abort itself gracefully. In addition this method waits for its data receiving server to terminate. This may cause possible deadlock if the method is called by the data receiving server."
HADOOP-269,add FAQ to Wiki,Hadoop should have an FAQ in the Wiki.  We can bootstrap this by reviewing the mailing list archives.
HADOOP-268,TaskTracker latency is slowing down maps because progress reporting is done inline,Moving the progress reporter to a separate thread should free up the user application to run faster.
HADOOP-266,RPC doesn not handle exceptions correctly,"1. Examining HADOOP-264 bug I realized that not all rpc server exceptions are actually returned to the client. Particularly, if an exception happens in 
org.apache.hadoop.ipc.Server.Connection.run()
for example inside 
param.readFields(in);
then it logged but never returned back to the client.
Client simply timeouts in this case, which is not exactly what one would expect.

2. On the way back 
org.apache.hadoop.ipc.Client.call()
rpc client always throws a RemoteException, rather than the exception wrapped into this RemoteException.
"
HADOOP-265,Abort tasktracker if it can not write to its local directories,"Currently if a task tracker is not able to write to any of its local directories, it continues to run and all the tasks assigned to it fail.

A task tracker should not start upif it has a problem reading/writing any of its local directories. It should abort if it gets the problem at run time."
HADOOP-264,WritableFactory has no permissions to create DatanodeRegistration,The datanode can not come up because the DatanodeRegistration is package local and the factory registration doesn't happen.
HADOOP-263,task status should include timestamps for when a job transitions,"It would help users to understand what happened if the task status included information about when the task transitioned:

Map:
   started
   finished

Reduce:
   started
   shuffle finished
   sort finished
   finished"
HADOOP-262,the reduce tasks do not report progress if they the map output locations is empty.,"The ReduceTaskRunner should report progress even if the number of ouput locations to copy is empty. In case, the last few maps are running on a tasktracker that goes down, all the reduce tasks waiting for these mapoutputs would fail."
HADOOP-261,"when map outputs are lost, nothing is shown in the webapp about why the map task failed","When a task tracker invokes mapOutputLost, it does not include a diagnostic that lets the user (or the web app) know what failed in the map. I want to add a diagnostic message."
HADOOP-260,the start up scripts should take a command line parameter --config making it easy to run multiple hadoop installation on same machines,--config paramter would allow getting rid of SSH_OPTS.
HADOOP-259,map output http client does not timeout,"The new map output http client uses java.net.URLConnection to fetch the data file. However under Java 1.4 there is no way to specify a timeout and it is set to infinite (or if not infinite at least 12 hours).  This causes reduce tasks to get ""stuck"" in the ""reduce > copy"" phase even after the ""Task failed to report status for 600 seconds. Killing."" message.

I will add the code in the ReduceTaskRunner to make sure that copies in-flight don't get stuck, but this is another point where a switch to java 1.5 would be helpful. Under 1.5 I could set the timeout on the connection and the read would timeout after the given interval and my entire change would be local to MapOutputLocation.copyFile.

For now, I'll assume that we need to maintain support for 1.4 and make the corresponding fix, but I'm grumbling..."
HADOOP-258,Cannot obtain block errors,"I've been seeing a lot of errors from dfs for the last couple days. The exception that I see is:

java.io.IOException: Could not obtain block: blk_2867431916903738534 file=/user/oom/big-rand/part000271 offset=0 at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:532) at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:641) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:83) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read1(BufferedInputStream.java:256) at java.io.BufferedInputStream.read(BufferedInputStream.java:313) at java.io.DataInputStream.readFully(DataInputStream.java:176) at java.io.DataInputStream.readFully(DataInputStream.java:152) at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:264) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:248) at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:238) at org.apache.hadoop.mapred.SequenceFileRecordReader.(SequenceFileRecordReader.java:36) at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:53) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:105) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:866)

I haven't had my fingers in that code recently, does it ring any bells for anyone?"
HADOOP-257,starting one data node thread to manage multiple data directories,"If a data node is configured with multiple data directories, current implementation of dfs will start multiple data node threads, each of which manages one data directory and talks to its name node independently. From the name node's point of view, it sees multiple data nodes instead of one.

I feel that a more scalable solution should be to start one data node thread that manages multiple data diretories. But the one data node thread needs to take care of the block allocation problem."
HADOOP-256,Implement a C api for hadoop dfs,Implement a C api for hadoop dfs to ease talking to hadoop's dfs from native C/C++ applications.
HADOOP-255,Client Calls are not cancelled after a call timeout,"In ipc/Client.java, if a call times out, a SocketTimeoutException is thrown but the Call object still exists on the queue.

What I found was that when transferring very large amounts of data, it's common for queued up calls to timeout. Yet even though the caller has is no longer waiting, the request is still serviced on the server and the data is sent to the client. The client after receiving the full response calls callComplete() which is a noop since nobody is waiting.

The problem is that the calls that timeout will retry and the system gets into a situation where data is being transferred around, but it's all data for timed out requests and no progress is ever made.

My quick solution to this was to add a ""boolean timedout"" to the Call object which I set to true whenever the queued caller times out. And then when the client starts to pull over the response data (in Connection::run) to first check if the Call is timedout and immediately close the connection.

I think a good fix for this is to queue requests on the client, and do a single sendParam only when there is no outstanding request. This will allow closing the connection when receiving a response for a request we no longer have pending, reopen the connection, and resend the next queued request. I can provide a patch for this, but I've seen a lot of recent activity in this area so I'd like to get some feedback first."
HADOOP-254,use http to shuffle data between the maps and the reduces,"To speed up the shuffle time, I'll use http (via the task tracker's jetty server) to send the map outputs."
HADOOP-253,we need speculative execution for reduces,"With my new http-based shuffle (on top of the svn head including sameer's parallel fetch), I just finished sorting 2010 g on 200 nodes in 8:49 with 9 reduce failures. However, the amusing part is that the replacement reduces were _not_ the slow ones. 8 of the original reduces were the only things running for the last hour. The job timings looked like:

Job 0001
  Total:
    Tasks: 16551
    Total: 10056104 secs
    Average: 607 secs
    Worst: task_0001_r_000291_0
    Worst time: 31050 secs
    Best: task_0001_m_013597_0
    Best time: 20 secs
  Maps:
    Tasks: 16151
    Total: 2762635 secs
    Average: 171 secs
    Worst: task_0001_m_002290_0
    Worst time: 2663 secs
    Best: task_0001_m_013597_0
    Best time: 20 secs
  Reduces:
    Tasks: 400
    Total: 7293469 secs
    Average: 18233 secs
    Worst: task_0001_r_000291_0
    Worst time: 31050 secs
    Best: task_0001_r_000263_1
    Best time: 5591 secs

And the number of tasks run per a node was very uneven:

#tasks node
124 node1161
117 node1307
117 node1124
116 node1253
114 node1310
111 node1302
111 node1299
111 node1298
111 node1249
111 node1221
110 node1288
110 node1286
110 node1211
109 node1268
108 node1292
108 node1202
108 node1200
107 node1313
107 node1277
107 node1246
107 node1242
107 node1231
107 node1214
106 node1243
105 node1251
105 node1212
105 node1205
104 node1272
104 node1269
104 node1210
104 node1203
104 node1193
104 node1128
103 node1300
103 node1285
103 node1279
103 node1209
103 node1173
103 node1165
102 node1276
102 node1239
102 node1228
102 node1204
102 node1188
101 node1314
101 node1303
100 node1301
100 node1252
99 node1287
99 node1213
99 node1206
98 node1295
98 node1186
97 node1293
97 node1265
97 node1262
97 node1260
97 node1258
97 node1235
97 node1229
97 node1226
97 node1215
97 node1208
97 node1187
97 node1175
97 node1171
96 node1291
96 node1248
96 node1224
96 node1216
95 node1305
95 node1280
95 node1263
95 node1254
95 node1153
95 node1115
94 node1271
94 node1261
94 node1234
94 node1233
94 node1227
94 node1225
94 node1217
94 node1142
93 node1275
93 node1198
93 node1107
92 node1266
92 node1220
92 node1219
91 node1309
91 node1289
91 node1270
91 node1259
91 node1256
91 node1232
91 node1179
89 node1290
89 node1255
89 node1247
89 node1207
89 node1201
89 node1190
89 node1154
89 node1141
88 node1306
88 node1282
88 node1250
88 node1222
88 node1184
88 node1149
88 node1117
87 node1278
87 node1257
87 node1191
87 node1185
87 node1180
86 node1297
86 node1178
85 node1195
85 node1143
85 node1112
84 node1281
84 node1274
84 node1264
83 node1296
83 node1148
82 node1218
82 node1168
82 node1167
81 node1311
81 node1240
81 node1223
81 node1196
81 node1164
81 node1116
80 node1267
80 node1230
80 node1177
80 node1119
79 node1294
79 node1199
79 node1181
79 node1170
79 node1166
79 node1103
78 node1244
78 node1189
78 node1157
77 node1304
77 node1172
74 node1182
71 node1160
71 node1147
68 node1236
68 node1183
67 node1245
59 node1139
58 node1312
57 node1162
56 node1308
56 node1197
55 node1146
54 node1106
53 node1111
53 node1105
49 node1145
49 node1123
48 node1176
46 node1136
44 node1132
44 node1125
44 node1122
44 node1108
43 node1192
43 node1121
42 node1194
42 node1138
42 node1104
41 node1155
41 node1126
41 node1114
40 node1158
40 node1151
40 node1137
40 node1110
40 node1100
39 node1156
38 node1140
38 node1135
38 node1109
37 node1144
37 node1120
36 node1118
34 node1133
34 node1113
31 node1134
26 node1127
23 node1101
20 node1131

And it should not surprise us that the last 8 reduces were running on nodes 1134, 1127,1101, and 1131. This really demonstrates the need to run speculative reduce runs. 

I propose that when the list of reduce jobs running is down to 1/2 the cluster size that we start running speculative reduces. I estimate that it would have saved around an hour on this run. Does that sound like a reasonable heuristic?"
HADOOP-252,add versioning to RPC,"currently, any change to any rpc message breaks the protocol, with mysterious exceptions occurring at run time.
a versioning sceme would have two benefits:
- intelligent error messages, indicating that an upgrade is required
- backwards compatibility could be supported.

the proposal is to add a ""const version"" for each protocol, and a method: int getVersion(int version) that sends the client's version and receives the server's version. This would be the first method invoked on connection. Both sides then either agree on the lowest version number, providing backwards compatibility support, or abort the connection as ""unsupported version""."
HADOOP-251,progress report failures kill task,"Communication problems in reporting progress to the task tracker kill the task because the exceptions are not caught. Since reporting progress is not critical, my patch catches the exception, logs it, and ignores it."
HADOOP-250,HTTP Browsing interface for DFS Health/Status,A web interface to view the DFS health/status (name and data nodes) is to be created. User can connect to the webserver on the namenode and a web page will be displayed. The web page will give some details about that namenode (startup time and the total cluster capacity). The web page will contain a table of 'live' and 'dead' datanodes. Each live datanode will be a link to the complete details of the datanode as given by DatanodeInfo (also see DataNodeReport).
HADOOP-249,Improving Map -> Reduce performance and Task JVM reuse,"These patches are really just to make Hadoop start trotting. It is still at least an order of magnitude slower than it should be, but I think these patches are a good start.

I've created two patches for clarity. They are not independent, but could easily be made so.

The disk-zoom patch is a performance trifecta: less disk IO, less disk space, less CPU, and overall a tremendous improvement. The patch is based on the following observation: every piece of data from a map hits the disk once on the mapper, and 3 (+plus sorting) times on the reducer. Further, the entire input for the reduce step is sorted together maximizing the sort time. This patch causes:

1)  the mapper to sort the relatively small fragments at the mapper which causes two hits to the disk, but they are smaller files.
2) the reducer copies the map output and may merge (if more than 100 outputs are present) with a couple of other outputs at copy time. No sorting is done since the map outputs are sorted.
3) the reducer  will merge the map outputs on the fly in memory at reduce time.

I'm attaching the performance graph (with just the disk-zoom patch) to show the results. This benchmark uses a random input and null output to remove any DFS performance influences. The cluster of 49 machines I was running on had limited disk space, so I was only able to run to a certain size on unmodified Hadoop. With the patch we use 1/3 the amount of disk space.

The second patch allows the task tracker to reuse processes to avoid the over-head of starting the JVM. While JVM startup is relatively fast, restarting a Task causes disk IO and DFS operations that have a negative impact on the rest of the system. When a Task finishes, rather than exiting, it reads the next task to run from stdin. We still isolate the Task runtime from TaskTracker, but we only pay the startup penalty once.

This second patch also fixes two performance issues not related to JVM reuse. (The reuse just makes the problems glaring.) First, the JobTracker counts all jobs not just the running jobs to decide the load on a tracker. Second, the TaskTracker should really ask for a new Task as soon as one finishes rather than wait the 10 secs.

I've been benchmarking the code alot, but I don't have access to a really good cluster to try the code out on, so please treat it as experimental. I would love to feedback.

There is another obvious thing to change: ReduceTasks should start after the first batch of MapTasks complete, so that 1) they have something to do, and 2) they are running on the fastest machines."
HADOOP-248,locating map outputs via random probing is inefficient,"Currently the ReduceTaskRunner polls the JobTracker for a random list of map tasks asking for their output locations. It would be better if the JobTracker kept an ordered log and the interface was changed to:

class MapLocationResults {
   public int getTimestamp();
   public MapOutputLocation[] getLocations();
}

interface InterTrackerProtocol {
  ...
  MapLocationResults locateMapOutputs(int prevTimestamp);
} 

with the intention that each time a ReduceTaskRunner calls locateMapOutputs, it passes back the ""timestamp"" that it got from the previous result. That way, reduces can easily find the new MapOutputs. This should help the ""ramp up"" when the maps first start finishing."
HADOOP-247,The Reduce Task thread for reporting progress during the sort exits in case of any IOException,"The Reduce task thread for reporting progress during the sort, exits in case of any exception (except InterruptedException). The solution would be to continue the thread in case of an exception."
HADOOP-246,the record-io generated c++ has wrong comments,"The comments on the namespaces on the closing come out backward:

} // end namespace org
} // end namespace apache
} // end namespace hadoop
} // end namespace record
} // end namespace test
"
HADOOP-245,record io translator doesn't strip path names,"When I run the record translator with a pathname, the path name is not stripped. So for example:

% bin/rcc --language c++ foo/bar/bat.jr

generates:

foo/bar/bat.jr.hh (instead of ./bat.jr.hh)
and the first line is #ifndef __FOO/BAR/BAT_JR_HH__

the first was unexpected and the second is clearly wrong."
HADOOP-244,very long cleanup after a job fails,"Eight hours after a job failed (it executed for about 14 hours prior to failing), many task trackers keep throwing the exceptions below:

060523 121732 Server handler 0 on 50040 caught: java.io.FileNotFoundException: LocalFS
java.io.FileNotFoundException: LocalFS
        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)
        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)
        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)
        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)
060523 121814 task_0006_r_000123_0 copy failed: task_0006_m_046105_0 from node5:50040
java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:305)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)
        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)
        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:112)
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:67)
060523 121814 task_0006_r_000123_0 0.13023989% reduce > copy > task_0006_m_046105_0@node5:50040
060523 121814 task_0006_r_000123_0 Copying task_0006_m_048815_0 output from node6
060523 121817 SEVERE Can't open map output:/hadoop/mapred/local/task_0006_m_031921_0/part-152.out
java.io.FileNotFoundException: LocalFS
        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)
        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)
        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)
        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)
060523 121817 Unknown child with bad map output: task_0006_m_031921_0. Ignored.
060523 121817 Server handler 1 on 50040 caught: java.io.FileNotFoundException: LocalFS
java.io.FileNotFoundException: LocalFS
        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:123)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)
        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)
        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:151)
        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)
060523 121914 task_0006_r_000123_0 copy failed: task_0006_m_048815_0 from node6:50040
java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:305)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:150)
        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)
        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:112)
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:67)
"
HADOOP-243,WI shows progress as 100.00% before actual completion (rounding error),"For jobs of over 50000 tasks, the rounding error in the WI is confusing.
When less than 0.005% of the map (or reduce) tasks remain to execute, the WI shows progress as 100.00%, which is misleading.
Rounding down to the nearest .01% would be better."
HADOOP-242,"job fails because of ""No valid local directories in property: "" exception","when running a fairly large job, of 70+K map tasks, I get many exceptions as shown below, and eventually the job failes when a task fails four times.
The exception doesn't really tell us enough information to debug this properly, so the first thing to do would be to add more information (path) to the exception.
The path indicated in the config file exists, is writable and valid, though 'path' may be anything.

the exception:
java.io.IOException: No valid local directories in property: mapred.local.dir at org.apache.hadoop.conf.Configuration.getLocalPath(Configuration.java:293) at org.apache.hadoop.mapred.JobConf.getLocalPath(JobConf.java:153) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:523) at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.launchTask(TaskTracker.java:572) at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:389) at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:303) at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:418) at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:920)

the code:
  public Path getLocalPath(String dirsProp, String path)
    throws IOException {
    String[] dirs = getStrings(dirsProp);
    int hashCode = path.hashCode();
    FileSystem fs = FileSystem.getNamed(""local"", this);
    for (int i = 0; i < dirs.length; i++) {  // try each local dir
      int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;
      Path file = new Path(dirs[index], path);
      Path dir = file.getParent();
      if (fs.exists(dir) || fs.mkdirs(dir)) {
        return file;
      }
    }
    throw new IOException(""No valid local directories in property: ""+dirsProp);
  }
"
HADOOP-241,TestCopyFiles fails under cygwin due to incorrect path,"Under cygwin TestCopyFiles generates an incorrect url which includes windows style path.
This is the result of concatenation of a win path with unix path.
File.getPath() should be used to produce a consistent path.
"
HADOOP-240,namenode should not log failed mkdirs at warning level,"when a namenode creates a directory, it also recursively tries to creates its parent directories. If they already exist, the lastSuccess is  false and the ""error"" is logged at the warning level.

The right approach is to set the logging leve lower to fine or simply ignore the the ""error""."
HADOOP-239,job tracker WI drops jobs after 24 hours,"The jobtracker's WI, keeps track of jobs executed in the past 24 hours.
if the cluster was idle for a day (say Sunday) it drops all its history.
Monday morning, the page is empty.
Better would be to store a fixed number of jobs (say 10 each of succeeded and failed jobs).

Also, if the job tracker is restarted, it loses all its history.
The history should be persistent, withstanding restarts and upgrades."
HADOOP-238,map outputs transfers fail with EOFException,"My patch on Friday unintentionally included part of my explorations with the file transfers, which broke map output transfers."
HADOOP-237,Standard set of Performance Metrics for Hadoop,"I am starting to use Hadoop's shiny new Metrics API to publish performance (and other) Metrics of running jobs and other daemons.

Which performance metrics are people interested in seeing ? If possible, please group them according to modules, such as map-reduce, dfs, general-cluster-related etc. I will follow this process:

1. collect this list
2. assess feasibility of obtaining metric
3. assign context/record/metrics names
4. seek approval for names
5. instrument the code.
"
HADOOP-236,job tracker should refuse connection from a task tracker with a different version number,"After one mapred system upgrade, we noticed that all tasks assigned to one task tracker failed. It turned out that for some reason the task tracker was not upgraded.

To avoid this, a task tracker should reports its version # when it registers itsself with a job tracker. If the job tracker receives an inconsistent version #, it should refuse the connection."
HADOOP-235,LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException,"openRaw should throw f.toString() on an error, not toString()."
HADOOP-234,Hadoop Pipes for writing map/reduce jobs in C++ and python,"MapReduce C++ support

Requirements 

1. Allow users to write Map, Reduce, RecordReader, and RecordWriter functions in C++, rest of the infrastructure already present in Java should be reused. 
2. Avoid users having to write both Java and C++ for this to work. 
3. Avoid users having to work with JNI methods directly by wrapping them in helper functions. 
4. The interface should be SWIG'able."
HADOOP-233,add a http status server for the task trackers,"I'd like to have:
  1. a jetty server on each task tracker to serve up logs and status
  2. the jetty server on the job tracker serve up logs"
HADOOP-232,jar files sent to task tracker should override existing jar,"jar files sent to task tracker are appended to list, rather than prepended to it.
this results in the original jar getting executed, although a new one was sent  - not the intent."
HADOOP-231,DFSShell Improvement wish list,"This is to discuss general DFSShell improvements, standardization, conventions, etc.
Here are links to assorted ideas on this issue from previous issues.
We need to generalize them at some point.

HADOOP-59
HADOOP-226
HADOOP-222#action_12412326
HADOOP-230
"
HADOOP-230,improve syntax of the hadoop dfs command,"the hadoop dfs syntax (hadoop dfs -option value -cmd arg) is clunky: options must appear before the command, the command looks like just another option.
I propose a more standard syntax:
1. the command (ls, mv, du etc.) always comes first
2. no '-' for the command
3. options may appear anywhere, including between the command and its arguments

allowed syntax would be:
hadoop dfs ls -dfs <dfs> /path
hadoop dfs ls /path -dfs <dfs>

other commands may benefit from a similar syntax change:
hadoop job [-status] job_0002 -jt <jt>
etc."
HADOOP-229,hadoop cp should generate a better number of map tasks,"hadoop cp currently assigns 10 files to copy per map task.
in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times.
better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker."
HADOOP-228,hadoop cp should have a -config option,"hadoop cp should have a -config option to enable overriding of default parameters.
it  would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job"
HADOOP-227,Namespace check pointing is not performed until the namenode restarts.,"In current implementation when the name node starts, it reads its image file, then
the edits file, and then saves the updated image back into the image file.
The image file is never updated after that.
In order to provide the system reliability reliability the namespace information should
be check pointed periodically, and the edits file should be kept relatively small.
"
HADOOP-226,DFSShell problems. Incorrect block replication detection in fsck.,"1. We need to adjust Dfsck to the new per-file replication feature.
fsck checks block replication based on the configured global replication parameter.
Which is now just the default. The actual file replication is returned in DFSFileInfo.
So at least the reporting is screwed by that, although I didn't check what will happen with
other options -move and -delete.

2. fsck throws NullPointerException if you type
bin/hadoop fsck -files /doc
instead of
bin/hadoop fsck /doc -files

3. Unfortunately, there are several commands that throw different kinds of Exceptions
rather than at least printing the usage info, when some of its arguments are missing or
misplaced. ArrayIndexOutOfBoundsException is one them. Try
bin/hadoop dfs -mv
bin/hadoop dfs -cp
bin/hadoop dfs -rm

4. In general the shell is growing and getting more sophisticated.
Should we work out a general convention on how the parameters should be structured, named,
short/long version of the keywords, help, etc."
HADOOP-225,tasks are left over when a job fails,"when jobs are stopped or otherwise fail, tasks are often left around.
the job tracker shows that there are map or reduce (mostly reduce) tasks running, when no job is running.
these accumulate over time.
eventually there are so many of those, that the job tracker can't launch new tasks, requiring a restart of the MR cluster."
HADOOP-222,Set replication from dfsshell,"Added ability to set replication for a directory/file from the command line.
Not heavily tested..."
HADOOP-221,make the number of map output configurable,"The number of map output server threads in each task tracker is currently set to the number of task slots. Since the optimum setting depends on the network, it would be nice to have more threads serving map output files than reduces."
HADOOP-220,Add -dfs and -jt command-line parameters to specify namenode and jobtracker.,Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.
HADOOP-219,SequenceFile#handleChecksumException NPE,"The SequenceFile#handleChecksumException assumes the conf data member has been set.  It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor.  The latter is used by ReduceTask Sorter:


java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400)
	at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837)
	at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881)
	at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766)
	at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702)
	at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)
"
HADOOP-218,Inefficient calls to get configuration values in TaskInprogress,"Each time a pollforNewTask in called on the JobTracker, the taskinprogress makes a call to hasSpeculativeTask() which current does a conf.getSpeculativeExecution() each time its called. The fix would be to store it in the TaskInProgress as soon as it is created and make only a single call to conf.getSpeculativeExecution()."
HADOOP-217,IllegalAcessException when creating a Block object via WritableFactories,"When I ran the dfs namenode, I received an error message listed below. Changing Block class to be public will be able to fix the problem.

java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""
java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:226)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:163)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:211)
        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:88)
        at org.apache.hadoop.ipc.Server$Connection.run(Server.java:154)
Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""
        at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)
        at java.lang.Class.newInstance0(Class.java:344)
        at java.lang.Class.newInstance(Class.java:303)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)
        ... 5 more
"
HADOOP-216,Task Detail web page missing progress,"You can see progress on the job view web page, but not the individual task page."
HADOOP-215,New method for obtaining report of NameNode and JobTracker internals,"Many weeks ago we (at shopping.com research) decided we wanted to be able to get reports from the internals of JobTracker and NameNode. The hadoop web server provides some of this, but we wanted a more structured output, and easier extensibility.

So, we decided to use xml, and I wrote it.

There is a very thin interface to ClientProtocol, and JobSubmissionProtocol like this:

public XmlReporter getXmlReport(String classname, String question);

The implementation (in JobTracker and NameNode ) looks like this:

public XmlReporter getXmlReport(String classname, String question)
    {
		XmlReporter reporter = XmlReporter.getInstance( classname, this,  question);
		reporter.report();
		return reporter;
    }

The idea being that you pass in some xml (question), an XmlReporter (classname) is instanciated and passed back. 

An XmlReporter object consists of nothing more that two org.w3c.dom.Document objects (one in the question, the other is the answer). The Writable interface, for the RPC, simply serializes the dom tree to a string, and then parses it back to a dom tree.

Anyway, here it is. I would like for it to either make it into the code, or for me to find anoher way. 

"
HADOOP-212,allow changes to dfs block size,"Trying to change the DFS block size, led the realization that the 32,000,000 was hard coded into the source code. I propose:
  1. Change the default block size to 64 * 1024 * 1024.
  2. Add the config variable dfs.block.size that sets the default block size.
  3. Add a parameter to the FileSystem, DFSClient, and ClientProtocol create method that let's the user control the block size.
  4. Rename the FileSystem.getBlockSize to getDefaultBlockSize.
  5. Add a new method to FileSytem.getBlockSize that takes a pathname.
  6. Use long for the block size in the API, which is what was used before. However, the implementation will not work if block size is set bigger than 2**31.
  7. Have the InputFormatBase use the blocksize of each file to determine the split size.

Thoughts?"
HADOOP-211,logging improvements for Hadoop,"Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 
broad changes to the way logging is currently done, these being:

- The use of a uniform logging format by all Hadoop subsystems
- The use of Apache commons logging as a facade above an underlying logging framework
- The use of Log4J as the underlying logging framework instead of java.util.logging

This is largely polishing work, but it seems like it would make log analysis and debugging
easier in the short term. In the long term, it would future proof logging to the extent of
allowing the logging framework used to change while requiring minimal code change. The 
propos changes are motivated by the following requirements which we think Hadoops 
logging should meet:

- Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc.
- Log entries should be clearly annotated with a timestamp and a logging level
- Log entries should be traceable to the subsystem from which they originated
- The logging implementation should allow log entries to be annotated with source code 
location information like classname, methodname, file and line number, without requiring
code changes
- It should be possible to change the logging implementation used without having to change
thousands of lines of code
- The mapping of loggers to destinations (files, directories, servers etc.) should be 
specified and modifiable via configuration


Uniform logging format:

All Hadoop logs should have the following structure.

<Header>\n
<LogEntry>\n [<Exception>\n]
.
.
.

where the header line specifies the format of each log entry. The header line has the format:
'# <Fieldname> <Fieldname>...\n'. 

The default format of each log entry is: '# Timestamp Level LoggerName Message', where:

- Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS
- Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.)
- LoggerName is the short name of the logging subsystem from which the message originated e.g.
fs.FSNamesystem, dfs.Datanode etc.
- Message is the log message produced


Why Apache commons logging and Log4J?

Apache commons logging is a facade meant to be used as a wrapper around an underlying logging
implementation. Bridges from Apache commons logging to popular logging implementations 
(Java logging, Log4J, Avalon etc.) are implemented and available as part of the commons logging
distribution. Implementing a bridge to an unsupported implementation is fairly striaghtforward
and involves the implementation of subclasses of the commons logging LogFactory and Logger 
classes. Using Apache commons logging and making all logging calls through it enables us to
move to a different logging implementation by simply changing configuration in the best case.
Even otherwise, it incurs minimal code churn overhead.

Log4J offers a few benefits over java.util.logging that make it a more desirable choice for the
logging back end.

- Configuration Flexibility: The mapping of loggers to destinations (files, sockets etc.)
can be completely specified in configuration. It is possible to do this with Java logging as
well, however, configuration is a lot more restrictive. For instance, with Java logging all 
log files must have names derived from the same pattern. For the namenode, log files could 
be named with the pattern ""%h/namenode%u.log"" which would put log files in the user.home
directory with names like namenode0.log etc. With Log4J it would be possible to configure
the namenode to emit log files with different names, say heartbeats.log, namespace.log,
clients.log etc. Configuration variables in Log4J can also have the values of system 
properties embedded in them.

- Takes wrappers into account: Log4J takes into account the possibility that an application
may be invoking it via a wrapper, such as Apache commons logging. This is important because
logging event objects must be able to infer the context of the logging call such as classname,
methodname etc. Inferring context is a relatively expensive operation that involves creating
an exception and examining the stack trace to find the frame just before the first frame 
of the logging framework. It is therefore done lazily only when this information actually 
needs to be logged. Log4J can be instructed to look for the frame corresponding to the wrapper
class, Java logging cannot. In the case of Java logging this means that a) the bridge from 
Apache commons logging is responsible for inferring the calling context and setting it in the 
logging event and b) this inference has to be done on every logging call regardless of whether
or not it is needed.

- More handy features: Log4J has some handy features that Java logging doesn't. A couple
of examples of these:
a) Date based rolling of log files 
b) Format control through configuration. Log4J has a PatternLayout class that can be 
configured to generate logs with a user specified pattern. The logging format described
above can be described as ""%d{MM/dd/yyyy:HH:mm:SS} %c{2} %p %m"". The format specifiers
indicate that each log line should have the date and time followed by the logger name followed
by the logging level or priority followed by the application generated message.
"
HADOOP-210,Namenode not able to accept connections,"I am running owen's random writer on a 627 node cluster (writing 10GB/node).  After running for a while (map 12% reduce 1%) I get the following error on the Namenode:

Exception in thread ""Server listener on port 60000"" java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:574)
        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:105)

After this, the namenode does not seem to be accepting connections from any of the clients. All the DFSClient calls get timeout. Here is a trace for one of them:
java.net.SocketTimeoutException: timed out waiting for rpc response
	at org.apache.hadoop.ipc.Client.call(Client.java:305)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:149)
	at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:419)
	at org.apache.hadoop.dfs.DFSClient$DFSInputStream.(DFSClient.java:406)
	at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:171)
	at org.apache.hadoop.dfs.DistributedFileSystem.openRaw(DistributedFileSystem.java:78)
	at org.apache.hadoop.fs.FSDataInputStream$Checker.(FSDataInputStream.java:46)
	at org.apache.hadoop.fs.FSDataInputStream.(FSDataInputStream.java:228)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:157)
	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:43)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:105)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:785).


The namenode then has around 1% CPU utilization at this time (after the outofmemory exception has been thrown). I have profiled the NameNode and it seems to be using around a maixmum heap size of 57MB (which is not much). So, heap size does not seem to be a problem. It might be happening due to lack of Stack space? Any pointers?
"
HADOOP-209,Add a program to recursively copy directories across file systems,"A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as:

hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir

""cp"" command would invoke a map-reduce program to copy files recursively.

I willl attach a patch as soon as svn is up and running."
HADOOP-208,add failure page to webapp,I'd like a webapp page that just lists the failures in a job so that I can find them more easily. I also want to put back the job detail page.
HADOOP-207,Patch to HADOOP-96 uses long deprecated call,System.getenv() was deprecated in Java 1.4. My mixed Java 1.4/Java 1.5 cluster won't start up with this code in place. Should probably change the scripts to pass the necessary environment variables in using -D or explicit arguments.
HADOOP-205,the job tracker does not schedule enough map on the cluster,The job tracker during my big sort runs only has 80-120 maps running at a time with 2 tasks/node and 179 nodes and a large queue of map tasks. It seems to be caused by the load balancing using the current load rather than the required load to run all of the queued tasks.
HADOOP-204,Need to tweak a few things in the metrics package to support the Simon plugin,"(1) added an extra metrics.jar target to the build.xml so that I can build a stand-alone library containing only the
metrics package and its subpackages.

(2) added serialversionUIDs to a bunch of classes to make Eclipse happy

(3) made AbstractMetricsContext.createRecord final, and added a protected newRecord that subclasses can use
to customize record creation without breaking the parent class.

(4) minor fix to how errors in callbacks are handled

(5) constructor in MetricsRecordImpl made protected rather than package private so that it can be subclassed

(6) extended Util.parse(String serverSpecs, int defaultPort) to handle the case of a null serverSpecs by defaulting to localhost

"
HADOOP-203,remove deprecated java.io.File methods,"Now that the 0.2 release is out, we should remove the deprecated FileSystem methods that use java.io.File, since using org.apache.hadoop.fs.Path is less error-prone."
HADOOP-202,sort should use a smaller number of reduces,We should see better performance with fewer reduces. I'll change the default number of reduces to be equal to the capacity of the cluster.
HADOOP-201,hadoop dfs -report throws exception,"Running hadoop dfs -report throws the lovely exception below.
Changing org.apache.hadoop.dfs.DatanodeInfo back to being a public class solves the problem.


~/hadoop$ bin/hadoop dfs -report
060508 104801 parsing file:/home/hadoop/hadoop/conf/hadoop-default.xml
060508 104801 parsing file:/home/hadoop/hadoop/conf/hadoop-site.xml
060508 104801 No FS indicated, using default:xxx:9000
060508 104801 Client connection to 10.0.0.12:9000: starting
Total raw bytes: 2763338170368 (2573.55 Gb)
Used raw bytes: 1548564473694 (1442.21 Gb)
% used: 56.03%

Total effective bytes: 145953744375 (135.93 Gb)
Effective replication multiplier: 10.609967427182013
-------------------------------------------------
060508 104801 Client connection to 10.0.0.12:9000 caught: java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers ""public""
java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers ""public""
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:226)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:163)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:211)
        at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:60)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:170)
Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.DatanodeInfo with modifiers ""public""
        at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)
        at java.lang.Class.newInstance0(Class.java:344)
        at java.lang.Class.newInstance(Class.java:303)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)
        ... 5 more
060508 104801 Client connection to 10.0.0.12:9000: closing
"
HADOOP-200,The map task names are sent to the reduces,"As each reduce is created, it is given the entire set of potential map names. For my large sort jobs with 64k maps, this means that each reduce task is given a two dimensional array that is 5 tasks/map * 64k maps = 320k strings. Since the reduce task is passed from the job tracker to the task tracker and down to the task runner, passing the entire list is very expensive. I suspect that this is the cause of the slow downs that I see in the task trackers heart beats when the reduce tasks are being launched.

I propose that the ReduceTask be changed to just get the count of maps, with ids from 0 .. maps -1.
  public ReduceTask(String jobFile, String taskId, int maps, int partition);
Then we need to change the protocol for finding map outputs:
  MapOutputLocation[] locateMapOutputs(String jobId, int[] mapIds, int partition);
"
HADOOP-199,reduce copy progress not updating,"I'm running with the svn head from Friday and my patch for hadoop-180 and I'm not getting progress updates until reduces finish. I'm worried that it may be related to my changes for hadoop-182. I'll track it down, but I wanted to let everyone know."
HADOOP-198,adding owen's examples to exampledriver,owen's sorter and randomwriter are not added to the examples.jar file
HADOOP-196,Fix buggy uselessness of Configuration( Configuration other) constructor,"The constructor 
public Configuration(Configuration other) "
HADOOP-195,improve performance of map output transfers,"The data transfer of the map output should be transfered via http instead rpc, because rpc is very slow for this application and the timeout behavior is suboptimal. (server sends data and client ignores it because it took more than 10 seconds to be received.)"
HADOOP-194,Distributed checkup of the file system consistency.,"This is a map-reduce based test that checks consistency of the file system
by  reading all blocks of all files, and detecting which of them are missing or corrupted.
See HADOOP-95 and HADOOP-101 for related discussions.

This could be an alternative to the sequential checkup in dfsck.
It would be nice to integrate distributed checkup with dfsck, but I don't yet see how.

This test reuses classes defined in HADOOP-193.
"
HADOOP-193,DFS i/o benchmark.,"DFS i/o benchmark is a map-reduce based test that measures performance of the cluster for reads and writes.
This is an evolved version of HADOOP-72, and HADOOP-95 test.

This test writes into or reads from a specified number of files.
File size is specified as a parameter to the test.
Each file is processed in a separate map task.
The unique reducer then collects stats.
Finally, the following information is displayed

# read or write test
# date and time the test finished
# number of files processed
# total number of bytes processed
# throughput in mb/sec (total number of bytes / sum of processing times)
# average i/o rate in mb/sec per file
# standard i/o rate deviation

I  included the test into the AllTestDriver."
HADOOP-192,Trivial JRE 1.5 versus 1.4 bug,Long.valueOf(long) is in 1.5 but not in 1.4.x.  Use new Long(long) instead.
HADOOP-191,add hadoopStreaming to src/contrib,"This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree.
hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks.
The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce)


TO test the patch: 
Merge the patch. 
The only existing file that is modified is trunk/build.xml
trunk>ant deploy-contrib
trunk>bin/hadoopStreaming : should show usage message
trunk>ant test-contrib    : should run one test successfully

TO add src/contrib/someOtherProject:
edit src/contrib/build.xml



"
HADOOP-190,Job fails though task succeeded if we fail to exit,"This is an odd case.  Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal.

My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running,  my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running.  After ten minutes, TT steps in,  kills the child and does cleanup of the successful output.  Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails.

Below is illustration of the problem using log output:

....
060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a
um.jpg 24891 image/jpeg
060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp
24891 image/jpeg
060501 090401 Task task_0001_m_000798_0 is done.
...
060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds
Killing.
....
060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0
060501 091410 task_0001_m_000798_0 done; removing files.

Then, subsequently....

060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa
-12.out
java.io.FileNotFoundException: LocalFS
...

and on and on."
HADOOP-189,"Add job jar lib, classes, etc. to CLASSPATH when in standalone mode","Currently, in standalone mode,  hadoop is unable to launch other than the most basic of job jars where 'basic' is a job jar with nought but class files at top level of the jar with Main-Class pointing at entry point.  If the job jar has dependencies on jars under the job jar lib or there are job jar plugins in the classes dir, etc.,  these dependencies are not loaded and the job fails launch."
HADOOP-188,more unprotected RPC calls in JobClient.runJob allow loss of job due to timeout,"I fixed one of the RPC calls in JobClient.runJob, but I missed a couple of others."
HADOOP-187,simple distributed dfs random data writer & sort example applications,"These are the examples/benchmark programs that I've been using to test Hadoop map/reduce with. The first is a program that runs 10 maps/node and each map writes 1 gig of random data to a dfs file as a SequenceFile of BytesWritable/BytesWritable.

The second uses the identity map and reduce to sort the data and write it out to dfs."
HADOOP-186,communication problems in the task tracker cause long latency,"The Task Tracker's offerService loop has no protection from exceptions, so that any communication problems with the Job Tracker, such as RPC timeouts, cause the TaskTracker to sleep 5 seconds and start again at the top of the loop. "
HADOOP-185,tasks are lost during pollForNewTask,"There is the potential for ""losing"" tasks that are assigned by the JobTracker to a TaskTracker, but that fail during returning the result (usually due to a RPC timeout). In this case, the Job becomes ""wedged"" in that the tasks will never run and never time out."
HADOOP-184,hadoop nightly build and regression test on a cluster,create a jar file for the tests and have  filesystem and mapreduce tests on the cluster
HADOOP-183,adjust file replication factor when loading image and edits according to replication.min and replication.max,"Currently in dfs, when namenode starts, a file's replication factor is loaded either from image or edits. The replication factor may be smaller than replication.min or greater than replication.max."
HADOOP-182,lost task trackers should not update status of completed jobs,"When a Task Tracker is lost (by not sending a heartbeat for 10 minutes), the JobTracker marks the tasks that were active on that node as failed. There are two issues:
   1. No task from a completed or failed job should be modified.
   2. No reduces should be marked as failed, since their output is in dfs and therefore not only on the dead node."
HADOOP-180,task tracker times out cleaning big job,"After completing a big job (63,920 maps, 1880 reduces, 188 nodes), lots of the TaskTrackers timed out because the task cleanup is handled by the same thread as the heartbeats."
HADOOP-179,task tracker ghosts remain after 10 minutes,"I had a bunch of TaskTrackers time out because they were in the middle of job cleanup and the JobTracker restarted them by responding to emitHeartbeat with UNKNOWN_TASKTRACKER. Afterwards, I ended up with both the new and the restarted TaskTrackers on the list:

node1100_1234, 0-10 seconds since heartbeat
node1100_4321,  >10,000 seconds since heartbeat"
HADOOP-178,piggyback block work requests to heartbeats and move block replication/deletion startup delay from datanodes to namenode,"Currently each datanode sends at least two messages to namenode within a heartbeat interval. One is a heartbeat message and another is block work request. By piggybacking the block work request to a heartbeat can greatly cut the number of messages between a datanode and the namenode.

Secondly each datanode waits for a configurable ""StartupPeriod"" before it sends a block work request in order to avoid uneccessary block replication at startup time. But if the namenode starts much later than datanodes, this scheme does not work. Furthermore, the namenode has more information to decide when to send block work to datanodes. For example, all datanodes send block reports etc. It is more resonable to move the startup delay from datanodes to the namenode "
HADOOP-177,improvement to browse through the map/reduce tasks,The Jobtracker webbapp currently shows all the maps and reduce tasks on a single page. This sometimes causes the browser to crash with 1000's of maps/recudes running.
HADOOP-176,comparators of integral writable types are not transitive for inequalities,"Consider the following code from IntWritable.java :

    public int compare(byte[] b1, int s1, int l1,
                       byte[] b2, int s2, int l2) {
      int thisValue = readInt(b1, s1);
      int thatValue = readInt(b2, s2);
      return thisValue - thatValue;
    }

If a Java Runtime subtracts 20 from -(2^31 - 10) it gets a huge positive number, not the negative value that the comparator should return.

LongWritable does this right, of course.

That last line should be

       return (thisValue<thatValue ? -1 : (thisValue==thatValue ? 0 : 1));

-dk
"
HADOOP-175,Utilities for reading SequenceFile and MapFile,"Most data in Hadoop is stored in SequenceFile-s and MapFile-s. Sometimes there is a need to examine such files, but no specialized utilities exist ro read them.

These two classes provide a functionality to examine individual records in such files, and also to dump the content of such files to a plain text output."
HADOOP-174,jobclient kills job for one timeout,"The launching application (via JobClient) checks the status once a second. If any timeouts or errors occur, the user's job is killed. "
HADOOP-173,optimize allocation of tasks w/ local data,"When a job first starts, all task trackers ask the job tracker for jobs at once.  With lots of task trackers, the job tracker gets very slow.  The first type of task that the job tracker attempts to find is one with some of its input data stored on the same node as the task tracker.  This case currently loops through tasks blindly, which, on average, requires numHosts/(replication*2) iterations to find a match (I think).  This could be optimized by adding a table mapping from host to task.
"
HADOOP-172,rpc doesn't handle returning null for a String[],"The job tracker gets errors in returning the result from pollForTaskWithClosedJob

060427 100434 Served: pollForTaskWithClosedJob 0
declaredClass = [Ljava.lang.String;
instance class = org.apache.hadoop.io.NullWritable
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95)
        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)
"
HADOOP-171,need standard API to set dfs replication = high,"There should be a standard way to indicate that files should be highly replicated, appropriate for files that all nodes will read.  This should be settable both on file creation and for already-existing files.  Perhaps specifying a particular replication value, like Short.MAX_VALUE, or zero, can be used to signal this.  The level should not be constant, but should be relative to the cluster size and network topography.  If more nodes are added or if nodes are deleted, the actual replication count should increase or decrease.

Initially, all that is needed is an API to specify this.  It could initially be implemented with a constant (e.g., 10) or with something related to the number of datanodes (sqrt?), and needn't auto-adjust as the cluster size changes  That is only  the long-term goal.

When JobClient copies job files (job.xml & job.jar) into the job's filesystem, it should specify this replication level.
"
HADOOP-170,setReplication and related bug fixes,"Having variable replication (HADOOP-51) it is natural to be able to
change replication for existing files. This patch introduces the functionality.
Here is a detailed list of issues addressed by the patch.

1) setReplication() and getReplication() methods are implemented.
2) DFSShell prints file replication for any listed file.
3) Bug fix. FSDirectory.delete() logs delete operation even if it is not successful.
4) Bug fix. This is a distributed bug.
Suppose that file replication is 3, and a client reduces it to 1.
Two data nodes will be chosen to remove their copies, and will do that.
After a while they will report to the name node that the copies have been actually deleted.
Until they report the name node assumes the copies still exist.
Now the client decides to increase replication back to 3 BEFORE the data nodes
reported the copies are deleted. Then the name node can choose one of the data nodes,
which it thinks have a block copy, to replicate the block to new data nodes.
This setting is quite unusual but possible even without variable replications.
5) Logging for name and data nodes is improved in several cases.
E.g. data nodes never logged that they deleted a block.

"
HADOOP-169,a single failure from locateMapOutputs kills the entire job,Any communication failure in locateMapOutputs kills both the reduce task and the entire job.
HADOOP-168,"JobSubmissionProtocol and InterTrackerProtocol don't include ""throws IOException"" on all methods","Timeouts for RPC's are thrown as IOExceptions so unless the method is declared as throwing IOException in the Procotol interface, the Java library wraps the exception in an UndeclaredThrowableException."
HADOOP-167,reducing the number of Configuration & JobConf objects created,"Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again."
HADOOP-166,IPC is unable to invoke methods that use interfaces as parameter,"Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.
"
HADOOP-165,long response times from task trackers under load,"we are seeing very slow response times from the task tracker. I put in some instrumentation to measure how long each call took for the RPC.Sever code to run the method (so it does not include serialization/deserialization time). The top of the list (in ms) looks like:

7581 progress node1192
7022 ping node1192
5393 ping node1162
4854 progress node1162
4749 progress node1194
4709 ping node1194
3813 ping node1100
3486 ping node1190
3266 progress node1190
3187 progress node1265
3078 ping node1203
2972 progress node1203
2947 progress node1240
2889 progress node1100
2875 ping node1116
2843 ping node1189
2772 ping node1183
2737 ping node1110
2727 progress node1183
2710 ping node1123
2563 ping node1304
2527 progress node1144
2479 ping node1137
2476 progress node1304
2430 ping node1240
2416 ping node1144
2377 progress node1176
2339 ping node1109
2321 progress node1114
2311 ping node1157
2185 ping node1265
2185 ping node1109
2172 ping node1114
2145 progress node1109
2127 ping node1176
2083 progress node1189
2076 ping node1229
2073 progress node1188
2072 progress node1123
2048 ping node1161
2003 progress node1110
1989 ping node1180
1963 ping node1114
"
HADOOP-163,If a DFS datanode cannot write onto its file system. it should tell the name node not to assign new blocks to it.,"I observed that sometime, if a file of a data node is not mounted properly, it may not be writable. In this case, any data writes will fail. The name node should stop assigning new blocks to that data node. The webpage should show that node is in an abnormal state.

"
HADOOP-162,concurrent modification exception in FSNamesystem.Lease.releaseLocks,"FSNameSystem.Lease.releaseLocks iterates through the creates set, calling InternalReleaseCreate on each element, which changes the creates set in the Lease. This causes a ConcurrentModificationException if you have more than two files that are owned by the lease that times out."
HADOOP-161,"dfs blocks define equal, but not hashcode","Findbugs reports that dfs.Block defines equals but not hashcode, which is problematic if it is ever put into a hash table."
HADOOP-160,sleeping with locks held,"I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that."
HADOOP-159,"name node at 100% cpu, making redundant replications","some hours after adding some new nodes to the cluster, the name node went into a state where it's consuming 100% cpu.
The log file keeps logging messages of the forms
060421 155049 Obsoleting block blk_8093115169359854355
060421 155049 Pending transfer (block blk_-6965677235456960523) from node1383:50010 to 2 destinations
060421 155049 Block report from node1283:50010: 2140 blocks.
060421 155049 Redundant addStoredBlock request received for block blk_-6836937139917042917 on node node1143:50010

many DFS operations time out, making useful work impossible.

restarting dfs solved the problem for a while, but it came back within an hour.
"
HADOOP-158,"dfs should allocate a random blockid range to a file, then assign ids sequentially to blocks in the file","A random number generator is used to allocate block ids in dfs.  Sometimes a block id is allocated that is already used in the filesystem, which causes filesystem corruption.

A short-term fix for this is to simply check when allocating block ids whether any file is already using the newly allocated id, and, if it is, generate another one.  There can still be collisions in some rare conditions, but these are harder to fix and will wait, since this simple fix will handle the vast majority of collisions.
"
HADOOP-157,job fails because pendingCreates is not cleaned up after a task fails,"When a task fails under map/reduce, if the client doesn't abandon the files in progress (usually because it was killed), the lease on the name node lasts 1 minute. During that minute, I see 3 backup copies of the task fail because pendingCreates is non-null."
HADOOP-156,Reducer  threw IOEOFException,"A job was running with all the map tasks completed.
The reducers were appending the intermediate files into the large intermediate file.
java.io.EOFException was thrown when the record reader tried to read the version number
during initialization. Here is the stack trace:

java.io.EOFException 
    at java.io.DataInputStream.readFully(DataInputStream.java:178) 
    at java.io.DataInputStream.readFully(DataInputStream.java:152) 
    at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:251) 
    at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:236) 
    at org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:226) 
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:205) 
    at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:709) 

Appearantly, the intermediate file was empty. I suspect that one map task
generated empty intermidiate files for all the reducers, since all the reducers
failed at the same place, and failed at the same place during retries.

Unfortunately, we cannot know which map task generated the empty files,
since the exception does not offer any clue.

One simple enhancement is that the record reader should catch IOException and re-throw it with additional 
information, such as the file name.

"
HADOOP-155,Add a conf dir parameter to the scripts,"We'd like a conf_dir parameter on the startup scripts (ie. ""-c <confdif>""). In particular, it would be nice if it propagated down from hadoop-daemons.sh to slaves.sh to hadoop-daemon.sh using the command line rather than using the ssh -o SendEnv=HADOOP_CONF_DIR, which is not supported in many environments."
HADOOP-154,fsck fails when there is no file in dfs,"First type in command
    
     hadoop namenode -format

Then 

     hadoop fsck /

Then is the following exception:

Exception in thread ""main"" java.lang.ArithmeticException: / by zero
        at org.apache.hadoop.dfs.DFSck$Result.toString(DFSck.java:563)
        at java.lang.String.valueOf(String.java:2577)
        at java.io.PrintStream.print(PrintStream.java:616)
        at java.io.PrintStream.println(PrintStream.java:753)
        at org.apache.hadoop.dfs.DFSck.main(DFSck.java:435)

Possible Solution: Check whether totalBlocks equal to 0 in line 563 of DFSck.java"
HADOOP-153,skip records that fail Task,"MapReduce should skip records that throw exceptions.

If the exception is thrown under RecordReader.next() then RecordReader implementations should automatically skip to the start of a subsequent record.

Exceptions in map and reduce implementations can simply be logged, unless they happen under RecordWriter.write().  Cancelling partial output could be hard.  So such output errors will still result in task failure.

This behaviour should be optional, but enabled by default.  A count of errors per task and job should be maintained and displayed in the web ui.  Perhaps if some percentage of records (>50%?) result in exceptions then the task should fail.  This would stop jobs early that are misconfigured or have buggy code.

Thoughts?"
HADOOP-152,Speculative tasks not being scheduled,"The criteria for starting up a speculative task includes a check that the ""average progress""-""progress"" > the speculative gap, currently 0.2.

I don't know if this is the right metric, but it doesn't seem to be correctly calculated. I've regularly seen the ""average progress"" with values of less than 0.01, while the ""progress"" value is showing in the range .90-1.0, and yet, still no speculative tasks are started up. This has caused at least one long-running task to run about 10% longer while overloaded hosts catch up."
HADOOP-151,RPC code has socket leak?,"In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below.  The comment above the declaration was a bit of giveaway: 

   //TODO mb@media-style.com: static client or non-static client?
  private static Client CLIENT;	

  private static class Invoker implements InvocationHandler {
    private InetSocketAddress address;

    public Invoker(InetSocketAddress address, Configuration conf) {
      this.address = address;
      CLIENT = (Client) conf.getObject(Client.class.getName());
      if(CLIENT == null) {
          CLIENT = new Client(ObjectWritable.class, conf);
          conf.setObject(Client.class.getName(), CLIENT);
      }
    }

    public Object invoke(Object proxy, Method method, Object[] args)
      throws Throwable {
      ObjectWritable value = (ObjectWritable)
        CLIENT.call(new Invocation(method, args), address);
      return value.get();
    }
  }

  /** Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. */
  public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) {
    return Proxy.newProxyInstance(protocol.getClassLoader(),
                                  new Class[] { protocol },
                                  new Invoker(addr, conf));
  }

  /** Expert: Make multiple, parallel calls to a set of servers. */
  public static Object[] call(Method method, Object[][] params,
                              InetSocketAddress[] addrs, Configuration conf)
    throws IOException {

    Invocation[] invocations = new Invocation[params.length];
    for (int i = 0; i < params.length; i++)
      invocations[i] = new Invocation(method, params[i]);
    CLIENT = (Client) conf.getObject(Client.class.getName());
    if(CLIENT == null) {
        CLIENT = new Client(ObjectWritable.class, conf);
        conf.setObject(Client.class.getName(), CLIENT);
    }
    Writable[] wrappedValues = CLIENT.call(invocations, addrs);
    
    if (method.getReturnType() == Void.TYPE) {
      return null;
    }

    Object[] values =
      (Object[])Array.newInstance(method.getReturnType(),wrappedValues.length);
    for (int i = 0; i < values.length; i++)
      if (wrappedValues[i] != null)
        values[i] = ((ObjectWritable)wrappedValues[i]).get();
    
    return values;
  }. 
"
HADOOP-150,tip and task names should reflect the job name,"The tip and task names should be related to the job id. I'd propose:

job name: job_<random 32bit/base36>
tip: tip_<job id>_[mr]_<fragment #>
task: task_<tip id>_<attempt #>

so examples would be:
job_abc123
tip_abc123_m_00034
task_abc123_m_00034_1
 
"
HADOOP-149,TaskTracker#unJar trashes file modes,"Last Changed Rev: 395069

The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time.  This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves.

I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry.  I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it).  Perhaps support for jobs as tar(.gz) bundles?  (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions)."
HADOOP-148,add a failure count to task trackers,"Adds a count of failures that have occurred on each TaskTracker in the TaskTrackerStatus. The webapp displays the list of failures for each TaskTracker. In addition, the TaskTracker that has the most failures is listed."
HADOOP-147,MapTask removed mapout files before the reduce tasks copy them,"I was running a job on a cluster of 138 nodes. The job had 1050 map tasks and 128 reduce tasks. It stucked at the reduce stage.
All the reduce tasks were trying to copy file from a map task with the following status show on the web interface:

reduce > copy > task_m_ehz5q1@node1262.foo.com:60040

However, the log on the machine node1262 (where the map task task_m_ehz5q1 ran) showed that the map task finished even before the 
reduce tasks copied the map output files:

060417 103554 Server connection on port 60050 from 72.30.117.220: starting
060417 103554 task_m_ehz5q1  Client connection to 0.0.0.0:60050: starting
060417 103554 task_m_ehz5q1 1.0% /user/runping/runping/proj/part-00039:0+71
060417 103554 Task task_m_ehz5q1 is done.
060417 103554 parsing file:/local/hadoop/conf2/hadoop-default.xml

......................

060417 103613 parsing file:/local/hadoop/conf2/hadoop-site.xml
060417 103623 task_m_ehz5q1 done; removing files.
060417 103633 parsing file:/local/hadoop/conf2/hadoop-default.xml
060417 103633 parsing file:/local/hadoop/conf2/mapred-default.xml
060417 103633 parsing file:/local/hadoop/conf2/hadoop-site.xml

...........................................

060417 190241 SEVERE Can't open map output:/local/hadoop/mapred/local/task_m_ehz5q1/part-32.out
java.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out
        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:115)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)
        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:154)
        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)
        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:117)
        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)
060417 190241 Unknown child with bad map output: task_m_ehz5q1. Ignored.
060417 190241 Server handler 2 on 60040 caught: java.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out
java.io.FileNotFoundException: /local/hadoop/mapred/local/task_m_ehz5q1/part-32.out
        at org.apache.hadoop.fs.LocalFileSystem.openRaw(LocalFileSystem.java:115)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.<init>(FSDataInputStream.java:46)
        at org.apache.hadoop.fs.FSDataInputStream.<init>(FSDataInputStream.java:228)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:154)
        at org.apache.hadoop.mapred.MapOutputFile.write(MapOutputFile.java:116)
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:117)
        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:64)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:231)
060417 190241 parsing file:/local/hadoop/conf2/hadoop-default.xml
060417 190241 parsing file:/local/hadoop/conf2/mapred-default.xml

And the above exceptions repeated for many (not sure whether it is tru for all the reduce task) other reduce tasks.

Another strange thing noticed from the logs.

On another machine's log, I saw:

060417 190528 parsing file:/local/hadoop/conf2/hadoop-site.xml
060417 190528 task_r_24d8k4 copy failed: task_m_ehz5q1 from node1262.foo.com/72.30.117.220:60040
java.io.IOException: timed out waiting for response
        at org.apache.hadoop.ipc.Client.call(Client.java:305)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:141)
        at org.apache.hadoop.mapred.$Proxy2.getFile(Unknown Source)
        at org.apache.hadoop.mapred.ReduceTaskRunner.prepare(ReduceTaskRunner.java:110)
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:66)
060417 190528 task_r_24d8k4 0.11523809% reduce > copy > task_m_ehz5q1@node1262.foo.com:60040
060417 190528 task_r_24d8k4 Copying task_m_epatk8 output from node1387.foo.com.
                               
which is expected. However, before this line, 
I saw another copy activity in the log:

060417 103608 parsing file:/local/hadoop/conf2/hadoop-site.xml
060417 103608 task_r_a4yl3t Copying task_m_ehz5q1 output from node1262.foo.com.
060417 103608 parsing file:/local/hadoop/conf2/hadoop-default.xml

And the task task_r_a4yl3t does not belong to the concerned job, 
according to the Web interface. That is strange.

And I checked a few other machines where some reduce tasks ran, 
and I saw the same thing.

I suspect there was a conflict in job ID. If two jobs had the same ID, 
when one closes, it may also mark the other as ""closed"" too, thus trggering map tasks
to clean up prematurely.

A simple way to avoid potential jobid conflict is to use sequential numbers.



"
HADOOP-146,"potential conflict in block id's, leading to data corruption","currently, block id's are generated randomly, and are not tested for collisions with existing id's.
while ids are 64 bits, given enough time and a large enough FS, collisions are expected.
when a collision occurs, a random subset of blocks with that id will be removed as extra replicas, and the contents of that portion of the containing file are one random version of the block.
to solve this one could check for id collision when creating a new block, getting a new id in case of conflict. This approach requires the name node to keep track of all existing block id's (rather than just the ones who have reported in), and to identify old versions of a block id as in valid (in case a data node dies, a file is deleted, then a block id is reused for a new file).
Alternatively, one could simply use sequential block id's. Here the downsides are: 
1. migration from an existing file system is hard, requiring compaction of the entire FS
2. once you cycle through 64 bits of id's (quite a few years at full blast), you're in trouble again (or run occasional/background compaction)
3. you must never lose the high watermark block id.


synchronized Block allocateBlock(UTF8 src) {
        Block b = new Block();
        FileUnderConstruction v = (FileUnderConstruction) pendingCreates.get(src);
        v.add(b);
        pendingCreateBlocks.add(b);
        return b;
    }


static Random r = new Random();

    /**
     */
    public Block() {
        this.blkid = r.nextLong();
        this.len = 0;
    }"
HADOOP-144,the dfs client id isn't relatable to the map/reduce task ids,"From the dfs logs you can't tell which map/reduce tasks where involved, which makes debugging harder."
HADOOP-143,exception call stacks are word wrapped in webapp,"The exception call stacks in the webapp word wrap, which makes them much harder to read. It is particularly unfortunate in the remote exceptions, which use a blank line to separate the local from the remote call stacks."
HADOOP-142,failed tasks should be rescheduled on different hosts after other jobs,"Currently when tasks fail, they are usually rerun immediately on the same host. This causes problems in a couple of ways. 
  1.The task is more likely to fail on the same host. 
  2.If there is cleanup code (such as clearing pendingCreates) it does not always run immediately, leading to cascading failures.

For a first pass, I propose that when a task fails, we start the scan for new tasks to launch at the following task of the same type (within that job). So if maps[99] fails, when we are looking to assign new map tasks from this job, we scan like maps[100]...maps[N], maps[0]..,maps[99].

A more involved change would avoid running tasks on nodes where it has failed before. This is a little tricky, because you don't want to prevent re-excution of tasks on 1 node clusters and the job tracker needs to schedule one task tracker at a time."
HADOOP-141,Disk thrashing / task timeouts during map output copy phase,"
MapOutputProtocol connections cause timeouts because of system thrashing and transferring the same file over and over again, ultimately leading to making no forward progress(medium sized job, 500GB input file, map output about as large as the input, 10 node cluster).

There are several bugs behind this, but the following two changes improved matters considerably.

(1) 

The buffersize in MapOutputFile is currently hardcoded to 8192 bytes (for both reads and writes). By changing this buffer size to 256KB, the number of disk seeks are reduced and the problem went away. 

Ideally there would be a buffer size parameter for this that is separate from the DFS io buffer size.

(2)

I also added the following code to the socket configuration in both Server.java and Client.java. No linger is a minor good idea in an enivronment with some packet loss (and you will have that when all the nodes get busy at once), but 256KB buffers is probably excessive, especially on a LAN, but it takes me two hours to test changes so I havent experimented.

socket.setSendBufferSize(256*1024);
socket.setReceiveBufferSize(256*1024);
socket.setSoLinger(false, 0);
socket.setKeepAlive(true);
"
HADOOP-140,General documentation,"Getting a grasp of how Hadoop works is a little hard, because one first has to get a grip on the whole MapReduce thing and then figure out how it is carried out in Hadoop. Judging from the mailing list a little more general documentation would help a lot.

"
HADOOP-139,Deadlock in LocalFileSystem lock/release,"LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 
1. First thread locks the file and starts some long-running process.
2. Second thread tries to lock the file and it blocks inside channel lock method. It  keeps LocalFileSystem instance ""locked"" as well. 
3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is ""locked"" by second thread - both threads are waiting to each other. 
"
HADOOP-138,stop all tasks,"When a tasktracker runs X tasks it require X heartbeats to stop all jobs.
Stop all tasks with one heartbeat improve the availability and free resources faster.   "
HADOOP-137,"Different TaskTrackers may get the same task tracker id, thus cause many problems.","In the TaskTracker#Initialize method, the following line assigns task tracker name (id):

this.taskTrackerName = ""tracker_"" + (Math.abs(r.nextInt()) % 100000);

For a fair size cluster, it is possible that different task trackers to get the same id, causing name conflict.
I encountered this problem with a cluster of 274 nodes. Once such conflict happens, a lot of strange things may happen.
For example, a reducer task tried to copy from a machine (task tracker) a map output file that was actually produced 
on another machine.
"
HADOOP-136,Overlong UTF8's not handled well,"When we feed an overlong string to the UTF8 constructor, two suboptimal things happen.

First, we truncate to 0xffff/3 characters on the assumption that every character takes three bytes in UTF8.  This can truncate strings that don't need it, and it can be overoptimistic since there are characters that render as four bytes in UTF8.

Second, the code doesn't actually handle four-byte characters.

Third, there's a behavioral discontinuity.  If the string is ""discovered"" to be overlong by the arbitrary limit described above, we truncate with a log message, otherwise we signal a RuntimeException.  One feels that both forms of truncation should be treated alike.  However, this issue is concealed by the second issue; the exception will never be thrown because UTF8.utf8Length can't return more than three times the length of its input.

I would recommend changing UTF8.utf8Length to let its caller know how many characters of the input string will actually fit if there's an overflow [perhaps by returning the negative of that number] and doing the truncation accurately as needed.

-dk

"
HADOOP-135,Potential deadlock in JobTracker.,"org.apache.hadoop.mapred.JobTracker.RetireJobs.run()
locks resources in this order
                synchronized (jobs) {
                    synchronized (jobInitQueue) {
                        synchronized (jobsByArrival) {

org.apache.hadoop.mapred.JobTracker.submitJob()
locks resources in a different order
        synchronized (jobs) {
            synchronized (jobsByArrival) {
                synchronized (jobInitQueue) {

This potentially can lead to a deadlock.
Unless there is common lock on top of it in which case these
three locks are redundant."
HADOOP-134,JobTracker trapped in a loop if it fails to localize a task,"
The symptoms:

    When I ran  jobs on a big cluster, I noticed that some jobs got stucked. Some map tasks never got started. When I look at the log of the task tracker responsible for the tasks, I saw the following exceptions:

060413 160702 Lost connection to JobTracker [kry1040/72.30.116.100:50020].  Retrying...
java.io.IOException: No valid local directories in property: mapred.local.dir
        at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:282)
        at org.apache.hadoop.mapred.JobConf.getLocalFile(JobConf.java:127)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.localizeTask(TaskTracker.java:391)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.<init>(TaskTracker.java:383)
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:270)
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:336)
        at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:756)

The reason for the exception is that the directory hadoop/mapred/local has ""wrong"" owner, thus the task tracker cannot access to it.
This caused the task tracker stucked into the following loops:

            while (running) {
                boolean staleState = false;
                try {
                    // This while-loop attempts reconnects if we get network errors
                    while (running && ! staleState) {
                        try {
                            if (offerService() == STALE_STATE) {
                                staleState = true;
                            }
                        } catch (Exception ex) {
                            LOG.log(Level.INFO, ""Lost connection to JobTracker ["" + jobTrackAddr + ""].  Retrying..."", ex);
                            try {
                                Thread.sleep(5000);
                            } catch (InterruptedException ie) {
                            }
                        }
                    }
                } finally {
                    close();
                }
                LOG.info(""Reinitializing local state"");
                initialize();
            }

Issue 1:
    Method offerService() must catch and handle the exceptions that may be thrown from new TaskInProgress() call, and report back to the job tracker if it cannot run the task. This way, the task can be assigned to other task tracker.

Issue 2:
    The taskTracker should check whether it can access to the local dir at the initialization time, before taking any tasks.


Runping
"
HADOOP-133,the TaskTracker.Child.ping thread calls exit,"The TaskTracker.Child.startPinging thread calls exit if the TaskTracker doesn't respond. Calling exit in a mutli-threaded program is really problematic. In particular, it prevents cleanup/finally clauses from running. We need to move to a model where it uses Thread.interrupt(), which means we need to check the interrupt flag in place in the map loop and reduce loop and stop masking the InterruptExceptions."
HADOOP-132,An API for reporting performance metrics,"I'd like to propose adding an API for reporting performance metrics.  I will post some javadoc as soon as I figure out how to do so.  The idea is for the API to be sufficiently abstract that various different implementations can be plugged in.  In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia.  It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network.

I'd be very interested in people's thoughts about what the requirements should be for such an API.

- David Bowen
"
HADOOP-131,Separate start/stop-dfs.sh and start/stop-mapred.sh scripts,"Hadoop needs start-dfs/mapred.sh scripts, and stop-dfs/mapred.sh scripts, to independently start mapred, or independently start dfs. This way, users can use a single subsystem of the full Hadoop component library."
HADOOP-130,"Should be able to specify ""wide"" or ""full"" replication","Should be able to specify that a file be ""fully"" or ""widely"" replicated, rather than an explicit replication count. This would be useful for job configuration and jar files, and probably other files whose use is wide enough to necessitate reducing latency to access them.

The current implementation will also complain if you specify replication that is wider than the system's maximum replication value, and has no facility to enable ""full"" replication should the number of datanodes exceed the current maximum settable value of 32k.
"
HADOOP-129,FileSystem should not name files with java.io.File,"In Hadoop's FileSystem API, files are currently named using java.io.File.  This is confusing, as many methods on that class are inappropriate to call on Hadoop paths.  For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file.  Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names.  For example, new File(""/foo"") is not absolute on Windows, and prints its path as ""\\foo"", which causes confusion.

To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI.  The advantage of URI is that it can also naturally include the namenode host and port.  The disadvantage is that URI does not support tree operations like getParent().

This change will cause a lot of incompatibility.  Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it."
HADOOP-128,Failure to replicate dfs block kills client,"When the datanode gets an exception, which is logged as:

060407 155835 13 DataXCeiver
java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:380)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:462)
        at java.lang.Thread.run(Thread.java:595)

It closes the user's connection to the data node, which causes the client to get an IOException from:

        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:380)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:883)
 "
HADOOP-127,Unclear precedence of config files and property definitions,"The order in which configuration resources are read is not sufficiently documented, and also there are no mechanisms preventing harmful re-definition of certain properties, if they are put in wrong config files.

From reading the code in Hadoop Configuration.java, JobConf.java and Nutch NutchConfiguration.java I _think_ this is what's happening.

There are two groups of resources: default resources, loaded first, and final resources, loaded at the end. All properties (re)-defined in files loaded later will override any previous definitions:

* default resources: loaded in the order as they are added. The following files are added here, in order:

    1. hadoop-default.xml (Configuration)
    2. nutch-default.xml  (NutchConfiguration)
    3. mapred-default.xml (JobConf)
    4. job_xx_xxx.xml       (JobConf, in JobConf(File config))

* final resource: which always come after default resources, i.e. if any value is defined here it will always override those set in default resources (NOTE: including per job settings!!!). The following files are added here, in reversed order:

    2. hadoop-site.xml (Configuration)
    1. nutch-site.xml    (NutchConfiguration)

(i.e. hadoop-site.xml will take precedence over anything else defined in any other config file).

I would appreciate checking that this is indeed the case, and suggestions how to ensure that you cannot so easily shoot yourself in the foot if you define wrong properties in hadoop-site or nutch-site ..."
HADOOP-126,"""hadoop dfs -cp"" does not copy crc files","DFSShell.copy() uses FileUtil.copyContents() which works with ""Raw"" files, and does not copy crc files.
Try
hadoop dfs -cp a b
Then either
hadoop dfs -cat b
hadoop dfs -copyToLocal b c
The last two complain about crc files
In fact DFSShell.copy() should use the same FileSystem methods as copyToLocal and copyFromLocal do.
"
HADOOP-125,LocalFileSystem.makeAbsolute bug on Windows,"
LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop).

Problem:  if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails.

Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three:

(1) relative to current directory (subdir/file)
(2) relative to current disk (/dir/subdir/file)
(3) absolute (c:/dir/subdir/file)

So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. 

The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem:

    private File makeAbsolute(File f) {
      if (f.isAbsolute()) {
        return f;
      } else {
        return new File(workingDir, f.toString());
      }
    }

Im happy to explain if this explanation is confusing... "
HADOOP-124,don't permit two datanodes to run from same dfs.data.dir,"DFS files are still rotting.

I suspect that there's a problem with block accounting/detecting identical hosts in the namenode. I have 30 physical nodes, with various numbers of local disks, meaning that my current 'bin/hadoop dfs -report"" shows 80 nodes after a full restart. However, when I discovered the  problem (which resulted in losing about 500gb worth of temporary data because of missing blocks in some of the larger chunks) -report showed 96 nodes. I suspect somehow there were extra datanodes running against the same paths, and that the namenode was counting those as replicated instances, which then showed up over-replicated, and one of them was told to delete its local block, leading to the block actually getting lost.

I will debug it more the next time the situation arises. This is at least the 5th time I've had a large amount of file data ""rot"" in DFS since January."
HADOOP-123,mini map/reduce cluster for junit tests,I would like a single process map-reduce pseduo-distributed cluster that can be used for unit tests. We should add a unit test with a word count map/reduce.
HADOOP-120,Reading an ArrayWriter does not work because valueClass does not get initialized,"If you have a Reducer whose value type is an ArrayWriter it gets enstreamed alright but at reconstruction type when ArrayWriter::readFields(DataInput in) runs on a DataInput that has a nonempty ArrayWriter , newInstance fails trying to instantiate the null class."
HADOOP-119,ReduceTask.configure() is called twice,"ReduceTask.configure() is called twice for each created reduce task 
First call is done indirect from 
org.apache.hadoop.mapred.JobConf.newInstance()
called in ReduceTask.run(). Second call was in ReduceTask.run() just after creating a new instance. I suggest to remove the second call. For all new instances created in case of map task there are no directly xxx.configure();"
HADOOP-118,Namenode does not always clean up pendingCreates,"In some failure modes, the pending creates list is not cleaned up and prevents that file from ever being created.

When I try to create the file after the first job was killed (hours previously), I get:

060404 084619 Cannot start file because pendingCreates is non-null. src=/user/oom/rand/part000118
060404 084619 Server handler 0 on 8020 call error: java.io.IOException: Cannot create file /user/oom/rand/part000118 on client DFSClient_-1656137458
java.io.IOException: Cannot create file /user/oom/rand/part000118 on client DFSClient_-1656137458
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:147)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:237)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:216)
"
HADOOP-117,mapred temporary files not deleted,"I found out that JobConf.java

Created interchanged names with parent being file and file being parent directory

As a result files were not getting deleted"
HADOOP-116,cleaning up /tmp/hadoop/mapred/system,"Clean up directories

submit_ which contain job.xml and job.jar files as and when the job is finished"
HADOOP-115,permit reduce input types to differ from reduce output types,"When map tasks write intermediate data out, they always use SequencialFile RecordWriter with key/value classes from the job object.

When the reducers write the final results out, its output format is obtained from the job object. By default, it is TextOutputFormat, and no conflicts.
However, if one wants to use SequencialFileFormat for the final results, then the key/value classes are also obtained from the job object, the same as the map tasks' output. Now we have a problem. It is impossible for the map outputs and reducer outputs use different key/value classes, if one wants the reducers generate outputs in SequentialFileFormat.

A simple fix would be to add another two attributes to JobConf class: mapOutputLeyClass and mapOutputValueClass. That allows the user to have different key/value classes for the intermediate and final outputs.

"
HADOOP-114,Non-informative error message,"060330 105006 mapred.child.heap.size is deprecated. Use mapred.child.heap.size instead. Meantime, interpolated child.heap.size into child.java.opt: -Xmx200m

The instructions inform you to use the deprecated option."
HADOOP-113,Allow multiple Output Dirs to be specified for a job,"Allow a single job to create multiple outputs. 2 additional simple functions only

This allows for more complex branching of the process to occur either with multiple steps of the same type or allow different actions to take place on each output directory depending on the required actions.


For my specific use, it allows me to run multiple Generate Outputs instead of a single Generate Output as submitted in NUTCH-171(http://issues.apache.org/jira/browse/NUTCH-171)"
HADOOP-112,copyFromLocal should exclude .crc files,"Doug Cutting says: ""The problem is that when copyFromLocal 
enumerates local files it should exclude .crc files, but it does not. 
This is the listFiles() call on DistributedFileSystem:160.  It should 
filter this, excluding files that are FileSystem.isChecksumFile().

BTW, as a workaround, it is safe to first remove all of the .crc files, 
but your files will no longer be checksummed as they are read.  On 
systems without ECC memory file corruption is not uncommon, but I have 
seen very little on clusters that have ECC.""

Original observations:

Hello Team,

I created a backup of my DFS database:

# bin/hadoop dfs -copyToLocal /user/root/crawl /mylocaldir

I now want to restore from the backup using:

# bin/hadoop dfs -copyFromLocal /mylocaldir/crawl /user/root

However I'm getting the following error:

copyFromLocal: Target /user/root/crawl/crawldb/current/part-00000/.data.crc
already exists

I get this message with every permutation of the command that I've tried, and
even after totally deleting all content in the DFS directories.

I'd be grateful for any pointers.

Many thanks,



"
HADOOP-111,JobClient.runJob() should return exit status for a job.,"JobClient.runJob() doesn't return any values. Any information about the exit status of a job is discarded (or appears only in logs).

It should be possible to return the exit status, so that JobClient users can determine whether a job was successfully completed or not. This is also important when using cmd-line tools in shell scripts - currently they don't return any exit codes, because it's not possible to determine the outcome of a job submitted through JobClient. As a consequence, it's difficult to automate repetitive jobs using shells scripts.

It would be also nice to have an exit message in case of errors, for human consumption.

I propose to implement one of the following:

* change the return type of this method from void to int

* or, better yet, to put the exit code and optional exit messages inside the JobConf instance under pre-defined keys."
HADOOP-110,new key and value instances are allocated before each map,Each time map is called with a new key and value rather than reusing the old ones.
HADOOP-109,Blocks are not replicated when...,"When the block is under-replicated the namenode places it into
FSNamesystem.neededReplications list.
When a datanode D1 sends getBlockwork() request to the namenode, the namenode
selects another node D2 (which it thinks is up and running) where the new replica of the
under-replicated block will be stored.
Then namenode removes the block from the neededReplications list and places it to
the pendingReplications list, and then asks D1 to replicate the block to D2.
If D2 is in fact down, then replication will fail and will never be retried later, because
the block is not in the neededReplications list, but rather in the pendingReplications list,
which namenode never checks.
"
HADOOP-108,EOFException in DataNode$DataXceiver.run,"This morning - after upgrade of the system - something got wrong and we started to get lot of exceptions.
Situation didn't change after removing everything and creating new file system. 

Multiple exceptions on all data nodes:
060328 145922 108 DataXCeiver
java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:380)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:448)
        at java.lang.Thread.run(Thread.java:595)

No errors on the name node.

DFS clients report following:
060328 150923 task_r_2twzsl  Error while writing.
060328 150923 task_r_2twzsl java.net.SocketTimeoutException: Read timed out
060328 150923 task_r_2twzsl     at java.net.SocketInputStream.socketRead0(Native Method)
060328 150923 task_r_2twzsl     at java.net.SocketInputStream.read(SocketInputStream.java:129)
060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
060328 150923 task_r_2twzsl     at java.io.BufferedInputStream.read(BufferedInputStream.java:313)
060328 150923 task_r_2twzsl     at java.io.DataInputStream.readFully(DataInputStream.java:176)
060328 150923 task_r_2twzsl     at java.io.DataInputStream.readLong(DataInputStream.java:380)
060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:776)
060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:751)
060328 150923 task_r_2twzsl     at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:814)
060328 150923 task_r_2twzsl     at org.apache.hadoop.fs.FileSystem.createNewFile(FileSystem.java:202)

"
HADOOP-107,"Namenode errors ""Failed to complete filename.crc  because dir.getFile()==null and null""","We're getting lot of these errors and here is what I see in namenode log: 

060327 002016 Removing lease [Lease.  Holder: DFSClient_1897466025, heldlocks: 0, pendingcreates: 0], leases remaining: 1
060327 002523 Block report from member2.local:50010: 91895 blocks.
060327 003238 Block report from member1.local:50010: 91895 blocks.
060327 005830 Failed to complete /feedback/.feedback_10.1.10.102-33877.log.crc  because dir.getFile()==null and null
060327 005830 Server handler 1 on 50000 call error: java.io.IOException: Could not complete write to file /feedback/.feedback_10.1.10.102-33877.log.crc by DFSClient_1897466025
java.io.IOException: Could not complete write to file /feedback/.feedback_10.1.10.102-33877.log.crc by DFSClient_1897466025
        at org.apache.hadoop.dfs.NameNode.complete(NameNode.java:205)
        at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:237)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:216)

I can't be 100% sure, but it looks like these errors happen with checksum files for very small data files. 
"
HADOOP-106,Data blocks should be record-oriented.,"If data blocks were starting and ending on data record boundaries, and not in random places within a file, it would give some important advantages:

* it would be possible to avoid ""fishing"" for the beginning of first record in a split (see SequenceFile.Reader.sync()).

* it would make recovering from DFS errors much more successful and easier - in most cases missing blocks could be just skipped and the remaining parts combined together."
HADOOP-105,DFS commands either do not support some popular unix commands or have inconsistent behaviors,"Several issues.

1. DFS commandline does not support rmdir
2. When executing a command like dfs -rmdir PATH, it goes through silently, without warning that -rmdir is not supported
3. DFS -rm PATH works even though PATH is a non-empty directory, which is inconsistent with unix convention.

"
HADOOP-104,Reflexive access to non-public class with public ctor requires setAccessible (with some JVMs),"Multiple times I have hit this problem which prevents the NameNode from starting.
The only fix I had so far was to loose all my DFS data...

Exception in thread ""main"" java.lang.RuntimeException: java.lang.IllegalAccessException: 
Class org.apache.hadoop.io.WritableFactories can not access a member of 
class org.apache.hadoop.dfs.Block with modifiers ""public""
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)
        at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)
        at org.apache.hadoop.dfs.FSDirectory.loadFSEdits(FSDirectory.java:374)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:347)
        at org.apache.hadoop.dfs.FSDirectory.<init>(FSDirectory.java:258)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:151)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:82)


According to this, 
http://forum.java.sun.com/thread.jspa?threadID=704100&messageID=4082902
this is a known issue when using
reflexive access to a non-public class with public ctor 
(class org.apache.hadoop.dfs.Block is such a class)

This problem may not occur with all JVM releases. 
(I build on 1.5.0-b64 and run on 1.5.0_05-b05)

This problem only occured for me when 
I upgrade code or change XML configuration AND 
have existing files in the DFS.
This problem does not occur when I just stop / restart the NameServer.

In any case, the attached patch fixes it by calling setAccessible
before constructing the instance with reflection.


"
HADOOP-103,introduce a common parent class for Mapper and Reducer,"I'd like to a base class that implements the default (empty) bodies for configure and close so that Mapper and Reducer classes that derive from it, which is optional, don't have to implement those methods."
HADOOP-102,Two identical consecutive loops in FSNamesystem.chooseTarget(),"I think this was meant to be the way I corrected.
Otherwise there is no difference in two loops except for double
bracketing in the if statement."
HADOOP-101,DFSck - fsck-like utility for checking DFS volumes,"This is a utility to check health status of a DFS volume, and collect some additional statistics."
HADOOP-100,Inconsistent locking of the JobTracker.taskTrackers field,"The JobTracker is using an inconsistant lock for protecting taskTrackers, which is the list of current task trackers. Some of the routines lock the JobTracker and others lock the taskTrackers field."
HADOOP-99,task trackers can only be assigned one task every heartbeat,"Task trackers only call pollForNewTask once per a heartbeat (10 seconds) rather than when a task finishes. Especially for quick maps this means that the cluster is under utilized, because each map finishes in a cycle and the task tracker never gets a second, third or fourth task to run."
HADOOP-98,The JobTracker's count of the number of running maps and reduces is wrong,"When a heatbeat comes in from a task tracker, the job tracker just adds the number of currently running maps and reduces. The jobs from the previous heartbear are never subtracted. This causes the scheduling to misjudge the ""loading"" levels of the task trackers."
HADOOP-97,DFSShell.cat returns NullPointerException if file does not exist,"DFSShell.cat crashes with a NullPointerException if file to be displayed does not exist.
The bug is fixed in the attached patch, and the general exception handling in main() is added."
HADOOP-96,"name server should log decisions that affect data: block creation, removal, replication","currently, there's no way to analyze and debug DFS errors where blocks disapear.
name server should log its decisions that affect data, including block creation, removal, replication:
- block <b> created, assigned to datanodes A, B, ...
- datanode A dead, block <b> underreplicated(1), replicating to datanode C
- datanode B dead, block <b> underreplicated(2), replicating to datanode D
- datanode A alive, block <b> overreplicated, removing from datanode D
- block <removed> from datanodes C, D, ...

that will enable me to track down, two weeks later, a block that's missing from a file, and to debug the name server.

extra credit:
- rotate log file, as it might grow large
- make this behaviour optional/configurable"
HADOOP-95,dfs validation,"Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing.

Dfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong, we have a clue what has happened."
HADOOP-94,disallow more than one datanode running on one computing sharing the same data directory,"Currently dfs disallows more one datanode to run on the same computer if they are started up using the same hadoop conf dir. However, this does not prevent more than one data node gets started, each using a different conf dir (strickly speaking, a different pid file). If every machine has two such datanodes running, namenode will be busy on deleting and replicating blocks or eventually lead to block loss.

Suggested solution: put pid file in  the data directory and disallow configuration.

"
HADOOP-93,allow minimum split size configurable,"The current default split size is the size of a block (32M) and a SequenceFile sets it to be SequenceFile.SYNC_INTERVAL(2K). We currently have a Map/Reduce application working on crawled docuements. Its input data consists of 356 sequence files, each of which is of a size around 30G. A jobtracker takes forever to launch the job because it needs to generate 356*30G/2K map tasks!

The proposed solution is to let the minimum split size configurable so that the programmer can control the number of tasks to generate."
HADOOP-92,Error Reporting/logging in MapReduce,"Currently Mapreduce does not tell you which machine failed to execute the task. Also, it would be nice to have features wherein there is a log report with each job, saying the number of tasks it ran (reporting which one failed and on which machine, listing any error information it can)  with  the start/end/execute time of each task. "
HADOOP-90,DFS is succeptible to data loss in case of name node failure,"Currently, DFS name node stores its log and state in local files.
This has the disadvantage that a hardware failure of the name node causes a total data loss. 
Several approaches may be used to address this flaw:
1. replicate the name server state files using copy or rsync once in a while, either manually or using a cron job.
2. set up secondary name servers and a protocol whereby the primary updates the secondaries. In case of failure, a secondary can take over.
3. store the state files as distributed, replicated files in the DFS itself. The difficulty is that it becomes a bootstrap problem, where the name node needs some information, typically stored in its state files, in order to read those same state files.

solution 1 is fine for non critical systems, but for systems that need to guarantee no data loss it's insufficient.
Solutions 2 and 3 both seem valid; 3 seems more elegant in that it doesn't require an extra protocol, it leverages the DFS and allows any level of replication for robustness. Below is a proposition for  solution 3.

1.	The name node, when it starts up, needs some basic information. That information is not large and can easily be stored in a single block of DFS. We hard code the block location, using block id 0. Block 0 will contain the list of blocks that contain the name node metadata - not the metadata itself (file names, servers, blocks etc), just the list of blocks that contain it. With a block identified by 8 bytes, and 32 MB blocks, we can fit 256K block id's in block 0. 256K blocks of 32MB each can hold 8TB of metadata, which can map a large enough file system, so a single block of block_ids is sufficient.
2.	The name node writes his state basically the same way as now: log file plus occasional full state. DFS needs to change to commit changes to open files while allowing continued writing to them, or else the log file wouldn't be valid on name server failure, before the file is closed. 
3.	The name node will use double buffering for its state, using blocks 0 and 1. Starting with block 0, it writes its state, then a log of changes. When it's time to write a new state it writes it to node 1. The state includes a generation number, a single byte starting at 0, to enable the name server to identify the valid state. A CRC is written at the end of the block to mark its validity and completeness. The log file is identified by the same generation number as the state it relates to. 
4.	The log file will be limited to a single block as well. When that block fills up a new state is written. 32MB of transaction logs should suffice. If not, we could set aside a set of blocks, and set aside a few locations in the super-block (block 0/1) to store that set of block ids.
5.	The super-block, the log and the metadata blocks may be exposed as read only files in reserved files in the DFS: /.metadata/* or something.
6.	When a name nodes starts, it waits for data nodes to connect to it to report their blocks. It waits until it gets a report about blocks 0 and 1, from which it can continue to read its entire state. After that it continues normally.
"
HADOOP-89,files are not visible until they are closed,"the current behaviour, whereby a file is not visible until it is closed has several flaws,including:
1. no practical way to know if a file/job is progressing
2. no way to implement files that never close, such as log files
3. failure to close a file results in loss of the file

The part of the file that's written should be visible."
HADOOP-88,Configuration: separate client config from server config (and from other-server config),"servers = JobTracker, NameNode, TaskTracker, DataNode
clients =  runs JobClient (to submit MapReduce jobs), or runs DFSShell (to browse )

Server machines are administered together.
So it is OK to have all server config together (esp file paths and network ports).
This is stored in hadoop-default.xml or hadoop-mycluster.xml

Client machines:
there may be as many client machines as there are MapRed developers.
the temp space for DFS needs to be writable by the active user.
So it should be possible to select the client temp space directory for the machine and for the user.
(The global /tmp is not an option as discussed elsewhere: partition may be full)

Current situation: 
Both the server and the clients have a copy of the server config: hadoop-default.xml
But the XML property  ""dfs.data.dir"" is being used as a LOCAL directory path 
on both the server machines (Data nodes) and the client machines.

Effect:
Exception in thread ""main"" java.io.IOException: No valid local directories in property: dfs.data.dir
 at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:286)
 at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:560)
 ...
 at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:267)


Current Workaround:
On the client use hadoop-site.xml to override dfs.data.dir

One proposed solution:

For the purpose of JobClient operations, use a different property in place of dfs.data.dir.
(Ex: dfs.client.data.dir) 
On the client, set this property in hadoop-site.xml so that it will override hadoop-default.xml 

Another proposed solution:

Handle the fact that the world is made of a federation of independant Hadoop systems.
They can talk to each other (as peers) but they are administered separately.
Each Hadoop system should have its own separate XML config file.
Clients should be able to specify the Hadoop system they want to talk to.
An advantage is that clients can then easily sync their local copy of a given Hadoop system config:
 just pull its config file

In this view of the world, a Job client is also a kind of independant (serverless) Hadoop system
In this case the client config file may have its own dfs.data.dir, which is 
separate from the dfs.data.dir in the server config file.

"
HADOOP-87,SequenceFile performance degrades substantially compression is on and large values are encountered,"The code snippet in quesiton is:

     if (deflateValues) {
        deflateIn.reset();
        val.write(deflateIn);
        deflater.reset();
        deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength());
        deflater.finish();
        while (!deflater.finished()) {
          int count = deflater.deflate(deflateOut);
          buffer.write(deflateOut, 0, count);
        }
      } else {
  
A couple of issues with this code:

1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem.

2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'.


Proposed fix:

1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size.

2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, needs to shrink as well.


"
HADOOP-86,"If corrupted map outputs, reducers get stuck fetching forever","In our rack, there is a machine that reliably corrupts map output parts.  When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException.  Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside).  And so it goes till the cows come home.

Doug applied a patch that in map output  file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again).  This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts.  

A further patch that clears the severe flag was posted to the list.  This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs.  During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail).

This issue covers implementing a better solution.  

Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT.  A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates JT when corrupted map output."
HADOOP-85,a single client stuck in a loop blocks all clients on same machine,"I was running a set of clients, running cp from one dfs to another dfs using fuse, one file per cp process.
One client got stuck in a loop because of a bad block in one of the files (separate bug filed).
The other clients, running in parallel in the background, all stopped making progress on their respective files."
HADOOP-84,client should report file name in which IO exception occurs,"A file in the DFS got corrupted somehow.
The client gets an exception accessing a block in the file:

060315 105907 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105917 No node available for block blk_2690692619196463439

It could (and should) report the file in which the error occurred, together with the block information."
HADOOP-83,infinite retries accessing a missing block,"A file in the DFS got corrupted - the reason for that is unknown, but might be justified.

when accessing the file, I get an infinite stream of error messages from the client - attached below.
The client aparently increments an error counter, but never checks it.

Correct behaviour is for the client to retry a few times, then abort.


060315 105436 No node available for block blk_2690692619196463439
060315 105436 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105446 No node available for block blk_2690692619196463439
060315 105446 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105456 No node available for block blk_2690692619196463439
060315 105456 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105506 No node available for block blk_2690692619196463439
060315 105506 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105516 No node available for block blk_2690692619196463439
060315 105516 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105526 No node available for block blk_2690692619196463439
060315 105526 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105536 No node available for block blk_2690692619196463439
060315 105536 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105546 No node available for block blk_2690692619196463439
060315 105546 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105556 No node available for block blk_2690692619196463439
060315 105556 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105606 No node available for block blk_2690692619196463439
060315 105606 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105616 No node available for block blk_2690692619196463439
060315 105616 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105626 No node available for block blk_2690692619196463439
060315 105626 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105636 No node available for block blk_2690692619196463439
060315 105636 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105646 No node available for block blk_2690692619196463439
060315 105646 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105657 No node available for block blk_2690692619196463439
060315 105657 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105707 No node available for block blk_2690692619196463439
060315 105707 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105717 No node available for block blk_2690692619196463439
060315 105717 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105727 No node available for block blk_2690692619196463439
060315 105727 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105737 No node available for block blk_2690692619196463439
060315 105737 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105747 No node available for block blk_2690692619196463439
060315 105747 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105757 No node available for block blk_2690692619196463439
060315 105757 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105807 No node available for block blk_2690692619196463439
060315 105807 Could not obtain block from any node:  java.io.IOException: No live nodes contain current block
060315 105817 No node available for block blk_2690692619196463439
"
HADOOP-82,JobTracker loses it: NoSuchElementException,"On a number of occasions, JobTracker goes into a loop that it never recovers from.  Over and over it prints the below to the jobtracker log.

060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException
java.io.IOException: java.util.NoSuchElementException
   at java.util.TreeMap.key(TreeMap.java:433)
   at java.util.TreeMap.firstKey(TreeMap.java:287)
   at java.util.TreeSet.first(TreeSet.java:407)
   at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at
org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122)
at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98)
at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158)
at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at
java.io.BufferedInputStream.read(BufferedInputStream.java:235) at
org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210)
at java.io.DataInputStream.readInt(DataInputStream.java:353) at
org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at
org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557)
at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523)
at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at
org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666)

I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks.  Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it:

060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 
060314 204758 Task 'task_m_4d6ht0' has been lost.
060314 204811 Adding task 'task_m_fb0wf0' to tip tip_fizr7m, for tracker 'tracker_70065' on ia109314.archive.org
060314 210118 Task 'task_m_fb0wf0' has been lost.
060314 210119 Adding task 'task_m_irar47' to tip tip_fizr7m, for tracker 'tracker_82285' on ia109324.archive.org
060314 211541 Taskid 'task_m_irar47' has finished successfully.
060314 211541 Task 'task_m_irar47' has completed.
060314 211543 Adding task 'task_m_qo1g69' to tip tip_fizr7m, for tracker 'tracker_97839' on ia109306.archive.org
060314 213004 Taskid 'task_m_qo1g69' has finished successfully.
060314 213004 Task 'task_m_qo1g69' has completed.
060314 213005 Adding task 'task_m_t0lnzk' to tip tip_fizr7m, for tracker 'tracker_57273' on ia109314.archive.org
060314 214118 Task 'task_m_t0lnzk' has been lost.

So, we lose two, complete two, then lose a third.

TIP should have been done on first completion.

TIP accounting is off.


"
HADOOP-81,speculative execution is only controllable from the default config,The application's JobConf is not consulted when checking whether speculative execution should be used.
HADOOP-80,binary key,"I needed a binary key type, so I extended BytesWritable to be comparable also."
HADOOP-79,listFiles optimization,"In FSDirectory.getListing() looking at line
listing[i] = new DFSFileInfo(curName, cur.computeFileLength(), cur.computeContentsLength(), isDir(curName));

1. computeContentsLength() is actually calling computeFileLength(), so this is called twice,
meaning that file length is calculated twice.
2. isDir() is looking for the INode (starting from the rootDir) that has actually been obtained
just two lines above, note that the tree is locked by that time.

I propose a simple optimization for this, see attachment.

3. A related question: Why DFSFileInfo needs 2 separate fields len for file length and
contentsLen for directory contents size? It looks like these fields are mutually exclusive,
and we can use just one, interpreting it one way or another with respect to the value of isDir."
HADOOP-78,rpc commands not buffered,Calls using Hadoop's RPC framework get sent across the network byte by byte.
HADOOP-77,hang / crash when input folder does not exists.,"The jobtracker hangs and jobtracker info server throws Internal Server Error when until task initialization a exception will be thrown. 
Future jobs will no processed and also the job info server does not show any information since it throws a a http 500.

This is a show blocker especially when hadoop is shell script driven. 


060312 235707 TaskInProgress tip_6jd6g8 has failed 4 times.
060312 235707 Aborting job job_hsg7y8
060312 235708 Server connection on port 50020 from  XX.100.XXX.2: exiting
060312 235709 Server connection on port 50020 from  XX.100.XXX.2: starting
060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml
060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml
060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml
060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml
060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml
060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml
060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml
060312 235710 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml
060312 235710 parsing /u1/hadoop-data/tmp/hadoop/mapred/local/jobTracker/job_2p6ywq.xml
060312 235710 parsing file:/home/myuser/nutch/conf/hadoop-site.xml
060312 235711 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/hadoop-default.xml
060312 235711 parsing jar:file:/home/myuser/nutch/lib/hadoop-0.1-dev.jar!/mapred-default.xml
060312 235711 parsing /u1/hadoop-data/tmp/hadoop/mapred/local/jobTracker/job_2p6ywq.xml
060312 235711 parsing file:/home/myuser/nutch/conf/hadoop-site.xml
060312 235712 job init failed
java.io.IOException: Not a file: /user/myuser/segments/20060312214035/crawl_fetch/part-00001/data
        at org.apache.hadoop.mapred.InputFormatBase.getSplits(InputFormatBase.java:99)
        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:127)
        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:208)
        at java.lang.Thread.run(Thread.java:595)
Exception in thread ""Thread-20"" java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobInProgress.kill(JobInProgress.java:437)
        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:212)
        at java.lang.Thread.run(Thread.java:595)
060312 235713 Server connection on port 50020 from  XX.100.XXX.2: exiting

...


60312 235715 parsing file:/home/myuser/nutch/conf/hadoop-site.xml
060312 235755 /jobtracker.jsp: 
java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobInProgress.finishedMaps(JobInProgress.java:205)
        at org.apache.hadoop.mapred.jobtracker_jsp.generateJobTable(jobtracker_jsp.java:67)
        at org.apache.hadoop.mapred.jobtracker_jsp._jspService(jobtracker_jsp.java:130)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
060313 014526 /jobtracker.jsp: 
"
HADOOP-76,Implement speculative re-execution of reduces,"As a first step, reduce task outputs should go to temporary files which are renamed when the task completes.
"
HADOOP-75,dfs should check full file availability only at close,Currently it appears that dfs checks that the namenode knows about all blocks in a file as each block is written.  It would be more efficient to only check that all blocks are stored somewhere when the file is closed.
HADOOP-74,hash blocks into dfs.data.dirs,"When dfs.data.dir has multiple values, we currently start a DataNode for each (all in the same JVM).  Instead we should run a single DataNode that stores block files into the different directories.  This will reduce the number of connections to the namenode.  We cannot hash because different devices might be different amounts full.  So the datanode will need to keep a table mapping from block id to file location, and add new blocks to less full devices."
HADOOP-73,bin/hadoop dfs -rm works only for absolute paths,every dfs command like -du works with relative paths but remove only works only for absolute paths. 
HADOOP-72,hadoop doesn't take advatage of distributed compiting in TestDFSIO,"TestDFSIO runs N map jobs, each either writing to or reading from a separate file of the same size, 
and collects statistical information on its performance. 
The reducer further calculates the overall statistics for all maps. 
It outputs the following data:
- read or write test
- date and time the test finished   
- number of files
- total number of bytes processed
- overall throughput in mb/sec
- average IO rate in mb/sec per file

__Results__
I run 7 iterations of the test one after another on a cluster of ~200 nodes. 
The file size is the same in all cases 320Mb. 
The number of files tried is 1,2,4,8,16,32,64.
The log file with statistics is attached.
It looks like we don't have any distributed computing here at all.
The total execution time increases proportionally to the total size of data both for writes and reads.
Another thing is that the io ratio for read is higher than the write rate just gradually.
For comparison I attach time measuring for the same ios performed on the same cluster but sequentially in a simple loop.
This is the summary:

Files	map/red time	sequential time
 1		49			  34 
 2		86			  69
 4		158			131
 8		299			266
16		569			532
32		1131
64		2218

This doesn't look good, unless there is something wrong with my test (attached) or the cluster settings.
"
HADOOP-71,The SequenceFileRecordReader uses the default FileSystem rather than the supplied one,"The mapred.TestSequenceFileInputFormat test was failing when run with a conf directory that pointed to a dfs cluster. The reason was that SequenceFileRecordReader was using the default FileSystem from the config, while the test program was assuming the ""local"" file system was being used."
HADOOP-70,the two file system tests TestDFS and TestFileSystem take too long,"Running ""ant test"" takes hours and uses the conf directory, which forces it to run on the real cluster.

I propose that we split rename the test classes from

src/test/org/apache/hadoop/dfs/TestDFS.java to LongTestDFS.java
src/test/org/apache/hadoop/fs/TestFileSystem.java to LongTestFileSystem.java

and then we set up a new ant target ""long-test"" that runs all tests that match ""**/LongTest*.java""."
HADOOP-69,Unchecked lookup value causes NPE in FSNamesystemgetDatanodeHints,
HADOOP-68,"""Cannot abandon block during write to <file>"" and ""Cannot obtain additional block for file <file>"" errors during dfs write test","In the namenode's log file, when trying to run my writer benchmark, I get a bunch of messages like:

060307 112402 Server handler 2 on 9020 call error: java.io.IOException: Cannot a
bandon block during write to /user/oom/random/.part001389.crc
java.io.IOException: Cannot abandon block during write to /user/oom/random/.part
001389.crc
        at org.apache.hadoop.dfs.NameNode.abandonBlock(NameNode.java:188)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)

and 

060307 112402 Server handler 1 on 9020 call error: java.io.IOException: Cannot a
bandon block during write to /user/oom/random/part001695
java.io.IOException: Cannot abandon block during write to /user/oom/random/part0
01695
        at org.apache.hadoop.dfs.NameNode.abandonBlock(NameNode.java:188)
        at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)

and

060307 112402 Server handler 2 on 9020 call error: java.io.IOException: Cannot obtain additional block for file /user/oom/random/part001274
java.io.IOException: Cannot obtain additional block for file /user/oom/random/pa
rt001274
        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:160)
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$1.call(RPC.java:208)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:200)

"
HADOOP-67,Added statistic/reporting info to DFS,"The DatanodInfo, DFSFileInfo, and DFSClient were recently changed to package level protection, this hampers the ability to get some useful reporting data that can be used for DataNode and DFS health/performance."
HADOOP-66,dfs client writes all data for a chunk to /tmp,"The dfs client writes all the data for the current chunk to a file in /tmp, when the chunk is complete it is shipped out to the Datanodes. This can cause /tmp to fill up fast when a lot of files are being written. A potentially better scheme is to buffer the written data in RAM (application code can set the buffer size) and flush it to the Datanodes when the buffer fills up.
"
HADOOP-65,add a record I/O framework to hadoop,"Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier."
HADOOP-64,DataNode should be capable of managing multiple volumes,"The dfs Datanode can only store data on a single filesystem volume. When a node runs its disks JBOD this means running a Datanode per disk on the machine. While the scheme works reasonably well on small clusters, on larger installations (several 100 nodes) it implies a very large number of Datanodes with associated management overhead in the Namenode.

The Datanod should be enhanced to be able to handle multiple volumes on a single machine."
HADOOP-63,problem with webapp when start a jobtracker,"there exist two issues with starting up webapp

1. webapp is not be able to be loaded from a jar file.
2. web.xml can not be parsed properly using java 1.4"
HADOOP-62,can't get environment variables from HADOOP_CONF_DIR,The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.
HADOOP-60,Specification of alternate conf. directory,"Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf.  Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct.  Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed."
HADOOP-59,support generic command-line options,"Hadoop commands should all support some common options.  For example, it should be possible to specify the namenode, datanode, and, for that matter, any config option, in a generic way.

This could be implemented with code like:

public interface Tool extends Configurable {
  void run(String[] args) throws Exception;
}

public class ToolBase implements Tool extends Configured {
  public final void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    ... parse config options from args into conf ...
    this.configure(conf);
    this.run();
  }
}

public MyTool extends ExcecutableBase {
  public static void main(String[] args) throws Exception {
    new MyTool().main(args);
  }
}

The general command line syntax could be:

bin/hadoop [generalOptions] command [commandOptions]

Where generalOptions are things that ToolBase handles, and only the commandOptions are passed to Tool.run().  The most important generalOption would be '-D', which would define name/value pairs that are set in the configuration.  This alone would permit folks to set the namenode, datanode, etc."
HADOOP-58,Hadoop requires configuration of hadoop-site.xml or won't run,"On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh"" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost.  Currently this is not the case.  Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'.  It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties.

Revision: 379930

"
HADOOP-57,hadoop dfs -ls / does not show root of file system,"hadoop dfs -ls / does not show root of file system - it shows the user's home directory.
It's thus impossible to learn the contents of the root file system via the shell."
HADOOP-56,hadoop nameserver does not recognise ndfs nameserver image,"hadoop nameserver does not recognise ndfs image
Thus, upgrading from ndfs to hadoop dfs results in total data loss.
The upgrade should be seemless, with the new server recognising all previous version that are not end-of-life'd."
HADOOP-55,map-reduce job overhead is too high,"map-reduce job overhead is too high. Running a null map-reduce job, with no input, no intermediate data and no output, on a several-hundred node cluster, takes several minutes, rather than several seconds.
Enhanced distribution, launching and monitoring mechanisms should reduce the overhead to levels that match human online patience for short jobs."
HADOOP-54,"SequenceFile should compress blocks, not individual entries","SequenceFile will optionally compress individual values.  But both compression and performance would be much better if sequences of keys and values are compressed together.  Sync marks should only be placed between blocks.  This will require some changes to MapFile too, so that all file positions stored there are the positions of blocks, not entries within blocks.  Probably this can be accomplished by adding a getBlockStartPosition() method to SequenceFile.Writer."
HADOOP-53,MapReduce log files should be storable in dfs.,It should be possible to cause a job's log output to be stored in dfs.  The jobtracker's log output and (optionally) all tasktracker log output related to a job should be storable in a job-specified dfs directory.
HADOOP-52,mapred input and output dirs must be absolute,"DFS converts relative pathnames to be under /user/$USER.  But MapReduce jobs may be submitted by a different user than is running the jobtracker and tasktracker.  Thus relative paths must be resolved before a job is submitted, so that only absolute paths are seen on the job tracker and tasktracker.  I think the simplest way to fix this is to make JobConf.setInputDir(), setOutputDir(), etc. resolve relative pathnames. "
HADOOP-51,per-file replication counts,"It should be possible to specify different replication counts for different files.  Perhaps an option when creating a new file should be the desired replication count.  MapReduce should take advantage of this feature so that job.xml and job.jar files, which are frequently accessed by lots of machines, are more highly replicated than large data files."
HADOOP-50,dfs datanode should store blocks in multiple directories,"The datanode currently stores all file blocks in a single directory.  With 32MB blocks and terabyte filesystems, this will create too many files in a single directory for many filesystems.  Thus blocks should be stored in multiple directories, perhaps even a shallow hierarchy."
HADOOP-49,JobClient cannot use a non-default server (unlike DFSShell),"JobClient cannot use a non-default Job Tracker server:
It will use the Job Tracker specified in conf/hadoop-default.xml or conf/hadoop-site.xml

For users with multiple Hadoop systems, it is useful to be able to specify the Job Tracker.

Other hadoop command-line tools like DFSShell already have:
>bin/hadoop dfs
Usage: java DFSShell [-local | -dfs <namenode:port>]  ...

Similarly I propose to add a -jt parameter:
>bin/hadoop job
JobClient -submit <job> | -status <id> | -kill <id> [-jt <jobtracker:port>|<config>]

Where: -jt <jobtracker:port> is similar to -dfs <namenode:port>
And:  -jt <config> will load as a final resource: hadoop-<config>.xml

The latter syntax is discoverable by users because on failure the tool will say:

>bin/hadoop job -kill m7n6pi -jt unknown
Exception in thread ""main"" java.lang.RuntimeException: hadoop-unknown.xml not found on CLASSPATH

Or in case of success:

>bin/hadoop job -kill job_m7n6pi -jt myconfig
060221 221911 parsing file:/trunk/conf/hadoop-default.xml
060221 221911 parsing file:/trunk/conf/hadoop-myconfig.xml
060221 221911 parsing file:/trunk/conf/hadoop-site.xml
060221 221911 Client connection to 66.196.91.10:7020: starting

And with a machine:port spec:
>bin/hadoop job -kill job_m7n6pi -jt machine:8020
060221 222109 parsing file:/trunk/conf/hadoop-default.xml
060221 222109 parsing file:/trunk/conf/hadoop-site.xml
060221 222109 Client connection to 66.196.91.10:8020: starting


Patch attached.



"
HADOOP-48,add user data to task reports,"The Reporter interface should permit tasks to set user-data for a task, either as a String or, better-yet, as a Writable.  This should be returned with each TaskReport.  This would facilitate programmatic instrumentation of tasks."
HADOOP-47,include records/second and bytes/second in  task reports,"TaskReport should include fields for total records processed, total bytes processed and total seconds of task execution.  These should also be reported in the web ui, as bytes/second and records/second.  The job display could sum these to report total bytes/second and records/second for map and reduce.  Better yet would be a graph displaying the total rates over the course of the job..."
HADOOP-46,user-specified job names,It should be possible to supply a name when a job is submitted.  This can then be used in the web ui to describe the job.  Perhaps this should just be a job property (mapred.job.name).
HADOOP-45,JobTracker should log task errors,Task errors are currently propagated to the JobTracker and viewable in the web interface but they are not logged.  These should be logged as well.
HADOOP-44,RPC exceptions should include remote stack trace,"Remote exceptions currently only report the exception string.  Instead they should report the entire remote stack trace, as a string, to facilitate debugging."
HADOOP-43,JobTracker dumps TaskTrackers if it takes too long to service an RPC call,"If the JobTracker takes too long to service any RPC request, it is unable to receive emitHeartBeat calls from TaskTrackers. The monitoring thread thus dumps the TaskTracker, losing any incomplete work and forcing a slow reconnect process to begin before it starts assigning work again."
HADOOP-42,PositionCache decrements its position for reads at the end of file,"See

int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) 

if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained.

The attached patch would fix it."
HADOOP-41,JAVA_OPTS for the TaskRunner Child,"Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process.  Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes.  

Examples of why its useful being able to set options are the child include:

+ Being able to set '-server' option or '-c64'.
+ Passing logging.properties to configure child logging.
+ Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process.  Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps."
HADOOP-40,"bufferSize argument is ignored in FileSystem.create(File, boolean, int)","org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize)

ignores the input parameter bufferSize.
It passes further down the internal configuration, which includes the buffer size, but not the parameter value.
This works fine within the file system, since everything that calls create extracts buffer size from the same config. 
MapReduce although is probably affected by that, see 

org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done)

The attached patch would fix it."
HADOOP-39,Create a job-configurable best effort for job execution,"I propose having a job option that when a tip fails 4 times, stops trying to run that tip, but does not kill the job."
HADOOP-38,default splitter should incorporate fs block size,"By default, the file splitting code should operate as follows.

  inputs are <file>*, numMapTasks, minSplitSize, fsBlockSize
  output is <file,start,length>*

  totalSize = sum of all file sizes;

  desiredSplitSize = totalSize / numMapTasks;
  if (desiredSplitSize > fsBlockSize)             /* new */
    desiredSplitSize = fsBlockSize;
  if (desiredSplitSize < minSplitSize)
    desiredSplitSize = minSplitSize;

  chop input files into desiredSplitSize chunks & return them

In other words, the numMapTasks is a desired minimum.  We'll try to chop input into at least numMapTasks chunks, each ideally a single fs block.

If there's not enough input data to create numMapTasks tasks, each with an entire block, then we'll permit tasks whose input is smaller than a filesystem block, down to a minimum split size.

This handles cases where:
  - each input record takes a lot of time to process.  In this case we want to make sure we use all of the cluster.  Thus it is important to permit splits smaller than the fs block size.
  - input i/o dominates.  In this case we want to permit the placement of tasks on hosts where their data is local.  This is only possible if splits are fs block size or smaller.

Are there other common cases that this algorithm does not handle well?

The part marked 'new' above is not currently implemented, but I'd like to add it.

Does this sound reasonble?
"
HADOOP-37,A way to determine the size and overall activity of the cluster,"There is currently no way for an application to determine the size or activity of the cluster.

"
HADOOP-36,Adding some uniformity/convenience to environment management,"Currently, ""slaves"" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves

Perhaps split slaves, having a different set for ""datanodes"" vs. ""tasktracker"" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption.

Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable.

These changes would probably be useful to the nutch project, too."
HADOOP-35,Files missing chunks can cause mapred runs to get stuck,"I've now several times run into a problem where a large run gets stalled as a result of a missing data block. The latest was a stall in the Summer - ie, the data might've all been there, but it was impossible to proceed because the CRC file was missing a block. It would be nice to:

1) Have a ""health check"" running on a map reduce. If any data isn't available, emmit periodic warnings, and maybe have a timeout for if the data never comes back. Such warnings *should* specify which file(s) are affected by the missing blocks.
2) Have a utility, possible part of the existing dfs utility, which can check for dfs files with unlocatable blocks. Possibly, even show a 'health' of a file - ie, what percentage of its blocks are currently at the desired replication level. Currently, there's no way that I know of to find out if a file in DFS is going to be unreadable."
HADOOP-34,Build Paths Relative to PWD in build.xml,"In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear:

BUILD FAILED
/home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory

I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile.

I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory."
HADOOP-33,DF enhancement: performance and win XP support,"1. DF is called twice for each heartbeat, which happens each 3 seconds.
There is a simple fix for that in the attached patch.

2. cygwin is required to run df program in windows environment.
There is a class org.apache.commons.io.FileSystemUtils, which can return disk free space
for different OSs, but it does not have means to get disk capacity.
In general in windows there is no efficient and uniform way to calculate disk capacity
using a shell command.
The choices are 'chkdsk' and 'defrag -a', but both of them are too slow to be called
every 3 seconds.
WinXP and 2003 server have a new tool called fsutil, which provides all necessary info.
I implemented a call to fsutil in case df fails, and the OS is right.
Other win versions should still run cygwin.
I tested this fetaure for linux, winXP and cygwin.
See attached patch."
HADOOP-32,Creating job with InputDir set to non-existant directory locks up jobtracker,"This, in the very least, affects anything using the default listFiles() from InputFormatBase. If no files are enumerated, an exception is thrown... but the JobTracker keeps attempting to run listFiles() for this job. Trying to stop the job with hadoop job -kill job_name just results in timeouts, and further started jobs also don't progress. This happens every single time with, say, ""wordcount"", and a non-existent input path."
HADOOP-31,Stipulate main class in a job jar when using 'hadoop jar JARNAME',"One use case I forsee is building one jar but using this one jar running multiple jobs: E.g. A single nutch job jar would now be used to do indexing job, later same jar is used to do dedup, etc. Currently, the recently added hadoop 'jar' option just takes the jar name then looks in the jar MANIFEST.MF for the Main-Class, failing if not present. This is grand but for the scenario above, it means I have to create a jar per job I want to run -- each with a different MANIFEST.MF#Main-Class entry. 

Can we pass the Main-Class on the hadoop command-line as an (optional) argument to 'hadoop jar JAR_NAME'? (I can make a patch if wanted). "
HADOOP-30,DFS shell: support for ls -r and cat,patch attached
HADOOP-29,JobConf newInstance() method imposes a default constructor,"The Nutch parse command fails, because the ParseSegment class has no default constructor.
However, ParseSegment extends Configured, so it can be directly instanciated with a Configuration parameter.
"
HADOOP-28,webapps broken,"Changing the classes to private broke the webapps. 

The required public classes are:
org.apache.hadoop.mapred.JobInProgress
org.apache.hadoop.mapred.JobProfile
org.apache.hadoop.mapred.JobStatus
org.apache.hadoop.mapred.TaskTrackerStatus

To fix, we need one of:
  1. The classes need to be made public again
  2. The functionality needs to be made available through the classes that are public
  3. The webapps need to move into the mapred package."
HADOOP-27,MapRed tries to allocate tasks to nodes that have no available disk space,"
  What it says above.  MapRed TaskTrackers should not offer task service if the local disk
space is too constrained."
HADOOP-26,DFS node choice doesn't take available space into account effectively,"
  We used to have node-allocation be sensitive to available disk space.  It turned out that
this was a little accident-prone and hard to debug on then-available machines, so I removed it.

  DFS is mature enough now that this needs to come back and work properly.  A node with less
than X bytes should never accept a new block for allocation.
"
HADOOP-25,a new map/reduce example and moving the examples from src/java to src/examples,The new example is the word count example from Google's paper. I moved the examples into a separate jar file to demonstrate how to run stand-alone application code.
HADOOP-24,make Configuration an interface,"The Configuration class should become an interface, e.g.:

public interface Configuration {
  String get(String nam);
  String set(String name, String value);

  int getInt(String name);
  void setInt(String name, int value);
  float getFloat(String name);
  void setFloat(String name, float value);
  //... other utility methods based on get(String) and set(String,String) ...
}

An abstract class named ConfigurationBase should be implemented as follows:

public abstract class ConfigurationBase implements Configuration {
  abstract public String get(String nam);
  abstract public String set(String name, String value);

  public  int getInt(String name) { ... implementation in terms of get(String) ... }
  public void setInt(String name, int value) {... implementation in terms of set(String, String) ...}
  public float getFloat(String name)  { ... implementation in terms of get(String) ... }
  public void setFloat(String name, float value)  {... implementation in terms of set(String, String) ...}
  //... other utility methods based on get(String) and set(String,String) ...
}

A concrete, default implementation will be provided as follows:

public class ConfigurationImpl implements Writable extends ConfigurationBase {
  private Properties properties;

  // implement abstract methods from ConfigurationBase
  public String get(String name) { ... implemented in terms of props ...}
  public String set(String name, String value) { .. implemented in terms of props ... }

  // Writable methods
  public write(DataOutputStream out);
  public readFields(DataInputStream in);

  // permit chaining of configurations
  public Configuration getDefaults();
  public void setDefaults(Configuration defaults);
}

Only code which creates configurations should need to be updated, so this shouldn't be a huge change."
HADOOP-23,single node cluster gets one reducer,"Running on a single node cluster (it runs a job tracker and a single task tracker), even though my application asks for 7 reduces, it only gets one. I haven't tracked down what is happening yet."
HADOOP-22,remove unused imports,Following patch will remove unused imports from java source files
HADOOP-21,the webapps need to be updated for the move from nutch,The webapp files need to be updated so that the reference the new packages. I'll include a patch to fix it.
HADOOP-20,"Mapper, Reducer need an occasion to cleanup after the last record is processed.","Mapper, Reducer need an occasion to do some cleanup after the last record is processed.
Proposal (patch attached)
in interface Mapper:
 add method void finished();
in interface Reducer:
 add method void finished();

finished() methods are called from MapTask, CombiningCollector, ReduceTask.
------------
Known limitation: Fetcher (a multithreaded MapRunnable) does not call finished().
This is not currently a problem bec. fetcher Map/Reduce modules do not do anything in finished().
The right way to add finished() support to Fetcher would be to wait for all threads to finish, 
then do:
     if (collector instanceof CombiningCollector) ((CombiningCollector)collector).finished();
------------
patch begins: (svn trunk)

Index: src/test/org/apache/nutch/mapred/MapredLoadTest.java
===================================================================
--- src/test/org/apache/nutch/mapred/MapredLoadTest.java	(revision 374781)
+++ src/test/org/apache/nutch/mapred/MapredLoadTest.java	(working copy)
@@ -69,6 +69,8 @@
                 out.collect(new IntWritable(Math.abs(r.nextInt())), new IntWritable(randomVal));
             }
         }
+        public void finished() {
+        }
     }
     static class RandomGenReducer implements Reducer {
         public void configure(JobConf job) {
@@ -81,6 +83,8 @@
                 out.collect(new UTF8("""" + val), new UTF8(""""));
             }
         }
+        public void finished() {
+        }
     }
     static class RandomCheckMapper implements Mapper {
         public void configure(JobConf job) {
@@ -92,6 +96,8 @@
 
             out.collect(new IntWritable(Integer.parseInt(str.toString().trim())), new IntWritable(1));
         }
+        public void finished() {
+        }
     }
     static class RandomCheckReducer implements Reducer {
         public void configure(JobConf job) {
@@ -106,6 +112,8 @@
             }
             out.collect(new IntWritable(keyint), new IntWritable(count));
         }
+        public void finished() {
+        }
     }
 
     int range;
Index: src/test/org/apache/nutch/fs/TestNutchFileSystem.java
===================================================================
--- src/test/org/apache/nutch/fs/TestNutchFileSystem.java	(revision 374783)
+++ src/test/org/apache/nutch/fs/TestNutchFileSystem.java	(working copy)
@@ -155,6 +155,8 @@
 
       reporter.setStatus(""wrote "" + name);
     }
+    
+    public void finished() {}
   }
 
   public static void writeTest(NutchFileSystem fs, boolean fastCheck)
@@ -247,6 +249,9 @@
 
       reporter.setStatus(""read "" + name);
     }
+    
+    public void finished() {}
+    
   }
 
   public static void readTest(NutchFileSystem fs, boolean fastCheck)
@@ -339,6 +344,9 @@
         in.close();
       }
     }
+    
+    public void finished() {}
+    
   }
 
   public static void seekTest(NutchFileSystem fs, boolean fastCheck)
Index: src/java/org/apache/nutch/indexer/DeleteDuplicates.java
===================================================================
--- src/java/org/apache/nutch/indexer/DeleteDuplicates.java	(revision 374776)
+++ src/java/org/apache/nutch/indexer/DeleteDuplicates.java	(working copy)
@@ -225,6 +225,7 @@
         }
       }
     }
+    public void finished() {}
   }
     
   private NutchFileSystem fs;
@@ -265,6 +266,8 @@
       reader.close();
     }
   }
+  
+  public void finished() {}
 
   /** Write nothing. */
   public RecordWriter getRecordWriter(final NutchFileSystem fs,
Index: src/java/org/apache/nutch/indexer/Indexer.java
===================================================================
--- src/java/org/apache/nutch/indexer/Indexer.java	(revision 374778)
+++ src/java/org/apache/nutch/indexer/Indexer.java	(working copy)
@@ -227,6 +227,8 @@
 
     output.collect(key, new ObjectWritable(doc));
   }
+  
+  public void finished() {}
 
   public void index(File indexDir, File crawlDb, File linkDb, File[] segments)
     throws IOException {
Index: src/java/org/apache/nutch/segment/SegmentReader.java
===================================================================
--- src/java/org/apache/nutch/segment/SegmentReader.java	(revision 374778)
+++ src/java/org/apache/nutch/segment/SegmentReader.java	(working copy)
@@ -143,7 +143,9 @@
     }
     output.collect(key, new ObjectWritable(dump.toString()));
   }
-
+  
+  public void finished() {}
+  
   public void reader(File segment) throws IOException {
     LOG.info(""Reader: segment: "" + segment);
 
Index: src/java/org/apache/nutch/mapred/Mapper.java
===================================================================
--- src/java/org/apache/nutch/mapred/Mapper.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/Mapper.java	(working copy)
@@ -39,4 +39,9 @@
   void map(WritableComparable key, Writable value,
            OutputCollector output, Reporter reporter)
     throws IOException;
+
+  /** Called after the last {@link #map} call on this Mapper object.
+      Typical implementations do nothing.
+  */
+  void finished();
 }
Index: src/java/org/apache/nutch/mapred/lib/RegexMapper.java
===================================================================
--- src/java/org/apache/nutch/mapred/lib/RegexMapper.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/lib/RegexMapper.java	(working copy)
@@ -53,4 +53,5 @@
       output.collect(new UTF8(matcher.group(group)), new LongWritable(1));
     }
   }
+  public void finished() {}
 }
Index: src/java/org/apache/nutch/mapred/lib/InverseMapper.java
===================================================================
--- src/java/org/apache/nutch/mapred/lib/InverseMapper.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/lib/InverseMapper.java	(working copy)
@@ -38,4 +38,6 @@
     throws IOException {
     output.collect((WritableComparable)value, key);
   }
+
+  public void finished() {}
 }
Index: src/java/org/apache/nutch/mapred/lib/IdentityReducer.java
===================================================================
--- src/java/org/apache/nutch/mapred/lib/IdentityReducer.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/lib/IdentityReducer.java	(working copy)
@@ -42,4 +42,5 @@
     }
   }
 
+  public void finished() {}
 }
Index: src/java/org/apache/nutch/mapred/lib/IdentityMapper.java
===================================================================
--- src/java/org/apache/nutch/mapred/lib/IdentityMapper.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/lib/IdentityMapper.java	(working copy)
@@ -39,4 +39,5 @@
     output.collect(key, val);
   }
 
+  public void finished() {}
 }
Index: src/java/org/apache/nutch/mapred/lib/LongSumReducer.java
===================================================================
--- src/java/org/apache/nutch/mapred/lib/LongSumReducer.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/lib/LongSumReducer.java	(working copy)
@@ -47,4 +47,6 @@
     // output sum
     output.collect(key, new LongWritable(sum));
   }
+
+  public void finished() {}
 }
Index: src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java
===================================================================
--- src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java	(working copy)
@@ -50,4 +50,6 @@
       output.collect(new UTF8(st.nextToken()), new LongWritable(1));
     }
   }
+
+  public void finished() {}
 }
Index: src/java/org/apache/nutch/mapred/ReduceTask.java
===================================================================
--- src/java/org/apache/nutch/mapred/ReduceTask.java	(revision 374781)
+++ src/java/org/apache/nutch/mapred/ReduceTask.java	(working copy)
@@ -275,6 +275,7 @@
       }
 
     } finally {
+      reducer.finished();
       in.close();
       lfs.delete(new File(sortedFile));           // remove sorted
       out.close(reporter);
Index: src/java/org/apache/nutch/mapred/MapTask.java
===================================================================
--- src/java/org/apache/nutch/mapred/MapTask.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/MapTask.java	(working copy)
@@ -50,7 +50,7 @@
   public void write(DataOutput out) throws IOException {
     super.write(out);
     split.write(out);
-    
+
   }
   public void readFields(DataInput in) throws IOException {
     super.readFields(in);
@@ -126,6 +126,10 @@
         }
 
       } finally {
+        if (combining) {
+          ((CombiningCollector)collector).finished();
+        }
+
         in.close();                               // close input
       }
     } finally {
@@ -147,5 +151,5 @@
   public NutchConf getConf() {
     return this.nutchConf;
   }
-  
+
 }
Index: src/java/org/apache/nutch/mapred/MapRunner.java
===================================================================
--- src/java/org/apache/nutch/mapred/MapRunner.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/MapRunner.java	(working copy)
@@ -38,18 +38,22 @@
   public void run(RecordReader input, OutputCollector output,
                   Reporter reporter)
     throws IOException {
-    while (true) {
-      // allocate new key & value instances
-      WritableComparable key =
-        (WritableComparable)job.newInstance(inputKeyClass);
-      Writable value = (Writable)job.newInstance(inputValueClass);
+    try {
+      while (true) {
+        // allocate new key & value instances
+        WritableComparable key =
+          (WritableComparable)job.newInstance(inputKeyClass);
+        Writable value = (Writable)job.newInstance(inputValueClass);
 
-      // read next key & value
-      if (!input.next(key, value))
-        return;
+        // read next key & value
+        if (!input.next(key, value))
+          return;
 
-      // map pair to output
-      mapper.map(key, value, output, reporter);
+        // map pair to output
+        mapper.map(key, value, output, reporter);
+      }
+    } finally {
+        mapper.finished();
     }
   }
 
Index: src/java/org/apache/nutch/mapred/CombiningCollector.java
===================================================================
--- src/java/org/apache/nutch/mapred/CombiningCollector.java	(revision 374780)
+++ src/java/org/apache/nutch/mapred/CombiningCollector.java	(working copy)
@@ -78,4 +78,9 @@
     count = 0;
   }
 
+  public synchronized void finished()
+  {
+    combiner.finished();
+  }
+
 }
Index: src/java/org/apache/nutch/mapred/Reducer.java
===================================================================
--- src/java/org/apache/nutch/mapred/Reducer.java	(revision 374737)
+++ src/java/org/apache/nutch/mapred/Reducer.java	(working copy)
@@ -38,4 +38,10 @@
   void reduce(WritableComparable key, Iterator values,
               OutputCollector output, Reporter reporter)
     throws IOException;
+
+  /** Called after the last {@link #reduce} call on this Reducer object.
+      Typical implementations do nothing.
+  */
+  void finished();
+
 }
Index: src/java/org/apache/nutch/crawl/CrawlDbReader.java
===================================================================
--- src/java/org/apache/nutch/crawl/CrawlDbReader.java	(revision 374737)
+++ src/java/org/apache/nutch/crawl/CrawlDbReader.java	(working copy)
@@ -50,9 +50,9 @@
 
 /**
  * Read utility for the CrawlDB.
- * 
+ *
  * @author Andrzej Bialecki
- * 
+ *
  */
 public class CrawlDbReader {
 
@@ -68,6 +68,7 @@
       output.collect(new UTF8(""retry""), new LongWritable(cd.getRetriesSinceFetch()));
       output.collect(new UTF8(""score""), new LongWritable((long) (cd.getScore() * 1000.0)));
     }
+    public void finished() {}
   }
 
   public static class CrawlDbStatReducer implements Reducer {
@@ -121,6 +122,7 @@
         output.collect(new UTF8(""avg score""), new LongWritable(total / cnt));
       }
     }
+    public void finished() {}
   }
 
   public static class CrawlDbDumpReducer implements Reducer {
@@ -133,8 +135,11 @@
 
     public void configure(JobConf job) {
     }
+
+    public void finished() {
+    }
   }
-  
+
   public void processStatJob(String crawlDb, NutchConf config) throws IOException {
     LOG.info(""CrawlDb statistics start: "" + crawlDb);
     File tmpFolder = new File(crawlDb, ""stat_tmp"" + System.currentTimeMillis());
@@ -219,7 +224,7 @@
       System.out.println(""not found"");
     }
   }
-  
+
   public void processDumpJob(String crawlDb, String output, NutchConf config) throws IOException {
 
     LOG.info(""CrawlDb dump: starting"");
@@ -270,4 +275,5 @@
     }
     return;
   }
+
 }
Index: src/java/org/apache/nutch/crawl/LinkDb.java
===================================================================
--- src/java/org/apache/nutch/crawl/LinkDb.java	(revision 374779)
+++ src/java/org/apache/nutch/crawl/LinkDb.java	(working copy)
@@ -118,7 +118,8 @@
     output.collect(key, result);
   }
 
-
+  public void finished() {}
+	
   public void invert(File linkDb, File segmentsDir) throws IOException {
     LOG.info(""LinkDb: starting"");
     LOG.info(""LinkDb: linkdb: "" + linkDb);
Index: src/java/org/apache/nutch/crawl/Injector.java
===================================================================
--- src/java/org/apache/nutch/crawl/Injector.java	(revision 374779)
+++ src/java/org/apache/nutch/crawl/Injector.java	(working copy)
@@ -65,6 +65,8 @@
                                              interval));
       }
     }
+    
+    public void finished() {}
   }
 
   /** Combine multiple new entries for a url. */
@@ -76,6 +78,7 @@
       throws IOException {
       output.collect(key, (Writable)values.next()); // just collect first value
     }
+    public void finished() {}
   }
 
   /** Construct an Injector. */
Index: src/java/org/apache/nutch/crawl/Generator.java
===================================================================
--- src/java/org/apache/nutch/crawl/Generator.java	(revision 374779)
+++ src/java/org/apache/nutch/crawl/Generator.java	(working copy)
@@ -63,6 +63,8 @@
       output.collect(crawlDatum, key);          // invert for sort by score
     }
 
+    public void finished() {}
+    
     /** Partition by host (value). */
     public int getPartition(WritableComparable key, Writable value,
                             int numReduceTasks) {
Index: src/java/org/apache/nutch/crawl/CrawlDbReducer.java
===================================================================
--- src/java/org/apache/nutch/crawl/CrawlDbReducer.java	(revision 374781)
+++ src/java/org/apache/nutch/crawl/CrawlDbReducer.java	(working copy)
@@ -115,4 +115,5 @@
     }
   }
 
+  public void finished() {}
 }
Index: src/java/org/apache/nutch/parse/ParseSegment.java
===================================================================
--- src/java/org/apache/nutch/parse/ParseSegment.java	(revision 374776)
+++ src/java/org/apache/nutch/parse/ParseSegment.java	(working copy)
@@ -78,6 +78,8 @@
     throws IOException {
     output.collect(key, (Writable)values.next()); // collect first value
   }
+  
+  public void finished() {}
 
   public void parse(File segment) throws IOException {
     LOG.info(""Parse: starting"");





"
HADOOP-19,Datanode corruption,"Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.

This happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.

The ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.


I think the solution is improved interlocking within the data directory itself (file locked with flock or something similar)."
HADOOP-18,Crash with multiple temp directories,"A brief read of the code indicated it may be possible to use multiple local directories using something like the below:

  <property>
    <name>mapred.local.dir</name>
    <value>/local,/local1,/local2</value>
    <description>The local directory where MapReduce stores intermediate
    data files.
    </description>
  </property>

This failed with the below exception during either the generate or update phase (not entirely sure which).

java.lang.ArrayIndexOutOfBoundsException
        at java.util.zip.CRC32.update(CRC32.java:51)
        at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)
        at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)
        at java.io.DataInputStream.readFully(DataInputStream.java:176)
        at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)
        at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)
        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)
        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)
        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)
        at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)
        at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)
        at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)
        at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)
        at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)
        at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:604)"
HADOOP-17,tool to mount ndfs on linux,tool to mount ndfs on linux. It depends on fuse and fuse-j.
HADOOP-16,RPC call times out while indexing map task is computing splits,"We've been using Nutch 0.8 (MapReduce) to perform some internet crawling. Things seemed to be going well until...

060129 222409 Lost tracker 'tracker_56288'
060129 222409 Task 'task_m_10gs5f' has been lost.
060129 222409 Task 'task_m_10qhzr' has been lost.
   ........
   ........
060129 222409 Task 'task_r_zggbwu' has been lost.
060129 222409 Task 'task_r_zh8dao' has been lost.
060129 222455 Server handler 8 on 8010 caught: java.net.SocketException: Socket closed
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.nutch.ipc.Server$Handler.run(Server.java:216)
060129 222455 Adding task 'task_m_cia5po' to set for tracker 'tracker_56288'
060129 223711 Adding task 'task_m_ffv59i' to set for tracker 'tracker_25647'

I'm hoping that someone could explain why task_m_cia5po got added to tracker_56288 after this tracker was lost.

The Crawl .main process died with the following output:

060129 221129 Indexer: adding segment: /user/crawler/crawl-20060129091444/segments/20060129200246
Exception in thread ""main"" java.io.IOException: timed out waiting for response
    at org.apache.nutch.ipc.Client.call(Client.java:296)
    at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)
    at $Proxy1.submitJob(Unknown Source)
    at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)
    at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)
    at org.apache.nutch.indexer.Indexer.index(Indexer.java:263)
    at org.apache.nutch.crawl.Crawl.main(Crawl.java:127)

However, it definitely seems as if the JobTracker is still waiting for the job to finish (no failed jobs).

Doug Cutting's response:
The bug here is that the RPC call times out while the map task is computing splits.  The fix is that the job tracker should not compute splits until after it has returned from the submitJob RPC.  Please submit a bug in Jira to help remind us to fix this.
"
HADOOP-12,InputFormat used in job must be in JobTracker classpath (not loaded from job JAR),"During development, I've been creating/tweaking custom InputFormat implementations. However, when you try to run a job against a running cluster, you get:
  Exception in thread ""main"" java.io.IOException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: my.custom.InputFormat
          at org.apache.nutch.ipc.Client.call(Client.java:294)
          at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)
          at $Proxy0.submitJob(Unknown Source)
          at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)
          at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)
          at com.parc.uir.wikipedia.WikipediaJob.main(WikipediaJob.java:85)

This error goes away if I restart the TaskTrackers/JobTracker with a classpath which includes the needed code. Other classes (Mapper, Reducer) appear to be available out of the jar file specified in the JobConf, but not the InputFormat. Obviously, it's less than idea to have to restart the JobTracker whenever there's a change to a job-specific class."
HADOOP-10,ndfs.replication is not documented within the nutch-default.xml configuration file.,ndfs.replication is not documented within the nutch-default.xml configuration file.
HADOOP-9,mapred.local.dir  temp dir. space allocation limited by smallest area,"When mapred.local.dir is used to specify multiple  temp dir. areas, space allocation limited by smallest area because the temp dir. selection algorithm is ""round robin starting from a randomish point"".   When round robin is used with approximately constant sized chunks, the smallest area runs out of space first, and this is a fatal error. 

Workaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.

I wrote a patch to JobConf (currenly being tested) which uses df to check available space (once a minute or less often) and then uses an efficient roulette selection to do allocation weighted by magnitude of available space. 

"
HADOOP-8,NDFS DataNode advertises localhost as it's address,
HADOOP-7,MapReduce has a series of problems concerning task-allocation to worker nodes,"The MapReduce JobTracker is not great at allocating tasks to TaskTracker worker nodes.

Here are the problems:
1) There is no speculative execution of tasks
2) Reduce tasks must wait until all map tasks are completed before doing any work
3) TaskTrackers don't distinguish between Map and Reduce jobs.  Also, the number of
tasks at a single node is limited to some constant.  That means you can get weird deadlock
problems upon machine failure.  The reduces take up all the available execution slots, but they
don't do productive work, because they're waiting for a map task to complete.  Of course, that
map task won't even be started until the reduce tasks finish, so you can see the problem...
4) The JobTracker is so complicated that it's hard to fix any of these.


The right solution is a rewrite of the JobTracker to be a lot more flexible in task handling.
It has to be a lot simpler.  One way to make it simpler is to add an abstraction I'll call
""TaskInProgress"".  Jobs are broken into chunks called TasksInProgress.  All the TaskInProgress
objects must be complete, somehow, before the Job is complete.

A single TaskInProgress can be executed by one or more Tasks.  TaskTrackers are assigned Tasks.
If a Task fails, we report it back to the JobTracker, where the TaskInProgress lives.  The TIP can then
decide whether to launch additional  Tasks or not.

Speculative execution is handled within the TIP.  It simply launches multiple Tasks in parallel.  The
TaskTrackers have no idea that these Tasks are actually doing the same chunk of work.  The TIP
is complete when any one of its Tasks are complete.

"
HADOOP-6,missing build directory in classpath,"When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory."
HADOOP-5,need commons-logging-api jar file,The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.
HADOOP-4,tool to mount dfs on linux,"This is a FUSE module for Hadoop's HDFS.

It allows one to mount HDFS as a Unix filesystem and optionally export
that mount point to other machines.

rmdir, mv, mkdir, rm are all supported. just not cp, touch, ..., but actual writes require: https://issues.apache.org/jira/browse/HADOOP-3485

For the most up-to-date documentation, see: http://wiki.apache.org/hadoop/MountableHDFS

BUILDING:


Requirements:

   1. a Linux kernel > 2.6.9 or a kernel module from FUSE - i.e., you
   compile it yourself and then modprobe it. Better off with the
   former option if possible.  (Note for now if you use the kernel
   with fuse included, it doesn't allow you to export this through NFS
   so be warned. See the FUSE email list for more about this.)

   2. FUSE should be installed in /usr/local or FUSE_HOME ant
   environment variable

To build:

   1. in HADOOP_HOME: ant compile-contrib -Dcompile.c++=1 -Dfusedfs=1 -Dlibhdfs=1


NOTE: for amd64 architecture, libhdfs will not compile unless you edit
the Makefile in src/c++/libhdfs/Makefile and set OS_ARCH=amd64
(probably the same for others too).

--------------------------------------------------------------------------------

CONFIGURING:

Look at all the paths in fuse_dfs_wrapper.sh and either correct them
or set them in your environment before running. (note for automount
and mount as root, you probably cannnot control the environment, so
best to set them in the wrapper)

INSTALLING:

1. mkdir /mnt/dfs (or wherever you want to mount it)

2. fuse_dfs_wrapper.sh dfs://hadoop_server1.foo.com:9000 /mnt/dfs -d
; and from another terminal, try ls /mnt/dfs

If 2 works, try again dropping the debug mode, i.e., -d

(note - common problems are that you don't have libhdfs.so or
libjvm.so or libfuse.so on your LD_LIBRARY_PATH, and your CLASSPATH
does not contain hadoop and other required jars.)

--------------------------------------------------------------------------------


DEPLOYING:

in a root shell do the following:

1. add the following to /etc/fstab -
  fuse_dfs#dfs://hadoop_server.foo.com:9000 /mnt/dfs fuse
  allow_other,rw 0 0

2. mount /mnt/dfs Expect problems with not finding fuse_dfs. You will
   need to probably add this to /sbin and then problems finding the
   above 3 libraries. Add these using ldconfig.

--------------------------------------------------------------------------------

EXPORTING:

Add the following to /etc/exports:

  /mnt/hdfs *.foo.com(no_root_squash,rw,fsid=1,sync)

NOTE - you cannot export this with a FUSE module built into the kernel
- e.g., kernel 2.6.17. For info on this, refer to the FUSE wiki.
--------------------------------------------------------------------------------

ADVANCED:

you may want to ensure certain directories cannot be deleted from the
shell until the FS has permissions. You can set this in the build.xml
file in src/contrib/fuse-dfs/build.xml

"
HADOOP-3,Output directories are not cleaned up before the reduces run,"The output directory for the reduces is not cleaned up and therefore if you can see left overs from previous runs, if they had more reduces. For example, if you run the application once with reduces=10 and then rerun with reduces=8, your output directory will have frag00000 to frag00009 with the first 8 fragments from the second run and the last 2 fragments from the first run."
HADOOP-2,Reused Keys and Values fail with a Combiner,"If the map function reuses the key or value by destructively modifying it after the output.collect(key,value) call and your application uses a combiner, the data is corrupted by having lots of instances with the last key or value."
HADOOP-1,initial import of code from Nutch,The initial code for Hadoop will be copied from Nutch.
