Bug_ID,Bug_Summary,Bug_Description
STORM-4131,Update log4j2 to a non-borked version,"Log4j2 2.24.1 has a serious issue that was fixed in 2.24.2: https://github.com/apache/logging-log4j2/issues/3143

We should upgrade.

"
STORM-4130,commons cli 1.9.0,
STORM-4129,Hikari 6.2.0,
STORM-4128,"Deprecate ""storm-hive""","as discussed on the list

 

Hive support will be dropped in the near future (most likely 2.8.0)"
STORM-4127,Netty 4.1.115.Final ,
STORM-4126,Kryo 5.6.2,
STORM-4125,clojure.logging.tools 1.3.0,
STORM-4124,Bouncycastle 1.79,
STORM-4123,Hikari 6.1.0,
STORM-4122,commons-exec 1.4.0,
STORM-4121,commons-csv:1.12.0,
STORM-4120,gson 2.11.0,
STORM-4119,commons-codec 1.17.1,
STORM-4118,commons-text 1.12.0,
STORM-4117,Snappy 1.1.10.7,
STORM-4115,Hive 4.0.1,"The Apache Hive Community has voted to declare the 3.x release line as End of Life (EOL). This means no further updates or releases will be made for this series.
We urge all Hive 3.x users to upgrade to the latest versions promptly to benefit from new features and ongoing support."
STORM-4114,jline 2.14.6,
STORM-4113,commons-lang3 3.17.0,
STORM-4112,Jetty 11.0.24,
STORM-4111,Zookeeper 3.9.3,
STORM-4110,Jedis 5.2.0,
STORM-4109,Netty TCNative 2.0.69.Final,
STORM-4108,Remove joda-time ,
STORM-4107,Remove ElasticSearch Module,The ES license is not compatible (flagged as Category X) and we should get rid of it. Users can still get the code by forking a previous version of storm (i.e. copy pasting). 
STORM-4106,Storm.ps1 does not show stdout\stderr output when launched from Powershell ISE,"In our product, we orchestrate Storm deployment and topology management via Powershell scripts hosted in Powershell ISE in Windows.

In this environment, the execution of Storm.ps1 does not display stdout\stderr in the Powershell ISE console output window in the same way that a regular Powershell window does.

While there [are more cumbersome ways|https://stackoverflow.com/questions/8761888/capturing-standard-out-and-error-with-start-process] of getting it to do this, there is a much simpler and more elegant way to fix this while still maintaining the desired behaviour from the changes in STORM-3847

 

I will have a pull request to address this shortly."
STORM-4105,clojure 1.12.0,
STORM-4104,Pacemaker server stability issues - e.g. shuts down when topology killed,"StormServerHandler used by Pacemaker Server (and by the Netty Server in each Worker) is fragile when handling certain Exceptions derived from IOException.

In Storm1 the same handler would ignore Exceptions and only terminate for serious JVM exceptions such as OutOfMemory.

The same in Storm2 does something similar but, instead of ignoring all 'regular' Exceptions, has a set of ALLOWED_EXCEPTIONS which can be ignored but this currently contains just IOException.

The code, as it currently stands, will only ignore specifically IOException. All other exceptions will cause the runtime to terminate after logging {color:#172b4d}""Received error in netty thread.. terminating server...""{color}

{color:#172b4d}When a connection from a worker to the Pacemaker Server terminates - either expected (e.g. killing a topology) or unexpected (e.g. node in cluster rebooting) - a SocketException is likely to be seen by Pacemaker Server. This will cause it to terminate.{color}

{color:#172b4d}Now, as SocketException is derived from IOException, I would say a more robust way for Pacemaker Server to handle this and achieve similar stability seen with Storm1 is to not only 'swallow' IOExceptions but any exception derived from IOException too (which will of course include SocketException).{color}

{color:#172b4d}Modifying the handleUncaughtException to make use of Utils.exceptionCauseIsInstanceOf would greatly enhance the stability of Pacemaker and, as StormServerHandler is used in the Worker's Netty Server, the Workers would also have greater stability from networking exceptions (e.g. a Worker receiving a transfer from a remote where the remote reboots should no longer cause the receiving Worker to restart - we do sometimes see a cascade of worker restarts under such scenarios)
{color}

{color:#172b4d}I have modified a build with such a change and can indeed see greater stability from Pacemaker Server.{color}

{color:#172b4d}I will have a pull request for the changes I have made linked to this issue soon.{color}"
STORM-4103,Guava 33.3.1-jre,
STORM-4102,hbase 2.6.1-hadoop3,
STORM-4101,Dropwizard 4.0.10,
STORM-4100,Rocksdb 9.7.3,
STORM-4099,Kafka Client 3.9.0,
STORM-4098,Prometheus Client 1.3.3,
STORM-4097,Avro 1.12.0,
STORM-4096,Metrics 4.2.28,
STORM-4095,Hadoop 3.4.1,
STORM-4094,JUnit 5.11.3,
STORM-4093,log4j2 2.24.1,
STORM-4092,Prometheus Client 1.3.2,
STORM-4091,commons-io 2.17.0,
STORM-4090,Kafka Client 3.8.1,
STORM-4089,Jersey 3.1.9,
STORM-4088,Curator 5.7.1,
STORM-4087,ASM 9.7.1,
STORM-4086,Bump com.fasterxml.jackson:jackson-bom from 2.17.2 to 2.18.1,"Jackson 2.17.3 and 2.18.1 have bug fixes.
* https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.17.3
* https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.18
* https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.18.1"
STORM-4085,Supervisor Threads being Killed,"The AsyncLocalizer class contains a scheduler that continuously performs downloadOrUpdate operations on blobs. Currently, if an exception occurs during this update process, the exception is not properly handled, causing the thread to terminate.
As a result, workers are unable to update blobs because the threads have been killed. The only way to recover from this is by restarting the supervisor."
STORM-4084,Commons Compress 1.27.1,
STORM-4083,Jetty 11.0.23,
STORM-4082,Prometheus Client 1.3.1,
STORM-4081,Dropwizard Metrics 4.2.27,
STORM-4077,Worker being reassigned when Nimbus leadership changes,"Hey guys, I'm using Storm v2.6.1 and every time I restart the nimbus leader (currently I have 3 for high availability) the workers get reassigned and this is a bad behaviour as every topology will have no workers running for a certain period(until new workers are assigned) due to a Nimbus leadership change.

Update:

Essentially, by using the modTime as the version, we have found that, while using the {{{}LocalFsBlobStoreFile{}}}, everytime the the Nimbus leader goes down the following occurs:
 # Nimbus (1) leader goes down and a new Nimbus (2) picks up the leadership.
 # If blobs in Nimbus (2) have a different modTime workers are restarted (even though they might be the same).
 # Nimbus (1) comes back up, syncs the blobs in the startup and updates the modTime, as it downloads the blobs again.
 # If Nimbus (2) leader goes down, all the workers will be restarted again as Nimbus (1) has new modTime again.
 # This can be repeated endless as the modTime will always be different in each Nimbus leader.

We suggest a new method that obtains the file version:
{code:java}
public abstract class BlobStoreFile {
    public abstract long getModTime() throws IOException;

    public long getVersion() throws IOException {
        return getModTime();
    }
}{code}
And defaults to the current approach if not implemented and the version of the file would be something in the lines:
{code:java}
public long getVersion() throws IOException {
    byte[] bytes = DigestUtils.sha1(new FileInputStream(path));
    return Arrays.hashCode(bytes);
} {code}

Soon, I'll open the PR and link it here."
STORM-4076,KafkaTridentSpoutEmitters can poll all partitions at once instead of one at a time,"{{Currently 'KafkaTridentTransactionalSpoutEmitter' and 'KafkaTridentOpaqueEmitter' polls every partition assigned to the spout one by one while emitting new batches. }}

{{But this can be improved by leveraging Kafka Consumer's usual polling strategy. That is Kafka Broker will take care of choosing the right partition.}}

{{Advantages of this are}}
{{1. If a spout is assigned multiple Topic Partitions, the consumer doesn't have to waste time on polling partitions with no data.}}
{{2. This change will give better control over the Trident batch size, by adjusting Kafka Consumer properties.}}

{{Note: This change will affect only when the batch is emitted for the first time.}}

 "
STORM-4075,Supprt mTLS between Storm and ZK,"Zookeeper supports mTls refer: https://issues.apache.org/jira/browse/ZOOKEEPER-2094.

To connect ZK from Storm using mTls we need to enable the following setting

 
{code:java}
-Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty
-Dzookeeper.client.secure=true
-Dzookeeper.ssl.keyStore.location=<>
-Dzookeeper.ssl.keyStore.password=<>
-Dzookeeper.ssl.trustStore.location=<>
-Dzookeeper.ssl.trustStore.password=<>
{code}
 "
STORM-4074,Bump joda-time:joda-time from 2.12.5 to 2.12.7,
STORM-4073,Bump com.fasterxml.jackson:jackson-bom from 2.16.1 to 2.17.2 ,
STORM-4072,Bump org.jctools:jctools-core from 2.0.1 to 4.0.5 ,
STORM-4071,Bump com.zaxxer:HikariCP from 5.0.1 to 5.1.0 ,
STORM-4070,mTls support,"Currently, Storm only supports *SASL* (Simple Authentication and Security Layer) and *Kerberos* for authentication. However, a more modern and increasingly popular method is *mTLS* (Mutual Transport Layer Security).

*mTLS* leverages SSL/TLS certificates for authentication and encryption, ensuring that both the client and server can verify each other's identities. This dual verification process provides an additional security layer compared to traditional methods.

For organizations already using trusted certificates within their infrastructure, integrating mTLS can be more straightforward. It allows them to use the existing certificate management systems, reducing the need for separate authentication setups.

In cloud environments, *mTLS* is often the preferred choice for securing communications. By addressing both encryption and authentication in a single solution, mTLS simplifies the configuration process while enhancing overall security."
STORM-4069, java.lang.ClassCastException for Trident-based Topology,"Copied from the mailing list:

 

Upgraded our storm versions to 2.6.3 but having issues on startup. Here is the error log:

2024-08-07 09:30:22.749 o.a.s.u.Utils Thread-22-$mastercoord-bg0-executor[1, 1] [ERROR] Async loop died!

java.lang.ClassCastException: class java.lang.Integer cannot be cast to class java.lang.Long (java.lang.Integer and java.lang.Long are in module java.base of loader 'bootstrap')

  at org.apache.storm.trident.topology.MasterBatchCoordinator.getStoredCurrTransaction(MasterBatchCoordinator.java:235) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.trident.topology.MasterBatchCoordinator.open(MasterBatchCoordinator.java:87) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:142) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:152) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:50) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.utils.Utils$1.run(Utils.java:393) [storm-client-2.6.3.jar:2.6.3]

  at java.base/java.lang.Thread.run(Thread.java:840) [?:?]

2024-08-07 09:30:22.756 o.a.s.e.e.ReportError Thread-22-$mastercoord-bg0-executor[1, 1] [ERROR] Error

java.lang.RuntimeException: java.lang.ClassCastException: class java.lang.Integer cannot be cast to class java.lang.Long (java.lang.Integer and java.lang.Long are in module java.base of loader 'bootstrap')

  at org.apache.storm.utils.Utils$1.run(Utils.java:413) ~[storm-client-2.6.3.jar:2.6.3]

  at java.base/java.lang.Thread.run(Thread.java:840) [?:?]

Caused by: java.lang.ClassCastException: class java.lang.Integer cannot be cast to class java.lang.Long (java.lang.Integer and java.lang.Long are in module java.base of loader 'bootstrap')

  at org.apache.storm.trident.topology.MasterBatchCoordinator.getStoredCurrTransaction(MasterBatchCoordinator.java:235) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.trident.topology.MasterBatchCoordinator.open(MasterBatchCoordinator.java:87) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:142) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:152) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:50) ~[storm-client-2.6.3.jar:2.6.3]

  at org.apache.storm.utils.Utils$1.run(Utils.java:393) ~[storm-client-2.6.3.jar:2.6.3]

  ... 1 more

2024-08-07 09:30:22.788 o.a.s.u.Utils Thread-22-$mastercoord-bg0-executor[1, 1] [ERROR] Halting process: Worker died"
STORM-4068,Support for elastic search versions 8.X,"The current implementations of request builders has ""type"" field mandatory. But it is not the case for Elastic Search version 8.0 or above.


[https://github.com/apache/storm/blob/235ed2ba2c12d2fb96d2a3995b55cad4c483f4ab/external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/trident/EsState.java#L104]


Probably have to add a method in the EsConfig builder to handle this."
STORM-4067,Jersey 3.1.7,
STORM-4066,Jetty 11.0.21,
STORM-4065,Dropwizard Metric 4.2.26,
STORM-4063,Curator 5.7.0,
STORM-4062,JSON Mini Dev 2.5.1,
STORM-4061,Hadoop 3.4.0,
STORM-4060,Netty client will wait up to 10 minutes to send messages to unreachable worker on close()," 

Since PENDING_MESSAGES_FLUSH_TIMEOUT_MS is hardcoded to 10 minutes, this means buffered messages will remain in the worker 10 minutes before being given up and reprocessed.
This leads to increased latencies in the topology.
It is proposed that PENDING_MESSAGES_FLUSH_TIMEOUT_MS is exposed as property to be configured by the user.

Code that leads to this situation:
{code:java}
public class Client extends ConnectionWithStatus implements ISaslClient {
    private static final long PENDING_MESSAGES_FLUSH_TIMEOUT_MS = 600000L; {code}
{code:java}
@Override
public void close() {
    if (!closing) {
        LOG.info(""closing Netty Client {}"", dstAddressPrefixedName);
        // Set closing to true to prevent any further reconnection attempts.
        closing = true;
        waitForPendingMessagesToBeSent();
        closeChannel();

        // stop tracking metrics for this client
        if (this.metricRegistry != null) {
            this.metricRegistry.deregister(this.metrics);
        }
    }
} {code}
{code:java}
private void waitForPendingMessagesToBeSent() {
    LOG.info(""waiting up to {} ms to send {} pending messages to {}"",
             PENDING_MESSAGES_FLUSH_TIMEOUT_MS, pendingMessages.get(), dstAddressPrefixedName);
    long totalPendingMsgs = pendingMessages.get();
    long startMs = System.currentTimeMillis();
    while (pendingMessages.get() != 0) {
        try {
            long deltaMs = System.currentTimeMillis() - startMs;
            if (deltaMs > PENDING_MESSAGES_FLUSH_TIMEOUT_MS) {
                LOG.error(""failed to send all pending messages to {} within timeout, {} of {} messages were not ""
                    + ""sent"", dstAddressPrefixedName, pendingMessages.get(), totalPendingMsgs);
                break;
            }
            Thread.sleep(PENDING_MESSAGES_FLUSH_INTERVAL_MS);
        } catch (InterruptedException e) {
            break;
        }
    }

} {code}
 "
STORM-4058,Provide ClusterMetrics via a Prometheus Preparable Reporter,as the title says. Prepare basic metrics via a pushgateway client for prometheus
STORM-4057,Fix Worker Termination in K8S with Security Context,https://lists.apache.org/thread/p79msxdkpzt3d57ycf1cpl8gn3j6tqkg
STORM-4056,Build with -Pnative on MacOS,Building with macOS is currently not supported if -Pnative is added to the list of profiles.
STORM-4055,ConcurrentModificationException on KafkaConsumer when running topology with Metrics Reporters,"After a recent upgrade to storm-kafka-client on storm server 2.6.1, we are seeing ConcurrentModificationException in our topology at runtime. I believe this is due to the re-use of a KafkaConsumer instance between the KafkaSpout and the 
KafkaOffsetPartitionMetrics which were added some time between 2.4.0 and 2.6.1.
 
h3. Steps to Reproduce:

Configure a topology with a basic KafkaSpout. Configure the topology with one of the metrics loggers. We used our own custom one, but reproduced it with ConsoleStormReporter as well. The JMXReporter did not reproduce the issue for us, but we did not dig into why.

*reporter config:*

{{topology.metrics.reporters: [}}
{{  {}}
{{    ""filter"": {}}
{{      ""expression"": "".*"",}}
{{      ""class"": ""org.apache.storm.metrics2.filters.RegexFilter""}}
{{    },}}
{{    ""report.period"": 15,}}
{{    ""report.period.units"": ""SECONDS"",}}
{{    ""class"": ""org.apache.storm.metrics2.reporters.ConsoleStormReporter""}}

    }
{{]}}
h3. Stacktrace:
{quote}[ERROR] Exception thrown from NewRelicReporter#report. Exception was suppressed.
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access. currentThread(name: metrics-newRelicReporter-1-thread-1, id: 24) otherThread(id: 40)
    at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:2484) ~[stormjar.jar:?]
    at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2465) ~[stormjar.jar:?]
    at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:2144) ~[stormjar.jar:?]
    at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:2123) ~[stormjar.jar:?]
    at org.apache.storm.kafka.spout.metrics2.KafkaOffsetPartitionMetrics.getBeginningOffsets(KafkaOffsetPartitionMetrics.java:181) ~[stormjar.jar:?]
    at org.apache.storm.kafka.spout.metrics2.KafkaOffsetPartitionMetrics$2.getValue(KafkaOffsetPartitionMetrics.java:93) ~[stormjar.jar:?]
    at org.apache.storm.kafka.spout.metrics2.KafkaOffsetPartitionMetrics$2.getValue(KafkaOffsetPartitionMetrics.java:90) ~[stormjar.jar:?]
    at com.codahale.metrics.newrelic.transformer.GaugeTransformer.transform(GaugeTransformer.java:60) ~[stormjar.jar:?]
    at com.codahale.metrics.newrelic.NewRelicReporter.lambda$transform$0(NewRelicReporter.java:154) ~[stormjar.jar:?]
    at java.base/java.util.stream.ReferencePipeline$7$1.accept(Unknown Source) ~[?:?]
    at java.base/java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.lambda$entryConsumer$0(Unknown Source) ~[?:?]
    at java.base/java.util.TreeMap$EntrySpliterator.forEachRemaining(Unknown Source) ~[?:?]
    at java.base/java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet$UnmodifiableEntrySetSpliterator.forEachRemaining(Unknown Source) ~[?:?]
    at java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]
    at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]
    at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(Unknown Source) ~[?:?]
    at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(Unknown Source) ~[?:?]
    at java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]
    at java.base/java.util.stream.ReferencePipeline.forEach(Unknown Source) ~[?:?]
    at java.base/java.util.stream.ReferencePipeline$7$1.accept(Unknown Source) ~[?:?]
    at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Unknown Source) ~[?:?]
    at java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source) ~[?:?]
    at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source) ~[?:?]
    at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source) ~[?:?]
    at java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source) ~[?:?]
    at java.base/java.util.stream.ReferencePipeline.collect(Unknown Source) ~[?:?]
    at com.codahale.metrics.newrelic.NewRelicReporter.report(NewRelicReporter.java:138) ~[stormjar.jar:?]
    at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:243) ~[metrics-core-3.2.6.jar:3.2.6]
    at com.codahale.metrics.ScheduledReporter$1.run(ScheduledReporter.java:182) [metrics-core-3.2.6.jar:3.2.6]
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [?:?]
    at java.base/java.util.concurrent.FutureTask.runAndReset(Unknown Source) [?:?]
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) [?:?]
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
    at java.base/java.lang.Thread.run(Unknown Source) [?:?]
{quote}
h3. Workaround

Configure the with RegexFilter or similar that excludes the KafkaOffsetPartitionMetrics.
h3. Impact

I am concerned that depending on the timing of the access to the spout that the offending metric could fast forward or rewind the spout. I did not do any further testing to see if the lock could be mis-managed in such a way that the spout is directly impacted, but it is feasible. Impact may need to be adjusted if it is confirmed that a simple metric reporter could result in skipping events or re-processing them.
h3. Potential Code Issues:

*KafkaSpout.java*

{{private transient Consumer<K, V> consumer;}}
{{...}}

{{public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {}}
{{        ...}}
{{        //this consumer will be used by the spout everywhere}}
{{        consumer = kafkaConsumerFactory.createConsumer(kafkaSpoutConfig.getKafkaProps());}}
{{        tupleListener.open(conf, context);}}
{{        this.kafkaOffsetMetricManager}}
{{            = new KafkaOffsetMetricManager<>(() -> Collections.unmodifiableMap(offsetManagers), () -> consumer, context);}}

() -> consumer does not appear to be a safe provider.  It re-uses the same instance of the KafkaConsumer as the KafkaSpout in another thread and KafkaConsumer is not thread safe.

{*}KafkaOffsetPartitionMetrics.java: getBeginningOffsets, getEndOffsets{*}{{{{}}{}}}

 

 

{{private Map<TopicPartition, Long> getBeginningOffsets(Set<TopicPartition> topicPartitions) {}}
{{    Consumer<K, V> consumer = consumerSupplier.get();}}
{{    ...}}
{{    try {}}
{{        // This will actually try to modify the KafkaSpout instance of the consumer which could negatively impact the spout}}
{{        beginningOffsets = consumer.beginningOffsets(topicPartitions);}}

{\\{    }}}
{{    ...}}
{{{}}{}}}{{{}private Map<TopicPartition, Long> getEndOffsets(Set<TopicPartition> topicPartitions) {{}}}
{{    Consumer<K, V> consumer = consumerSupplier.get();}}
{{    ...}}
{{    try {}}
{{        // This will actually try to modify the KafkaSpout instance of the consumer which could negatively impact the spout}}
{{{}        endOffsets = consumer.endOffsets(topicPartitions);{}}}{\{    }

}}
{{    ...}}
{{}}}"
STORM-4054,Add Worker CPU Metric,Allow reporting of worker cpu usage if no cgroups are configured.
STORM-4053,Add Hadoop client API dependency back to storm-hdfs,See https://github.com/apache/incubator-stormcrawler/pull/1189
STORM-4052,Simplify/Remove double delete/lookup in heartbeat cleanup code,"This changes slightly simplifies the heartbeat cleanup code so it no longer tries to delete the heartbeat files twice. It also removes an unneeded directory listing (and possible race) by truncating the versions list and using it for removal instead of for keeping.

Removing the double delete attempt is important because it removes a lookup for now non-existent files. Looking up non existent files, especially highly unique (like timestamped) ones can adversely affect many operating systems as these lookups are cached as negative dentries.
[https://lwn.net/Articles/814535/]

When cleanup runs, it iterates over the heartbeat directory that contains a token and version file for each heartbeat. It calls deleteVersion for each file in the directory which attempts to delete both files associated with the heartbeat. As deleteVersion already deletes both when it first iterates over the token file, the iteration for the version file has nothing to do.

Before removing, the deleteVersion code checks for the existence of these now non existent files. On linux (and other OSs) a lookup for a non-existent path will create a negative dentry in the operating system's cache. On some configurations this cache can grow effectively unbounded leading to performance issues. On newer systems this cache is better managed, but this will still dilute an otherwise useful OS cache with useless entries.

 

Copied from [https://github.com/apache/storm/pull/3635] (Author: sammac)

 

 

 

 

 

 "
STORM-4051,Scheduler needs to include acker memory for topology resources,"The scheduler has a bug where acker memory is not considered in the scheduling estimate. The case I found was where a topology should fit on two supervisors, but the cluster has 1 available and 2 blacklisted. The scheduler thinks the topology should fit on one supervisor and fails to schedule, but also fails to release a supervisor from the blacklist, resulting in the topology never getting scheduled.

With this fix, the scheduler properly detects the topology will need to be scheduled on two supervisors and releases one from the blacklist and schedules successfully.

Switched some scheduling logs from trace to debug to make debugging scheduling issues easier."
STORM-4050,rocksdbjni:8.10.0 ,
STORM-4049,snakeyaml:2.2,
STORM-4048,netty 4.1.107.Final ,
STORM-4047,jackson 2.16.1,
STORM-4046,caffeine:3.1.8,
STORM-4045,log4j2 2.23.0 ,
STORM-4044,commons-lang3:3.14.0,
STORM-4043,commons-io:2.14.0,
STORM-4042,Clojure 1.11.2,
STORM-4041,Zookeeper 3.9.2,
STORM-4040,Nimbus fails to start up on older CPUs (RocksDB v7.x.x onwards),"When Nimbus start-up and storm_rocks already contains data, the JVM will encounter an illegal instruction exception if the CPU is pre-Haswell era.

This can be seen on such a CPU by deploying a running topology then restarting Nimbus. It will fail to startup because the JVM will have crashed when RocksDB reads the state from storm_rocks folder. (You can recover from this by deleting the storm_rocks folder - Nimbus will then startup ok ... until the next time it restarts!)

The issue is that RocksDB v7.x.x or higher (so, applies to Storm 2.5.0 onwards from this commit [https://github.com/apache/storm/commit/d7b4c084a89961a4060edb2e755491a21015c200]) is a C++ component built for modern CPUs. The fix for this is to downgrade RocksDB to pre v7.x.x (or you can build your own version of v7+ from source with certain compiler options set)

You can find other reports of this - e.g. [https://github.com/facebook/rocksdb/issues/11096] or [https://github.com/trezor/blockbook/issues/684]

This is not a bug in Storm, of course. Nor is it a bug - more a 'minimum requirements'.

So I have a pull request to suggest workarounds for this in the Troubleshooting guide."
STORM-4038,Cleanup Hadoop Dependencies,hadoop pulls in a lot of stuff we do not actually need. might be good to prune / exclude
STORM-4031,Fix broken Talks & Video Webpage,"External content seems to be blocked now:

 

[https://storm.apache.org/talksAndVideos.html]

 

We should either host those stuff ourselves or just link it."
STORM-4030,Dependency upgrades,"{code:java}
mvn versions:display-dependency-updates ""-Dmaven.version.ignore=.*-M.*,.*-alpha.*,.*-beta.*,.*-BETA.*,.*-b.*,.*.ALPHA.*"" | grep '\->' | sort | uniq{code}
{{shows a large number of upgradable dependencies.}}"
STORM-4029,Bump org.apache.commons:commons-compress from 1.21 to 1.26.,
STORM-4028,Curator 5.6.0,https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12314425&version=12353185
STORM-4027,Kyro 5.6.0,https://github.com/EsotericSoftware/kryo/releases
STORM-4026,Thrift 0.19.0,https://github.com/apache/thrift/blob/master/CHANGES.md
STORM-4025,ClassCastException when changing log level in Storm UI,"A ClassCastException is raised in Storm UI when trying to change the log level for a topology (cf. attached screenshot and full error stack).
 * POST request payload sent to the logconfig web service:

{code:java}
{""namedLoggerLevels"":{""com.example"":{""target_level"":""DEBUG"",""reset_level"":""INFO"",""timeout"":30}}}{code}
 * Error message received:

{noformat}
""error"": ""500 Server Error"",
""errorMessage"": ""java.lang.ClassCastException: java.lang.Integer incompatible with java.lang.Long\n\tat org.apache.storm.daemon.ui.UIHelpers.putTopologyLogLevel(UIHelpers.java:2422)\n\tat org.apache.storm.daemon.ui.resources.StormApiResource.putTopologyLogconfig(StormApiResource.java:469)\n\tat
[...]
{noformat}
The timeout parameter seems to be parsed as an Integer whereas it is cast into a Long in the code, then raising a ClassCastException:

cf. [https://github.com/apache/storm/blob/ae3a96e762095553311d9e335f7505c0b351d810/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L2422C13-L2422C67]

This issue could be related to the recent change of the JSON parser having a different behavior when parsing numbers:

cf. [https://github.com/apache/storm/commit/1406f680c8d65de591c997066d2ca2cd80e56c4f#diff-67de3adeec3548f570568d351b76a4b3a936ee9ed0f3f59445ff3def0505f247]

 
 
 "
STORM-4024,Bolt Input Stats are blank if topology.acker.executors is null or 0,"On StormUI (and via API) the bolt Input Stats do not work when topology.acker.executors is null or 0 (see attachements showing difference with and without ackers)

Also, some of the per-bolt instance Executed and latency fields are also not working"
STORM-4023,Background periodic Kerberos re-login should use same JAAS configuration as initial login,"In the [Login|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/messaging/netty/Login.java] class, a background thread is started that periodically performs a re-login to the Kerberos Ticket Granting Server.

For the initial login, a custom Configuration instance is [created|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/messaging/netty/Login.java#L257] and [supplied to the LoginContext constructor|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/messaging/netty/Login.java#L300] potentially using a custom JAAS file location.

However, the background refresh thread does not then subsequently provide the JAAS file location or Configuration to the [reLogin method|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/messaging/netty/Login.java#L222], so it tries to construct a LoginContext with [just a context name and subject|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/messaging/netty/Login.java#L409] but no Configuration parameter, which means that the underlying {{Configuration.getConfiguration()}} call has to load one from [system defaults|https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/jdk-11%2B28/src/java.base/share/classes/javax/security/auth/login/LoginContext.java#L242], which could possibly specify a different file or none at all.

In our application where this issue was found, we had set the {{java.security.auth.login.config}} value to a valid JAAS file location as a Storm client property along with other standard connectivity properties, since the [Netty client framework|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyClient.java#L61] loads it [from the topology configuration|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/security/auth/ClientAuthUtils.java#L64]. It looks like the Netty server framework [does the same|https://github.com/apache/storm/blob/v2.6.0/storm-client/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyServer.java#L55] as well. The initial login succeeded and the following Storm Nimbus interactions were successful, but a while later it lost the ability to communicate with Storm with this error being logged,
{noformat}
ERROR [Refresh-TGT] org.apache.storm.messaging.netty.Login Could not refresh TGT for principal: <REDACTED>
javax.security.auth.login.LoginException: No LoginModules configured for StormClient
   at java.base/javax.security.auth.login.LoginContext.init(LoginContext.java:267)
   at java.base/javax.security.auth.login.LoginContext.<init>(LoginContext.java:385)
   at org.apache.storm.messaging.netty.Login.reLogin(Login.java:409)
   at org.apache.storm.messaging.netty.Login$1.run(Login.java:222)
   at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}
It appears that a viable workaround for this issue is to also set the system property,

{{-Djava.security.auth.login.config=/some/path/jaas.conf}}

for the application, referencing the same JAAS file location as was set in the Storm configuration. After doing so the background refresh thread was able to correctly function in our situation.

To address this, we can update the {{reLogin}} method to use the same JAAS configuration. Furthermore it should use the same callback handler instance that was originally provided also, instead of creating a new default one."
STORM-4022,Avoid flooding nimbus logs with dead topology heatbeat timeout error messages,"When running a topology on storm in AWS EKS environment with a lot of large number of submits and kills of topologies, it was observed that nimbus logs get flooded with warning messages ""Exception when getting heartbeat timeout""."
STORM-4021,Add worker hooks via storm.yaml configuration file,"Why can't I add worker hooks via storm.yaml configuration file?

Is it going to affect performance?"
STORM-4018,Wrong order of the params in Storm API resource,"[https://github.com/apache/storm/blob/master/storm-webapp/src/main/java/org/apache/storm/daemon/ui/resources/StormApiResource.java#L551]

Wrong order of the params


{noformat}
UIHelpers.putTopologyDebugActionSpct(
                            nimbusClient.getClient(), id, component, action, spct
                    ),{noformat}
where as it should be as
{noformat}
UIHelpers.putTopologyDebugActionSpct(
        nimbusClient.getClient(), id, action, spct, component
){noformat}"
STORM-4017,isAnyWindowsProcessAlive does not work with multiple pids,"The method isAnyWindowsProcessAlive in ServerUtils.java does not work with 2 or more pids in the supplied pid collection

The refactor done as part of STORM-3638 has made an incorrect assumption that the 'tasklist' Windows tool can accept multiple arguments of ""/fi pid eq <id>"" and treat this list as an 'or' list when, in fact, the /fi arguments should be considered an 'and' list.

e.g. tasklist /fi ""pid eq 123"" /fi ""pid eq 456"" means 'give me information on processes where each process has a pid of 123 and 456'

The effect of this means that isAnyWindowsProcessAlive will always return false when given 2 or more (different) pids."
STORM-4016,Kafka spout: start using poll(Duration),"Kafka has deprecated poll(long) in favour of poll(Duration): [KIP-266: Fix consumer indefinite blocking behavior|https://cwiki.apache.org/confluence/display/KAFKA/KIP-266%3A+Fix+consumer+indefinite+blocking+behavior]


There is also an interesting report about the behaviour of it poll:



_The pre-existing variant {{poll(long timeout)}} would block indefinitely for metadata updates if they were needed, then it would issue a fetch and poll for {{timeout}} ms for new records. The initial indefinite metadata block caused applications to become stuck when the brokers became unavailable. The existence of the timeout parameter made the indefinite block especially unintuitive._

_We will add a new method {{poll(Duration timeout)}} with the semantics:_
 # _iff a metadata update is needed:_
 ## _send (asynchronous) metadata requests_
 ## _poll for metadata responses (counts against timeout)_
 *** _if no response within timeout, return an empty collection immediately_
 # _if there is fetch data available, return it immediately_
 # _if there is no fetch request in flight, send fetch requests_
 # _poll for fetch responses (counts against timeout)_
 ** _if no response within timeout, return an empty collection (leaving async fetch request for the next poll)_
 ** _if we get a response, return the response_

_We will deprecate the original method, {{{}poll(long timeout){}}}, and we will not change its semantics, so it remains:_
 # _iff a metadata update is needed:_
 ## _send (asynchronous) metadata requests_
 ## _poll for metadata responses indefinitely until we get it_
 # _if there is fetch data available, return it immediately_
 # _if there is no fetch request in flight, send fetch requests_
 # _poll for fetch responses (counts against timeout)_
 ** _if no response within timeout, return an empty collection (leaving async fetch request for the next poll)_
 ** _if we get a response, return the response_

_One notable usage is prohibited by the new {{{}poll{}}}: previously, you could call {{poll(0)}} to block for metadata updates, for example to initialize the client, supposedly without fetching records. Note, though, that this behavior is not according to any contract, and there is no guarantee that {{poll(0)}} won't return records the first time it's called. Therefore, it has always been unsafe to ignore the response._

 

 

[https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75974886|http://example.com]"
STORM-4015,Storm now requires Java 11 - docs/website should reflect it,"Fix [https://storm.apache.org/releases/current/Setting-up-a-Storm-cluster.html]

It is required to run with Java 11+"
STORM-4014,Add Docker file ,"See https://lists.apache.org/thread/cdcq5c9bbjd09n574sq147vcjqpt3mqm

We want to be able to generate Docker images for Apache Storm and push them to DockerHub"
STORM-4013,Add Round Robin Scheduler with node limit,"Resource aware schedulers used in Storm optimize on the use of compute, memory and network usage by concentrating executors on as few nodes as possible.

In some instances, this may not be desirable. In cases where the cluster is not shared on when the user explicitly wants the load to be evenly distributed amongst supervisors that have been pre-allocated (for example in a kubernetes environment (AWS EKS or GKE), storm should provide a scheduler to distribute the load evenly, with an optional limit on the number of supervisors that the load will be distributed on - default value will be to use all supervisors.

Since a worker runs only one topology, using all workers for a topology will essentially prevent any other topology from running in that cluster. Proper configuration is therefore required when using Round Robin strategy."
STORM-4011,Migrate Storm itself to use Commons Collections 4,
STORM-4010,Reduce ASM Mess in dependency tree,"We have lots of different asm versions within our classpath / libs.
We should manage these dependencies to avoid having different versions around.

       * ASM Analysis (org.ow2.asm:asm-analysis:6.0_BETA - http://asm.objectweb.org/asm-analysis/)
        * ASM Commons (org.ow2.asm:asm-commons:6.0 - http://asm.objectweb.org/asm-commons/)
        * ASM Core (org.ow2.asm:asm:6.0 - http://asm.objectweb.org/asm/)
        * ASM Tree (org.ow2.asm:asm-tree:6.0 - http://asm.objectweb.org/asm-tree/)
        * ASM Util (org.ow2.asm:asm-util:6.0_BETA - http://asm.objectweb.org/asm-util/)"
STORM-4009,Regenerate Thrift Code with current Thrift version used in Storm,
STORM-4008,Implement timed GH actions to publish SNAPSHOTS,"as the title says

Related to https://issues.apache.org/jira/browse/STORM-4006"
STORM-4007,Add the getName() method in order to obtain the applied label to the Trident stream.,https://github.com/apache/storm/pull/2792/commits/9b49fb28877d89661da414ac20530b2b7337c38e
STORM-4006,Implement timed GH actions to publish to nightlies.a.o,"We got the secrets, cf https://issues.apache.org/jira/browse/INFRA-25232

Now it is time to implement a publish for the dist artifacts ;-)"
STORM-4005,ElasticSearch 7.17.13,
STORM-4004,Upgrade Kafka Clients to 3.6.0,
STORM-4003,"storm-kafka-monitor fails with Java 17 runtime, missing jakarta.xml.bind dependency",
STORM-4002,Security Vulnerability - Action Required: “Incorrect Permission Assignment for Critical Resource” vulnerability in some components of  org.apache.storm," I think the method org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.checkPermissionOfOther(FileSystem fs, Path path, FsAction action, Map<URI, FileStatus> statCache) may have an “Incorrect Permission Assignment for Critical Resource”vulnerability which is vulnerable in in some components of  org.apache.storm. It shares similarities to a recent CVE disclosure _CVE-2017-3166_ in the project _""apache/hadoop""_ project. The influencing components are listed below:
 # org.apache.storm:storm-kafka-examples in the versions between 1.1.0 and 1.2.4.
 # org.apache.storm:storm-starter in the versions of 1.1.2-1.1.3 and 1.2.0-1.2.2

The source vulnerability information is as follows: !https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.1&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ_bBS_0CMiL9kNUgnr95IJelNJAQJp906nnAonpFswrxMbSt1EVV1S2q6kq_ur-YE-1H49gOCjMGqFYtm5xBOS_EBOZci8ukIw2Hn8kM-9OIKVIxXrlhcRm6LA&disp=emb&realattid=ii_lmt56kbv0|width=1,height=1!!https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.2&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ-8wPNUdQ35WBKaadck2X1lP34blTQ_qiyhu5T7l0G8T4cboSCiFNgfxaCQZZsK-Pm3ebzj4JSWBs558OxWHJPM1uJqKlMvPMhpx9J0TiojhC85DNqeLu3dr2Q&disp=emb&realattid=ii_lmt6415i0|width=1,height=1!!https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.0.1&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ9XERxykP1zaB9Codaz3lisQ9gKwLHXnEIHP4p4oUcINmdFEWTJAWeDMfayncBsWIBj_kc2cAKHx4c7InMtKL98nDb2Dnt3TpfGLQCcJhdFsSBhemVA14CI0rA&disp=emb&realattid=ii_loxzzieb0|width=1,height=1!

*Vulnerability Detail:*

*CVE Identifier:* CVE-2017-3166

{*}Description{*}: In Apache Hadoop versions 2.6.1 to 2.6.5, 2.7.0 to 2.7.3, and 3.0.0-alpha1, if a file in an encryption zone with access permissions that make it world readable is localized via YARN's localization mechanism, that file will be stored in a world-readable location and can be shared freely with any application that requests to localize that file.

*Reference:*[ |http://goog_608275719/] [https://nvd.nist.gov/vuln/detail/CVE-2017-3166]

{*}Patch{*}: [https://github.com/apache/hadoop/commit/a47d8283b136aab5b9fa4c18e6f51fa799d91a29]
*Vulnerability Description:* The vulnerability is present in the class  org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager  of method  checkPermissionOfOther(FileSystem fs, Path path, FsAction action, Map<URI, FileStatus> statCache)  , which is responsible for checking the permissions of other files in the distributed cache.. {*}But t{*}{*}he check snippet is similar to the vulnerable snippet for CVE-2017-3166{*} and may have the same consequence as CVE-2017-3166: {*}a file in an encryption zone with access permissions  will be stored in a world-readable location and can be freely shared with any application that requests the file to be localized{*}. Therefore, maybe you need to fix the vulnerability with much the same fix code as the CVE-2017-3166 patch. 
    Considering the potential risks it may have, I am willing to cooperate with you to verify, address, and report the identified vulnerability promptly through responsible means. If you require any further information or assistance, please do not hesitate to reach out to me. Thank you and look forward to hearing from you soon.
 "
STORM-4001,log4j2 2.21.1,
STORM-3999,Add Java 21 to Build Matrix,
STORM-3998,Guava 32.1.3-jre,
STORM-3997,Netty 4.1.100,
STORM-3996,Jetty 9.4.53.v20231009,
STORM-3995,commons-text 1.11.0,
STORM-3994,LocalCluster init fails on Windows because of missing librocksdbjni-win64.dll,"Apache Storm 2.5.0 upgraded rocksdbjni to 8.1.1 as part of STORM-3913 which does not include the native library for Windows. See below link

[https://github.com/facebook/rocksdb/issues/11420]

The LocalCluster initialization fails with the following error on Windows

 
{noformat}
Caused by: java.lang.ExceptionInInitializerError
    at org.apache.storm.metricstore.rocksdb.RocksDbStore.prepare(RocksDbStore.java:67)
    at org.apache.storm.metricstore.MetricStoreConfig.configure(MetricStoreConfig.java:34)
    at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:542)
    at org.apache.storm.LocalCluster.<init>(LocalCluster.java:245)
    at org.apache.storm.LocalCluster.<init>(LocalCluster.java:160)
    at com.example.test.LocalTestCluster.<init>(LocalTestCluster.java:23)
    at com.example.test.ExampleTestDriver.<init>(ExampleTestDriver.java:73)
    at com.example.test.ExampleTest.<init>(ExampleTest.java:38)
    ... 28 more
Caused by: java.lang.RuntimeException: librocksdbjni-win64.dll was not found inside JAR.
    at org.rocksdb.NativeLibraryLoader.loadLibraryFromJarToTemp(NativeLibraryLoader.java:118)
    at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:102)
    at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:82)
    at org.rocksdb.RocksDB.loadLibrary(RocksDB.java:70)
    at org.rocksdb.RocksDB.<clinit>(RocksDB.java:39)
    ... 37 more{noformat}
The latest version of rockdbjni (8.5.4) includes the missing library.

 

 "
STORM-3993,ActiveMQ 5.18.3,
STORM-3991,Avro 1.11.3,
STORM-3990,Hadoop 2.10.2,
STORM-3989,Snappy Java 1.1.10.4,
STORM-3988,Removal of modules residing in /external,"> Remove the following external components:
> 
> - storm-cassandra
> - storm-eventhubs
> - storm-hbase
> - storm-hive
> - storm-kinesis
> - storm-mongodb
> - storm-mqtt
> - storm-openmtsdb
> - storm-pmml
> - storm-pulsar
> - storm-rocketmq
> - storm-solr
>

 

https://lists.apache.org/thread/1h5kk47tvv2bmvfgxo34gkjyzfl8xsbc"
STORM-3987,Zookeeper 3.9.1,
STORM-3986,Get rid of BlacklistScheduler timer [INFO] logs,"Every 10 seconds, we have the following INFO-level line of log:

2023-10-01 08:02:01.607 o.a.s.s.b.BlacklistScheduler timer [INFO]
 Supervisors [] are blacklisted.

I propose to change current behavior to only print the list of blocklisted supervisors only if there is at least one which is blocklisted (to use a more neutral wording than ""blacklisted"")."
STORM-3985,Upgrade dependencies to ones based on Jakarta EE APIs,
STORM-3983,Refactor File.createTempFile with Files.create(...),
STORM-3982,"Update README.md, DOAP/RDF and Dev section in POM",
STORM-3981,Negative Acknowledge not implemented for Pulsar Storm Adapter ,"[https://github.com/apache/storm/blob/a837e6add1fee99115eb426077f6e62fd406eea2/external/storm-pulsar/src/main/java/org/apache/pulsar/storm/PulsarSpout.java]

v2.11.0 pulsar-storm
There is no way to negatively acknowledge the consumer and the registry method for DeadLetterPolicy is broken

{{ConsumerConfigurationData<byte[]> subscriptionConfig = new ConsumerConfigurationData<>(); subscriptionConfig.setSubscriptionInitialPosition(SubscriptionInitialPosition.Earliest); subscriptionConfig.setSubscriptionType(SubscriptionType.Shared); subscriptionConfig.setDeadLetterPolicy(DeadLetterPolicy.builder() .deadLetterTopic(viestiSourceConfig.getDeadLetterTopic()).build());}}
{{PulsarSpoutV2 pulsarSpout = new PulsarSpoutV2( spoutConfiguration, ((ClientBuilderImpl) createBuilder(viestiSourceConfig)) .getClientConfigurationData() .clone(), subscriptionConfig);}}

This above code doesnt stick while creating PulsarSpout.

 
{{static class SpoutConsumer implements PulsarSpoutConsumer {}}
{{private Consumer<byte[]> consumer;}}
{{public SpoutConsumer(Consumer<byte[]> consumer) \{
        this.consumer = consumer;
    }

    public Message<byte[]> receive(int timeout, TimeUnit unit) throws PulsarClientException \{
        return this.consumer.receive(timeout, unit);
    }

    public void acknowledgeAsync(Message<?> msg) \{
        this.consumer.acknowledgeAsync(msg);
    }

    public void close() throws PulsarClientException \{
        this.consumer.close();
    }

    public void unsubscribe() throws PulsarClientException \{
        this.consumer.unsubscribe();
    }
}}}
 
Also there is no Mechanism to negativelyAcknowledge a message.

*Expected behavior*

While Setting DeadletterPolicy It should not drop it while serialising. Negative Acks support should be there

*Desktop (please complete the following information):*
MacOs Ventura 13.4.1
java version ""1.8.0_333""
Java(TM) SE Runtime Environment (build 1.8.0_333-b02)
Java HotSpot(TM) 64-Bit Server VM (build 25.333-b02, mixed mode)
Apache Storm 2.2.1

Trying to consume from Pulsar Topic in Apache Storm"
STORM-3980,Updates developer docs and removing obsolete content ,
STORM-3979,JUnit 5.10.0,
STORM-3978,"Rep,ace json-simple with minidev json",
STORM-3977,Minidev JSON 2.5.0,
STORM-3976,j2html 1.6.0,
STORM-3975,Dropwizard 1.3.29,
STORM-3974,Jersey 2.40,
STORM-3973,log4j2 2.20.0,
STORM-3972,commons-lang3 3.13.0,
STORM-3971,Storm-cassandra shows errors in Eclipse,"Errors in Eclipse as reported by Alexandre Vermeerbergen 
On Thursday, August 31, 2023 at 06:15:55 AM PDT,
 
First set of errors is very strange: in /storm-cassandra/src/test/java/org/apache/storm/cassandra/trident/WeatherBatchSpout.java,
 
I have these imports:

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.UUID;

with all the same errror: ""The package java.util is accessible from
more than one module: <unnamed>, java.base""
=> this look like an issue with the notion of Java modules"
STORM-3970,storm-sql-core in fails to build Eclipse for generated java code,"storm-sql-core extract Parser.jj and then uses javacc to generate java classes which are then used in the project.

Eclipse does not seem to generate/add these generated classes - leading to errors.

The project builds properly externally using maven commands."
STORM-3968,Add missing groupId in storm-core/pom.xml for maven-dependency-plugin,"In storm-core/pom.xml, missing groupId for maven-dependcy-plugin causes failure to parse the module in eclipse ide"
STORM-3967,Set generated sources directory for use by IDE like eclipse,"When storm is imported as a maven project into Eclipse, the IDE does not generate, detect or use the generated sources (example storm-sql-core).

Set the generated sources directory in main pom.xml so that all subprojects can be successfully imported."
STORM-3966,Remove illegal access warning in JDK11 test of storm-sql-hdfs,"sql/storm-sql-hdfs module test warns about illegal access when run under JDK11.

This is due to the following dependcy tree:
+- org.apache.storm:storm-hdfs:jar:2.6.0-SNAPSHOT:provided
| +- org.apache.hadoop:hadoop-auth:jar:2.10.1:provided

to fix Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil
(org.apache.hadoop:hadoop-auth:2.10.1)
to method sun.security.krb5.Config.getInstance() add
""add-opens java.security.jgss/sun.security.krb5=ALL-UNNAMED"""
STORM-3965,Remove illegal access warning in JDK11 test of storm-sql-core,"sql/storm-sql-core module test warns about illegal access when run under JDK11.

This is due to the following dependcy tree:
{color:#93a1a1} +- org.apache.calcite:calcite-core:jar:1.16.0:compile{color}

{color:#93a1a1} | +- org.apache.calcite.avatica:avatica-core:jar:1.11.0:compile{color}

{color:#93a1a1} | | \- com.google.protobuf:protobuf-java:jar:3.3.0:compile{color}

{color:#93a1a1} {color}

{color:#93a1a1} to fix Illegal reflective access by com.google.protobuf.UnsafeUtil {color}

{color:#93a1a1} (com.google.protobuf:protobuf-java:3.3.0) to field java.nio.Buffer.address add{color}

{color:#93a1a1} ""add-opens java.base/java.nio=ALL-UNNAMED""{color}

 "
STORM-3964,Remove or Update doap_Storm.rdf,"Storm project includes a file doap_Storm.rdf. This file has not been updated since 2015. The original doap project server [http://usefulinc.com/ns/doap] is not running. The successor to DOAP seems to be here [https://github.com/ewilderj/doap.]

Either update or remove https://github.com/apache/storm/blob/master/doap_Storm.rdf"
STORM-3963,Import and Build Apache Storm in Eclipse,"Importing Apache Storm into eclipse, shows a bunch of errors and warnings. "
STORM-3961,Modernize Storm UI's 3rd party dependencies,"Java dependencies only.
Bootstrap / JQuery etc are a separate topic."
STORM-3959,Add Matomo Tracking to Website,"ASF as a privacy-conform matomo instance for analytics. We should add it to the website to see how often Storm website is actually used to download stuff, etc."
STORM-3958,Capacity to set Storm UI's title in conf/storm.yaml,"I have several Storm clusters to manage (integration test cluster, pre-production ones, QA, several productions), and I always find disturbing to have the same ""Storm UI"" title in my web browser's tabs for the Storm UI of all these different environments.

I know it's trivial to update the content of {{public/index.html }}to change the {{<title>Storm UI</title>}} line to whatever I'd like, but I feel it would be cleaner to have the possibility to set this title in {{conf/storm.yaml}} file, for example with {{ui.title}} key.

Of course {{conf/defaults.yaml}} would need to have an extra line with:

{{ui.title: ""Storm UI""}}

to avoid any regression for anyone not willing to customize Storm UI's title.

Also I think that, for consistency, this other place in {{public/index.html}} where 'Storm UI' can be found should also get its value from {{ui.title}} key:

{{    <div class=""col-md-9"">}}
{{      <h1><a href=""/"">Storm UI</a></h1>}}
{{    </div>}}

If there is no objection to this proposal, then I could self-assign this task."
STORM-3956,Fix cli monitor component's argument type ,"From https://github.com/apache/storm/pull/3423

The `component` value is a string, not a number, see [Monitor.java](https://github.com/apache/storm/blob/master/storm-core/src/jvm/org/apache/storm/utils/Monitor.java#L156).

Attempting to use a number throws a stacktrace like such:

```
~/apache-storm-2.3.0/bin/storm monitor -m wordGenerator production-topology
topology	component	parallelism	stream	time-diff ms	emitted	throughput (Kt/s)
Available components for production-topology :
------------------
__acker
wordGenerator
intermediateRanker
counter
finalRanker
------------------
Exception in thread ""main"" java.lang.IllegalArgumentException: component: wordGeneratotor not found
	at org.apache.storm.utils.Monitor.metrics(Monitor.java:128)
	at org.apache.storm.utils.Monitor.metrics(Monitor.java:83)
	at org.apache.storm.command.Monitor$1.run(Monitor.java:53)
	at org.apache.storm.utils.NimbusClient.withConfiguredClient(NimbusClient.java:128)
	at org.apache.storm.utils.NimbusClient.withConfiguredClient(NimbusClient.java:117)
	at org.apache.storm.command.Monitor.main(Monitor.java:50)
```
"
STORM-3954,Remove Logback pulled in by Zookeeper,"We are using log4j2, so better to replace an additional logging provider which lurked in by the recent 3.9.0 upgrade."
STORM-3949,libthrift 0.18.1,
STORM-3945,Upgrade Zookeeper to 3.9.0 / Curator 5,"Currently used Zookeeper version is EOL. 

We should upgrade to a more modern version of the 3.x series, i.e. 3.8.x

Note: This also involves to update Curator to 5.x, which will remove exhibitor support: https://curator.apache.org/breaking-changes.html"
STORM-3940,Missing dependency prevents Storm from being built,"Storm can't be built because the dependency _pentaho-aggdesigner-algorithm_ is not available. 

This dependency is excluded in some places of the poms but not systematically, excluding it everywhere allows the code to be built."
STORM-3939,Remove JDK 8 support,"Remove JDK-8 Support for source, target and runtime. Lowest supported version will be JDK11"
STORM-3938,Unhandled InterruptedException in Supervisor's close() method,"In the Supervisor class's close() method, we are attempting to close the asyncLocalizer object which can throw an InterruptedException. However, currently, the InterruptedException thrown by asyncLocalizer.close() is not being caught or logged. This is an issue because InterruptedException is a more specific exception that might need to be handled separately from the general Exception catch block.

This can lead to a situation where if an InterruptedException is thrown, we will not have any log information about it, making it difficult to debug the issue.

Proposed solution:

Add a separate catch block for InterruptedException before the general Exception catch block to handle and log the InterruptedException properly.

https://github.com/apache/storm/pull/3554"
STORM-3937,Fix errors/warnings found while generating javadoc in storm-starter,"[INFO] [WARNING] Javadoc Warnings
[INFO] [WARNING] ...../storm/examples/storm-starter/src/jvm/org/apache/storm/starter/tools/SlidingWindowCounter.java:63: warning - Tag @link: reference not found: PeriodicSlidingWindowCounter"
STORM-3936,Fix errors/warnings found while generating javadoc in storm-kafka-client,"{code:java}
[INFO] [WARNING] Javadoc Warnings
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaTuple.java:27: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaTuple.java:27: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicAssigner.java:40: warning - @param argument ""K"" is not a parameter name.
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicAssigner.java:40: warning - @param argument ""V"" is not a parameter name.
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/KafkaBolt.java:71: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/KafkaBolt.java:71: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/KafkaBolt.java:75: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
[INFO] [WARNING] ...../storm/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/KafkaBolt.java:75: warning - Tag @see cannot be used in inline documentation.  It can only be used in the following types of documentation: overview, package, class/interface, constructor, field, method.
 {code}"
STORM-3935,Fix errors/warnings found while generating javadoc in storm-cassandra,"{code:java}
[INFO] [WARNING] Javadoc Warnings
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/context/WorkerCtx.java:73: warning - Tag @link: reference not found: T
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java:40: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/builder/ObjectMapperCqlStatementMapperBuilder.java:33: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/builder/ObjectMapperCqlStatementMapperBuilder.java:33: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java:40: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java:40: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/builder/ObjectMapperCqlStatementMapperBuilder.java:33: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/builder/ObjectMapperCqlStatementMapperBuilder.java:33: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java:40: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java:40: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
[INFO] [WARNING] ...../storm/external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/builder/ObjectMapperCqlStatementMapperBuilder.java:33: warning - Tag @link: reference not found: com.datastax.driver.mapping.annotations.Table
 {code}"
STORM-3933,Fix errors/warnings found while generating javadoc in storm-server,"{code:java}
[INFO] [WARNING] Javadoc Warnings
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java:61: warning - Tag @link: can't find NodeSorter(Cluster, TopologyDetails, NodeSortType) in org.apache.storm.scheduler.resource.strategies.scheduling.sorter.NodeSorter
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ObjectResourcesItem.java:42: warning - Tag @link: reference not found: NodeSorter#sortObjectResourcesCommon(ObjectResourcesSummary, ExecutorDetails, NodeSorter.ExistingScheduleFunc)
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ObjectResourcesItem.java:55: warning - Tag @link: reference not found: NodeSorter#sortObjectResourcesCommon(ObjectResourcesSummary, ExecutorDetails, NodeSorter.ExistingScheduleFunc)
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorter.java:92: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType#GENERIC_RAS
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorter.java:92: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType#DEFAULT_RAS
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorter.java:92: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType#COMMON
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorter.java:92: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorterHostProximity.java:97: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType#GENERIC_RAS
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorterHostProximity.java:97: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType#DEFAULT_RAS
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorterHostProximity.java:97: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType#COMMON
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorterHostProximity.java:97: warning - Tag @link: reference not found: BaseResourceAwareStrategy.NodeSortType
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/utils/EnumUtil.java:33: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/utils/EnumUtil.java:33: warning - @param argument ""U"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java:112: warning - @param argument ""callback"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java:200: warning - Tag @link: reference not found: NodeAssignments
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java:81: warning - @LeaderListenerCallback is an unknown tag.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java:81: warning - @LeaderListenerCallback is an unknown tag.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java:81: warning - @LeaderListenerCallback is an unknown tag.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/metric/timed/TimerDecorated.java:30: warning - Tag @link: reference not found: Timer.Context#stop()
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java:81: warning - @LeaderListenerCallback is an unknown tag.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java:81: warning - @LeaderListenerCallback is an unknown tag.
[INFO] [WARNING] ...../storm/storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java:81: warning - @LeaderListenerCallback is an unknown tag.
 {code}"
STORM-3932,Fix errors/warnings found while generating javadoc in storm-client,"In Storm-client
{code:java}
[INFO] [WARNING] Javadoc Warnings
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:51: warning - Tag @see:illegal character: ""123"" in ""{@link ShellLogHandler#setUpContext}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:51: warning - Tag @see:illegal character: ""64"" in ""{@link ShellLogHandler#setUpContext}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:66: warning - Tag @see:illegal character: ""123"" in ""{@link ShellLogHandler#log}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:66: warning - Tag @see:illegal character: ""64"" in ""{@link ShellLogHandler#log}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/Config.java:139: warning - @deprecated: is an unknown tag.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/Config.java:875: warning - Tag @link: reference not found: org.apache.storm.scheduler.multitenant.MultitenantScheduler
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/Config.java:875: warning - Tag @link: reference not found: org.apache.storm.scheduler.multitenant.MultitenantScheduler
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/Config.java:875: warning - Tag @link: reference not found: org.apache.storm.scheduler.resource.ResourceAwareScheduler
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/Config.java:875: warning - Tag @link: reference not found: org.apache.storm.scheduler.resource.strategies.scheduling.RoundRobinResourceAwareStrategy
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/Config.java:1275: warning - @Deprecated is an unknown tag -- same as a known tag except for case.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/Config.java:1282: warning - @Deprecated is an unknown tag -- same as a known tag except for case.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/ILocalCluster.java:82: warning - @param argument ""topologyName"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/ILocalCluster.java:206: warning - @param argument ""steps"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java:319: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java:339: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStateStorage.java:69: warning - @return tag cannot be used in method with void return type.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:68: warning - Tag @link: reference not found: LeaderListenerCallback
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:68: warning - Tag @link: reference not found: LeaderListenerCallback
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:156: warning - @deprecated: is an unknown tag.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:184: warning - @deprecated: is an unknown tag.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:191: warning - @deprecated: is an unknown tag.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:198: warning - @deprecated: is an unknown tag.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:205: warning - @deprecated: is an unknown tag.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:68: warning - Tag @link: reference not found: LeaderListenerCallback
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:68: warning - Tag @link: reference not found: LeaderListenerCallback
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/blobstore/BlobStore.java:262: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Pair.java:53: warning - @param argument ""T1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Pair.java:53: warning - @param argument ""T2"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:65: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:77: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:91: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:105: warning - @param argument ""A"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:105: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:186: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:202: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:202: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:222: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:238: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:238: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:258: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:274: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:274: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:294: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:310: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:310: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:358: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:372: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/PairStream.java:391: warning - @param argument ""V1"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:107: warning - @param argument ""K"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:107: warning - @param argument ""V"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:135: warning - @param argument ""K"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:135: warning - @param argument ""V"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:195: warning - @param argument ""A"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:195: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:214: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/Stream.java:366: warning - @param argument ""V"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/StreamBuilder.java:109: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/StreamBuilder.java:124: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/StreamBuilder.java:138: warning - @param argument ""K"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/StreamBuilder.java:138: warning - @param argument ""V"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/StreamBuilder.java:153: warning - @param argument ""K"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/StreamBuilder.java:153: warning - @param argument ""V"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/operations/CombinerAggregator.java:33: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/operations/CombinerAggregator.java:33: warning - @param argument ""R"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/operations/StateUpdater.java:31: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/streams/operations/StateUpdater.java:31: warning - @param argument ""S"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:51: warning - Tag @see:illegal character: ""123"" in ""{@link ShellLogHandler#setUpContext}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:51: warning - Tag @see:illegal character: ""64"" in ""{@link ShellLogHandler#setUpContext}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:66: warning - Tag @see:illegal character: ""123"" in ""{@link ShellLogHandler#log}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:66: warning - Tag @see:illegal character: ""64"" in ""{@link ShellLogHandler#log}""
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:51: warning - Tag @see: reference not found: {@link ShellLogHandler#setUpContext}
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java:66: warning - Tag @see: reference not found: {@link ShellLogHandler#log}
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/Utils.java:857: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/utils/VersionedStore.java:34: warning - @param argument ""The"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java:453: warning - Tag @link: reference not found: SerializationFactory.IdDictionary
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/trident/operation/ITridentResource.java:24: warning - Tag @see: reference not found: ResourceDeclarer
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/trident/Stream.java:535: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/trident/Stream.java:575: warning - @param argument ""T"" is not a parameter name.
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/metric/api/IMetric.java:26: warning - Tag @link: can't find handleDataPoints(org.apache.storm.metric.api.IMetricsConsumer
[INFO] [WARNING] .TaskInfo,
[INFO] [WARNING] java.util.Collection) in org.apache.storm.metric.api.IMetricsConsumer
[INFO] [WARNING] ...../storm/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java:68: warning - Tag @link: reference not found: LeaderListenerCallback
 {code}"
STORM-3925,Allow user resources (in WorkerTopologyContext) to be set by Worker Hooks,"The current implementation of WorkerTopologyContext in WorkerState will always lead to empty userResources as no interface exposes a way to allow user to set them.
{code:java}
    private Map<String, Object> makeUserResources() {
        /* TODO: need to invoke a hook provided by the topology, giving it a chance to create user resources.
         * this would be part of the initialization hook
         * need to separate workertopologycontext into WorkerContext and WorkerUserContext.
         * actually just do it via interfaces. just need to make sure to hide setResource from tasks
         */
        return new HashMap<>();
    } {code}
The intention will be to expose the relevant methods under a separate class which can then allow users to set the resources from WorkerHooks while only providing get access from Tasks (i.e. via TopologyContext)"
STORM-3924,Support for declaring WorkerHook in Flux topology definitions,"Support for declaring WorkerHook in TopologyBuilder was added. Related JIRA: https://issues.apache.org/jira/browse/STORM-126 (related PR: [#884|https://github.com/apache/storm/pull/884]).

 

Add support for the aforementioned feature for topology definitions submitted as Flux yaml."
STORM-3923,Cassandra module fails tests probably OOM,After switchover to Git Actions.
STORM-3919,"Upgrade Hadoop to version 3 (+ depending frameworks: Hive, HBase)",There are several fixes and enhancements in Hadoop version 3.0.0 that Storm can benefit from.
STORM-3899,"MIgrate dev-tools travis scripts to ""gitact"" after github actions are fully functional",
STORM-3898,Add Integration Test Github Action,Need a github action for integration test since travis CI has been discontinued.
STORM-3896,Create github actions since Travis is no longer supported,
STORM-3895,Add JDK-17 to build pipeline in preparation of removing JDK8,Security fix for testng upgrade to 7.7.0 breaks JDK8 build. Need to deprecate JDK8 build and add JDK17 build.
STORM-3894,Bump snakeyaml from 1.32 to 2.0,"Suggestion by dependabot in PR: [https://github.com/apache/storm/pull/3516]

But the compile fails - so further code change is required."
STORM-3888,HdfsBlobStoreFile set wrong permission for file,"{code:java}
    public OutputStream getOutputStream() throws IOException {
        FsPermission fileperms = new FsPermission(BLOBSTORE_FILE_PERMISSION);
        try {
            out = fileSystem.create(path, (short) this.getMetadata().get_replication_factor());
            fileSystem.setPermission(path, fileperms);
            fileSystem.setReplication(path, (short) this.getMetadata().get_replication_factor());
        } catch (IOException e) {
           ......
            out = fileSystem.create(path, (short) this.getMetadata().get_replication_factor());
            fileSystem.setPermission(path, dirperms);
            fileSystem.setReplication(path, (short) this.getMetadata().get_replication_factor());
        }
        ......
    }
{code}
We can see that there are permission settings for path in both try and catch, but the permission in catch is different from that in try. In catch, the permission `dirperms` is given to the file. I think there is a problem here, and it should be the same as The permissions in try are consistent.

Permissions should be set according to the following code
{code:java}
    public OutputStream getOutputStream() throws IOException {
        FsPermission fileperms = new FsPermission(BLOBSTORE_FILE_PERMISSION);
        try {
            out = fileSystem.create(path, (short) this.getMetadata().get_replication_factor());
            fileSystem.setPermission(path, fileperms);
            fileSystem.setReplication(path, (short) this.getMetadata().get_replication_factor());
        } catch (IOException e) {
           ......
            out = fileSystem.create(path, (short) this.getMetadata().get_replication_factor());
            fileSystem.setPermission(path, fileperms);
            fileSystem.setReplication(path, (short) this.getMetadata().get_replication_factor());
        }
        ......
    }
{code}"
STORM-3885,Security fix upgrade com.fasterxml.jackson.core:jackson-databind to 2.15.2,
STORM-3884,Bump calcite-core from 1.14.0 to 1.32.0,"h1. Bump calcite-core from 1.14.0 to 1.32.0

Also fix this error with 1.32.0
{code:java}
[INFO] --- fmpp-maven-plugin:1.0:generate (generate-fmpp-sources) @ storm-sql-core ---
- Executing: Parser.jj
log4j:WARN No appenders could be found for logger (freemarker.cache).
log4j:WARN Please initialize the log4j system properly.
  !!! FAILED
[ERROR] FMPP processing session failed.
Caused by: freemarker.core.InvalidReferenceException: The following has evaluated to null or missing:
==> default  [in template ""Parser.jj"" at line 1965, column 26]


----
Tip: If the failing expression is known to be legally refer to something that's sometimes null or missing, either specify a default value like myOptionalVar!myDefault, or use <#if myOptionalVar??>when-present<#else>when-missing</#if>. (These only cover the last step of the expression; to cover the whole expression, use parenthesis: (myOptionalVar.foo)!myDefault, (myOptionalVar.foo)??
----


----
FTL stack trace (""~"" means nesting-related):
	- Failed at: #list (parser.joinTypes!default.parse...  [in template ""Parser.jj"" at line 1965, column 1]
----
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  8.338 s
[INFO] Finished at: 2022-10-21T10:25:31-07:00 {code}

Offending lines in generated Parser.jj (in target/codegen/templates/Parser.jj) is 
{code}{
    (
    LOOKAHEAD(3) // required for ""LEFT SEMI JOIN"" in Babel
<#list (parser.joinTypes!default.parser.joinTypes) as method>
        joinType = ${method}()
    |
</#list>
{code}
"
STORM-3881,How to dynamically update variable information in memory after the cluster starts storm,"_I want to dynamically update some of the memory information in the Bolt processing logic, but the number and concurrency of Bolts are too large. Another way of thinking, can it be achieved by changing the memory of each worker process?_"
STORM-3880,Why does Apache package in tgz like examples/**/*.java? ,
STORM-3876,Cannot Compile from Master,"After a long search I landed here and wonder if there are any requirements for compiling and building the master branch (except JDK > 7 ; I use 11).
I have loaded the master branch from Storm into IntelliJ and only get error messages during the build process running `clean`and `compile` 
{code:java}
...
...
[INFO] ----------------< org.apache.storm:storm-maven-plugins >----------------
[INFO] Building storm-maven-plugins 2.5.0-SNAPSHOT                       [8/68]
[INFO] ----------------------------[ maven-plugin ]----------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.4.1:enforce (enforce-maven-version) @ storm-maven-plugins ---
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.0.0:check (validate) @ storm-maven-plugins ---
[INFO] Beginne Prüfung...
Prüfung beendet.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (process-resource-bundles) @ storm-maven-plugins ---
[INFO] 
[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ storm-maven-plugins ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/joao/IdeaProjects/storm/storm-buildtools/storm-maven-plugins/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ storm-maven-plugins ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 2 source files to /home/joao/IdeaProjects/storm/storm-buildtools/storm-maven-plugins/target/classes
[INFO] 
[INFO] -------------------< org.apache.storm:storm-client >--------------------
[INFO] Building Storm Client 2.5.0-SNAPSHOT                              [9/68]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Storm 2.5.0-SNAPSHOT:
[INFO] 
[INFO] Storm .............................................. SUCCESS [  4.470 s]
[INFO] Apache Storm - Checkstyle .......................... SUCCESS [  0.096 s]
[INFO] Shaded Deps for Storm Client ....................... SUCCESS [  0.737 s]
[INFO] multilang-javascript ............................... SUCCESS [  0.101 s]
[INFO] multilang-python ................................... SUCCESS [  0.070 s]
[INFO] multilang-ruby ..................................... SUCCESS [  0.063 s]
[INFO] maven-shade-clojure-transformer .................... SUCCESS [  0.672 s]
[INFO] storm-maven-plugins ................................ SUCCESS [  0.736 s]
[INFO] Storm Client ....................................... FAILURE [  0.015 s]
[INFO] storm-server ....................................... SKIPPED
[INFO] storm-clojure ...................................... SKIPPED
[INFO] Storm Core ......................................... SKIPPED
[INFO] Storm Webapp ....................................... SKIPPED
[INFO] storm-clojure-test ................................. SKIPPED
[INFO] storm-submit-tools ................................. SKIPPED
[INFO] storm-autocreds .................................... SKIPPED
[INFO] storm-hdfs ......................................... SKIPPED
[INFO] storm-hdfs-blobstore ............................... SKIPPED
[INFO] storm-hdfs-oci ..................................... SKIPPED
[INFO] storm-hbase ........................................ SKIPPED
[INFO] storm-hive ......................................... SKIPPED
[INFO] storm-jdbc ......................................... SKIPPED
[INFO] storm-redis ........................................ SKIPPED
[INFO] storm-eventhubs .................................... SKIPPED
[INFO] storm-elasticsearch ................................ SKIPPED
[INFO] storm-solr ......................................... SKIPPED
[INFO] storm-metrics ...................................... SKIPPED
[INFO] storm-cassandra .................................... SKIPPED
[INFO] storm-mqtt ......................................... SKIPPED
[INFO] storm-mongodb ...................................... SKIPPED
[INFO] storm-kafka-client ................................. SKIPPED
[INFO] storm-kafka-migration .............................. SKIPPED
[INFO] storm-opentsdb ..................................... SKIPPED
[INFO] storm-kafka-monitor ................................ SKIPPED
[INFO] storm-kinesis ...................................... SKIPPED
[INFO] storm-jms .......................................... SKIPPED
[INFO] storm-pmml ......................................... SKIPPED
[INFO] storm-rocketmq ..................................... SKIPPED
[INFO] blobstore-migrator ................................. SKIPPED
[INFO] Storm Integration Test ............................. SKIPPED
[INFO] flux ............................................... SKIPPED
[INFO] flux-wrappers ...................................... SKIPPED
[INFO] flux-core .......................................... SKIPPED
[INFO] flux-examples ...................................... SKIPPED
[INFO] storm-sql-runtime .................................. SKIPPED
[INFO] storm-sql-core ..................................... SKIPPED
[INFO] storm-sql-kafka .................................... SKIPPED
[INFO] storm-sql-redis .................................... SKIPPED
[INFO] storm-sql-mongodb .................................. SKIPPED
[INFO] storm-sql-hdfs ..................................... SKIPPED
[INFO] sql ................................................ SKIPPED
[INFO] storm-starter ...................................... SKIPPED
[INFO] storm-loadgen ...................................... SKIPPED
[INFO] storm-mongodb-examples ............................. SKIPPED
[INFO] storm-redis-examples ............................... SKIPPED
[INFO] storm-opentsdb-examples ............................ SKIPPED
[INFO] storm-solr-examples ................................ SKIPPED
[INFO] storm-kafka-client-examples ........................ SKIPPED
[INFO] storm-jdbc-examples ................................ SKIPPED
[INFO] storm-hdfs-examples ................................ SKIPPED
[INFO] storm-hbase-examples ............................... SKIPPED
[INFO] storm-hive-examples ................................ SKIPPED
[INFO] storm-elasticsearch-examples ....................... SKIPPED
[INFO] storm-mqtt-examples ................................ SKIPPED
[INFO] storm-pmml-examples ................................ SKIPPED
[INFO] storm-jms-examples ................................. SKIPPED
[INFO] storm-rocketmq-examples ............................ SKIPPED
[INFO] Storm Perf ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  7.559 s
[INFO] Finished at: 2022-07-18T18:48:33+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to parse plugin descriptor for org.apache.storm:storm-maven-plugins:2.5.0-SNAPSHOT (/home/joao/IdeaProjects/storm/storm-buildtools/storm-maven-plugins/target/classes): No plugin descriptor found at META-INF/maven/plugin.xml -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginDescriptorParsingExceptionProcess finished with exit code 1
 {code}"
STORM-3872,Workaround for build failure with maven version >= 3.8.1,"When maven version at or above 3.8.1 is used to build storm, the build fails because repositories with ""http"" is blocked and only ""https"" is allowed.

 

Fix this by adding .mvn directory workaround, like discussed here [https://stackoverflow.com/questions/66980047/maven-build-failure-dependencyresolutionexception] or remove dependency on http repositories."
STORM-3863,tirupathi trip from chennai,"Padmavathi Travels T.Nagar Provides Chennai to tirupati Car Packages and Services at best Price. *Our Tirupati Tour Package by car* includes all the customer requirments, We are operating Daily Tirupati Balaji Darshan from Chennai for more than 23+ years. Padmavathi Travels chennai is considered as one of the best travel agents in chennai.

https://padmavathitravels.com/index.amp.shtml"
STORM-3858,Bump hadoop-common from 2.8.5 to 2.10.1,"PR created by dependabot: https://github.com/apache/storm/pull/3468
Fix license file(s)"
STORM-3852,Storm 1.2.4 Vulnerability in Grype Scan,"[ Grype|https://github.com/anchore/grype] scan done on Storm 1.2.4 distribution identifies several vulnerabilities due dependent jars of several modules. Please refer to attached xls workbook for a detailed listing.

Summary of all CVEs are as below. Mitigating critical and high vulnerabilities are much needed for production deployment of storm. Please investigate and advise how the critical and high defects can be addressed at minimum.


||Severity||Count||
|Critical|63|
|High|122|
|Medium|43|
|Low|7|

*NOTE* : Over 90% of reported issues are originating from Storm external folder artifacts. Without considering artifacts in external folder the reported summary is as below.
||Severity||Count||
|Critical|14|
|High|31|
|Medium|24|
|Low|4|

 "
STORM-3845,Upgrade activemq version from 5.15.3 to 5.18.2,"Upgrade activemq version from 5.15.3 to 5.18.2 in storm-mqtt and storm-jms modules and remove the older guava dependency.

Requires JDK11."
STORM-3840,log4j vulnerability,"Hi Team,

 

When we ran our vulnerability scanner we found following components has log4j vulnerability

lib/jetty-servlets-9.4.14.v20181114.jar
lib/kafka-clients-0.11.0.3.jar
lib-tools/sql/core/protobuf-java-3.1.0.jar
lib-tools/sql/runtime/calcite-core-1.14.0.jar
lib-tools/sql/runtime/guava-16.0.1.jar
lib-tools/sql/runtime/guava-16.0.1.jar
lib-webapp/dropwizard-validation-1.3.5.jar
lib-webapp/dropwizard-validation-1.3.5.jar
lib-webapp/hibernate-validator-5.4.2.Final.jar
lib-webapp/hibernate-validator-6.0.17.Final.jar
lib-webapp/hibernate-validator-6.0.17.Final.jar
lib-webapp/jakarta.el-3.0.2.jar

 

Required versions to resolve vulnerabilities :

 

jetty-servlets > 9.4.41.v20210516
kafka-clients > 2.1.1
protobuf-java > 3.4.0
calcite-core > 1.26.0
guava > 30.0
dropwizard-validation > 1.3.21
hibernate-validator > 6.0.20
jakartha-el > 3.0.4

 

is there any procedure to follow to resolve this vulnerability issue while changing the required libraries in the given storm version? or Apache Storm team is planning to release a new version of Storm which handles the vulnerability issues?

 

Kindly let is know your feedback so that we can either upgrade the given packages under the current version of storm we have or we download the newer version of storm which implicitly handles this issue.

 

Thanks in advance

 

Regards,

Adarsh"
STORM-3833,Migrate to JUnit5 and remove JUnit4,"JUnit4 is old and works upto JDK 1.7. pom.xml contains org.junit.vintage to provide compatibility with JUnit4 classes. The tests break with running with more recent versions of JDK - for example with openjdk-17. The vintage classes cannot detect the Test methods.

Remove the vintage junit, migrate fully to junit5."
STORM-3830,exclude all old log4j,
STORM-3827,upgrade jetty to 9.4.45 due to cve,https://mvnrepository.com/artifact/org.eclipse.jetty/jetty-server
STORM-3826,upgrade commons-io due to security issue,https://github.com/advisories/GHSA-gwrp-pvrq-jmwv
STORM-3825,update libthrift due to security vulnerability,"https://github.com/advisories/GHSA-g2fg-mr77-6vrm

 

 "
STORM-3823,Release Prep fails in storm-client/test/py/test_storm_cli.py,"When running the release preparation script via:
      *mvn release:prepare -P dist,rat,externals,examples*
the python test script *storm-client/test/py/test_storm_cli.py* fails because the command generated by storm.py is slightly different than what the test script expects.

In particular, directories are added to java classpath with a ""/*"". This is correct.
However, the test script does not expect this.

Fix the test script storm-client/test/py/test_storm_cli.py to append ""/*"" to directories that contain jar files."
STORM-3820,storm uses jackson version that has a security issue (entity expansion),"[https://github.com/apache/storm/blob/master/pom.xml#L342] (v2.10.0)

[https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-25649]

Using v2.10.5.1 will fix this issue with fewer risks than upgrading jackson to 2.13.x."
STORM-3816,Unrecognized VM option 'PrintGCDateStamps',"When starting storm using the official docker images [https://hub.docker.com/_/storm] following the listed example, then deploying a topology - the worker does not come up (logs inside of the supervisor):
{code:java}
2022-01-10 14:24:14.803 STDERR Thread-0 [INFO] Unrecognized VM option 'PrintGCDateStamps'
2022-01-10 14:24:14.803 STDERR Thread-1 [INFO] [0.001s][warning][gc] -Xloggc is deprecated. Will use -Xlog:gc:artifacts/gc.log instead.
2022-01-10 14:24:14.811 STDERR Thread-0 [INFO] Error: Could not create the Java Virtual Machine.
2022-01-10 14:24:14.811 STDERR Thread-0 [INFO] Error: A fatal exception has occurred. Program will exit. {code}"
STORM-3814,"storm-core: Remediate log4j critical vulnerabilities -> 2.16.0 or newer, prefer 2.17.1","* [https://logging.apache.org/log4j/2.x/security.html]

 

*In order to remediate these bugs with Log4j, please update Storm 2.3.0 and 1.2.3* 
 * Criticals
 ** Fixed in Log4j 2.16.0 (Java 8) and Log4j 2.12.2 (Java 7)
 *** [CVE-2021-45046|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-45046]: Apache Log4j2 Thread Context Lookup Pattern vulnerable to remote code execution in certain non-default configurations
 ** Fixed in Log4j 2.15.0 (Java 8)
 *** [CVE-2021-44228|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228]: Apache Log4j2 JNDI features do not protect against attacker controlled LDAP and other JNDI related endpoints.
 * Moderates
 ** Fixed in Log4j 2.17.1 (Java 8), 2.12.4 (Java 7) and 2.3.2 (Java 6)
 *** [CVE-2021-44832|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44832]: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.
 ** Fixed in Log4j 2.17.0 (Java 8), 2.12.3 (Java 7) and 2.3.1 (Java 6)
 *** [CVE-2021-45105|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-45105]: Apache Log4j2 does not always protect from infinite recursion in lookup evaluation

 "
STORM-3813,Build Failure,"I follow the getting started steps at 

[https://github.com/apache/storm/tree/v2.3.0/examples/storm-starter]

download storm source , 

install maven 3.8.4 , and java openJdk 1.8.0_292

run mvn clean install and it failed , in order to fix it i modify root pom.xml 

 

attached below the repositories section that works 

    <repositories>
        <repository>
            <releases><enabled>true</enabled></releases>
            <snapshots><enabled>false</enabled></snapshots>
            <id>central</id>
            <url>https://repo1.maven.org/maven2/</url>
        </repository>
        <repository>
            <releases><enabled>true</enabled></releases>
            <snapshots><enabled>false</enabled></snapshots>
            <id>clojars</id>
            <url>https://clojars.org/repo/</url>
        </repository>
        <repository>
          <id>repository.jboss.org-public</id>
          <url>https://repository.jboss.org/nexus/content/groups/public</url>
          <releases><enabled>true</enabled></releases>
          <snapshots><enabled>false</enabled></snapshots>
     </repository>
     <repository>
       <id>maven.restlet.org</id>
       <url>https://maven.restlet.talend.com</url>
       <releases><enabled>true</enabled></releases>
       <snapshots><enabled>false</enabled></snapshots>
    </repository>
    </repositories>
 

 

 

 "
STORM-3812,Storm release packages log4j v1,"log4j v1 is at it's EOL, but due to some implicit package references in maven, some tools/libs is still packaging log4j. All latest releases are all being impacted. 

 

Packages impacted:
 * storm-autocreds
 * storm-kafka-monitor

 

It would be good to fix/release this together with log4j v2 recent CVEs, thus vulnerability scan will be clear for log4j vulnerability.

 "
STORM-3810,CVE-2021-44228 Log4J vulnerability,"Recent critical CVE about Log4J ([https://www.cvedetails.com/cve/CVE-2021-44228/)] affects Storm.

Please upgrade to latest Log4j2 >= 2.16.0 (see [https://search.maven.org/artifact/org.apache.logging.log4j/log4j/2.16.0/pom)] in 1.2.X Storm branch and also in 2.X.X Storm branches.

Thank you!"
STORM-3809,CVE-2021-44228 Log4Shell: upgrade log4j2,"Recent critical CVE about log4shell ([https://www.cvedetails.com/cve/CVE-2021-44228/)] affects Storm. (Eg: in Storm 2.2.0, it uses log4j-api-2.11.2.jar) Any log4j2 between 2.0 and 2.14 is affected.

 

I did not found any issue or news related to Apache Storm and a fix. So I create this ticket to track it down.

Please upgrade to latest Log4j2 >= 2.16.0 (see [https://search.maven.org/artifact/org.apache.logging.log4j/log4j/2.16.0/pom)] in both 2.2.X and 2.3.X Storm branches. Thank you!

 "
STORM-3808,Bump log4j version to 2.16.0 (original ticket was 2.15.0),"For CVE-2021-44228 to bump log4j 2.15.0
{code:java}
News
CVE-2021-44228

The Log4j team has been made aware of a security vulnerability, CVE-2021-44228, that has been addressed in Log4j 2.15.0.

Log4j’s JNDI support has not restricted what names could be resolved. Some protocols are unsafe or can allow remote code execution. Log4j now limits the protocols by default to only java, ldap, and ldaps and limits the ldap protocols to only accessing Java primitive objects by default served on the local host.

One vector that allowed exposure to this vulnerability was Log4j’s allowance of Lookups to appear in log messages. As of Log4j 2.15.0 this feature is now disabled by default. While an option has been provided to enable Lookups in this fashion, users are strongly discouraged from enabling it.

For those who cannot upgrade to 2.15.0, in releases >=2.10, this behavior can be mitigated by setting either the system property log4j2.formatMsgNoLookups or the environment variable LOG4J_FORMAT_MSG_NO_LOOKUPS to true. For releases >=2.7 and <=2.14.1, all PatternLayout patterns can be modified to specify the message converter as %m{nolookups} instead of just %m. For releases >=2.0-beta9 and <=2.10.0, the mitigation is to remove the JndiLookup class from the classpath: zip -q -d log4j-core-*.jar org/apache/logging/log4j/core/lookup/JndiLookup.class.
{code}"
STORM-3805,change retrying blob update log message to WARN,"We should be able to search supervisor logs for actual errors. This seems like it should be a WARN:

 "
STORM-3800,Fix Resocue leak due to Files.list and Files.walk,"Files.list and Files.walk will open dir stream, we should close it.

 

see jdk:

the {[@code|https://github.com/code] try}-with-resources construct should be used to ensure that the
stream's {[@link|https://github.com/link] Stream#close close} method is invoked after the stream
operations are completed."
STORM-3796,How many topologies can create in a cluster?,"This is not a bug. I want to know the topologies behaviour. I read many sites related to Storm's topologies design setup. But, I didn't get clarity.

In my project, I am going to processing more than a million records. So, I planned to create topologies dynamically based on internal modules. The count might be reached more than a thousand. My doubt is what is the best way to manage topologies? How many topologies can be created in a single cluster? Are there any problems with maintaining multiple topologies?"
STORM-3795,storm list displays logs and execution information,"We are using storm list command to get the list of running topology along with its statuses. The execution response shows few log statements and then a table containing the topology details.

Is there any option available in list command to suppress the meta information (Running:... and logs) and only show the topology details.

 
{code:java}
$ sudo /opt/storm/bin/storm list
 Running: java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/opt/storm -Dstorm.log.dir=/opt/storm/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib:/usr/lib64 -Dstorm.conf.file= -cp /opt/storm/:/opt/storm/lib/:/opt/storm/extlib/:/opt/storm/extlib-daemon/:/opt/storm/conf:/opt/storm/bin org.apache.storm.command.ListTopologies
 11:18:10.808 [main] INFO o.a.s.v.ConfigValidation - Will use [class org.apache.storm.DaemonConfig, class org.apache.storm.Config] for validation
 11:18:10.897 [main] WARN o.a.s.v.ConfigValidation - task.heartbeat.frequency.secs is a deprecated config please see class org.apache.storm.Config.TASK_HEARTBEAT_FREQUENCY_SECS for more information.
 11:18:10.999 [main] INFO o.a.s.u.NimbusClient - Found leader nimbus : hostname:6627
 Topology_name Status Num_tasks Num_workers Uptime_secs Topology_Id Owner 
 ----------------------------------------------------------------------------------------
 TestJob_1 ACTIVE 6 1 5263482 TestJob_1-13-1627046008 xyz 
 ${code}
 "
STORM-3794,storm list displays logs and execution information,"We are using storm list command to get the list of running topology along with its statuses. The execution response shows few log statements and then a table containing the topology details.

Is there any option available in list command to suppress the meta information (Running:... and logs) and only show the topology details.

 
{code:java}
[vagrant@agent1 bin]$ sudo /opt/storm/bin/storm list
 Running: java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/opt/storm -Dstorm.log.dir=/opt/storm/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib:/usr/lib64 -Dstorm.conf.file= -cp /opt/storm/:/opt/storm/lib/:/opt/storm/extlib/:/opt/storm/extlib-daemon/:/opt/storm/conf:/opt/storm/bin org.apache.storm.command.ListTopologies
 11:18:10.808 [main] INFO o.a.s.v.ConfigValidation - Will use [class org.apache.storm.DaemonConfig, class org.apache.storm.Config] for validation
 11:18:10.897 [main] WARN o.a.s.v.ConfigValidation - task.heartbeat.frequency.secs is a deprecated config please see class org.apache.storm.Config.TASK_HEARTBEAT_FREQUENCY_SECS for more information.
 11:18:10.999 [main] INFO o.a.s.u.NimbusClient - Found leader nimbus : agent1.corp.com:6627
 Topology_name Status Num_tasks Num_workers Uptime_secs Topology_Id Owner 
 ----------------------------------------------------------------------------------------
 TestJob_1 ACTIVE 6 1 5263482 TestJob_1-13-1627046008 xyz 
 [vagrant@agent1 bin]${code}{code}
 "
STORM-3792,Change pom.xml to use more test JVM threads without reuse,"Maven surefire plugin configuration has two important flags:

  (1) reuseForks - setting of true mean reuse created JVMs.

  (2) forkCount - number of JVMs to create for testing. Pure number is an absolute count, whereas 1.0C means same as the number of CPUs.

Reusing forked JVMs can cause somewhat indeterminate failures in test when global class instances are not properly initialized or cleaned. An example of such a singleton class Time. 

Not reusing will cause a slowdown in tests. 

To mitigate this slowdown, wherever the ""forkCount=1"", change it to ""forkCount=1.0C"" and add reuseForks=false if not already present. However, some modules (example storm-hdfs) forkCount cannot be increased from 1 to 1.0C, due to either resources limits or global locking (as in hdfs tests). "
STORM-3784,my supervisor will shut down on 2:00 am everyday,"The cluster has one nimbus and two supervisors.one of the supervisors is alone with nimbus.

I deployed two topology that PradarLinkTopology and PradarLogTopology.

PradarLogTopology run with 4 workers.PradarLinkTopology run with 1 workers.

on 2:00 am everyday, all supervisors will shut down,i havn't find out the reason.

I try to clean up the status directory,but the problem still exsit.

this is my supervisor.log
{code:java}
//代码占位符
2021-07-21 02:03:42.070 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:Error occurred during initialization of VM2021-07-21 02:03:42.070 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:Error occurred during initialization of VM2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:java.lang.Error: Properties init: Could not determine current working directory.2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28: at java.lang.System.initProperties(Native Method)2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28: at java.lang.System.initializeSystemClass(System.java:1166)2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:2021-07-21 02:03:42.323 o.a.s.d.s.BasicContainer SLOT_6702 [INFO] Removed Worker ID dcae9231-4be4-4842-9ed0-988e1b8a2b282021-07-21 02:03:42.329 o.a.s.d.s.Slot SLOT_6702 [INFO] STATE kill msInState: 68588 topo:PradarLogTopology-3-1626751922 worker:null -> empty msInState: 32021-07-21 02:03:42.329 o.a.s.d.s.Slot SLOT_6702 [INFO] SLOT 6702: Changing current assignment from LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:1, task_end:1)], resources:WorkerResources(mem_on_heap:256.0, mem_off_heap:0.0, cpu:20.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=256.0, cpu.pcore.percent=20.0}, shared_resources:{}), owner:root) to null2021-07-21 02:03:42.353 o.a.s.d.s.Supervisor pool-10-thread-1 [WARN] Topology config is not localized yet...2021-07-21 02:03:42.449 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700 all processes are dead...2021-07-21 02:03:42.449 o.a.s.d.s.Container SLOT_6700 [INFO] Cleaning up 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86:b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:42.450 o.a.s.d.s.Container SLOT_6700 [INFO] GET worker-user for b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:42.450 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/pids/163262021-07-21 02:03:43.322 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/26b5ffbd-08b6-46df-aa04-6b86f78b8ad8/pids2021-07-21 02:03:43.322 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/26b5ffbd-08b6-46df-aa04-6b86f78b8ad8/tmp2021-07-21 02:03:45.209 o.a.s.d.s.BasicContainer Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28 exited with code: 12021-07-21 02:03:45.224 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.224 o.a.s.d.s.Supervisor pool-10-thread-7 [WARN] Topology config is not localized yet...2021-07-21 02:03:45.224 o.a.s.d.s.Container SLOT_6701 [INFO] REMOVE worker-user 26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.224 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/heartbeats2021-07-21 02:03:45.224 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers-users/26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.224 o.a.s.t.ProcessFunction pool-10-thread-7 [ERROR] Internal error processing sendSupervisorWorkerHeartbeatorg.apache.storm.utils.WrappedNotAliveException: PradarLinkTopology-2-1626337413 does not appear to be alive, you should probably exit at org.apache.storm.daemon.supervisor.Supervisor$1.sendSupervisorWorkerHeartbeat(Supervisor.java:442) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:374) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:353) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:174) [storm-client-2.1.0.jar:2.1.0] at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.1.0.jar:2.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_201] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_201] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]2021-07-21 02:03:45.225 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/pids2021-07-21 02:03:45.225 o.a.s.d.s.BasicContainer Thread-16 [INFO] Worker Process b7963273-452a-43af-bc00-d814e0629f96 exited with code: 2542021-07-21 02:03:45.225 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/tmp2021-07-21 02:03:45.226 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.226 o.a.s.d.s.Container SLOT_6700 [INFO] REMOVE worker-user b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.226 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers-users/b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.227 o.a.s.d.s.BasicContainer SLOT_6701 [INFO] Removed Worker ID 26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.228 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Removed Worker ID b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.229 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE kill msInState: 81385 topo:PradarLogTopology-3-1626751922 worker:null -> empty msInState: 02021-07-21 02:03:45.229 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Changing current assignment from LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:3, task_end:3)], resources:WorkerResources(mem_on_heap:128.0, mem_off_heap:0.0, cpu:10.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=128.0, cpu.pcore.percent=10.0}, shared_resources:{}), owner:root) to null2021-07-21 02:03:45.230 o.a.s.d.s.Slot SLOT_6701 [INFO] STATE kill-and-relaunch msInState: 95356 topo:PradarLogTopology-3-1626751922 worker:null -> waiting-for-blob-localization msInState: 12021-07-21 02:03:45.231 o.a.s.d.s.Slot SLOT_6701 [INFO] SLOT 6701: Changing current assignment from LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:3, task_end:3)], resources:WorkerResources(mem_on_heap:128.0, mem_off_heap:0.0, cpu:10.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=128.0, cpu.pcore.percent=10.0}, shared_resources:{}), owner:root) to null2021-07-21 02:03:45.231 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE empty msInState: 2 -> waiting-for-blob-localization msInState: 02021-07-21 02:03:45.232 o.a.s.d.s.Slot SLOT_6701 [ERROR] Error when processing eventjava.io.FileNotFoundException: File '/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLinkTopology-4-1626751925/stormconf.ser' does not exist at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:303) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:464) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:298) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:351) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:452) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:440) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:228) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:931) [storm-server-2.1.0.jar:2.1.0]2021-07-21 02:03:45.234 o.a.s.u.Utils SLOT_6701 [ERROR] Halting process: Error when processing an eventjava.lang.RuntimeException: Halting process: Error when processing an event at org.apache.storm.utils.Utils.exitProcess(Utils.java:512) [storm-client-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:978) [storm-server-2.1.0.jar:2.1.0]2021-07-21 02:03:45.235 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Created Worker ID 68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.236 o.a.s.d.s.Container SLOT_6700 [INFO] Setting up 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86:68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.236 o.a.s.d.s.Container SLOT_6700 [INFO] GET worker-user for 68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.240 o.a.s.d.s.Container SLOT_6700 [INFO] SET worker-user 68102ac7-a341-4d84-b1aa-db0f72934f99 root2021-07-21 02:03:45.241 o.a.s.d.s.Container SLOT_6700 [INFO] Creating symlinks for worker-id: 68102ac7-a341-4d84-b1aa-db0f72934f99 storm-id: PradarLogTopology-3-1626751922 for files(1): [resources]2021-07-21 02:03:45.241 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Launching worker with assignment LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:1, task_end:1)], resources:WorkerResources(mem_on_heap:256.0, mem_off_heap:0.0, cpu:20.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=256.0, cpu.pcore.percent=20.0}, shared_resources:{}), owner:root) for this supervisor 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86 on port 6700 with id 68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.243 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Launching worker with command: '/usr/local/java/bin/java' '-cp' '/data/apache-storm-2.1.0/lib-worker/*:/data/apache-storm-2.1.0/extlib/*:/data/apache-storm-2.1.0/conf:/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/stormjar.jar' '-Xmx64m' '-Dlogging.sensitivity=S3' '-Dlogfile.name=worker.log' '-Dstorm.home=/data/apache-storm-2.1.0' '-Dworkers.artifacts=/data/apache-storm-2.1.0/logs/workers-artifacts' '-Dstorm.id=PradarLogTopology-3-1626751922' '-Dworker.id=68102ac7-a341-4d84-b1aa-db0f72934f99' '-Dworker.port=6700' '-Dstorm.log.dir=/data/apache-storm-2.1.0/logs' '-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector' '-Dstorm.local.dir=/data/apache-storm-2.1.0/status' '-Dworker.memory_limit_mb=256' '-Dlog4j.configurationFile=/data/apache-storm-2.1.0/log4j2/worker.xml' 'org.apache.storm.LogWriter' '/usr/local/java/bin/java' '-server' '-Dlogging.sensitivity=S3' '-Dlogfile.name=worker.log' '-Dstorm.home=/data/apache-storm-2.1.0' '-Dworkers.artifacts=/data/apache-storm-2.1.0/logs/workers-artifacts' '-Dstorm.id=PradarLogTopology-3-1626751922' '-Dworker.id=68102ac7-a341-4d84-b1aa-db0f72934f99' '-Dworker.port=6700' '-Dstorm.log.dir=/data/apache-storm-2.1.0/logs' '-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector' '-Dstorm.local.dir=/data/apache-storm-2.1.0/status' '-Dworker.memory_limit_mb=256' '-Dlog4j.configurationFile=/data/apache-storm-2.1.0/log4j2/worker.xml' '-Xmx256m' '-XX:+PrintGCDetails' '-Xloggc:artifacts/gc.log' '-XX:+PrintGCDateStamps' '-XX:+PrintGCTimeStamps' '-XX:+UseGCLogFileRotation' '-XX:NumberOfGCLogFiles=10' '-XX:GCLogFileSize=1M' '-XX:+HeapDumpOnOutOfMemoryError' '-XX:HeapDumpPath=artifacts/heapdump' '-Xms2g' '-Xmx2g' '-XX:MaxDirectMemorySize=512m' '-XX:+HeapDumpOnOutOfMemoryError' '-XX:HeapDumpPath=java.hprof' '-XX:MetaspaceSize=256m' '-XX:MaxMetaspaceSize=256m' '-XX:-OmitStackTraceInFastThrow' '-Djava.library.path=/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/resources/Linux-amd64:/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/resources:/usr/local/lib:/opt/local/lib:/usr/lib:/usr/lib64' '-Dstorm.conf.file=' '-Dstorm.options=' '-Djava.io.tmpdir=/data/apache-storm-2.1.0/status/workers/68102ac7-a341-4d84-b1aa-db0f72934f99/tmp' '-cp' '/data/apache-storm-2.1.0/lib-worker/*:/data/apache-storm-2.1.0/extlib/*:/data/apache-storm-2.1.0/conf:/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/stormjar.jar' 'org.apache.storm.daemon.worker.Worker' 'PradarLogTopology-3-1626751922' '8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86' '6628' '6700' '68102ac7-a341-4d84-b1aa-db0f72934f99'. 2021-07-21 02:03:45.243 o.a.s.u.Utils Thread-5 [INFO] Halting after 1 seconds2021-07-21 02:03:45.244 o.a.s.d.s.Supervisor Thread-6 [INFO] Shutting down supervisor 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86
{code}"
STORM-3777,Ability to have a default doubling of resource when worker count increases,"Let's say we have a topology with the following executors configurations
{code:java}
reader: 1
bolt1: 2
bolt2: 4
bolt3: 4
Topology Memory: 1 GB
noOfWorkers: 1

{code}
When we submit the topology the bolts and readers are provided this many executors but when we change the noOfWorkers to say 2. The topology still is keeping the executors the same for the bolts though there is a second worker process.

Is it possible to have a default behavior or a policy configuration like an autoscale of API. If 1 Worker is present it would take this but when more than 1 worker is present it would double these by default (or a policy) without expecting to be provided manually again.

This way the configuration remains same and only the noOfWorkers keep changing for scale out and scale in.

This is related to the previous issue of simplified Auto Scaling Auto-Scaling Resources in a Topology

If this is present, a signal to increase or decrease the workers can be provided with lessor configuration changes.

 

 

 "
STORM-3776,Ability to change the configuration of a topology without restarting,"Have gone through this issue of Autoscale https://issues.apache.org/jira/browse/STORM-594.

We are trying a simpler approach for scaling in and out.

We create a topic in Kafka and would want to start processing the messages in the topology by having one worker.

As the lag in topic in Kafka increases, we want to be able to intimate Storm nimbus to change the topology workers and the number of executors in the bolts. Once this is done the topology should detect this change and scale out without needing a restart. Once we monitor the Kafka lag coming down, we want to send the scale in signal to the Storm nimbus to change back.

Currently due to no support for dynamic changes, we are forced to restart the topology when configuration changes...

 

 

 

 "
STORM-3773,Worker Reassignment - Difference between Storm 2.x  and Storm 1.x,"We are currently on Storm 1.2.1 and was in the process of upgrading it to Storm 2.2.0
 Observed the below while upgrading it to 2.2.0:

1) In a storm cluster (4 nodes) with 8 topologies running  ( with a mapping of 1-1 between worker and topologies), when i bring down nimbus,supervisor in one of the node (let's say Node 1, which is not nimbus leader) the workers running on that node gets reassigned to other 3, even though it is running on that node (Node 1). So i have 2 worker process for the same topology running at the same time ( saw the behaviour with or without using pacemaker). The worker process does get killed when nimbus and supervisor is brought up in Node 1

2) Observed from worker logs that it sends heartbeat to local supervisor and nimbus leader , which with 1.2.1 used to happen using Zookeeper ( i saw this behaviour in 2.2.0 with or without using Pacemaker). 
 If i bring down nimbus and supervisor on node where nimbus is a leader, it reassigns worker processes and in some cases leads to zombie worker processess ( is not killed when storm kill is executed)

These above behaviour (reassignment of worker) doesn't happen with Storm 1.2.1

Since this is a fundamental design change between 1.x and 2.x , are there any documentation which describes it in detail? ( couldn't find from Release Notes)

(I am raising this as a bug because its preventing us from moving to 2.2.0 due to the issue mentioned in 2) )

 "
STORM-3772,"With pacemaker, zNode /strom/workerbeats entries are not removed once topology is killed","With Pacemaker storage strategy, 

When topology is submitted, there is entry created inside zNode /strom/workerbeats. But same is not deleted when topology is killed. Because of this we are ending up having all old topology entries in this zNode.

Since pacemaker is taking care of worker heartbeat, shouldn't we stop adding entries in zNode /strom/workerbeats when topology submitted in case of pacemaker storage strategy?"
STORM-3771,Execute doCleanup in its own timer thread without lock,"_Sometimes doCleanup can take a long time (several minutes) while holding submitLock. This slows down other ""submitLock"" dependent activities. Execute doCleanup without a lock and within its own timer thread. Ignore exception that may occur inside doCleanup() method._"
STORM-3770,Workaround for Issue STORM-3677 (assignment was null) in version 2.2.0,"Please suggest workaround for the issue STORM-3677 in version 2.2.0.

https://issues.apache.org/jira/browse/STORM-3677

 

Is there any other way to kill stale/orphan worker?



We are getting below exception-
2021-05-10 10:42:36.369 o.a.s.d.w.WorkerState refresh-connections-timer [WARN] Failed to read assignment. This should only happen when topology is shutting down.
 java.lang.RuntimeException: Failed to read worker assignment. Supervisor client threw exception, and *assignment in Zookeeper was null*
 at org.apache.storm.daemon.worker.WorkerState.getLocalAssignment(WorkerState.java:665) ~[storm-client-2.2.0.jar:2.2.0]
 at org.apache.storm.daemon.worker.WorkerState.refreshConnections(WorkerState.java:389) ~[storm-client-2.2.0.jar:2.2.0]
 
Worker is not listed in Storm UI, hence we are doing manual intervention every time to kill the process. 
As process is not releasing port, there is topology imbalance happening and load is going to only few supervisors. Attached screenshot."
STORM-3766,Update org.apache.calcite version to 1.26.0 due to security vulnerability,"Update calcite version to 1.26.0 to avoid ""Missing Authentication for Critical Function in Apache Calcite"" as detailed here https://github.com/advisories/GHSA-hxp5-8pgq-mgv9"
STORM-3764,Nimbus has internal scheduling errors when backtracking,"While backtracking in scheduling code, Ackers are skipped. However, under certain circumstances a null WorkerSlot is passed as parameter to SchedulingSearcherState.backtrack(). This null key is used to retrieve an entry from Map. The returned null map value causes NPE. 

{code}
2021-03-30 15:27:03.212 o.a.s.s.r.ResourceAwareScheduler timer [ERROR] mx3_test-5-1614799784 Internal Error - Exception thrown when scheduling. Please check logs for details java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_262]
    at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_262]
    at org.apache.storm.scheduler.resource.ResourceAwareScheduler.scheduleTopology(ResourceAwareScheduler.java:191) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.scheduler.resource.ResourceAwareScheduler.schedule(ResourceAwareScheduler.java:132) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.scheduler.blacklist.BlacklistScheduler.schedule(BlacklistScheduler.java:131) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:2314) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.daemon.nimbus.Nimbus.lambda$lockingMkAssignments$44(Nimbus.java:2493) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.daemon.nimbus.Nimbus.executeWithLock(Nimbus.java:4549) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.daemon.nimbus.Nimbus.lockingMkAssignments(Nimbus.java:2491) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2478) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2415) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$17(Nimbus.java:1420) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.3.0.y.jar:2.3.0.y]

Caused by: java.lang.NullPointerException
    at org.apache.storm.scheduler.resource.strategies.scheduling.SchedulingSearcherState.backtrack(SchedulingSearcherState.java:292) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.scheduler.resource.strategies.scheduling.BaseResourceAwareStrategy.scheduleExecutorsOnNodes(BaseResourceAwareStrategy.java:559) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.scheduler.resource.strategies.scheduling.BaseResourceAwareStrategy.schedule(BaseResourceAwareStrategy.java:172) ~[storm-server-2.3.0.y.jar:2.3.0.y]
    at org.apache.storm.scheduler.resource.ResourceAwareScheduler.lambda$scheduleTopology$1(ResourceAwareScheduler.java:189) ~[storm-server-2.3.0.y.jar:2.3.0.y]     
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_262]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_262]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_262]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_262]
{code}"
STORM-3762,Set a default character set in InputStreamReader to solve potential garbled problems,"When a InputStreamReader is used, the parameter setting of a default character set is recommended to solve potential garbled problems.
 "
STORM-3761,Apache Storm 2.2.0,"Does Apache Storm 2.2.0 work with Python 3.8 ? 

I get a message saying 'version 2.6 is needed. "
STORM-3760,Storm 2.2.0 not reporting newWorkerEvents metric,"Hi everyone,
  
 We have recently migrated from Storm 0.10.0 to Storm 2.2.0, we have a custom _StatsdMetricConsumer_ which implements _IMetricsConsumer_ interface. 
  
 Storm is still reporting some metrics (__transfer-count,_ _ack-count,_ _metrics, etc)_ but it seems after the migration it stopped reporting _*newWorkerEvents*_ metric. I made sure this is not a problem in our implementation by logging all the metrics received _handleDataPoints_ method.
  
 Is this a known issue? any way to get that metric fixed?

Best regards, thanks."
STORM-3756,"build failed on AArch64, Fedora 33 ","[jw@cn05 storm]$ mvn install -DskipTests
...
[INFO] -------------------< org.apache.storm:storm-metrics >-------------------
[INFO] Building storm-metrics 2.3.0-SNAPSHOT                            [26/67]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-enforcer-plugin:1.4.1:enforce (enforce-maven-version) @ storm-metrics ---
[INFO]
[INFO] --- maven-antrun-plugin:1.8:run (prepare) @ storm-metrics ---
[WARNING] Parameter tasks is deprecated, use target instead
[INFO] Executing tasks

main:
     [echo] Downloading sigar native binaries...
      [get] Getting: https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/magelan/hyperic-sigar-1.6.4.zip
      [get] To: /home/jw/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip
      [get] Error getting https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/magelan/hyperic-sigar-1.6.4.zip to /home/jw/.m2/repository/org/fusesource/sigar/1.6.4/hyperic-sigar-1.6.4.zip
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Storm 2.3.0-SNAPSHOT:
[INFO]
[INFO] Storm .............................................. SUCCESS [ 18.997 s]
[INFO] Apache Storm - Checkstyle .......................... SUCCESS [  1.841 s]
[INFO] Shaded Deps for Storm Client ....................... SUCCESS [ 21.133 s]
[INFO] multilang-javascript ............................... SUCCESS [  0.741 s]
[INFO] multilang-python ................................... SUCCESS [  0.561 s]
[INFO] multilang-ruby ..................................... SUCCESS [  0.588 s]
[INFO] maven-shade-clojure-transformer .................... SUCCESS [  2.142 s]
[INFO] storm-maven-plugins ................................ SUCCESS [  6.387 s]
[INFO] Storm Client ....................................... SUCCESS [01:45 min]
[INFO] storm-server ....................................... SUCCESS [ 48.387 s]
[INFO] storm-clojure ...................................... SUCCESS [ 13.658 s]
[INFO] Storm Core ......................................... SUCCESS [ 17.257 s]
[INFO] Storm Webapp ....................................... SUCCESS [ 15.816 s]
[INFO] storm-clojure-test ................................. SUCCESS [ 10.183 s]
[INFO] storm-submit-tools ................................. SUCCESS [  2.640 s]
[INFO] storm-autocreds .................................... SUCCESS [ 14.750 s]
[INFO] storm-hdfs ......................................... SUCCESS [ 21.546 s]
[INFO] storm-hdfs-blobstore ............................... SUCCESS [ 13.625 s]
[INFO] storm-hbase ........................................ SUCCESS [ 16.407 s]
[INFO] storm-hive ......................................... SUCCESS [ 20.343 s]
[INFO] storm-jdbc ......................................... SUCCESS [  2.764 s]
[INFO] storm-redis ........................................ SUCCESS [  3.598 s]
[INFO] storm-eventhubs .................................... SUCCESS [  3.151 s]
[INFO] storm-elasticsearch ................................ SUCCESS [  3.772 s]
[INFO] storm-solr ......................................... SUCCESS [  4.058 s]
[INFO] storm-metrics ...................................... FAILURE [  0.564 s]
[INFO] storm-cassandra .................................... SKIPPED
[INFO] storm-mqtt ......................................... SKIPPED
[INFO] storm-mongodb ...................................... SKIPPED
[INFO] storm-kafka-client ................................. SKIPPED
[INFO] storm-kafka-migration .............................. SKIPPED
[INFO] storm-opentsdb ..................................... SKIPPED
[INFO] storm-kafka-monitor ................................ SKIPPED
[INFO] storm-kinesis ...................................... SKIPPED
[INFO] storm-jms .......................................... SKIPPED
[INFO] storm-pmml ......................................... SKIPPED
[INFO] storm-rocketmq ..................................... SKIPPED
[INFO] blobstore-migrator ................................. SKIPPED
[INFO] Storm Integration Test ............................. SKIPPED
[INFO] flux ............................................... SKIPPED
[INFO] flux-wrappers ...................................... SKIPPED
[INFO] flux-core .......................................... SKIPPED
[INFO] flux-examples ...................................... SKIPPED
[INFO] storm-sql-runtime .................................. SKIPPED
[INFO] storm-sql-core ..................................... SKIPPED
[INFO] storm-sql-kafka .................................... SKIPPED
[INFO] storm-sql-redis .................................... SKIPPED
[INFO] storm-sql-mongodb .................................. SKIPPED
[INFO] storm-sql-hdfs ..................................... SKIPPED
[INFO] sql ................................................ SKIPPED
[INFO] storm-starter ...................................... SKIPPED
[INFO] storm-loadgen ...................................... SKIPPED
[INFO] storm-mongodb-examples ............................. SKIPPED
[INFO] storm-redis-examples ............................... SKIPPED
[INFO] storm-opentsdb-examples ............................ SKIPPED
[INFO] storm-solr-examples ................................ SKIPPED
[INFO] storm-kafka-client-examples ........................ SKIPPED
[INFO] storm-jdbc-examples ................................ SKIPPED
[INFO] storm-hdfs-examples ................................ SKIPPED
[INFO] storm-hbase-examples ............................... SKIPPED
[INFO] storm-hive-examples ................................ SKIPPED
[INFO] storm-elasticsearch-examples ....................... SKIPPED
[INFO] storm-mqtt-examples ................................ SKIPPED
[INFO] storm-pmml-examples ................................ SKIPPED
[INFO] storm-jms-examples ................................. SKIPPED
[INFO] storm-rocketmq-examples ............................ SKIPPED
[INFO] Storm Perf ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  06:14 min
[INFO] Finished at: 2021-03-17T15:05:13+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.8:run (prepare) on project storm-metrics: An Ant BuildException has occured: java.net.UnknownHostException: storage.googleapis.com
[ERROR] around Ant part ...<get skipExisting=""true"" src=""https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/magelan/hyperic-sigar-1.6.4.zip"" dest=""${settings.localRepository}/org/fusesource/sigar/1.6.4/""/>... @ 5:210 in /home/jw/apache/storm/external/storm-metrics/target/antrun/build-main.xml: Unknown host storage.googleapis.com
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :storm-metrics
[jw@cn05 storm]$ "
STORM-3754,Upgrade Guava version because of security vulnerability,"storm-hdfs-examples and storm-hive-examples use com.google.guava:guava:16.0.1
This has know vulnerability https://nvd.nist.gov/vuln/detail/CVE-2018-10237

""Unbounded memory allocation in Google Guava 11.0 through 24.x before 24.1.1 allows remote attackers to conduct denial of service attack.""

The guava version downgrade was required earlier because of hadoop-hdfs 2.6.1.
Since storm is now using hadoop-hdfs 2.8.5, this downgrade may not be necessary.

It is possible that the a separate jar may need to be added as dependency com.google.guava:failureaccess:1.0. See https://github.com/google/guava/releases around Oct 18, 2018 when Guava version 27.0 was released. Note that Hadoop HDFS 2.8.5 was released on Sep 8, 2018 (i.e. before the guava version 27.0)."
STORM-3753,Change String.getBytes() to DFSUtil.string2Bytes(String) to avoid Unsupported Encoding Exception,"Hello,
I found that DFSUtil.string2Bytes(String) can be used here instead of String.getBytes(). Otherwise, the API String.getBytes() may cause potential risk of UnsupportedEncodingException since the behavior of this method when the string cannot be encoded in the default charset is unspecified. One recommended API is DFSUtil.string2Bytes(String) which provides more control over the encoding process and can avoid this exception."
STORM-3750,Deactivation throws java.nio.channels.ClosedSelectorException in KafkaSpout,"When deactivating a topology that uses a kafka spout, the following exception is thrown:
{code:java}
java.lang.RuntimeException: java.nio.channels.ClosedSelectorExceptionjava.lang.RuntimeException: java.nio.channels.ClosedSelectorException at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:70) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.daemon.executor$fn__10727$fn__10742$fn__10773.invoke(executor.clj:634) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [storm-core-1.2.2.jar:1.2.2] at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]Caused by: java.nio.channels.ClosedSelectorException at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:83) ~[?:1.8.0_181] at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) ~[?:1.8.0_181] at org.apache.kafka.common.network.Selector.select(Selector.java:499) ~[stormjar.jar:?] at org.apache.kafka.common.network.Selector.poll(Selector.java:308) ~[stormjar.jar:?] at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:349) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:226) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:188) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.Fetcher.retrieveOffsetsByTimes(Fetcher.java:408) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.Fetcher.beginningOrEndOffset(Fetcher.java:451) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.Fetcher.beginningOffsets(Fetcher.java:436) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:1473) ~[stormjar.jar:?] at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[stormjar.jar:?] at org.apache.storm.daemon.executor$metrics_tick$fn__10651.invoke(executor.clj:345) ~[storm-core-1.2.2.jar:1.2.2] at clojure.core$map$fn__4553.invoke(core.clj:2622) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?] at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?] at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?] at clojure.core$filter$fn__4580.invoke(core.clj:2679) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?] at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.7.0.jar:?] at clojure.lang.RT.next(RT.java:674) ~[clojure-1.7.0.jar:?] at clojure.core$next__4112.invoke(core.clj:64) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6523.invoke(protocols.clj:170) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6478$G__6473__6487.invoke(protocols.clj:19) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?] at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?] at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.executor$metrics_tick.invoke(executor.clj:349) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.daemon.executor$fn__10727$tuple_action_fn__10733.invoke(executor.clj:522) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.daemon.executor$mk_task_receiver$fn__10716.invoke(executor.clj:471) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.disruptor$clojure_handler$reify__10135.onEvent(disruptor.clj:41) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.2.jar:1.2.2] ... 7 more{code}
Problem with that is that it leads to the worker dying. "
STORM-3747,Remove deprecated NodeSortTypes and related code,"The following classes are no longer in use and should be removed.
(1) NodeSorter
(2) DefaultResourceAwareStrategyOld
(3) GenericResourceAwareStrategyOld
(4) NodeSorterType

Unused/deprecated methods in BaseResourceAwareStrategy can be removed when made obsolete by removal of classes above.
"
STORM-3746,Outline steps to anonymize topologies for TestLargeCluster,"TestLargeCluster can various tests against simulated large clusters. Topology names and their components are anonymized so as to not reveal proprietary information.

Steps on creating these simulation should be documented so users can create them for  contribution (or just for internal testing). "
STORM-3745,Exclude blacklisted hosts when scheduling,Blacklisted hosts are not excluded when sorting nodes via NodeSorter[HostProximity]
STORM-3738,Add Authorization for listBlobs?,
STORM-3736,remove topologyId and worker port from V2 metrics API,the topologyId and port are now available to the StormMetricsRegistry and should be removed from the existing metric API
STORM-3734,IntegerValidator doesn't force the object type to be Integer,"The IntegerValidator allows the non-integer object, like Double(1.0).
https://github.com/apache/storm/blob/7bef73a6faa14558ef254efe74cbe4bfef81c2e2/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java#L404-L415

It can be reproduced by 

{code:java}
        IntegerValidator validator = new IntegerValidator();
        validator.validateInteger(""test"", 1.0);
{code}

More details at https://github.com/apache/storm/pull/3365#issuecomment-754775896
"
STORM-3732,Fix test test-cluster in storm-core: org.apache.storm.scheduler-test ,"Asserts in TopologyDetails.initConfigs fail the test ""test-cluster"" in storm-core: org.apache.storm.scheduler-test "
STORM-3730,Remove PMD Exceptions,"At top level. run ""mvn install -DskipTests"" or ""mvn pmd:pmd"".

Redirect to file to capture output. Approximately 200 PMDExceptions. 

 "
STORM-3726,JCQueue::overflowQ can use an highly concurrent variant of JCTools, The overflowQ used on https://github.com/apache/storm/blob/7bef73a6faa14558ef254efe74cbe4bfef81c2e2/storm-client/src/jvm/org/apache/storm/utils/JCQueue.java can use the xadd variant on the very last JCTools variant if the use assume an high concurrent usage scenario
STORM-3716,Nodes underutilized,"Topologies employing anchored tuples do not distribute across multiple nodes, regardless of the computation demands of the bolts. It works fine on a single node, but when throwing multiple nodes into the mix, only one machine gets pegged. When we disable anchoring, it will distribute across all nodes just fine, pegging each machine appropriately.

This bug manifests from version 2.1 forward. I first encountered this issue with my own production cluster on an app that does significant NLP computation across hundreds of millions of documents. This topology is fairly complex, so I developed a very simple exemplar that demonstrates the issue with only one spout and bolt. I pushed this demonstration up to github to provide the developers with a mechanism to easily isolate the bug, and maybe provide some workaround. I used gradle to build this simple topology and software and package the results. This code is well documented, so it should be fairly simple to reproduce the issue. I first encountered this issue on 3 32 core nodes, but when I started experimenting, I set up a test cluster with 8 cores, and then I increased each node to 16 cores, and plenty of memory in every case.

The topology can be accessed from github at https://github.com/cowchipkid/storm-issue.git <https://github.com/cowchipkid/storm-issue.git>."
STORM-3713,Possible race condition between zookeeper sync-up and killing topology,"When nimbus re-gains leadership, the leaderCallback will sync-up with zookeeper:

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java#L106] [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java#L212]  

When killing topology, both zookeeper and in-memory assignments map get cleaned up.

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L313]  

However, in the syncRemoteAssignments call, it will get the information from zookeeper into stormIds. The after some processing (including deserialization), it will then put it into local in-memory assignments backend. If the zookeeper deletion happens between these two steps, then there will be mismatch between remote zookeeper and local backends.  


We found this issue since we observed a NPE when making assignments.
{code:java}
2020-11-04 19:56:17.703 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event java.lang.RuntimeException: java.lang.NullPointerException at
org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$17(Nimbus.java:1419) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.StormTimer$1.run(StormTimer.java:110) ~[storm-client-2.3.0.y.jar:2.3.0.y] at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.3.0.y.jar:2.3.0.y] Caused by: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.HeartbeatCache.getAliveExecutors(HeartbeatCache.java:199) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.daemon.nimbus.Nimbus.aliveExecutors(Nimbus.java:2029) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.daemon.nimbus.Nimbus.computeTopologyToAliveExecutors(Nimbus.java:2109) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:2272) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lockingMkAssignments(Nimbus.java:2467) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2453) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2397) ~[storm-server-2.3.0.y.jar:2.3.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$17(Nimbus.java:1415) ~[storm-server-2.3.0.y.jar:2.3.0.y] ... 2 more 2020-11-04 19:56:17.703 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event  

{code}
[https://github.com/apache/storm/blob/fe2f7102e244336e288d26f2dde8089198ee4c33/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2108]  

The existingAssignment comes from in-memory backend while the topologyToExecutors comes from zookeeper which did not include a deleted topolgy id. [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2108] [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2111|https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2108] [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/HeartbeatCache.java#L199]

So NPE happens.      "
STORM-3710,Storm error/timeout handling when using trident topology with window,"My question is basically as described in an open stackoverflow question:

[https://stackoverflow.com/questions/64575976/storm-error-timeout-handling-when-using-trident-topology-with-window]

Nobody accrually answered this, though it would be really helpfull to understand what goes behind the scene when trident topology timeout/failure happends, and the questions found in the github issue.

Please help :)"
STORM-3705,Storm UI and nimbus CLI not working,"When deploying a topology on nimbus, there were errors in topology because of wrong configuration, due to which every request was failing in the topology. In our code, there is this logic that if a topology observes error more than a particular threshold, then it will issue storm deactivate topology command to nimbus.

The restart of topologies was being done via script.

The scenario was:
2 topologies were restarted successfully but facing errors due to wrong configuration. Because of the same, deactivate command was submitted to nimbus.
3rd topology was killed successfully and command for topology submission for the same was received successfully by nimbus.

At this point, storm UI stopped responding completely. When tried to run kill command via CLI on nimbus, it didn't work either and stayed stuck.

At this point, the 2 topologies with errors were still running as deactivation was not successful via nimbus. And the 3rd topology wasn't restarted via nimbus.

Any other commands ran on nimbus, stayed at stuck state until the nimbus was stopped on the leader machine and another nimbus was made leader.

There are no helpful logs in nimbus or supervisors or worker logs of affected topologies, apart from zookeeper info logs below:
{color:#FF0000}zookeeper [INFO] exceptionorg.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /errors/<AFFECTED-TOPOLOGY-ID>/<BOLTNAME-WITH_ERRORS>/e0000001494

{color}Storm version: 1.2.1"
STORM-3703,Add cpu throttle metrics,"cgroup metrics like 

 
{code:java}
cat cpu.stat
nr_periods 17435
nr_throttled 17435
throttled_time 4986881206991

{code}
seems useful. Storm should report them.

 "
STORM-3702,Change thrift exception classes to contain getMessage() method,"Thrift classes are currently being generated without camel casing for method names. And the exception classes have a member variable called ""msg"". This generates, get_msg() and set_msg() method names. Since this class extends java Exception class, when an TException is thrown, the getMessage() method returns null and cannot be relied upon.

To get around this problem with getMessage() method, storm code has Wrapper classes, eg WrappedNotAliveException with a getMessage() method that return wraps get_msg() method in base class.

Proposed Change:
 * Change the TException classes to rename member variable and generate camel cased class so that getMessage() is generated
 * Limit the change to specific TException classes only to avoid large change
 * Deprecate Wrapped classes"
STORM-3700,"Storm UI functionality ""Stop flight recording"" doesn't work properly","The code is implemented at
[https://github.com/apache/storm/blob/36204eda00bca7e03ac3979d9c0d3527d1f08330/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java#L841-L878]

 

JPROFILE_STOP is used for both start flight recording and stop flight recording. The logic in the code is that if there is already a JPROFILE_STOP with the same topoId and request content (host, port, timestamp), the JPROFILE_STOP becomes a stop command; otherwise, it is a start command.

But the problem is every time when we invoke stop on UI:

[https://github.com/apache/storm/blob/3fb289b87c7d72bfe01ee1c7028adbc69f012439/storm-webapp/src/main/java/org/apache/storm/daemon/ui/resources/StormApiResource.java#L621-L631]

 
the request is actually configured with timestamp=0:

[https://github.com/apache/storm/blob/bb199d574eae337d0512670dcc4957f3c7ef4922/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L2320]

so it will never equal to any request in the pending profile action. So stop will never happen
 "
STORM-3698,AbstractHdfsBolt does not sync Writers that are purged,"We just discovered when using a SequenceFileBolt (although it might happen with other implementations as well) that the writers it uses, held in the map AbstractHdfsBolt.writers are not closed/synced when they are removed from the map by the removeEldestEntry method.

This leads to data loss.

Can be reproduced by creating a SequenceFileBolt.withMaxOpenFiles(1) and writing to just two different files.

One will have a size of zero, the other has the data in it. "
STORM-3689,Storm build fails when obtaining third party license,"Travis build fails. Ths log shows the following:
{code:java}
// code placeholder
[INFO] Downloading from clojars: https://clojars.org/repo/org/ow2/asm/asm/5.0.3/asm-5.0.3-third-party.properties
[ERROR] Failed to execute goal org.codehaus.mojo:license-maven-plugin:2.0.0:aggregate-add-third-party (generate-and-check-licenses) on project storm: could not execute goal AggregatorAddThirdPartyMojo for reason : ArtifactResolutionException: Unable to locate third party descriptor: Could not transfer artifact org.ow2.asm:asm:properties:third-party:5.0.3 from/to sonatype (https://oss.sonatype.org/content/repositories/releases/): Transfer failed for https://oss.sonatype.org/content/repositories/releases/org/ow2/asm/asm/5.0.3/asm-5.0.3-third-party.properties: Connect to repo1.maven.org:443 [repo1.maven.org/199.232.64.209] failed: Connection timed out (Connection timed out) -> [Help 1]
 
{code}
In fact there is no artifact for [https://oss.sonatype.org/content/repositories/releases/org/ow2/asm/asm/5.0.3/asm-5.0.3-third-party.properties]

Its web site for ASM indicates that it is covered by  BSD 3-Clause License.

It appears that THIRD-PARTY.properties file needs an entry for ASM version 5.0.3.

Not sure why this build failure started 15 days ago."
STORM-3688,Copy storm.daemon.metrics.reporter.plugin.* configs from Config class to DaemonConfig class and deprecate them in Config class,"After the change in STORM-3670, the following configs 

{code:java}
 public static final String STORM_DAEMON_METRICS_REPORTER_PLUGIN_LOCALE = ""storm.daemon.metrics.reporter.plugin.locale"";
public static final String STORM_DAEMON_METRICS_REPORTER_PLUGIN_RATE_UNIT = ""storm.daemon.metrics.reporter.plugin.rate.unit"";
public static final String STORM_DAEMON_METRICS_REPORTER_PLUGIN_DURATION_UNIT = ""storm.daemon.metrics.reporter.plugin.duration.unit"";

{code}

are only used in daemon side. We can't move it to DaemonConfig from Config class because if a topology code refers to them directly, it will break (ClassNotFoundException) if the variables are moved out of Config class in a newer version of storm cluster.


A better way is to duplicate them in DaemonConfig and ""@deprecate"" them in Config class.

While doing so, I saw errors unit test (nimbus_test.clj)

{code:java}
[INFO] --- clojure-maven-plugin:1.7.1:test (test-clojure) @ storm-core ---
Exception in thread ""main"" Syntax error compiling at (org/apache/storm/nimbus_test.clj:1:1).
	at clojure.lang.Compiler.load(Compiler.java:7647)
	at clojure.lang.RT.loadResourceScript(RT.java:381)
	at clojure.lang.RT.loadResourceScript(RT.java:372)
	at clojure.lang.RT.load(RT.java:463)
	at clojure.lang.RT.load(RT.java:428)
	at clojure.core$load$fn__6824.invoke(core.clj:6126)
	at clojure.core$load.invokeStatic(core.clj:6125)
	at clojure.core$load.doInvoke(core.clj:6109)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invokeStatic(core.clj:5908)
	at clojure.core$load_one.invoke(core.clj:5903)
	at clojure.core$load_lib$fn__6765.invoke(core.clj:5948)
	at clojure.core$load_lib.invokeStatic(core.clj:5947)
	at clojure.core$load_lib.doInvoke(core.clj:5928)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invokeStatic(core.clj:667)
	at clojure.core$load_libs.invokeStatic(core.clj:5985)
	at clojure.core$load_libs.doInvoke(core.clj:5969)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invokeStatic(core.clj:667)
	at clojure.core$require.invokeStatic(core.clj:6007)
	at clojure.core$require.doInvoke(core.clj:6007)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at org.apache.storm.testrunner$eval194$iter__195__199$fn__200$fn__201.invoke(test_runner.clj:43)
	at org.apache.storm.testrunner$eval194$iter__195__199$fn__200.invoke(test_runner.clj:42)
	at clojure.lang.LazySeq.sval(LazySeq.java:42)
	at clojure.lang.LazySeq.seq(LazySeq.java:51)
	at clojure.lang.RT.seq(RT.java:531)
	at clojure.core$seq__5387.invokeStatic(core.clj:137)
	at clojure.core$dorun.invokeStatic(core.clj:3133)
	at clojure.core$dorun.invoke(core.clj:3133)
	at org.apache.storm.testrunner$eval194.invokeStatic(test_runner.clj:42)
	at org.apache.storm.testrunner$eval194.invoke(test_runner.clj:42)
	at clojure.lang.Compiler.eval(Compiler.java:7176)
	at clojure.lang.Compiler.load(Compiler.java:7635)
	at clojure.lang.Compiler.loadFile(Compiler.java:7573)
	at clojure.main$load_script.invokeStatic(main.clj:452)
	at clojure.main$script_opt.invokeStatic(main.clj:512)
	at clojure.main$script_opt.invoke(main.clj:507)
	at clojure.main$main.invokeStatic(main.clj:598)
	at clojure.main$main.doInvoke(main.clj:561)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.lang.Var.applyTo(Var.java:705)
	at clojure.main.main(main.java:37)
Caused by: java.lang.IllegalStateException: STORM-DAEMON-METRICS-REPORTER-PLUGIN-DURATION-UNIT already refers to: #'org.apache.storm.daemon-config/STORM-DAEMON-METRICS-REPORTER-PLUGIN-DURATION-UNIT in namespace: org.apache.storm.nimbus-test
	at clojure.lang.Namespace.warnOrFailOnReplace(Namespace.java:88)
	at clojure.lang.Namespace.reference(Namespace.java:110)
	at clojure.lang.Namespace.refer(Namespace.java:168)
	at clojure.core$refer.invokeStatic(core.clj:4252)
	at clojure.core$refer.doInvoke(core.clj:4217)
	at clojure.lang.RestFn.invoke(RestFn.java:410)
	at clojure.lang.AFn.applyToHelper(AFn.java:154)
	at clojure.lang.RestFn.applyTo(RestFn.java:132)
	at clojure.core$apply.invokeStatic(core.clj:667)
	at clojure.core$load_lib.invokeStatic(core.clj:5966)
	at clojure.core$load_lib.doInvoke(core.clj:5928)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invokeStatic(core.clj:667)
	at clojure.core$load_libs.invokeStatic(core.clj:5989)
	at clojure.core$load_libs.doInvoke(core.clj:5969)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invokeStatic(core.clj:669)
	at clojure.core$use.invokeStatic(core.clj:6093)
	at clojure.core$use.doInvoke(core.clj:6093)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at org.apache.storm.nimbus_test$eval212$loading__6706__auto____213.invoke(nimbus_test.clj:16)
	at org.apache.storm.nimbus_test$eval212.invokeStatic(nimbus_test.clj:16)
	at org.apache.storm.nimbus_test$eval212.invoke(nimbus_test.clj:16)
	at clojure.lang.Compiler.eval(Compiler.java:7176)
	at clojure.lang.Compiler.eval(Compiler.java:7165)
	at clojure.lang.Compiler.load(Compiler.java:7635)
	... 43 more
{code}

So I filed this as a separate task. And anyone can work on it later.


"
STORM-3686,o.a.s.t.s.AbstractNonblockingServer$FrameBuffer pool-14-thread-10 [ERROR] Unexpected throwable while invoking! java.lang.NullPointerException: null,"Hi Team,

 

Very often I am getting below error in storm nimbus and after that the storm UI loads only with some content.

 

2020-08-05 02:45:01.802 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer pool-14-thread-10 [ERROR] Unexpected throwable while invoking!
 java.lang.NullPointerException: null

 

Attached the storm nimbus and ui error from their logs.

 

Please advise."
STORM-3683,Check for consistency in JVM options for worker launch,"If GC options are specified in topology.worker.childopts, they could conflict with the cluster settings and cause problems launching the JVM. This leads to storm alerts due to failures launching containers.

We could catch these on submission and prevent the topology from launching with an error message.

*Kishor Patil's further comments:*

Submit time validation of JVM Options includes..

Running `java -showversion` kind of command while topology is being submitted using JVM_OPTIONS derived using
 # topology.worker.childopts
 # topology.worker.gc.childopts
 # topology.worker.logwriter.childopts
 # worker.childopts
 # worker.gc.childopts
 # worker.profiler.childopts

And removing replacement strings such as ""%ID%"", ""%WORKER-ID%"", ""%TOPOLOGY-ID%"", ""%WORKER-PORT"", ""%OFF-HEAP-MEM%"", ""%LIMIT-MEM%"" with dummy values. To perform these replacements with dummy values, I would use externalize and use the logic from [BasicContainer#substituteChildOptsInternal|https://git.vzbuilders.com/storm/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java#L440] method."
STORM-3680,Upgrade Jedis library in storm-jedis module to 3.2.x,"To support future improvements to the storm-redis package, lets upgrade its underlying library (Jedis) it uses to communicate with Redis.

 

Upgrading will clear the way for #STORM-3665 as Stream support in the Jedis library was not added until 3.x"
STORM-3678,从StormTopology中获得TopologyBuilder,在Storm项目中，我们可以从TopologyBuilder获得StormTopology，但是能否从StormTopology得到TopologyBuilder呢？或者说，从用户的Storm程序Jar中获得TopologyBuilder呢？
STORM-3672,How does WindowedBoltExecutor handle FailedException?,"After a tuple falls out of the window, the WindowedBoltExecutor does a {{windowedOutputCollector.ack(tuple);}}. However, I could not find what happens when {{boltExecute}} method throws a {{FailedException}}.

{{BasicBoltExecutor}} handles this explicitly like ([this|https://github.com/apache/storm/blob/e7731a547ae113d2fbc0fda6a9f02fc3d78de5fb/storm-client/src/jvm/org/apache/storm/topology/BasicBoltExecutor.java#L50]). I could not find the equivalent handling for {{WindowedBoltExecutor}}. How are the failed tuples handled?  And how are retries managed? Thanks in advance."
STORM-3669,Validation against topology regarding to resource constraints should only be performed when ResourceAwareScheduler is being used,"For example, for https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L3150-L3151

validateTopologyWorkerMaxHeapSizeConfigs shouldn't be performed when a scheduler other than ResourceAwareScheduler is being used.

{code:java}
if (ServerUtils.isRas(conf))
{code}
OR
{code:java}
if (underlyingScheduler instanceof ResourceAwareScheduler)
{code}

can be used for this purpose."
STORM-3668,Nimbus.validateTopologyWorkerMaxHeapSizeConfigs doesn't work properly when a topology doesn't set component heap size explicitly,"This [validateTopologyWorkerMaxHeapSizeConfigs|https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L3150-L3151] takes topoConf to get the default heap size for components. But at this point, the topoConf might not have ""topology.component.resources.onheap.memory.mb"". And in this case, validateTopologyWorkerMaxHeapSizeConfigs just passes.

For example, submission with


{code:java}
storm jar  storm-starter.jar  org.apache.storm.starter.WordCountTopology -c topology.worker.max.heap.size.mb=100  wc
{code}

will succeed while the config ""topology.component.resources.onheap.memory.mb"" is default to 120mb > 100mb.

While submission succeeds, nimbus is not able to schedule the topology.


{code:java}
2020-07-06 21:41:07.376 o.a.s.s.r.s.s.BaseResourceAwareStrategy pool-21-thread-1 [WARN] Not Enough Resources to schedule Task [6, 6] - split Normalized resources: {onheap.memory.mb=128.0, cpu.pcore.percent=10.0, offheap.memory.mb=0.0}
{code}

Note: this is when ResourceAwareScheduler is being used."
STORM-3665,Add support for Redis Streams,"Starting from Redis 5, Redis supports a new data structure call Streams.

As it name suggests Redis Streams add a full low latency stream support on top of Redis [https://redis.io/topics/streams-intro.]

 "
STORM-3664,Nimbus cannot recover from LocalFsBlobStore deletion,"When all Nimbus instances in a cluster loose access to previously stored Blobs while at least one topology is deployed, the cluster cannot recover as none of the nodes is ever elected as leader due to missing blobs. Recovery is only possible when manually removing blob and topology data from Zookeeper.

I understand that the LocalFs blob store implementation is not particularly suited for high availability deployments. However, this issue prevents sensible automated disaster recovery on small deployments where a full deployment of HDFS would not provide any benefits and simply introduce additional complexity.
h3. Reproduction Steps
 # Deploy one or multiple Nimbus instances
 # Deploy a Topology (such as the WordCount example)
 # Stop all Nimbus Instances
 # Remove all Blob directories
 # Start all Nimbus Instances

h3. Expected Behavior

When a topology's blobs are permanently lost, the topology itself should be marked as failed in favor of maintaining the cluster's availability as a single lost topology suffices to take down the entire system."
STORM-3651,Give producerTasks in ExecutorTransferMultiThreadingTest.testExecutorTransfer more time to finish,"Currently in ExecutorTransferMultiThreadingTest, we only wait for 100ms for the producerTasks to finish. Sometimes it is not enough in case of lack of cpu resources

{code:java}
executorService.awaitTermination(100, TimeUnit.MILLISECONDS);
{code}

We should increase it. This is just the maximum wait time."
STORM-3646,Flush only happens on executor main thread,"Flush only happens on executor main thread.
https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java#L70-L74

If topologies have their own threads within bolts, and when

{code:java}
topology.producer.batch.size
topology.transfer.batch.size
{code}

are larger than 1, some tuples will be batched and possible never be sent to downstream.

The default value for them is 1. So it is fine when the configs are not explicitly changed. "
STORM-3645,worker launcher should consistently use ERRORFILE for error messages,
STORM-3643,Bring queue metrics documentation up to date,[https://github.com/apache/storm/blob/master/docs/Metrics.md#queue-metrics] should be updated 
STORM-3640,timed out health check processes should be killed,We noticed some hung health check scripts that were timed up eating CPU.  We should make sure they are killed on timeout.
STORM-3639,Remove asserts from Storm daemon code,"By default jvm disables assertion for performance, and we allow JVM options to be configured at deployment time,  it is best to avoid use of _*{{assert}}*_ statement and use explicit clauses to validate critical checks withing storm-daemon code. Also, it would be helpful in many cases to explicitly log and proceed on failure to meet such assert expectation for debugging purpose."
STORM-3637,Looping topology structure can cause backpressure to deadlock,"When you have a topology structure with loops in it (BoltA and BoltB send tuples to each other), it can cause backpressure to deadlock.

The scenario is that BoltA suddenly takes a long time to process a tuple (in our situation, it's doing a database operation). This causes the task input queue to fill up, setting the backpressure flag.

BoltB, which is sending a tuple to BoltA, then cannot send, and the tuple is held in the emit queue. This blocks any tuples behind it, and also stops BoltB from executing. This means the input queue to BoltB will build up, until that backpressure flag is also set - and then when BoltA next wants to send a tuple to BoltB, it will irrevocably deadlock."
STORM-3636,"Enable SSL credentials auto reload for storm UI, LogViewer and DRPC server",This enables ssl auto reload without restarting servers
STORM-3634,validate numa ports are contained in supervisor.slots.ports,"It's currently possible to have a numa port configured that is not in supervisor.slots.ports.  When a supervisor restarts, any worker running on numa ports not in supervisor.slots.ports will be killed.  We should consider this an invalid configuration."
STORM-3633,Add message that supervisor is killing detached workers,"[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java#L116]


From this code we will see messages that workers are killed, but not the reason why.  We should add a message."
STORM-3632,Reduce SimpleSaslServerCallbackHandler supervisor logging,"This message floods our logs and seems to provide little use:

LOG.info(""Successfully authenticated client: authenticationID = {} authorizationID = {}"",
 nid, zid);"
STORM-3631,Wrong format of logs.users/groups in topology conf can cause supervisor/logviewer to terminate,"If users submit a topology with logs.users set as a single string, it will cause ClassCastException and cause Supervisor to terminate
{code:java}
2020-04-28 19:33:59.901 o.a.s.d.s.Slot SLOT_6707 [ERROR] Error when processing event
java.lang.ClassCastException: java.lang.String cannot be cast to java.util.List
        at org.apache.storm.daemon.supervisor.Container.writeLogMetadata(Container.java:)
{code}

Can be easily reproduced by 

{code:java}
storm jar storm-starter.jar org.apache.storm.starter.WordCountTopology wc -c logs.users=[null, ""fake-groups""] 
{code}


If users submit with logs.users set with a list with null member, for example, logs.users='[null, ""fake-2-users""]', it will cause NullPointerException and cause logviewer to terminate

{code:java}
Caused by: java.lang.NullPointerException
        at org.apache.storm.utils.ObjectReader.getStrings(ObjectReader.java:)
        at org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer.getLogUserGroupWhitelist(ResourceAuthorizer.java)
{code}

Can be easily reproduced by 

{code:java}
storm jar storm-starter.jar org.apache.storm.starter.WordCountTopology wc -c logs.users=""fake-users""
{code}


Ideally logs.users and logs.groups should be daemon config only and we should have something like topology.logs.users and topology.logs.groups for topology level config sit in Config.java (so it can be validated by ConfigValidation).   Because now there two configs are in DaemonConfig, it wont' be validated against ""@isStringList"" rule when it is from topo conf. 

But even with the rule, it doesn't validate when logs.users include a null member in the list. 

For backwards compatibility, we have to fix these configs instead of removing them from topo conf (by adding topology.logs.users).
"
STORM-3630,Remove unwanted check from LocalFsBlobStore,"The LocalFsBlobStore _getBlob_ and _getBlobMeta_, alls synchronized methods _checkForBlobOrDownload_ which do not make sense for local disk."
STORM-3629, Logviewer should always allow admins to access logs,"https://github.com/apache/storm/blob/v2.1.0/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java#L86-L89

Currently if there is any problems with reading worker.yaml, no one can access logs from logviewer, including admins. Admins should always be able to access."
STORM-3628,Nimbus tries Worker HB on non un-deployed topologies,"I am running Storm 2.1.0 version.

After I undeploy topologies and later deploy new topologies, Nimbus log repeats the exception:

Please note: *fetch-inbox-16-1587324922*  is artifact id that does not exist anymore.

The Nimbus log is full of this HBs Exceptions.

org.apache.storm.utils.WrappedNotAliveException: *fetch-inbox-16-1587324922*
 at org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConf(Nimbus.java:987) ~[storm-server-2.1.0.jar:2.1.0]
 at org.apache.storm.daemon.nimbus.Nimbus.sendSupervisorWorkerHeartbeat(Nimbus.java:4727) [storm-server-2.1.0.jar:2.1.0]
 at org.apache.storm.generated.Nimbus$Processor$sendSupervisorWorkerHeartbeat.getResult(Nimbus.java:4817) [storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.generated.Nimbus$Processor$sendSupervisorWorkerHeartbeat.getResult(Nimbus.java:4796) [storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0]"
STORM-3627,Allow use of shortNames for Metrics for worker in Metrics-V2,Adding topology configuration _topology.metrics.use.shortname_ to allow for use of shortnames in case metrics tick is enabled using _topology.enable.v2.metrics.tick_.
STORM-3626,"storm-kafka-migration should pull in storm-client as ""provided"" dependency","https://github.com/apache/storm/blob/master/external/storm-kafka-migration/pom.xml#L34-L39


{code:java}
<dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
        </dependency>

{code}

it is ""compile"" dependency as of now."
STORM-3625,Storm CLI should validate topology name on client side,"Storm commands like kill and rebalance need to use nimbus client to do remote calls using the given topology name.

We should add validation on client side to make potential errors more transparent and prevent unnecessary client-server connections."
STORM-3623,v2 metrics tick reports all worker metrics within each executor,"see https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L335-L341


{code:java}
    private void addV2Metrics(List<IMetricsConsumer.DataPoint> dataPoints) {
        boolean enableV2MetricsDataPoints = ObjectReader.getBoolean(topoConf.get(Config.TOPOLOGY_ENABLE_V2_METRICS_TICK), false);
        if (!enableV2MetricsDataPoints) {
            return;
        }
        StormMetricRegistry stormMetricRegistry = workerData.getMetricRegistry();
{code}


This should be reporting just the metrics for the Executor."
STORM-3622,Race Condition in CachedThreadStatesGaugeSet registered at SystemBolt,"We noticed that with the change in https://github.com/apache/storm/pull/3242, there is a race condition causing NPE.
{code:java}
2020-04-14 18:22:12.997 o.a.s.u.Utils Thread-17-__acker-executor[16, 16] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
 at org.apache.storm.executor.Executor.accept(Executor.java:291) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.utils.JCQueue.consume(JCQueue.java:111) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:172) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.utils.Utils$1.run(Utils.java:434) [storm-client-2.2.0.y.jar:2.2.0.y]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.lang.NullPointerException
 at com.codahale.metrics.jvm.ThreadStatesGaugeSet.getThreadCount(ThreadStatesGaugeSet.java:95) ~[metrics-jvm-3.2.6.jar:3.2.6]
 at com.codahale.metrics.jvm.ThreadStatesGaugeSet.access$000(ThreadStatesGaugeSet.java:20) ~[metrics-jvm-3.2.6.jar:3.2.6]
 at com.codahale.metrics.jvm.ThreadStatesGaugeSet$1.getValue(ThreadStatesGaugeSet.java:56) ~[metrics-jvm-3.2.6.jar:3.2.6]
 at org.apache.storm.executor.Executor.addV2Metrics(Executor.java:344) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.Executor.metricsTick(Executor.java:320) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:218) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.Executor.accept(Executor.java:287) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 ... 6 more
{code}


This is due to a race condition in CachedGauge https://github.com/dropwizard/metrics/blob/v3.2.6/metrics-core/src/main/java/com/codahale/metrics/CachedGauge.java#L49-L53

There are two issues here.
The first one is STORM-3623. 
This makes all the executors to get values for all the metrics. So multiple threads will access the same metric.

So the threads gauges are now accessed by multiple threads. But in CachedGauge,


{code:java}
 @Override
    public T getValue() {
        if (shouldLoad()) {
            this.value = loadValue();
        }
        return value;
    }
{code}

this method is not thread-safe. Two threads can reach to getValue at the same time.
The first thread reaching shouldLoad knows it needs to reload, so it calls the next line this.value=loadValue()
The second thread is a little bit late so shouldLoad returns false. Then it returns the value directly.

There is a race condition between first thread calling loadValue() and the second thread returning value.

If the first thread finishes loadValue() first, both values returned to the threads are the same value (and current value). But if the second thread returns earlier, the second thread gets the original value (which is null ), hence NPE.

To summarize, the second issue is CachedThreadStatesGaugeSet is not thread-safe

To fix this NPE, we should avoid using CachedThreadStatesGaugeSet. 

But we still need to fix STORM-3623  to avoid unnecessary computations and redundant metrics."
STORM-3620,Data corruption can happen when components are multi-threaded because of non thread-safe serializer,"OutputCollector is not thread-safe in 2.x. 

It can cause data corruption if multiple threads in the same executor calls OutputCollector to emit data at the same time:

1. Every executor has an instance of ExecutorTransfer
https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L146

2. Every ExecutorTransfer has its own serializer

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java#L44

3. Every executor has its own outputCollector

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java#L146-L147

4. When outputCollector is called to emit to remote workers, it uses ExecutorTransfer to transfer data

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java#L66

5. which will try to serialize data

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerTransfer.java#L116

6. But serializer is not thread-safe

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L33-L43


----

But in the doc, http://storm.apache.org/releases/2.1.0/Concepts.html, it says outputCollector is thread-safe. 
{code:java}
Its perfectly fine to launch new threads in bolts that do processing asynchronously. OutputCollector is thread-safe and can be called at any time.
{code}


We should either fix it to make it thread-safe, or update the document to not mislead users"
STORM-3619,Add null check for the topology name,"Currently with 

{code:java}
StormSubmitter.submitTopology(null, ...)
{code}
 
submission will fail:
{code:java}
Exception in thread ""main"" java.lang.RuntimeException: org.apache.storm.thrift.TApplicationException: Internal error processing isTopologyNameAllowed
	at org.apache.storm.StormSubmitter.topologyNameExists(StormSubmitter.java:438)
	at org.apache.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:247)
	at org.apache.storm.StormSubmitter.submitTopology(StormSubmitter.java:206)
	at org.apache.storm.StormSubmitter.submitTopologyWithProgressBar(StormSubmitter.java:411)
	at org.apache.storm.StormSubmitter.submitTopologyWithProgressBar(StormSubmitter.java:392)
	at com.example.SimplifiedTwoCompTopology.main(SimplifiedTwoCompTopology.java:49)
Caused by: org.apache.storm.thrift.TApplicationException: Internal error processing isTopologyNameAllowed
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.storm.generated.Nimbus$Client.recv_isTopologyNameAllowed(Nimbus.java:1209)
	at org.apache.storm.generated.Nimbus$Client.isTopologyNameAllowed(Nimbus.java:1196)
	at org.apache.storm.StormSubmitter.topologyNameExists(StormSubmitter.java:436)
	... 5 more
{code}

And on nimbus:

{code:java}
2020-04-08 18:38:46.356 o.a.s.t.ProcessFunction pool-34-thread-475 [ERROR] Internal error processing isTopologyNameAllowed
java.lang.NullPointerException: null
        at java.util.regex.Matcher.getTextLength(Matcher.java:1283) ~[?:1.8.0_242]
        at java.util.regex.Matcher.reset(Matcher.java:309) ~[?:1.8.0_242]
        at java.util.regex.Matcher.<init>(Matcher.java:229) ~[?:1.8.0_242]
        at java.util.regex.Pattern.matcher(Pattern.java:1093) ~[?:1.8.0_242]
        at org.apache.storm.daemon.nimbus.Nimbus.validateTopologyName(Nimbus.java:1189) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.isTopologyNameAllowed(Nimbus.java:4667) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.generated.Nimbus$Processor$isTopologyNameAllowed.getResult(Nimbus.java:4423) ~[storm-client-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.generated.Nimbus$Processor$isTopologyNameAllowed.getResult(Nimbus.java:4402) ~[storm-client-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:152) [storm-client-2.2.0.y
.jar:2.2.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [storm-shaded-deps-2.2.0.y.jar:2.2.0.y
]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
{code}

This is because https://github.com/apache/storm/blob/v2.1.0/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L1175-L1180

the topology name is null so NullPointerException 

But the error message is not obvious. We should add null check when validating topology name and report the error indicating the topology name being null.

 

 

 "
STORM-3618,add meter for tracking internal scheduling errors,
STORM-3617,Remove deprecation from PlainSaslTransportPlugin,"Whenever PlainSaslTransportPlugin is used, it logs an error about it being insecure. The class itself is also marked as deprecated - which is fair, as it is doesn't provide any security at all.

However, we are running in situations where we don't _need _security, and we don't want the extra fragility introduced by adding in security. We're perfectly ok with using an insecure transport, and we're using PlainSaslTransport over the default because we don't want to configure the buffer size of the default transport.

So please could you remove the deprecation marker from PlainSaslTransport, and downgrade the ERROR message to an INFO. We don't need security in our situations, and so running a default basic transport is exactly what we want to do."
STORM-3616,"If running upload credentials and no autocreds are found, we should have an option to fail","User tried to upload credentials on a box with a bad setup. Because the command passed, it was assumed to be ok. Then the topology certs timed out and dev got involved.

If the command had just failed, we would not have gotten involved at all.

Existing output:
{code:java}
1809 [main] INFO  b.s.s.a.AuthUtils - Got AutoCreds []
1810 [main] WARN  b.s.StormSubmitter - No credentials were found to push to edgestorm
1812 [main] INFO  b.s.c.upload-credentials - Uploaded new creds to topology: edgestorm{code}"
STORM-3615,Add documentation for Storm NUMA support,
STORM-3614,update SystemBolt metrics to use v2 API,
STORM-3613,storm.py should include lib-worker instead of lib directory in the classpath while submitting a topology,"Currently the classpath is:
{code:java}
-cp /<path>/storm/2.2.0/*:/<path>/storm/2.2.0/lib/*:/<path>/storm/2.2.0/extlib/*:/tmp/storm-examples-1.0-SNAPSHOT.jar:/<path>/storm/2.2.0/conf:/<path>/storm/2.2.0/bin: 
{code}

 for ""storm jar"" command.

It should include lib-worker/ instead of lib/.

This can cause problems because we don't shade deps in lib/ so topology jar could conflict with jars in lib/."
STORM-3612,Scheduling log messages in nimbus log should specify topology,"It could be helpful to add topology id in scheduling log messages.

{code} 
 2020-02-04 12:00:29.178 o.a.s.s.r.s.s.BaseResourceAwareStrategy pool-21-thread-1 [WARN] Not Enough Resources to schedule Task [979, 979] - ProjectorAdRequest Normalized resources: \{onheap.memory.mb=1024.0, cpu.pcore.percent=50.0, offheap.memory.mb=128.0, network.resource.units=10.0}

2020-02-04 12:00:29.178 o.a.s.s.r.s.s.BaseResourceAwareStrategy pool-21-thread-1 [WARN] Not Enough Resources to schedule Task [979, 979] - ProjectorAdRequest Normalized resources: \{onheap.memory.mb=1024.0, cpu.pcore.percent=50.0, offheap.memory.mb=128.0, network.resource.units=10.0}

2020-02-04 12:00:30.101 o.a.s.s.r.s.s.GenericResourceAwareStrategy pool-21-thread-1 [WARN] Scheduling [[293, 293], [330, 330], [294, 294], [331, 331], [295, 295], [332, 332], [296, 296], [333, 333], [297, 297], [334, 334], [298, 298], [335, 335], [299, 299], [336, 336], [263, 263], [300, 300], [337, 337], [264, 264], [301, 301], [338, 338], [265, 265], [302, 302], [339, 339], [266, 266], [303, 303], [340, 340], [267, 267], [304, 304], [341, 341], [268, 268], [305, 305], [342, 342], [269, 269], [306, 306], [343, 343], [270, 270], [307, 307], [344, 344], [271, 271], [308, 308], [345, 345], [272, 272], [309, 309], [346, 346], [273, 273], [310, 310], [347, 347], [274, 274], [311, 311], [348, 348], [275, 275], [312, 312], [349, 349], [276, 276], [313, 313], [350, 350], [277, 277], [314, 314], [351, 351], [278, 278], [315, 315], [352, 352], [279, 279], [316, 316], [353, 353], [280, 280], [317, 317], [354, 354], [281, 281], [318, 318], [355, 355], [282, 282], [319, 319], [356, 356], [283, 283], [320, 320], [357, 357], [284, 284], [321, 321], [358, 358], [285, 285], [322, 322], [359, 359], [286, 286], [323, 323], [360, 360], [287, 287], [324, 324], [361, 361], [288, 288], [325, 325], [362, 362], [289, 289], [326, 326], [363, 363], [290, 290], [327, 327], [364, 364], [291, 291], [328, 328], [292, 292], [329, 329]] left over task (most likely sys tasks)

2020-02-04 12:00:50.735 o.a.s.s.r.s.s.GenericResourceAwareStrategy pool-21-thread-1 [WARN] Scheduling [[1463, 1463], [1464, 1464], [1465, 1465], [1466, 1466], [1467, 1467], [1468, 1468], [1469, 1469], [1470, 1470], [1471, 1471], [1472, 1472], [1473, 1473], [1474, 1474], [1475, 1475], [1476, 1476], [1477, 1477], [1478, 1478], [1479, 1479], [1480, 1480], [1481, 1481], [1482, 1482], [1483, 1483], [1484, 1484], [1485, 1485], [1486, 1486], [1487, 1487], [1488, 1488], [1489, 1489], [1490, 1490], [1491, 1491], [1492, 1492], [1493, 1493], [1494, 1494], [1495, 1495], [1496, 1496], [1497, 1497], [1498, 1498], [1499, 1499], [1500, 1500], [1501, 1501], [1502, 1502], [1503, 1503], [1504, 1504], [1505, 1505], [1506, 1506], [1507, 1507], [1508, 1508], [1509, 1509], [1510, 1510], [1511, 1511], [1512, 1512], [1513, 1513], [1514, 1514], [1515, 1515], [1516, 1516], [1517, 1517], [1518, 1518], [1519, 1519], [1520, 1520], [1521, 1521], [1522, 1522], [1523, 1523], [1524, 1524], [1525, 1525], [1526, 1526], [1527, 1527], [1528, 1528], [1529, 1529], [1530, 1530], [1531, 1531], [1532, 1532], [1533, 1533], [1534, 1534], [1535, 1535], [1536, 1536], [1537, 1537], [1538, 1538], [1539, 1539], [1540, 1540], [1541, 1541], [1542, 1542], [1543, 1543], [1544, 1544], [1545, 1545], [1546, 1546], [1547, 1547], [1548, 1548], [1549, 1549], [1550, 1550], [1551, 1551], [1552, 1552], [1553, 1553], [1554, 1554], [1555, 1555], [1556, 1556], [1557, 1557], [1558, 1558], [1559, 1559], [1560, 1560], [1561, 1561], [1562, 1562], [1563, 1563], [1564, 1564], [1565, 1565], [1566, 1566], [1567, 1567], [1568, 1568], [1569, 1569], [1570, 1570], [1571, 1571], [1572, 1572], [1573, 1573], [1574, 1574], [1575, 1575], [1576, 1576], [1577, 1577], [1578, 1578], [1579, 1579], [1580, 1580], [1581, 1581], [1582, 1582], [1583, 1583], [1584, 1584], [1585, 1585], [1586, 1586], [1587, 1587], [1588, 1588], [1589, 1589], [1590, 1590], [1591, 1591], [1592, 1592], [1593, 1593], [1594, 1594], [1595, 1595], [1596, 1596], [1597, 1597], [1598, 1598], [1599, 1599], [1600, 1600], [1601, 1601], [1602, 1602], [1603, 1603], [1604, 1604], [1605, 1605], [1606, 1606], [1607, 1607], [1608, 1608], [1609, 1609], [1610, 1610], [1611, 1611], [1612, 1612], [1613, 1613], [1614, 1614], [1615, 1615], [1616, 1616], [1617, 1617], [1618, 1618], [1619, 1619], [1620, 1620], [1621, 1621], [1622, 1622], [1623, 1623], [1624, 1624], [1625, 1625], [1626, 1626], [1627, 1627], [1628, 1628], [1629, 1629], [1630, 1630], [1631, 1631], [1632, 1632], [1633, 1633], [1634, 1634], [1635, 1635], [1636, 1636], [1637, 1637], [1638, 1638], [1639, 1639], [1640, 1640], [1641, 1641], [1642, 1642], [1643, 1643], [1644, 1644], [1645, 1645], [1646, 1646], [1647, 1647], [1648, 1648], [1649, 1649], [1650, 1650], [1651, 1651], [1652, 1652], [1653, 1653], [1654, 1654], [1655, 1655], [1656, 1656], [1657, 1657], [1658, 1658], [1659, 1659], [1367, 1367], [1660, 1660], [1368, 1368], [1661, 1661], [1369, 1369], [1662, 1662], [1370, 1370], [1663, 1663], [1371, 1371], [1664, 1664], [1372, 1372], [1665, 1665], [1373, 1373], [1666, 1666], [1374, 1374], [1667, 1667], [1375, 1375], [1668, 1668], [1376, 1376], [1669, 1669], [1377, 1377], [1670, 1670], [1378, 1378], [1671, 1671], [1379, 1379], [1672, 1672], [1380, 1380], [1673, 1673], [1381, 1381], [1674, 1674], [1382, 1382], [1675, 1675], [1383, 1383], [1676, 1676], [1384, 1384], [1677, 1677], [1385, 1385], [1678, 1678], [1386, 1386], [1679, 1679], [1387, 1387], [1680, 1680], [1388, 1388], [1681, 1681], [1389, 1389], [1682, 1682], [1390, 1390], [1683, 1683], [1391, 1391], [1684, 1684], [1392, 1392], [1685, 1685], [1393, 1393], [1686, 1686], [1394, 1394], [1687, 1687], [1395, 1395], [1688, 1688], [1396, 1396], [1689, 1689], [1397, 1397], [1690, 1690], [1398, 1398], [1691, 1691], [1399, 1399], [1692, 1692], [1400, 1400], [1693, 1693], [1401, 1401], [1694, 1694], [1402, 1402], [1695, 1695], [1403, 1403], [1696, 1696], [1404, 1404], [1697, 1697], [1405, 1405], [1698, 1698], [1406, 1406], [1699, 1699], [1407, 1407], [1700, 1700], [1408, 1408], [1701, 1701], [1409, 1409], [1702, 1702], [1410, 1410], [1703, 1703], [1411, 1411], [1704, 1704], [1412, 1412], [1705, 1705], [1413, 1413], [1706, 1706], [1414, 1414], [1707, 1707], [1415, 1415], [1708, 1708], [1416, 1416], [1709, 1709], [1417, 1417], [1710, 1710], [1418, 1418], [1711, 1711], [1419, 1419], [1712, 1712], [1420, 1420], [1713, 1713], [1421, 1421], [1714, 1714], [1422, 1422], [1715, 1715], [1423, 1423], [1716, 1716], [1424, 1424], [1717, 1717], [1425, 1425], [1718, 1718], [1426, 1426], [1719, 1719], [1427, 1427], [1720, 1720], [1428, 1428], [1721, 1721], [1429, 1429], [1722, 1722], [1430, 1430], [1723, 1723], [1431, 1431], [1724, 1724], [1432, 1432], [1725, 1725], [1433, 1433], [1726, 1726], [1434, 1434], [1435, 1435], [1436, 1436], [1437, 1437], [1438, 1438], [1439, 1439], [1440, 1440], [1441, 1441], [1442, 1442], [1443, 1443], [1444, 1444], [1445, 1445], [1446, 1446], [1447, 1447], [1448, 1448], [1449, 1449], [1450, 1450], [1451, 1451], [1452, 1452], [1453, 1453], [1454, 1454], [1455, 1455], [1456, 1456], [1457, 1457], [1458, 1458], [1459, 1459], [1460, 1460], [1461, 1461], [1462, 1462]] left over task (most likely sys tasks)

{code}"
STORM-3610,CLONE - Topology runtime exception - Error on initialization,"{code:java}
2018-03-14 13:28:41.399 o.a.s.d.worker main [INFO] Reading Assignments. 2018-03-14 13:28:41.511 o.a.s.m.TransportFactory main [INFO] Storm peer transport plugin:org.apache.storm.messaging.netty.Context 2018-03-14 13:28:41.935 o.a.s.m.n.Server main [INFO] Create Netty Server Netty-server-localhost-6712, buffer_size: 5242880, maxWorkers: 1 2018-03-14 13:28:41.980 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6712 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data$fn__5244.invoke(worker.clj:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:269) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544.run(worker.clj:613) ~[storm-core-1.1.0.jar:1.1.0] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_51] at javax.security.auth.Subject.doAs(Subject.java:415) ~[?:1.7.0_51] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto____5543.invoke(worker.clj:611) ~[storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?] at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0] Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:444) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:436) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_51] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.1.0.jar:1.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_51] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_51] at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_51] 2018-03-14 13:28:42.004 o.a.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"") at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0]
{code}"
STORM-3609,ClassCastException when credentials are updated for ICredentialsListener spout/bolt instances,"
{code:java}
2020-03-26 21:04:38.526 o.a.s.u.Utils Thread-14-spout-executor[2, 2] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.storm.generated.Credentials cannot be cast to java.util.Map
	at org.apache.storm.executor.Executor.accept(Executor.java:291) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:111) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:102) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:170) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:159) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:433) [storm-client-2.2.0.y.jar:2.2.0.y]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.lang.ClassCastException: org.apache.storm.generated.Credentials cannot be cast to java.util.Map
	at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:303) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.Executor.accept(Executor.java:287) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 7 more
{code}



note: ""2.2.0.y"" is our internal version, which is master branch."
STORM-3608,Upgrade snakeyaml from 1.11 to 1.26 (latest),snakeyaml-1.11 (sep 2012) is really old. We should just upgrade to latest version 1.26
STORM-3607,Document the exceptions topologies will see from TGT renewal thread,This is to document STORM-3606 in the code so users can be less confusing about the exceptions from TGT renewal thread.
STORM-3606,AutoTGT shouldn't invoke TGT renewal thread (from UserGroupInformation.loginUserFromSubject),"When hadoop security is enabled, 
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGT.java#L199-L209

AutoTGT will invoke ""loginUserFromSubject"", and it will spawn a TGT renewal thread (""TGT Renewer for <username>""). 
https://github.com/apache/hadoop/blob/branch-2.8.5/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L928-L957

which will eventually invoke system command ""kinit -R"", and then fail with the exception

{code:java}
org.apache.hadoop.util.Shell$ExitCodeException: kinit: Credentials cache file '/tmp/krb5cc_xxx' not found while renewing credentials

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1004) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:898) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:1011) [stormjar.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}


""kinit"" will never work from worker process since Storm don't keep TGT in local cache. Instead, TGT is saved in zookeeper and in memory of Worker process. 

This exception is confusing but not harmful to topologies. And the TGT renewal thread will eventually abort. 

It's better to find a real solution for it. But for now we can document what might happen in AutoTGT code.

To be clear, we still need loginUserFromSubject or some sort but we don't want to spawn TGT renewal thread.  This is found with hadoop-2.8.5. Other versions are similar. But it can also change in the future release."
STORM-3605,add meter to track scheduling timeouts,
STORM-3604,HealthChecker should print out error message when it fails,"Currently in the code https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/healthcheck/HealthChecker.java#L122-L130


{code:java}
           if (process.exitValue() != 0) {
                String str;
                InputStream stdin = process.getInputStream();
                BufferedReader reader = new BufferedReader(new InputStreamReader(stdin));
                while ((str = reader.readLine()) != null) {
                    if (str.startsWith(""ERROR"")) {
                        LOG.warn(""The healthcheck process {} exited with code {}"", script, process.exitValue());
                        return FAILED;
                    }
                }
                return FAILED_WITH_EXIT_CODE;
            }
{code}

The healthcheck doesn't really print out the error message so it's not easy to debug when healthcheck fails. "
STORM-3603,Broken links on download page,"The SHA512 links for the 1.2.3 release on the download page [1] are broken.

e.g. the SHA512 for apache-storm-1.2.3.tar.gz points to 
https://downloads.apache.org/storm/apache-storm-1.2.3/apache-storm-1.2.3.tar.gz.sha
which does not exist.

The link should be
https://downloads.apache.org/storm/apache-storm-1.2.3/apache-storm-1.2.3.tar.gz.sha512

The download page needs to be fixed for 4 of the broken SHA512 links.


[1] http://storm.apache.org/downloads.html"
STORM-3602,loadaware shuffle can overload local worker,"We were seeing a worker overloaded and tuples timing out with loadaware shuffle enabled.  From investigating, we found that the code allows switching from Host local to Worker local if the load average is lower than the low water mark.  It really should be checking the load on the worker instead. 

 

What's happening is the worker is overloaded with tons of idle host local tasks, so it switches to HOST_LOCAL.  Then the calculation across all the host tasks is below the low water mark and it immediately switches back to the overloaded worker local task.

 

 "
STORM-3601,"when i run DRPCServerTest, it occur java.lang.NoClassDefFoundError: org/apache/storm/thrift/TBase","when i run DRPCServerTest, occur a error:
{code:java}
java.lang.NoClassDefFoundError: org/apache/storm/thrift/TBase{code}
the project seem no such class called TBase in package :
{code:java}
org/apache/storm/thrift/{code}
, it is missing in a history version?"
STORM-3600,ResourceAwareScheduler taking too long to schedule,We found out that calculateSharedOffHeapNodeMemory is called too many times during scheduling (from java profiling result). Add caching in Cluster.calculateSharedOffHeapNodeMemory should speed up the scheduling process
STORM-3599,Bump the rocksdbjni to 5.18.4,"The rocksdb community release a special relaese for ARM(aarch64) [1].

And we can bump our storm rocksdbjni from 5.18.3 to 5.18.4 to enable ARM support for STORM.

[1] https://github.com/facebook/rocksdb/releases/tag/v5.18.4
"
STORM-3598,Storm UI visualization throws NullPointerException,"We encountered an issue with visualization on UI. 

 
{code:java}
2020-03-09 19:59:01.756 o.a.s.d.u.r.StormApiResource qtp1919834117-167291 [ERROR] Failure getting topology visualization
java.lang.NullPointerException: null
        at org.apache.storm.stats.StatsUtil.mergeWithAddPair(StatsUtil.java:1855) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.expandAveragesSeq(StatsUtil.java:2308) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.aggregateAverages(StatsUtil.java:832) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.aggregateBoltStats(StatsUtil.java:731) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.boltStreamsStats(StatsUtil.java:900) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.daemon.ui.UIHelpers.getVisualizationData(UIHelpers.java:1939) ~[storm-webapp-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyVisualization(StormApiResource.java:423) ~[storm-webapp-2.2.0.y.jar:2.2.0.y]
{code}
This is a bug in the code. https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/stats/StatsUtil.java#L1846-L1858
{code:java}
for (K kk : mm1.keySet()) {
                    List seq1 = mm1.get(kk);
                    List seq2 = mm2.get(kk);
                    List sums = new ArrayList();
                    for (int i = 0; i < seq1.size(); i++) {
                        if (seq1.get(i) instanceof Long) {
                            sums.add(((Number) seq1.get(i)).longValue() + ((Number) seq2.get(i)).longValue());
                        } else {
                            sums.add(((Number) seq1.get(i)).doubleValue() + ((Number) seq2.get(i)).doubleValue());
                        }
                    }
                    tmp.put(kk, sums);
                }
{code}
It assume mm1 and mm2 always have the same key, which is not true. 

And it can be reproduced by my example code: 

 {code:java}
public class  WordCountTopology extends ConfigurableTopology {
    private static final Logger LOG = LoggerFactory.getLogger(WordCountTopology.class);

    public static void main(String[] args) {
        ConfigurableTopology.start(new WordCountTopology(), args);
    }

    protected int run(String[] args) {
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(""spout1"", new RandomSpout(1), 1);
        builder.setSpout(""spout2"", new RandomSpout(2), 1);
        builder.setBolt(""bolt"", new RandomBolt(), 2).directGrouping(""spout1"", ""stream1"")
                .directGrouping(""spout2"", ""stream2"");

        String topologyName = ""word-count"";

        conf.setNumWorkers(3);

        if (args != null && args.length > 0) {
            topologyName = args[0];
        }
        return submit(topologyName, conf, builder);
    }

    static class RandomSpout extends BaseRichSpout {
        String stream;
        int id;

        public RandomSpout(int id) {
            this.id = id;
            stream = ""stream"" + id;
        }

        int taskId = 0;
        SpoutOutputCollector collector;
        public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
            taskId = context.getThisTaskId();
            this.collector = collector;
        }

        /**
         * Different spout send tuples to different bolt via different stream.
         */
        public void nextTuple() {
            LOG.info(""emitting {}"", id);
            if (id == 1) {
                Values val = new Values(""test a sentence"");
                collector.emitDirect(2, stream, val, val);
            } else {
                Values val = new Values(""test 2 sentence"");
                collector.emitDirect(3, stream, val, val);
            }
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declareStream(stream, new Fields(""word""));
        }
    }

    static class RandomBolt extends BaseBasicBolt {

        public void execute(Tuple input, BasicOutputCollector collector) {
            LOG.info(""executing:"" + input.getSourceComponent());
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {

        }
    }
}
{code}

 In this example, one of the bolt will only receive data from stream1 and another bolt will only receive data from stream2. So in the map, 

 {code:java}
                    List seq1 = mm1.get(kk);
                    List seq2 = mm2.get(kk);
{code}
seq1 is null if kk is stream1, seq2 is null if kk is stream2.

 

 We have other places aggregating executor stats without this problem because it's using different code https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/stats/StatsUtil.java#L502-L513 and this problem has been taken cared of."
STORM-3596,Feed send assignment status into blacklist scheduler,"We occasionally see hiccups sending assignments to supervisors, which are usually transitory.  But we have seen more persistent issues with a supervisor when its disk became read-only.  The supervisor remained up and was unable to start workers.  Nimbus continually tried to send it assignments and failed, but just ate the exception and continued on.  

 

We should be able to send this information to the blacklist scheduler and add the node to the blacklist when some threshold occurs.

 "
STORM-3594,Add checkstyle rule WhitespaceAfter,"[https://checkstyle.sourceforge.io/config_whitespace.html#WhitespaceAfter]

 

This is already exercised in Storm code base. Adding it to make it clearer "
STORM-3593,Inconsistent library versions notice.," 
 
Hi. I have implemented a tool to detect library version inconsistencies. Your project have 10 inconsistent libraries and 2 false consistent libraries.
 
Take commons-io:commons-io for example, this library is declared as version 2.6 in storm-shaded-deps, 1.4 in external/storm-solr, 2.5 in storm-submit-tools and etc... Such version inconsistencies may cause unnecessary maintenance effort in the long run. For example, if two modules become inter-dependent, library version conflict may happen. It has already become a common issue and hinders development progress. Thus a version harmonization is necessary.
 
Provided we applied a version harmonization, I calculated the cost it may have to harmonize to all upper versions including an up-to-date one. The cost refers to POM config changes and API invocation changes. Take commons-io:commons-io for example, if we harmonize all the library versions into 2.6. The concern is, how much should the project code adapt to the newer library version. We list an effort table to quantify the harmonization cost.
 
The effort table is listed below. It shows the overall harmonization effort by modules. The columns represents the number of library APIs and API calls(NA,NAC), deleted APIs and API calls(NDA,NDAC) as well as modified API and API calls(NMA,NMAC). Modified APIs refers to those APIs whose call graph is not the same as previous version. Take the first row for example, if upgrading the library into version 2.6. Given that 9 APIs is used in module storm-server, 0 of them is deleted in a recommended version(which will throw a NoMethodFoundError unless re-compiling the project), 0 of them is regarded as modified which could break the former API contract.



||Index||Module||NA(NAC)||NDA(NDAC)||NMA(NMAC)||
|1|storm-server|9(15)|0(0)|0(0)|
|2|integration-test|4(4)|0(0)|0(0)|
|3|storm-submit-tools|1(1)|0(0)|1(1)|
|4|..|..|..|..|

 
Also we provided another table to show the potential files that may be affected due to library API change, which could help to spot the concerned API usage and rerun the test cases. The table is listed below.



||Module||File||Type||API||
|storm-submit-tools|storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/DependencyResolverTest.java|modify|org.apache.commons.io.FileUtils.deleteQuietly(java.io.File)|

 

 
As for false consistency, take org.apache.commons commons-lang3 jar for example. The library is declared in version 3.3 in all modules. However they are declared differently. As components are developed in parallel, if one single library version is updated, which could become inconsistent as mentioned above, may cause above-mentioned inconsistency issues



If you are interested, you can have a more complete and detailed report in the attached PDF file."
STORM-3592,Vulnerable dependencies in your project.(CVEs),"Hi,
I found some CVEs in the library dependencies, which may affect the security of your projects. In order to avoid threats, I recommend updating to a safe version. Here is the detailed information:
 
Vulnerable Library Version: org.apache.hadoop : hadoop-common : 2.8.5
  CVE ID: [CVE-2018-8029](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-8029), [CVE-2018-8009](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-8009)
  Import Path: external/storm-hdfs/pom.xml, external/storm-hdfs-blobstore/pom.xml, external/storm-blobstore-migration/pom.xml
  Suggested Safe Versions: 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1

 Vulnerable Library Version: org.eclipse.jetty : jetty-server : 9.4.14.v20181114
  CVE ID: [CVE-2019-10247](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10247)
  Import Path: examples/storm-loadgen/pom.xml, storm-core/pom.xml
  Suggested Safe Versions: 10.0.0-alpha0, 10.0.0.alpha1, 9.4.17.v20190418, 9.4.18.v20190429, 9.4.19.v20190610, 9.4.20.v20190813, 9.4.24.v20191120, 9.4.25.v20191220, 9.4.26.v20200117

 Vulnerable Library Version: org.apache.commons : commons-compress : 1.18
  CVE ID: [CVE-2019-12402](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12402)
  Import Path: storm-server/pom.xml, examples/storm-pmml-examples/pom.xml
  Suggested Safe Versions: 1.19, 1.20

 Vulnerable Library Version: org.eclipse.jetty : jetty-util : 9.4.14.v20181114
  CVE ID: [CVE-2019-10246](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10246), [CVE-2019-10241](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-10241)
  Import Path: storm-core/pom.xml
  Suggested Safe Versions: 10.0.0-alpha0, 10.0.0.alpha1, 9.4.17.v20190418, 9.4.18.v20190429, 9.4.19.v20190610, 9.4.20.v20190813, 9.4.21.v20190926, 9.4.22.v20191022, 9.4.23.v20191118, 9.4.24.v20191120, 9.4.25.v20191220, 9.4.26.v20200117

 Vulnerable Library Version: org.apache.kafka : kafka_2.11 : 0.11.0.3
  CVE ID: [CVE-2019-17196](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-17196)
  Import Path: external/storm-kafka-client/pom.xml, external/storm-kafka-client/pom.xml
  Suggested Safe Versions: 2.1.1, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0

 Vulnerable Library Version: com.google.guava : guava : 17.0
  CVE ID: [CVE-2018-10237](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-10237)
  Import Path: external/storm-solr/pom.xml, examples/storm-solr-examples/pom.xml
  Suggested Safe Versions: 24.1.1-android, 24.1.1-jre, 25.0-android, 25.0-jre, 25.1-android, 25.1-jre, 26.0-android, 26.0-jre, 27.0-android, 27.0-jre, 27.0.1-android, 27.0.1-jre, 27.1-android, 27.1-jre, 28.0-android, 28.0-jre, 28.1-android, 28.1-jre, 28.2-android, 28.2-jre

 Vulnerable Library Version: com.google.guava : guava : 16.0.1
  CVE ID: [CVE-2018-10237](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-10237)
  Import Path: sql/storm-sql-runtime/pom.xml, sql/storm-sql-external/storm-sql-hdfs/pom.xml...(The rest of the 17 paths is hidden.)
  Suggested Safe Versions: 24.1.1-android, 24.1.1-jre, 25.0-android, 25.0-jre, 25.1-android, 25.1-jre, 26.0-android, 26.0-jre, 27.0-android, 27.0-jre, 27.0.1-android, 27.0.1-jre, 27.1-android, 27.1-jre, 28.0-android, 28.0-jre, 28.1-android, 28.1-jre, 28.2-android, 28.2-jre

 Vulnerable Library Version: org.apache.thrift : libthrift : 0.9.3
  CVE ID: [CVE-2018-1320](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1320)
  Import Path: external/storm-hive/pom.xml
  Suggested Safe Versions: 0.12.0, 0.13.0
 Vulnerable Library Version: org.apache.activemq : activemq-client : 5.15.8
  CVE ID: [CVE-2019-0222](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0222)
  Import Path: examples/storm-jms-examples/pom.xml
  Suggested Safe Versions: 5.15.10, 5.15.11, 5.15.9

 Vulnerable Library Version: org.apache.solr : solr-core : 5.5.5
  CVE ID: [CVE-2017-3164](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-3164), [CVE-2019-0192](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0192)
  Import Path: external/storm-solr/pom.xml
  Suggested Safe Versions: 7.7.0, 7.7.1, 7.7.2, 8.0.0, 8.1.0, 8.1.1, 8.2.0, 8.3.0, 8.3.1, 8.4.0, 8.4.1

 Vulnerable Library Version: org.fusesource.mqtt-client : mqtt-client : 1.14
  CVE ID: [CVE-2019-0222](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0222)
  Import Path: examples/storm-mqtt-examples/pom.xml
  Suggested Safe Versions: 1.16

 Vulnerable Library Version: org.fusesource.mqtt-client : mqtt-client : 1.10
  CVE ID: [CVE-2019-0222](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0222)
  Import Path: external/storm-mqtt/pom.xml
  Suggested Safe Versions: 1.16

 Vulnerable Library Version: com.fasterxml.jackson.core : jackson-databind : 2.9.8
  CVE ID: [CVE-2020-8840](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-8840), [CVE-2019-16335](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16335), [CVE-2019-20330](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-20330), [CVE-2019-12384](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12384), [CVE-2019-12086](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12086), [CVE-2019-17531](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-17531), [CVE-2019-14439](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-14439), [CVE-2019-12814](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12814), [CVE-2019-16943](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16943), [CVE-2019-14379](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-14379), [CVE-2019-14540](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-14540), [CVE-2019-17267](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-17267), [CVE-2019-16942](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16942)
  Import Path: sql/storm-sql-runtime/pom.xml, external/storm-hbase/pom.xml, external/storm-elasticsearch/pom.xml, external/storm-kafka-migration/pom.xml, external/storm-redis/pom.xml, external/storm-opentsdb/pom.xml, external/storm-kafka-client/pom.xml, storm-webapp/pom.xml
  Suggested Safe Versions: 2.10.0, 2.10.1, 2.10.2, 2.9.10.3
"
STORM-3591,Improve GRAS Strategy Log,"[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java#L123]
{code:java}
2020-02-24 14:53:59.652 o.a.s.s.r.s.s.GenericResourceAwareStrategy pool-21-thread-1 [WARN] Scheduling [[1, 1]] left over task (most likely sys tasks)
{code}
This message seems to be confusing on debugging.

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java#L82]

Default Strategy actually uses debug level instead of warn."
STORM-3590,Add a test to validate that GRAS's node sorting is stable to prevent excessive fragmentation and starvation of non-GRAS topologies,
STORM-3589,Iterator in BaseResourceStrategy is potentially buggy,"[https://github.com/apache/storm/blame/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java#L280]

We should probably only peek but not remove value from nodeIterator in hasNext() function.

 

[https://github.com/apache/storm/blame/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java#L296-L300]

 

And two consecutive next() call will cause problem."
STORM-3588,RAS scheduler should not pre-empt and evict topologies due to generic resource,"Currently, when we enabled generic resource support in our cluster, RAS scheduler will evict scheduled topologies if new inappropriate asks for crazy amount of generic resources.

We have identified the currently getScore() function in DefaultSchedulingPriorityStrategy does not consider the generic resource factor. Ideally, if user topology is asking way too much generic resources, it should be assigned lower priority in scheduling and won't affect other running topologies."
STORM-3587,Allow Scheduler futureTask to gracefully exit and register message on timeout,"ResourceAwareScheduler creates a FutureTask with timeout specified in DaemonConfig.

ConstraintSolverStrategy uses the the another configuration variable to determine when to terminate its effort. Limit this value so that it terminates at most slightly before TimeoutException. This graceful exit allows result (and its error) to be available in ResourceAwareScheduler.

 "
STORM-3586,Prevent deletion of topology blobs during initial scheduling,"If a cluster has a long scheduling period compared to the blob cleanup interval, it's possible to have a user submit a topology, kill it, and have the blob removed before supervisors get the scheduling assignment.  If this happens, the supervisors will get FNF exceptions looking for the blob.

 

A simple fix is to just have the blob cleanup interval be larger than the scheduling timeout.

 

 

 "
STORM-3585,Change ConstraintSolverStrategy to allow max co-Location Count for spreading components,"We need constraint solver strategy to evolve around to using Map<Component, maxCoLocationCount> instead of simple list of components that get maximum of 1 collocation as specified in config _topology.spread.components_."
STORM-3584, Support getting version info from a wildcard classpath entry,"[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/VersionInfo.java#L134]

 

Current VersionInfo.getFromClasspath method only tries to get version info from the specific property file under a directory, or jar/zip files. It should support a classpath entry like /<parent-path>/*. "
STORM-3583,Handle exceptions when AsyncLocalizer tries to get local resources,"Supervisor relies on AsyncLocalizer to download blobs from blob store.
AsyncLocalizer uses downloadService pool to process CompletableFuture objects in parallel.

We have noticed a case that while the downloading task is waiting for a thread to execute, new assignment changes will try to release the slot by dereferences all of the related local resources.

However, reading local resources assumes two base blob downloading task have been completed which is not always true.
 "
STORM-3581,Change log level to info to show the config classes being used for validation,"[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java#L82]

This is trivial but since it's caused some confusion, it's better to have it in the log as INFO instead of DEBUG
{code:java}
LOG.debug(""Will use {} for validation"", ret);
{code}
 

Because the classes being used for validation depends on whether the following file is in the classpath or not

 

[https://github.com/apache/storm/blob/master/storm-server/src/main/resources/META-INF/services/org.apache.storm.validation.Validated]"
STORM-3580,Config overrides supplied using -c in storm.py not passed to all commands,"-c is used to supply configuration overide options. Storm.py in the client code converts these overrides into one -Dstorm.options system property. However, this jvm option is not handled properly when the actual command is executed. For example, Rebalance command completely ignores this setting.

Commands that currently process ""-c"" options:
 * Activate - Not Needed
 * *AdminCommands - Yes*
 * BasicDrpcClient - Not Needed
 * Blobstore - Not Needed
 * CLI - Not Needed
 * *ConfigValue - Yes*
 * Deactivate - Not Needed
 * *DevZookeeper - Yes*
 * *DRPCServer - Yes*
 * GetErrors - Not Needed
 * *HealthCheck - Yes*
 * *Heartbeats - Yes*
 * KillTopology - Not Needed
 * *KillWorkers - Yes*
 * ListTopologies - Not Needed
 * *LogViewerServer - Yes*
 * *LocalCluster - Yes*
 * Monitor - Not Needed
 * *Nimbus - Yes*
 * *Pacemaker - Yes*
 * Rebalance - {color:#ff0000}Added as part of this Jira{color}
 * SetLogLevel - Not Needed
 * *ShellSubmission - Yes*
 * *StormSqlRunner - Yes*
 * Supervisor -Not Needed
 * *UI - Yes*
 * *UploadCredentials - Yes, but specific options*
 * VersionsInfo - Not Needed

 "
STORM-3579,Fix Kerberos connection from Worker to Nimbus/Supervisor,BUG2 in the parent JIRA
STORM-3578,ClientAuthUtils.insertWorkerTokens removes exiting and new WorkerToken altogether if they are equal,BUG1 in the parent JIRA
STORM-3577,upload-credentials Breaks Topology in secure cluster,"*Background*

Worker uses WorkerToken to connect to Nimbus/Supervisor, (e.g. in Worker.doHeartBeat method). If WorkerToken is not in place, it will fall back to Kerberos.

 

*Issue:*

Users can submit topology and the topology is running fine.

But error shows up in worker log if ""storm upload-credentials"" is executed (with AutoTGT being used). (2.2.0.y is our internal version of apache-storm master branch)

 
{code:java}
2020-02-04 00:12:57.975 o.a.s.d.w.Worker heartbeat-timer [WARN] Exception when send heartbeat to local supervisor
2020-02-04 00:12:57.984 o.a.s.s.a.k.ClientCallbackHandler heartbeat-timer [WARN] Could not login: the client is being asked for a password, but the  client code does not currently support obtaining a password from the user. Make sure that the client is configured to use a ticket cache (using the JAAS configuration setting 'useTicketCache=true)' and restart the client. If you still get this message after that, the TGT in the ticket cache has expired and must be manually refreshed. To do so, first determine if you are using a password or a keytab. If the former, run kinit in a Unix shell in the environment of the user who is running this client using the command 'kinit <princ>' (where <princ> is the name of the client's Kerberos principal). If the latter, do 'kinit -k -t <keytab> <princ>' (where <princ> is the name of the Kerberos principal, and <keytab> is the location of the keytab file). After manually refreshing your cache, restart this client. If you continue to see this message after manually refreshing your cache, ensure that your KDC host's clock is in sync with this host's clock.
2020-02-04 00:12:57.984 o.a.s.s.a.k.KerberosSaslTransportPlugin heartbeat-timer [ERROR] Server failed to login in principal:javax.security.auth.login.LoginException: No password provided
javax.security.auth.login.LoginException: No password provided
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:919) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.storm.messaging.netty.Login.login(Login.java:300) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.messaging.netty.Login.<init>(Login.java:84) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:112) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:69) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:80) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:221) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:179) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.heartbeatToMasterIfLocalbeatFail(Worker.java:456) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.doHeartBeat(Worker.java:361) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.lambda$loadWorker$2(Worker.java:209) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.2.0.y.jar:2.2.0.y]
2020-02-04 00:12:57.985 o.a.s.u.NimbusClient heartbeat-timer [WARN] Ignoring exception while trying to get leader nimbus info from quadiumtan-ni.tan.ygrid.yahoo.com. will retry with a different seed host.
java.lang.RuntimeException: java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:108) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:69) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:80) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:221) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:179) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.heartbeatToMasterIfLocalbeatFail(Worker.java:456) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.doHeartBeat(Worker.java:361) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.lambda$loadWorker$2(Worker.java:209) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.2.0.y.jar:2.2.0.y]
Caused by: java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:117) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 10 more
Caused by: javax.security.auth.login.LoginException: No password provided
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:919) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.storm.messaging.netty.Login.login(Login.java:300) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.messaging.netty.Login.<init>(Login.java:84) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:112) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 10 more
{code}
It can be reproduced by
{code:java}
/storm jar /home/y/lib64/jars/storm-starter.jar  org.apache.storm.starter.WordCountTopology wc -c topology.debug=false

kinit -R # refresh TGT. This is must-have. So upload-credentials will do something and trigger the bug

storm upload-credentials wc

## Errors will show up in worker log in up to 30s (credential refresh period)
{code}
 

*BUGS*

 

*BUG1* When new credentials got uploaded, Worker will try to update credentials. But while it does it, it will also try to replace WorkerToken if it changes. But it has a bug in the code:

[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/ClientAuthUtils.java#L411-L416]

 

Here in the code, ""token"" could equal to ""previous"" if tokens didn't change because WorkerToken.equals() method only cares about the content of WorkerToken. The result of this function is the tokens got removed completely.

So in this case, because tokens are not present, Worker will fall back to use kerberos to connect to Nimbus/Supervisor. [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java#L122-L139]

And here comes the second bug

*BUG2*. Kerberos connection from Worker to Nimbus/Supervisor is not working properly, hence the error logs above. "
STORM-3575,Fix Scheduler Status on failure after multiple attempts,"The RAS on multiple attempts when fails to schedule a topology, it is overriding status as to with {color:#FF0000}_Failed to schedule within 5 attempts_{color}
But I think, it should append this message to existing reason/status on the topology.

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java#L239]"
STORM-3574,Rebalance should re-computeExecutors so metrics consumers can be added,"Since metrics consumers are a system component and dynamically adding them should be possible with simple configuration change _topology.metrics.consumer.register_. Currently the _computeExecutors_ or more precisely - _StormCommon#startTaskInfo_ which evaluates this variable is only invoked during _startTopology_ , the rebalance functionality is not starting newer tasks.

We need to ensure rebalance recalculates the idToExecutors again - hopefully only adding new tasks for added executors without removing old ids."
STORM-3572,Topology visualization can fail if executor is not up,
STORM-3571,Add topology info to slot warning messages,"{code:java}
2020-01-30 10:57:56.639 o.a.s.d.s.Slot SLOT_6714 [WARN] SLOT 6714: HB is too old 32000 > 30000
2020-01-30 11:31:44.107 o.a.s.d.s.Slot SLOT_6723 [WARN] SLOT 6723: HB is too old 31000 > 30000
2020-01-30 11:39:15.256 o.a.s.d.s.Slot SLOT_6728 [WARN] SLOT 6728: HB is too old 32000 > 30000
{code}

Stale HB is one of the most common situations we need to deal with. It could be helpful to see topology id in such logs when debugging."
STORM-3570,add config name when validation fails with ClassNotFoundException,
STORM-3569,Travis fails from time to time because of Authorization failures for kafka-schema-registry-client,"Very often we see this failure on travis build
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (process-resource-bundles) on project flux-examples: Error resolving project artifact: Could not transfer artifact io.confluent:kafka-schema-registry-client:pom:1.0 from/to sonatype (https://oss.sonatype.org/content/repositories/releases/): Authorization failed for https://oss.sonatype.org/content/repositories/releases/io/confluent/kafka-schema-registry-client/1.0/kafka-schema-registry-client-1.0.pom 403 Forbidden for project io.confluent:kafka-schema-registry-client:jar:1.0 -> [Help 1]
{code}
"
STORM-3568,"Topology UI page ""Change Log Level"" should not allow empty logger name",500 Server Error will be shown on the bottom. Users have complained the call stack was not informative. We should prevent them from leaving logger field empty.
STORM-3567,Topology UI page is showing total resources for each component if not scheduled,
STORM-3566,add serialVersionUID field to class which implement Serializable interface.,add serialVersionUID field to class which implement Serializable interface
STORM-3565,Allow users to add dimensionsfor storm metrics,"We have encountered a use-case that users want a better control over metrics.

 

For example, a kafka spout may stream in data from multiple kafka partitions. And the partition list could be dynamic. For a simple count metric, our users might want to a better fine-grained group like grouping the counts by kafka partition id.

 

It would be more convenient if storm could allow users to add dimensions to individual yamas metrics"
STORM-3564,Deprecated task.heartbeat.frequency.secs still set in defaults,"task.heartbeat.frequency.secs has been deprecated but it is still being set in the default config:

[https://github.com/apache/storm/blob/master/conf/defaults.yaml#L217]

As such logs get full of:
{code:java}
o.a.s.v.ConfigValidation main [WARN] task.heartbeat.frequency.secs is a deprecated config please see class org.apache.storm.Config.TASK_HEARTBEAT_FREQUENCY_SECS for more information. {code}"
STORM-3563,Travis fails because of missing maven package from the mirror,"Travis fails because of the following error:
{code:java}
0.39s$ wget http://mirrors.rackhosting.com/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz -P $HOME
--2020-01-03 06:21:16--  http://mirrors.rackhosting.com/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz
Resolving mirrors.rackhosting.com (mirrors.rackhosting.com)... 77.247.64.34, 2a02:4de0:21::2
Connecting to mirrors.rackhosting.com (mirrors.rackhosting.com)|77.247.64.34|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2020-01-03 06:21:17 ERROR 404: Not Found.
{code}

The latest maven version is 3.6.3. 3.6.1 is removed from the mirror. 

We should use https://archive.apache.org/dist/maven/maven-3/3.6.1/ to prevent this from happening every time there is a new maven minor release.

Also we should cache maven to avoid re-download every time. 

"
STORM-3562,"Storm code can not built repeatedly. It means that the same code builds different packages at different times. After comparison, the class file with the same name has the same content, but the location of the method is different. Is there any solution?",
STORM-3561,java.lang.RuntimeException: java.io.UTFDataFormatException,"My application encountered the following problems online, which caused my worker to exit unexpectedly. Does anyone know why this is? I am considering whether to use the  java native serialization instead of Kryo

 

2019-12-29 21:35:41.669 o.a.s.m.n.StormServerHandler Netty-server-localhost-1800-worker-1 [ERROR] server errors in handling the request
java.lang.RuntimeException: java.io.eTFDataFormatException
 at org.apache.storm.serialization.SerializableSerializer.read(SerializableSerializer.java:53) ~[storm-client-2.1.0.jar:2.1.0]
 at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:793) ~[kryo-3.0.3.jar:?]
 at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134) ~[kryo-3.0.3.jar:?]
 at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
 at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:689) ~[kryo-3.0.3.jar:?]
 at org.apache.storm.serialization.KryoValuesDeserializer.deserializeFrom(KryoValuesDeserializer.java:31) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.serialization.KryoTupleDeserializer.deserialize(KryoTupleDeserializer.java:45) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.DeserializingConnectionCallback.recv(DeserializingConnectionCallback.java:66) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.netty.Server.enqueue(Server.java:146) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.netty.Server.received(Server.java:264) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.netty.StormServerHandler.channelRead(StormServerHandler.java:51) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:323) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:426) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_231]
Caused by: java.io.UTFDataFormatException
 at java.io.ObjectInputStream$BlockDataInputStream.readUTFSpan(ObjectInputStream.java:3470) ~[?:1.8.0_231]
 at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3414) ~[?:1.8.0_231]
 at java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3226) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readUTF(ObjectInputStream.java:1133) ~[?:1.8.0_231]
 at java.io.ObjectStreamClass.readNonProxy(ObjectStreamClass.java:768) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readClassDescriptor(ObjectInputStream.java:891) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1857) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) ~[?:1.8.0_231]
 at org.apache.storm.serialization.SerializableSerializer.read(SerializableSerializer.java:51) ~[storm-client-2.1.0.jar:2.1.0]
 ... 31 more
2019-12-29 21:35:41.673 o.a.s.m.n.StormServerHandler Netty-server-localhost-1800-worker-1 [INFO] Received error in netty thread.. terminating server...
2019-12-29 21:35:41.674 o.a.s.d.w.Worker Thread-43 [INFO] Shutting down worker owl_analyze_1228_0-16-1577543204 f611fb0f-3a97-4043-bb42-8f74190926ff-10.16.20.6 1800
2019-12-29 21:35:41.675 o.a.s.u.Utils Thread-42 [INFO] Halting after 3 seconds"
STORM-3560,Worker Process 2d4d998d-4a8c-43ca-aafd-19808c63e4da exited with code: 139,"Our online storm application has the problem of worker exiting abnormally. There is no abnormal information in worker.log. You can see the restart log in worker.log. It is found in supervisor.log:


Worker Process 2d4d998d-4a8c-43ca-aafd-19808c63e4da exited with code: 139
  2019-12-30 12: 33: 26.248 o.a.s.d.s. Slot SLOT_1801 [WARN] SLOT 1801: main process has exited


Then the worker exits and is restarted by the supervisor. Is this a segment fault generated by the jni call to the C code?"
STORM-3559,storm2.0.0 can not support python2.6.6 ,"Due to the lack of the argparse module in Python2.6.6, the various roles of Storm fail to start
but storm2.0.0 claims to support Python2.6.6"
STORM-3558,com/codahale/metrics/JmxReporter is not found on LocalCluster,"Hi team,

I need add the following metrics-core dependency in the pom.xml, otherwise the exception will be thrown when I run storm locally. Is it a bug?

pom.xml:

    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-server</artifactId>
        </dependency>

        <dependency>
            <groupId>com.codahale.metrics</groupId>
            <artifactId>metrics-core</artifactId>
            <scope>compile</scope>
        </dependency>

    </dependencies>

Exception log:

18:10:10.810 [main] INFO  o.a.s.d.m.ClientMetricsUtils - Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
18:10:10.811 [main] INFO  o.a.s.d.m.r.JmxPreparableReporter - Preparing...
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/codahale/metrics/JmxReporter
	at org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter.prepare(JmxPreparableReporter.java:32)
	at org.apache.storm.metric.StormMetricsRegistry.startMetricsReporters(StormMetricsRegistry.java:74)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:287)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:159)
	at com.cc.trident.TridentWordCountTopologyLocal.main(TridentWordCountTopologyLocal.java:29)
Caused by: java.lang.ClassNotFoundException: com.codahale.metrics.JmxReporter
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 5 more
"
STORM-3557,allow health checks to pass on timeout,"We've seen nodes with high loads that timeout health checks periodically.  This leads to killing workers unnecessarily.

 

I'd like an option to not fail when timeouts occur, and to have a metric to track when these occur.

 "
STORM-3556,blob meta exception in Nimbus log,"Hi team,

When I kill topology on the UI, [WARN] get blob meta exception will be thrown in the Nimbus log. Is it a bug?


2019-12-18 02:14:16.782 o.a.s.d.n.Nimbus pool-29-thread-46 [INFO] Created download session ec557d66-96d9-4071-b99f-3122fa50f61c for word-count-9-1576653249-stormjar.jar
2019-12-18 02:14:16.794 o.a.s.d.n.Nimbus pool-29-thread-50 [INFO] Created download session 1bbae64e-89c6-4337-b87b-634dbb78f61c for word-count-9-1576653249-stormconf.ser
2019-12-18 02:14:16.807 o.a.s.d.n.Nimbus pool-29-thread-40 [INFO] Created download session a3f78bd3-673f-4780-a402-90708bd754f7 for word-count-9-1576653249-stormcode.ser
2019-12-18 02:14:17.472 o.a.s.d.n.Nimbus pool-29-thread-18 [INFO] Created download session a05848fa-e83e-4c13-b025-f14a468acf14 for word-count-9-1576653249-stormjar.jar
2019-12-18 02:14:17.483 o.a.s.d.n.Nimbus pool-29-thread-58 [INFO] Created download session 9119de95-cd34-48a4-b7ef-ae0f84de5689 for word-count-9-1576653249-stormcode.ser
2019-12-18 02:14:17.495 o.a.s.d.n.Nimbus pool-29-thread-29 [INFO] Created download session d8d06e2e-e82c-46a8-beee-ee8bf313e584 for word-count-9-1576653249-stormconf.ser
2019-12-18 02:14:17.538 o.a.s.d.n.Nimbus pool-29-thread-21 [INFO] Created download session f634e74b-a161-4295-bf17-bd631b3d6e6d for word-count-9-1576653249-stormjar.jar
2019-12-18 02:14:17.546 o.a.s.d.n.Nimbus pool-29-thread-18 [INFO] Created download session 29e1592a-136a-4ff6-a4de-9c4420d55849 for word-count-9-1576653249-stormcode.ser
2019-12-18 02:14:17.557 o.a.s.d.n.Nimbus pool-29-thread-48 [INFO] Created download session ca75e5b0-e855-4b5f-962f-f7accefdc260 for word-count-9-1576653249-stormconf.ser
2019-12-18 02:23:42.461 o.a.s.d.n.Nimbus pool-29-thread-36 [INFO] TRANSITION: word-count-9-1576653249 KILL 5 true
2019-12-18 02:23:42.463 o.a.s.d.n.Nimbus pool-29-thread-36 [INFO] Delaying event REMOVE for 5 secs for word-count-9-1576653249
2019-12-18 02:23:42.471 o.a.s.d.n.Nimbus pool-29-thread-36 [INFO] Adding topo to history log: word-count-9-1576653249
2019-12-18 02:23:47.464 o.a.s.d.n.Nimbus timer [INFO] TRANSITION: word-count-9-1576653249 REMOVE null false
2019-12-18 02:23:47.467 o.a.s.d.n.Nimbus timer [INFO] Killing topology: word-count-9-1576653249
2019-12-18 02:25:58.331 o.a.s.d.n.Nimbus timer [INFO] Cleaning up word-count-9-1576653249
2019-12-18 02:25:58.338 o.a.s.c.StormClusterStateImpl timer [INFO] Removing worker keys under /secretkeys/NIMBUS/word-count-9-1576653249
2019-12-18 02:25:58.339 o.a.s.c.StormClusterStateImpl timer [INFO] Removing worker keys under /secretkeys/DRPC/word-count-9-1576653249
2019-12-18 02:25:58.340 o.a.s.c.StormClusterStateImpl timer [INFO] Removing worker keys under /secretkeys/SUPERVISOR/word-count-9-1576653249
2019-12-18 02:25:58.341 o.a.s.d.n.Nimbus timer [INFO] Removing dependency jars from blobs - []
2019-12-18 02:26:04.774 o.a.s.d.n.Nimbus pool-29-thread-61 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: word-count-9-1576653249-stormcode.ser
	at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:257) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:287) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3651) [storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3951) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3930) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:174) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
2019-12-18 02:26:04.784 o.a.s.d.n.Nimbus pool-29-thread-5 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: word-count-9-1576653249-stormjar.jar
	at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:257) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:287) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3651) [storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3951) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3930) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:174) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]"
STORM-3555,Add meter for tracking errors killing workers,We've seen nodes fail to kill workers when they the processes end up in a defunct state.  I would like a meter to track these failures for alerting.
STORM-3554,LocalCluster can't be shutdown and JVM still run,"Hi team,

[Storm version: 2.1.0] 

According to the doc [https://storm.apache.org/releases/2.1.0/Local-mode.html], after I shutdown LocalCluster by calling shutdown or close method, JVM still run and can't exit on IDEA. I think it is a simple demo blow. I also try storm 1.2.3 and JVM can exit. Is it a bug for storm 2 ?

 

import org.apache.storm.Config;
 import org.apache.storm.LocalCluster;
 import org.apache.storm.topology.TopologyBuilder;

public class WordCountTopology {

public static void main(String[] args) throws Exception

{

TopologyBuilder builder = new TopologyBuilder();

// builder.setSpout(""spout"", new RandomSentenceSpout(), 5);

// builder.setBolt(""split"", new SplitSentenceBolt(), 8).shuffleGrouping(""spout"");

// builder.setBolt(""count"", new WordCountBolt(), 2).fieldsGrouping(""split"", new Fields(""word""));

Config conf = new Config();

conf.setDebug(true);

String topologyName = ""word-count"";

conf.setNumWorkers(2);

LocalCluster cluster = null;

cluster = new LocalCluster();

cluster.submitTopology(topologyName, conf, builder.createTopology());

Thread.sleep(10000);

// cluster.close();

cluster.shutdown();

}

}


POM.xml:

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <storm.version>2.1.0</storm.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${storm.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-server</artifactId>
            <version>${storm.version}</version>
        </dependency>
    </dependencies>
"
STORM-3552,Storm CLI set_log_level no longer updates the log level,"Using the example StatefulWindowingTopology, when trying to update the log level via command line with the following command a NullPointer is thrown in the worker log and the log level is not updated.
{code:java}
storm set_log_level -l ROOT=DEBUG:0 test{code}
{code:java}
2019-12-09 17:16:02.600+0100 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl main-EventThread [ERROR] Event listener threw exception
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_131]
        at org.apache.logging.log4j.Level.getLevel(Level.java:261) ~[log4j-api-2.11.2.jar:2.11.2]
        at org.apache.storm.daemon.worker.LogConfigManager.setLoggerLevel(LogConfigManager.java:145) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.daemon.worker.LogConfigManager.processLogConfigChange(LogConfigManager.java:98) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.daemon.worker.Worker.checkLogConfigChanged(Worker.java:422) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl.issueMapCallback(StormClusterStateImpl.java:177) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl$1.changed(StormClusterStateImpl.java:122) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.ZKStateStorage$ZkWatcherCallBack.execute(ZKStateStorage.java:243) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.zookeeper.ClientZookeeper.lambda$mkClientImpl$0(ClientZookeeper.java:314) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$7.apply(CuratorFrameworkImpl.java:1048) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$7.apply(CuratorFrameworkImpl.java:1041) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.processEvent(CuratorFrameworkImpl.java:1040) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.access$000(CuratorFrameworkImpl.java:66) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$1.process(CuratorFrameworkImpl.java:126) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.ConnectionState.process(ConnectionState.java:185) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:533) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]{code}
This appears to be a regression from the migration from clojure to java in STORM-1267"
STORM-3551,Fix LocalAssignment Equivalency in Slot for Generice Resource Aware Scheduler,"If supervisor defines generic resource then it needs to ignore it from comparison while assignment is not using such generic resource.

 

{code}

2019-12-03 21:02:10.635 o.a.s.d.s.Slot SLOT_6726 [INFO] SLOT 6726: Assignment Changed from LocalAssignment(topology_id:TEST-WordCount-281-1570127542, executors:[ExecutorInfo(task_start:261, task_end:261), ExecutorInfo(task_start:188, task_end:188)], resources:WorkerResources(mem_on_heap:10000.0, mem_off_heap:0.0, cpu:400.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:\{offheap.memory.mb=0.0, onheap.memory.mb=10000.0, cpu.pcore.percent=400.0}, shared_resources:{}), owner:bud_storm) to LocalAssignment(topology_id:TEST-WordCount-281-1570127542, executors:[ExecutorInfo(task_start:261, task_end:261), ExecutorInfo(task_start:188, task_end:188)], resources:WorkerResources(mem_on_heap:10000.0, mem_off_heap:0.0, cpu:400.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:\{offheap.memory.mb=0.0, network.resource.units=0.0, onheap.memory.mb=10000.0, cpu.pcore.percent=400.0}, shared_resources:{}), owner:bud_storm)

{code}"
STORM-3550,Remove unused dependencies,"There are some dependencies declared in *storm-core* that are not used. These dependencies should be removed to make the core library slimmer and its dependency tree less complex, which helps to maintain the project.

Unused dependencies with scope compile
 * *joda-time:joda-time*
 * *org.eclipse.jetty:jetty-util*
 * *org.eclipse.jetty:jetty-servlet*
 * *org.eclipse.jetty:jetty-servlets*
 * *com.fasterxml.jackson.core:jackson-core*
 * *com.fasterxml.jackson.dataformat:jackson-dataformat-smile*
 * *commons-fileupload:commons-fileupload*
 * *commons-codec:commons-codec*

Unused dependencies with scope test
 * *org.hamcrest:java-hamcrest*

PS: For some of these dependencies, such as *jackson-core* and *commons:codec,* I noticed that the same version is declared and included transitively in the dependency tree. This is redundant and should be avoided. The following is an excerpt of the Maven dependency tree that illustrates this situation.
{code:java}
+- com.fasterxml.jackson.core:jackson-core:jar:2.9.8:compile 
+- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.9.8:compile 
|  \- (com.fasterxml.jackson.core:jackson-core:jar:2.9.8:compile - omitted for duplicate)
{code}
{code:java}
+- commons-codec:commons-codec:jar:1.11:compile 
+- org.apache.hadoop:hadoop-auth:jar:2.8.5:compile
|  +- (commons-codec:commons-codec:jar:1.11:compile - omitted for duplicate)
. . .
{code}
 "
STORM-3549,use of topology specific jaas conf doesn't work with kafka,"{code:java}
2019-09-17 19:22:23.006 o.a.s.u.Utils Thread-22-line-reader-spout-executor[4, 4] [ERROR] Async loop died!
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:702) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) [storm-client-2.0.1.y.jar:2.0.1.y]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
Caused by: org.apache.kafka.common.KafkaException: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:86) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	... 10 more
Caused by: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:940) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.kafka.common.security.authenticator.AbstractLogin.login(AbstractLogin.java:69) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.kerberos.KerberosLogin.login(KerberosLogin.java:110) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.&lt;init&gt;(LoginManager.java:46) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:68) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:78) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	... 10 more
2019-09-17 19:22:23.196 o.a.s.e.e.ReportError Thread-22-line-reader-spout-executor[4, 4] [ERROR] Error
java.lang.RuntimeException: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.storm.utils.Utils$1.run(Utils.java:445) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:702) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	... 1 more
Caused by: org.apache.kafka.common.KafkaException: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:86) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	... 1 more
Caused by: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:940) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.kafka.common.security.authenticator.AbstractLogin.login(AbstractLogin.java:69) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.kerberos.KerberosLogin.login(KerberosLogin.java:110) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.&lt;init&gt;(LoginManager.java:46) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:68) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:78) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	... 1 more
2019-09-17 19:22:23.277 o.a.s.u.Utils Thread-22-line-reader-spout-executor[4, 4] [ERROR] Halting process: Worker died
java.lang.RuntimeException: Halting process: Worker died
	at org.apache.storm.utils.Utils.exitProcess(Utils.java:550) [storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$3.run(Utils.java:846) [storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.error.ReportErrorAndDie.uncaughtException(ReportErrorAndDie.java:41) [storm-client-2.0.1.y.jar:2.0.1.y]
	at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_181]
2019-09-17 19:22:23.281 o.a.s.u.Utils Thread-26 [INFO] Halting after 1 seconds
{code}"
STORM-3548,Remove iterator from Task.sendUnanchored,"Storm 2.x aims to remove iterators from the critical path to reduce garbage.

 

This method is called to send acking tuples so should use a for loop instead of list iterator."
STORM-3547,KinesisSpout incorrectly handles closed shards,"The KinesisSpout throws an exception when consuming closed Kinesis shards, which return null from `GetShardIterator`, as it tries to call `GetRecords` with the null `shardIterator` value instead of abandoning the closed shard.

This means KinesisSpout fails after re-sharding a stream with an exception like the following:

 
{code:java}
java.lang.RuntimeException: com.amazonaws.services.kinesis.model.AmazonKinesisException: 1 validation error detected: Value null at 'shardIterator' failed to satisfy constraint: Member must not be null (S ervice: AmazonKinesis; Status Code: 400; Error Code: ValidationException; Request ID: d85076a4-3953-fb6e-8e08-331e7d91ef0f) at org.apache.storm.utils.Utils$1.run(Utils.java:407) ~[storm-client-2.1.0.jar:2.1.0] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101] Caused by: com.amazonaws.services.kinesis.model.AmazonKinesisException: 1 validation error detected: Value null at 'shardIterator' failed to satisfy constraint: Member must not be null (Service: AmazonKin esis; Status Code: 400; Error Code: ValidationException; Request ID: d85076a4-3953-fb6e-8e08-331e7d91ef0f) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1695) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1350) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1101) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:758) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:732) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:714) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:674) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:656) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:520) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.doInvoke(AmazonKinesisClient.java:2803) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.invoke(AmazonKinesisClient.java:2772) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.invoke(AmazonKinesisClient.java:2761) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.executeGetRecords(AmazonKinesisClient.java:1288) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.getRecords(AmazonKinesisClient.java:1259) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisConnection.fetchRecords(KinesisConnection.java:113) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisRecordsManager.fetchNewRecords(KinesisRecordsManager.java:329) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisRecordsManager.next(KinesisRecordsManager.java:135) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisSpout.nextTuple(KinesisSpout.java:82) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:192) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:159) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.utils.Utils$1.run(Utils.java:392) ~[storm-client-2.1.0.jar:2.1.0]
{code}
 

Same Problem was resolved in https://issues.apache.org/jira/browse/BEAM-2582

 

 "
STORM-3546,Add container cleanup thread to Resource Isolation Interface ,
STORM-3545,blob update spews errors until cleanup occurs after topology killed,
STORM-3544,Ability to find topologies with low worker uptime on UI,useful for finding bad topologies / nodes
STORM-3543,Avoid iterators for task hook info objects,"A big drive was made in 2.0 to avoid iterators on the critical path to reduce garbage collection.

 

When task hooks are used, an iterator is used for each info object to pass it to all hooks.

 

As this is on the critical path when task hooks are used, the same reasoning should apply and the iterators be replaced with for loops."
STORM-3542,storm-kafka-monitor has wrong classpath,"When
{code:ruby}
ui.disable.spout.lag.monitoring: false
{code}
storm-kafka-monitor execution fails with   
{code:java}
Could not find or load main class org.apache.storm.kafka.monitor.KafkaOffsetLagUtil""
{code}
 

storm-kafka-monitor contains the following instruction:
{code:bash}
exec $JAVA $STORM_JAAS_CONF_PARAM $STORM_JAR_JVM_OPTS -cp $STORM_BASE_DIR/toollib/storm-kafka-monitor-*.jar org.apache.storm.kafka.monitor.KafkaOffsetLagUtil ""$@""{code}
When it should contain this one:
{code:bash}
exec $JAVA -Xms64m -Xmx64m $STORM_JAAS_CONF_PARAM -cp ""$STORM_BASE_DIR/lib-tools/storm-kafka-monitor/*"" org.apache.storm.kafka.monitor.KafkaOffsetLagUtil ""$@""
{code}"
STORM-3541,allow reporting of v2 metrics api using metrics tick,"We would like to be able to report v2 metrics api through the metrics tick, like v1 metrics.  

 

The main reason (besides keeping one reporter) is to reduce connection load on our openTSDB monitoring service.  Having one node doing reporting for a topology decreases the number of connections.

 

This also allows an easy migration path for users to the new v2 API.

 "
STORM-3540,Pacemaker race condition can cause continual reconnection,Seeing issues with connections to pacemaker with some workers despite pacemaker being up.
STORM-3539,Add metric for worker start time out,
STORM-3538,Add Meter for sendSupervisorAssignments exception,"We've observed exceptions which are only logged. Adding a meter allows improved tracking. 

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java#L292-L293]
2019-11-12 18:39:47.466 o.a.s.n.AssignmentDistributionService pool-25-thread-9 [ERROR] Exception when trying to send assignments to node 21b6cb54-da09-4e5e-a4ca-d0dc3a0df5bd-10.209.156.143-numa-1: SASL authentication not complete
2019-11-12 18:41:54.833 o.a.s.n.AssignmentDistributionService pool-25-thread-2 [ERROR] Exception when trying to send assignments to node 21b6cb54-da09-4e5e-a4ca-d0dc3a0df5bd-10.209.156.143-numa-1: java.net.SocketTimeoutException: Read timed out
2019-11-12 18:43:59.525 o.a.s.n.AssignmentDistributionService pool-25-thread-6"
STORM-3537,Ports configed in storm.yaml file can be used by other application.,"When submit a storm topology, it fail if the port is used by other application, for example a python Flask application.  
{code:java}
//代码占位符
// storm.yaml
supervisor.slots.ports:
 - 6720orc@bj2904:

// start storm nibums ans Supervisor

// start python flask 
~/program/simhash_doc_title$ ./start.sh 
 * Serving Flask app ""/home/orc/program/simhash_doc_title/simhash.py""
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://0.0.0.0:6720/ (Press CTRL+C to quit)


// submit storm topology
2019-11-15 18:16:12.310 o.a.s.u.Utils main [ERROR] Received error in thread main.. terminating server...
java.lang.Error: java.security.PrivilegedActionException: java.net.BindException: Address already in use
    at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:653) ~[storm-client-2.1.0.jar:2.1.0]
    at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:632) ~[storm-client-2.1.0.jar:2.1.0]
    at org.apache.storm.utils.Utils.lambda$createDefaultUncaughtExceptionHandler$2(Utils.java:1014) ~[storm-client-2.1.0.jar:2.1.0]
    at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_191]
    at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_191]
    at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_191]
Caused by: java.security.PrivilegedActionException
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_191]
    at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_191]
    at org.apache.storm.daemon.worker.Worker.start(Worker.java:180) ~[storm-client-2.1.0.jar:2.1.0]
    at org.apache.storm.daemon.worker.Worker.main(Worker.java:144) ~[storm-client-2.1.0.jar:2.1.0]
Caused by: java.net.BindException: Address already in use
    at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_191]
    at sun.nio.ch.Net.bind(Net.java:433) ~[?:1.8.0_191]
    at sun.nio.ch.Net.bind(Net.java:425) ~[?:1.8.0_191]
    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[?:1.8.0_191]
    at org.apache.storm.shade.io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:130) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1358) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:1019) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:366) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_191]
{code}"
STORM-3536,Add Generic-resources.md,
STORM-3535,KafkaOffsetMetric is not properly synchronized,"KafkaOffsetMetric.getValueAndReset runs in a different thread from the rest of the spout. It uses the KafkaConsumer from the KafkaSpout. The consumer is accessed through an unsynchronized field, and the spout may replace the consumer at any time.

We should consider whether we can fix this, or if it would be better to give the offset metric it's own consumer.

Also the metric accesses a number of properties in OffsetManagers, which are also not synchronized."
STORM-3534,Add generic resources to UI,
STORM-3532,Support Kubernetes in Storm Supervisor,"Hello,

We are using Apache Storm 1.x and we want to migrate it to 2.x on Kubernetes. I found that we can't request resources from Kubernetes directly. I searched if there is any integration with Kubernetes: when Storm Supervisor can run workers' slots inside separate pods(I refer to the [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java#L852]) instead of run multiple supervisors on the same K8s node.

I see the following tickets https://issues.apache.org/jira/browse/STORM-911 and https://issues.apache.org/jira/browse/STORM-1740 which could be related to my proposal.

 

I propose to add an ability to Storm Supervisor manage worker processes via Kubernetes /another API and not just on the same host.

 

 "
STORM-3531,Kafka Spouts Lag error when worker die,"I deploy topology on cluster (3 Supervisors) with 3 workers, 3 Spouts read topic kafka with 3 partitions.

Step 1: I kill worker1  on Supervisor1

Step 2: Worker1 automatic running again.

Step 3: Kafka spout of worker1 register partition 1 and read not lag. Worker2 and worker3 error lag kafka, not continue read and commit offset

Log worker: 

{color:#FF0000}Not polling on partition [TEST-0]. It has [250] uncommitted offsets, which exceeds the limit of [250]{color}"
STORM-3530,Improve Scheduling Failure Message,Users find it difficult to determine the cause of a topology scheduling failure.
STORM-3529,Catch and log RetriableException in KafkaOffsetMetric,"When the KafkaOffsetMetric.getValueAndReset method calls the KafkaClient methods, exceptions may be thrown. When these exceptions are retriable, we should not crash the worker by letting them escape the method. We should instead catch and log the exception.

An example of the desired behavior can be seen at https://github.com/apache/storm/blob/7b1a98fc10fad516ef9ed0b3afc53a1d7be8a169/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L295"
STORM-3528,Allow users to provide their own custom TriggerPolicy/EvictionPolicy in BaseWindowBolt,
STORM-3527,Container.getWorkerUser() should check if the user name is empty,"Sometimes supervisor got terminated/died during writing username to workers-users file. And when it happens, the file could be empty. And when supervisor recovers after, it wouldn't be able to get the correct username because the workers-users file is present but empty. So supervisor would never be able to clean up this worker and you could see in supervisor log file:

{code:java}
2019-10-21 18:26:48.272 o.a.s.u.LocalState timer [WARN] LocalState file '/home/y/var/storm/workers/a9290217-f83f-4c16-ac54-781aca150d7f/heartbeats/1571508791911' contained no data, resetting state
2019-10-21 18:26:49.282 o.a.s.u.LocalState timer [WARN] LocalState file '/home/y/var/storm/workers/94967b6b-c666-4020-9d2c-363551d1229b/heartbeats/1571508791904' contained no data, resetting state
2019-10-21 18:26:49.282 o.a.s.u.LocalState timer [WARN] LocalState file '/home/y/var/storm/workers/5aa891f0-9b9c-4914-8745-c55e99537ba1/heartbeats/1569158099433' contained no data, resetting state
2019-10-21 18:26:49.283 o.a.s.u.LocalState timer [WARN] LocalState file '/home/y/var/storm/workers/060056f4-9589-4473-b6d0-9ab5fdc278e2/heartbeats/1561524903510' contained no data, resetting state
2019-10-21 18:26:49.283 o.a.s.u.LocalState timer [WARN] LocalState file '/home/y/var/storm/workers/bb189497-eb21-48c4-ba62-48ee02acde94/heartbeats/1571508791741' contained no data, resetting state
{code}
"
STORM-3526,Always override Login Config from System property,"Many a times, daemons should be able to read system property passed as argument to JVM  to override default config file setting for login config"
STORM-3525,Large Contraint Solver test fails on some VM,TestConstraintSolverStrategy.testScheduleLargeExecutorConstraintCount() method fails on some VMs when the parallelismMultiplier is set 20.
STORM-3524,worker fails to launch due to missing parent directory for localized resource,"{code:java}
2019-10-14 14:59:29.839 o.a.s.l.LocalizedResource AsyncLocalizer Executor - 2 [WARN] Nothing to cleanup with badeDir /home/y/var/storm/supervisor/usercache/xxx/filecache/files even though we expected there to be something there 2019-10-14 14:59:29.839 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [WARN] Failed to download blob xxx:xxx.topology.yaml will try again in 100 ms java.nio.file.NoSuchFileException: /home/y/var/storm/supervisor/usercache/xxx/filecache/files at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[?:1.8.0_181] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_181] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_181] at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) ~[?:1.8.0_181] at java.nio.file.Files.createDirectory(Files.java:674) ~[?:1.8.0_181] at org.apache.storm.localizer.LocalizedResource.lambda$fetchUnzipToTemp$4(LocalizedResource.java:257) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.LocallyCachedBlob.fetch(LocallyCachedBlob.java:92) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.LocalizedResource.fetchUnzipToTemp(LocalizedResource.java:250) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$10(AsyncLocalizer.java:277) ~[storm-server-2.0.1.y.jar:2.0.1.y] at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) [?:1.8.0_181] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_181] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}
 

A worker on a supervisor was failing to come up with this error continually presenting."
STORM-3523,supervisor restarts when releasing slot with missing file,"{code:java}
2019-10-03 16:25:32.809 o.a.s.d.s.Slot SLOT_6719 [ERROR] Error when processing event
java.io.FileNotFoundException: File 'x/storm/supervisor/stormdist/xxx-190213-004131-001-209-1550018519/stormconf.ser' does not exist
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:308) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:469) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:303) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:359) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:460) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:435) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:229) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:900) [storm-server-2.0.1.y.jar:2.0.1.y]
2019-10-03 16:25:32.810 o.a.s.u.Utils SLOT_6719 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:550) [storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:947) [storm-server-2.0.1.y.jar:2.0.1.y]
{code}"
STORM-3522,Some code can not find,"When I download source code by github, end import the source code into intellij idea. but the code do not work, some code can not find.

How can i resolve this issue

For example, the org.apache.storm.daemon.nimbus.Nimbus depend on org.apache.storm.shade.com.google.common.base.Strings, but can not find the java file in storm-core

 "
STORM-3521,Storm CLI jar command doesn't handle topology arguments correctly,
STORM-3519,Change ConstraintSolverStrategy::backtrackSearch to avoid StackOverflowException,"When ConstraintSolverStrategy::backtrackSearch recursively call itself - after approximately 20000 calls, there is a StackOverflowException. This can be replicated by running TestConstraintSolverStrategy::testScheduleLargeExecutorConstraintCount."
STORM-3518,configs for favored nodes and unfavored nodes should support range of numbers,"Apache Storm has two configs for topologies to choose favored nodes and unfavored nodes. 

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/Config.java#L351
{code:java}
    /**
     * A list of host names that this topology would prefer to be scheduled on (no guarantee is given though). This is intended for
     * debugging only.
     */
    @IsStringList
    public static final String TOPOLOGY_SCHEDULER_FAVORED_NODES = ""topology.scheduler.favored.nodes"";
    /**
     * A list of host names that this topology would prefer to NOT be scheduled on (no guarantee is given though). This is intended for
     * debugging only.
     */
    @IsStringList
    public static final String TOPOLOGY_SCHEDULER_UNFAVORED_NODES = ""topology.scheduler.unfavored.nodes"";
{code}

It only support plain text currently, for example


{code:java}
host1.yahoo.com
host2.yahoo.com
hostA3.yahoo.com
hostA4.yahoo.com
{code}

 It would be nice to be able to support ranges like

{code:java}
host[1-2].yahoo.com
hostA[3-4].yahoo.com
{code}


"
STORM-3517,NotifyBuilder.from does not normalize endpoint URI,"When using NotifyBuilder.from, it is possible to supply both a URI pattern and an actual URI for matching. NotifyBuilder matches actual endpoint URIs against the specified pattern using EndpointHelper.matchEndpoint(URI endpointUri, String pattern), which does either exact or regex matching. Before matching, matchEndpoint will normalize the URI in order to ensure that e.g. query parameter order doesn't matter.

It would be nice if NotifyBuilder.from would attempt to normalize the pattern/URI specified in the ""from"" method. If it isn't possible to normalize, the pattern can be passed to matchEndpoint without modification, but if the ""from"" method is used with a concrete URI, query parameter order there can be some surprising behavior during the matching.

For example, given a route like

from(""sjms:queue:my-queue?transacted=true&consumerCount=1"")

the following will not match

new NotifyBuilder()
.from(""sjms:queue:my-queue?transacted=true&consumerCount=1"")

The reason for this is that Camel will normalize the route URI given in the actual route before passing it to EndpointHelper, but the pattern given to NotifyBuilder is not normalized, so the order of query parameters end up not matching.

It would be good to either update NotifyBuilder so it tries normalizing the URI, or updating EndpointHelper.matchEndpoint, so it tries normalizing both the URI parameter and the pattern parameter."
STORM-3516,Delayed Kill or Rebalance Topology not processed on Nimbus restart,
STORM-3514,"""topology.enable.message.timeouts: false"" has no effect on ackers","""topology.enable.message.timeouts: false"" does prevent tuples from being failed if not acked in ""topology.message.timeout.secs"" seconds, but it still prevents __ackers from acking anchored tuples to the spout. When used with ""topology.max.spout.pending"" this effectively stalls the spout completely as the tuple is neither failed, nor acked."
STORM-3513,Add note to install instructions for Windows users that they need to install Visual C++ redistributable,"See the note at the bottom of this section https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#maven

And also this stackoverflow question https://stackoverflow.com/questions/58123218/i-have-a-trouble-running-nimbus-apache-storm-2-0-0-on-windows"
STORM-3512,Nimbus failing on startup with `GLIBC_2.12' not found,"Nimbus failing to start with and exception (see below).

 
{code:java}
2019-09-25 17:21:56.013 o.a.s.u.Utils main [ERROR] Received error in thread main.. terminating server...
java.lang.Error: java.lang.UnsatisfiedLinkError: /tmp/librocksdbjni3787537456845796855.so: /lib64/libpthread.so.0: version `GLIBC_2.12' not found (required by /tmp/librocksdbjni3787537456845796855.so)
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:647) ~[storm-client-2.0.0.jar:2.0.0]
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:626) ~[storm-client-2.0.0.jar:2.0.0]
        at org.apache.storm.utils.Utils.lambda$createDefaultUncaughtExceptionHandler$2(Utils.java:982) ~[storm-client-2.0.0.jar:2.0.0]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_211]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_211]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_211]
Caused by: java.lang.UnsatisfiedLinkError: /tmp/librocksdbjni3787537456845796855.so: /lib64/libpthread.so.0: version `GLIBC_2.12' not found (required by /tmp/librocksdbjni3787537456845796855.so)
        at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_211]
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_211]
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[?:1.8.0_211]
        at java.lang.Runtime.load0(Runtime.java:809) ~[?:1.8.0_211]
        at java.lang.System.load(System.java:1086) ~[?:1.8.0_211]
        at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:56) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.RocksDB.loadLibrary(RocksDB.java:64) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.RocksDB.<clinit>(RocksDB.java:35) ~[rocksdbjni-5.8.6.jar:?]
        at org.apache.storm.metricstore.rocksdb.RocksDbStore.prepare(RocksDbStore.java:67) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.metricstore.MetricStoreConfig.configure(MetricStoreConfig.java:33) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:528) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:471) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:465) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.launchServer(Nimbus.java:1282) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1307) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1312) ~[storm-server-2.0.0.jar:2.0.0]
 {code}
 

Environment:
{code:java}
>>> uname -a
Linux gctdwp03 3.0.101-108.98-default #1 SMP Mon Jul 15 13:58:06 UTC 2019 (262a94d) x86_64 x86_64 x86_64 GNU/Linux
 
>>> cat /etc/SuSE-release
SUSE Linux Enterprise Server 11 (x86_64)
VERSION = 11
PATCHLEVEL = 4{code}
 "
STORM-3511,Nimbus logs got flood with TTransportException Error messages (because of thrift 0.12.0),"Submitting a wordCountTopology works in secure cluster. But the following

{code:java}
2019-09-25 13:53:46.560 o.a.s.t.s.TThreadPoolServer pool-15-thread-1 [ERROR] Thrift error occurred during processing of message.
org.apache.storm.thrift.transport.TTransportException: null
        at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.read(TSaslTransport.java:433) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:27) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310) [shaded-deps-2.0.1.y.jar:2.0.1.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}
flood the nimbus log. (2.0.1.y is our internal version. The code is basically community 2.0.0)

This is similar to the issue found in Thrift community and got fixed in 0.13.0 but it's not released yet https://issues.apache.org/jira/browse/THRIFT-4805

"
STORM-3510,WorkerState.transferLocalBatch backpressure resend logic fix,"WorkerState.transferLocalBatch uses an int lastOverflowCount to track the size of the overflow queue, and periodically resend the backpressure status to remote workers if the queue continues to grow.

 

The current implementation has two problems:
 * The single variable tracks the receive queue of every executor in the worker, meaning it will be overwritten as tuples are sent to different executors.
 * The variable is locally scoped, and so is not carried over between mini-batches.

 

This only comes in to effect when the overflow queue grows beyond 10000, which shouldn't happen unless a backpressure signal isn't received by an upstream worker, but if it does happen then a backpressure signal is going to be sent for every mini-batch processed.  I do not know if this is the intended behaviour, but the way the code is written seems to indicate that it isn't.

 

I have thought of two redesigns to fix these problems and make the behaviour align with how one would interpret the code:

 
 #  *Change the lastOverflowCount variable to a map of taskId to overflow count* - This will retain the behaviour of resending the backpressure update every mini-batch once over the threshold, if that behaviour is intended.  However, it will increase garbage by creating a new map every time WorkerState.transferLocalBatch is called by the NettyWorker thread.
 # *Change the lastOverflowCount variable to a map of taskId to overflow count* *and move it to the BackPressureTracker class* - This will retain the counter between batches, and so only resend backpressure status every 10000 received tuples per task.

 

My preference is for the second option, as if the intended behaviour is to resend every mini batch it should be rewritten so the intent is explicit from the code.

 

It is also possible that doing it the second way could run in to concurrency issues i didn't think of, but as far as i can tell the topology.worker.receiver.thread.count config option isn't used at all?  If that's the case and there is only one NettyWorker thread per worker then it should be fine.

 

I have implemented both methods and attempted to benchmark them with [https://github.com/yahoo/storm-perf-test] but as i am running all workers on one machine i couldn't get it to the point that the relevant code was ever called."
STORM-3509,Improved RAS scheduling,Don't scheduling topology if topology resource requirements exceeds total available on cluster.
STORM-3508,The links to download in setting up environmtn page are broken,Navigate to [https://storm.apache.org/releases/2.0.0/Setting-up-development-environment.html] clicking on downloads page will result in 404
STORM-3507,Need feedback from blacklisting to scheduling,"It would be really nice if the scheduler could know which nodes would have been blacklisted but were released because the cluster was full. Then the scheduler could give good nodes to high priority jobs, and low priority once get the possibly bad nodes."
STORM-3506,prevent topology from overriding STORM_CGROUP_HIERARCHY_DIR and WORKER_METRICS,We had an issue where users were using older versions of storm the set these differing values than the cluster supported.  These parameters don't make sense for topologies to override.
STORM-3505,improve topology config creation,"We currently allow topologies to possibly override any field in a config.  This can cause problems requiring special case filtering when creating the conf at submission time.

Going forward it would be nice to have a strict naming convention for all topology conf keys (and code support) to prevent overriding anything that is not topology specific.

 

 "
STORM-3504,AsyncLocalizerTest is stubbing file system operations,"AsyncLocalizerTest mocks AdvancedFSOps in order to avoid interacting with the real file system. This is most likely unnecessary, and could be replaced with using temporary files/directories. If possible, we should rewrite the tests to use temporary files, and use the real AdvancedFSOps."
STORM-3503,Create unit tests for blacklistOnBadSlot option,follow up on STORM-3492
STORM-3502,metrics-core isn't shaded,"""Apache Storm version 1.2 introduced a new metrics system for reporting internal statistics (e.g. acked, failed, emitted, transferred, disruptor queue metrics, etc.) as well as a new API for user defined metrics.

The new metrics system is based on [Dropwizard Metrics|http://metrics.dropwizard.io/].""

Currently at my company we're still using a storm version that does not have any metric reporter (1.1.x), we're planning to move to 2.2 version, however since, we've been using codahale metrics reporter to track some events on storm and since the newer versions already include this, it means that we have to change every topology we already have in production.

It would be nice, that this dependency inside of storm could be shaded in order to ease any migration, or at least without a lot of refactoring on the topologies, I assume this problem must be similar in other companies, and storm already does this for other artifacts.

I would be happy to perform the necessary changes in order to accomplish this."
STORM-3499,allow bulk indexing in elasticsearch,"For a particular use case, I created custom bolt that can push data to ES in bulk. It's quick and dirty code as of now. The bolt acks tuples as soon as it revives it, before sending to ES. (need more thoughts/suggestions on this behavior). Can we make this as feature of storm? I am willing to contribute."
STORM-3498,Fix missing cases of invoking bash directly without /bin/env,
STORM-3497,Add connector for Alibaba Log Service,"Alibaba Log Service is a big data service which has been widely used in Alibaba Group and thousands of customers of Alibaba Cloud. The core storage engine of Log Service is named Loghub which is a large scale distributed storage system, which provides producer and consumer to push and pull data like AWS Kinesis or Azure Eventhubs does. 

Log Service provides a complete solution to help user collect data from both on premise and cloud data sources. More than 10 PB data is sent to and consumed from Loghub every day. And hundreds of thousands of users of Alibaba are working with Log Service, and Storm is an important component for building their big data system.

We'd like to contribute a connector to Storm. Happy to hear any comments. Thank you in advance."
STORM-3496,Improve ConstraintSolverStrategy::backtrackSearch,"Method is a recursive call, which can result in stack overflow.

Convert to iterative method."
STORM-3495,TestConstraintSolverStrategy is not stable on travis,"This test fails occasionally, hitting stack overflow on with parallelismMultiplier of 5. This test requires recursing 3000 times, but overflow has occurred with 1000."
STORM-3494,Use UserGroupInformation to login to HDFS only once per process,"UserGroupInformation (UGI) loginUserFromKeytab should be used only once in a process to login to hdfs because it overrides static fields. Also loginUserFromKeytabAndReturnUGI function is also problematic according to hadoop team. So the correct way to connect to hdfs is to use UGI loginUserFromKeytab once and only in a process.

Currently we only use HDFS in hdfs-blobstore. It works correctly. But the code is implemented in the hdfs-blobstore plugin. It will be problematic if we want to add another plugin which also needs to connect to HDFS.

So the proposal here is to remove the login piece of code from hdfs-blobstore. And explicitly login to hdfs once and only once when the server (nimbus, supervisor, etc) or the client (storm cli command) launches. It can guarantee one login per process.

The plugins like hdfs-blobstore then simply assume the process has already logged in."
STORM-3493,Allow overriding python interpreter by environment variable,"$subj

https://github.com/apache/storm/pull/3111

 "
STORM-3492,Adding configuration for blacklisting scheduler behavior,"Recently, we have experience situations where supervisor kept restarting workers when some problematic topologies were submitted. This will cause supervisor get blacklisted even if the supervisor is good.

Some large topologies may causes a significant part of nodes to blacklist. 

 "
STORM-3491,BoltReaderRunnable shouldn't throw IllegalArgumentException for sync command,"So I was preparing for a new release candidate i.e. rc3. I can build it from source without any problem. Then I set up a standalone cluster and submitted a WordCountTopology. The workers kept restarting because of 

{code:java}
2019-08-19 15:09:49.635 o.a.s.t.ShellBolt Thread-30 [ERROR] Halting process: ShellBolt died. Command: [python, splitsentence.py], ProcessInfo pid:3756, name:split exitCode:-1, errorString:
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
2019-08-19 15:09:49.635 o.a.s.e.e.ReportError Thread-30 [ERROR] Error
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
2019-08-19 15:09:49.636 o.a.s.t.ShellBolt Thread-28 [ERROR] Halting process: ShellBolt died. Command: [python, splitsentence.py], ProcessInfo pid:3755, name:split exitCode:-1, errorString:
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
2019-08-19 15:09:49.637 o.a.s.e.e.ReportError Thread-28 [ERROR] Error
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}


https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/task/ShellBolt.java#L365-L367 I believe we shouldn’t throw exceptions here. "
STORM-3490,Add checkstyle rule RedundantModifier,Rule is already practiced in most cases. And enforcing it increases clarity.
STORM-3489,Improve RAS scheduling eviction,"When RAS scheduling trial fails, scheduler evicts lower priority topologies to make room. Scheduler attempts to evict until released resources >= topology required resources.This ignores partially scheduled topology, causing more evicting that needed."
STORM-3488,Scheduling can cause RAS_Node resources to become negative,"When attempting to add a new executor to an empty worker slot, shared node memory is ignored."
STORM-3487,Storm UI does not display detailed version info correctly,"h2. Cluster Summary
||Version||
URL: ${version-info.scm.uri} 
Revision: ${version-info.scm.commit} "
STORM-3486,Upgrade to Jersey 2.29,Jersey 2.29 supports Java 11 http://blog.supol.cz/?p=144
STORM-3485,VersionInfoMojo fails to run when the sources are built outside the Git repo,
STORM-3484,Add Blacklisted Supervisors Info To UI,"Mentioned in earlier blacklist scheduler discussion yet no follow-up on it: [https://github.com/apache/storm/pull/1674#issuecomment-244952101]

I think this could be a piece of useful information to monitor the cluster status "
STORM-3483,Backpressure turns on after topology is idle for 160 minutes,"Storm version used 1.2.2

While testing a topology we noticed that backpressure was being triggered on an idle topology, without any data going through the system for roughly 160 minutes. However once we starting putting data through the system, backpressure behaved as normal and is off.

We have storm metrics logger to log the disruptor queue sizes, and during this test we can see the receive queue population is 0 when backpressure kicks in. 

 

We confirmed this by turning debug logs on for storm.

!image-2019-08-08-17-32-10-241.png!

 

In an idle system where backpressure occurs... 

!image-2019-08-08-16-02-09-750.png!

 

 

What a normal system with data going through looks like...

!image-2019-08-08-15-59-25-655.png!

 "
STORM-3482,Implement One Worker Per Component Option,"User complaints saying that Storm is hard to debug, mainly because Storm runs executors from different components (spout, bolt) in a single JVM process.

Adding an option that schedules executors from from a single component on a worker. "
STORM-3481,IllegalArgumentException in ConstraintSolverStrategy,"We found this scheduling error based on our internal mirror. 
{code:java}
2019-08-06 13:00:20.344 o.a.s.s.r.ResourceAwareScheduler timer [ERROR] propane-0-170-1564778552 Internal Error - Exception thrown when scheduling. Please check logs for details
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Don't know how to convert null to int
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_181]
        at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_181]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.scheduleTopology(ResourceAwareScheduler.java:164) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.schedule(ResourceAwareScheduler.java:117) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.blacklist.BlacklistScheduler.schedule(BlacklistScheduler.java:118) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:2092) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lockingMkAssignments(Nimbus.java:2256) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2242) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2187) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$29(Nimbus.java:2890) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.1.y.jar:2.0.1.y]
Caused by: java.lang.IllegalArgumentException: Don't know how to convert null to int
        at org.apache.storm.utils.ObjectReader.getInt(ObjectReader.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.strategies.scheduling.ConstraintSolverStrategy.schedule(ConstraintSolverStrategy.java:263) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.lambda$scheduleTopology$3(ResourceAwareScheduler.java:161) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_181]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_181]
{code}
"
STORM-3480,Implement One Executor Per Worker RAS Option,This would create single worker per executor option to get people to submit their topology with same parallelism but be able to clearly identify the workers that are slow/crashing/ or be able to estimate their resource requirements and capacity.
STORM-3479,HB timeout configurable on a topology level,"HB timeout on a cluster level may not be applicable to some GC/memory heave bolts. And it can easily get HB timeout which causes the worker get killed.
HB timeout on a topology level should be supported and users should be able to configure it."
STORM-3478,Upgrade to JUnit 5.5.1,"https://github.com/apache/storm/pull/2990 upgraded to 5.5.0-M1, we should upgrade to a final version."
STORM-3477,HDFS blobstore isRemoteBlobExists performs unnecessary file opens,"isRemoteBlobExists eventually performs an HDFS file open and returns an input file stream, which is not closed.

 

We should just be calling file exists instead.  Lower HDFS overhead."
STORM-3476,LocalizedResourceRetentionSet cleanup causing excessive load on Hadoop namenode,"One of our local dev Hadoop devs noticed our storm user was by far creating the heaviest load on our production Hadoop cluster.  Looking at one of the heaviest supervisor nodes, and comparing debug logs to the Hadoop audit log, it looks like LocalizedResourceRetentionSet cleanup was constantly doing opens and never deleting any files.

 

The frequency can be addressed by supervisor.localizer.cleanup.interval.ms, but even so, it seems we will continually look for files to delete even when the target size is acceptable, resulting in unnecessary calls to Hadoop.

 

 "
STORM-3475,Add ConstraintSolverStrategy Unit Test,"When constraint solver has large number of executors, scheduling can fail due to StackOverflow"
STORM-3474,Create a test case with a fragmented cluster to simulate/reproduce large scheduling time,"Adding a test to ensure that changes to scheduling don't adversely affect scheduling time, particularly with large fragmented clusters."
STORM-3473,Hive can't read records written from HiveBolt,"I'm trying to stream items from storm into hive using the HiveBolt, but Hive does not seem to see the records at all.

Test program:
{code:java}
package com.datto.hivetest;

import org.apache.storm.Config;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.AlreadyAliveException;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.hive.bolt.HiveBolt;
import org.apache.storm.hive.bolt.mapper.JsonRecordHiveMapper;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.streams.StreamBuilder;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.Time;

import java.util.Map;
import java.util.Random;

public class MainStorm {
	public static void main(String[] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
		HiveOptions hiveOptions = new HiveOptions(
			""<url>"",
			""default"",
			""test_table"",
			new JsonRecordHiveMapper()
				.withColumnFields(new Fields(""value""))
		)
			.withAutoCreatePartitions(true);

		StreamBuilder builder = new StreamBuilder();
		builder.newStream(new TestSpout())
			.map(tup -> tup.getStringByField(""word"").toLowerCase())
			.to(new HiveBolt(hiveOptions));

		Config config = new Config();
		config.setMessageTimeoutSecs(30);
		config.setMaxSpoutPending(1024);
		config.setClasspath(""/etc/hadoop/conf/"");

		StormSubmitter.submitTopology(""hive-test"", config, builder.build());
	}

	public static class TestSpout extends BaseRichSpout {
		private transient SpoutOutputCollector out;
		private transient Random random;

		@Override
		public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
			out = collector;
			random = new Random();
		}

		@Override
		public void nextTuple() {
			try {
				Time.sleep(100);
			} catch (InterruptedException e) {
				Thread.currentThread().interrupt();
				throw new RuntimeException(e);
			}

			final String[] words = new String[]{ ""nathan"", ""mike"", ""jackson"", ""golda"", ""bertels"" };
			final String word = words[random.nextInt(words.length)];
			out.emit(new Values(word));
		}

		@Override
		public void declareOutputFields(OutputFieldsDeclarer declarer) {
			declarer.declare(new Fields(""word""));
		}
	}
}
{code}
Table creation:
{code:sql}
CREATE TABLE test_table (value string) CLUSTERED BY (value) INTO 4 BUCKETS STORED AS ORC TBLPROPERTIES('orc.compress' = 'ZLIB', 'transactional' = 'true');

GRANT ALL ON test_table TO USER storm;{code}

Setting the ACL:

{code}
sudo -u hdfs hdfs dfs -setfacl -m user:storm:rwx /warehouse/tablespace/managed/hive/test_table
sudo -u hdfs hdfs dfs -setfacl -m default:user:storm:rwx /warehouse/tablespace/managed/hive/test_table
{code}

Hive results after running for around 10 minutes:

{code:java}
> SELECT COUNT(*) FROM test_table;
INFO  : Compiling command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403): SELECT COUNT(*) FROM test_table
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403); Time taken: 1.138 seconds
INFO  : Executing command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403): SELECT COUNT(*) FROM test_table
INFO  : Completed executing command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403); Time taken: 0.013 seconds
INFO  : OK
+------+
| _c0  |
+------+
| 0    |
+------+
{code}

So hive thinks there are no results, which isn't good. But if I look at hdfs, there are some files there:

{code}
# sudo -u hdfs hdfs dfs -ls -R -h /warehouse/tablespace/managed/hive/test_table
drwxrwx---+  - storm hadoop          0 2019-07-22 19:15 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100
-rw-rw----+  3 storm hadoop          1 2019-07-22 19:15 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/_orc_acid_version
-rw-rw----+  3 storm hadoop     74.4 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00001
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00001_flush_length
-rw-rw----+  3 storm hadoop     73.4 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00002
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00002_flush_length
-rw-rw----+  3 storm hadoop     84.9 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00003
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00003_flush_length
{code}

And they seem to have valid rows:

{code}
❯❯❯ ./orc-contents /tmp/bucket_00002  | head
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 0, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 1, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 2, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 3, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 4, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 5, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 6, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 7, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 8, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 9, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{code}

I can insert into the table manually, and I've also written a test java program that uses the hive streaming API to write one row, and hive sees those inserts. I don't see any errors in the storm logs; the tuples seem to be flushed and acked ok. I don't think I've seen any errors in the metastore logs either.

Anyone know what's up? I can get more info if needed."
STORM-3471,{{page.git-blob-base}} variable is mis-interpreted across all documentation,"*Edited. After having looked into source code, it looks like many hyperlinks are using parameter 
{\{page.git-blob-base}} which is rendered as [https://github.com/apache/storm/blob/master/docs/%7B%7Bpage.git-blob-base%7D%7D] instead of the correct path [https://github.com/apache/storm/blob/master/] .
 Looks like a configuration issue, but I am not sure where to make the proper fix.

For example in [Metrics.md|https://github.com/apache/storm/blob/master/docs/Metrics.md]:
 Clicking into [AssignableMetric](
{\{page.git-blob-base}}/storm-client/src/jvm/org/apache/storm/metric/api/AssignableMetric.java) will lead to a 404 page [https://github.com/apache/storm/blob/master/docs/%7B%7Bpage.git-blob-base%7D%7D/storm-client/src/jvm/org/apache/storm/metric/api/AssignableMetric.java]. The correct location will be [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/metric/api/AssignableMetric.java]"
STORM-3470,Possible Null Dereference in SimpleSaslServer authentication function,"On line 183, nid could possible be null. By comparing using ""nid.equals()"" we will get a null pointer exception instead of comparing possibly null values."
STORM-3468,https://github.com/apache/storm/blob/asf-site/README.md incorrect pointer,"https://github.com/apache/storm/blob/asf-site/README.md says:

bq. All of this moved back to subversion. Please see https://svn.apache.org/viewvc/storm/site/

However it is at:

https://github.com/apache/storm-site"
STORM-3466,storm-kafka-monitor not found jar,"Hi, i'm a beginner to the storm. in order words, i'm a newbie.

 

I have tried to upgrade from Storm 1.2.2 to 2.0.0. and confirmed that kafka spout lag does not work. changed the value of ui.disable.spout.lag.monitoring to false, but it did not work.

So I checked the log of storm ui and got the following error message.

 

org.apache.storm.utils.ShellUtils$ExitCodeException: Error: Could not find or load main class .apache-storm-2.0.0.lib-tools.storm-kafka-monitor.audience-annotations-0.5.0.jar

at org.apache.storm.utils.ShellUtils.runCommand(ShellUtils.java:271) ~[storm-client-2.0.0.jar:2.0.0]
 at org.apache.storm.utils.ShellUtils.run(ShellUtils.java:194) ~[storm-client-2.0.0.jar:2.0.0]...

 

In version 1.2.2, the storm-kafka-monitor works fine. However, version 2.0.0 throws an error."
STORM-3459,Allow pull request authors to restart their failed Travic CI builds,"I opened about 10 pull requests within the last week and almost none of them passed CI because of various issues which are inside (integration test failures) and outside the scope of Storm (apt-get and maven connection failures). Often only one job fails. I guess devs don't feel that great merging a red build and also have the overhead to evaluate the build failure.

I can start the builds of the PR branch of my fork, but since I don't have permissions to restart the builds of my pull requests on the project, I need to amend the commit and run the complete set of jobs again. This is a waste of resources and often another fails instead so that I have to repeat the process.

As soon as [https://github.com/travis-ci/travis-ci/issues/887] is fixed it should be enabled for the project."
STORM-3442,Add owner to supervisor summary,It would be convenient to have the topology owner on the supervisor page. Reduces searching when trying to track down problem topologies.
STORM-3439,Upgrade checkstyle version used by maven-checkstyle-plugin,Allows finding certain issues which are not covered by the current version.
STORM-3436,"TupleInfo.id is not set, making debugging more difficult than it should be","TupleInfo has an id field, which is supposed to contain the tuple root id. This is printed when we enable topology debug logging, and is supposed to make it easier to track down e.g. why a tuple timed out.

We currently don't set this field, so it's always null. The logs end up looking like

2019-06-29 11:26:31.990 o.a.s.e.s.SpoutExecutor Thread-14-kafka_spout-executor[4, 4] [INFO] SPOUT Acking message null {topic-partition=kafka-spout-test-0, offset=28, numFails=0, nullTuple=false}

The message doesn't contain the root id, and is therefore not useful for debugging."
STORM-3434,server: fix all checkstyle warnings,Fix all checkstyle issues in server
STORM-3423,Logviewer Log Is Ambiguous,"{code:java}
2019-06-20T20:38:11.626Z gsbl839n04.blue.ygrid.yahoo.com [logviewer][99056] No more files able to delete this round, but this directory is over quota by 5269.828259999999 MB
2019-06-20T20:38:11.626Z gsbl839n04.blue.ygrid.yahoo.com [logviewer][99056] No more files able to delete this round, but this directory is over quota by 5269.828259999999 MB
2019-06-20T20:38:11.626Z gsbl839n04.blue.ygrid.yahoo.com [logviewer][99056] No more files able to delete this round, but this directory is over quota by 5269.828259999999 MB
{code}

Log information is not clear for debugging"
STORM-3412,Jira and Central Log Links Disappear In Topo Page,"When clicking into topology page in Storm UI, at top right part, the bug-tracker and central-log outlink icon will disappear even if they are configured."
STORM-3411,Worker Log Download Only Create Generic Name Worker.log,"1) Navigate to a worker log
2) Download full file

Actual: Browser saves as \{{worker.log}} every time.

Expected: Browser saves a file name corresponding to \{code}<topo-id>-<port>-worker.log\{code}"
STORM-3410,Support Redis Sentinel,"The Redis bolts and state backend support single-node and Redis Cluster setups, but it does not support Redis Sentinel.

We at Datto use the sentinel setup, so we can't use the Redis bolts (at least not with high availability).

I might start working on a patch to implement this, depending on what we decide to use."
STORM-3407,Storm 2.x ConstraintSolverStrategy failing on StackOverflow,"Customer running old (version < 2.x) topology fails ConstraintSolverStrategy.

The 2.x _ConstraintSolverStrategy_ is using _topology.ras.constraint.max.state.search_, changed from _topology.ras.constraint.max.state.traversal_.

For backward compatibility, support _topology.ras.constraint.max__.state.traversal_ for old topologies."
STORM-3406,Allow customizing storm-hdfs FileReader ,Allow customizing storm-hdfs _FileReader_. The interface is currently package private preventing any possible custom implementation
STORM-3404,storm v1.2.2 KafkaOffsetLagUtil cant pull the offset correctly,"when use SASL_PLAIN kafka JAAS auth, missing sasl.mechanism will lead to KafkaOffsetLagUtil cant pull the offset correctly
 kafka version 2.2.0

storm version 1.2.2

The PR url is: https://github.com/apache/storm/pull/3377

 "
STORM-3403,Incorrect Assigned memory displayed on Storm UI,"Hi Team,

 

We are working on storm upgrade from 1.0.2 to 1.2.2. During upgrade, we realised that assigned memory displayed on Storm UI is incorrect. Attached screenshot for more details.

 

Looks like assigned memory is sum of memory provided in topology.worker.childopts and topology.worker.logwriter.childopts options.

 

Multiple scenarios we have observed where  assigned memory is not correct-
 # If topology.worker.childopts memory is more than 3 GB, in this case assigned memory is showing 65 MB.


 # If topology.worker.childopts memory+ topology.worker.logwriter.childopts memory is below 50 for last 2 digits.
For eg.,
topology.worker.childopts = 2048 MB
topology.worker.logwriter.childopts = 64 MB
Total = 2112 MB
Here, last 2 digits are below 50, in this case assigned memory is showing 65 MB."
STORM-3399,Add ability to set priority among partitions in KafkaSpout,"This idea was raised for the old storm-kafka spout at https://issues.apache.org/jira/browse/STORM-3160. As I'm closing that issue, I'm raising this one to implement that functionality in the new spout.

I believe doing this should be possible using KafkaConsumer.pause and KafkaConsumer.resume."
STORM-3395,Modifiy Topology page placeholder components to show task counts,placeholder current values set to 0. 
STORM-3394,Fix placeholder system bolt filtering for Topology page,"Addresses comments in [YSTORM-3992 pull request|[https://github.com/apache/storm/pull/3008]]

 "
STORM-3393,OffsetManager doesn't recover after missing offsets,"When missing offsets are encountered, but a committable offset exists after the missing offset, the condition is detected and logged but not properly processed.  You will see three log messages in this case:
{code:java}
Processed non-sequential offset.  The earliest uncommitted offset is no longer part of the topic.  Missing offset: [{}], Processed: [{}]
...
Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset
...
Topic-partition [{}] has no offsets ready to be committed{code}
However, this is not the proper handling.  While a committable offset has been found, the found flag is not set to true (resulting in the 3rd log message).

The fix is to add a found=true within this logic:

In OffsetManager.java
{code:java}
if (nextEmittedOffset != null && currOffset == nextEmittedOffset) {
                        LOG.debug(""Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset"",
                            currOffset, nextCommitOffset);
                        nextCommitOffset = currOffset + 1;
                        found = true;       //  ADD THIS LINE TO FIX THIS BUG
                    }{code}
Because of this bug, offsets are not committed properly."
STORM-3391,"MongoMapState causes ""IllegalArgumentException: Invalid BSON field name _id"" while multiPut operation","MongoMapState causes ""IllegalArgumentException: Invalid BSON field name _id"" while multiPut operation. 

mongoDB server: 3.6.7
mongo-java-driver: 3.8.2"
STORM-3389,"The behavior of ClientSupervisorUtils.launchProcess depends on having logPrefix or not, which is confusing","The behavior of ClientSupervisorUtils.launchProcess depends on having logPrefix or not, which is confusing.

If logPrefix is not null, it will read the output from the launched process. If logPrefix is null, it will not read output from the launched process. This is confusing. "
STORM-3387,Test failure in integration test,"{code}
[ERROR] testTumbleTime(org.apache.storm.st.tests.window.TumblingWindowTest)  Time elapsed: 36.751 s  <<< FAILURE!
com.google.gson.JsonSyntaxException: com.google.gson.stream.MalformedJsonException: Unterminated string at line 1 column 32 path $.now
	at com.google.gson.internal.Streams.parse(Streams.java:60)
	at com.google.gson.internal.bind.TreeTypeAdapter.read(TreeTypeAdapter.java:65)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:129)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:220)
	at com.google.gson.Gson.fromJson(Gson.java:887)
	at com.google.gson.Gson.fromJson(Gson.java:852)
	at com.google.gson.Gson.fromJson(Gson.java:801)
	at com.google.gson.Gson.fromJson(Gson.java:773)
	at org.apache.storm.st.topology.window.data.TimeData.fromJson(TimeData.java:58)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:373)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.storm.st.wrapper.TopoWrap.deserializeLogData(TopoWrap.java:303)
	at org.apache.storm.st.wrapper.TopoWrap.getDeserializedDecoratedLogLines(TopoWrap.java:295)
	at org.apache.storm.st.tests.window.WindowVerifier.runAndVerifyTime(WindowVerifier.java:100)
	at org.apache.storm.st.tests.window.TumblingWindowTest.testTumbleTime(TumblingWindowTest.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:84)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:714)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:901)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1231)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:127)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:111)
	at org.testng.TestRunner.privateRun(TestRunner.java:767)
	at org.testng.TestRunner.run(TestRunner.java:617)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:334)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:329)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:291)
	at org.testng.SuiteRunner.run(SuiteRunner.java:240)
	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86)
	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149)
	at org.testng.TestNG.run(TestNG.java:1057)
	at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:135)
	at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeMulti(TestNGDirectoryTestSuite.java:193)
	at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:94)
	at org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:146)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: com.google.gson.stream.MalformedJsonException: Unterminated string at line 1 column 32 path $.now
	at com.google.gson.stream.JsonReader.syntaxError(JsonReader.java:1559)
	at com.google.gson.stream.JsonReader.nextQuotedValue(JsonReader.java:1017)
	at com.google.gson.stream.JsonReader.nextString(JsonReader.java:815)
	at com.google.gson.internal.bind.TypeAdapters$29.read(TypeAdapters.java:718)
	at com.google.gson.internal.bind.TypeAdapters$29.read(TypeAdapters.java:714)
	at com.google.gson.internal.Streams.parse(Streams.java:48)
	... 50 more
{code}"
STORM-3380,"Change String.equals to Objects.equals(String,String) to avoid possible NullPointerException","Hello,
I found that if the Map ""data"" does not have the key ""TYPE"", the String ""compType"" may cause potential risk of NullPointerException since it is immediately used after initialization and there is no null checker. One recommended API is Objects.equals(String,String) which can avoid this exception.
"
STORM-3378,"Clean up integration test a bit, and switch to JUnit 5",
STORM-3369,Apache Storm 2.0.0 release artifacts are not avaible in maven central,"Hi All,

Couple of weeks ago, Apache storm 2.0.0 was released (based on github release branches). We have been eyeing the release for some time. With such great news, we wanted to start utilizing it.

However, the release artifacts have not been published yet into maven central: [https://search.maven.org/search?q=org.apache.storm]

We were wondering if it was a matter of time and the release process is not complete yet, or it was a missed step.

Thanks"
STORM-3368,"Apply for translation of the Chinese version, I hope to get authorization! ","Hello everyone, we are [ApacheCN|https://www.apachecn.org/], an open-source community in China, focusing on Big Data and AI.

Recently, we have been making progress on translating Storm documents.

 - [Source Of Document|https://github.com/apachecn/storm-doc-zh]
 - [Document Preview|http://storm.apachecn.org/]

There are several reasons:
 *1. The English level of many Chinese users is not very good.*
 *2. Network problems, you know (China's magic network)!*
 *3. Online blogs are very messy.*

We are very willing to do some Chinese localization for your project. If possible, please give us some authorization and create a link on your project homepage.

Yifan Yuan from Apache CN

You may contact me by mail [tsingjyujing@163.com|mailto:tsingjyujing@163.com] for more details"
STORM-3367,Upgrade Dropwizard Metrics to 5.0.0-rc2,"Dropwizard Metrics 5 introduces metrics tagging. 

One of the goals of metrics v2 was to move worker metrics to dropwizard metrics instead of sending the metrics via Zookeeper. A good way to do this would be to register the metrics with Dropwizard and create a NimbusMetricsReporter that sends metrics to Nimbus. Since we don't want to send all metrics, tags would be useful to allow the reporter to send only metrics relevant for Storm UI.

In relation to https://issues.apache.org/jira/browse/STORM-3204, it will probably also be an easier upgrade, since Metrics 5 has a new package/artifact name, so there won't be conflicts with dependencies."
STORM-3359,ConstraintSolverStrategy Deprecated Config breaks backward compatibility,"The config _topology.constraints_ is deprecated while _topology.ras.constraints_ is not available in prior versions, this breaks backwards compatibility."
STORM-3358,Upgrade Storm to Hadoop 3,"We should upgrade to Hadoop 3 at some point.

We are currently blocked by https://issues.apache.org/jira/browse/HBASE-22027, but I believe that is the only bit that prevents us from upgrading."
STORM-3346,ClassNotFoundException: clojure.lang.persistentList whiile submitting topology to local cluster in storm,"Getting below exceptions while submitting storm topology to local cluster 

Exception in thread ""main"" java.lang.ExceptionInInitializerError
 at clojure.lang.Namespace.<init>(Namespace.java:34)
 at clojure.lang.Namespace.findOrCreate(Namespace.java:176)
 at clojure.lang.Var.internPrivate(Var.java:156)
 at org.apache.storm.LocalCluster.<clinit>(Unknown Source)
 at KafkaCEPTopology.main(KafkaCEPTopology.java:53)
Caused by: Syntax error compiling . at (clojure/core.clj:20:8).
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7114)
 at clojure.lang.Compiler.analyze(Compiler.java:6789)
 at clojure.lang.Compiler.access$300(Compiler.java:38)
 at clojure.lang.Compiler$DefExpr$Parser.parse(Compiler.java:596)
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7106)
 at clojure.lang.Compiler.analyze(Compiler.java:6789)
 at clojure.lang.Compiler.analyze(Compiler.java:6745)
 at clojure.lang.Compiler.eval(Compiler.java:7180)
 at clojure.lang.Compiler.load(Compiler.java:7635)
 at clojure.lang.RT.loadResourceScript(RT.java:381)
 at clojure.lang.RT.loadResourceScript(RT.java:372)
 at clojure.lang.RT.load(RT.java:463)
 at clojure.lang.RT.load(RT.java:428)
 at clojure.lang.RT.doInit(RT.java:471)
 at clojure.lang.RT.<clinit>(RT.java:338)
 ... 5 more
Caused by: java.lang.ClassNotFoundException: clojure.lang.PersistentList
 at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
 at clojure.lang.DynamicClassLoader.findClass(DynamicClassLoader.java:69)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at clojure.lang.DynamicClassLoader.loadClass(DynamicClassLoader.java:77)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at java.lang.Class.forName0(Native Method)
 at java.lang.Class.forName(Class.java:348)
 at clojure.lang.RT.classForName(RT.java:2207)
 at clojure.lang.RT.classForNameNonLoading(RT.java:2220)
 at clojure.lang.Compiler$HostExpr.maybeClass(Compiler.java:1041)
 at clojure.lang.Compiler$HostExpr$Parser.parse(Compiler.java:982)
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7106)

 "
STORM-3345,AbstractNonblockingServer$FrameBuffer pool-14-thread-163 [ERROR] Unexpected throwable while invoking!,"Hi Team,

We are working on storm 1.2.3 and  we are getting continuous the below error message from nimbus (Leader). Could you please let me know what is the issue and how to fix this error.

Error message:

o.a.s.t.s.AbstractNonblockingServer$FrameBuffer pool-14-thread-133 [ERROR] Unexpected throwable while invoking!

java.lang.IllegalArgumentException: No matching clause:
 at org.apache.storm.stats$thriftify_comp_page_data.invoke(stats.clj:1338) ~[storm-core-1.2.3.jar:1.2.3]"
STORM-3341,DRPCSpout should check before connecting to DrpcServer,"Now DRPCSpout always try to connect all Drpc Servers in the method 'open'. If one Drpc server shut down, the topology will restart over and over again.

DRPCSpout should check before connecting to DrpcServer"
STORM-3340,Nicer path handling in storm-webapp,Some of the path handling in storm-webapp is a little haphazard and differs between methods that do similar things. We should make the handling more uniform.
STORM-3339,Port all the AtomicReference to ConcurrentHashMap for Nimbus,"Now for many concurrent access resource in Nimbus.java, we use AtomicReference to make them multi thread safe. The resources summarized below:

1. heartbeatsCache
2. schedulingStartTimeNs
3. idToSchedStatus
4. nodeIdToResources
5. idToWorkerResources
6. idToExecutors

The 1, 4, 5 and 6 may grows huge if we have hundreds of topologies on cluster, when we update AtomicReference, actually we passed in a Function and use compareAndSet to update the whole val to the new returned by the Function, in that case, we must do a reference copy and merge the changes, which seems not necessary.

I think the reason to use AtomicReference is a legacy from old Clojure code, we can replace them totally with ConcurrentHashMap which supported better performance."
STORM-3338,Your project apache/storm is using buggy third-party libraries [WARNING],"
Hi, there!

    We are a research team working on third-party library analysis. We have found that some widely-used third-party libraries in your project have major/critical bugs, which will degrade the quality of your project. We highly recommend you to update those libraries to new versions.

    We have attached the buggy third-party libraries and corresponding jira issue links below for you to have more detailed information.

	1. commons-io commons-io
	version: 2.6

	Jira issues:
	.gitattributes not correctly applied
	affectsVersions:2.6
	https://issues.apache.org/jira/projects/IO/issues/IO-516?filter=allopenissues
	FilenameUtils.normalize should verify hostname syntax in UNC path
	affectsVersions:2.6
	https://issues.apache.org/jira/projects/IO/issues/IO-559?filter=allopenissues
	Missing Javadoc in FilenameUtils causing Travis-CI build to fail
	affectsVersions:2.6
	https://issues.apache.org/jira/projects/IO/issues/IO-570?filter=allopenissues


	2. commons-codec commons-codec
	version: 1.11

	Jira issues:
	InputStream not closed
	affectsVersions:1.10,1.11
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-225?filter=allopenissues


	3. org.apache.logging.log4j log4j-core
	version: 2.11.1

	Jira issues:
	NameAbbreviator skips first fragments
	affectsVersions:2.11.0,2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2365?filter=allopenissues
	Predeployment of PersistenceUnit that using Log4j as session logger failed (#198)
	affectsVersions:2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2397?filter=allopenissues
	Exceptions are added to all columns when a JDBC Appender's ColumnMapping uses a Pattern
	affectsVersions:2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2413?filter=allopenissues
	NullPointerException when closing never used RollingRandomAccessFileAppender
	affectsVersions:2.10.0,2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2418?filter=allopenissues
	AbstractAppender.setHandler(null) should not set a null ErrorHandler
	affectsVersions:3.0.0,2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2441?filter=allopenissues
	ErrorHandler should be invoked with the failing LogEvent when possible
	affectsVersions:3.0.0,2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2444?filter=allopenissues
	RollingRandomAccessFileManager ignores new file patterns from programmatic reconfiguration
	affectsVersions:2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2457?filter=allopenissues
	ColumnMapping literal not working
	affectsVersions:2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2466?filter=allopenissues
	org.apache.log4j.SimpleLayout and ConsoleAppender missing in log4j-1.2-api
	affectsVersions:2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2476?filter=allopenissues
	BasicContextSelector cannot be used in a OSGI application
	affectsVersions:2.11.1
	https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2482?filter=allopenissues


	4. org.apache.httpcomponents httpclient
	version: 4.5.6

	Jira issues:
	Support relatively new HTTP 308 redirect - RFC7538
	affectsVersions:3.1 (end of life),4.5.6
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1946?filter=allopenissues


	5. org.apache.httpcomponents httpclient
	version: 4.5

	Jira issues:
	NTLM auth failed because NTLMEngineImpl strip domain to base domain name
	affectsVersions:4.5
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1662?filter=allopenissues
	RequestBuilder ignores Charset 
	affectsVersions:4.5
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1667?filter=allopenissues
	connectTimeout used as socketTimeout in Request
	affectsVersions:4.5
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1668?filter=allopenissues
	org.apache.http.entity.mime.content is missing from exports of OSGi bundle
	affectsVersions:4.5
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1673?filter=allopenissues
	307 redirect throws ClientProtocolException using POST method
	affectsVersions:4.5
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1680?filter=allopenissues
	ZipException occurs when content-encoding-header is set for 304-response 
	affectsVersions:4.5
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1690?filter=allopenissues
	OSGiRoutePlanner examines only the first proxy exception and also crashes processing IP address exception
	affectsVersions:4.4.1;4.5;5.0
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1710?filter=allopenissues
	org.apache.http.impl.client.AbstractHttpClient#createClientConnectionManager Does not account for context class loader
	affectsVersions:4.4.1;4.5;4.5.1;4.5.2
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1727?filter=allopenissues
	PoolingHttpClientConnectionManager has no option to close long leased connections
	affectsVersions:4.4.1;4.5
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1760?filter=allopenissues


	6. org.apache.httpcomponents httpclient
	version: 4.5.2

	Jira issues:
	org.apache.http.impl.client.AbstractHttpClient#createClientConnectionManager Does not account for context class loader
	affectsVersions:4.4.1;4.5;4.5.1;4.5.2
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1727?filter=allopenissues
	Memory Leak in OSGi support
	affectsVersions:4.4.1;4.5.2
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1749?filter=allopenissues
	SystemDefaultRoutePlanner: Possible null pointer dereference
	affectsVersions:4.5.2
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1766?filter=allopenissues
	Null pointer dereference in EofSensorInputStream and ResponseEntityProxy
	affectsVersions:4.5.2
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1767?filter=allopenissues
	[OSGi] WeakList needs to support ""clear"" method
	affectsVersions:4.5.2;5.0 Alpha1
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1772?filter=allopenissues
	[OSGi] HttpProxyConfigurationActivator does not unregister HttpClientBuilderFactory
	affectsVersions:4.5.2
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1773?filter=allopenissues
	Why is Retry around Redirect and not the other way round
	affectsVersions:4.5.2
	https://issues.apache.org/jira/projects/HTTPCLIENT/issues/HTTPCLIENT-1800?filter=allopenissues


	7. commons-cli commons-cli
	version: 1.2

	Jira issues:
	Unable to select a pure long option in a group
	affectsVersions:1.0;1.1;1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-182?filter=allopenissues
	Clear the selection from the groups before parsing
	affectsVersions:1.0;1.1;1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-183?filter=allopenissues
	Commons CLI incorrectly stripping leading and trailing quotes
	affectsVersions:1.1;1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-185?filter=allopenissues
	Coding error: OptionGroup.setSelected causes java.lang.NullPointerException
	affectsVersions:1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-191?filter=allopenissues
	StringIndexOutOfBoundsException in HelpFormatter.findWrapPos
	affectsVersions:1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-193?filter=allopenissues
	HelpFormatter strips leading whitespaces in the footer
	affectsVersions:1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-207?filter=allopenissues
	OptionBuilder only has static methods; yet many return an OptionBuilder instance
	affectsVersions:1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-224?filter=allopenissues
	Unable to properly require options
	affectsVersions:1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-230?filter=allopenissues
	OptionValidator Implementation Does Not Agree With JavaDoc
	affectsVersions:1.2
	https://issues.apache.org/jira/projects/CLI/issues/CLI-241?filter=allopenissues


	8. commons-collections commons-collections
	version: 3.2.1

	Jira issues:
	Inconsistent Javadoc comment and code in addIgnoreNull(Collection<T>; T) in org.apache.commons.collections.CollectionUtils
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-400?filter=allopenissues
	ListUtils.subtract is very slow 
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-406?filter=allopenissues
	ListOrderedSet.removeAll() is slow
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-407?filter=allopenissues
	ListOrderedSet.addAll() is very slow
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-409?filter=allopenissues
	Performance problem in DualHashBidiMap
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-413?filter=allopenissues
	AbstractLinkedList.removeAll() is very slow
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-415?filter=allopenissues
	AbstractLinkedList.retainAll() is very slow
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-417?filter=allopenissues
	Surprising exception by CompositeSet in a situation where CompositeCollection works fine
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-424?filter=allopenissues
	performance problem in ListOrderedMap.remove()
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-425?filter=allopenissues
	performance problem in ListOrderedSet.retainAll()
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-426?filter=allopenissues
	performance problem in SetUniqueList.retainAll()
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-427?filter=allopenissues
	SetUniqueList may become inconsistent
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-444?filter=allopenissues
	findBugs Warnings: several classes in package functors may expose their internal representation
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-453?filter=allopenissues
	findBugs Warning: Flat3Map - 3 iterators which are ""both an Iterator and a Map.Entry""
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-454?filter=allopenissues
	wasted work in AbstractMapBag.containsAll()
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-472?filter=allopenissues
	ListOrderedSet can have duplicates
	affectsVersions:3.2.1;4.0
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-524?filter=allopenissues
	ExtendedProperties causes AccessControlException when framework is called from a script
	affectsVersions:3.2.1
	https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-538?filter=allopenissues


	9. commons-io commons-io
	version: 1.4

	Jira issues:
	FileCleaningTrackerTestCase hangs
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-161?filter=allopenissues
	Fix case-insensitive string handling
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-167?filter=allopenissues
	Symbolic links (symlinks) followed when deleting directory.
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-168?filter=allopenissues
	StringIndexOutOfBounds exception on FilenameUtils.getPathNoEndSeparator
	affectsVersions:1.3.2;1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-179?filter=allopenissues
	FileSystemUtils.freeSpaceWindows blocks
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-185?filter=allopenissues
	FileSystemUtils.freeSpaceKb doesn't work with relative paths on Linux
	affectsVersions:1.2;1.3;1.3.1;1.3.2;1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-187?filter=allopenissues
	CountingInputStream/CountingOutputStream only partially synchronized
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-201?filter=allopenissues
	NotFileFilter documentation is incorrect
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-202?filter=allopenissues
	Manifest for OSGi has invalid syntax
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-204?filter=allopenissues
	FileSystemUtils.freeSpaceKb fails to return correct size for a windows mount point
	affectsVersions:1.4;2.0;3.x
	https://issues.apache.org/jira/projects/IO/issues/IO-209?filter=allopenissues
	Delete files quietly when an exception is thrown during initialization
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-216?filter=allopenissues
	FileUtils.copyDirectoryToDirectory makes infinite loops
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-217?filter=allopenissues
	FileCleaningTracker Vector performs badly under load
	affectsVersions:1.0;1.1;1.2;1.3;1.3.1;1.3.2;1.4;2.0;3.x
	https://issues.apache.org/jira/projects/IO/issues/IO-220?filter=allopenissues
	IOUtils.copy Javadoc inconsistency (return -1 vs. throw ArithmeticException)
	affectsVersions:1.3;1.3.1;1.3.2;1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-223?filter=allopenissues
	FileUtils generate wrong exception message in isFileNewer method
	affectsVersions:1.4
	https://issues.apache.org/jira/projects/IO/issues/IO-231?filter=allopenissues


	10. commons-io commons-io
	version: 2.5

	Jira issues:
	ant test fails - resources missing from test classpath
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/IO/issues/IO-451?filter=allopenissues
	Exceptions are suppressed incorrectly when copying files.
	affectsVersions:2.4;2.5
	https://issues.apache.org/jira/projects/IO/issues/IO-502?filter=allopenissues
	ThresholdingOutputStream.thresholdReached() results in FileNotFoundException
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/IO/issues/IO-512?filter=allopenissues
	Tailer.run race condition runaway logging
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/IO/issues/IO-528?filter=allopenissues
	Thread bug in FileAlterationMonitor#stop(int)
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/IO/issues/IO-535?filter=allopenissues
	2.5 ExceptionInInitializerError
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/IO/issues/IO-536?filter=allopenissues


	11. commons-codec commons-codec
	version: 1.3

	Jira issues:
	[codec] Using US_ENGLISH static in Soundex causes NPE
	affectsVersions:1.3
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-10?filter=allopenissues
	org.apache.commons.codec.net.URLCodec.ESCAPE_CHAR isn't final but should be
	affectsVersions:1.2;1.3;1.4
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-111?filter=allopenissues
	[codec] Base64.isArrayByteBase64() throws an ArrayIndexOutOfBoundsException for negative octets.
	affectsVersions:1.3
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-22?filter=allopenissues
	[codec] Source tarball spews files all over the place
	affectsVersions:1.3
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-6?filter=allopenissues
	Base64.encodeBase64() throws NegativeArraySizeException on large files
	affectsVersions:1.3
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-61?filter=allopenissues
	Fix case-insensitive string handling
	affectsVersions:1.3
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-65?filter=allopenissues
	Make string2byte conversions indepedent of platform default encoding
	affectsVersions:1.3
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-73?filter=allopenissues
	All links to fixed bugs in the ""Changes Report"" http://commons.apache.org/codec/changes-report.html point nowhere; e.g. http://issues.apache.org/jira/browse/34157. Looks as if all JIRA tickets were renumbered.
	affectsVersions:1.1;1.2;1.3;1.4
	https://issues.apache.org/jira/projects/CODEC/issues/CODEC-76?filter=allopenissues


	12. org.slf4j slf4j-api
	version: 1.7.21

	Jira issues:
	Cannot re-initialize the SimpleLogger anymore.
	affectsVersions:1.7.21
	https://jira.qos.ch/projects/SLF4J/issues/SLF4J-370?filter=allopenissues
	Marker lost in EventRecodingLogger
	affectsVersions:1.7.21
	https://jira.qos.ch/projects/SLF4J/issues/SLF4J-379?filter=allopenissues
	Support for JCL 1.2
	affectsVersions:1.7.21
	https://jira.qos.ch/projects/SLF4J/issues/SLF4J-383?filter=allopenissues


	13. commons-lang commons-lang
	version: 2.6

	Jira issues:
	Remove unnecessary synchronization from registry lookup in EqualsBuilder and HashCodeBuilder
	affectsVersions:2.6
	https://issues.apache.org/jira/projects/LANG/issues/LANG-1230?filter=allopenissues
	LocaleUtils - DCL idiom is not thread-safe
	affectsVersions:2.6
	https://issues.apache.org/jira/projects/LANG/issues/LANG-803?filter=allopenissues
	Exception when combining custom and choice format in ExtendedMessageFormat
	affectsVersions:2.5;2.6
	https://issues.apache.org/jira/projects/LANG/issues/LANG-917?filter=allopenissues


	14. org.apache.commons commons-lang3
	version: 3.3

	Jira issues:
	SerializationUtils.ClassLoaderAwareObjectInputStream should use static initializer to initialize primitiveTypes map.
	affectsVersions:3.2;3.3;3.4
	https://issues.apache.org/jira/projects/LANG/issues/LANG-1251?filter=allopenissues
	Failing tests with Java 8 b128
	affectsVersions:3.3
	https://issues.apache.org/jira/projects/LANG/issues/LANG-978?filter=allopenissues
	NumberUtils#createNumber() returns positive BigDecimal when negative Float is expected
	affectsVersions:3.x
	https://issues.apache.org/jira/projects/LANG/issues/LANG-1087?filter=allopenissues


	15. commons-lang commons-lang
	version: 2.5

	Jira issues:
	Testing with JDK 1.7
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/LANG/issues/LANG-593?filter=allopenissues
	Some StringUtils methods should take an int character instead of char to use String API features.
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/LANG/issues/LANG-608?filter=allopenissues
	SystemUtils.getJavaVersionAsFloat throws StringIndexOutOfBoundsException on Android runtime/Dalvik VM
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/LANG/issues/LANG-624?filter=allopenissues
	NumberUtils createNumber throws a StringIndexOutOfBoundsException when argument containing ""e"" and ""E"" is passed in
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/LANG/issues/LANG-638?filter=allopenissues
	FastDateFormat.format() outputs incorrect week of year because locale isn't respected
	affectsVersions:2.5
	https://issues.apache.org/jira/projects/LANG/issues/LANG-645?filter=allopenissues
	Exception when combining custom and choice format in ExtendedMessageFormat
	affectsVersions:2.5;2.6
	https://issues.apache.org/jira/projects/LANG/issues/LANG-917?filter=allopenissues




Sincerely~
FDU Software Engineering Lab
Feb 15th,2019"
STORM-3337,KafkaTridentSpoutOpaque can lose offset data in Zookeeper,"I see this issue happening once a twice a week for a number of topologies I have running in production that are reading from Kafka. I was able to reproduce it in a more pared down environment using 1 worker with a parallelism hint of at least 2 reading from a topic that had 16 partitions. The issue is reproducible using less partitions but it occurs less frequently.

What happens is that while committing offsets for the first transaction after a worker crash the partition offset data in Zookeeper is wiped for a subset of that worker's partitions. The state is restored after the next batch or two is committed and the worker continues as normal, however if the worker happens to crash a 2nd time before the data restores itself then it gets lost and those partitions reset (in my case to their earliest offsets since I used a reset strategy of UNCOMMITTED_EARLIEST) after the worker restarts again.

I've attached a log file showing what's happening. In this example ZK had offset data committed for all partitions for txid=29 before the worker crashed. After the worker came back up partitions 1, 3, 5, 7, 9, 11, 13, 15 lose their child nodes in ZK, the remaining partitions get child nodes created for txid=30. The important steps are:

1. Thread-7 and Thread-19 both hit 'Emitted Batch' for txid=30 here:

[https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L146]
 Thread-7 is assigned even numbered partitions 0-14, has _index=0, _changedMeta=true, coordinatorMeta=[] 
 Thread-19 is assigned odd numbered partitions 1-15, has _index=1, _changedMeta=true, coordinatorMeta=[]
 Even though `coordinatorMeta` is empty the `KafkaTridentSpoutEmitter` ignores this parameter in `getPartitionsForTask` and returns partition assignments for each thread:
 [https://github.com/apache/storm/blob/v1.2.3/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutEmitter.java#L253]

2. Both threads hit 'Committing transaction' here: 
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L160]

3. Thread-19 begins create state for txid=30 for its partitions:
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L186]

4. Thread-7 enters this special condition since it has `_index==0` and `_changedMeta==true`
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L169]

5. Thread-7 calls ` _emitter.getOrderedPartitions(_savedCoordinatorMeta)`, which for the KakfaTridentSpoutEmitter implementation returns an empty list since coordinatorMeta was empty for this batch:
 [https://github.com/apache/storm/blob/v1.2.3/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutEmitter.java#L241]

6. Thread-7 does a list for all the partition nodes for this component in ZK and since `validIds` is empty they all pass the check and `removeState` is called on each of them for txid=30:
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L177]
 This is the key part that causes the bug. Thread-7 hasn't started committing state for txid=30 for any of its assigned partitions so the calls to `removeState` on those partitions won't do anything. However Thread-19 is running concurrently so any partitions it has already written state for will get deleted here.

7. Thread-7 and Thread-19 enter the success handler and call `cleanupBefore(txid=29)`:
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L153]
 For the partitions owned by Thread-19 that got their state removed in #6 that means that both the nodes for 29 and 30 are deleted and the partitions' sates are empty in ZK.

Couple things that are contributing to this bug. I'm not sure if it's expected that `coordinatorMeta` be passed into `emitBatch` as an empty list for the first batch after the worker restart. If this contained all the partitions assigned across all tasks (which it does on subsequent batches) then `validIds` wouldn't trigger for partitions that were owned by other tasks and their state wouldn't accidentally get removed. This is exacerbated by the fact that the `KafkaTridentSpoutEmitter` returns valid partitions for `getPartitionsForTask` even when `getOrderedPartitions` returns empty which seems to break expectations.

One additional safe-guard that might be worth investigation is having `cleanupBefore` make sure it wasn't going to leave a node in ZK without any children before running."
STORM-3336,Create a spout for listening and emitting the change streams in MongoDB,"MongoDB provides real-time updates to documents at a collection or database level using Change Streams. From MongoDB documentation ([https://docs.mongodb.com/manual/changeStreams/)]
{quote}Change streams allow applications to access real-time data changes without the complexity and risk of tailing the oplog. Applications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them. Because change streams use the aggregation framework, applications can also filter for specific changes or transform the notifications at will.
{quote}
There can be use cases for external systems to process these change events in real time.

The idea is to create a spout, which can listen to change events and emit them as tuples. There should also be a mechanism to save the state of change streams for a spout's id using `resumeToken` provided by MongoDB to resume the topology from that point."
STORM-3334,Storm java processes using very high CPU with very low load,"We observer that Storm_Nimbus, storm_supervisor and NMStormTopology is consuming 100% CPU with all CPU cores. 

We found that Dispatcher Thread is taking quiet lot CPU. We observer that this issue was also reported earlier in Jira with title ""{color:#333333}Short disruptor queue wait time leads to high CPU usage when idle{color}"" 

https://issues.apache.org/jira/browse/STORM-503

This say the issue got fixed in the version 0.9.6 while in the version 1.0.0 seems the issue is still fixed.

Kindly help us to resolve this issue, or advice us with the stable version of Storm which has this issue resolved.

 

Regards,

Adarsh"
STORM-3331,Test for PR linking,This is a test ticket for INFRA-17791. Please do not close.
STORM-3329,allow HttpForwardingMetricsConsumer to be used generally by topologies,
STORM-3328,Allow overriding function name in BasicDRPCTopology,
STORM-3324,connection attempt 3 to Netty-client-compute06/192.168.28.110:6708 failed,connection attempt 3 to Netty-client-compute06/192.168.28.110:6708 failed
STORM-3323,Make storm.py work without wrapper scripts,"I think it would be good if we could get rid of the platform specific shell scripts wrapping storm.py.

I think we should be able to move all the functionality in the shell scripts into storm.py. The only change we'd likely have to make is to have a storm-env.py file people can modify in storm/conf, instead of having the storm-env.sh/storm-env.ps1 as we do now."
STORM-3322,Batch support for JdbcInsertBolt was not merged to master branch,"There was some work done for batching in JdbcInsertBolt https://issues.apache.org/jira/browse/STORM-1957?jql=project%20%3D%20STORM

Opening this PR so that this can be merged into latest release"
STORM-3316,supervisor cannot connect nimbus,"2019-01-18 02:40:51.369 o.a.s.m.n.Client client-boss-1 [ERROR] connection attempt 1460 to Netty-Client-/172.16.2.118:6700 failed: java.net.ConnectException: Connection refused: /172.16.2.118:6700
2019-01-18 02:40:51.369 o.a.s.u.StormBoundedExponentialBackoffRetry client-boss-1 [WARN] WILL SLEEP FOR 1000ms (MAX)"
STORM-3315,Upgrade to Kryo 4,"A user seems to be hitting https://github.com/EsotericSoftware/kryo/issues/462, which is fixed in Kryo 4.

We should upgrade. It seems like we can ensure compatibility with current Kryo by setting kryo.getFieldSerializerConfig().setOptimizedGenerics(true), going by the Kryo 4 release notes."
STORM-3314,Acker redesign,"*Context:* The ACKing mechanism has come focus as one of the next major bottlenecks to address. The strategy to timeout and replay tuples has issues discussed in STORM-2359

*Basic idea:* Every bolt will send an ACK msg to its upstream spout/bolt once the tuples it emitted have been *fully processed* by downstream bolts.

*Determining ""fully processed”* : For every incoming (parent) tuple, a bolt can emit 0 or more “child” tuples. the Parent tuple is considered fully processed once a bolt receives ACKs for all the child emits (if any). This basic idea cascades all the way back up to the spout that emitted the root of the tuple tree.

This means that, when a bolt is finished with all the child emits and it calls ack() no ACK message will be generated (unless there were 0 child emits). The ack() marks the completion of all child emits for a parent tuple. The bolt will emit an ACK to its upstream component once all the ACKs from downstream components have been received.

*Operational changes:* The existing spouts and bolts don’t need any change. The bolt executor will need to process incoming acks from downstream bolts and send an ACK to its upstream component as needed. In the case of 0 child emits, ack() itself could immediately send the ACK to the upstream component. Field grouping is not applied to ACK messages.

Total ACK messages: The spout output collector will no longer send an ACK-init message to the ACKer bolt. Other than this, the total number of emitted ACK messages does not change. Instead of the ACKs going to an ACKer bolt, they get spread out among the existing bolts. It appears that this mode may reduce some of the inter-worker traffic of ACK messages.

*Memory use:* If we use the existing XOR logic from ACKer bolt, we need about 20 bytes per outstanding tuple-tree at each bolt. Assuming an average of say 50k outstanding tuples at each level, we have 50k*20bytes = 1MB per bolt instance. There may be room to do something better than XOR, since we only need to track one level of outstanding emits at each bolt.

*Replay:* [needs more thinking] One option is to send REPLAY or TIMEOUT msgs upstream. Policy of when to emit them needs more thought. Good to avoid Timeouts/replays of inflight tuples under backpressure since this will lead to ""event tsunami"" at the worst possible time. Ideally, if possible, replay should be avoided unless tuples have been dropped. Would be nice to avoid sending TIMEOUT_RESET msgs upstream when under backpressure ... since they are likely to face backpressure as well.

On receiving an ACKs or REPLAYs from downstream components, a bolt needs to clears the corresponding 20 bytes tracking info.

 

*Concerns:* ACK tuples traversing upstream means it takes longer to get back to Spout.

 

 

*Related Note:* +Why ACKer is slow ?:+

Although lightweigh internally, the ACKer bolt has a huge impact on throughput. The latency hit does not appear to be as significant.

I have some thoughts around why ACKer slows down throughput. 

Consider the foll simple topo: 

{{Spout ==> Bolt1 ==> Bolt2}}

If we add an ACKer to the above topo, the Acker bolt receives 3x more incoming messages than the Bolt1 & Bolt2. Thats instantly a 3x hit on its throughput on ACKer. Additionally each spout and bolt now emits 2 msgs instead of 1 (if acker were absent). This slows down the spouts and bolts."
STORM-3313,Batch support for JdbcInsertBolt was not merged to master branch,"There was some work done for batching in JdbcInsertBolt https://issues.apache.org/jira/browse/STORM-1957?jql=project%20%3D%20STORM

But wondering why it hasn't been merged to master branch. 

I am using storm-jdbc 1.2.2 (latest) but it does not have batching. Please fix this"
STORM-3311,Use Java 7 Files API for IO instead of the older API,"We should try to use the NIO Files API for file IO. The older file API causes issues on Windows, since it doesn't set the FILE_SHARE_DELETE flag when opening files. This causes Storm to be unable to delete files that have open handles, which is unlike the behavior on Linux. This can cause e.g. unnecessary supervisor crashes because one thread tries to delete a file that is open in another.

For the same reason, we should get rid of uses of common-io FileUtils, and get rid of uses of Guava's Files class that opens IO streams."
STORM-3308,o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key,"Related to issue STORM-2736

We run a single instance of storm-nimbus (Non HA). Due to issues with connections issues with zookeeper, the ephemeral child nodes at  /storm.104/blobstore/key/  were lost that was created by the instance. Nimbus constantly logs the following line which was fixed in the issue mentioned above - 
{code:java}
o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<key>
{code}

But, Shouldn't nimbus automatically create the children with hostname:port-sequencenumber automatically if it is up and running ? In my case, nimbus did not crash but the ephemeral nodes( the children in the case) vanished  when the connection was reset between nimbus and zookeeper. I don't see any code path that creates the children if they are missing in the /blobstore zk path."
STORM-3306,Some tests in storm-core/test/jvm/org/apache/storm/integration/TopologyIntegrationTest.java are using Thrift to build topologies. They should use TopologyBuilder instead. ,
STORM-3305,Storm-hbase tests don't run on Java 11,
STORM-3304,Storm-hdfs tests don't run on Java 11,
STORM-3299,Thrift ShellComponent should support multiple arguments ,"As [this comment in storm.thrift from 7 years ago says|https://github.com/apache/storm/blob/master/storm-client/src/storm.thrift#L69], `ShellComponent` should support multiple arguments. Currently it supports only `execution_command` and `script`.

 

I am trying to make streamparse support self-contained JARs (where the virtualenv is packaged up inside the JAR so we don't need SSH access to the workers), but I am hitting a wall because of the combination of this issue and STORM-152. I can make an executable that contains the virtualenv using the Python shiv package, but when it gets packaged up in the JAR, it loses its executable bit, so I can't use it as `execution_command`. Therefore, I wanted to make a little shell script that runs first that fixes the executable bit, and then calls the shiv file with the Python module it needs to launch. That would require me to be able to pass the Python module to the wrapper script, but I cannot because I can only specify `bash` as `execution_command` and `my_wrapper_script.sh` as `script`. If I try to make `script` longer (like `my_wrapper_script.sh -arg1 -arg2`), `bash` complains that `my_wrapper_script.sh -arg1 -arg2` doesn't exist, because it's all sent as a single argument.

 

An alternative (but less desirable) fix that I could also work with would be if there were some environment variables set when the commands were launched that told you what component was being launched, and other basic context information. I could probably figure out the module to launch from there.


 

Still another alternative would be to add environment variables to the Thrift `ShellComponent` definition so we could pass arbitrary ones."
STORM-3298,Add Cassandra-Spout for bulk and streaming reads from Cassandra,"Cassandra is a very important data source and a frequent need is to update a secondary data-sink (like Kafka, Solr, Elastic-Search or an RDBMS) by pulling data from Cassandra.

Both kind of reads in [lambda-architecture|https://en.wikipedia.org/wiki/Lambda_architecture] are required to support Cassandra as a data-source in Storm.
 - Batch-processing
 - Stream-processing

For comparison with other stream-processing engines:
- Spark has [spark-cassandra-connector|https://github.com/datastax/spark-cassandra-connector]
- Nifi has [QueryCassandra|https://www.nifi.rocks/apache-nifi-processors/#QueryCassandra]
- Beam has [CassandraIO|https://beam.apache.org/releases/javadoc/2.2.0/org/apache/beam/sdk/io/cassandra/CassandraIO.html]

In the similar spirit, Storm should have a Cassandra Spout too which can do both batch and stream processing"
STORM-3293,Add resources requested to component summary page,"On component summary UI, it would be nice to see CPU and memory here."
STORM-3291,Worker can't run as the user who submitted the topology,"Without principal, worker can't be launched as the user who submitted the topology even we set ""supervisor.run.worker.as.user"" to true.Because the submitterUser will be overwrited by the user who launched nimbus."
STORM-3287,f,d
STORM-3286,MENIFEST.MF may loss by use of storm-rename-hack,"Function shadeJarStream in class DefaultShader may produce lost  of MENIFEST.MF,because of JarOutputStream created by the contructor without manifest.For some situation, such as mongodb connection, may lead a NullPointerException."
STORM-3285,A way to disable/enable nimbus rescheduling assignments,"As an Apache Storm administrator, I would like to be able to skip topology assignment updates without stopping the nimbus daemon, so that I can debug issues with the cluster in a more stable situation.

In scenarios where supervisor black-listing is active, topology workers can be reassigned away from supervisor hosts more frequently than desired for debugging.

While it is possible to disable changes to topology assignments by stopping nimbus, this has the effect of disable other services, such as the UI.

It would be nice to have some way to ""deactivate"" the assignment changes while letting everything else—including the scheduling logic—to run as normal. This could be done with a reload-able config, a special ""kill file"" whose presence tells nimbus to skip, or some control instruction via the API (e.g., {{storm admin suspend-assignments <TIMEOUT-SECONDS>}})."
STORM-3283,Can not connect to kerberized hdfs with HdfsSpout,"In HdfsSpout hdfs FileSystem object is [first created|https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L407] and then [additional settings are read|https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L418].

As a result config parameters like:
{code:java}
conf.set(""hadoop.security.authentication"", ""kerberos"");
{code}
can not be set and HdfsSpout fails with the following exception:
{code:java}
org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN, KERBEROS]
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_171]
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_171]
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_171]
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_171]
 at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1656) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1583) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1580) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1595) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.storm.hdfs.spout.HdfsSpout.validateOrMakeDir(HdfsSpout.java:518) [stormjar.jar:?]
 at org.apache.storm.hdfs.spout.HdfsSpout.open(HdfsSpout.java:435) [stormjar.jar:?]
 at org.apache.storm.daemon.executor$fn__9593$fn__9608.invoke(executor.clj:615) [storm-core-1.2.1.3.3.0.0-157.jar:1.2.1.3.3.0.0-157]
 at org.apache.storm.util$async_loop$fn__555.invoke(util.clj:482) [storm-core-1.2.1.3.3.0.0-157.jar:1.2.1.3.3.0.0-157]
 at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
Caused by: org.apache.hadoop.ipc.RemoteException: SIMPLE authentication is not enabled. Available:[TOKEN, KERBEROS]
 at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.Client.call(Client.java:1443) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.Client.call(Client.java:1353) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at com.sun.proxy.$Proxy47.getFileInfo(Unknown Source) ~[?:?]
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_171]
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_171]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at com.sun.proxy.$Proxy48.getFileInfo(Unknown Source) ~[?:?]
 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
{code}"
STORM-3281,ZK ACL checks breaks nimbus HA with LocalFsBlobStore,"A new check [1] is introduced to check the ZK ACLs before nimbus starts up. However this can fail with Nimbus HA enabled with LocalFsBlobStore causing nimbus to not start up.

1. Set up a cluster with 3 Nimbus (blobstore replication factor = 2). It might be reproducible with two nimbus but I havent checked.
2. Deploy topology
3. Once the topology is activated and running, Kill the leader nimbus.
4. The nimbus becomes offline and one of the other two is elected as a leader
5. Now try to bring up the killed nimbus.

It will fail with following exception


{noformat}
2018-11-06 10:22:05.195 o.a.s.t.t.TSaslTransport main [DEBUG] CLIENT: reading data length: 37
2018-11-06 10:22:05.204 o.a.s.b.BlobStoreUtils main [DEBUG] Updating state inside zookeeper for an update
2018-11-06 10:22:05.209 o.a.s.u.StormBoundedExponentialBackoffRetry main [DEBUG] The baseSleepTimeMs [2000] the maxSleepTimeMs [60000] the maxRetries [5]
2018-11-06 10:22:05.224 o.a.s.m.n.Login main [INFO] successfully logged in.
2018-11-06 10:22:05.224 o.a.s.s.a.k.KerberosSaslTransportPlugin main [DEBUG] SASL GSSAPI client transport is being established
2018-11-06 10:22:05.238 o.a.s.s.a.k.KerberosSaslTransportPlugin main [DEBUG] do as:storm_componentsrandYPTkmQBYmmTRvE@EXAMPLE.COM
2018-11-06 10:22:05.240 o.a.s.t.t.TSaslTransport main [DEBUG] opening transport org.apache.storm.thrift.transport.TSaslClientTransport@75dc1c1c
2018-11-06 10:22:05.241 o.a.s.s.a.k.KerberosSaslTransportPlugin main [ERROR] Client failed to open SaslClientTransport to interact with a server during session initiation: org.apache.storm.thrift.transport.TT
ransportException: java.net.ConnectException: Connection refused (Connection refused)
org.apache.storm.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
        at org.apache.storm.thrift.transport.TSocket.open(TSocket.java:226) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.thrift.transport.TSaslTransport.open(TSaslTransport.java:266) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:145) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:141) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_151]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_151]
        at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:140) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:53) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:104) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:73) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:131) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStoreUtils.createStateInZookeeper(BlobStoreUtils.java:233) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:269) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.LocalFsBlobStore.checkForBlobUpdate(LocalFsBlobStore.java:344) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:274) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:271) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:300) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.zookeeper.AclEnforcement.verifyAcls(AclEnforcement.java:111) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2588) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2612) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at clojure.lang.AFn.applyToHelper(AFn.java:152) ~[clojure-1.7.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus.main(Unknown Source) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_151]
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_151]
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_151]
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_151]
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_151]
        at java.net.Socket.connect(Socket.java:589) ~[?:1.8.0_151]
{noformat}


The ACL check tries to download the topology conf from blobstore [2] and in that process tries to update the nimbus state in ZK via itself [3] causing the connection failure.

To fix it I think we should wait until nimbus comes up before doing ACL checks.

cc [~revans2] to suggest the right fix since you may have more context.

[1] https://github.com/apache/storm/blob/1.x-branch/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L2588
[2] https://github.com/apache/storm/blob/1.x-branch/storm-core/src/jvm/org/apache/storm/zookeeper/AclEnforcement.java#L111
[3] https://github.com/apache/storm/blob/1.x-branch/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L269"
STORM-3280,Trident-based windowing does not appear to guarantee at-least-once,"[~shaikasifullah] mentioned that he was experiencing lost tuples when restarting a Trident topology that uses windowing alongside the opaque Kafka spout.

I think this is due to a bug in the Trident windowing implementation.

Trident doesn't use the regular acking mechanism to keep track of all tuples in a batch. Instead, the bolt executors in Trident send ""coordinator"" tuples downstream following each batch, indicating how many tuples were in the batch. These coordinator tuples are anchored to the initial ""emit batch"" tuple at the master batch coordinator (MBC). The next bolt executor in line checks if it received all the expected tuples, and fails the ""emit batch"" tree if not. Otherwise, the entire batch is considered acked when the coordinator tuple is acked, which happens as soon as it is received (purposefully ignoring the commit mechanism here).

The bolt executor notifies the wrapped bolt when a batch starts, and when it finishes. The expectation is that the bolt will emit any new tuples it wants anchored to the coordinator tuple before the bolt executor considers the batch finished. See https://github.com/apache/storm/blob/19fbfb9ac8f82719cf70fedf6a024acaeec4e804/storm-client/src/jvm/org/apache/storm/trident/topology/TridentBoltExecutor.java#L127.

The windowing mechanism in Trident is implemented via a processor https://github.com/apache/storm/blob/19fbfb9ac8f82719cf70fedf6a024acaeec4e804/storm-client/src/jvm/org/apache/storm/trident/windowing/WindowTridentProcessor.java#L147. The processor collects received tuples grouped by batch, and only passes them to the WindowManager when a batch is considered complete. At this point, it will also check if any triggers have fired (e.g. due to timeout), and will emit any resulting windows.

The issue here is that there is no correlation between the finished batch and which tuples the window processor chooses to emit during the finishBatch call. Unless it emits exactly the tuples from the received batch, there is a risk of losing the at-least-once property, since the bolt executor will ack the coordinator tuple immediately following finishBatch.

Just to give a concrete example:

MBC starts txid 1 by emitting an ""emit batch"" tuple
Spout executor receives the tuple, emits tuple 1-10, then emits coordinator tuple containing expected count of 10 tuples.
Bolt executor receives tuple 1-10
Bolt executor receives coordinator tuple from upstream spout, containing an expected count of 10 tuples
Bolt executor calls finishBatch
Window processor is configured with a window of 10 seconds, and decides not to emit the 10 tuples. Since nothing is emitted, no new tuples are anchored at the coordinator tuple.
Bolt executor acks the coordinator tuple at the MBC
The MBC sees that the ""emit batch"" tuple has been acked, and starts the commit process. At this point Trident is free to assume the 10 tuples have been correctly processed and e.g. write to Zookeeper that the Kafka spout should pick up at offset 10 next time it starts."
STORM-3278,Permissions issue on logviewer download for heap dump,"On secure storm cluster, we are not able to download heap dump file from UI."
STORM-3266,"prevent flooding supervisor log with ""Successfully authenticated"""," flooding of messages:
{code:java}
2018-10-19 22:34:21.317 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-9 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:21.445 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-10 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:22.050 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-11 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:22.327 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-12 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:22.452 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-13 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:23.059 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-14 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:23.334 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-15 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:23.461 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-16 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:24.065 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-1 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:24.342 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-2 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
2018-10-19 22:34:24.468 o.a.s.s.a.s.SimpleSaslServerCallbackHandler pool-12-thread-3 [INFO] Successfully authenticated client: authenticationID = ethan@ETHAN.STORM.NET authorizationID = ethan@ETHAN.STORM.NET
{code}

I submitted wordcount topology on a secure storm cluster and it started to run. And that's when it happens."
STORM-3265,flight.bash fall back to use java utils directly if $JAVA_HOME not set and it's not in $BINPATH,"Got


{code:java}
/tmp/verify-release/apache-storm-2.0.0/bin/flight.bash: line 66: /usr/bin/jstack: No such file or directory
{code}

We should fall back to use jstack directly if this happens. 

Note: profiling itself works (manually removed $BINPATH and retry). "
STORM-3264,"""local variable 'ret' referenced before assignment"" in storm.py","In https://github.com/apache/storm/blob/master/bin/storm.py#L142
{code:java}
# If given path is a dir, make it a wildcard so the JVM will include all JARs in the directory.
def get_wildcard_dir(path):
    if os.path.isdir(path):
        ret = [(os.path.join(path, ""*""))]
    elif os.path.exists(path):
        ret = [path]
    return ret
{code}

It's possible to get

{code:java}
  File ""/tmp/verify-release/apache-storm-2.0.0/bin/storm.py"", line 147, in get_wildcard_dir
    return ret
UnboundLocalError: local variable 'ret' referenced before assignment
{code}

because there is no ""else"" branch."
STORM-3259,NUMA support for Storm,"* Supervisors with a config will present to Nimbus as multiple supervisors (one per NUMA zone and one left over if ports/resources are left)
 * Workers scheduled on a particular NUMA supervisor will be launched by the actual supervisor pinned to that NUMA zone"
STORM-3257,'storm kill' command line should be able to continue on error,"If you invoke 'storm kill' as follows:

 
{code:java}
storm kill topo1 topo2 topo3 topo4 ...{code}
 

and the first action fails (e.g NotAliveException or AuthorizationException), none of the other items in the list are killed.

 

It would be helpful if (perhaps as an option) we could continue to attempt to kill everything else as well."
STORM-3256,"If all thread counts exceed 32767, the system will generate errors","If all thread counts exceed 32767, the system will generate errors, MessageBatch.java Line141.

 

Storm systems are used for low latency and a linear increase in the number of concurrencies as servers increase.
If you set up four workers per server, it's easy to achieve 4000 degrees of parallelism, that is, 4000 threads per server, then 10 servers will exceed 32767.
Supporting only 32767 threads is a disaster for large-scale computing.
It is hoped that this function can be improved and repaired as soon as possible."
STORM-3231,TopologyBySubmissionTimeComparator does not consider priority,"TopologyBySubmissionTimeComparator indicates ""Comparator that sorts topologies by priority and then by submission time"", but the code only considers uptime.

 

I am not sure what the intent should be.  Either the code or comment should be fixed."
STORM-3214,使用 kafka.topic.wildcard.match =true的时候，ZkCoordinator.refresh中deletedManagers会出现逻辑错误,"使用 kafka.topic.wildcard.match =true的时候，如果topic数目大于1，ZkCoordinator.refresh中deletedManagers会出现逻辑错误

只需要将ZkCoordinator@L91：    Map<Integer, PartitionManager> deletedManagers = new HashMap<>();

将key修改为topic+partition

 

在org.apache.storm.kafka.ZkCoordinatorTest中添加了如下测试
{code:java}
//代码占位符
public static GlobalPartitionInformation buildPartitionInfo(int numPartitions, int brokerPort, String topic) {
GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation(topic);
for (int i = 0; i < numPartitions; i++) {
globalPartitionInformation.addPartition(i, Broker.fromString(""broker-"" + i + "" :"" + brokerPort));
}
return globalPartitionInformation;
}

@Test
public void testTwoTopicPartitionsChange() throws Exception {
int numPartitions = 2;
int partitionsPerTask = 1;
final Set<Partition> unregisterList = new HashSet<>();
Mockito.doAnswer(new Answer() {
@Override
public Object answer(InvocationOnMock invocation) throws Throwable {
Object[] arguments = invocation.getArguments();
Partition partition = new Partition((Broker) arguments[0], (String) arguments[1], (int) arguments[2], false);
unregisterList.add(partition);
return null;
}
}).when(dynamicPartitionConnections).unregister(any(Broker.class), any(String.class), anyInt());

List<ZkCoordinator> coordinatorList = buildCoordinators(partitionsPerTask);
ArrayList<GlobalPartitionInformation> prePartitionInformations = Lists.newArrayList(buildPartitionInfo(numPartitions, 9092, ""TOPIC1""), 
buildPartitionInfo(numPartitions, 9092, ""TOPIC2""));
when(reader.getBrokerInfo()).thenReturn(prePartitionInformations);
List<List<PartitionManager>> partitionManagersBeforeRefresh = getPartitionManagers(coordinatorList);
waitForRefresh();
when(reader.getBrokerInfo()).thenReturn(Lists.newArrayList(buildPartitionInfo(numPartitions, 9093, ""TOPIC1""), buildPartitionInfo(numPartitions, 9093, ""TOPIC2"")));
List<List<PartitionManager>> partitionManagersAfterRefresh = getPartitionManagers(coordinatorList);
List<Partition> allPrePartition = KafkaUtils.calculatePartitionsForTask(prePartitionInformations, 1, 0, 0);
assertEquals(unregisterList.size(), allPrePartition.size());
for (Partition partition : allPrePartition) {
assertTrue(unregisterList.contains(partition));
}
}
{code}"
STORM-3212,Trident Kafka Spout throws NPE if Translator Returns Null,"The Javadoc for the RecordTranslator#apply(ConsumerRecord) method says to return null to discard a ConsumerRecord. But doing that when using a Trident Kafka Spout causes the spout to throw a NullPointerException.

 "
STORM-3211,WindowedBoltExecutor NPE if wrapped bolt returns null from getComponentConfiguration,"{code}
Exception in thread ""main"" java.lang.NullPointerException
    at org.apache.storm.topology.WindowedBoltExecutor.declareOutputFields(WindowedBoltExecutor.java:309)
    at org.apache.storm.topology.TopologyBuilder.getComponentCommon(TopologyBuilder.java:432)
    at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:120)
    at Main.main(Main.java:23)
{code}"
STORM-3209,org.apache.storm.jdbc.common.Util,"Util.getJavaType(int sqlType) not support NUMERIC,

when my sqlType is NUMERIC( 2 also),it throw ex;

 

 "
STORM-3204,Upgrade Dropwizard Metrics to 4.0.3,"Since we've removed metrics-ganglia, we can upgrade to the latest Metrics version. It has some fixes for Java 9+ https://github.com/dropwizard/metrics/pull/1236."
STORM-3202,Include offset information to spout metrics and remove storm-kafka-monitor,"To provide offset information on Kafka spout (old and new), we have storm-kafka-monitor module which is being run by UI (shell). This approach requires UI doing too many things - basically UI process does most of things via interacting with Nimbus - and also running external shell process in UI process per opening topology page doesn't look right.

We could just let Spout include offset information into spout metric, and let UI leverage the information. I have been thinking about this approach but forgot about addressing it while thinking about generalizing the format. Now I think we don't have to put too much effort to generalize format, because Kafka spout is used mainly.

 "
STORM-3195,IllegalArgumentException when rolling upgrade nimbus from 1.x to 2.x,"
{code:java}
2018-08-15 15:49:06.550 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.IllegalArgumentException: Don't know how to convert 1.7604166666666667 to int
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2837) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: java.lang.IllegalArgumentException: Don't know how to convert 1.7604166666666667 to int
        at org.apache.storm.utils.ObjectReader.getInt(ObjectReader.java:78) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.StormCommon.addAcker(StormCommon.java:256) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.StormCommon.systemTopologyImpl(StormCommon.java:523) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.StormCommon.systemTopology(StormCommon.java:440) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.StormCommon.stormTaskInfoImpl(StormCommon.java:544) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.StormCommon.stormTaskInfo(StormCommon.java:460) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeExecutors(Nimbus.java:1732) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getOrUpdateExecutors(Nimbus.java:1357) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeExecutorToComponent(Nimbus.java:1752) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.readTopologyDetails(Nimbus.java:1582) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getResourcesForTopology(Nimbus.java:2034) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl(Nimbus.java:2676) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.sendClusterMetricsToExecutors(Nimbus.java:2701) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2834) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
{code}

We were rolling upgrading our storm cluster to 2.x and we found this error from nimbus.log. It's not necessary to do rolling upgrade but good news is we basically made it work. 

"
STORM-3193,Improve LogviewerLogSearchHandler,"One thing that is worthy of noticing: Storm UI currently interweaves different search API regarding searching functionalities and it's kind of confusing.

Specifically:

For the search button at homepage, it uses a single deep search API to search all ports (server side process), both archived and non-archived.
For non-archived search at a specific topology page, it invokes search API on each port inside a loop (client side process).
For archived search at a specific topology page, it invokes deep search API (search-archived=on) on each port inside a loop (client side process)
As a result, metrics for these APIs may not accurately reflect how many searches are invoked from client's perspective.

Additionally, `findNMatches` can be simplified, along with STORM-3189"
STORM-3192,ClusterSummaryMetrics is activating/deactivating by polling for whether Nimbus is leader. It should be notified instead.,See https://github.com/apache/storm/pull/2764#discussion_r209333494.
STORM-3191,Migrate more items from ClusterSummary to metrics,"The following summary items haven't been ported as nimbus metrics yet.

//Declared in StormConf. I don't see the value in reporting so.
SUPERVISOR_TOTAL_RESOURCE,

//May be able to aggregate based on status;
TOPOLOGY_STATUS,
TOPOLOGY_SCHED_STATUS,

//May be aggregated: e.g., distinct values
NUM_DISTINCT_NIMBUS_VERSION;
"
STORM-3190,Unnecessary null check of directory stream in LogCleaner,This should be using try-with-resources https://github.com/apache/storm/blob/a1b3e02aab57b4e458b8b5763a0d467852906bb7/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogCleaner.java#L263
STORM-3189,Remove unused data file LogViewer api,"Discovered in STORM-3133.

`findNMatches` in LogviewerLogSearchHandler returns a `Matched` object which contains a field `fileOffset`. However, in current implementation, `fileOffset` behaves a bit odd and is not being used anywhere in the app. I'm wondering if we should remove this field altogether

Specifically, the difference in behavior follows,
`fileOffset is passed in as the desired amount of file to skip in search (equiv to index of first file to search)

if desired amount of matches found, fileOffset will be the index of last scanned file (starting from 0).
if not enough matches found in all logs, fileOffset will be number of all logs (equiv to one past the index of last file)

See 
https://github.com/apache/storm/pull/2754#discussion_r208691016
https://github.com/apache/storm/pull/2754#discussion_r208726809"
STORM-3188,Removing try-catch block from getAndResetWorkerHeartbeats,"After refactoring, SupervisorUtils.readWorkerHeartbeats no longer throws checked Exceptions. I'm wondering if we still want to keep the try-catch block to wrap around its invocation in getAndResetWorkerHeartbeats in ReportWorkerHeartbeats.java."
STORM-3187,Nimbus code refactoring and cleanup,"Nimbus.java is bloated with many legacy code that are convoluted and inefficient. It would be nice if we can clean up the code a bit, especially now that we're moving away from Clojure.

Several suggestion are made in STORM-3133, including,

1. Remove logging that is of the same purpose of some metrics: https://github.com/apache/storm/pull/2764#discussion_r203727117

2. Refactor data type of return values/parameters to improve readability: https://github.com/apache/storm/pull/2764#discussion_r208699933
https://github.com/apache/storm/pull/2764#discussion_r208721202
https://github.com/apache/storm/pull/2764#discussion_r208707855

3. Other performance improvement
https://github.com/apache/storm/pull/2764#discussion_r208714561"
STORM-3186,Customizable configuration for metric reporting interval,"In current implementation, all subclass of ScheduledReporter are hard coded report interval of 10 seconds. However I think it would make sense to make this an item in configuration so user can change the reporting frequency to fit their needs.

See discussion https://github.com/apache/storm/pull/2764#discussion_r203726617"
STORM-3185, patternMetrics undefined in log4j2/cluster.xml,"https://github.com/apache/storm/blob/master/log4j2/cluster.xml#L61


{code:java}
    <RollingFile name=""METRICS""
                 fileName=""${sys:storm.log.dir}/${sys:logfile.name}.metrics""
                 filePattern=""${sys:storm.log.dir}/${sys:logfile.name}.metrics.%i.gz"">
        <PatternLayout>
            <pattern>${patternMetrics}</pattern>
        </PatternLayout>
        <Policies>
            <SizeBasedTriggeringPolicy size=""2 MB""/>
        </Policies>
        <DefaultRolloverStrategy max=""9""/>
    </RollingFile>
{code}

patternMetrics is undefined. "
STORM-3178,Decouple `ClientSupervisorUtils` and refactor metrics registration,See conversation https://github.com/apache/storm/pull/2710#discussion_r207576736
STORM-3177,MockRemovableFile returns true on `#exists` even after `#delete` is called.,See conversation in https://github.com/apache/storm/pull/2788#pullrequestreview-142918985
STORM-3176,KafkaSpout commit offset occurs CommitFailedException which leads to worker dead,"KafkaSpout use the commitAsync api of Consumer, if the interval time between the call of consumer.poll() more than _max.poll.interval.ms_ or the heartbeat of consumer timeout, that will occur CommitFailedException,  and then the worker will die, the log like this: 
{code:java}
// 2018-07-31 19:19:03.341 o.a.s.util [ERROR] Async loop died!
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer th
an the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in
poll() with max.poll.records.
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:698) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:577) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1126) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:XXX) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:430) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:264) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$fn__10936$fn__10951$fn__10982.invoke(executor.clj:647) ~[XXX.jar:?]
at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [XXX.jar:?]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2018-07-31 19:19:03.342 o.a.s.d.executor [ERROR]
{code}
I find it will catch the Exception in auto-commit mode of consumer, the source code is:
{code:java}
// private void maybeAutoCommitOffsetsSync(long timeoutMs) {
    if (autoCommitEnabled) {
        Map<TopicPartition, OffsetAndMetadata> allConsumedOffsets = subscriptions.allConsumed();
        try {
            log.debug(""Sending synchronous auto-commit of offsets {} for group {}"", allConsumedOffsets, groupId);
            if (!commitOffsetsSync(allConsumedOffsets, timeoutMs))
                log.debug(""Auto-commit of offsets {} for group {} timed out before completion"",
                        allConsumedOffsets, groupId);
        } catch (WakeupException | InterruptException e) {
            log.debug(""Auto-commit of offsets {} for group {} was interrupted before completion"",
                    allConsumedOffsets, groupId);
            // rethrow wakeups since they are triggered by the user
            throw e;
        } catch (Exception e) {
            // consistent with async auto-commit failures, we do not propagate the exception
            log.warn(""Auto-commit of offsets {} failed for group {}: {}"", allConsumedOffsets, groupId,
                    e.getMessage());
        }
    }
}
{code}
I think KafkaSpout should do like this, catch the Exception avoid to worker die. And when the msg ack failed, Spout should judge the offset of the msgID is larger than the last commit offset(Spout can guarantee that these msgs which offset less than the last commit offset are all ack), if not, the msg should not retry.

 "
STORM-3174,Standardize exit codes,"Exit codes are hard-coded.  It would be better to centralize and tie them to a meaningful constant.

 

 "
STORM-3173,flush metrics to ScheduledReporter on shutdown,"We lose shutdown related metrics that we should alert on at shutdown. We should flush metrics on a shutdown.

https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L4497"
STORM-3171,java.lang.NoSuchMethodError in org.apache.storm:storm-kafka-monitor:jar:1.1.2 caused by dependency conflict issue,"Hi, we found a dependency conflict issue in *org.apache.storm:storm-kafka-monitor:jar:1.1.2*, *caused by org.apache.zookeeper:zookeeper:jar*. As shown in the following dependency tree, due to Maven version management, *org.apache.zookeeper:zookeeper:jar:3.4.6* will be loaded, during the packaging process.

 

However, method *<org.apache.zookeeper.server.quorum.flexible.QuorumMaj: void <init>(java.util.Map)>* only defined in *org.apache.zookeeper:zookeeper:jar 3.5.3-beta*, so that there is a crash with the following stack trace when your project referencing the missing method.

 

*Stack trace:*

Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.zookeeper.server.quorum.flexible.QuorumMaj.<init>(Ljava/util/Map;)V

         at org.apache.curator.framework.imps.EnsembleTracker.<init>(EnsembleTracker.java:57)

         at org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:159)

         at org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:158)

         at org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:109)

 

*Dependency tree:*

org.apache.storm:storm-kafka-monitor:jar:1.1.2

+- org.apache.kafka:kafka-clients:jar:0.10.1.0:compile
|  +- net.jpountz.lz4:lz4:jar:1.3.0:compile|
|  +- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile|
|  - org.slf4j:slf4j-api:jar:1.7.21:compile|

+- org.apache.curator:curator-framework:jar:4.0.0:compile
|  - org.apache.curator:curator-client:jar:4.0.0:compile|
|     +- *org.apache.zookeeper:zookeeper:jar:3.4.6:compile (version managed from 3.5.3-beta)*|
|     +- jline:jline:jar:0.9.94:compile| |
|      - io.netty:netty:jar:3.9.9.Final:compile (version managed from 3.7.0.Final)| |
|     +- com.google.guava:guava:jar:16.0.1:compile (version managed from 20.0)|
|     - (org.slf4j:slf4j-api:jar:1.7.21:compile - version managed from 1.7.6; omitted for duplicate)|

+- com.googlecode.json-simple:json-simple:jar:1.1:compile

+- commons-cli:commons-cli:jar:1.3.1:compile
 - junit:junit:jar:4.11:test

   - org.hamcrest:hamcrest-core:jar:1.3:test

 

*Solution:*

One choice is to upgrade *org.apache.zookeeper:zookeeper:jar to 3.5.3-beta,* but it is not the best solution, as 3.5.3-beta is not a release version.**

 

Thanks a lot!

Regards,

Leo"
STORM-3165,Better Unified Metrics API with Dimensions,"The current metrics system is really painful.  We have multiple different metrics APIs for both daemon and worker metrics.  We don't support dimensions consistently because we hacked them on top of only one of the metrics APIs in a way that is not extensible compatible.

 

We need a real final solution.  Internally at Oath we have a new metrics client API that fulfills a lot of these, and I am working with the author to open source it, but at the same time I don't think everyone wants to use this API, some will want to use dropwizard or some other reporter so I am going to put up a thin API that will allow us to have multiple different back ends, probably similar to how bookkeeper does it."
STORM-3163,ShellLogHandler loses thread context between setup and use,"*UPDATE*

Turns out that this is much better solved by leveraging {{log4j2}}'s {{isThreadContextMapInheritable}} property, which hands child threads a point-in-time copy of the parent's {{MDC}} contents, which completely solves the issue this sought to address.

---

*ORIGINAL*

tl;dr: {{ShellLogHandler}} is handed context in one thread, before being used exclusively from another, this obstructs sane usage of {{slf4j}}'s {{MDC}} feature which is thread local.

---

{{ShellBolt}} instantiates the {{ShellLogHandler}} and calls its {{setUpContext}} as part of {{prepare}}, immediately before it spawns its {{BoltReaderRunnable}} and {{BoltWriterRunnable}} threads which are responsible for communication with the {{ShellProcess}} that's already been spawned.

The {{ShellLogHandler}} is used exclusively from {{BoltReaderRunnable}}. The upshot of this is that {{setUpContext}} is executed in the task thread (i.e. {{Thread-21-joiner-executor[2 2]}}) while the {{log}} method is executed in the anonymous thread (i.e. {{Thread-30}}) running the {{BoltReaderRunnable}}.

This creates a problem when trying to leverage {{slf4j}}'s {{MDC}} (or {{NDC}}) which are used for augmenting log messages with additional information which is localised and persisted at the thread level.

The current work around for this is to store all relevant context on the {{ShellLogHandler}} during {{setUpContext}}, and then write it into the {{MDC}} during the {{log}} call, and taking precautions around that state being thread safe. The thread safety requirement is the first drawback, the second is that each {{log}} call has the additional overhead of either unconditionally writing to the {{MDC}} or checking that the {{MDC}} is already populated. Neither is very appealing.

The suggested solution is to pass {{stormConf}}, {{_process}}, and {{_context}} into the constructor of {{BoltReaderRunnable}} (and perhaps also {{BoltWriterRunnable}}) and instantiate {{ShellLogHandler}} and call its {{setUpContext}} from that thread.
"
STORM-3155,IOutputSerializer implementations always allocates a new ByteBuffer,"The IOutputSerializer javadoc specifies that the user may optionally provide a ByteBuffer to serialize into.

https://github.com/apache/storm/blob/af42f434f4a4c3d9087c6058b359033736d3b5e8/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/IOutputSerializer.java#L26

None of the IOutputSerializer implementations we ship with actually do this. They all ignore the ByteBuffer parameter.

If this is a useful feature, I think that we should update them to use the supplied ByteBuffer if not null. If it isn't a useful feature, we should instead remove the ByteBuffer parameter from the API."
STORM-3154,Prometheus /metrics http endpoint for monitoring integration,"Feature Request to add Prometheus /metrics http endpoint for monitoring integration:

[https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E]

Prometheus metrics format for that endpoint:

[https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md]

 "
STORM-3153,Restore storm sql provider tests affected via STORM-2406,"Current proposed patch for STORM-2406 gets rid of major parts of tests for provider, because we can't do same test newly changed code. To restore tests additional code change is needed.

This issue is to track the effort of making change of Storm SQL to be able to restore previous tests.

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-hdfs/src/test/org/apache/storm/sql/hdfs/TestHdfsDataSourcesProvider.java#L90]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-kafka/src/test/org/apache/storm/sql/kafka/TestKafkaDataSourcesProvider.java]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-mongodb/src/test/org/apache/storm/sql/mongodb/TestMongoDataSourcesProvider.java]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-redis/src/test/org/apache/storm/sql/redis/TestRedisDataSourcesProvider.java]

 "
STORM-3152,Storm has supported ipv6 but Troubleshooting.md didn't update,"As storm's socket get inetaddress by java's InetAddress class,I think there is no difficulty to run Topology on ipv6.And I have test it on ipv6,all socket can be established on ipv6's ip."
STORM-3151,Negative Scheduling Resource/Overscheduling issue,"Possible overscheduling captured when follow steps are performed (Logging is added in STORM-3147)

1) launch nimbus & zookeeper

2) launch supervisor 1

3) launch topology 1 (I used org.apache.storm.starter.WordCountTopology)

4) launch supervisor 2

5) launch topology 2 (I used org.apache.storm.starter.ExclamationTopology)

{noformat}
2018-07-13 12:58:43.196 o.a.s.d.n.Nimbus timer [WARN] Memory over-scheduled on 176ec6d4-2df3-40ca-95ca-c84a81dbcc22-172.130.97.212
{noformat}

Indicating there may be issues inside scheduler.
It is discovered when I ported ClusterSummay to StormMetrics"
STORM-3149,Why did an exception in the client read not bring down the entire worker,"[https://github.com/apache/storm/pull/2762]

 

ran into some issues where we got an array index out of bounds, but it didn't bring down the worker, just caused issues with the one message being sent.  We should understand what was happening and if there is anything we should fix."
STORM-3146,dependencies conflict,"I have configure apache hadoop cluster (2.9.1) . storm hdfs by default it takes 2.6.1 hadoop dependencies  , I have excluded it from dependencies and add hadoop 2.9.1 dependencies. I have attached my pom along with it. 

I am finding following error:

java.lang.NoSuchMethodError: org.apache.hadoop.security.authentication.util.KerberosUtil.hasKerberosTicket(Ljavax/security/auth/Subject;)Z
 at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:666) ~[hadoop-common-2.9.1.jar:?]
 at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:861) ~[hadoop-common-2.9.1.jar:?]

 

When I change the hadoop version to 2.6.1 ,I am not finding that error.

 

 

 "
STORM-3145,UNCOMMITTED_EARLIEST and UNCOMMITTED_LATEST doesn't work as expected,"I am using storm-kafka-client 1.2.2.

When I ran my topology I got NPE as mentioned in https://issues.apache.org/jira/browse/STORM-3046.

So I modified the KafkaTridentSpoutEmitter.java#seek with patch mentioned in the Jira STORM-3046. Below is the modified code.
{noformat}
private long seek(TopicPartition tp, KafkaTridentSpoutBatchMetadata lastBatchMeta, long transactionId) {

if (isFirstPoll(tp, transactionId)) {

if (firstPollOffsetStrategy == EARLIEST) {

LOG.debug(""First poll for topic partition [{}], seeking to partition beginning"", tp);

kafkaConsumer.seekToBeginning(Collections.singleton(tp));

} else if (firstPollOffsetStrategy == LATEST) {

LOG.debug(""First poll for topic partition [{}], seeking to partition end"", tp);

kafkaConsumer.seekToEnd(Collections.singleton(tp));

} else if (lastBatchMeta != null) {

LOG.debug(""First poll for topic partition [{}], using last batch metadata"", tp);

kafkaConsumer.seek(tp, lastBatchMeta.getLastOffset() + 1); // seek next offset after last offset from previous batch

} else if (firstPollOffsetStrategy == UNCOMMITTED_EARLIEST) {

LOG.debug(""First poll for topic partition [{}] with no last batch metadata, seeking to partition beginning"", tp);

kafkaConsumer.seekToBeginning(Collections.singleton(tp));

} else if (firstPollOffsetStrategy == UNCOMMITTED_LATEST) {

LOG.debug(""First poll for topic partition [{}] with no last batch metadata, seeking to partition end"", tp);

kafkaConsumer.seekToEnd(Collections.singleton(tp));

}

firstPollTransaction.put(tp, transactionId);

} else if (lastBatchMeta != null) {
        kafkaConsumer.seek(tp, lastBatchMeta.getLastOffset() + 1);  // seek next offset after last offset from previous batch
        LOG.debug(""First poll for topic partition [{}], using last batch metadata"", tp);
} else {
        long initialFetchOffset = firstPollTransaction.get(tp);
        OffsetAndMetadata lastCommittedOffset = kafkaConsumer.committed(tp);
        kafkaConsumer.seek(tp, lastCommittedOffset.offset());
        LOG.debug(""First poll for topic partition [{}], no last batch metadata present.""
                  + "" Using stored initial fetch offset [{}]"", tp, initialFetchOffset);
}

    final long fetchOffset = kafkaConsumer.position(tp);
    LOG.debug(""Set [fetchOffset = {}] for partition [{}]"", fetchOffset, tp);
    return fetchOffset;
}{noformat}
Now after code change - when the offset strategy is UNCOMMITTED_EARLIEST for the very first time when it is the first poll for the spout, lastBatchMeta is null and the kafka consumer seeks to the beginning offset. Should I be checking for last commit and starting from there? Likewise for the next fetch (when lastBatchMeta is not null), kafka consumer seeks to lastBatchMeta.getLastOffset() + 1. Should I be doing same here, checking for last commit and starting from there?

 "
STORM-3144,Extend metrics on Nimbus,"Metrics include:
 # File upload time
 # Nimbus restart count
 # Nimbus loss of leadership: meter marking when a nimbus node gains or loses leadership
 # Excessive scheduling time (both duration distribution and current longest)"
STORM-3140,Duplicated method in Logviewer REST API?,"{code:java}
    /**
     * Handles '/searchLogs' request.
     */
    @GET
    @Path(""/searchLogs"")
    public Response searchLogs(@Context HttpServletRequest request) throws IOException {
        String user = httpCredsHandler.getUserName(request);
        String topologyId = request.getParameter(""topoId"");
        String portStr = request.getParameter(""port"");
        String callback = request.getParameter(""callback"");
        String origin = request.getHeader(""Origin"");

        return logviewer.listLogFiles(user, portStr != null ? Integer.parseInt(portStr) : null, topologyId, callback, origin);
    }

    /**
     * Handles '/listLogs' request.
     */
    @GET
    @Path(""/listLogs"")
    public Response listLogs(@Context HttpServletRequest request) throws IOException {
        meterListLogsHttpRequests.mark();

        String user = httpCredsHandler.getUserName(request);
        String topologyId = request.getParameter(""topoId"");
        String portStr = request.getParameter(""port"");
        String callback = request.getParameter(""callback"");
        String origin = request.getHeader(""Origin"");

        return logviewer.listLogFiles(user, portStr != null ? Integer.parseInt(portStr) : null, topologyId, callback, origin);
    }{code}

These two methods are identical although they seem to serve different functions."
STORM-3139,worker fails to start - KeeperErrorCode = NoAuth for /credentials/topologyname," 

Seeing a sporadic test failure internally for us with a worker that won't come up.  The test schedules a bunch of topologies, kills the supervisors, restarts nimbus, and then starts up the supervisors and validates the topologies are all fully running.

 

I've seen this test failure twice in the last two weeks.  The worker has migrated and cannot come up:

 
{code:java}
2018-06-30 10:15:24.102 b.s.util main [WARN] Expecting exception of class: class java.nio.channels.ClosedByInterruptException, but exception chain only contains: (#<RuntimeException java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966> #<NoAuthException org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966>) 2018-06-30 10:15:24.102 b.s.d.worker main [ERROR] Error on initialization of server mk-worker java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966 at backtype.storm.util$wrap_in_runtime.invoke(util.clj:53) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.zookeeper$get_data.invoke(zookeeper.clj:135) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4249.get_data(zookeeper_state_factory.clj:125) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__4296.get_data(pacemaker_state_factory.clj:175) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at backtype.storm.cluster$mk_storm_cluster_state$reify__3910.credentials(cluster.clj:563) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$fn__7710$exec_fn__1599__auto____7711.invoke(worker.clj:623) ~[storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?] at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$fn__7710$mk_worker__7803.doInvoke(worker.clj:598) [storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$_main.invoke(worker.clj:810) [storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?] at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.2.y.jar:0.10.2.y] Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966 at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:113) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:327) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:316) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:313) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:304) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:35) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.zookeeper$get_data.invoke(zookeeper.clj:131) ~[storm-core-0.10.2.y.jar:0.10.2.y] ... 31 more 2018-06-30 10:15:24.199 b.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"")
{code}"
STORM-3133,Extend metrics on Nimbus and LogViewer,"Include but not limited to

Logviewer

1. Clean-up time
 2. Time to complete one clean up loop Time. 
 3. Disk usage by logs before cleanup and After cleanup loop. ( Just like GC.?)
 4. Failures/exceptions.
 5. Search request Cnt: By category - Archived/non-archived
 6. Search Request - Response time
 7. Search Request - 0 result Cnt
 8. Search Result - open files
 9. File partial read count
 10. File Download request Cnt/ And Size served
 11. Disk IO by logviewer
 12. CPU usage ( for unzipping files)

Nimbus Additional:
 * Topology stormjar.ser/stormconf.ser/stormser.ser file upload time.
 * Scheduler related metrics would be a long list generic and specific to different strategies.
 * Most if not all cluster summary can be pushed as Metrics.
 * Restart cnt
 * Nimbus loss of leadership !/jira/images/icons/emoticons/help_16.png|width=16,height=16,align=absmiddle!
 * UI not responding ([https://jira.ouroath.com/browse/YSTORM-4838])
 * Negative resource scheduling events ([https://jira.ouroath.com/browse/YSTORM-4940])
 * Excessive scheduling time  !/jira/images/icons/emoticons/help_16.png|width=16,height=16,align=absmiddle!"
STORM-3132,NPE in Values Constructor,"Passing null argument to the `Values` Constructor can cause worker to crash.

 

{code}2018-06-29 05:30:53.088 o.a.s.e.e.ReportError Thread-17-b-2-executor[8, 8] [ERROR] Error
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NullPointerException
    at org.apache.storm.utils.Utils$2.run(Utils.java:365) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
    at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:169) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:156) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.utils.Utils$2.run(Utils.java:350) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    ... 1 more
Caused by: java.lang.NullPointerException
    at org.apache.storm.tuple.Values.&lt;init&gt;(Values.java:26) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.starter.trident.TridentWordCount$Split.execute(TridentWordCount.java:80) ~[stormjar.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.trident.planner.processor.EachProcessor.execute(EachProcessor.java:65) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.trident.planner.SubtopologyBolt$InitialReceiver.receive(SubtopologyBolt.java:227) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.trident.planner.SubtopologyBolt.execute(SubtopologyBolt.java:169) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:247) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:232) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.executor.Executor.accept(Executor.java:275) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:169) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:156) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.utils.Utils$2.run(Utils.java:350) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    ... 1 more
2018-06-29 05:30:53.116 o.a.s.u.Utils Thread-17-b-2-executor[8, 8] [ERROR] Halting process: Worker died
java.lang.RuntimeException: Halting process: Worker died
    at org.apache.storm.utils.Utils.exitProcess(Utils.java:470) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.utils.Utils$4.run(Utils.java:753) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at org.apache.storm.executor.error.ReportErrorAndDie.uncaughtException(ReportErrorAndDie.java:41) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
    at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_131]\{code}"
STORM-3130,Add Timer registration and Timed object wrapper to Storm metrics util.,This allows us to time method running duration or variable/resource lifespan.
STORM-3128,Connection refused error in AsyncLocalizerTest,"In AsyncLocalizerTest testKeyNotFoundException, a localBlobStore is created and tries but failed to connect to zookeeper due to connection error. I'm not sure if this compromises the test even though it is passed after connection retry timeout. But it's nice to keep in mind.

{noformat}
2018-06-27 13:05:28.005 [main-SendThread(localhost:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-06-27 13:05:28.032 [main] INFO  org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl - Default schema
2018-06-27 13:05:28.035 [main-SendThread(localhost:2181)] WARN  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_171]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_171]
	at org.apache.storm.shade.org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
{noformat}

I managed to track down the source where the exception is thrown, but it's really strange that this is called by a StormTimer inside Supervisor, which is not declared anywhere in this test. I'm completely lost by now.


{noformat}
2018-08-08 11:45:30.217 [heartbeatTimer] ERROR org.apache.storm.zookeeper.ClientZookeeper - e: {}
org.apache.storm.shade.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /supervisors
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:268) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:257) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForegroundStandard(ExistsBuilderImpl.java:254) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:247) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:206) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:35) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:145) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:292) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.cluster.ZKStateStorage.set_ephemeral_node(ZKStateStorage.java:129) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl.supervisorHeartbeat(StormClusterStateImpl.java:522) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.timer.SupervisorHeartbeat.run(SupervisorHeartbeat.java:96) [classes/:?]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.0-SNAPSHOT.jar:?]
{noformat}
"
STORM-3112,Incremental scheduling supports,"As https://issues.apache.org/jira/browse/STORM-3093 described, now the scheduling work for a round is a complete scan and computation for all the topologies on cluster, which is a very heavy work when topologies increment to hundreds.

So this JIRA is to refactor the scheduling logic that only care about topologies that need to.

Promotions list:
1. Cache the id to storm base mapping which reduce the pressure to ZooKeeper.
2. Only schedule the topologies that need to: with dead executors or not enough running workers.
3. For some schedulers we still need a full scheduling, i.e. IsolationScheduler.
4. Cache the scheduling resource bestride multi scheduling round, i.e. nodeId -> used slot, nodeId -> used resource, nodeId -> totalResource.

Cause in https://issues.apache.org/jira/browse/STORM-3093 i already cache the storm-id -> executors mapping, now for a scheduling round, thing we will do:
1. Scan all the active storm bases( cached ) and local storm-conf/storm-topology, then to refresh the heartbeats cache, and we will know which topologies need to schedule.
2. Compute scheduleAssignment only for need scheduling topologies.

About robustness when nimbus restarts:
1. The cached storm-bases are taken care of by ILocalAssignmentsBackend.
2. the scheduling cache will be refresh for the first time scheduling through a full topologies scheduling.
"
STORM-3108,strom-hdfs can support write tuple to orc file format,strom-hdfs can support write tuple to orc file format
STORM-3107,Nimbus confused about leadership after crash,"Nimbus crashed and restarted without shutting down zookeeper due to a deadlock in the timer shutdown code.  This could however also happen for various other issues.  

 

The problem is that once Nimbus restarted, it was really confused about who the leader was:

 
{code:java}
2018-05-24 09:27:21.762 o.a.s.z.LeaderElectorImp main [INFO] Queued up for leader lock.
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping assignments
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping cleanup
2018-05-24 09:27:22.633 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping credential renewal.

2018-05-24 09:27:40.771 o.a.s.d.n.Nimbus pool-37-thread-63 [WARN] Topology submission exception. (topology name='topology-testOverSubscribe-1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-05-24 09:27:40.771 o.a.s.b.BlobStoreUtils Timer-1 [ERROR] Could not download the blob with key: topology-testOverCapacityScheduling-2-1519992333-stormcode.ser
2018-05-24 09:27:40.771 o.a.t.s.TThreadPoolServer pool-37-thread-63 [ERROR] Error occurred during processing of message.
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2961) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more
{code}
The session timeout was set to 20 seconds, but we're exceeding this period, and Nimbus did not recover leadership.  It needed to be restarted manually to recover.

 "
STORM-3106,Clarify hive version requirement in storm-hive documentation,"{{[storm-hive|https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/external/storm-hive/pom.xml]}} uses 3 [dependencies|https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/external/storm-hive/pom.xml#L54] from {{org.apache.hive.hcatalog}}. All the dependencies are compiled with {{hive.version}} hardcoded [here|https://github.com/apache/storm/blob/0eb6b5116f251e17b6f14a61cebfadfc286faa59/pom.xml#L292].

If {{hive.version}} doesn't match the hive cluster's version there could be issues because of version mismatch such as [this|https://lists.apache.org/thread.html/36f674f74e73f9ddc83b6b2452acfd9773af4e6394c8cb157dd367fd@%3Cuser.storm.apache.org%3E]. I see two ways of fixing this -
 * Allow users to specify {{hive.version}} when using storm-hive dependency.
 * Update {{storm-hive}} documentation to state that user's need to copy the 3 dependencies needed by \{{storm-hive}} and specify the correct hive version for them.

Open to any other ideas."
STORM-3104,Delayed worker launch due to accidental transitioning in state machine,"In Slot.java, there is a comparison in 
{code:java}
handleWaitingForBlobUpdate()
{code}
 between dynamic state's current assignment and new assignment, which accidentally route back state machine just transitioned from WAIT_FOR_BLOB_LOCALIZATION back to WAIT_FOR_BLOB_LOCALIZATION, because the current assignment in this case is highly likely to be null and different from new assignment (I'm not sure if it's guaranteed). This causes delay for a worker to start/restart.

The symptom can be reproduced by launching an empty storm server and submit any topology. Here's the log sample (relevant transition starting from 2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [DEBUG]):

{code:sh}
2018-06-13 16:57:10.254 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE EMPTY msInState: 6024 -> EMPTY msInState: 6024
2018-06-13 16:57:10.255 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE EMPTY
2018-06-13 16:57:10.257 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from EMPTY to WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:10.257 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE EMPTY msInState: 6027 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2018-06-13 16:57:10.258 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:10.258 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingChangingBlobs are []
2018-06-13 16:57:11.259 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 1003 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 1003
2018-06-13 16:57:11.260 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:11.260 o.a.s.d.s.Slot SLOT_6700 [DEBUG] found changing blobs [BLOB CHANGING LOCAL TOPO BLOB TOPO_CONF test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02), BLOB CHANGING LOCAL TOPO BLOB TOPO_CODE test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)] moving them to pending...
2018-06-13 16:57:12.262 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 2005 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 2005
2018-06-13 16:57:12.263 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.263 o.a.s.d.s.Slot SLOT_6700 [DEBUG] found changing blobs [BLOB CHANGING LOCAL TOPO BLOB TOPO_JAR test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)] moving them to pending...
2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02) == current null ? false
2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [INFO] There are pending changes, waiting for them to finish before launching container...
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from WAITING_FOR_BLOB_LOCALIZATION to WAITING_FOR_BLOB_UPDATE
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 2018 -> WAITING_FOR_BLOB_UPDATE msInState: 1
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_UPDATE
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization: null, new: LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02), current: null, pdchanging: LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)
2018-06-13 16:57:12.276 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Assignment Changed from null to LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)
2018-06-13 16:57:12.278 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from WAITING_FOR_BLOB_UPDATE to WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.278 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_UPDATE msInState: 4 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingChangingBlobs are []
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02) == current null ? false
2018-06-13 16:57:12.280 o.a.s.d.s.Slot SLOT_6700 [DEBUG] launch container for the first time
2018-06-13 16:57:12.284 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Created Worker ID 4155b2bb-ebd1-431d-907f-d8a4ff1e1da4

{code}

I would like to know if this is actually the desired behavior of the state machine, or I can help fix the bug. The implementation would be to redesign the if statement."
STORM-3100,"Minor optimization: Replace HashMap<Integer, T> with an array backed data structure for faster lookups","* Introduce _CustomIndexArray_: An array backed data structure to speedup HashMap<Integer, T> use cases *in critical path*. It needs to supported -ve indexing and a user defined (on construction) Upper and Lower Index range. Does not need to be dynamically resizable given the nature of use cases we have.
 * Use this data structure for _GeneralTopologyContext._taskToComponent_ mapping which is looked up in the critical path _Task.getOutgoingTasks._ This lookup happens at least once for every emit and consequently can happen millions of times per second.
 * Also use this for _JCQueue.localReceiveQueues_ where the basic idea is already in use but not in a reusable manner."
STORM-3099,"Extend metrics on supervisor, workers, and DRPC","This patch serves to extend metrics on supervisor and worker. Currently the following metrics are being implemented, including but not limited to:

Worker:
# Kill Count by Category - Assignment Change/HB too old/Heap Space
# Time spent in each state
# Time to Actually Kill worker (from identifying need by supervisor and actual change in the state of the worker) - per worker?
# Time to start worker for topology from reading assignment for the first time.
# Worker cleanup Time/Worker cleanup Retries
# Worker Suicide Count - category: internal error or Assignment Change

Supervisor:
# Supervisor restart Count 
# Blobstore (Request to download time) 
    - # Download time individual blob (inside localizer) localizer gettting requst to actually download hdfs request to finish
    - # Download rate individual blob (inside localizer)
    - # Supervisor localizer thread blob download - how long (outside localizer)
# Blobstore Update due to Version change Cnts
# Blobstore Storage by users

DRPC:
#  Avg/Max Time to respond to Http Request

There might be more metrics added later. 

This patch will also refactor code in relevant files. Bugs found during the process will be reported in other issues and handled separately."
STORM-3088,Request to get storm's Kryo configuration for other use,"In short, I'd like a way to get a Kryo serializer for ""private"" use that has the same configuration used by storm for serializing inter-worker tuples. Ideally, that would be a version of SerializationFactory.getKryo() that returned a Kryo object with the same configuration of storm's, but that doesn't exist.

Obviously, we can pass the topology configuration Map to getKryo(), but there's no way for a library, whose internal workings should be opaque to storm components, to get access to that Map.

We've worked around this by adding an initialization call in our components' open/prepare methods, but that's just icky. It would be much cleaner if the library could handle this on its own without bothering component developers that shouldn't have to deal with this.

I can think of several solutions, any of which would be acceptable. Some would probably be useful for cases other than ours.
 * As mentioned above, a variant of SerializationUtils.getKryo() that returned a Kryo object with the same configuration as storm's.
 * An API that could be called anywhere that returned the Map passed to components' open/prepare methods.
 * A mechanism to allow registering an initialization function to be called on worker startup. It would be passed the above-mentioned Map, and all initializers would be called before the worker started any components. (I kind of like this one best. Seems most flexible.)

Background:

We have a custom Kryo serializer for our events that implements lazy deserialization. Most tuples have just a single event object. On serialization, some fields of the event are serialized using Kryo, others with a a more primitive method. But on deserialization on receipt in a worker, no fields are actually deserialized; fields are only deserialized when referenced by the receiving bolt. On re-serialization for output, only fields modified within the worker are serialized. Since a large majority of fields in our events never change as they flow through multiple bolts, this saves considerable CPU in serialization/deserialization.

The issue is: When the event is serialized by storm, we use storm's Kryo serializer. But deserialization of a field may happen when a bolt references a serialized field, and at that point we don't have storm's Kryo, only one we created ourselves. Ensuring the two Kryos are configured the same requires access to the storm configuration Map.

Like I say, we've hacked around this issue, but would prefer a cleaner solution."
STORM-3085,Expose Ability to Disable CORS for Storm UI Rest API,"STORM-1960 enabled CORS by default in Storm UI Rest API. Some users may need to disable CORS to comply with security restrictions, so it should be possible to disable this."
STORM-3081,Storm kafka client not consuming messages properly,"A single thread is pushing some messages to kafka serially and we consume it at the storm's end using storm-kafka-client.
After a few requests, the consumer is not able to consume from the queue blocking the thread which waits for the response from storm's end. 
We added one more consumer with a different consumer group and there the messages are getting read properly.
So we know there is some problem at the storm kafka client consumer's end.
The kafka spout config is written like this - 

KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(kafkaProperties.getProperty(""bootstrap.servers""), stormProperties.getProperty(""TOPIC""))
                .setFirstPollOffsetStrategy(KafkaSpoutConfig.FirstPollOffsetStrategy.LATEST)
                .setRecordTranslator(new MessageDeserializer(), arguments)
                .build();

I can't seem to figure out the issue.
Can someone please help me out?

Thanks."
STORM-3080,Update Documentation for Lifecycle-of-a-topology.html page,"Currently content of page: [http://storm.apache.org/releases/2.0.0-SNAPSHOT/Lifecycle-of-a-topology.html] is still based on 0.7.1 code and most of the links are dead. 

This is quite important page and helpful for anyone new to understand how storm works and what exactly happens inside, so request to update it with the latest code and current stable version."
STORM-3078,HBAuthorizationException appears unused,Looks like this class is safe to remove.
STORM-3076,"In Strom0.9.5,there is no LocalCluster implements ILocalCluster, which results in examples in external can not be compiled","In the  external package,the modules include storm-hbase,storm-hdfs and storm-kafka , can not be compiled successfully, due to the lack of specific implements of ILocalCluster in the storm-core-0.9.5 。"
STORM-3075,NPE starting nimbus,"{code:java}
2018-05-15 14:14:59.873 o.a.c.f.l.ListenerContainer main-EventThread [ERROR] Listener (org.apache.storm.zookeeper.Zookeeper$1@26d820eb) threw an exception
java.lang.NullPointerException: null
        at org.apache.storm.nimbus.LeaderListenerCallback.leaderCallBack(LeaderListenerCallback.java:118) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.Zookeeper$1.isLeader(Zookeeper.java:124) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:665) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:661) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) ~[curator-client-4.0.1.jar:?]
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:660) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:539) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:590) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) ~[zookeeper-3.4.6.jar:3.4.6-1569965]

{code}"
STORM-3074,Inconsistent null checking in SaslMessageToken,"The SaslMessageToken class will throw an NPE if buffer() is called and the payload is null. While the buffer method checks whether the token is null in a few places before dereferencing, the encodedLength method is called right off the bat, and it doesn't check for null.

The payload is always generated by either https://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslServer.html#evaluateResponse(byte[]) or https://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslClient.html#evaluateChallenge(byte[]). The javadoc indicates that if these return null, authentication has succeeded and it is unnecessary to send any more messages to the other party.

I think if null SaslMessageToken payloads are never sent over the wire, we should remove all the null checking in SaslMessageToken and MessageDecoder, and ensure that the SASL handlers check for null before deciding to write tokens."
STORM-3071,change checkstyle plugin setting logViolationsToConsole to true,"There's no value in hiding the details of checkstyle warnings and errors with `logViolationsToConsole`, so it should be `true`."
STORM-3066,Storm Flux variable substitution,"we are using flux to submit storm topology, for topology yml file, we need substitute variables, from [https://github.com/apache/storm/tree/master/flux], it says variable can be substituted in format of ${variable.name}, my question is, in my case, variable is a list, and I only want to get first element of the list, how can I do it?


 I try to use ${variable.0} and ${variable}[0], it doesn't work, after substitution, the result is element_name.0 or element_name[0], which is not I expected(expected one should be element_name), please let me know how to specify the first element of a list in flux."
STORM-3062,Document JMXStormReporter configuration,"discussion on the PR for STORM-2988(https://issues.apache.org/jira/browse/STORM-2988) to document JMXStormReporter.

 "
STORM-3057,ERROR An exception occurred processing Appender syslog,"*ERROR An exception occurred processing Appender syslog*

2018-05-04 14:44:43,721 ERROR An exception occurred processing Appender syslog org.apache.logging.log4j.core.appender.AppenderLoggingException: Error flushing stream UDP:localhost:514
 at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:159)
 at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:112)
 at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:99)
 at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:430)
 at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:409)
 at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:367)
 at org.apache.logging.log4j.core.Logger.logMessage(Logger.java:112)
 at org.apache.logging.slf4j.Log4jLogger.log(Log4jLogger.java:375)
 at org.apache.log4j.Category.differentiatedLog(Category.java:186)
 at org.apache.log4j.Category.error(Category.java:272)
 at com.surfilter.analyse.es.common.service.impl.EsIndexServiceImpl.insertEs(EsIndexServiceImpl.java:141)
 at com.surfilter.analyse.es.common.service.impl.EsIndexServiceImpl.bulkIndex(EsIndexServiceImpl.java:97)
 at com.surfilter.analyse.es.service.RecordESService.saveDnsObjectToES(RecordESService.java:122)
 at com.surfilter.analyse.dns.dao.illegalWeb.IllegalWebESDAO.batchSave(IllegalWebESDAO.java:128)
 at com.surfilter.analyse.dns.service.illegalWeb.IllegalWebService.dowork(IllegalWebService.java:71)
 at com.surfilter.analyse.dns.storm.bolt.DataStoreHandler.storeData(DataStoreHandler.java:28)
 at com.surfilter.analyse.dns.storm.bolt.DnsCheckBolt.execute(DnsCheckBolt.java:168)
 at backtype.storm.daemon.executor$fn__5694$tuple_action_fn__5696.invoke(executor.clj:690)
 at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:435)
 at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58)
 at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:132)
 at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106)
 at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)
 at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:818)
 at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479)
 at clojure.lang.AFn.run(AFn.java:22)
 at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: 消息过长
 at java.net.PlainDatagramSocketImpl.send(Native Method)
 at java.net.DatagramSocket.send(DatagramSocket.java:676)
 at org.apache.logging.log4j.core.net.DatagramOutputStream.flush(DatagramOutputStream.java:103)
 at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:156)
 ... 26 more

2018-05-04 14:56:27,877 ERROR Unable to write to stream UDP:localhost:514 for appender syslog
2018-05-04 15:27:45,512 ERROR Logger contains an invalid element or attribute ""appender""
2018-05-04 16:18:08,456 ERROR Logger contains an invalid element or attribute ""appender""

 

*but storm  worker  often auto shutdown 。*"
STORM-3054, Add Topology level configuration socket timeout for DRPC Invocation Client,"The Thrift Invocation Client socket connection timeout  {{SUPERVISOR_THRIFT_SOCKET_TIMEOUT_MS}} is 10 minutes which is too long for DRPC clients. Also, this configuration for DRPC should configurable at topology level."
STORM-3051,some  Potential NPEs ,"We have developed a static analysis tool [NPEDetector|https://github.com/lujiefsi/NPEDetector] to find some potential NPE. Our analysis shows that some callees may return null in corner case(e.g. node crash , IO exception), some of their callers have  _!=null_ check but some do not have. 

*Bug:*

1.  callee CgroupCenter#getSubSystems return null when meet exception:
{code:java}
} catch (Exception e) {
LOG.error(""Get subSystems error {}"", e);
}
return null;
{code}
but its caller use it without check:
{code:java}
public boolean isSubSystemEnabled(SubSystemType subSystemType) {
  Set<SubSystem> subSystems = this.getSubSystems();
  for (SubSystem subSystem : subSystems) {
     if (subSystem.getType() == subSystemType) {
     return true;
  }
  }
  return false;
}{code}
other callee and caller pair that have same problem.

2. callee RAS_Node#getUsedSlots and caller RAS_Node#totalSlotsUsed
 3. CgroupCenter#getHierarchies and caller CgroupCenter#isMounted, CgroupCenter#getHierarchyWithSubSystems

4. callee LocalState#getApprovedWorkers and caller BasicContainer#cleanUpForRestart,BasicContainer#<init>"
STORM-3050,a potential NPE in ConstraintSolverStrategy#checkResourcesCorrect,"We have developed a static analysis tool [NPEDetector|https://github.com/lujiefsi/NPEDetector] to find some potential NPE. Our analysis shows that some callees may return null in corner case(e.g. node crash , IO exception), some of their callers have  _!=null_ check but some do not have. 

*Bug:*

callee TopologyDetails#getTotalCpuReqTask have two callers, one of them have null checker:
{code:java}
public double getTotalRequestedCpu() {
double totalCpu = 0.0;
for (ExecutorDetails exec : this.getExecutors()) {
Double execCpu = getTotalCpuReqTask(exec);
if (execCpu != null) {
totalCpu += execCpu;
}
}
return totalCpu;
}
{code}
but ConstraintSolverStrategy#checkResourcesCorrect have no checker:
{code:java}
for (ExecutorDetails executor : entry.getValue()) {
supervisorUsedCpu += topology.getTotalCpuReqTask(executor);
}
{code}
 "
STORM-3049,a potential NPE in SupervisorSimpleACLAuthorizer#permit SimpleACLAuthorizer#permit,"We have developed a static analysis tool [NPEDetector|https://github.com/lujiefsi/NPEDetector] to find some potential NPE. Our analysis shows that some callees may return null in corner case(e.g. node crash , IO exception), some of their callers have  _!=null_ check but some do not have. 

*Bug:*

callee ReqContext#principal have 12 callers, 10 of them have null checker like:
{code:java}
public boolean permit(ReqContext context, String operation, Map<String, Object> topoConf) {
    return context.principal() != null ? users.contains(context.principal().getName()) : false;
}
{code}
but SupervisorSimpleACLAuthorizer#permit  and SimpleACLAuthorizer#permit have no, just like:
{code:java}
//SupervisorSimpleACLAuthorizer#permit 
String principal = context.principal().getName();{code}"
STORM-3045,Microsoft Azure EventHubs: Storm Spout and Bolt improvements,
STORM-3038,'not a leader' exception submitting topologies to LocalCluster,"In our local tests running on our openstack build servers, we get intermittent failures due to the following exception being thrown when submitting topologies to LocalCluster:

{code}275354 [main] WARN  o.a.s.d.nimbus - Topology submission exception. (topology name='AnalyseUpdate') #error {
 :cause not a leader, current leader is NimbusInfo{host='<hostname>', port=6627, isLeader=true}
 :via
 [{:type java.lang.RuntimeException
   :message not a leader, current leader is NimbusInfo{host='<hostname>', port=6627, isLeader=true}
   :at [org.apache.storm.daemon.nimbus$is_leader doInvoke nimbus.clj 150]}]
 :trace
 [[org.apache.storm.daemon.nimbus$is_leader doInvoke nimbus.clj 150]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10799 submitTopologyWithOpts nimbus.clj 1681]
  [org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10799 submitTopology nimbus.clj 1774]
  [sun.reflect.GeneratedMethodAccessor299 invoke nil -1]
  [sun.reflect.DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl.java 43]
  [java.lang.reflect.Method invoke Method.java 498]
  [clojure.lang.Reflector invokeMatchingMethod Reflector.java 93]
  [clojure.lang.Reflector invokeInstanceMethod Reflector.java 28]
  [org.apache.storm.testing$submit_local_topology invoke testing.clj 310]
  [org.apache.storm.LocalCluster$_submitTopology invoke LocalCluster.clj 49]
  [org.apache.storm.LocalCluster submitTopology nil -1]{code}

(note that {{isLeader=true}} in the exception message)

This is despite a retry mechanism we've implemented; this seems to be a continual failure, once its hit. This tends to be surrounded by lots of zookeeper connection lost/reconnection log message."
STORM-3034,Log exception stacktrace for executor failures in worker,"Sometimes the Stacktrace is not available as part of the Exceptions generated by component.

Adding stackTrace print to the logs would be helpful for users to identify exceptions"
STORM-3033,NullPointerException in consumeBatchToCursor method,"Error in consumeBatchToCursor method after few days or running. Error occurred on bolt's ack method call.

Linked Issue: https://issues.apache.org/jira/browse/STORM-770

 
{code:java}
java.lang.RuntimeException: java.lang.NullPointerException
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:74) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__5043$fn__5056$fn__5109.invoke(executor.clj:861) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:785) [?:?]
Caused by: java.lang.NullPointerException
at org.apache.storm.stats$emitted_tuple_BANG_.invoke(stats.clj:123) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$mk_tasks_fn$fn__4656.invoke(task.clj:166) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$send_unanchored.invoke(task.clj:119) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__5043$fn$reify__5068.ack(executor.clj:816) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.task.OutputCollector.ack(OutputCollector.java:213) ~[storm-core-1.2.1.jar:1.2.1]
at bolt execute method
at org.apache.storm.daemon.executor$fn__5043$tuple_action_fn__5045.invoke(executor.clj:739) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:468) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]
... 6 more
{code}
 "
STORM-3032,NullPointerException in KafkaSpout emitOrRetryTuple,"{code:java}
java.lang.NullPointerException: null
at clojure.lang.Numbers.ops(Numbers.java:1013) ~[clojure-1.7.0.jar:?]
at clojure.lang.Numbers.multiply(Numbers.java:148) ~[clojure-1.7.0.jar:?]
at org.apache.storm.stats$transferred_tuples_BANG_.invoke(stats.clj:131) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$mk_tasks_fn$fn__4656.invoke(task.clj:167) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$send_unanchored.invoke(task.clj:119) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$fn__4990$send_spout_msg__4996.invoke(executor.clj:594) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$fn$reify__5006.emit(executor.clj:618) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:50) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.kafka.spout.KafkaSpout.emitOrRetryTuple(KafkaSpout.java:496) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.emitIfWaitingNotEmitted(KafkaSpout.java:440) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:308) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$fn__4975$fn__4990$fn__5021.invoke(executor.clj:654) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:785) [?:?]
{code}
Error in KafkaSpout emitOrRetry method"
STORM-3031,size of particules in StormUI visualization,"In StormUI visualization, the size of particules is driven by the length of the component name in my opinion it's counter-intuitive.
I think that a fixed size or may be a configurable size would be a better choice

May be a good choice would be that the size be linked with some kind of performance or latency.
Currently the color is the capacity, we could set the size as a reflect of the component latency (with an lower and upper limit)?"
STORM-3030,Does storm 0.10 support secure kafka 0.10 (kerberized) with SASL_PLAINTEXT ?,"Does storm 0.10 support secure kafka 0.10 (kerberized) with SASL_PLAINTEXT ?

 "
STORM-3024,Allow scheduling for RAS to happen in the background,"We have run into some issues recently where occasionally a strategy on a very large cluster will take an extra long amount of time finish scheduling.  This slowness cascades into other issues, like topologies not being able to be killed because the timer thread is still in use trying to run scheduling.

The plan is to make scheduling happen in a thread pool.  The main thread will wait for up to a configurable amount of time for the topology to be scheduled, but if it does not complete in that time it will be left to keep running in the background thread in hopes that later on it will be scheduled.

If for some reason the state of the cluster changes while scheduling is happening in the background we will cancel the scheduling, as any scheduling it produced may not be able to fit on the cluster.  The next time the scheduler runs it will restart the scheduling and hopefully allow the cluster to reach a steady state even if it takes a while, but without blocking kills and other critical operations from happening.

Note that we are also working on optimizing scheduling as well so that these issues don't happen in the first place."
STORM-3019,StormReporter doesn't have information on where it's running,"Metrics2 StormReporter implementations don't have a lot of information on where they're running. In particular, they are missing:
 * Whether they are running for nimbus, supervisor, or worker, and what the worker is.
 * The full deployed config - it's just provided with the basic topology configuration, not the full effective configuration as specified at topology deployment
 * A TopologyContext object"
STORM-3016,Nimbus gets down when job has large amount of parallelism components,"When a job having large amount of parallelism components( total parallelism rises to 5000 for example) been submmited to storm cluster, Nimubs might get crashed, the work flow is as below:

1)  Nimbus computting assignment

2) Nimbus sending assignment to zk

{color:#ff0000}3) When assignment mapping info string is too long due to  total parallelism of job being too large, sending this info to zk will fail (zNode datalength set default is 1M ){color}

{color:#333333}4) Nimbus keeps trying sending this assignment info, after some times, it gives up and crashed, with that happend, the stablity of the cluster will be greatly impacted{color}"
STORM-3014,TickTupleTest.testTickTupleWorksWithSystemBolt fails intermittently in Travis CI,"[https://travis-ci.org/apache/storm/jobs/359417643]

 
{code:java}
classname: org.apache.storm.TickTupleTest / testname: testTickTupleWorksWithSystemBolt
java.lang.AssertionError: took over 110000 ms of simulated time to get a message back...
	at org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt(TickTupleTest.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121){code}
 

Note that it is not failing consistently. Here's the build which the test passed.

[https://travis-ci.org/HeartSaVioR/storm/builds/359567639]

 "
STORM-3011,Use default bin path in flight.bash if $JAVA_HOME is undefined,"I had to modify the _flight.bash_ script as $JAVA_HOME is undefined, this yields
 
_/usr/share/apache-storm-1.2.1/bin/flight.bash: line 62: /bin/jstack: No such file or directory_
 
however, the  files can be found in the default path _/usr/bin_
 
I will submit a PR so that the script checks that $JDKPATH is not empty and exist before using it for BINPATH. I can't see any downsides in doing so."
STORM-3010,.NET Core adapter for Storm multi-lang protocol,"I have been using Storm with all topology components implemented in C# (.NET Core 2.0). I used [https://www.nuget.org/packages/Storm.Net.Adapter/] adapter but it has some problems and gaps (e.g. Storm context and configuration are not available, performance issue while reading message from standard input etc.). So I have implemented new adapter and would love to contribute it to Storm code base. The adapter is already implemented and I would like push it and create PR.

 

I tried to push the changes but it is insufficient permission issue. So what is the right way to do it?"
STORM-3008,Add Windows CI coverage,"I'd like to see us add test runs on Windows to our CI. Tests occasionally break on Windows, and we don't catch it during PR review because Travis only tests on Linux. 

Since Travis doesn't offer Windows support, we could look at using https://www.appveyor.com/, which is also free for open source projects. It's already used by a few other Apache projects, like Thrift and Spark."
STORM-3003,Nimbus should cache assignments to avoid excess state polling,"Since nimbus ( scheduler generates assignments) it can cache it instead of polling for it from ZK or other state manager.

This would improve scheduling iteration time, as well as all UI pages that require assignment information."
STORM-3002,Checkstyle plugin failure,"I followed the directions on this page: [https://github.com/apache/storm/tree/master/examples/storm-starter#build-and-install-storm-jars-locally] working on ref 8c8c0c31c70f3e73b85083d84f5ab9475c6f1e2b

 

When I run:
{code:java}
mvn clean install -DskipTests=true{code}
 

I get the following error:
{code:java}
...[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:49 min
[INFO] Finished at: 2018-03-20T21:54:28-05:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project storm-starter: Execution validate of goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check failed: Plugin org.apache.maven.plugins:maven-checkstyle-plugin:2.17 or one of its dependencies could not be resolved: Could not find artifact org.apache.storm:storm-checkstyle:jar:2.0.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR]{code}"
STORM-3001,Download releases link points to invalid location 404 Not found,"Go to [http://storm.apache.org/releases/current/Setting-up-development-environment.html]

Click on Download a storm release. It takes you to [http://storm.apache.org/releases//downloads.html] which returns 404"
STORM-2995,Topology runtime exception - Error on initialization,"{code:java}
2018-03-14 13:28:41.399 o.a.s.d.worker main [INFO] Reading Assignments. 2018-03-14 13:28:41.511 o.a.s.m.TransportFactory main [INFO] Storm peer transport plugin:org.apache.storm.messaging.netty.Context 2018-03-14 13:28:41.935 o.a.s.m.n.Server main [INFO] Create Netty Server Netty-server-localhost-6712, buffer_size: 5242880, maxWorkers: 1 2018-03-14 13:28:41.980 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6712 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data$fn__5244.invoke(worker.clj:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:269) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544.run(worker.clj:613) ~[storm-core-1.1.0.jar:1.1.0] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_51] at javax.security.auth.Subject.doAs(Subject.java:415) ~[?:1.7.0_51] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto____5543.invoke(worker.clj:611) ~[storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?] at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0] Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:444) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:436) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_51] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.1.0.jar:1.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_51] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_51] at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_51] 2018-03-14 13:28:42.004 o.a.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"") at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0]
{code}"
STORM-2992,Add support for starting storm-kafka-client spout consumer at a specific timestamp ,"The 0.10.1.0 KafkaConsumer has support for getting the offset corresponding to a given timestamp (the offsetsForTimes method). We could provide a new FirstPollOffsetStrategy to allow topologies to start consumption at a specific timestamp, instead of earliest or latest offset."
STORM-2991,Use MockConsumer for tests where possible in storm-kafka-client instead of using a raw Mockito mock,"MockConsumer seems like it will be less brittle for testing than using a raw Mockito mock, e.g. we don't have to manually stub the Consumer.position call. We should try to replace the mocks in existing tests."
STORM-2984,Introduce jitter in executor heartbeat,"The executorHeartbeatTimer is scheduled without jitter. In cases where state storage is Zookeeper and not pacemaker, introducing jitter will help spread out the I/O over time on Zookeeper. This will be helpful for large clusters.

The jitter time could be 0 < jitterTime < worker.heartbeat.frequency.secs"
STORM-2982,Support exactly-once stateful processing with Streams API,This issue is to track the effort of supporting exactly-once stateful processing with Streams API. Note that this issue doesn't cover end-to-end exactly-once processing.
STORM-2975,Worker died if KafkaSpout catched a kafka CommitFailedException,"org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:184) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:605) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1173) ~[stormjar.jar:?]
 at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:384) ~[stormjar.jar:?]
 at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:220) ~[stormjar.jar:?]
 at org.apache.storm.daemon.executor$fn__4962$fn__4977$fn__5008.invoke(executor.clj:646) ~[storm-core-1.1.1.jar:1.1.1]
 at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.1.jar:1.1.1]
 at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]"
STORM-2970,JCQueue can be full and will throw IllegalStateException,"I ran 

{code:java}
storm jar /tmp/storm-loadgen-2.0.0-SNAPSHOT.jar  org.apache.storm.loadgen.ThroughputVsLatency --spouts 1 --splitters 2  --counters 1 --rate 1000 -c topology.debug=true
{code}

and the topology was not running properly. And some exception showed up in the worker.log:



{code:java}
2018-02-23 15:13:29.939 o.a.s.u.Utils Thread-15-spout-executor[7, 7] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.IllegalStateException: Queue full
        at org.apache.storm.executor.Executor.accept(Executor.java:288) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:309) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.JCQueue.consume(JCQueue.java:290) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.JCQueue.consume(JCQueue.java:281) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:173) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:163) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$2.run(Utils.java:352) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.IllegalStateException: Queue full
        at java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:1.8.0_131]
        at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:112) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:526) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:71) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.Task.sendUnanchored(Task.java:196) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.sendSpoutMsg(SpoutOutputCollectorImpl.java:164) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.emit(SpoutOutputCollectorImpl.java:75) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:51) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.loadgen.LoadSpout.fail(LoadSpout.java:135) ~[stormjar.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutExecutor.failSpoutMsg(SpoutExecutor.java:362) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:126) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:119) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.RotatingMap.rotate(RotatingMap.java:77) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:298) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.Executor.accept(Executor.java:284) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 7 more
{code}


More log from the worker.log:

https://gist.github.com/Ethanlm/bcab1289a3813af6d7135aa95b1aaa14"
STORM-2966,Nimbus fails to log errors when shutting down at startup,"While testing, I encountered a NoClassDefFoundError for Nimbus at startup.  This caused Nimbus to shutdown with 0 lines in the log file.

 

Adding test code to catch the exception, I was able to see the log messages and fix the issue (see callstack below).

 

We should be logging the error on shutdown for debugging.

 

2018-02-19 14:56:28.842 o.a.s.d.n.Nimbus main [ERROR] Failed to initialize metric store

org.apache.storm.metricstore.MetricException: Failed to create metric store

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:41) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1113) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1103) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1098) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.launchServer(Nimbus.java:1008) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1026) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1031) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/io/RawComparator

        at org.apache.storm.hbasemetricstore.HBaseStore.<clinit>(HBaseStore.java:87) ~[storm-hbasemetricstore-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]

        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        ... 6 more

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.RawComparator

        at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_131]

        at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_131]

        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) ~[?:1.8.0_131]

        at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_131]

        at org.apache.storm.hbasemetricstore.HBaseStore.<clinit>(HBaseStore.java:87) ~[storm-hbasemetricstore-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]

        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        ... 6 more"
STORM-2964,Deprecate configs/interfaces/classes in 1.x line,This is to track the effort on deprecating removed configs/interfaces/classes from Storm 2.0.0 to 1.x version line. We may be late on the party but still be better to annotate rather than suddenly introducing the change.
STORM-2962,KeyValue State Resharding,"Storm's KeyValueState leverages namespace which is typically composed to component name + task id, which means the key-value pair is bound to the task. To allow rebalancing topology with different parallelism for stateful component, we should support resharding of the state.

I wonder more thing about current State implementation but it should be filed to other issues after verification."
STORM-2959,"Remove deprecated, old/unused config settings for 2.0","See 3 settings that can be removed :

nimbus.host

storm.messaging.netty.max_retries

storm.local.mode.zmq

 "
STORM-2957,Offsets are not committed when no ackers set (ackers=0) and KafkaSpout stops polling messages once it reaches the max uncommitted offsets ,"A KafkaSpout stops polling messages from topic when topology has no ackers set (""topology.acker.executors""= 0). This happens after KafkaSpout polls (and emits) messages equal to the max uncommitted offsets (KafkaspoutConfig.maxUncommittedOffsets).

This happens because the KafkaSpout.ack() is called even before the emit method adds the message to ""emitted"" collection (Set). In such cases, the message is not added ""acked"" collection (Map), and hence never committed. This seem to be bug introduced in a version later than storm-kafka-client 1.0.1 and current code (1.0.5) has a check in ack() that checks if the acked message is available in ""emitted"" collection and if not, ignores it (never adds it to ""acked"" collection)

Steps to reproduce issue:
1. Need a topology with KafkaSpout from storm-kafka-client
2. Set number of ackers (""topology.acker.executors"") to o (org.apache.storm.Config.setNumAckers(0)
3. While creating KafkaSpout instance, set SpoutConfig.setMaxUncommittedOffsets to small number (50 or 100)
4. Start topology, see to it that KafkaSpout polls more messages from Kafka that setMaxUncommittedOffsets
5. KafkaSpout stops polling messages from topic"
STORM-2954,Add HBase metricstore implementation,"In addition to RocksDB, we would like an HBase implementation of a MetricStore class."
STORM-2947,Review and fix/remove deprecated things in Storm 2.0.0,"We've been deprecating the things but haven't have time to replace/get rid of them. It should be better if we have time to review and address them.
"
STORM-2946,Upgrade storm-hbase to HBase 2.0,HBase 2.0 API changes break storm-hbase. This is to update that component to be compatible with HBase 2.0.
STORM-2945,Nail down and document how to support background emits in Spouts and Bolts,
STORM-2944,Eliminate these deprecated methods in Storm 3,"In Storm 2 we deprecated these methods. These methods were retained to allow storm 2.x nimbus & supervisor to manage older Storm 1.x workers.  

These are the methods in IStormClusterState.java 

{code}
/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */

@Deprecated
 boolean topologyBackpressure(String stormId, long timeoutMs, Runnable callback);

/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */
 @Deprecated
 void setupBackpressure(String stormId);

/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */
 @Deprecated
 void removeBackpressure(String stormId);

/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */
 @Deprecated
 void removeWorkerBackpressure(String stormId, String node, Long port);
{code}"
STORM-2938,Share CuratorFramework instance in a process (daemons/worker),"According to Curator web site, each process (nimbus, supervisor, worker, etc.) can share singleton curator framework instance per ZK cluster since it's thread-safe.

[http://curator.apache.org/curator-framework/]
{quote}CuratorFrameworks are allocated using the CuratorFrameworkFactory which provides both factory methods and a builder for creating instances. IMPORTANT: CuratorFramework instances are fully thread-safe. You should share one CuratorFramework per ZooKeeper cluster in your application.
{quote}
We are using Curator 4.0 both master and 1.x branch, hence we are good to apply this.

We already applied this for Nimbus via STORM-2901. This issue is to track the efforts for other processes."
STORM-2931,defaultSchedule function of DefaultScheduler looks not right,"I am not sure if this is a bug but the function logic looks not right to me:

DefaultScheduler.java: defaultSchedule()

 
{code:java}
public static void defaultSchedule(Topologies topologies, Cluster cluster) {
    for (TopologyDetails topology : cluster.needsSchedulingTopologies()) {
        List<WorkerSlot> availableSlots = cluster.getAvailableSlots();
        Set<ExecutorDetails> allExecutors = topology.getExecutors();

        Map<WorkerSlot, List<ExecutorDetails>> aliveAssigned = EvenScheduler.getAliveAssignedWorkerSlotExecutors(cluster, topology.getId());
        Set<ExecutorDetails> aliveExecutors = new HashSet<ExecutorDetails>();
        for (List<ExecutorDetails> list : aliveAssigned.values()) {
            aliveExecutors.addAll(list);
        }

        Set<WorkerSlot> canReassignSlots = slotsCanReassign(cluster, aliveAssigned.keySet());
        int totalSlotsToUse = Math.min(topology.getNumWorkers(), canReassignSlots.size() + availableSlots.size());

        Set<WorkerSlot> badSlots = null;
        if (totalSlotsToUse > aliveAssigned.size() || !allExecutors.equals(aliveExecutors)) {
            badSlots = badSlots(aliveAssigned, allExecutors.size(), totalSlotsToUse);                
        }
        if (badSlots != null) {
            cluster.freeSlots(badSlots);
        }

        EvenScheduler.scheduleTopologiesEvenly(new Topologies(topology), cluster);
    }
}{code}
 

EvenScheduler.scheduleTopologiesEvenly:

 
{code:java}
public static void scheduleTopologiesEvenly(Topologies topologies, Cluster cluster) {
    for (TopologyDetails topology : cluster.needsSchedulingTopologies()) {
        String topologyId = topology.getId();
        Map<ExecutorDetails, WorkerSlot> newAssignment = scheduleTopology(topology, cluster);
        Map<WorkerSlot, List<ExecutorDetails>> nodePortToExecutors = Utils.reverseMap(newAssignment);

        for (Map.Entry<WorkerSlot, List<ExecutorDetails>> entry : nodePortToExecutors.entrySet()) {
            WorkerSlot nodePort = entry.getKey();
            List<ExecutorDetails> executors = entry.getValue();
            cluster.assign(nodePort, topologyId, executors);
        }
    }
}
{code}
 

In the for-loop of DefaultScheduler.defaultSchedule(), it calls EvenScheduler.scheduleTopologiesEvenly()  every time. But  there is another for-loop in EvenScheduler.scheduleTopologiesEvenly(). These two for-loops both iterate on 
{code:java}
cluster.needsSchedulingTopologies() {code}
and never uses the first parameter which is 
{code:java}
Topologies topologies
{code}
 "
STORM-2930,BlobStoreTest and HdfsBlobStoreImplTest don't run ,"BlobStoreTest and HdfsBlobStoreImplTest don't run for a very long time.

Recently we separate storm-hdfs-blobstore from storm-hdfs and the two tests doesn't run:

[https://travis-ci.org/apache/storm/jobs/336285239] (about 19 hours ago)

 

Then I checked with the builds before that. These two unit tests were not running:

[https://travis-ci.org/apache/storm/jobs/328377991] (20 days ago)

[https://travis-ci.org/apache/storm/jobs/311718625] (2 months ago)

 

Go back to 5 months ago:

[https://travis-ci.org/apache/storm/jobs/274213948]

These two unit tests were running. 

 

 

 "
STORM-2929,fix storm-hdfs docs if any because of STORM-2916,STORM-2916 made a change to separate storm-hdfs-blobstore from storm-hdfs module. We need to fix related documents if any
STORM-2928,(For 2.0.0) In a secure cluster with storm-autocreds enabled storm-druid can fail with NoSuchMethodError,"storm-autocreds brings in the curator 4.0 via transitive dependency of storm-core, even though storm-core is listed as provided scope, the app assembler plugin puts the dependency (curator 4.0) into external/storm-autocreds directory. This conflicts with the storm-druid tranquility library which depends on curator 2.6.0
2018-01-22 08:43:54.047 o.a.s.d.executor Thread-15-54-Dashboard-Violation-Predicted-executor[20 20] [ERROR]
java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$com$metamx$tranquility$beam$ClusteredBeam$$zpathWithDefault$1.apply(ClusteredBeam.scala:125)"
STORM-2926,org.apache.storm.st.tests.window.SlidingWindowTest.testWindowCount in integration test fails intermittently,"Lost a build link... From what I've seen is, one of 13th trials in test had failed. So not often, but really intermittent."
STORM-2925,org.apache.storm.sql.TestStormSql consistently fails,"1. java.lang.RuntimeException: java.lang.IllegalStateException: It took over 60000ms to shut down slot Thread[SLOT_1024,5,main]

[https://travis-ci.org/apache/storm/jobs/335344937]

 

2. [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project storm-sql-core: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?

[https://travis-ci.org/apache/storm/jobs/334308676]

 

I've skimmed a bit and looks like twos are same issue."
STORM-2923,org.apache.storm.st.tests.window.SlidingWindowTest in integration test fails intermittently with 1.x version lines,"Unfortunately lost the build link... The following message was presented:

java.lang.RuntimeException: Failed to kill topology SlidingWindowTest-window200-slide100. Subsequent tests may fail because worker slots are occupied"
STORM-2922,org.apache.storm.hdfs.spout.TestDirLock is consistently stuck with JDK 7 in Travis CI,"[https://travis-ci.org/apache/storm/jobs/333477869]

 "
STORM-2921,integration.org.apache.storm.testing4j-test fails intermittently on 1.x version line,[https://travis-ci.org/apache/storm/jobs/332445614]
STORM-2920,org.apache.storm.trident.tuple-test fails intermittently on 1.x version line,[https://travis-ci.org/apache/storm/jobs/332147829]
STORM-2919,integration.org.apache.storm.integration-test fails intermittently on 1.x version line,"testname ""test-submit-inactive-topology"" failed occasionally: I've seen various kinds of failures, refer the links to see detail:

[https://travis-ci.org/apache/storm/jobs/333477868]

[https://travis-ci.org/apache/storm/jobs/333477867]"
STORM-2915,How could I to get the fail Number   in Bolt When I use  Kafka Spout,"I want to get fail num in bolt , how could  I  to get it? 

if  fail it  retry, I see This 

if (!isScheduled || retryService.isReady(msgId)) {
 final String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;

 if (!isAtLeastOnceProcessing()) {
 if (kafkaSpoutConfig.isTupleTrackingEnforced()) {
 collector.emit(stream, tuple, msgId);
 LOG.trace(""Emitted tuple [{}] for record [{}] with msgId [{}]"", tuple, record, msgId);
 } else {
 collector.emit(stream, tuple);
 LOG.trace(""Emitted tuple [{}] for record [{}]"", tuple, record);
 }
 } else {
 emitted.add(msgId);
 offsetManagers.get(tp).addToEmitMsgs(msgId.offset());
 if (isScheduled) { // Was scheduled for retry and re-emitted, so remove from schedule.
 retryService.remove(msgId);
 }
 collector.emit(stream, tuple, msgId);
 tupleListener.onEmit(tuple, msgId);
 LOG.trace(""Emitted tuple [{}] for record [{}] with msgId [{}]"", tuple, record, msgId);
 }
 return true;
}"
STORM-2905,Supervisor still downloads storm blob files when the topology was killed.,"When we kill a topology, at the moment of topology blob-files be removed, Supervisor executor still request blob-files and get an KeyNotFoundException.

I stepped in and found the reason:
1. We do not add a guarded lock on `topologyBlobs` of AsyncLocalizer which is singleton to a supervisor node.
2. And we remove jar/code/conf blob keys in `topologyBlobs` of killed storm only in a timer task: cleanUp() method of AsyncLocalizer, the remove condition is :[no one reference the blobs] AND [ blobs removed by master OR exceeds the max configured size ], the default scheduling interval is 30 seconds.
3. When we kill a storm on a node[ which means that the slot container are empty], the AsyncLocalizer will do: releaseSlotFor, which only remove reference on the blobs [topologyBlobs keys are still there.]
4. Then the container is empty, and Slot.java will do: cleanupCurrentContainer, which will invoke AsyncLocalizer #releaseSlotFor to release the slot.
5. AsyncLocalizer have a timer task: updateBlobs to update base/user blobs every 30 seconds, which based on the AsyncLocalizer#`topologyBlobs`
6. We know that AsyncLocalizer#`topologyBlobs` overdue keys are only removed by its AsyncLocalizer#cleanUp which is also a timer task.
7. So when we kill a storm, AsyncLocalizer#updateBlobs will update based on a removed jar/code/conf blob-key and fire a exception, then retried until the configured max times to end.

Here is how i fixed it:
1. just remove the base blob keys eagerly when we do AsyncLocalizer #releaseSlotFor when there is no reference [no one used] on the blobs, and remove the overdue keys in AsyncLocalizer#`topologyBlobs`
2. Guard the AsyncLocalizer#updateBlobs and AsyncLocalizer #releaseSlotFor on the same lock.
3. When container is empty, we do not need to exec AsyncLocalizer #releaseSlotFor[because we have already deleted them].
4. I also add a new RPC api for decide if there exists a remote blob, we can use it to decide it the blob could be removed instead of use getBlobMeta and catch an confusing KeyNotFoundException [both on supervisors and master log for every base blobs].

This is the partial of Supervisor log:

2018-03-31 13:41:17.089 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/pids/1115
 2018-03-31 13:41:17.090 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/heartbeats
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/pids
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/tmp
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.103 o.a.s.d.s.Container SLOT_6700 [INFO] REMOVE worker-user b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.103 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers-users/b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.104 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Removed Worker ID b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.105 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE KILL msInState: 25 topo:word_count_fk_11-2-1522472558 worker:null -&gt; EMPTY msInState: 0
 2018-03-31 13:41:17.105 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Changing current assignment from LocalAssignment(topology_id:word_count_fk_11-2-1522472558, executors:[ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:6, task_end:6), ExecutorInfo(task_start:5, task_end:5), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:3, task_end:3), ExecutorInfo(task_start:2, task_end:2), ExecutorInfo(task_start:1, task_end:1)], resources:WorkerResources(mem_on_heap:896.0, mem_off_heap:0.0, cpu:70.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=896.0, cpu.pcore.percent=70.0}, shared_resources:{}), owner:danny0405) to null
 2018-03-31 13:41:18.001 o.a.s.d.s.t.SupervisorHealthCheck timer [INFO] Running supervisor healthchecks...
 2018-03-31 13:41:18.002 o.a.s.h.HealthChecker timer [INFO] The supervisor healthchecks succeeded.
 2018-03-31 13:41:36.837 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 1 [WARN] Failed to download blob LOCAL TOPO BLOB TOPO_JAR word_count_fk_11-2-1522472558 will try again in 100 ms
 org.apache.storm.generated.KeyNotFoundException: null
         at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25225) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25193) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25124) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[libthrift-0.9.3.jar:0.9.3]
         at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:825) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:812) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:318) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:176) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$5(AsyncLocalizer.java:249) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) [?:1.8.0_151]
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_151]
         at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_151]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_151]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_151]
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_151]
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_151]
         at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]"
STORM-2895,Add CPU usage to database,
STORM-2890,Update the UI to use the new metrics query API,"Once we have a query API and at least the same metric that go into ZK are going into the new metrics engine we should update the UI to display the metrics from the new query API where possible, and fall back on ZK when they are not available.

We want to get rid of the metrics in the heartbeats at some point soon, but to be able to support older topologies running under the newer versions of storm we will keep this code around until a 3.x build."
STORM-2889,Add Metrics Query API (Thrift and REST),"Once we have a storage engine and even if it is just a few metrics we want to add in an API to query the metrics.

It should be able to do simple aggregations (Min, Max, Mean, and Sum) across different time ranges, with different roll up windows.  It should also be able to filter based off of things like topology id, component name, component id, stream name, and host name.  It is fine to have some of these required, such as topology id, at the beginning.

It should be able to return results either all together or split up by the different topology ids, component names, component ids, stream names, or host names.

For Example the UI shows things like

SUM of {tuples_emitted, tuples_transferred, acked, failed}  in the last {10 mins, 3 hours, 1 day, all time} for all spouts a.k.a a set of component names that we got from the topology itself.
AVERAGE of {complete_latency} in the last {10 mins, 3 hours, 1 day, all time} for a set of component names.

SUM of {tuples_emitted, tuples_transferred, acked, failed}  in the last {10 mins, 3 hours, 1 day, all time} for all components separated by each component name.
AVERAGE of {complete_latency} in the last {10 mins, 3 hours, 1 day, all time} for all components separated by each component name.

And there are others too.



The API should be open to expansion for other types of filters, aggregates, and even data types.  The following are not things that should be implemented, but are things that we should think about how we can leave the API open for expansion alter on so we could support them in a clean way.

Specifically some filters I would like to see in the future include topology name or even better a basic pattern we can use to match topology ids and the owner of a topology.

I would also like to eventually support percentile sketches as a data type so instead of just having an average of the 99th%ile collected by each component, we get an much more accurate approximation of what the 99th%ile really is in aggregate.
"
STORM-2888,Integrate storage with metrics V2.,Once STORM-2887 and STORM-2153 are done and in this is to add in the glue code that allows the supervisor to forward all of the metrics to the storage engine.
STORM-2886,Look into better phemeral port support in LocalCluster.withNimbusDaemon,"We have a few tests that use a LocalCluster and then launch an actual Nimbus Thrift Server instance.  This is mostly to test that the thrift code works properly.

In some cases this can cause issues because by default it is going to get a single hard coded port.

In most cases we work around this, like in STORM-2885 by finding a free port and then using that for both client and server configs.

This does not eliminate the race, but it makes it very unlikely.  It would be great if we could eliminate the race by somehow bringing up the thrift server on an ephemeral port, and then providing a way to get that port through an API in LocalCluster.

One of the issues we would have to overcome is that we explicitly check for positive port numbers, and we would only want to turn that off for this particular use case."
STORM-2884,Storm-druid topologies fail with NoSuchMethodError (Storm 2.0),"Deploy a sample topology with storm-druid (E.g. the SampleDruidBoltTopology available in the storm git repo). The worker crashes with below error:

{noformat}

2017-12-28 15:47:36.382 o.a.s.util Thread-11-113-Violation-Events-Cube-executor9 9 ERROR Async loop died!
java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;
at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$com$metamx$tranquility$beam$ClusteredBeam$$zpathWithDefault$1.apply(ClusteredBeam.scala:125) ~dep-org.apache.storm-storm-druid-jar-1.2.0.3.1.0.0-420.jar.1514384427000:1.2.0.3.1.0.0-420
at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$com$metamx$tranquility$beam$ClusteredBeam$$zpathWithDefault$1.apply(ClusteredBeam.scala:122) ~dep-org.apache.storm-storm-druid-jar-1.2.0.3.1.0.0-420.jar.1514384427000:1.2.0.3.1.0.0-420
at com.metamx.common.scala.Predef$EffectOps.withEffect(Predef.scala:44) ~dep-org.apache.storm-storm-druid-jar-1.2.0.3.1.0.0-420.jar.1514384427000:1.2.0.3.1.0.0-420
at 
{noformat}

storm-druid has dependency on curator 2.6.0, but the storm parent pom has defined 4.0.0 version in the dependency Management.

Due to this storm-druid is inheriting the 4.0.0 version of curator and including that version in the jar.

If we explicitly mention the curator dependency in the storm-druid pom.xml, this can be addressed."
STORM-2880,Minor optimisation about kafka spout,"Based on the single responsibility principle, method isAtLeastOnceProcessing() should reside in KafkaSpoutConfig rather than KafkaSpout. This patch removes the dependency of KafkaSpoutConfig.ProcessingGuarantee from KafkaSpout."
STORM-2878,Supervisor collapse continuously when there is a expired assignment for overdue storm,"For now, when a topology is reassigned or killed for a cluster, supervisor will delete 4 files for an overdue storm:
- storm-code
- storm-ser
- storm-jar
- LocalAssignment

Slot.java
static DynamicState cleanupCurrentContainer(DynamicState dynamicState, StaticState staticState, MachineState nextState) throws Exception {
        assert(dynamicState.container != null);
        assert(dynamicState.currentAssignment != null);
        assert(dynamicState.container.areAllProcessesDead());
        
        dynamicState.container.cleanUp();
        staticState.localizer.releaseSlotFor(dynamicState.currentAssignment, staticState.port);
        DynamicState ret = dynamicState.withCurrentAssignment(null, null);
        if (nextState != null) {
            ret = ret.withState(nextState);
        }
        return ret;
    }

But we do not make a transaction to do this, if an exception occurred during deleting storm-code/ser/jar, an overdue local assignment will be left on disk.

Then when supervisor restart from the exception above, the slots will be initial and container will be recovered from LocalAssignments, the blob store will fetch the files from Nimbus/Master, but will get a KeyNotFoundException, and supervisor collapses again.

This will happens continuously and supervisor will never recover until we clean up all the local assignments manually.

This is the stack:
2017-12-27 14:15:04.434 o.a.s.l.AsyncLocalizer [INFO] Cleaning up unused topologies in /opt/meituan/storm/data/supervisor/stormdist
2017-12-27 14:15:04.434 o.a.s.d.s.AdvancedFSOps [INFO] Deleting path /opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785
2017-12-27 14:15:04.445 o.a.s.d.s.Slot [INFO] STATE EMPTY msInState: 109 -> WAITING_FOR_BASIC_LOCALIZATION msInState: 1
2017-12-27 14:15:04.471 o.a.s.d.s.Supervisor [INFO] Starting supervisor with id 255d3fed-f3ee-4c7e-8a08-b693c9a6a072 at host gq-data-rt48.gq.sankuai.com.
2017-12-27 14:15:04.502 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.611 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.718 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.825 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.932 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.039 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.140 o.a.s.u.Utils [INFO] Could not extract resources from /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar
2017-12-27 14:15:05.142 o.a.s.d.s.Slot [INFO] STATE WAITING_FOR_BASIC_LOCALIZATION msInState: 697 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2017-12-27 14:15:05.142 o.a.s.l.AsyncLocalizer [WARN] Caught Exception While Downloading (rethrowing)... 
java.io.FileNotFoundException: File '/opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785/stormconf.ser' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:264) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:376) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:370) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:226) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:213) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]"
STORM-2875,Hadoop-auth dependency in storm-core results in exception,"Storm-core 1.1.1 ships with hadoop-auth dependency for version 2.6.1 which results in exceptions while working with hadoop version 2.8.x and above due to changes in KerberosUtil class.


_java.lang.NoSuchMethodError: org.apache.hadoop.security.authentication.util.KerberosUtil.hasKerberosKeyTab(Ljavax/security/auth/Subject;)Z
	at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:715) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:925) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:873) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:740) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3472) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3462) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3304) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476) ~[stormjar.jar:?]
	at org.apache.storm.hdfs.bolt.HdfsBolt.doPrepare(HdfsBolt.java:106) ~[stormjar.jar:?]
	at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.prepare(AbstractHdfsBolt.java:124) ~[stormjar.jar:?]
	at org.apache.storm.daemon.executor$fn__5030$fn__5043.invoke(executor.clj:793) ~[storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.1.1.jar:1.1.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]_"
STORM-2871,Performance optimizations for getOutgoingTasks ,"Task.getOutgoingTasks() is in critical messaging path. Two observed bottlenecks in it :

- Looking up HashMap 'streamToGroupers'. Need to look into converting HashMap into Array lookup ?
- [outTasks.addAll(compTasks)|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/Task.java#L139]  seems to be impacting throughput as well. Identified by .. running ConstSpoutNullBoltTopo with 1 spout & bolt paralllelism (no Ack) and replacing this line with hard coded logic to add the single known bolt's taskID. "
STORM-2866,ImportError: No module named shlex,"Hello Team,

Am trying to get Stormcrawler working on my Debian VM. I get this error when I do the following command:

root@demo76:/opt# storm

Traceback (most recent call last):

 File ""/opt/storm/apache-storm-1.1.0/bin/storm.py"", line 23, in <module>

 import shlex

ImportError: No module named shlex

 I have python 2.7.9 version. I tried a simple python program with shlex and it works fine but am not sure why the storm is not recognizing shlex. Can you please help?

Kind Regards,
Sai"
STORM-2865,KafkaSpout constructor with KafkaConsumerFactory shoud be public,"When we use custom implement of interface ""Deserializer""，KafkaSpout constructor can only use class ""KafkaConsumerFactoryDefault""，the method “configure” of Interface “Deserializer” will not be called。

We need change the ""KafkaSpout"" constructor with ""KafkaConsumerFactory"" to be public, so we can create custom custom implement of interface ""KafkaConsumerFactory""."
STORM-2861,Explicit reference kafka-schema-registry-client,"storm-hdfs compile failure due to io.confluent.kafka.schemaregistry.client.* not found.
Changing the dependence from kafka-avro-serializer to kafka-schema-registry-client fixed this issue."
STORM-2852,Dup log4j jars in storm-local,"After upgrading to 1.0.2, every time there was ""Could not initialize class org.apache.log4j.Log4jLoggerFactory"" error when deploying a topology, but nothing suspicious found against the dependency tree.

Turned out there are dup log4j implementation jars under storm lib dir
log4j-over-slf4j-1.6.6.jar
log4j-slf4j-impl-2.1.jar

After removing log4j-over-slf4j-1.6.6.jar from lib, the whole world is running well.

Could you someone help fix? It's quite tricky to figure out the problem"
STORM-2849,[Flux] Support reflist for properties,"We're providing a means to invoke static factory methods for flux components via STORM-2796.
This is follow-up issue to also provide a means to support reflist for properties.

{code}
    properties:
      - name: ""timeLen""
        args:
          - ref: ""time1""
      - name: ""timeLenArr""
        args:
          - refList: [""time1"", ""time2""]
{code}

Note that we also have simple workaround here, so I'm just going to set priority to be 'minor'.

{code}
    configMethods:
      - name: ""setTimeLenArr""
        args:
          - reflist: [""time1"", ""time2""]
{code}"
STORM-2848,[storm-sql] Separate the concept of STREAM and TABLE,"Currently we only support STREAM type of table, and don't provide type selection for users.

We have future plan of supporting join on stream and table, which requires tables to be defined either stream or table, because left join which left side is table and right side is stream doesn't make sense and vice versa.

This issue tracks the effort of separating STREAM and TABLE. We may want to also apply the constraint that only STREAM supports SELECT statement."
STORM-2846,Add extra classpath for nimbus and supervisor,"Currently we have STORM_EXT_CLASSPATH_DAEMON and STORM_EXT_CLASSPATH for extra classpath. We might want to have extra classpath for nimbus and supervisor specifically. One of the issue I am facing with is about setting up HdfsBlobstore. I point STORM_EXT_CLASSPATH_DAEMON to the classpath of existing hadoop cluster. But because the hadoop classpath includes jersey-core-1.9, storm logviewer and drpc server failed to run (since storm uses jersey-2.x).


{code:java}
java.lang.Error: java.lang.NoSuchMethodError: javax.ws.rs.core.Application.getProperties()Ljava/util/Map;
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:568) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:547) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils$5.uncaughtException(Utils.java:877) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) ~[?:1.8.0_131]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) ~[?:1.8.0_131]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_131]
Caused by: java.lang.NoSuchMethodError: javax.ws.rs.core.Application.getProperties()Ljava/util/Map;
        at org.glassfish.jersey.server.ApplicationHandler.<init>(ApplicationHandler.java:331) ~[jersey-server-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.WebComponent.<init>(WebComponent.java:392) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:177) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:369) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at javax.servlet.GenericServlet.init(GenericServlet.java:244) ~[javax.servlet-api-3.1.0.jar:3.1.0]
        at org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:637) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:421) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:760) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:348) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:785) ~[jetty-server-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:261) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) ~[jetty-util-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) ~[jetty-util-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.server.Server.start(Server.java:449) ~[jetty-server-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105) ~[jetty-util-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:113) ~[jetty-server-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.server.Server.doStart(Server.java:416) ~[jetty-server-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) ~[jetty-util-9.4.7.v20170914.jar:9.4.7.v20170914]
        at org.apache.storm.daemon.logviewer.LogviewerServer.start(LogviewerServer.java:129) ~[storm-webapp-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.logviewer.LogviewerServer.main(LogviewerServer.java:170) ~[storm-webapp-2.0.0.y.jar:2.0.0.y]
{code}

Adding the jvm option -verbose:class, we have the following message:

{code:java}
[Loaded javax.ws.rs.core.Application from file:/home/gs/gridre/yroot.openqe74blue/share/hadoop-2.8.1.7.1709112043/share/hadoop/common/lib/jersey-core-1.9.jar]
{code}

It means that the javax.ws.rs.core.Application is loaded from jersey-core-1.9. "
STORM-2839,Need ability to create Kryo serializer equivalent to Storm's,"What I _really_ want is to construct a Kryo serializer to serialize/deserialize a blob that's used down deep in a library. Said serializer doesn't have to be identical to Storm's, but I want it to have all of the same custom serializations registered that Storm uses.

Ideally what I'd like is a public API to return a Kryo (de)serializer with the same configuration as Storm's.

In practice, I _could_ configure a serializer using information in the Map passed to components' open/prepare, but I need that information down in the innards of the library. I can't depend on the caller pass down that down (since this is an internal detail of the library), and I'm not convinced that the library won't be invoked by Storm before components are even activated."
STORM-2838,Replace log4j-over-slf4j with log4j-1.2-api,"I tried to setup HdfsBlobStore and an exception shows up when I launch the nimbus.

{code:java}
Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError.
{code}
Found an explanation: https://www.slf4j.org/codes.html#log4jDelegationLoop

This is because storm and hadoop use different logging system:

{code:java}
Storm:  log4j-over-slf4j --> slf4j --> log4j2  or slf4j --> log4j2
Hadoop:  slf4j --> log4j1.2  or   log4j1.2 
(note: --> means redirecting)
{code}


When we add hadoop common lib classpath to nimbus,  log4j-over-slf4j.jar and slf4j-log4j12.jar coexist. 
One way to let storm work with hadoop is to replace log4j-over-slf4j in storm with log4j1.2. "
STORM-2836,Backpressure can cause OOM if delete zk node manually,"The problem is more easily reproduced with sleep topology, the bolt just sleep, not do anything.

I have test storm 1.0.2 ABP in our cluster, it soon reach highWaterMark, and slow down the tuple sending speed of the spouts.

If i delete the zookeeper flag node manually, the spouts will continue to consume normally, but it cause OOM. That's not what we want to see.

In my  opinion, it should soon reach highWaterMark again, and it would creat a new znode to slow down the tuple sending speed of the spouts. But it didn't work.

I suspect it was cause ""update the worker's backpressure flag to zookeeper only when it has changed"" in worker.clj, if i transfer ""when (not= prev-backpressure-flag curr-backpressure-flag)"" to ""when curr-backpressure-flag"" in the mk-backpressure-handler method of file worker.clj, it works. which may put more load on zookeeper, I will seek a better solution."
STORM-2831,Doc links show Kafka images instead of storm on the top of doc pages.,Doc links show kafka image instead of storm on top section of the [page |http://storm.apache.org/releases/current/Metrics.html]. Attached one of the page screenshots.
STORM-2828,DaemonConfig.UI_HEADER_BUFFER_BYTES is read by Logviewer but not used,"The DaemonConfig.UI_HEADER_BUFFER_BYTES read in the Logviewer code, but not used. It seems like it might originally have been a copy-paste error from the Storm UI code, because the Clojure code had the same issue.

The variable at https://github.com/apache/storm/blob/61a944fd15e564482a5c55655daad9736948d961/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java#L66 is not used. I'm not sure if the right thing to do is remove it, or pass it to the Jetty server further down the method."
STORM-2824,Ability to configure topologies for exactly once processing,"The default implementation of a spout  (Kafka) is to wait for acknowledgement, if an acknowledgement is not provided the tuple is replayed leading to an at least once processing model.

Can an option be provided to always acknowledge even in the event of error in any spout or bolt and the user decide which mode the topology should be configured.

There are cases like multiple bolts (B) inserting to persistent stores (PS) like B1 - PS1, B2-PS2, B3-PS3, the fact that B2-PS2 bolt fail doesn't mean that the tuple needs to be replayed leading to complexity on the logic of bolts, it would be easier if this was configurable and the user of the topology decides which style to choose.
"
STORM-2823,Ability to have an option to combine topologies at run time in a single process space,"Unlike an API server which service multiple APIs within the same process space, the Topologies needs to run in separate processes.
Lets say we have Topology TP-1 which use 1 GB of memory.
Now we create the same for n Topologies of TP-1..... TP-n
As the topologies increase the memory allocation is now multiplied by the number of topologies.
This design though scalable is not similar to the API route we have before which was within the same process space.

So in a micros services world, each topology would be responsible for a similar set of objects, like employee, customer, product, order, order details etc.

As the number of topologies increase the worker allocation is not sufficient. Most topologies are not utilized fully but since these are in different process space the memory allocated can't be used.

If we have an ability to say that TP-1 --- TP 10 Can run within the same process space but behave like individual topologies we could conserve the resource usage.

Now user are forced to combine topologies to the hardware provided with ""if"" logics to route the correct object that needs to be processed.

This way one can still configure topologies as API in the same API server and reuse resources collectively for related group of topologies acting as micro services.




"
STORM-2822,Remove LinearDRPCTopologyBuilder,We should look into removing the deprecated LinearDRPCTopologyBuilder ibn 2.0.0.  But we need to make sure first that all of the use cases for it are clearly documented and covered by the other ways of using DRPC.
STORM-2821,Remove TransactionalTopologySupport.,"Transactional Topologies have been in storm for a long time, but Trident overrides it and the former is deprecated.  We should remove all of the code that supports it and from the code base."
STORM-2819,Ability to natively support JSON serialization in topologies,Now that the world is moving towards NoSQL and most of the data is in JSON. Can a native JSON Serializer be implemented similar to support for Kryo. 
STORM-2818,Storm UI doesn't show which version of the code was used to run the topology,"Lets say we create a Topology 1 with version 1 of the uber Jar namely Topololog1V1.jar
We submit this jar and the Topology1 is shown in Storm UI

We then do changes and now have another version of the Topology 1 which is Topology1V2.jar 
We submit this jar and the Topology1 is again shown in the Storm UI

In both case we can't find which version of the code are we running without actually logging in to the Storm Supervisor instance and find the process start parameters.

Can this start parameters be made available in the UI so that we can easily find which version of the code are we running for the topology?
"
STORM-2817,Topology Restart Counts are not maintained in Storm UI,"On the Storm UI, we need an ability to have a Topology Submission Time, Topology Uptime as well as how many times a Topology worker process has restarted since last Submission.

The reason been, lets say we have a Supervisor with 8 GB RAM.
We also have 4 Slots on this Supervisor.
We submit 4 Topologies each with worker memory of 3 GB leading to a total of 12GB / 8 GB utilization assuming not all topologies would use up all the memory at the same time.

Now, we find that topologies are dying behind the scenes due to out of memory and Storm Nimbus keeps restarting these topologies again.

The uptime requests as part of  [STORM-2816] (https://issues.apache.org/jira/browse/STORM-2816) we can address the uptime but it still won't say we have a deeper issue and the topologies are restarting behind the scene. Adding this counter would help to flag issues.

The counts should be at both per topology level like

Topology 1 
     Submission Time T1
     Uptime T2
     Restarts 4 (Possible log links to why restarted)

The other should be at the Storm UI level

Total Topologies : 20
Total Topologies Restart since Submission : 12 (Possible links to topologies that got restarted)

This way monitoring and alerting systems can hook into these counts and alert when things go wrong.
"
STORM-2816,Topology Summary Uptime is not reflecting worker restarts,"The Storm UI , Topology Summary Uptime is not reflecting the Worker Process restarts and always gives the initial topology submission uptime.

So if we submitted the Topology1 at time T1
The topology was running in 1 worker instance.
If the worker instance goes down at time T2 and a new worker instance was started at time T3.
The uptime always show time T1 and not starting with time T3

We might need to split the Topology Submission Time and Topology Uptime

"
STORM-2812,Localizer and Container tests are broken if running in Windows,
STORM-2802,Storm-cassandra tests don't run on JDK 9,"The storm-cassandra tests don't run on JDK 9. 

{quote}
opaqueStateTest(org.apache.storm.cassandra.trident.MapStateTest)  Time elapsed: 0.627 sec  <<< ERROR!
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [localhost/127.0.0.1:9042] Error writing))
        at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:233)
        at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:79)
        at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1473)
        at com.datastax.driver.core.Cluster.init(Cluster.java:159)
        at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:330)
        at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:305)
        at com.datastax.driver.core.Cluster.connect(Cluster.java:247)
        at org.apache.storm.cassandra.trident.MapStateTest.setUp(MapStateTest.java:163)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:564)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)

{quote}

Java 9 support for Cassandra is tracked at https://issues.apache.org/jira/browse/CASSANDRA-9608"
STORM-2801,Storm-Hive tests don't run on JDK 9,"The Storm-Hive tests error out when running on JDK 9. 

{quote}
java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:444)
	at org.apache.storm.hive.bolt.TestHiveBolt.<init>(TestHiveBolt.java:110)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1449)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2661)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2680)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	... 34 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	... 39 more
Caused by: javax.jdo.JDOFatalInternalException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
NestedThrowables:
org.datanucleus.exceptions.NucleusException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:591)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy27.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	... 44 more
Caused by: org.datanucleus.exceptions.NucleusException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
	at org.datanucleus.store.rdbms.mapping.RDBMSMappingManager.getDatastoreMappingClass(RDBMSMappingManager.java:1215)
	at org.datanucleus.store.rdbms.mapping.RDBMSMappingManager.createDatastoreMapping(RDBMSMappingManager.java:1378)
	at org.datanucleus.store.rdbms.table.AbstractClassTable.addDatastoreId(AbstractClassTable.java:392)
	at org.datanucleus.store.rdbms.table.ClassTable.initializePK(ClassTable.java:1087)
	at org.datanucleus.store.rdbms.table.ClassTable.preInitialize(ClassTable.java:247)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTable(RDBMSStoreManager.java:3118)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTables(RDBMSStoreManager.java:2909)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3182)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045)
	at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365)
	at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827)
	at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571)
	at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513)
	at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232)
	at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	... 62 more
{quote}

Someone asked about this error on Stack Overflow (https://stackoverflow.com/questions/43086904/error-with-nucleusexception-and-jdofatalexception-when-starting-hive), and it sounds like we'll need to wait for Hive to release a JDK 9 compatible version (https://issues.apache.org/jira/browse/HIVE-17632)"
STORM-2798,Build Storm with JDK 11,"Track what we need to do to make Storm build on Java 11 (i.e. fix issues introduced in Java 9, 10 and 11)."
STORM-2797,LogViewer worker logs broken on Windows,"LogViewer worker logs are broken on Windows. Attempting to access the log (e.g. http://localhost:8000/log?file=word-topo-5-1509750559%5C6701%5Cworker.log) leads to a 500 Server Error.

I've attached the LogViewer logs which show the stack trace. The issue is pretty clear from the log: on line 123 of logviewer.clj, the path is split using the path separator as a regex. This is fine on Posix systems as / is a normal character in regex; however, on Windows, backslash is the path separator. As this is also the regex escape character, it is not a valid regular expression."
STORM-2789,When scan storm with Nessus there  are some security vulnerabilities ,"When scan storm with Nessus there  are some security vulnerabilities .For example,Generic Overflow Detection,you can check the details form the picture.Would storm community fix this bug?"
STORM-2788,supervisor.worker.version.classpath.map should support regex,"To enable 0.10 (or other version) topology to run on 2.x cluster, we need to set supervisor.worker.version.classpath.map and something else. But now the classpath.map doesn't support regex yet. We have to list all the jar file path.

{code:java}
supervisor.worker.version.classpath.map:
    0.10.2.y: ""/home/y/lib64/ystorm_compatibility/current/lib/asm-4.0.jar:/home/y/lib64/ystorm_compatibility/current/lib/auth_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/bcpkix.jar:/home/y/lib64/ystorm_compatibility/current/lib/bcprov.jar:/home/y/lib64/ystorm_compatibility/current/lib/bouncer_auth_java.jar:/home/y/lib64/ystorm_compatibility/current/lib/clojure-1.6.0.jar:/home/y/lib64/ystorm_compatibility/current/lib/data_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/disruptor-3.3.2.jar:/home/y/lib64/ystorm_compatibility/current/lib/junixsocket.jar:/home/y/lib64/ystorm_compatibility/current/lib/kryo-2.21.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-1.2-api-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-api-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-core-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-slf4j-impl-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/servlet-api-2.5.jar:/home/y/lib64/ystorm_compatibility/current/lib/sia_java_client.jar:/home/y/lib64/ystorm_compatibility/current/lib/slf4j-api-1.7.7.jar:/home/y/lib64/ystorm_compatibility/current/lib/storm-core-0.10.2.y.jar:/home/y/lib64/ystorm_compatibility/current/lib/storm_yahoo-0.10.2.y.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_byauth.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_filter_logic.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_servlet.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_servlet_filters.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_yca.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_ysecure.jar:/home/y/lib64/ystorm_compatibility/current/lib/zts_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/zts_java_client.jar:""
{code}

We want to have the following configs working.

{code:java}
supervisor.worker.version.classpath.map:
    0.10.2.y: ""/home/y/lib64/ystorm_compatibility/current/lib/*""
{code}
"
STORM-2785,Researching Build Breakage,"Dear developers of the Apache Storm project,
We are MSc students doing research on why builds break in open source projects for the Delft University of Technology. To do this, we are analyzing Travis build statistics and build history of Apache Storm.

By analyzing the build history of Apache Storm, we found the following:
* The project has 70% failing builds in the history
* The project has a very high amount of build failures compared to other projects

When looking at Storm’s build metrics, we could not find clear factors that are highly correlated with this result.

To identify the reasons behind build breakage, we would like to collect developer insights into the reasons why build breakage occurs. If you have five minutes, please answer the [survey|https://www.surveymonkey.com/r/PQ7XTGM] we have created. Your response will be very useful in our study.

Thank you."
STORM-2783,"De-couple ""Spout Lag"" metrics on Storm UI from Kafka Spout and StormKafkaMonitor","As a developer of a spout, I'd love to be able to publish lag metrics to the storm UI.  After digging into the source code for how the UI interacts with storm-kafka-monitor to get these metrics, it appears to be strongly coupled.  I believe that the concept of ""Spout Lag"" extends beyond the scope of just consuming from Kafka. 

I'd like to propose the idea of restructuring how these metrics are queried by StormUI to a way that allows developers of other spouts to be able to ""plug into"" the UI.  The easiest way that springs to mind is to provide an interface that allows developers to code against.

"
STORM-2780,MetricsConsumer record unnecessary timestamp,"In a topology I could call conf.registerMetricsConsumer(LoggingMetricsConsumer.class,2);
to generate a file named worker.log.metrics with metrics data.
like  this:
2017-10-17 10:16:13,272 307174   1508206573           host1:6702     16:count       __fail-count            {}
2017-10-17 10:16:13,272 307174   1508206573           host1:6702     16:count       __emit-count            {default=41700}
2017-10-17 10:16:13,272 307174   1508206573           host1:6702     16:count       __execute-count         {split:default=41700}
2017-10-17 10:29:20,898 126900   1508207360           host1:6702     29:spout       __ack-count             {}
2017-10-17 10:29:20,906 126908   1508207360           host1:6702     29:spout       __sendqueue             {sojourn_time_ms=0.0, write_pos=299526, read_pos=299526, overflow=1, arrival_rate_secs=2643.9024390243903, capacity=1024, population=0}

But it records both date-and-timestamp(2017-10-17 10:29:20,906 126908) and only-timestamp(1508207360),I think the only-timestamp should be deleted because it's unnecessary."
STORM-2776, I'm facing issues with storm-starter samples,"Hi,


would you please help solving the following issues :


Issue#1:

I'm trying to run the storm-starter samples (Exclamation topology) but I'm getting the following Error: ""Could not find or load main class org.apache.storm.starter.ExclamationTopolog""


IDE: IntelliJ

SDK ver: 1.8

maven ver: 3.5

environment : Docker (please find attached ""docker-compose.yml"" and ""storm.yaml"" attached


Issue#2:

I tried to run ""Exclamation Topology"" from within IntelliJ, and although it found the nimbus server ""INFO  o.a.s.u.NimbusClient - Found leader nimbus : 427cb9b8470c:6627"", I'm getting the following error:

org.apache.storm.utils.NimbusLeaderNotFoundException: Could not find leader nimbus from seed hosts [localhost]


I've been struggling with those issues since 3 days and looking forward your help.


Regards,

Ayman"
STORM-2774,Workers get killed with FileNotFoundException on stormjar.jar,"Hi,

 Worker processes sometimes get killed unable to find stormjar.jar in tmp directory. The stacktrace looks as below. 

10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO] Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be c
reated
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308)
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO]       ... 85 more
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO] Caused by: java.util.ServiceConfigurationError: javax.xml.parsers.DocumentBuilderFactory: Error reading config
uration file
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.fail(ServiceLoader.java:232)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.parse(ServiceLoader.java:309)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.access$200(ServiceLoader.java:185)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:293)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.security.AccessController.doPrivileged(Native Method)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       ... 85 more
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO] Caused by: java.io.FileNotFoundException: /var/log/storm/tmp/supervisor/stormdist/R2Topology-80-1507783551/stormjar.jar (No such file or directory)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.open(Native Method)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.<init>(ZipFile.java:219)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.<init>(ZipFile.java:149)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.jar.JarFile.<init>(JarFile.java:166)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.jar.JarFile.<init>(JarFile.java:103)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:93)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at java.net.URL.openStream(URL.java:1045)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.parse(ServiceLoader.java:304)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       ... 92 more"
STORM-2773,"If a drpcserver node in cluster is down,drpc cluster won't work if we don't modify the drpc.server configuration and restart the cluster","There is a cluster which includes three nodes named storm1,storm2,storm3.And there is a drpcserver in every node,a worker which has been started on strom1.When strom1 was down with hardware failure,my drpc topology won't work,when I send request from drpcclient.
As storm1 was down,so the worker will be restarted on another node,but it can't Initialize successfully because the call method of Adder will throw a RuntimeException,when drpcspout try to connect to storm1,so the worker will restart again. 

In conclusion,If a drpcserver node in cluster is down,drpc cluster won't work until we modify the drpc.server configuration and restart the cluster,but in production,it's difficult to restart whole cluster.

So I think we should catch the RuntimeException and log it,and the drpc topology will work normally."
STORM-2768,Sorry I broke the build,"Not totally sure what happened, but when I try to build I am getting an error like

{code}
[ERROR]   The project org.apache.storm:blobstore-migrator:2.0.0-SNAPSHOT (/home/evans/src/storm/external/storm-blobstore-migration/pom.xml) has 1 error
[ERROR]     'dependencies.dependency.version' for org.apache.hadoop:hadoop-hdfs:jar must be a valid version but is '${hdfs.version}'. @ line 64, column 22
{code}

I will fix it"
STORM-2766,"java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 349, Size: 349 at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:495) at org.apache.storm.util","java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 349, Size: 349 at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:495) at org.apache.storm.util"
STORM-2763,Add blobstore scripts to 'storm admin' command,There are blobstore-related scripts that will be merged as part of STORM-2760. It would be ideal to make these available to the 'storm admin' command so they are easier to use.
STORM-2755,Sharing data between Bolts,"Hello, 
i am facing a problem and your help will be really appreciated. 
so, i am using storm 1.0.1, i prepared a topology that transfers data throw a preparation bolt then to a python prediction bolt. 
in prediction bolt i am collecting data in real time and creating group of data then sending them to prediction function. 
this topology works perfectly in one machine where the machine can process all the data, but not  in distributed cluster as each machine work separately and some data needed for grouping can be processed by different machine.
Is there a way that bolts communicate with each other and so we can group data even if they are on different machines? 
can you please advice and thank you"
STORM-2754,Not killing on exceptions in other threads,We probably don't want to kill the process if the exceptions are from other threads
STORM-2753,Avoid shutting down netty server on netty exception,We should avoid shutting down netty server on netty exception
STORM-2752,Nimbus crashes silently if scheduler is not found,"When nimbus is started and the custom scheduler specified in storm.yaml is not in the classpath, nimbus hangs and exits with status 13 about 10s later. No errors are logged.

Affected versions 1.0.3-5, I did not test any other. OpenJDK 8."
STORM-2749,Remove state spout since it's never supported by storm,"We think we probably want to get rid of state spout stuff since it's never being implemented. 
The related code can be traced back to very early day of storm, e.g.
[TopologyBuilder#setStateSpout|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java#L436-L443]"
STORM-2747,"Make the windowing classes use long instead of int for time parameters, and rename millisecond based time parameters so it's clear which unit they are.","Some parameters in the windowing classes are using int instead of long for describing time, which requires a bunch of casting to/from long in the code. Some variables are always in milliseconds but are named e.g. ""value"", which makes it easy to accidentally use the wrong timeunit. Such variables should be named e.g. ""valueMs"" to disambiguate the unit."
STORM-2746,Max Open Files does not close files for the oldest entry,"Description:

AbstractHDFSBolt has WritersMap. This evicts least recently used AbstractHDFSWriter out of the writers map, however, does not close the file in open state by the oldest entry.

Steps to reproduce  error: 

1) Use new Max open files feature and set the value to 1.
2) Write data to two or three different files in hdfs using AvroBolt.
3) Check output directory using fsck in hdfs.
   
Expected: only one file open in output directory.
Actual: > 1 files are in open state."
STORM-2745,Hdfs Open Files problem,"Issue:

Problem exists when there are multiple HDFS writers in writersMap. Each writer keeps an open hdfs handle to the file. Incase of Inactive writer(i.e. one which is not consuming any data from long period), the files are not closed and always remain in open state.

Ideally, these files should get closed and Hdfs writers removed from the WritersMap.

Solution:
Implement a ClosingFilesPolicy that is based on Tick tuple intervals. At each tick tuple all Writers are checked and closed if they exist for a long time."
STORM-2739,Storm UI fails to bind to ui.host when using https,"When using https with the Storm UI, it ignores the value of ui.host, and binds to 0.0.0.0.

Starting with this config:


{code}
storm.local.dir: ""/opt/storm""
storm.zookeeper.servers:
    - ""bigstorm.porcupineracing.com""
nimbus.seeds: [""bigstorm.porcupineracing.com""]
nimbus.childopts: ""-Xmx1024m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
ui.childopts: ""-Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
supervisor.childopts: ""-Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
storm.thrift.transport: ""org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin""
java.security.auth.login.config: ""/keytabs/jaas.conf""
storm.zookeeper.superACL: ""sasl:storm@PORCUPINERACING.COM""

ui.host: 127.0.0.1

nimbus.authorizer: ""org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer""
nimbus.admins:
  - ""storm/bigstorm.porcupineracing.com@PORCUPINERACING.COM""
  - ""storm@PORCUPINERACING.COM""
  - ""storm""
nimbus.supervisor.users:
  - ""storm/bigstorm.porcupineracing.com@PORCUPINERACING.COM""
  - ""storm@PORCUPINERACING.COM""
  - ""storm""
nimbus.users:
   - ""steven.miller""
   - ""steven.miller@PORCUPINERACING.COM""
{code}

I can start the UI and verify using lsof that it's only listening on localhost:


{code}
[root@bigstorm bin]# ps axuww | grep ui.core
root      5080  0.1  5.6 2850232 217688 pts/1  Sl   Sep14   1:31 java -server -Ddaemon.name=ui -Dstorm.options= -Dstorm.home=/opt/apache-storm-1.1.1 -Dstorm.log.dir=/opt/apache-storm-1.1.1/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /opt/apache-storm-1.1.1/lib/asm-5.0.3.jar:/opt/apache-storm-1.1.1/lib/clojure-1.7.0.jar:/opt/apache-storm-1.1.1/lib/disruptor-3.3.2.jar:/opt/apache-storm-1.1.1/lib/kryo-3.0.3.jar:/opt/apache-storm-1.1.1/lib/log4j-api-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-core-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-over-slf4j-1.6.6.jar:/opt/apache-storm-1.1.1/lib/log4j-slf4j-impl-2.8.2.jar:/opt/apache-storm-1.1.1/lib/minlog-1.3.0.jar:/opt/apache-storm-1.1.1/lib/objenesis-2.1.jar:/opt/apache-storm-1.1.1/lib/reflectasm-1.10.1.jar:/opt/apache-storm-1.1.1/lib/ring-cors-0.1.5.jar:/opt/apache-storm-1.1.1/lib/servlet-api-2.5.jar:/opt/apache-storm-1.1.1/lib/slf4j-api-1.7.21.jar:/opt/apache-storm-1.1.1/lib/storm-core-1.1.1.jar:/opt/apache-storm-1.1.1/lib/storm-rename-hack-1.1.1.jar:/opt/apache-storm-1.1.1:/opt/apache-storm-default/conf -Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf -Dlogfile.name=ui.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/opt/apache-storm-1.1.1/log4j2/cluster.xml org.apache.storm.ui.core
root     19913  0.0  0.0 112648   972 pts/1    R+   09:26   0:00 grep --color=auto ui.core

[root@bigstorm bin]# lsof -p 5080 -P | grep LISTEN
java    5080 root   27u     IPv6             597116       0t0      TCP localhost:8080 (LISTEN)
{code}


Now if I add the https config:

{code}
ui.https.host: ""localhost""
ui.https.port: 8443
ui.https.keystore.type: ""jks""
ui.https.keystore.path: ""/keytabs/keystore.jks""
ui.https.keystore.password: ""sooper-sekrit""
ui.https.key.password: ""sooper-sekrit""
{code}

and I restart the UI, I can see that it's listening on *:8443:

{code}
[root@bigstorm bin]# ps axuww | grep ui.core
root     19921 17.2  5.4 2849188 210896 pts/1  Sl   09:26   0:04 java -server -Ddaemon.name=ui -Dstorm.options= -Dstorm.home=/opt/apache-storm-1.1.1 -Dstorm.log.dir=/opt/apache-storm-1.1.1/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /opt/apache-storm-1.1.1/lib/asm-5.0.3.jar:/opt/apache-storm-1.1.1/lib/clojure-1.7.0.jar:/opt/apache-storm-1.1.1/lib/disruptor-3.3.2.jar:/opt/apache-storm-1.1.1/lib/kryo-3.0.3.jar:/opt/apache-storm-1.1.1/lib/log4j-api-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-core-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-over-slf4j-1.6.6.jar:/opt/apache-storm-1.1.1/lib/log4j-slf4j-impl-2.8.2.jar:/opt/apache-storm-1.1.1/lib/minlog-1.3.0.jar:/opt/apache-storm-1.1.1/lib/objenesis-2.1.jar:/opt/apache-storm-1.1.1/lib/reflectasm-1.10.1.jar:/opt/apache-storm-1.1.1/lib/ring-cors-0.1.5.jar:/opt/apache-storm-1.1.1/lib/servlet-api-2.5.jar:/opt/apache-storm-1.1.1/lib/slf4j-api-1.7.21.jar:/opt/apache-storm-1.1.1/lib/storm-core-1.1.1.jar:/opt/apache-storm-1.1.1/lib/storm-rename-hack-1.1.1.jar:/opt/apache-storm-1.1.1:/opt/apache-storm-default/conf -Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf -Dlogfile.name=ui.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/opt/apache-storm-1.1.1/log4j2/cluster.xml org.apache.storm.ui.core
root     20018  0.0  0.0 112648   968 pts/1    R+   09:27   0:00 grep --color=auto ui.core
[root@bigstorm bin]# lsof -p 19921 -P | grep LISTEN
java    19921 root   38u  IPv6             677914       0t0      TCP *:8443 (LISTEN)
{code}

I have a situation in which I'm trying to limit access to the UI on a per-user basis.  The UI seems, as far as I can tell, only to support limiting access to users with valid Kerberos tickets (which is everyone here :) ), so I was trying to put a proxy in front of the UI and run it just on localhost, and rely on the proxy to do the authentication.

This bug means that if I was to do that, I'd have to run the UI without https, which means that people's credentials would be bouncing around in the clear (again, as far as I can tell; I tcpdumped that and I could see, say, storm@PORCUPINERACING.COM in the base64 decode of the Authorization: HTTP header, at least, which I figure was a bad sign).

I looked at the code and didn't see anything obvious but since I don't know Clojure or Netty it was probably staring me in the face. :) . But if you could fix this that'd be awesome, and it'd let me secure this in a way that I'd find much more reassuring.  Thanks!"
STORM-2737,"Storm-hive integration ,tuple support null value fields","code in the file : org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper
the method mapRecord seems not support not decleared fields,when the tuple does not contains field,it will throw the null pointer error;
so i had the blow solution:
{code:java}
// Some comments here
	    @Override
	    public byte[] mapRecord(Tuple tuple) {
	        StringBuilder builder = new StringBuilder();
	        if(this.columnFields != null) {
	            for(String field: this.columnFields) {
	                builder.append(tuple.contains(field)?tuple.getValueByField(field) :null);
	                builder.append(fieldDelimiter);
	            }
	        }
	        return builder.toString().getBytes();
	    }

{code}
just check contains and put null into it;"
STORM-2735,LocalCluster and other testing classes are not documented in Javadoc because they are part of storm-server,"We don't publish any Javadoc about LocalCluster or the other testing tools because they are part of the storm-server module. Since users are expected to interact with these classes directly, we should figure out a way to publish Javadoc for them. 
"
STORM-2723,Nimbus crashes on joining cluster,"Cluster with N nodes and with running topologies. N new nodes join and the old machines start to be disconnected.
Some of the new nimbus fail with this message:

{code:java}
2017-09-06T16:30:53.551Z cluster [INFO] setup-path/blobstore/Topology-1-1504685635-stormconf.
ser/node02:6627-1
2017-09-06T16:30:53.608Z nimbus [ERROR] Error when processing event
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: or
g.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:98) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.nimbus$fn__10607.invoke(nimbus.clj:1458) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.MultiFn.invoke(MultiFn.java:233) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$fn__11005$exec_fn__1364__auto____11006$fn__11021.invoke(nimbus.clj:2460) ~[storm-core-1.1.0.j
ar:1.1.0]
	at org.apache.storm.timer$schedule_recurring$this__1737.invoke(timer.clj:105) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:50) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.storm.th
rift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:120) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.storm.thrift.transport.TTransportExc
eption
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:266) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.lang.RuntimeException: java.io.IOException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:194) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.io.IOException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:156) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:182) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:186) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.generated.Nimbus$Client.recv_downloadBlobChunk(Nimbus.java:866) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.generated.Nimbus$Client.downloadBlobChunk(Nimbus.java:853) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.readMore(NimbusBlobStore.java:168) ~[storm-core-1.1.0.j
ar:1.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:146) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:182) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:186) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
2017-09-06T16:30:53.618Z util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__9808.invoke(nimbus.clj:212) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:71) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
2017-09-06T16:30:53.619Z nimbus [INFO] Shutting down master

{code}
"
STORM-2721,Add mapping for KafkaConsumer metrics to storm metrics in KafkaTridentSpoutOpaque,"The current KafkaTridentSpoutOpque, does not have any metrics. We can use the metrics() call of the KafkaConsumer https://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#metrics(), to get various metrics and map that to the storm metrics.

Eg:
We can add a generic KafkaClientMetric which would implement IMetric and has a getValueAndReset(), where the consumer metric calls are made.
requiredMetric can be initialized to any metrics like records-lag-max.

{code}
        @Override
        public Object getValueAndReset() {

            for (Map.Entry<MetricName, ? extends Metric> metricKeyVal : ((Map<MetricName, ? extends Metric>) kafkaConsumer.metrics()).entrySet()) {

                // Sample structure of Metric
                // MetricName [name=records-lag-max, group=consumer-fetch-manager-metrics, description=The maximum lag in terms of number of records for any partition in this window, tags={client-id=consumer-1}] metric.name()=MetricName [name=records-lag-max, group=consumer-fetch-manager-metrics, description=The maximum lag in terms of number of records for any partition in this window, tags={client-id=consumer-1}] metric.value()=-Infinity

                Metric metric = metricKeyVal.getValue();
                if(metric.metricName().name().equals(requiredMetric)) {
                    return metric.value();
                }

            }

            return null;
        }
{code}
"
STORM-2719,Trident Kafka Spout Emitters do not get full partition information in getOrderedPartitions(),"The storm kakfa trident spout uses the KafkaTridentSpoutTopicPartitionRegistry, to get partition information. The coordinator calls the getTopicPartitions() method to get partition information and passes it to the emitters. But this partition information will not be accurate as all instances of KafkaTridentSpoutTopicPartitionRegistry will not be updated with full partition information.

The update to the registry is done when the consumer subscribes using KafkaSpoutConsumerRebalanceListener. This calls the KafkaTridentSpoutTopicPartitionRegistry.INSTANCE.addAll(partitions); These calls would only update the registry in that particular worker with partition information for consumers in that worker.

So when the coordinator calls the getOrderedPartitions() and passes it to each emitter by calling getOrderedPartitions(), the full partition information will not be present. The only probable case this would work is if the emitters and coordinators were on the same worker."
STORM-2715,[storm-jms] testFailure on JmsSpoutTest fails intermittently,"I've seen this failure several times on PR build, and also some on my local machine. If possible we should fix the bothering bug and make build stable."
STORM-2714,Release Apache Storm 2.0.0,"This is to track remaining issues on releasing Storm 2.0.0.
"
STORM-2713,"when the connection to the first zkserver is timeout,storm-kafka's kafkaspout will throw a exception","when the connection to the first zkserver is timeout,storm-kafka's kafkaspout will throw a exception without attempting to connect other zkserver,even zk can also work with one node down."
STORM-2711,use KafkaTridentSpoutOpaque poll msg slowly.,"At first I run producer examples to make msgs in kafka, which topic is 5 partition 1 replication, then the number of total message was about 4000, per partition almost 800. Then I {color:red}run the part of consumer example in TridentKafkaClientWordCountNamedTopics in storm-kafka-client-examples{color}, First pull messages at a certain speed, when each partition to more than 500, significantly {color:red}slower speed {color}. I wonder why"
STORM-2710,Release Apache Storm 1.2.0,This is to track remaining issues on releasing Storm 1.2.0. These are in addition to the issues listed in https://issues.apache.org/jira/browse/STORM-2709.
STORM-2709,Release Apache Storm 1.1.2,This is to track remaining issues on releasing Storm 1.1.2.
STORM-2707,Nimbus loops forever with getClusterInfo error if it looses storm.local.dir contents,"Hello,

Short issue description:
* Remove storm.local.dir directory
* Storm UI isn't anymore able to query anything from Nimbus
* Nimbus process prints getClusterInfo exceptions in its log whenever it gets a query from Nimbus UI or from ""storm"" command line
* To fix this issue, we have to stop all Storm processes, cleanup the content of Zookeeper nodes, then restart & redeploy our topologies

Excepted behavior:
* In such case, Storm should cleanup the content of Zookeeper and recover in a mode allowing to kill & restart all topologies

More details:
===========
Sometimes we loose the content of storm.local.dir on our single-node Nimbus production cluster.

We haven't yet considered deploying Nimbus in HA because this is a relatively modest deployment with budget constrains on the number of the number of IaaS resources which can be used for this application. So far so good, because in our environment, Nimbus & Nimbus UI (hosted on same VM) are supervized, and we also have self-healing crons to automatically kill & restart topologies blocked in Kafka consumption or having too many failed tuples (because Storm back pressure has some fuzzy limits, so we use this by-pass, as approved by Roshan in a past discussion, but that's not the point here).

Our problem is that sometime, we loose the content of storm.local.dir.

When it happens, our supervision detects the issue because it cannot anymore query Nimbus REST services on Nimbus-UI process.

In such case it tries to restart Storm-UI but this doesn't help because queries to Storm-UI fails with the following stack trace when it tries to list all topologies:

org.apache.storm.thrift.TApplicationException: Internal error processing getClusterInfo
	at org.apache.storm.thrift.TApplicationException.read(TApplicationException.java:111)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.storm.generated.Nimbus$Client.recv_getClusterInfo(Nimbus.java:1168)
	at org.apache.storm.generated.Nimbus$Client.getClusterInfo(Nimbus.java:1156)
	at org.apache.storm.ui.core$cluster_summary.invoke(core.clj:356)
	at org.apache.storm.ui.core$fn__9556.invoke(core.clj:1113)
	at org.apache.storm.shade.compojure.core$make_route$fn__5976.invoke(core.clj:100)
	at org.apache.storm.shade.compojure.core$if_route$fn__5964.invoke(core.clj:46)
	at org.apache.storm.shade.compojure.core$if_method$fn__5957.invoke(core.clj:31)
	at org.apache.storm.shade.compojure.core$routing$fn__5982.invoke(core.clj:113)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__5986.invoke(core.clj:118)
	at org.apache.storm.shade.ring.middleware.cors$wrap_cors$fn__8891.invoke(cors.clj:149)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__8838.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6618.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__7901.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__6871.invoke(helpers.clj:50)
	at org.apache.storm.ui.core$catch_errors$fn__9758.invoke(core.clj:1428)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__6538.invoke(keyword_params.clj:35)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__6581.invoke(nested_params.clj:84)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__6510.invoke(params.clj:64)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6618.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__6833.invoke(flash.clj:35)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__6819.invoke(session.clj:98)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__6368.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__6372.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.ui.helpers$x_frame_options_filter_handler$fn__6964.invoke(helpers.clj:189)
	at org.apache.storm.ui.helpers.proxy$java.lang.Object$Filter$abec9a8f.doFilter(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:748)

In Nimbus.log, we also have this kind of exception each time Nimbus UI is queried:

org.apache.storm.generated.KeyNotFoundException: null
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:147) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:299) ~[storm-core-1.1.0.jar:1.1.0]
        at sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:489) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.daemon.nimbus$get_cluster_info$iter__10687__10691$fn__10692.invoke(nimbus.clj:1550) ~[storm-core-1.1.0.jar:1.1.0]
        at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
        at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
        at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
        at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
        at clojure.core$dorun.invoke(core.clj:3009) ~[clojure-1.7.0.jar:?]
        at clojure.core$doall.invoke(core.clj:3025) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1524) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10782.getClusterInfo(nimbus.clj:1971) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3920) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3904) ~[storm-core-1.1.0.jar:1.1.0]

Even if Storm was shutting down in such case, this wouldn't help because we have to cleanup all Zookeepers to put our Storm cluster back to life.

Ideally, we would like that when storm.local.dir is lost, Nimbus will re-create it ""blank"" and cleanups Zookeeper nodes (that's the tricky part); then we expect that Supervisors (which are unaffected by this issue when it occurs) will re-register themselves to make Nimbus aware that topologies are Running. 

Also, Topologies restart from UI should consistently fail until topologies JARs are re-submitted (please make the error message very clear and easy to ""grep"" when such case occurs);

This is my first JIRA, I hope I provided everything to let Storm developers dig this issue ; otherwise please let me know if more information is required: I will be glad to help as much as I can... Storm rocks!

Best regards,
Alexandre Vermeerbergen

"
STORM-2697,Failed to cleanup worker when GET worker-user failed,"""2017-08-15 11:25:53,554"" | INFO  | [Thread-4] | Shutting down and clearing state for id f5906569-41db-4c7f-9048-b3c551603fb4. Current supervisor time: 1502767553. State: :not-started, Heartbeat: nil | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,554"" | INFO  | [Thread-4] | Shutting down 136d9652-7b8b-4e3d-8d45-33d72dfe1462:f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | INFO  | [Thread-4] | GET worker-user f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.config (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | WARN  | [Thread-4] | Failed to get worker user for f5906569-41db-4c7f-9048-b3c551603fb4. #<FileNotFoundException java.io.FileNotFoundException: /var/streaming_data/stormdir/workers-users/f5906569-41db-4c7f-9048-b3c551603fb4 (No such file or directory)> | backtype.storm.config (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | WARN  | [Thread-4] | Failed to cleanup worker f5906569-41db-4c7f-9048-b3c551603fb4. Will retry later #<IllegalArgumentException java.lang.IllegalArgumentException: User cannot be blank when calling worker-launcher.> | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | INFO  | [Thread-4] | Shut down 136d9652-7b8b-4e3d-8d45-33d72dfe1462:f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) "
STORM-2696,storm-kafka-client doesn't emit tuple when numUncommittedOffsets - readyMessageCount >= maxUncommittedOffsets,"for example, when set maxUncommittedOffsets = 10, at first kafka-comsumer fetech 500 messges from remote, with 490 messages failed and 10 messgaes acked. at this condititon the 
 `waitingToEmit` is empty, and `numUncommittedOffsets - readyMessageCount  = 10 >= maxUncommittedOffsets` , the client will never meet the condition to poll again"
STORM-2688,"Schedule grouped topology components on ""network wise"" close slots",
STORM-2687,"Group Topology executors by network proximity needs and schedule them on ""network wise"" close slots",
STORM-2685,Calculate and update taskNetworkDistance in WorkerState periodically,
STORM-2683,Storm UI - Topology action button error response hardcoded,"Right now on confirming the topology ui action buttons (https://github.com/apache/storm/blob/4966d7a69318d2ca690c47dd43466b03574e5e9e/storm-core/src/ui/public/templates/topology-page-template.html#L607) the UI behaviour is to reload the page always (https://github.com/apache/storm/blob/10d381b303c9176ede0d1260428ad61c7757e396/storm-core/src/ui/public/js/script.js#L169) and in case of an error display a hardcoded error message _""Error while communicating with Nimbus.""_

While this behaviour is okay for workflows with no authorization it gets confusing when the action buttons are put behind some form of authorization. It will be much clearer to define an error message format and have the UI display the correct error message on authorization and other non-nimbus related failures. "
STORM-2681,Regarding Issue with Multi-Lang in Storm While Running Python (WordCountTopology ),"
I am trying to run the WordCountTopology in storm local mode before running my own application and I am unable to run it. I am continuously getting the following error. I have verified that the splitsentence.py file is kept at the multilang/resources directory which is in the examples/storm-starter directory.

5729 [Thread-22-split-executor[7 7]] ERROR o.a.s.util - Async loop died!
java.lang.RuntimeException: org.apache.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read.
Serializer Exception:
python: can't open file 'splitsentence.py': [Errno 2] No such file or directory

        at org.apache.storm.utils.ShellProcess.launch(ShellProcess.java:91) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.task.ShellBolt.prepare(ShellBolt.java:131) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.executor$fn__4973$fn__4986.invoke(executor.clj:791) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.0.3.jar:1.0.3]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_51]
5734 [Thread-30-spout-executor[9 9]] INFO  o.a.s.d.executor - Opening spout spout:(9)
5734 [Thread-28-split-executor[6 6]] ERROR o.a.s.util - Async loop died!

I have followed the instructions during installation and built my package successfully with maven. My storm version is 1.0.3 and my python version is 2.7.13. I ran the ExclamationTopology and some of my own code which does not need multi-language support and it works like a charm.

I am not able to understand why the multi-lang support in storm is not working. Any help with this is greatly appreciated."
STORM-2669,Extend the BinaryEventDataScheme in storm-eventhubs to include MessageId in addition to system properties,"Currently there are two types of EventDataScheme included with the storm-eventhubs spout.

The default is the StringEventDataScheme that emits a single output field, the message itself as a string.

There is an additional BinaryEventDataScheme that passes the message as is, but also has two additional fields: metadata and system_metadata that is passed by eventhubs-client.

The system_metadata only contains the sequence number, offset and enqeued time of an event.

As part of recent requirements by certain applications for tracking an event, they also need the partition id. The partition id is NOT sent by the eventhubs-client, instead the partition manager in the spout already has this information.

The goal of this JIRA is to introduce another output field in BinaryEventDataScheme that contains the MessageId for an event. The messageId will contain: partitionId, sequence number and the offset information for any downstream bolt to be able to locate where the message arrived from.

I will also be fixing any maven checkstyle warnings/errors in the files that I will be committing changes in."
STORM-2668,org.apache.storm.kafka.FailedFetchException,"2017-08-01 11:34:42.446 o.a.s.k.KafkaUtils [ERROR] Error fetching data from [Partition{host=xxx, topic=ABC, partition=5}] for topic [ABC]: [UNKNOWN]
2017-08-01 11:34:42.446 o.a.s.k.KafkaSpout [WARN] Fetch failed
org.apache.storm.kafka.FailedFetchException: Error fetching data from [Partition{host=xxx, topic=ABC, partition=5}] for topic [ABC]: [UNKNOWN]"
STORM-2667,Exception Handling in the AbstractHdfsBolt causes bolt to restart,"Recently while reviewing the HDFSBolt code because of a question on the mailing list, I noticed that the abstract bolt will fail a tuple if an IOException is thrown while trying to write it out, and then force a sync in those cases.

https://github.com/apache/storm/blob/64e29f365c9b5d3e15b33f33ab64e200345333e4/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L150-L160

A RuntimeException thrown by the formatter, on the other hand, bubbles up and forces the the worker to restart. 

Any IOException thrown by an Hdfs Output Stream means at that point the stream is closed and cannot be used any more.  As part of our recovery we will try to sync, but this will also fail because the stream is closed by the exception that was thrown, and will result in the sync failing an a RuntimeException being thrown, and the entire worker being restarted.

The current code ""works"" and eventually will recover from these issues, but it may take a while.  It also means that we are likely to have more data loss than needed for some output formats.

I would suggest that we try to recover from RuntimeExceptions in the same say that we are trying to recover from IOExceptions now.  I also would suggest that we handle the special case where the {{tupleBatch.size()}} is 0 but we got an IOException from the writer, as the forceSync will not happen so tuples will continue to fail until the sync policy decides to sync, at which point the worker will crash and then recover."
STORM-2661,JmsSpout should support additional ack mode or should have option for child classes,"Currently , JmsSpout supports 3 acknowledge modes. But some of the queue Providers (like Solace) supports some more ack modes. Like :
Sol_client_ack which acknowledges each message individually to support Guaranteed Delivery.
As JmsSpout's 'jmsAcknowledgeMode' is private and setter has validations, so we can't simply extend JmsSpout(and override required APIs only) to support this additional features. We need to take the complete code.

JmsSpout reference Url :https://github.com/apache/storm/blob/v1.1.0/external/storm-jms/src/main/java/org/apache/storm/jms/spout/JmsSpout.java

So, either there should be support for enhancements by child classes or 
this additional ack mode(which seems generic requirement) should be included in JmsSpout itself - we've some reference code which is working fine. can share the same if required.  
"
STORM-2656,Cannot start Nimbus,"After installation of apache storm, when run ""sudo bin/storm nimbus"", get error logs below. Already googled, if the problem is related to clean states, could you please tell me how to do that? Thanks!
2017-07-24 17:45:29.841 o.a.s.d.common main [INFO] Started statistics report plugin...
2017-07-24 17:45:29.871 o.a.s.d.nimbus main [INFO] Starting nimbus server for storm version '1.1.0'
2017-07-24 17:45:30.739 o.a.s.d.nimbus timer [ERROR] Error when processing event
java.lang.RuntimeException: java.lang.RuntimeException: java.util.zip.ZipException: Not in GZIP format
	at org.apache.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:53) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.utils.Utils.deserialize(Utils.java:216) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.cluster$maybe_deserialize.invoke(cluster.clj:224) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.cluster$mk_storm_cluster_state$reify__4395.supervisor_info(cluster.clj:410) ~[storm-core-1.1.0.jar:1.1.0]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$all_supervisor_info$fn__9895.invoke(nimbus.clj:448) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.core$map$fn__4553.invoke(core.clj:2624) ~[clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
	at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
	at clojure.core$mapcat.doInvoke(core.clj:2660) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.invoke(RestFn.java:423) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$all_supervisor_info.invoke(nimbus.clj:446) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.nimbus$all_supervisor_info.invoke(nimbus.clj:442) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.nimbus$read_all_supervisor_details.invoke(nimbus.clj:725) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.nimbus$compute_new_scheduler_assignments.invoke(nimbus.clj:858) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:986) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.RestFn.invoke(RestFn.java:410) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$fn__11005$exec_fn__1364__auto____11006$fn__11017.invoke(nimbus.clj:2446) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$schedule_recurring$this__1737.invoke(timer.clj:105) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:50) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: java.util.zip.ZipException: Not in GZIP format
	at org.apache.storm.utils.Utils.gunzip(Utils.java:288) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:50) ~[storm-core-1.1.0.jar:1.1.0]
	... 30 more
Caused by: java.util.zip.ZipException: Not in GZIP format
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:165) ~[?:1.8.0_131]
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:79) ~[?:1.8.0_131]
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:91) ~[?:1.8.0_131]
	at org.apache.storm.utils.Utils.gunzip(Utils.java:278) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:50) ~[storm-core-1.1.0.jar:1.1.0]
	... 30 more
2017-07-24 17:45:30.755 o.a.s.util timer [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__9808.invoke(nimbus.clj:212) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:71) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2017-07-24 17:45:30.758 o.a.s.d.nimbus Thread-7 [INFO] Shutting down master
"
STORM-2651,Executor stopped working after connectivity issues with ZK. Executor is not restarted  by nimbus scheduler. ,"After connectivity issues, nimbus scheduler assigned the appropriate executor to the slots(custom scheduler). 


{code:java}

{panel:title=My title}
o.a.s.d.nimbus [INFO] Setting new assignment for topology id <<topology_name>>-1499356635: #org.apache.storm.daemon.common.Assignment{:master-code-dir ""/opt/storm_datadir"", :node->host {""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" ""test1"", ""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" ""test2"", ""c13b0fc8-d5c1-4335-8339-17b3c048b160"" ""test3"", ""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" ""test4""}, :executor->node+port {[8 8] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [12 12] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [2 2] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [7 7] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [22 22] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [3 3] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [24 24] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [1 1] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [18 18] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [6 6] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [20 20] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [9 9] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [23 23] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [11 11] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [16 16] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [13 13] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [19 19] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [21 21] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [5 5] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [10 10] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [14 14] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [4 4] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [15 15] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [17 17] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702]}, :executor->start-time-secs {[8 8] 1499356646, [12 12] 1499356646, [2 2] 1499356646, [7 7] 1499356646, [22 22] 1499356646, [3 3] 1499356646, [24 24] 1499356646, [1 1] 1499356646, [18 18] 1499356646, [6 6] 1499356646, [20 20] 1499356646, [9 9] 1499356646, [23 23] 1499356646, [11 11] 1499356646, [16 16] 1499356646, [13 13] 1499356646, [19 19] 1499356646, [21 21] 1499356646, [5 5] 1499356646, [10 10] 1499356646, [14 14] 1499356646, [4 4] 1499356646, [15 15] 1499356646, [17 17] 1499356646}, :worker->resources {[""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702] [0.0 0.0 0.0], [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702] [0.0 0.0 0.0], [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703] [0.0 0.0 0.0], [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703] [0.0 0.0 0.0]}}
{panel}

{code}

Then all the executor are started working properly.  

When I checked in-depth I found that,  one of the spout executor has not started and also found  that nimbus stopped logging after this issue.

o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<<topology-name >>-1499356635-stormconf.ser


"
STORM-2647,Reduce Number of Threads running in the Worker,"Below is an account of all the threads in a single worker running a topology with 1 spout instance, 1 bolt instance, and 1 acker bolt instance. Topology debugging feature was disabled as it brings in additional threads.

Total 34 threads:  
10 timer threads
2 Curator threads
2 ZooKeeper threads
4 Netty threads
7 Disruptor thread
1 Spout executor thread
2 worker thread
1 Back Pressure thread
4 thread - unclear what these are
1 Finalizer thread



- Would be good to collapse the Timer threads into SingleThreadScheduledExecutor.
- We have 4 threads to communicate with ZK (2curator + 2ZK). If necessary can we eliminate curator to accomplish that ?
- Similarly see if we can reduce the number of Netty threads.
- How many threads does topology debugging need, can we reduce that ?
- Same with event logging (topology.eventlogger.executors).
- The Back pressure and Disruptor threads will be reduced by STORM-2306 "
STORM-2646,NimbusClient Class cast exception when nimbus seeds is not an array of hosts,
STORM-2637,ClassCastException in logviewer get-log-user-group-whitelist function,"
{code:java}
2017-07-17 18:12:42.073 o.e.j.s.ServletHandler qtp1452355939-21 [WARN] /log
java.lang.ClassCastException: java.io.File cannot be cast to java.lang.String
        at org.apache.storm.daemon.logviewer$get_log_user_group_whitelist.invoke(logviewer.clj:314) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPS
HOT]
        at org.apache.storm.daemon.logviewer$authorized_log_user_QMARK_.invoke(logviewer.clj:330) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHO
T]
        at org.apache.storm.daemon.logviewer$log_page.invoke(logviewer.clj:419) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.logviewer$fn__4141.invoke(logviewer.clj:1038) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at compojure.core$make_route$fn__355.invoke(core.clj:100) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at compojure.core$if_route$fn__343.invoke(core.clj:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at compojure.core$if_method$fn__336.invoke(core.clj:31) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at compojure.core$routing$fn__361.invoke(core.clj:113) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at clojure.core$some.invoke(core.clj:2570) ~[clojure-1.7.0.jar:?]
        at compojure.core$routing.doInvoke(core.clj:113) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at clojure.lang.RestFn.applyTo(RestFn.java:139) ~[clojure-1.7.0.jar:?]
        at clojure.core$apply.invoke(core.clj:632) ~[clojure-1.7.0.jar:?]
        at compojure.core$routes$fn__365.invoke(core.clj:118) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.ui.helpers$requests_middleware$fn__1810.invoke(helpers.clj:54) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at ring.middleware.keyword_params$wrap_keyword_params$fn__1502.invoke(keyword_params.clj:35) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAP
SHOT]
        at ring.middleware.nested_params$wrap_nested_params$fn__1545.invoke(nested_params.clj:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHO
T]
        at ring.middleware.params$wrap_params$fn__1474.invoke(params.clj:64) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.logviewer$conf_middleware$fn__4195.invoke(logviewer.clj:1202) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT
]
        at ring.util.servlet$make_service_method$fn__1240.invoke(servlet.clj:127) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at ring.util.servlet$servlet$fn__1244.invoke(servlet.clj:136) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSH
OT]
..
{code}

In get-log-user-group-whitelist function, the result returned by getLogMetaDataFile is a File; the readYamlFile function requires a String."
STORM-2636,Drpc server doesn't send http headers to storm topology,The Drpc server doesn't send the http headers received from the Jetty server (in the case of http requests submitted to drpc server) to the storm topology. Due to this context in the headers such as request-id is lost. It would be useful if this information can also be submitted along with message body to the topology.
STORM-2625,KafkaSpout is not calculating uncommitted correctly,"This happens when:
1. KafkaSpout has already committed offsets to a topic before, and is not running/activated now;
2. There're messages in topic after the committed offsets;
3. The same consumer group topology with multi works is started/activated again;

The same issue may happen when running topology gets consumer group partition re-assignment with offsets not being able to be committed in time.

The underlying issue is:

a. Because workers are registering kafka consumers one by one, when the first consumer A registers itself with kafka broker with the consumer group, it's assigned all the partitions, say partition 0 & 1. Consumer A then retrieves messages from all the assigned partitions if possible, and started processing. With every tuple KafkaSpout A emits, UNCOMMITTED count numUncommittedOffsets++ (KafkaSpout#emitTupleIfNotEmitted());

b. At this point a second consumer B registers with the broker for the same consumer group. the broker then re-assigns the partitions among existing consumers, say consumer A is assigned partition 0, and consumer B assigned partition 1. 

b.1 At this point KafkaSpout A will try committing acked offsets, and remove the partition 1 offsets it's tracking (KafkaSpout.KafkaSpoutConsumerRebalanceListener#onPartitionsRevoked()); However because the tuples are not all acked, KafkaSpout is not able to commit full list of offsets to kafka broker.

b.2 Then KafkaSpout A will remove tracked partition 1 offsets in offsetManagers as well as emitted (
org.apache.storm.kafka.spout.KafkaSpout.KafkaSpoutConsumerRebalanceListener#onPartitionsAssigned()
org.apache.storm.kafka.spout.KafkaSpout.KafkaSpoutConsumerRebalanceListener#initialize()), resulting the not acked tuples won't be acked for ever (org.apache.storm.kafka.spout.KafkaSpout#ack()), also the UNCOMMITTED count numUncommittedOffsets will never be reduced back to a correct result.

"
STORM-2624,Kafka Storm Spout: Got fetch request with offset out of range,"If partition offset is out of range then kafka spout stops emitting new messages and keeps logging following warning:
2016-10-26 11:11:31.070 o.a.s.k.KafkaUtils [WARN] Partition{host=somehost.org:9092, topic=my-topic, partition=0} Got fetch request with offset out of range: [3]
2016-10-26 11:11:31.078 o.a.s.k.KafkaUtils [WARN] Partition{host=somehost.org:9092, topic=my-topic, partition=0} Got fetch request with offset out of range: [3]
...

I believe the trivial fix is in PartitonManager.java in fill method 
line 237:
{code:java}
            long partitionLatestOffset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.LatestTime());
            if (partitionLatestOffset < offset) {
                offset = partitionLatestOffset;
            } else {
                offset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.EarliestTime());
            }
{code}
change to:
{code:java}
            offset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, _spoutConfig.startOffsetTime);
{code}

line 259:
{code:java}
            if (offset > _emittedToOffset) {
                _lostMessageCount.incrBy(offset - _emittedToOffset);
                _emittedToOffset = offset;
                LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
            }
{code}
change to:
{code:java}
            if (offset > _emittedToOffset) {
                _lostMessageCount.incrBy(offset - _emittedToOffset);
            }
            _emittedToOffset = offset;
            LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
{code}
"
STORM-2617,log4j2 RollingFile rotation failing,"I noticed that the default log rotation configuration isn't working as expected.  A specific example is worker.log.  Here is the default log4j2 configuration for worker.log (log4j2/worker.xml):
{code:xml}
    <RollingFile name=""A1""
                fileName=""${sys:workers.artifacts}/${sys:storm.id}/${sys:worker.port}/${sys:logfile.name}""
                filePattern=""${sys:workers.artifacts}/${sys:storm.id}/${sys:worker.port}/${sys:logfile.name}.%i.gz"">
        <PatternLayout>
            <pattern>${pattern}</pattern>
        </PatternLayout>
        <Policies>
            <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
        </Policies>
        <DefaultRolloverStrategy max=""9""/>
    </RollingFile>
{code}

Even thought the DefaultRolloverStrategy is set to 9, I only ever see worker.log and worker.log.1.gz.  It seems like rotation is continually overwriting worker.log.1.gz .  I expect this is either an issue with log4j2 or an issue related to the RollingFileAppender and the associated filePattern."
STORM-2612,Supervisor crashes when use the StormSubmitter.submitTopologyAs api to submit another user's topology.,"when use the StormSubmitter.submitTopologyAs api to impersonate another user , the supervisor crashes.
I used the user ""nimbus"" to start the supervisor, then I submit a topology as ""Micheal"", then I got this:
{color:red}2017-07-03 17:47:11.340 o.a.s.u.Utils [ERROR] An exception happened while downloading /home/Micheal/apache-storm-1.0.0/data/supervisor/tmp/5331e7d8-eb6f-403d-a2c5-45cecde9ca25/stormjar.jar from blob store.
AuthorizationException(msg:[nimbus] does not have [READ ] access to topo3-8-1499073314-stormjar.jar)
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:25754)
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:25731)
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:25662)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:825)
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:812)
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357)
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:516)
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:497)
	at org.apache.storm.daemon.supervisor$fn__9590.invoke(supervisor.clj:948)
	at clojure.lang.MultiFn.invoke(MultiFn.java:243)
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9351$fn__9369.invoke(supervisor.clj:582)
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9351.invoke(supervisor.clj:581)
	at org.apache.storm.event$event_manager$fn__8903.invoke(event.clj:40)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
{color}
I wonder if there is some parameter I can set to make this work.
Thanks in advance."
STORM-2611,a batched kafkaspout with offsets in zookeeper,"There are some issues with org.apache.storm.kafka.spout.KafkaSpout.
1. When the topology is running in multi workers in different supervisors, it is very often to trigger kafkaspout rebalance. And so the streaming is not stable. And it will cause massive retransmission of lost packets.
2. When max.uncommitted.offsets is less than 200000 (for limited flow), sometimes there is deadlock. The phenomenon is the heartbeat between spout and kafka can not be performed.
3. When the data is from storm to hbase,  batch is used to improve writing productivity. So using batch from spout to bolt is better for special scene.
4. So a batched kafkaspout and bolt with offsets in zookeeper will be valuable."
STORM-2606,Bolt execute() called many times ( 2 ~ 4 times ),"Hello~

I am getting some problem.
The problem is that My Develop Bolt execute method is called over twice..
The Bolt Function is logging to HBASE.. So.. If execute method is called twice over, The Data is logging to HBASE twice over..

It is disaster in my project..

Please Recommand this problem to me.."
STORM-2605,Versioning State,"From upcoming STORM-2369 we're just breaking compatibility with State structure. 

In this case it would be better to provide two features:

1. Fail fast if version mismatch occurs between stored state and current KV state implementation, and provide proper error message to let users know the reason of failure.
2. Migration tool to migrate old state to the new state

Since STORM-2369 is the first time to break compatibility so I strictly don't think we should resolve this issue along with STORM-2369, but still better to have this feature in future."
STORM-2604,Add support for not scheduling on young supervisors,"Don't schedule on something that is always crashing. See: https://github.com/apache/storm/pull/1674 

The more complicated blacklisting solution [STORM-2083] is yet to finish. This simpler solution deals with the problem by avoiding scheduling on young supervisors."
STORM-2603,url encoding issue when submit topology name with space,"How to reproduce: 
Use 1.0.3
LocalCluster lc = new LocalCluster();
lc.submitTopology(""Hello Storm"", conf, tb.createTopology());

I got following error:
Error on initialization of server mk-worker
java.io.FileNotFoundException: File '/var/folders/1t/3jbxpldd5dgd34fvtd74mc6r0000gp/T/f1e5f859-895c-4767-b586-cd84ba2d0fc7/supervisor/stormdist/Hello%20Storm-1-1498499584/stormconf.ser' does not exist

I looked at my directory: I have the directory Hello+Storm-1-1498499584 instead of Hello%20Storm-1-1498499584

full log file and full code: https://github.com/richardxin/hello_storm/blob/master/err.log

"
STORM-2600,Improve or replace storm-kafka-monitor,"The storm-kafka-monitor module, which is used by Storm UI to show offset lag for topologies with Kafka spouts, has some shortcomings:

* The Storm UI integration code doesn't seem to be able to support topic subscriptions that change after topology submission. The UI code (https://github.com/apache/storm/blob/64e29f365c9b5d3e15b33f33ab64e200345333e4/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java#L91) gets the topic list it should request offset lag for via the spout's getComponentConfiguration method, as far as I can tell through this call https://github.com/apache/storm/blob/9e31509d47c4e91c1009f55c7ccf321d7d7e63aa/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java#L541. It seems like the component configuration is intended to be static once the topology has started running. This prevents us from showing the right topic list for subscriptions that are not known at submission time, which is currently the case for Pattern subscriptions. The topic list for that type of subscription isn't known until the spout has started the KafkaConsumer in {{ISpout.open()}}. I don't see a way to fix this, unless there is some way to update the component configuration when the subscription changes.
* The jar is installed along with the cluster, and depends on the Kafka version specified in Storm's root POM. Kafka guarantees backwards compatible client-server communication for one release only, so there's a potential coupling between Storm cluster version and Kafka version. If users want to update the Kafka version in storm-kafka-monitor, they have to rebuild that module and replace the jar in their Storm install.
* The UI integration uses the storm-kafka-monitor Bash script to start the monitoring code, in order to avoid a dependency between storm-core and storm-kafka-monitor. This prevents the UI integration from working on Windows. We could supply a Windows script as well, but then we'd need to keep the two in sync.

I am wondering if these problems could be solved by implementing offset lag monitoring via the metrics system instead. The spout could periodically seek to the log end offset and submit a metric for how far behind the committed offset is, then seek back to where it left off.
"
STORM-2596,Storm Worker not reconnect the Netty Client,"I have report the simliar bugs at [STORM-2561|https://issues.apache.org/jira/browse/STORM-2561] on the version of 0.10.1.

And these days I upgrade the storm to 1.1.0, but today the bug is appeared agagin.

The worker.log shows
{code:java}
$ cat worker.log|grep '10.24.40.254:6812'|more
2017-06-22 15:14:25.295 o.a.s.m.n.Client main [INFO] creating Netty Client, connecting to 10.24.40.254:6812, bufferSize: 5242880
2017-06-23 11:23:32.570 o.a.s.m.n.StormClientHandler client-worker-1 [INFO] Connection to /10.24.40.254:6812 failed:
2017-06-23 11:23:35.654 o.a.s.m.n.Client refresh-connections-timer [INFO] closing Netty Client Netty-Client-/10.24.40.254:6812
2017-06-23 11:23:35.655 o.a.s.m.n.Client refresh-connections-timer [INFO] waiting up to 600000 ms to send 0 pending messages to Netty-Client-/10.24.40.254
:6812
2017-06-23 14:57:03.352 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 14:57:59.777 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 14:59:16.038 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:01:27.092 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:04:08.654 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:06:59.777 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
{code}
The worker close the netty client on 2017-06-23 11:23:35.654, and never start the netty client. So the messages later on that worker are been discarded.
 
On that time Storm Node(10.24.40.254:6812) is OOM.

{code:java}
2017-06-23 11:22:59.623 g.a.s.s.t.SolrPersistApi pool-10-thread-8 [INFO] write 200 doc at:invoketrace success cost 228060
2017-06-23 11:22:59.625 g.a.s.s.t.SolrPersistApi pool-10-thread-5 [INFO] write 66 doc at:invoketrace success cost 226739
2017-06-23 11:22:59.626 g.a.s.s.t.SolrPersistApi pool-10-thread-7 [INFO] write 200 doc at:invoketrace success cost 167869
2017-06-23 11:23:32.242 STDERR Thread-2 [INFO] java.lang.OutOfMemoryError: Java heap space
2017-06-23 11:23:32.253 STDERR Thread-2 [INFO] Dumping heap to artifacts/heapdump ...
@
{code}

"
STORM-2595,Apply new code style to storm-solr,
STORM-2593,Apply new code style to storm-redis,
STORM-2592,Apply new code style to storm-pmml,
STORM-2591,Apply new code style to storm-opentsdb,
STORM-2590,Apply new code style to storm-mqtt,
STORM-2588,Apply new code style to storm-metrics,
STORM-2587,Apply new code style to storm-kinesis,
STORM-2586,Apply new code style to storm-kafka-monitor,
STORM-2585,Apply new code style to storm-kafka,
STORM-2584,Apply new code style to storm-jms,
STORM-2583,Apply new code style to storm-jdbc,
STORM-2582,Apply new code style to storm-hive,
STORM-2581,Apply new code style to storm-hdfs,
STORM-2580,Apply new code style to storm-hbase,
STORM-2579,Apply new code style to storm-eventhubs,
STORM-2577,Apply new code style to storm-druid,
STORM-2576,Apply new code style to storm-caasandra,
STORM-2575,Apply new code style to storm-autocreds,
STORM-2574,Apply new code style to storm-sql,
STORM-2573,Apply new code style to flux,
STORM-2572,Apply new code style to storm-submit-tools,
STORM-2571,Apply new code style to storm-webapp,
STORM-2570,Apply new code style to storm-core,
STORM-2569,Apply new code style to storm-client-misc,
STORM-2567,Apply new code style to storm-server,"Put effort to reduce max allowed violation count greatly, ideally 0, but even can't get rid of all, do as many as possible."
STORM-2566,Apply new code style to storm-client,"Put effort to reduce max allowed violation count greatly, ideally 0, but even can't get rid of all, do as many as possible."
STORM-2565,Apply new code style to current codebase,"We've introduced code style and also introduced checkstyle, but also set max allowed violation count for each modules to let build pass.

We should put effort to reduce max allowed violation count greatly, ideally 0, but even we can't get rid of all, we should do as many as possible."
STORM-2561,Netty Client is closed but the Worker is already using that Client.,"The Worker 's Netty Client is been closed  and The field ""closing"" in backtype.storm.messaging.netty.Client has been updated  to ""true"".
{code:java}
@Override
    public void close() {
        if (!closing) {
            LOG.info(""closing Netty Client {}"", dstAddressPrefixedName);
            context.removeClient(dstAddress.getHostName(),dstAddress.getPort());
            // Set closing to true to prevent any further reconnection attempts.
            closing = true;
            waitForPendingMessagesToBeSent();
            closeChannel();
        }
    }
{code}

But the worker is already  using that Netty Client. Because of the field 'closing' is true,  Connect in the Client will never reconnecting the target.

{code:java}
public void run(Timeout timeout) throws Exception {
            if (reconnectingAllowed()) {
             .....
             } else {
                close();
                throw new RuntimeException(""Giving up to scheduleConnect to "" + dstAddressPrefixedName + "" after "" +
                        connectionAttempts + "" failed attempts. "" + messagesLost.get() + "" messages were lost"");

            }
{code}

{code:java}
    private boolean reconnectingAllowed() {
        return !closing;
    }
{code}

So, How can me find the cause why Worker close the NettyClient while the NettyClient is just working.

The logs is uploaded to Attachments by command ""cat stt-jstorm-spout-4-28-1497837924-worker-6709.log|grep '10.24.41.10:6710' > /tmp/ClientClosed.txt"".

Please let me know any other useful logs  i can support.
"
STORM-2560,Storm-Kafka on CDH 5.11 with kerberos security enabled.,"Hi,
 
I have installed Apache Storm 1.1.0 manually on CDH 5.11 cluster. This cluster is secured with kerberos. 
I have storm sample written which ingest data from kafka topic and inserts into HDFS directory in real time. So, this sample uses storm-kafka as well as storm-hdfs. 
When I run the storm topology it gives the following error in kafka-spout.

 {color:#d04437}2017-06-18 22:29:31.297 o.a.z.ClientCnxn Thread-14-kafka-spout-executor[5 5]-SendThread(localhost:2181) [INFO] Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error){color}
 
{color:#d04437}2017-06-18 22:29:31.571 k.c.SimpleConsumer Thread-14-kafka-spout-executor[5 5] [INFO] Reconnect due to error:
java.nio.channels.ClosedChannelException: null
        at kafka.network.BlockingChannel.send(BlockingChannel.scala:110) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:85) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) [stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) [stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:129) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__4976$fn__4991$fn__5022.invoke(executor.clj:644) [storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.0.jar:1.1.0]{color}
 
Kafka version: 2.1.1-1.2.1.1.p0.18
 
There is no storm-kafka*.jat present in - ""/usr/local/storm""
But this sample was workin fine before kerberizing the cluster, even in this case.
 
 
I have tried the same example on Hortonworks and after adding the below code to set security protcol, the topology runs fine:
*spoutConfig.securityProtocol = ""SASL_PLAINTEXT"";*
After Adding above code in case of cloudera it gives error: ""Symbol not found""
 
Please let me know if you nedd any other information...
Thanks in advance.."
STORM-2554,Trident Kafka Spout Refactoring to Include Manual Partition Assignment,Incorporate changes done in STORM-2541 and do some refactoring to internal state partition management to make it cleaner and more properly handle partitions reassignment.
STORM-2550,Supervisor dies if a worker dies in Windows,"When both the Supervisor and Workers are running on a Windows Server 2012 R2, killing a worker using the task manager will cause the Supervisor to die a while after.

It's important to mention that the supervisor.log shows nothing to explain this before it dies. Here are the final logs:

{code:java}
2017-06-13 11:28:40.460 o.a.s.u.Utils SLOT_6705 [INFO] Error when trying to kill 16152. Process is probably already dead.
2017-06-13 11:28:40.460 o.a.s.u.Utils SLOT_6700 [INFO] Error when trying to kill 12048. Process is probably already dead.
2017-06-13 11:28:40.461 o.a.s.u.Utils SLOT_6704 [INFO] Error when trying to kill 6796. Process is probably already dead.
2017-06-13 11:28:40.476 o.a.s.u.Utils SLOT_6709 [INFO] Error when trying to kill 6264. Process is probably already dead.
2017-06-13 11:28:40.673 o.a.s.u.Utils SLOT_6706 [INFO] Error when trying to kill 3932. Process is probably already dead.
2017-06-13 11:28:40.674 o.a.s.u.Utils SLOT_6707 [INFO] Error when trying to kill 1324. Process is probably already dead.
2017-06-13 11:28:42.144 o.a.s.d.s.Container SLOT_6708 [INFO] Found 3868 running as SYSTEM, but expected it to be root
2017-06-13 11:28:42.144 o.a.s.d.s.Slot SLOT_6708 [WARN] SLOT 6708 all processes are dead...
{code}
"
STORM-2547,Apache Metron stream: default not found,"Hi together,
we tried to install Metron 0.4.0 accordingly to this tutorial: https://community.hortonworks.com/articles/88843/manually-installing-apache-metron-on-ubuntu-1404.html
The installation itself worked, but during step “Smoke Test Metron” two main problems aroused:
In the storm topology profiler the splitterBolt doesn’t work. The kafkaSpout receives data and forwards it to the splitterBolt which works but splitterBolt can’t handle the data.
We got the following exception:
Exception in thread ""main"" java.lang.IllegalArgumentException: stream: default not found
        at org.apache.storm.utils.Monitor.metrics(Monitor.java:223)
        at org.apache.storm.utils.Monitor.metrics(Monitor.java:159)
        at org.apache.storm.command.monitor$_main.doInvoke(monitor.clj:36)
        at clojure.lang.RestFn.applyTo(RestFn.java:137)
        at org.apache.storm.command.monitor.main(Unknown Source)

The same exception occurs in the topology indexing at the indexingBolt.
All of the other topologies do work. Also, we receive data in HDFS but not in Elasticsearch. 
We’d appreciate any tips!
Thanks in advance! :) "
STORM-2545,BaseConfigurationDeclarer should not ignore null values,"Currently the BaseConfigurationDeclarer class allows you to set memory requests that are null, and simply ignores them.  It would be better to throw an NPE if they are null."
STORM-2540,Get rid of window compaction in WindowManager,"Storm's windowing support uses trigger and eviction policies to control the size of the windows passed to WindowingBolts. The WindowManager has a hard coded limit of 100 tuples before tuples will start getting evicted from the window, probably as an attempt to avoid overly huge windows when using time based eviction policies. Whenever a tuple is added to the window, the hard cap is checked, and if the number of tuples in the window exceeds the cap the WindowManager evaluates the EvictionPolicy for the tuples to figure out if some can be removed.

This hard cap is ineffective in most configurations, and has a surprising interaction with the count based policy.

If the windowing bolt is configured to use timestamp fields in the tuples to determine the current time, the WatermarkingXPolicy classes are used. In this configuration, the compaction isn't doing anything because tuples cannot be evicted until the WatermarkGenerator sends a new watermark, and when it does the TriggerPolicy causes the WindowManager to evict any expired tuples anyway.

If the windowing bolt is using the count based policy, compaction has the unexpected effect of hard capping the user's configured max count to 100. If the configured count is less than 100, the compaction again has no effect.

When the bolt is configured to use the tuple arrival time based policy, the compaction only has an effect if there are tuples older than the configured window duration, which only happens if the window happens to trigger slightly late. This can cause tuples to be evicted from the window before the user's bolt sees them. Even when tuples are evicted with the compaction mechanism they are kept in memory until the next time a window is presented to the user's bolt.

I think the compaction mechanism should be removed. The only policy that benefits is the time based policy, and in that case it would be better to just add a configurable max tuple count to that policy. "
STORM-2538,New kafka spout emits duplicate tuples,"Currently, KafkaSpout in storm-kafka-client can cause duplicate tuples to be emitted. Reason is the implementation of ConsumerRebalanceListener interface is called by kafka everytime a new executor comes up. However, on PartitionsRevoked we already have some in flight tuples and are emitting the same ones from the new executor on which the onPartitionsAssigned was called. We need to make sure that we emit only one tuple per kafka message."
STORM-2531,CheckPointState.Action should have a serializer registered,CheckPointStae.Action does is not registered in Kryo so users of stateful topologies must register it.  Like other core classes this should be registered by Storm at startup.
STORM-2530,Make trident Kafka Spout log its assigned partition ,Include taskID in logs generated by trident Kafka Spout. This is to maintain consistency between the plain Kafka Spout and trident Kafka Spout. Refer to [STORM-2506|https://issues.apache.org/jira/browse/STORM-2506] for detailed information about exact log lines where taskID field was added. 
STORM-2529,KeyNotFoundException on topology undeploy,"On undeploying a topology, the following is logged in the nimbus log:

{code}2017-05-23T11:59:17,785 o.a.s.d.nimbus [INFO] Killing topology: <topology>
2017-05-23T11:59:18,284 o.a.s.d.nimbus [INFO] Cleaning up <topology>
2017-05-23T11:59:18,359 o.a.s.d.nimbus [INFO] Removing dependency jars from blobs - 
2017-05-23T11:59:28,498 o.a.s.d.nimbus [INFO] Cleaning up <topology>
2017-05-23T11:59:28,515 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:<topology>-stormcode.ser){code}

This causes the client making the thrift call to nimbus to undeploy the topology to block indefinitely"
STORM-2523,Trident : Cannot join two streams if only one is passed through a persistant aggregator.,"When i define a topology such :
Spout1 -> persistantAggregator -> Stream ->  join
Spout2 ->  Join (the same one)

Join never produces data.

To produce data i need to add a persistantAggregator in other stream.
This  persistantAggregator aggregate nothing and just repeat input tuple.


I join a java class testing each configuration

thx
                    "
STORM-2514,Incorrect logs for mapping between Kafka partitions and task IDs,"While working on [STORM-2506|https://issues.apache.org/jira/browse/STORM-2506], the worker logs were generated with debug mode on. The information printed about mapping between Task IDs and kafka partitions was contradictory to my assumptions. I ran a topology which used KafkaSpout from the storm-kafka-client module, it had a parallelism hint of 2 (number of executors) and a total of 16 tasks. 
The log lines mentioned below show assigned mapping between executors and kafka partitions:
{noformat}
o.a.k.c.c.i.ConsumerCoordinator Thread-12-kafkaspout-executor[3 10] [INFO] Setting newly assigned partitions [8topic-4, 8topic-6, 8topic-5, 8topic-7] for group kafkaSpoutTestGroup
o.a.s.k.s.KafkaSpout Thread-12-kafkaspout-executor[3 10] [INFO] Partitions reassignment. [taskID=10, consumer-group=kafkaSpoutTestGroup, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@108e79ce, topic-partitions=[8topic-4, 8topic-6, 8topic-5, 8topic-7]]
o.a.k.c.c.i.ConsumerCoordinator Thread-8-kafkaspout-executor[11 18] [INFO] Setting newly assigned partitions [8topic-2, 8topic-1, 8topic-3, 8topic-0] for group kafkaSpoutTestGroup
o.a.s.k.s.KafkaSpout Thread-8-kafkaspout-executor[11 18] [INFO] Partitions reassignment. [taskID=15, consumer-group=kafkaSpoutTestGroup, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@2dc37126, topic-partitions=[8topic-2, 8topic-1, 8topic-3, 8topic-0]]
{noformat}

It is evident that only tasks (with ID 3, 4, 5, 6, 7, 8, 9, 10) in Executor1 (3 10) will be reading from kafka partitions 4, 5, 6 and 7. Similarly, tasks in Executor2 (11 18) will be reading from kafka partitions 0, 1, 2 and 3. These log lines are being printed by Tasks with IDs 10 and 15 in respective executors. 

Logs which emit individual messages do not abide by the above assumption. For example in the log mentioned below, Task ID 3 (added code, as a part of debugging STORM-2506, to print the Task ID right next to component ID) which runs on Executor1 reads from partition 2 (the second value inside the square brackets), instead of 4, 5, 6 or 7. 

{noformat}Thread-12-kafkaspout-executor[3 10] [INFO] Emitting: kafkaspout 3 default [8topic, 2, 0, null, 1]{noformat}

This behavior has been summarized in the table below : 
{noformat}
Task IDs ------- 3, 4, 7, 8, 9, 11, 15, 18 ------------ Partitions 0, 1, 2, 3
Task IDs ------- 5, 6, 10, 12, 13, 14, 16, 17 --------- Partition 4, 5, 6, 7
{noformat}

[You can find the relevant parts of log file here.|https://gist.github.com/srishtyagrawal/f7c53db6b8391e2c3bd522afc93b5351] 

Am I misunderstanding something here? Do tasks {{5, 6, 10, 12, 13, 14, 16, 17}} correspond to executor1 and {{3, 4, 7, 8, 9, 11, 15, 18}} correspond to executor2? Are (3 10) not the starting and ending task IDs in Executor1? 

Another interesting thing to note is that, Task IDs 10 and 15 are always reading from the partitions they claimed to be reading from (while setting partition assignments). 

If my assumptions are correct, there is a bug in the way the mapping information is being/passed to worker logs. If not, we need to make changes in our docs."
STORM-2513,NPE possible in getLeader call,"The getLeader call actually reads data from two different locations

https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L2371-L2385

One is /leader-lock and the other is /nimbuses.  There is a really rare possibility that these two can get out of sync when the leader crashes and we read from leader election saying it is still the leader, but after that it's entry is removed from ZK for /nimbuses.  So we either need to make them not be separate entries, or we need to add in some kind of a retry when this happens.

Also NimbusClient has not retry built in.  Not all operations are idempotent, but we really should look at adding a retry with possibly switching to a new nimbus on idempotent operations."
STORM-2509,Writers in AbstractHDFSBolt are not closed/rotated when evicted from WritersMap,"When the eldest entry in the WritersMap in the AbstractHDFSBolt gets removed due to the number of writers exceeding maxWriters (see below), the writer is not closed and rotation actions are not executed (doRotationAndRemoveWriter is not called).

{code}
static class WritersMap extends LinkedHashMap<String, AbstractHDFSWriter> {
    final long maxWriters;

    public WritersMap(long maxWriters) {
        super((int)maxWriters, 0.75f, true);
        this.maxWriters = maxWriters;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<String, AbstractHDFSWriter> eldest) {
        return this.size() > this.maxWriters;
    }
}
{code}"
STORM-2508,"storm-solr enhancement: update solrj to 5.5, support custom SolrClientFactory and commit operation","I have a case that the SolrCloud in my organization is protected by SSL + Basic Auth, so I need to provide customized SolrClient instance to SolrUpdateBolt. Likewise, the RestJsonSchemaBuilder cannot access Schema API without auth.
In addition, for ""near real-time search"", soft commit is preferred, so it is better to expose an interface for user to customize commit operation.
I think those features will also be useful for other people who needs to use custom SolrClient implementation and control the detail of commit operation. So, finally, I plan an enhancement for storm-solr:
1. update solrj and related dependencies to 5.5;
2. improve SolrConfig to support custom SolrClientFactory and CommitCallBack;
3. provide a SchemaBuilder implementation which use custom SolrClient to request Schema API."
STORM-2507,Master branch build failure due to additional checkstyle violations in storm-webapp module,"Master branch build failure due to additional checkstyle violations in storm-webapp module

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project storm-webapp: You have 314 Checkstyle violations. The maximum number of allowed violations is 290. -> [Help 1]"
STORM-2504,"At the start of topology, the KafkaTridentSpoutOpaque will sometimes emit the first batch twice ","The unit test in the attachment can reproduce the problem, and there is my simple fix."
STORM-2502,Use PID in file name for heap dumps,"The default JVM options for the workers specify the path to use for the heap dumps, see 
[https://github.com/apache/storm/blob/1.x-branch/conf/defaults.yaml#L171], however when a memory error happens more than once for the same worker, only the first dump is kept as the file can't be overridden. Instead, would it make sense to use something like 
_""-XX:HeapDumpPath=artifacts/heapdump_<pid>.hprof""_ so that a different dump is generated for each JVM instance? Or is the current pattern used on purpose to avoid too much disk space being used?"
STORM-2499,Add Serialization plugin for EventHub System Properties,The current EventHub Serialization Scheme does not serialize systemproperties of eventhub. There has been a push to get this as an out of the box option. 
STORM-2487,getting com.mongodb.MongoBulkWriteException while trying to save bulk messages using apache storm mongo,"While trying to save bulk numbers of messages by using storm-mongo, we are getting below exception

com.mongodb.MongoBulkWriteException: Bulk write operation error on server mongoserver:27017. Write errors: [BulkWriteError{index=0, code=11000, message='E11000 duplicate key error collection: NextMDC.EMAIL index: _id_ dup key: { : ""22596
079-1260-44f1-b4df-a5857f48f22d"" }', details={ }}].
        at com.mongodb.connection.BulkWriteBatchCombiner.getError(BulkWriteBatchCombiner.java:176) ~[stormjar.jar:?]
        at com.mongodb.connection.BulkWriteBatchCombiner.throwOnError(BulkWriteBatchCombiner.java:205) ~[stormjar.jar:?]
        at com.mongodb.connection.BulkWriteBatchCombiner.getResult(BulkWriteBatchCombiner.java:146) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:190) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:168) ~[stormjar.jar:?]
        at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:230) ~[stormjar.jar:?]
        at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:221) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:168) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:74) ~[stormjar.jar:?]
        at com.mongodb.Mongo.execute(Mongo.java:781) ~[stormjar.jar:?]
        at com.mongodb.Mongo$2.execute(Mongo.java:764) ~[stormjar.jar:?]
        at com.mongodb.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:323) ~[stormjar.jar:?]
        at org.apache.storm.mongodb.common.MongoDBClient.insert(MongoDBClient.java:61) ~[stormjar.jar:?]
        at org.apache.storm.mongodb.bolt.MongoInsertBolt.execute(MongoInsertBolt.java:85) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:464) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]

But for less number of messages it is working fine.
"
STORM-2485,re-include example jars in storm distribution,"Beginning with Apache 1.1.0, it appears the example jar ""examples/storm-starter/storm-starter-topologies-$STORMVERSION.jar"" is no longer included in the distribution.

I maintain a project that has a series of ""sanity tests"" when new versions of projects came out.  The WordCountTopology example in the storm-start topology was the ""sanity test"" we used.

It would certainly be nice to have the jar re-included as it would automatically allow the tests to be run.  I'm sure for others, it is a quick way to try out Storm rather than have to build the jars by hand."
STORM-2483,wrong parameters order,"org.apache.storm.utils.Utils#getGlobalStreamId has wrong parameters order:
    
public static GlobalStreamId getGlobalStreamId(String streamId, String componentId) {
        if (componentId == null) {
            return new GlobalStreamId(streamId, DEFAULT_STREAM_ID);
        }
        return new GlobalStreamId(streamId, componentId);
    }

but GlobalStreamId constructor is:   public GlobalStreamId(
    String componentId,
    String streamId)

so i think the nice code is:
    public static GlobalStreamId getGlobalStreamId(String streamId, String componentId) {
        if (streamId == null) {
            return new GlobalStreamId(componentId, DEFAULT_STREAM_ID);
        }
        return new GlobalStreamId(componentId, streamId);
    }
"
STORM-2473,KafkaTridentSpoutOpaque's implementation is incorrect.,"The coordinator relies on emitter to start subscription, this is in correct since coordinator and emitter may run on different machines."
STORM-2472,kafkaspout should work normally in kerberos mode with kafka 0.9.x API,"storm 1.0.x-branch didn't support kafkaspout to consume from kafka in kerberos mode with we can't  set kafka's parameter ,'java.security.auth.login.config', in storm's process .So I solve it via preparing a kafkaspout."
STORM-2470,kafkaspout should support kerberos,kafkaspout should  work normally when the cluster(storm and kafka) is in kerberos mode.
STORM-2466,The example of jaas.conf in jaas_kerberos.conf should give more details ,"This documentation lacks several section such as ""Client"" and ""Server"".The Client section is used by processes wanting to talk to ZooKeeper and really only needs to be included with nimbus and the supervisors.The Server section is used by the ZooKeeper servers. ""jaas_kerberos.conf"" should give users a good lead,so it should should give more details as SECURITY.md .Also,Having unused sections in the jaas is not a problem.So I think The example of jaas.conf in jaas_kerberos.conf should be the same as the PR."
STORM-2462,Regex transformation for KerberosPrincipalToLocal,"In some environments it may be the case that the local unix usernames are different than the ones in Kerberos account. This feature lets you override the username and map it to the one locally available.

Added settings:
storm.principal.mapper.regex = what to replace in the existing principal
storm.principal.mapper.replacement = what to replace in the existing principal with

e.g.

Kerberos principal is 0001@storm.apache.org 
storm.principal.mapper.regex=^
storm.principal.mapper.replacement=A

Would result in local principal=A0001"
STORM-2460,Test with Storm testings completeTopology and Maven surefire fail,"Running tests that use Storm testings completeTopology fail when running with Maven surefire in some environments.

Some tests are run successfully and it is not always the same phase of tests that fail.

It seems to be issue similar to STORM-130."
STORM-2459,Support SSL for Redis (Jedis 2.9.0),"Jedis 2.9.0 added SSL support.  This helps with connecting to hosted Redis environments, such as in Azure, which are SSL-only by default.

However, the Redis support in Storm doesn't currently expose an option to use this.  I would hope for something like:

JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
    .setHost(host)
    .setPort(port)
    .useSSL(true)
    .build();

Thanks."
STORM-2456,Error when running Flux on Windows in non-elevated command prompt,"Running:
{code}
mvn compile exec:java -Dexec.args=""--local -R /topology.yaml""
{code}

I get the following errors.

It works if I run in an elevated command prompt though.

{code}
17:22:48 [SLOT_1027] ERROR org.apache.storm.daemon.supervisor.Slot - Error when processing event
java.nio.file.FileSystemException: C:\Users\maurgi\AppData\Local\Temp\0924ce14-5d18-4da2-ab49-abfe37e59742\workers\c487e249-7225-4836-b7f0-b840f1a05732\artifacts: A required privilege is
 not held by the client.
        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsFileSystemProvider.createSymbolicLink(WindowsFileSystemProvider.java:585) ~[?:1.8.0_121]
        at java.nio.file.Files.createSymbolicLink(Files.java:1043) ~[?:1.8.0_121]
        at org.apache.storm.daemon.supervisor.AdvancedFSOps.createSymlink(AdvancedFSOps.java:354) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Container.createArtifactsLink(Container.java:383) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Container.setup(Container.java:321) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.LocalContainerLauncher.launchContainer(LocalContainerLauncher.java:44) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:387) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:741) [storm-core-1.0.3.jar:1.0.3]
17:22:48 [SLOT_1027] ERROR org.apache.storm.utils.Utils - Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1749) [storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) [storm-core-1.0.3.jar:1.0.3]
{code}"
STORM-2452,Storm Metric classes are not thread safe,Classes in org.apache.storm.metric.api are not thread-safe.
STORM-2446,Add more ml algorithm to storm-ml as a new external,"Create a new external named storm-ml that contained not only pmml but also other ml algorithm like recommend ,linear and so on."
STORM-2444,Nimbus sometimes throws NPE when clicking show topology visualization button,"Here's error message from Nimbus (containing stack trace): 

{code}
{""error"":""Internal Server Error"",""errorMessage"":""java.lang.NullPointerException\n\tat org.apache.storm.stats.StatsUtil.mergeWithAddPair(StatsUtil.java:1997)\n\tat org.apache.storm.stats.StatsUtil.expandAveragesSeq(StatsUtil.java:2511)\n\tat org.apache.storm.stats.StatsUtil.aggregateAverages(StatsUtil.java:877)\n\tat org.apache.storm.stats.StatsUtil.aggregateBoltStats(StatsUtil.java:776)\n\tat org.apache.storm.stats.StatsUtil.boltStreamsStats(StatsUtil.java:942)\n\tat org.apache.storm.ui.core$visualization_data$iter__3002__3006$fn__3007.invoke(core.clj:239)\n\tat clojure.lang.LazySeq.sval(LazySeq.java:40)\n\tat clojure.lang.LazySeq.seq(LazySeq.java:49)\n\tat clojure.lang.Cons.next(Cons.java:39)\n\tat clojure.lang.RT.next(RT.java:674)\n\tat clojure.core$next__4112.invoke(core.clj:64)\n\tat clojure.core$dorun.invoke(core.clj:3010)\n\tat clojure.core$doall.invoke(core.clj:3025)\n\tat org.apache.storm.ui.core$visualization_data.invoke(core.clj:268)\n\tat org.apache.storm.ui.core$build_visualization.invoke(core.clj:591)\n\tat org.apache.storm.ui.core$fn__3641.invoke(core.clj:1204)\n\tat org.apache.storm.shade.compojure.core$make_route$fn__324.invoke(core.clj:100)\n\tat org.apache.storm.shade.compojure.core$if_route$fn__312.invoke(core.clj:46)\n\tat org.apache.storm.shade.compojure.core$if_method$fn__305.invoke(core.clj:31)\n\tat org.apache.storm.shade.compojure.core$routing$fn__330.invoke(core.clj:113)\n\tat clojure.core$some.invoke(core.clj:2570)\n\tat org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)\n\tat clojure.lang.RestFn.applyTo(RestFn.java:139)\n\tat clojure.core$apply.invoke(core.clj:632)\n\tat org.apache.storm.shade.compojure.core$routes$fn__334.invoke(core.clj:118)\n\tat org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1383.invoke(json.clj:56)\n\tat org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__918.invoke(multipart_params.clj:118)\n\tat org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__747.invoke(reload.clj:22)\n\tat org.apache.storm.ui.helpers$requests_middleware$fn__2903.invoke(helpers.clj:54)\n\tat org.apache.storm.ui.core$catch_errors$fn__3813.invoke(core.clj:1462)\n\tat org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2632.invoke(keyword_params.clj:35)\n\tat org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2675.invoke(nested_params.clj:84)\n\tat org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2604.invoke(params.clj:64)\n\tat org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__918.invoke(multipart_params.clj:118)\n\tat org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2890.invoke(flash.clj:35)\n\tat org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2876.invoke(session.clj:98)\n\tat org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2498.invoke(servlet.clj:127)\n\tat org.apache.storm.shade.ring.util.servlet$servlet$fn__2502.invoke(servlet.clj:136)\n\tat org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)\n\tat org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)\n\tat org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)\n\tat org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)\n\tat org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\n""}
{code}"
STORM-2439,HealthCheck feature does not work,"There are a few issues with this feature:

1. The default timeout value produces `java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long at org.apache.storm.command.HealthCheck.processScript(HealthCheck.java:79)` because the value, 5000, is automatically deserialized by Jackson as an Integer, but we attempt to cast it to a long. (I successfully worked around this by setting a timeout greater than the maximum int.)

2. The documentation says that a script should print ""ERROR"" if the node is unhealthy, but in fact the script must *also* exit with a non-zero exit code. This appears to be the opposite of what is intended, given a comment that says ""We treat non-zero exit codes as indicators that the scripts failed to execute properly, not that the system is unhealthy"". I believe the test in this line is inverted: https://github.com/apache/storm/blob/70102643e74d577728adf5f8719920d1bf60e98a/storm-core/src/jvm/org/apache/storm/command/HealthCheck.java#L97

3. Even with workarounds for the above two bugs, a failing health check does not cause workers to shut down in my testing with Storm 1.0.3. I have not determined the cause, but because the previous two issues suggest to me that this code is rarely if ever tested, I do not plan to investigate further at the moment.

If this feature is, as it appears, untested and non-functional, I would suggest that it be removed from the code and documentation."
STORM-2437,LocalCluster in Unit Test crash the VM,"When unit testing Storm, we use LocalCluster. There is nothing to say when the Unit Test is working, the Unit Test ends gracefully.

However, when there are RuntimeException, for instance in the prepare functions, Storm crash and calls in Utils.mkSuicideFn, which calls Runtime.getRuntime().exit. So the VM crash and this is contradictory to Maven Surefire design (http://maven.apache.org/surefire/maven-surefire-plugin/faq.html#vm-termination).

I searched many ways to either prevent Storm from exiting (using SecurityManager), or make Unit Test accept the crash of the forked process.

If the Unit Test s VM crash, surefire will be unable to continue. My suggestion is to allow a configuration of LocalCluster that avoids System.exit, but just kills the topology (and closes all ressources if possible, but in the short term, this is not really important in a forked process)."
STORM-2434,Storm spout is not reading/emitting data in storm cluster mode (version 1.0.0),"I am using apache storm 1.0.0 both in local as well as cluster mode. For the spout, I am reading the data from kafka topic (I am using kafka 2.11-0.8.2.1). Spout is reading the data from kafka topic and also emitting the data when I am using storm in local mode but the storm spout is not emitting any data when I am running storm in cluster mode.

My topology implementation for reading kafka data is as follow:

brokerHosts = new ZkHosts(kafkaZookeeper);
SpoutConfig kafkaConfig = new SpoutConfig(brokerHosts, kafkaTopicIn, """", ""storm"");
kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout(""spout"", new KafkaSpout(kafkaConfig), 2);

My storm configuration file:
storm.zookeeper.servers:
- ""localhost""
storm.zookeeper.port: 2181
nimbus.seeds: [""localhost""]

storm.local.dir: ""/tmp/storm""

I am also not getting any error while submitting Storm topology in cluster mode.

Any idea why topology spout is not emitting any data in cluster mode ??
Any help would be greatly appreciated."
STORM-2433,Dup Execution due to Topology Error,"We are using storm 1.0.1, at times we get below errors in the topology after topology running for a long time (e.g. 20 days), which resulting dup executions. Seems to be some race condition. Could not find a good way to debug, can someone help out?"
STORM-2430,Potential Race condition in Kafka Spout,"Kafka spout hangs when the number of uncommitted messages exceeds the max allowed uncommitted messages and some intermediate tuples have failed in down stream bolt.

Steps of reproduction.
Create a simple topology with one kafka spout and a slow bolt. 
In kafka spout set the maximum uncommitted messages to a small number like 100.
Bolt should process 10 tuples in second. And program it to fail on some random tuples. For eg: say tuple number 10 fails. Also assume  there is only 1 Kafka partition the spout reads from.

Spout on first execution of nextTuple() gets 110 records and emits them. At this point number of uncommitted message would be 110.
First 9 tuples are acked by the bolt. 10th tuple is failed by the bolt. KafkaSpout puts it on retry queue.
Tuple number 11 to 110 are acked by bolt . But spout only commits till offset 9.[link | https://github.com/apache/storm/blob/1.0.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L510]

Now, the number of uncommitted  messages = 110 - 9 = 101 > 100 (max allowed uncommitted messages)
No new records are polled from kafka.[link | https://github.com/apache/storm/blob/1.0.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L239]. The spout is stuck as the nothing is polled. 

Solution is to explicitly go through retry queue explicitly and emit tuples that are ready on every nextTuple().
"
STORM-2428,Flux-core jar contains unpacked dependencies,"The jar file for flux-core contains classes from /org/apache/http/. This was not the case before and causes problems with projects which rely on a different version of http-client. 
I can't see any references to http-client in the pom though."
STORM-2424,Supervisor fails silently if started with old supervisor/localstate content,"If the following method in LocalState encounters an Exception and throws a RuntimeException, the supervisor quits silently without generating an error. I had to debug this by connecting with a remote debugger. Instead the method should generate an error to the user as to the source of the error. In my case, because I was upgrading my installation, the problem was due to a missing parameter in the content under the supervisor/localstate directory.

    private Map<String, ThriftSerializedObject> partialDeserializeLatestVersion(TDeserializer td) {
        try {
            String latestPath = _vs.mostRecentVersionPath();
            Map<String, ThriftSerializedObject> result = new HashMap<>();
            if (latestPath != null) {
                byte[] serialized = FileUtils.readFileToByteArray(new File(latestPath));
                if (serialized.length == 0) {
                    LOG.warn(""LocalState file '{}' contained no data, resetting state"", latestPath);
                } else {
                    if (td == null) {
                        td = new TDeserializer();
                    }
                    LocalStateData data = new LocalStateData();
                    td.deserialize(data, serialized);
                    result = data.get_serialized_parts();
                }
            }
            return result;
        } catch(Exception e) {
            throw new RuntimeException(e);
        }
    }"
STORM-2419,Fix storm-perf module's parent module version and path.,
STORM-2418,add proper log message on metrics server fails/absent,"my metrics consumer writes to opentsdb server.
when this server went down, i see the following error message in my storm jobs.
which is very misleading.
create a ticket, so we can address this in future versions better.

{code:language=java}
2017-03-16 01:22:43.648 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) [storm-core-0.10.0.jar:0.10.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
Caused by: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
	at backtype.storm.util$get_iterator.invoke(util.clj:929) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:432) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:127) ~[storm-core-0.10.0.jar:0.10.0]
	... 6 more
{code}"
STORM-2417,Storm kafka monitor throws IllegalArgumentException if there is no data in kafka topic,If a topology has a kafka spout in it and if kafka topic does not have any data or topology has not committed offsets to zk node then storm-kafka-monitor throws an IllegalArgumentException and it shows up in UI. While this is not breaking any functionality showing a better message can be displayed
STORM-2415,Storm fails to properly handle Zookeeper hosts going down,"We run a storm cluster (v.1.0.3) on AWS and have 3 Zookeepers supporting it. Because AWS sometimes terminates VMs, we sometimes lose a Zookeeper instance. When this happens, the hostname cannot be resolved for that zookeeper instance as AWS has taken the VM away. We noticed that in this case storm fails to connect to zookeeper – even though there are still 2 Zookeeper instances running. It fails with an exception something like:
{noformat}
java.net.UnknownHostException: zookeeper3
  at java.net.InetAddress.getAllByName0(InetAddress.java:1280) 
  at java.net.InetAddress.getAllByName(InetAddress.java:1192) 
  at java.net.InetAddress.getAllByName(InetAddress.java:1126) 
  at org.apache.storm.shade.org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61) 
  at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445) 
  at org.apache.storm.shade.org.apache.curator.utils.DefaultZookeeperFactory.newZooKeeper(DefaultZookeeperFactory.java:29) 
  at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$2.newZooKeeper(CuratorFrameworkImpl.java:150) 
  at org.apache.storm.shade.org.apache.curator.HandleHolder$1.getZooKeeper(HandleHolder.java:94) 
  at org.apache.storm.shade.org.apache.curator.HandleHolder.getZooKeeper(HandleHolder.java:55) 
  at org.apache.storm.shade.org.apache.curator.ConnectionState.reset(ConnectionState.java:218) 
  at org.apache.storm.shade.org.apache.curator.ConnectionState.start(ConnectionState.java:103) 
  at org.apache.storm.shade.org.apache.curator.CuratorZookeeperClient.start(CuratorZookeeperClient.java:190) 
  at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:259) 
  at org.apache.storm.zookeeper$mk_client.doInvoke(zookeeper.clj:86) 
  at clojure.lang.RestFn.invoke(RestFn.java:494)
  at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState.invoke(zookeeper_state_factory.clj:28) 
  at org.apache.storm.cluster_state.zookeeper_state_factory.mkState(Unknown Source) 
  <SNIP REST OF STACKTRACE>
{noformat}
Having done some research it looks like this error is caused by a bug in the Zookeeper client library. There is an issue for it here:
[https://issues.apache.org/jira/browse/ZOOKEEPER-1576]
This issue has been resolved in the version 3.5.x branch of Zookeeper. However, after 2.5 years and 3 releases the 3.5.x branch of Zookeeper is still in Alpha .
Despite the fact that it is in alpha, there is a branch of Curator (v.3.x.x) that uses it, but Storm uses Curator version 2.x.x – possibly because it doesn’t rely on alpha code. So the bug is still unpatched in Storm.
I realise that an upgrade to alpha code may be too much of a risk, but this problem is a serious issue for those running Storm in a containerised or cloud environment - so perhaps it may be worth considering?"
STORM-2405,[Storm SQL] Support TUMBLE and HOP window,"Calcite 1.12.0 will support tumble and hop window via CALCITE-1603 and CALCITE-1615.

Storm can utilize this to support grouping by tumbling and sliding window 
(Hop window is what we're saying sliding window in Storm. Sliding window in Calcite has different meaning.)

Before addressing this issue, it might need to change underlying API of Storm SQL. We dropped grouping and join features from Storm SQL to not relying on Trident semantic (within micro batch), so this feature should be implemented under tuple-by-tuple manner. Either core API or streams API would be good, and streams API should be easier to change since it can (and should) cover Trident API.

TUMBLE_START, TUMBLE_END, HOP_START, HOP_END might be implemented from another issue."
STORM-2404,Give exactly once support in storm core.,
STORM-2402,KafkaSpout sub-classes should be able to customize tuple processing,"We need a {{KafkaSpout}} that writes unprocessable records to a ""dead-letter-topic"". For this to function we sub-classed {{KafkaSpout}} and added the corresponding code. Without the incoming patch sub-classses can not have access to the actual tuples/records but just the {{KafkaSpoutMessageId}} in {{ack()}} and {{fail()}}."
STORM-2401,org.apache.storm.deamon.supervisor can not be found,"org.apache.storm.deamon.supervisor can not be found.
:supervisor
  set CLASS=org.apache.storm.daemon.supervisor

 in storm-core-1.0.3.jar . I can not find org.apache.storm.deamon.supervisor class, but can find org.apache.storm.deamon.supervisor$_main.class."
STORM-2399,How to configure different logback.xml in storm-project for each topology in the cluster.?,"I hava a storm cluster that runs multiple Topologies ,  I changed the ${storm_dir}/logback/cluster.xml in supervisor machines, so my topologies are using the same log configurations, how to configure the logback.xml in different project instead of changing it in storm-cluser? "
STORM-2398,some questions about storm distributed cache,
STORM-2394,KafkaSpout: Has no leader of partitions for a short time,"In our case, there is something wrong with network for a short time. So some partitions of Kafka have no leaders.
The nextTuple of KafkaSpout throw an exception of ""No leader found for partition 0"" at the position of ""_coordinator.refresh();"". The exception is from the function getLeaderFor in DynamicBrokersReader.java. So the spout is hanged.
The partitions of Kafka have recover for a short time. But the spout can not deal with this problem. This problem appears several times on our server. Such as:
Feb 25 06:31:19 CST 2017, KafkaSpout threw the exception.
Feb 25 06:31:21 CST 2017, Kafka partitions recoverd.
To be stronger, I think that the ""_coordinator.refresh();"" can try times. At the last time, throw the exception. Anyway, it will die, why not try one more time?"
STORM-2393,Support Hortonworks schema registry with HDFS connector,
STORM-2392,Thrift source code generated by Storm not found,"In Maven, we can find storm-core-sources.jar, and this file contains Storm sources files. But it does not contains the Thrift source code, that Storm renamed in order to have multiple Thrift in Storm. We need source code to be in this Jar, so something should be done by storm assembly that generated Thrift code, so that these considered as source code by Maven when it deploys to Nexus.

Use case: I had to dig inside of Thrift Storm github page, download the matching branch, etc., rename the package because of thrift7 instead of thrift package etc. There are many things to be done in order to investigate Storm code, while it is much much simpler if Storm includes Thrift source natively."
STORM-2390,The storm-*-examples jars are missing in the binary distro,
STORM-2384,Add a log statement when spout skips calling nextTuple.,"I have come across threads where people ask questions about a topology being stuck because the spout isn't emitting anything. Having spent considerable time debugging this myself, I think adding a log statement for the case when spout skips calling nextTuple() because maxSpoutPending is reached or because throttling is on could save many developer hours."
STORM-2382,log4j and slf4j conflicting libraries issue,"my project storm 1.0.1 job's dependencies (log4j & slf4j) conflict with apache&hdp storm 1.0.1 default libraries (STORM_HOME/lib) and is preventing submitting new storm job.

* 1. shadow my storm job jar *
I get the following error:

{code:language=java}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.5.0.0-1245/storm/lib/log4j-slf4j-impl-2.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/tmp/356865dafc1a11e69341ecb1d7ac1510.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread ""main"" java.lang.IllegalAccessError: tried to access method org.apache.logging.log4j.core.lookup.MapLookup.newMap(I)Ljava/util/HashMap; from class org.apache.logging.log4j.core.lookup.MainMapLookup
	at org.apache.logging.log4j.core.lookup.MainMapLookup.<clinit>(MainMapLookup.java:37)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.logging.log4j.core.util.ReflectionUtil.instantiate(ReflectionUtil.java:185)
	at org.apache.logging.log4j.core.lookup.Interpolator.<init>(Interpolator.java:65)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:346)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:161)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:359)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:420)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:138)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:147)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:175)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:102)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:43)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:29)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:277)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)
	at org.apache.storm.utils.LocalState.<clinit>(LocalState.java:45)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at org.apache.storm.config__init.__init7(Unknown Source)
	at org.apache.storm.config__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at clojure.lang.RT.loadClassForName(RT.java:2182)
	at clojure.lang.RT.load(RT.java:436)
	at clojure.lang.RT.load(RT.java:412)
	at clojure.core$load$fn__5448.invoke(core.clj:5866)
	at clojure.core$load.doInvoke(core.clj:5865)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5671)
	at clojure.core$load_lib$fn__5397.invoke(core.clj:5711)
	at clojure.core$load_lib.doInvoke(core.clj:5710)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:632)
	at clojure.core$load_libs.doInvoke(core.clj:5753)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:634)
	at clojure.core$use.doInvoke(core.clj:5843)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at org.apache.storm.command.config_value$loading__5340__auto____12764.invoke(config_value.clj:16)
	at org.apache.storm.command.config_value__init.load(Unknown Source)
	at org.apache.storm.command.config_value__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at clojure.lang.RT.loadClassForName(RT.java:2182)
	at clojure.lang.RT.load(RT.java:436)
	at clojure.lang.RT.load(RT.java:412)
	at clojure.core$load$fn__5448.invoke(core.clj:5866)
	at clojure.core$load.doInvoke(core.clj:5865)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.lang.Var.invoke(Var.java:379)
	at org.apache.storm.command.config_value.<clinit>(Unknown Source)
{code}


* 2. upgrade STORM_HOME/lib *
I get this warning.  looks like a racing condition (?)

{code:language=java}
log4j:WARN No appenders could be found for logger (org.apache.storm.utils.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
log4j:WARN No appenders could be found for logger (org.apache.storm.utils.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{code}"
STORM-2380,worker.childopts with whitespace inside one param will be split into pieces,"worker.childopts params with whitespace inside, like -XX:OnError=""pstack %p >~/pstack%p.log"", will be split into pieces for supervisor use string.split(""\\s+"") to split params."
STORM-2378,Implement Kinesis Bolt,We already have kinesis spout in storm at this point. It will be useful to have a kinesis bolt as well that will allow for writing tuples to kinesis streams.
STORM-2377,Storm UI in secure mode should provide read-only option,"Currently, when a user configures storm in secure (kerberos) mode, one way to access the UI is by configuring SPNEGO filter. This requires users to kinit locally and configure browser to do the SPNEGO auth before they can access the UI. It will be useful to have a read-only option which can be configured  and UI will show topologies without the rebalance, activate, deactivate, kill options and only metrics are shown."
STORM-2376,New KafkaSpout security integration tests,"With new KafkaSpout and KafkaBolt using new consumer/producer api, talking to secure Kafka clusters is just a configuration changes. We should add integration tests into storm-kafka-client and add docs on how to enable security for kafka spout and bolt for both SSL & kerberos. Kafka core comes with Integration utils to enable this."
STORM-2375,New KafkaSpout should support reading from user specified offset/timestamp,"With new Kafka consumer API, one can specify a timestamp or an offset to start from. We should provide an option in KafkaSpout that allows the user to specify an offset to start from."
STORM-2373,HDFS Spout should support multiple ignore extensions,Currently hdfs spout supports only .ignore or user provided one extension. It should support multiple extension to be ignored
STORM-2370,Add groups whitelist for nimbus admins,"Akin to the groups based whitelisting in place for logs access, it would be useful to have a groups based authorization mechanism in place for the NIMBUS_ADMINS group.  

In large scale deployments, there are some organizations that already use operating system groups to designate which of their users have privileges on  a given set of hosts. It would be useful, and more secure for them to not have to configure privileges in multiple places."
STORM-2368,[multilang] ShellSpout/ShellBolt doesn't expose the interface to register metrics,"Multi-lang protocol provides the feature to let subprocess passing metric information to its ShellSpout or ShellBolt. 

ShellSpout and ShellBolt are implementing it correctly, but unfortunately they don't expose the way to register metrics which makes the feature not able to use.

ShellSpout and ShellBolt should provide the interface for users to register metrics so that metrics are initialized and registered to the topology context."
STORM-2366,SpoutTracker does not delegate to all methods,"h2. Problem
{{SpoutTracker}} does not implement and delegate to the following methods:

- {{activate}}
- {{deactivate}}
- {{getComponentConfiguration}}

h2. Effect
This causes problems for spouts that require initialization in or to be operational.

h2. Solution
The recommended fix is adding the following:

{code}
    @Override
    public void activate() {
        _delegate.activate();
    }

    @Override
    public void deactivate() {
        _delegate.deactivate();
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        return _delegate.getComponentConfiguration();
    }
{code}
"
STORM-2364,Ensure kafka-monitor jar is included in classpath for UI process,The kafka-monitor jar has been moved into  toollib/ and not featuring in UI process's classpath. Leading to ClassNotFoundException for KafkaOffsetLagUtil class ... seen in the ui.log
STORM-2362,Cassandra Spout,
STORM-2359,Revising Message Timeouts,"A revised strategy for message timeouts is proposed here.

Design Doc:
 https://docs.google.com/document/d/1am1kO7Wmf17U_Vz5_uyBB2OuSsc4TZQWRvbRhX52n5w/edit?usp=sharing"
STORM-2358,Update storm hdfs spout to remove specific implementation handlings,"I was looking at storm hdfs spout code in 1.x branch, I found below
improvements can be made in below code.

  1.  Make org.apache.storm.hdfs.spout.AbstractFileReader as public so
that it can be used in generics.

  2.  org.apache.storm.hdfs.spout.HdfsSpout requires readerType as
String. It will be great to have class<? extends AbstractFileReader>
readerType; So we will not use Class.forName at multiple places also it
will help in below point.

  3.  HdfsSpout also needs to provide outFields which are declared as
constants in each reader(e.g.SequenceFileReader). We can have abstract
API AbstractFileReader in which return them to user to make it generic."
STORM-2356,Storm-HDFS: NPE on empty & stale lock file,"In HDFSSpout a NPE can occur if a stale lock file is empty.

{{LogEntry.deserialize}} tries to split the line by colons.
If the line is null, the split will cause a NPE:
https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java#L179

Moreover the callee of {{getLastEntry}} is also mishandling empty log files.
The {{lastEntry.eventTime}} could also cause a NPE if the above scenario is passed and the log file is empty:
https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java#L149-L160"
STORM-2355,Storm-HDFS: inotify support,"This is a proposal to implement inotify based watch dir monitoring in Storm-HDFS Spout.

*Motivation*
Storm-HDFS's HdfsSpout currently polls the Spout’s input directory using Hadoop's {{FileSystem.listFiles()}}. This operation is expensive since it returns the block locations and all stat information of the files inside the watch directory. Moreover HdfsSpout currently uses only one element of the returned Path list which is inefficient as the rest of the entries are thrown away without processing.
The proposed design provides greater efficiency through the inotify interface and also enables to easier extension of the original ({{listFiles()}} based) monitoring with buffering (see Further work section below). 

*High level design*
Goal is to leverage [HDFS inotify API|http://hadoop.apache.org/docs/current/api//org/apache/hadoop/hdfs/DFSInotifyEventInputStream.html] to monitor new file arrival to HdfsSpout's input directory.
The inotify based monitoring is an addition to the original {{FileSystem.listFiles()}} based implementation, the default behavior of the spout will be unchanged by this modification.

To unify the two monitoring methods and enable buffering an iterator based ({{HdfsDirectoryMonitor}}) class is created.

To retain backward compatibility the HdfsSpout's default monitoring behavior is unchanged, inotify based monitoring could be enabled through a parameter.
As inotify requires administrative privileges (see Caveat section below) a fallback mechanism is be implemented in HdfsSpout to use the original {{listFiles()}} based monitoring if initialization fails for inotify based monitoring.


*Implementation details*
As inotify provides only a delta of the filesystem events from a given Tx Id (of Hdfs Edit Log) it is required to do a {{FileSystem.listFiles()}} based collection during the Spout's initialization to ensure that any left over files are processed.
The inotify based implementation uses HdfsAdmin's [{{DFSInotifyEventInputStream.poll()}}|http://hadoop.apache.org/docs/current/api//org/apache/hadoop/hdfs/DFSInotifyEventInputStream.html#poll--] method to fetch and buffer the list of new files created since the provided Tx Id to {{newFileList}} buffer.
During {{HdfsSpout.nextTuple()}} call one element is taken from the {{newFileList}} buffer and processed by the spout.
The {{newFileList}} buffer is extended with the result of the {{DFSInotifyEventInputStream.poll(lastTxId)}} call in every nextTuple() call.

Since HdfsSpout is able to create it's own {{HdfsAdmin()}} instance there will be no need for the user to do additional initialization for the spout even if inotify is enabled.

*Caveat*
HDFS inotify is currently available through hdfs administrator user only, but there is ongoing discussion in Hadoop community to extend its support to users. See: HDFS-8940 

*Further work*
1) The number of calls to {{DFSInotifyEventInputStream.poll(lastTxId)}} could be further reduced if the locking directory is moved away from the input directory. With the current design updates on the lock dir are also included in the {{newFileList}} buffer hence the buffer will never get completely empty.
2) The original {{listFiles()}} based solution could be improved through {{HdfsDirectoryMonitor}} to buffer and use all the returned items from the work directory, similarly to inotify based monitoring. Such improvement will reduce the number of calls made to namenode. 
These improvements are currently not part of this ticket.

*Error scenarios*
 - Inability of HdfsAdmin instance creation (e.g. lack of privileges):
   The spout falls back to the original {{listFiles()}} based method.

 - Namenode's edit log is not yet open for write during {{HdfsSpout.open()}}:
   The initialization will be postponed to the {{HdfsSpout.nextTuple()}} call(s).

 - Hdfs gets disconnected while the topology is running:
   HdfsSpout reports an error and retries in the next call of nextSpout() call. 
   No data will be skipped as the update will be requested from the last known Tx Id.
 
*Testing related changes*
The {{TestHdfsSpout}} testcase should be parametrized to check for both the poll & inotify based solution.
Additional testcases are added to ensure that inotify is able to pick up any leftover files.

"
STORM-2354,Upgrade to Driver 3.1,"Added a pull request at https://github.com/apache/storm/pull/1936

"
STORM-2351,Unable to build native code on OS X,"Compilation of Storm fails under OS X if native profile is enabled for multiple reasons:

1)
{code}
~/w/storm ❯❯❯ mvn clean install -Pnative -DskipTests
....
....
[INFO] --- exec-maven-plugin:1.2.1:exec (default) @ storm-core ---
cp: illegal option -- u
usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file target_file
       cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file ... target_directory
{code}

The problem is caused by the lack of ""u"" (upgrade) flag of cp.

2)
{code}
[INFO] 	gcc -DPACKAGE_NAME=\""worker-launcher\"" -DPACKAGE_TARNAME=\""worker-launcher\"" -DPACKAGE_VERSION=\""1.0.0\"" -DPACKAGE_STRING=\""worker-launcher\ 1.0.0\"" -DPACKAGE_BUGREPORT=\""user@storm.apache.org\"" -DPACKAGE_URL=\""\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -D__EXTENSIONS__=1 -D_ALL_SOURCE=1 -D_GNU_SOURCE=1 -D_POSIX_PTHREAD_SEMANTICS=1 -D_TANDEM_SOURCE=1 -DPACKAGE=\""worker-launcher\"" -DVERSION=\""1.0.0\"" -DHAVE_UNISTD_H=1 -DHAVE__BOOL=1 -DHAVE_STDBOOL_H=1 -DHAVE_DECL_STRERROR_R=1 -DHAVE_STRERROR_R=1 -DHAVE_MKDIR=1 -DHAVE_UNAME=1 -I.    -I./impl -Wall -g -Werror -DEXEC_CONF_DIR=/etc/storm  -MT impl/worker-launcher.o -MD -MP -MF $depbase.Tpo -c -o impl/worker-launcher.o impl/worker-launcher.c &&\
[INFO] 	mv -f $depbase.Tpo $depbase.Po
[INFO] impl/worker-launcher.c:60:15: error: use of undeclared identifier 'PATH_MAX'
[INFO]   char buffer[PATH_MAX];
[INFO]               ^
[INFO] impl/worker-launcher.c:61:20: error: use of undeclared identifier 'PATH_MAX'
[INFO]   snprintf(buffer, PATH_MAX, ""/proc/%u/exe"", getpid());
[INFO]                    ^
[INFO] impl/worker-launcher.c:62:27: error: use of undeclared identifier 'PATH_MAX'
[INFO]   char *filename = malloc(PATH_MAX);
[INFO]                           ^
[INFO] impl/worker-launcher.c:67:44: error: use of undeclared identifier 'PATH_MAX'
[INFO]   ssize_t len = readlink(buffer, filename, PATH_MAX);
[INFO]                                            ^
[INFO] impl/worker-launcher.c:72:21: error: use of undeclared identifier 'PATH_MAX'
[INFO]   } else if (len >= PATH_MAX) {
[INFO]                     ^
[INFO] impl/worker-launcher.c:74:13: error: use of undeclared identifier 'PATH_MAX'
[INFO]             PATH_MAX, filename, PATH_MAX);
[INFO]             ^
[INFO] impl/worker-launcher.c:74:33: error: use of undeclared identifier 'PATH_MAX'
[INFO]             PATH_MAX, filename, PATH_MAX);
[INFO]                                 ^
[INFO] impl/worker-launcher.c:99:9: error: unused variable 'binary_euid' [-Werror,-Wunused-variable]
[INFO]   uid_t binary_euid = filestat.st_uid; // Binary's user owner
[INFO]         ^
[INFO] impl/worker-launcher.c:450:42: error: use of undeclared identifier 'PATH_MAX'
[INFO]     char *(paths[]) = {strndup(local_dir,PATH_MAX), 0};
[INFO]                                          ^
[INFO] impl/worker-launcher.c:597:42: error: use of undeclared identifier 'PATH_MAX'
[INFO]     char *(paths[]) = {strndup(full_path,PATH_MAX), 0};
[INFO]                                          ^
[INFO] impl/worker-launcher.c:725:3: error: implicit declaration of function 'fcloseall' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
[INFO]   fcloseall();
[INFO]   ^
[INFO] impl/worker-launcher.c:725:3: note: did you mean 'fclose'?
[INFO] /usr/include/stdio.h:232:6: note: 'fclose' declared here
[INFO] int      fclose(FILE *);
[INFO]          ^
[INFO] 11 errors generated.
[INFO] make: *** [impl/worker-launcher.o] Error 1
{code}"
STORM-2348,Capability support in worker-launcher,"worker-launcher elevates it's privileges using {{setuid(0)}} and {{setgid(group_info->gr_gid)}} calls:
https://github.com/apache/storm/blob/master/storm-core/src/native/worker-launcher/impl/main.c#L116-L119

The current implementation does not validate the return value of those calls, rather it checks' the privileges (setuid + root ownership) of the binary through {{check_executor_binary()}}

This approach works correctly, but it could be improved: 
If we'd check the return values of setuid(0) & setgid() and drop the binary check it would be possible to gain elevated privileges using CAP_SETUID & CAP_SETGID. "
STORM-2347,JS errors in Topology Visualization,"On a freshly downloaded 1.0.2 after starting ZK, UI, NM, and SP, submitting a topology the Topology Visualization isn't working and 300+ JS errors are appearing on the console.
No changes to config files were made.
Any help would be appreciated.



{code}
...
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at screenUpdate (arbor.js:33)
gather_stream_count @ visualization.js:314
(anonymous) @ visualization.js:268
(anonymous) @ arbor.js:35
each @ jquery-1.11.1.min.js:2
eachEdge @ arbor.js:35
calculate_total_transmitted @ visualization.js:257
redraw @ visualization.js:54
screenUpdate @ arbor.js:33
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at screenUpdate (arbor.js:33)
gather_stream_count @ visualization.js:314
(anonymous) @ visualization.js:268
(anonymous) @ arbor.js:35
each @ jquery-1.11.1.min.js:2
eachEdge @ arbor.js:35
calculate_total_transmitted @ visualization.js:257
redraw @ visualization.js:54
screenUpdate @ arbor.js:33
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at Object.success (visualization.js:420)
    at j (jquery-1.11.1.min.js:2)
    at Object.fireWith [as resolveWith] (jquery-1.11.1.min.js:2)
...
{code}"
STORM-2342,storm-kafka-client consumer group getting stuck consuming from kafka 0.10.1.1,"I've created a topology that will read from kafka using storm-kafka-client but when it reaches the last message on kafka log it stops consuming and get stuck, new messages are never consumed, here are the kafka logs:
{quote}
[2017-02-03 19:15:26,865] INFO [GroupCoordinator 1002]: Preparing to restabilize group kafka-spout with old generation 29 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:15:26,865] INFO [GroupCoordinator 1002]: Stabilized group kafka-spout generation 30 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:15:26,868] INFO [GroupCoordinator 1002]: Assignment received from leader for group kafka-spout for generation 30 (kafka.coordinator.GroupCoordinator)
{quote}
========= here storm starts consuming messages, then, when it hits the last message, I can see this log in kafka == >
{quote}
[2017-02-03 19:16:01,266] INFO [GroupCoordinator 1002]: Preparing to restabilize group kafka-spout with old generation 30 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:16:01,266] INFO [GroupCoordinator 1002]: Group kafka-spout with generation 31 is now empty (kafka.coordinator.GroupCoordinator)
{quote}
=====
and then storm consumer group is stuck, no new messages are read from kafka. my topology/ spout are configured that way:

*Topology:*

      c.put(SConfig.TOPOLOGY_MAX_SPOUT_PENDING, 1000)
      c.put(SConfig.NIMBUS_SEEDS, ""my nimbus seeds"")
      c.put(SConfig.NIMBUS_THRIFT_PORT, 6627)
      c.put(SConfig.TOPOLOGY_WORKERS, 2)      c.put(SConfig.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS, 100)

*Spout:*

    props.put(KafkaSpoutConfig.Consumer.ENABLE_AUTO_COMMIT, true)
    props.put(KafkaSpoutConfig.Consumer.BOOTSTRAP_SERVERS, ...)
    props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""kafka-spout"")
    props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, keyDeserializer)
    props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, valueDeserializer)
 
Also the offsets are not seeming to be committed, despite I've set *enable.auto.commit* and *auto.commit.interval.ms* properties, because if I kill the topology and send it again, the same messages are being reprocessed...

 any hints?"
STORM-2341,worker-launcher is not included in binary distribution,Even though the documentation refers to [worker-launcher|http://storm.apache.org/releases/1.0.2/SECURITY.html] and the Travis builds with -Pnative the {{worker-launcher}} binary is not included in apache-storm-xyz.tar.gz files.
STORM-2339,Python code format cleanup in storm.py,"{{bin/storm.py}} has multiple stylistic shortcomings:
 - PEP8 standard is not followed
 - the python interpreter is hard-wired to /usr/bin/python
 - unnecessary global statements are posted before reading globals

These issues shadows error reporting by modern IDEs (such as PyCharm).
"
STORM-2332,b.s.m.n.Client [ERROR] connection to Netty-Client,"Hi!
I have configured Apache storm on 4 virtual machines (1nimbus and 3 supervisors). I have developed a algorithm that reads data from kafka broker and perform some processing on storm bolds. But, I'm getting this error when turning the cluster on:
 b.s.m.n.Client [ERROR] connection attempt 22 to Netty-Client-02.novalocal/208.113.164.115:6700 failed:java.net.ConnectException: Connection refused: 02.novalocal/208.113.164.115:6700
I would like to know what can cause this error in storm?
Thank you!"
STORM-2329,Topology halts when getting HDFS writer in a secure environment,"Simple topologies writing to Kerberized HDFS will sometimes stop while getting a new writer (storm-hdfs) in a Kerberized environment: 

java.io.IOException: Failed on local exception: java.io.IOException: Couldn't setup connection for principal@realm to nn1/nn1IP; Host Details : local host is: ""hostname/ip""; destination host is: ""nn hostname"":8020; at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772) at org.apache.hadoop.ipc.Client.call(Client.java:1473) at org.apache.hadoop.ipc.Client.call(Client.java:1400) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) at com.sun.proxy.$Proxy26.create(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296) at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy27.create(Unknown Source) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1726) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1668) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1593) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775) at org.apache.storm.hdfs.bolt.AvroGenericRecordBolt.makeNewWriter(AvroGenericRecordBolt.java:115) at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.getOrCreateWriter(AbstractHdfsBolt.java:222) at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.execute(AbstractHdfsBolt.java:154) at backtype.storm.daemon.executor$fn_3697$tuple_action_fn3699.invoke(executor.clj:670) at backtype.storm.daemon.executor$mk_task_receiver$fn3620.invoke(executor.clj:426) at backtype.storm.disruptor$clojure_handler$reify3196.onEvent(disruptor.clj:58) at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) at backtype.storm.daemon.executor$fn3697$fn3710$fn3761.invoke(executor.clj:808) at backtype.storm.util$async_loop$fn_544.invoke(util.clj:475) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Couldn't setup connection for principal@realm to nn1/nn1IP:8020 at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:673) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:644) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:731) at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:369) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1522) at org.apache.hadoop.ipc.Client.call(Client.java:1439) ... 35 more Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413) at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:554) at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:369) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:723) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:719) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:718) ... 38 more Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ... 47 more

Typically seen on low throughput topologies but recently witnessed in a topology that rotates files within minutes.  

From the trace it happens here: https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L151

My suspicion is this happens only when opening a new file otherwise I don't see why there would be a Kerberos context to complain about until a flush/sync perhaps. 

My shoot from the hip reaction is to pull that out of the current try and simply let the bolt fail and restart to establish a security context.  Thoughts? "
STORM-2328,Batching And Vector Operations,"Sub-topic of Storm Worker redesign. 
Design doc:  https://docs.google.com/document/d/13n0omjkc04h6KObC9-h7l4h4OL8Cp9Qnqz0XXtNJtno/edit?usp=sharing"
STORM-2326,Upgrade log4j and slf4j,"The dependencies to log4j could be upgraded from 2.1 to 2.7, same for slf4j to 1.7.21.

This would help fix [STORM-1386]

BTW any idea why we need log4j-over-slf4j?
"
STORM-2322,Could not find or load main class blobstore ,I got this error (Could not find or load main class blobstore) on Windows machine while I'm trying to run a command: storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1. This error occurs for other commands that I'm trying to use and for different machines.
STORM-2319,Remove hadoop-auth depeendency and add spnego filter in storm-core,
STORM-2318,Look at combining FiledNameTopicSelector and FieldIndexTopicSelector,Especially in 2.x it would be nice to combine the logic of FiledNameTopicSelector and FieldIndexTopicSelector so that I can select by both index and/or name.
STORM-2316,Enumeration support for properties configuration,It would be great if a Flux builder will resolve enumeration within properties configuration. This feature is only available for constructor arguments.
STORM-2314,Workers is dead or locked when netty connection is timeout,"Storm is running ,but some workers can not emit data when throw the following exception,  the exception offen occurs in heigh pressure

2017-01-22 18:23:25.137 s.k.CollectorZkCoordinator [INFO] Task [3/3] Finished refreshing
2017-01-22 18:24:08.170 o.a.s.m.n.StormClientHandler [INFO] Connection to xxxxxxx/192.168.175.25:6703 failed:
java.io.IOException: Connection timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.7.0_80]
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.7.0_80]
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.7.0_80]
	at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.7.0_80]
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384) ~[?:1.7.0_80]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-1.0.2.jar:1.0.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_80]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_80]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80]"
STORM-2313,CPU Pinning,"Design Document:
https://docs.google.com/document/d/1zI0ax-8dE9SmLkF8ZoJaLnrM11EL7NV1uO3VymWG3OA/edit?usp=sharing"
STORM-2312,Memory Management,"Refer to this Doc for details

https://docs.google.com/document/d/1a-RLv1KKBn2vVliztLcdfC-5vDStxpNlR0ZLu33lwxg/edit?usp=sharing"
STORM-2311,A browser which is outside the cluster cant's access  storm ui when storm cluster in a security mode,"when storm cluster in a security mode such as kerberos,We can not use a browser which is outside the cluster to access  storm ui,even if we remove the configuration item which named ui.filter.There is a mistake like ""server 500"",because those methods to get cluster's info for ui can't access the cluster which is in  a security mode"
STORM-2310,Handling Back Pressure,"Design Doc:
https://docs.google.com/document/d/1btmpBpFeEl-bh1uhQ_W4Ao-cQK7muNBYbXXMwuc4YaU/edit?usp=sharing"
STORM-2309,Elasticity - for Storm topologies,
STORM-2308,Support for Non-replayable Sources,"In order to recover from failures without data loss, Storm (and other streaming systems) places the responsibility of buffering events on the source system. In the event of a crash or other failure, in-flight events can be re-fetched from the source and their processing can be retried on recovery. A nice benefit of this approach is that it keeps Storm’s architecture simple. 

While it is desirable to avoid the complexities of creating an internal reliable buffering system, it is not necessary to restrict Spouts to accept data only from persistent sources such Kafka, Hdfs or databases. Some amount of data loss is acceptable in many uses cases. Storm already supports such use cases by allowing ACK-ing to be disabled. 

Users who can tolerate data loss, benefit from having spouts that can accept data directly from a wider variety of sources such as HTTP, TCP/UDP, Syslog, Flume etc. For such use cases, by not forcing all data to go through a system like Kafka, end-to-end latency improves in addition to simplifying management and reducing cost of the data pipeline. Users who care about not losing data can always funnel the incoming data via Kafka or another persistent store and enable ACKs.
"
STORM-2307,Revised Threading and Execution Model,"Design Doc:
https://docs.google.com/document/d/1PBGQomJQ67gsLR0CNZlYfVWjGyzAEJMsQjpKumuyHuQ/edit?usp=sharing"
STORM-2304,[storm-redis] Provide binary version of TupleMapper and corresponding mappers,"For now storm-redis only provides string version of mappers which makes issues with binary key, field, and value. While converting data, data can be messed, or at least there should be time / space consuming.

Redis originally takes binary data, and also Jedis has binary methods set. It should be better that storm-redis supports binary data, and after this state is not needed to be converted with base64."
STORM-2302,New Kafka spout doesn't support seek to given offset,"I was looking at code of current KafkaTridentSpoutEmitter & KafkaSpout class. Can we add functionality based on user provided offset to start from particular offset? This would be useful incase user wants to reprocess particular data set. Another example user has changed the group id & aware where old offset committed & he wants to start processing from same position.

Please refer attachment for further discussion happened over mail."
STORM-2299,Stop user from killing topology before X (configured) amount of time,"Currently user can kill topology directly without waiting for some amount of time so that all inflight messages will get processed.  For example, storm is writing to file & user kills topology, file is not closed or moved to proper location. We need to educate operation guys to do the right things also there are some chances that it will be not followed causing system to go in inconsistent state.
 
Can we set mandatory timeout (configurable) when user kills storm topology? User should not be allowed kill topology with time less than mentioned time.

Some case: 
1) If topology is long running don't allow user to kill but time not less than mentioned one
2) If topology is just deployed allow him to kill instantly (as it might be some mistake)
3) Handle same cases from command-line.
"
STORM-2292,"Kafka spout enhancement, for our of range edge cases","@hmcl and all, we have communicated via email for a while and going forward let's talk in this thread so everyone is in same page.
Base on the spout from the community(written by you), we have several fixes and it worked quite stable in our production for about 6 months.

We want to share the latest spout to you and could you please kindly help review and merge to the community version if any fix is reasonable? we want to avoid diverging too much from the community version.

Below are our major fixes:

For failed message, in next tuple method, originally the spout seek back to the non-continuous offset, so the failed message will be polled again for retry, say we seek back to message 10 for retry, now if kafka log file was purged, earliest offset is 1000, it means we will seek to 10 but reset to 1000 as per the reset policy, and we cannot poll the message 10, so spout not work.
Our fix is: we manually catch the out of range exception, commit the offset to earliest offset first, then seek to the earliest offset

Currently the way to find next committed offset is very complex, under some edge cases – a), if no message acked back because bolt has some issue or cannot catch up with the spout emit; b) seek back is happened frequently and it is much faster than the message be acked back
We give each message a status – None, emit, acked, failed(if failed number is bigger than the maximum retry, set to acked)

One of our use cases need ordering in partition level, so after seek back for retry, we re-emit all the follow messages again no matter they have emitted or not, if possible, maybe you can give an option here to configure it – either re-emit all the message from the failed one, or just emit the failed one, same as current version.

We record the message count for acked, failed, emitted, just for statistics.

Could you please kindly help review and let us know if you can merge it into the community version? Any comments/concern pls feel free to let us know. Btw, our code is attached in this Jira."
STORM-2291,A Hash Collision Problem of Fields Grouping in Windowing Method,"I‘d like to discuss the hash collision issue that occurs when applying the _Grouping_ method [http://storm.apache.org/releases/current/Concepts.html] to _Windowing_ method [http://storm.apache.org/releases/current/Windowing.html] in Storm.

I first assume the following situation. Spout constantly emits tuples to Bolt. At this time, Bolt tries to perform operations while moving tuples of a certain interval. e.g. moving average, etc. To solve this situation, Storm provides _Windowing_ method.

However, consider the following complex situation. Two problems are added in the above situation.
# As a first problem, the tuple emitted by Spout is multidimensional with multiple pieces of information. For example, Alice, Bob, and Clark are mapped to random real numbers. That is, the tuples emitted from Spout are {[Alice, 0.18322], [Clark, 0.57833], [Bob, 0.27902], [Clark, 0.24553], [Alice, 0.50164], [Alice, 0.06463], ...}. While those tuples are transmitted to the next windowed bolt, they must be necessarily separated by keys such as Alice, Bob, and Clark. In other words, each tuples in which Alice, Bob, and Clark are mapped must belong to different windows.
# The second problem is Storm's parallelism. Spout and Bolt can be operated as multiple objects on multiple servers. This problem is that the tuples with the same key must be emit in the same window even if they are created by a different Spout object.

Storm can specify the Bolt objects which the tuples is to be input as a _Grouping_ method. Storm provides various _Grouping_ methods, but a fields grouping is best suited as a way to solve the above problems. The fields grouping is a way of partitioning an input stream by a specified field. With the fields grouping, tuples of the same field can only be passed to the same Bolt object. However, the fields grouping has been implemented as a hash method. ([http://storm.apache.org/releases/current/Tutorial.html]) Therefore, it can be cause *a hash collision problem* that can include the tuples in the same window although they have different fields. So I am interested in solving the hash collision.

The source code of [https://github.com/dke-knu/i2am/tree/master/i2am-app/fields-window-grouping/src/main/java/org/fields/window/grouping/as_is] is a situation where the hash collision occurs. First, Spout randomly emits the tuples mapping Alice, Bob, and Clark on random real numbers. Next, Bolt which extends BaseWindowedBolt prints the TupleWindow objects received from Spout. At this time, Bolt uses the fields grouping. Spout emits three fields: Alice, Bob, and Clark. If the parallelism of Bolt is set less than 3, it surely cause the hash collision. Conversely, the greater the parallelism of Bolt than 3, the lower the probability of the hash collision. But, it can not be guaranteed that the hash collision does not occur.

As an alternative to this hash collision, I used two-step IRichBolt instead of BaseWindowedBolt. The source code for this is [https://github.com/dke-knu/i2am/tree/master/i2am-app/fields-window-grouping/src/main/java/org/fields/window/grouping/to_be]. The first Bolt is important. This Bolt takes tuples from Spout and manages them through a hash map of list according to the field. If the list is as filled as a predefined window size, the oldest tuple is removed and a new tuple is added. And then, Bolt emits this list to the next Bolt.

Using this method, the above problems can be solved. That is, the tuples of the same fields is always managed in the same window regardless of the number of fields and the number of parallelism. 

I'm concerned that this problem will frequently happen to many Storm users who use the Windowing method. If there is not a better way than the one I presented, I think there should be a new grouping method for Windowing method."
STORM-2290,Upgrading zookeeper to 3.4.9 for stability,"We should upgrade zookeeper to 3.4.9 as it brings in a lot of stability improvements (http://zookeeper.apache.org/releases.html) and storm is still using 3.4.6 (https://github.com/apache/storm/blob/master/pom.xml)

One serious issue affecting zookeeper 3.4.6 is https://issues.apache.org/jira/browse/ZOOKEEPER-1506 which prohibits zookeeper from getting a quorum and hence affects storm's stability as well."
STORM-2288,Nimbus client can timeout in log running tests,
STORM-2287,DemoTest fails intermittently on 1.x-branch & master branch,"See the following runs:
https://travis-ci.org/apache/storm/builds/191507849"
STORM-2286,Storm Rebalance command should support arbitrary component parallelism,"For legacy reasons, config TOPOLOGY-TASKS is considered first when schedule a topology, for a component, if user don’t specify TOPOLOGY-TASKS, storm just override it to be equal to component parallelism hint, and schedule based on TOPOLOGY-TASKS later on.

This works for the most cases, but not Rebalance command. Now, when do Rebalance, the StormBase :component->executors attribute will be overridden in Zookeeper which is used to partition component tasks into executors, as we said above, the TOPOLOGY-TASKS is considered here as the real tasks number for components, something goes weird here:

If we override a bigger executor numbers for a component when do rebalance, it just don’t work because smaller TOPOLOGY-TASKS [ not changed since first submitted at all ]is partitioned into bigger number of executors which read from ZooKeeper overridden by Rebalance command, but for smaller task, it works fine.

I see that storm support a command like this now: [storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*] which indicate that user can override a component parallelism freely, i think it’s more sensible to support this and it's meaningless to have a restriction like before."
STORM-2285,Streams api - support transform operation,"Support stream-to-stream transformations so that common patterns can be provided as built in transformations and complex transformation can be built on top.

Beam has the concept of composite transforms, similar concept is also supported in spark streaming transform() api.

At a high level it could be something like,

{code}
interface Transform<T, R> {
  Stream<R> apply(Stream<T> input);
}
{code}

And then in the Stream<T>, we add an api that applies the transform.

{code}
Stream<T> {
  ...
  Stream<R> apply(Transform<T, R> transform);
  ...
}
{code}

"
STORM-2284,Storm Worker Redesign,"Much has been learnt from evolving the 1.x line. We can now use the benefit of hindsight and apply these learnings into the future work on 2.x line. 

The goal is to rethink the Worker to improve performance, enhance its abilities and also retain compatibility.


*Overview Document*:
Also covers results from experiments that motivate this work.
https://docs.google.com/document/d/1EzeHL3d7EE-RyyBEpN7CwRmWz3oqjbbKiVVAlzFp2Nc/edit?usp=sharing    "
STORM-2282,Streams api - provide options for handling errors,"Adding relevant discussions from PR 1693 below.

Allow users to be explicit about how to handle errors. I don't know if any API out there does it... so this would be unique to Storm.

In short:
Broadly speaking there are two kinds of tuple process errors that the users need to be concerned about:

1- Retry worthy Errors - For instance, failure to deliver to destination service due to connection issues.
2- Not worth retrying - For instance, Parsing errors due to bad data in the tuple. These problems can jam up the pipeline if they are retried repeatedly. Such tuples can be sent to a configurable Dead-Letter-Queue.

---

>1- Retry worthy Errors

Right now theres no explicit `fail` api. When a stage in the stream completes processing (and possibly emits results), the underlying tuples are acked automatically. The only way spout will re-emit is after the message timeout. It may be good to have a fail fast api, but I am not sure how it would help. The replayed tuple could fail again in processing. Instead the processing logic itself can have some retry logic (say retry 3 times) and forward to an error stream  and ack the tuple.

> 2- Not worth retrying

This can be handled via branch logic. E.g. send valid values to stream1 and bad values to stream2.
"
STORM-2273,Starting nimbus from arbitrary dir fails,"Here is the output that I got:
{code}
storm@node1:/home/vagrant$ storm nimbus
Running: java -server -Ddaemon.name=nimbus -Dstorm.options= -Dstorm.home=/usr/share/apache-storm-2.0.0-SNAPSHOT -Dstorm.log.dir=/usr/share/apache-storm-2.0.0-SNAPSHOT/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib:/usr/lib64 -Dstorm.conf.file= -cp /usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-api-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-slf4j-impl-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/clojure-1.7.0.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/slf4j-api-1.7.7.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/asm-5.0.3.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/minlog-1.3.0.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/reflectasm-1.10.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-over-slf4j-1.6.6.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-core-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/kryo-3.0.3.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/storm-rename-hack-2.0.0-SNAPSHOT.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/disruptor-3.3.2.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/storm-core-2.0.0-SNAPSHOT.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/objenesis-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/servlet-api-2.5.jar:/usr/share/storm/conf -Xmx1024m -Dlogfile.name=nimbus.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/usr/share/apache-storm-2.0.0-SNAPSHOT/log4j2/cluster.xml org.apache.storm.daemon.nimbus.Nimbus
{code}
Log added to nimbus.log.
{code}
2017-01-04 21:44:33.089 main o.a.s.n.NimbusInfo [INFO] Nimbus figures out its name to node1
{code}
"
STORM-2269,Dynamic reconfiguration for the nodes in Nimbus/Pacemaker clusters,"Reference: https://zookeeper.apache.org/doc/trunk/zookeeperReconfig.html

It would be nice to have a similar functionality for Nimbus/Pacemaker clusters too.
As that would eliminate the need for restarting servers in the Nimbus/Pacemaker clusters whenever a node exits or joins these clusters.


----------------------------------------------------------
Reply from Bobby Evans on the dev group:
----------------------------------------------------------

There is nothing for that right now on pacemaker.
You can do it with nimbus so long as at least one of the original nodes is still up.

But in either case it would not be too difficult to make it all fully functional.
The two critical pieces would be in giving the workers and daemons a way to reload these specific configs dynamically.
Then it would be documenting the order of operations to be sure nothing goes wrong.

*Adding Pacemaker Node(s)*
# bring up the new node(s).
# update nimbus configs to start reading from the new nodes.
# update all of the worker nodes to let workers start writing to the new node.

*Removing Pacemaker Node(s)*
# Shut down pacemaker nodes/update configs on workers (order should not matter so long as there are enough pacemaker nodes up to handle the load)
# update the nimbus configs to not try and read from the old nodes


*Adding new Nimbus Node(s)*
# Bring up the new nimbus with the new config.
# update all of the other nodes (including any machines that clients come from) with new config (order does not matter)

*Removing Nimbus Node(s)*
# Shut down the old nodes and update the configs on all the boxes in any order you want.
# This should just work so long as you have at least one nimbus node still up.
"
STORM-2265,Incorrectly Serialized JSON in TransactionalState causes Worker to Die,"TransactionalState uses JSONValue to serialize / deserialize objects. However, the object GlobalPartitionInformation is incorrectly serialized by default, causing the exception bellow. To get around this problem, GlobalPartitionInformation must implement JSONAware.

2016-12-23 14:37:26.980 o.a.s.e.e.ReportError Thread-21-$spoutcoord-spout-spout1-executor[2, 2] [ERROR] Error
java.lang.RuntimeException: java.lang.RuntimeException: Unexpected character (G) at position 1.
        at org.apache.storm.utils.Utils$6.run(Utils.java:2190) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.RuntimeException: Unexpected character (G) at position 1.
        at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:174) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$6.run(Utils.java:2179) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 1 more
Caused by: org.apache.storm.shade.org.json.simple.parser.ParseException
        at org.apache.storm.shade.org.json.simple.parser.Yylex.yylex(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.nextToken(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.JSONValue.parseWithException(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:167) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$6.run(Utils.java:2179) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 1 more
2016-12-23 14:37:26.987 o.a.s.u.Utils Thread-21-$spoutcoord-spout-spout1-executor[2, 2] [ERROR] Halting process: Worker died
java.lang.RuntimeException: Halting process: Worker died
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$4.run(Utils.java:1800) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.error.ReportErrorAndDie.uncaughtException(ReportErrorAndDie.java:45) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1956) [?:1.8.0_112]
2016-12-23 14:37:26.987 o.a.s.d.w.Worker Thread-38 [INFO] Shutting down worker tkst-consumer-4-1482532570 556a1e7b-49f7-4dc2-a936-d17e5e4ba9de 6700
2016-12-23 14:37:26.988 o.a.s.d.w.Worker Thread-38 [INFO] Terminating messaging context
"
STORM-2263,Streams api - support custom operators,"This is to provide users a way to extend the api and provide custom operators not covered by the standard set of apis.

We could also expose a “stream.process()” method that takes a custom implementation of the Processor interface. (Internally the built-in operations are also executed via the processor interface).

Processor<T> customProcessor = new Processor<T>() {...}
Stream<R> stream2 = stream1.process(customProcessor);

The user defined custom processor implements the Standard processor interface used by the built in operators (The interface may need a bit of tweaking)."
STORM-2262,Streams api - add option to access late tuple stream,Could be an additional api like windowWithLatetupleStream(...) which returns an array of two streams (windowed and late tuple stream)
STORM-2261,Streams api - support Union,"Union returns a new stream that contains the union of the elements in the source stream and other stream. 

Similar constructs are supported in spark streaming and flink."
STORM-2260,Streams api - support side inputs,"Side inputs are additional inputs computed at runtime that can be used within the main computation.

E.g. Joining an IP address stream with a blacklist (side-input) which is periodically updated.

Also see - https://cloud.google.com/dataflow/model/par-do#side-inputs"
STORM-2259,Streams api - support for specifying resource hints,"Eventually we want to support true elasticity. 

Till then provide  options for users to specify resource requirements (e.g. CPU, memory, network etc) may be via the stream builder."
STORM-2255,"can not compile master branch with error on ""Could not find artifact org.apache.storm:storm-sql-runtime:jar:tests:2.0.0-SNAPSHOT in clojars (https://clojars.org/repo/) -> [Help 1]""","I can not compile storm master with error on ... (mvn install)
""[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project storm-sql-core: Could not resolve dependencies for project org.apache.storm:storm-sql-core:jar:2.0.0-SNAPSHOT: Could not find artifact org.apache.storm:storm-sql-runtime:jar:tests:2.0.0-SNAPSHOT in clojars (https://clojars.org/repo/) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :storm-sql-core""

=================
Is this an issue? How to fix it? "
STORM-2253,Storm PMML Bolt - Unit Tests,"Currently the patch has integration tests through the form of a test topology that loads a PMML Model and raw input data from a CSV file. The {@link RawInputFromCSVSpout}
creates a stream of tuples with raw inputs, and the {@link PMMLPredictorBolt} computes the predicted scores.

The main focus of the initial patch was to design the classes in such a way that they can accommodate arbitrary runtime environments. The default implementation provided uses one such runtime execution library, which is more suited to be tested using integration tests.

Will add some unit tests around to assert for edge and some common cases"
STORM-2252,Storm 0.10.0 metrics refresh slow,"Storm 0.10.0 metrics refresh is extremely slow; usually once in 5 minutes unlike the older versions of Storm which was at least every 60 seconds.

Topology metrics are delivered at the 60 second configured frequency however, built-in metrics like Process and Execute latencies are refreshed at around the 5 minute interval."
STORM-2249,Make Distribution Scripts Put Examples to the Correct Locations,Make binary.xml put all the examples that exist in source packages with name pattern COMPONENT-NAME-examples-x.y.x.jar to STORM_HOME/examples/storm-COMPONENT-NAME-examples/COMPONENT-NAME-examples-x.y.x.jar
STORM-2248,Storm UI in Apache storm 1.0.2 does not update Executors and Tasks after Rebalance .Is anyone else facing this issue ,
STORM-2247,Logviewer should change group read permission to files owned by user if it cant read them,"In some situations, like with heap dumps, files written to the log directory (where logviewer looks) could be owned by user but not by logviewer. Therefore the logviewer isn't able to open to serve. This is to port to apache work that [~ppoulosk] did to chmod these files before being served to requester."
STORM-2242,Trident state persisting does not honor batch.size.rows configuration,"Persisting the Trident state in {{org.apache.storm.cassandra.trident.state.CassandraState}} with batching enabled does not honor the configuration for {{cassandra.batch.size.rows}}.

This results in a warning at least:
{code}
10:33:33.720 [SharedPool-Worker-16] WARN  o.a.c.cql3.statements.BatchStatement - Batch of prepared statements for [gin.ngram_count] is of size 5200, exceeding specified threshold of 5120 by 80.
{code}

An exception like this is also possible:
{code}
10:30:54.287 [SharedPool-Worker-1] ERROR o.a.c.cql3.statements.BatchStatement - Batch of prepared statements for [gin.df] is of size 103428, exceeding specified threshold of 51200 by 52228. (see batch_size_fail_threshold_in_kb)
10:30:54.295 [Thread-29-b-1-executor[7 7]] WARN  o.a.s.c.trident.state.CassandraState - Batch write operation is failed.
10:30:54.297 [Thread-29-b-1-executor[7 7]] ERROR org.apache.storm.daemon.executor -
com.datastax.driver.core.exceptions.InvalidQueryException: Batch too large
    at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:50) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:64) ~[cassandra-driver-core-3.1.0.jar:na]
    at org.apache.storm.cassandra.trident.state.CassandraState.updateState(CassandraState.java:159) ~[storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.cassandra.trident.state.CassandraStateUpdater.updateState(CassandraStateUpdater.java:34) [storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.cassandra.trident.state.CassandraStateUpdater.updateState(CassandraStateUpdater.java:30) [storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.trident.planner.processor.PartitionPersistProcessor.finishBatch(PartitionPersistProcessor.java:98) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.trident.planner.SubtopologyBolt.finishBatch(SubtopologyBolt.java:151) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.trident.topology.TridentBoltExecutor.finishBatch(TridentBoltExecutor.java:266) [storm-core-1.0.2.jar:1.0.2]
{code}

This effectivly disables the usage of batching."
STORM-2241,KafkaSpout implementaion,"Storm ISpout documentaion say 'Storm executes ack, fail, and nextTuple all on the same thread. This means that an implementor of an ISpout does not need to worry about concurrency issues between those methods. However, it also means that an implementor must ensure that nextTuple is non-blocking: otherwise the method could block acks and fails that are pending to be processed.'

Where as KafkaSpout has below nextTuple() implementation
@Override
    public void nextTuple() {
        List<PartitionManager> managers = _coordinator.getMyManagedPartitions();
        for (int i = 0; i < managers.size(); i++) {

            try {
                // in case the number of managers decreased
                _currPartitionIndex = _currPartitionIndex % managers.size();
                EmitState state = managers.get(_currPartitionIndex).next(_collector);
                if (state != EmitState.EMITTED_MORE_LEFT) {
                    _currPartitionIndex = (_currPartitionIndex + 1) % managers.size();
                }
                if (state != EmitState.NO_EMITTED) {
                    break;
                }
            } catch (FailedFetchException e) {
                LOG.warn(""Fetch failed"", e);
                _coordinator.refresh();
            }
        }

        long now = System.currentTimeMillis();
        if ((now - _lastUpdateMs) > _spoutConfig.stateUpdateIntervalMs) {
            commit();
        }
    }

We are seeing events are getting replayed when there is slower bolt in the topology chain causing duplicate messages.

Is there any way this can be fixed.

"
STORM-2237,Nimbus reports bad supervisor heartbeat - unknown version or thrift exception,
STORM-2233,BlobStore Cleanup/Optimization,"KeySequenceNumber creates a new connection to zookeeper each time.  This really does not need to happen.  At a minimum we should cache the connection and/or reuse it.

Also everywhere that we check for LocalBlobStore so we can store some blob state should be refactored so it can be inside by LocalBlobStore itself. "
STORM-2232,nimbus crash!  ,"storm cluster version:   apache-storm-1.0.2

2016-12-02 09:21:11.812 o.a.s.t.s.THsHaServer [ERROR] run() exiting due to uncaught error
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:371)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:203)
        at org.apache.storm.thrift.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:203)
        at org.apache.storm.thrift.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:154)

java-version : 1.8.0_60
niumbs info:
-Xmx2048m-XX:MetaspaceSize=96m-XX:MaxMetaspaceSize=256m-Xloggc:/tmp/nimbus.gc.log-XX:+PrintGCTimeStamps-XX:+PrintGCDetails-XX:+PrintGCDateStamps-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=/tmp/nimbus.heapdump-Dlogfile.name=nimbus.log-DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector-Dlog4j.configurationFile=/home/data/soft/apache-storm-1.0.2/log4j2/cluster.xmlorg.apache.storm.daemon.nimbus"
STORM-2230,Unable to send same topic in different streams using KafkaSpoutStreamsNamedTopics,
STORM-2229,KafkaSpout does not resend failed tuples,"When the topology fails a tuple, it is never resent by the KafkaSpout. This can easily be shown by constructing a small topology failing every tuple.

Apparent reason:

{code}
public class KafkaSpout<K, V> extends BaseRichSpout {
//...
private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                kafkaConsumer.seekToEnd(toArrayList(rtp));    // Seek to last committed offset <== Does seek to end of partition
            }
        }
    }
{code}

The code seeks to the end of the partition instead of seeking to the first uncommited offset.

Preliminary fix (worked for me, but needs to be checked by an expert)

{code}
    private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                OffsetAndMetadata committed = kafkaConsumer.committed(rtp);
                if(committed == null) {
                    // No offsets commited yet for this partition - start from beginning 
                    kafkaConsumer.seekToBeginning(toArrayList(rtp));
                } else {
                   // Seek to first uncommitted offset
                    kafkaConsumer.seek(rtp, committed.offset() + 1);
                }
            }
        }
    }
{code}
"
STORM-2227,Better User Control of GC Options,"As a user, I would like to override certain JVM garbage connection options instead of overriding them all, so that I can make simpler, safer changes.

Currently, if a user wants to add some gc option in a topology, the user must copy everything from {{worker.gc.childopts}} to {{topology.worker.gc.childopts}} and make needed edits/additions.  This is error prone, since the provided cluster-wide options can change, and because they are overwritten by default.

A user can easily override settings unwittingly by adding new options if they forget to also copy the cluster-wide settings."
STORM-2221,Update DRPC Example,"Provide an example of how to do DRPC properly, and not use the deprecated LinearDRPCTopologyBuilder."
STORM-2219,In HDFSBolt and SequenceFileBolt the files are overridden if they already exist,"In both bolts the files are opened in create mode. That implies that if the file already exists it is overridden. So, if for some reason the bolt is restarted (rebalancing or some crash), the data is lost. I think that is specially grave. What's more, since the rotation number is stored in memory, all the files will be eventually wiped out.

I think there are two possible approaches:
- If the file already exists, open it in append mode. I see some problems here, (1) the tuples data written to the several rotations will not keep its order unless we jump to the last rotation, (2) the TimedRotationPolicy and other that rely on memory stored data will not behave exactly as expected and (3) if the case of the SequenceFileBolt, if the file has different compression code or type it will raise an exception. Besides, we should change the way the HDFSWriter handles the writing offset because it depends on the size of the Tuples being written and not on the size of the file (and that would affect the FileSizeRotationPolicy). This doesn't affect the SequenceFileWriter, since it is using the getLength() method of SequenceFile.Writer that handles the append mode properly.
- If the file exists, move to the next rotation. The problem I see is that if the rotation number is not part of the file name it will enter in a endless loop. Another issue is that if the the restart of the bolt is caused by some problem that is not fixed after the restart, it could be creating new files infinitely until collapsing the NameNode.

I guess the solution will be a mix of both approaches and I think I can be able to implement it. But first I would like to ask if anyone has any other concern about it.

By the moment I just wrote a bolt that satisfies my use case, with Sequence Files opened in append mode if the file exists and rotating based on size. But this solution should be more general. 
"
STORM-2218,When using Block Compression in the SequenceFileBolt some Tuples may be acked before the data is flushed to HDFS,"In AbstractHDFSBolt, the tuples are being acked after calling syncAllWriters(), that basically ends up calling doSync() in every writer. In the case of the SequenceFileWriter, that is the same as calling the hsync() method of SequenceFile.Writer:

https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/SequenceFileWriter.java#L52

The problem in the case of the block compression is that if there is a compression block opened it is not flushed with hsync(), instead it is necessary to call the sync() method, that adds a sync marker, compresses the block and writes it to the output stream that is flushed with hsync(). This is also done automatically when a certain size is reached in the compression block, but we cannot have certainty of the data being flushed until we call sync() and then hsync():

https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java#L1549

The easy fix is just add a call to sync() in case the writer is using Block Compression. I'm concerned about the impact that would have in the block size, but I think it is the only way of writing the data reliably in this case."
STORM-2211,KafkaSpout error after recreating kafka topic,"I have a storm topology with a KakfaSpout which was processing messages successfully. I shut down the topology, deleted the kafka topic and recreated it. After restarting the topology, it appears storm saved the partition in zookeeper and was unable to identify that the partition no longer existed. I saw the following error message in the storm logs:

o.a.s.k.KafkaUtils [WARN] Partition{host=<removed>, topic=<removed>, partition=0} Got fetch request with offset out of range: [10650]"
STORM-2203,Add a getAll method to KeyValueState interface,"A getAll method which would return all the key value pairs present in the state could be really useful in Stateful bolts. Example use case - Loop over all key value pairs in state on receiving a tick tuple and store all values satisfying a given criterion to database.
I'll be happy to provide a patch if you guys think it's a good idea."
STORM-2199,Module enabling storm to write memory mapped files,"Add module to write from storm to memory-mapped files.
    # Support multiple file formats 
    # Support multiple file rotation and sync policy implementation
    


"
STORM-2195,Clean up some of worker-launcher code,worker-launcher has a lot of confusing and unnecessary code in it. We should try to clean it up a bit.
STORM-2187,[Storm SQL] Finding a way to store the values in record to matching fields,"We're now serializing record into a ByteBuffer and store it to a single field.
That was needed for Kafka and Redis, but I feel it's not natural for some data sources which supports schema (like JDBC, MongoDB, etc.) 
For these data sources, record should be stored as it is (not serialized).

If new approach doesn't break backward compatibility it should be great, but if it's necessary to break, I think it's OK to break since Storm SQL is still marked as 'experiment'."
STORM-2183,BaseStatefulBoltExecutor does not handle cyclic graphs,"BaseStatefulBoltExecutor::getCheckpointInputTaskCount() returns the number of sources that a state transaction must wait for to process a transaction.  In a graph where there is a loop (e.g. A->B->C->D->C) components 'C' and 'D' the required number of tuples can will never be received.  The function shouldProcessTransaction will never receive the correct number of tuples, because the set required to come back form 'D' to 'C' will never be forwarded from 'C' to  'D' to begin with.  

Bolt 'C' and 'D' never finish the state initialization step and as such will never pass tuples to their wrapped bolt."
STORM-2181,Spout dose not emit tuples any more when back-pressure enabled,"After a period of running, Spout dose not emit tuples any more when back-pressure enabled. And this will be resumed after back-pressure disabled."
STORM-2180,KafkaSpout ack() should handle re-balance,Currently the kafkaSpout ack() routine doesn't handle if a re-balance happened and an ack for a partition it no longer handles get sent back to it. It will throw a null pointer exception. We should check to make sure that partition exists before calling .add.
STORM-2179,Storm.py doesn't detect JAVA_HOME properly on Windows,"line 92 in storm.py, should be inclusing a check for OS and using java.exe (Windows) vs java (Linux).

JAVA_CMD = 'java' if not JAVA_HOME else os.path.join(JAVA_HOME, 'bin', 'java.exe')"
STORM-2178,version not being returned on Windows,"""storm.cmd version"" returns blank string."
STORM-2169,Define Naming Convention for Metric Namespaces,"Code Hale's metrics library allows you to define names for metrics and the ability to group metrics in separate {{MetricRegistry}}s.

One important consideration, particularly when it comes to aggregating metrics, is the naming convention and metrics hierarchy.

One approach would be to have a high-level grouping based on the service/process a metric belongs to (similar to how Storm's configuration keys are namespaced). For example:

*Topology Metrics*
{{storm.topology.*}}

*Worker Metrics*
{{storm.worker.*}}

Building on that naming convention would allow for implicit hierarchy that would enable metric aggregation at various levels. For example, if an ""emitted"" metric were named such that it identifies the path to the individual component, e.g.:

{{storm.topology.$topology_name.$worker_host.$worker_port.$component_id.emitted}}

Then when the metric hit the metric store, various levels of aggregation could be achieved by parsing the metric name and iterating right to left over the tokens in the path and updating the value in the database.

For the example above, this would yield the following aggregated metrics:

*Component Level (not aggregated)*
{{storm.topology.$topology_name.$worker_host.$worker_port.$component_id.emitted}}

*Worker Level*
{{storm.topology.$topology_name.$worker_host.$worker_port.emitted}}

*Host Level*
{{storm.topology.$topology_name.$worker_host.emitted}}

*Topology Level*
{{storm.topology.$topology_name.emitted}}
"
STORM-2168,Deprecate metrics going through Zookeeper from workers,"After reporters are in place, Supervisor is reading stats and sending over to Nimbus, we should remove the metrics going through Zookeeper."
STORM-2167,Decide on a query language the UI can use to query metrics,"This task is large, and needs to be subtasked.

Initially, I can imagine us wanting the UI's interface not to change as we should support the metrics we already have. However, as we look to support time series queries for example, we should consider using GraphQL or some variation on OpenTSDB or Druid's query language, like Bobby suggested."
STORM-2166,Expand set of codahale metrics for UI,Extend/adjust the set of metrics we expose from the UI daemon through codahale.
STORM-2165,Implement default worker metrics reporter,"A reporter that writes metrics using netty or thrift to Nimbus, this should be configured using STORM-2164.

We need to tune the frequency at which metrics are sent from workers."
STORM-2163,Expand set of codahale Metrics for Nimbus,Expanding what we currently track in Nimbus to be tracked using codahale.
STORM-2162,Expand set of codahale Metrics for Supervisor,"Currently we expose number of slots used, but we should decide what else to add. 

This is more of a ""nice to have"" task and is not in the critical path of standardizing around codahale in general."
STORM-2159,Codahale-ize Executor and Worker builtin-in stats,"We want built-in metrics in CommonStats and [Spout|Bolt]ExecutorStats to use codahale Meter directly. 

Each worker process should keep a MetricRegistry and have a configurable set of reporters that can be instantiated. 

Other reporters can be added for the registry for direct reporting to the metric backend of choice.

Note that I am proposing we leave old metrics through ZooKeeper in place after this task (taken care of by STORM-2168), but we can change this if others prefer to go cold turkey."
STORM-2157,Search text in the Storm UI should be remained unchanged,"Each time press the 'Search' button in the Storm UI, the text in the search box is escaped.

For example, if I type 'Prepared bolt' and press the button, it becomes 'Prepared+bolt'. And if press the button again, now it becomes 'Prepared%2Bbolt'. So only the first search succeeds, and the second search fails.

For continuous successes of the search, the search text should be remained unchanged.
"
STORM-2156,Add RocksDB instance in Nimbus and write heartbeat based metrics to it,"There should be a RocksDB instance in Nimbus where we write metrics from the heartbeats. This should allow us to replace storage for the statistics we see in the UI and expand the abilities of UIs to allow for time series charting.

Eventually this data will likely come via thrift to Nimbus as the overall metric system is overhauled. "
STORM-2155,Come up with design for improving the streams api guarantees,"This is a placeholder for interested people to collaborate on the ideas for improving the guarantees for the stream api (phase 2).

Initial thoughts are here - https://docs.google.com/document/d/1Ew7uFF1UJ6e_zq0t4bM6A9auuEaArviAjjWYSpVFqPY/edit#heading=h.nmf14n

We can discuss and may be prototype some of the ideas and/or come up with new ideas and then choose the best approach before proceeding with the implementation."
STORM-2154,Prototype beam runner using unified streams api,This is mostly to identify any gaps and validate the proposed apis. It will be just a prototype runner using the apis proposed in STORM-1961.
STORM-2152,Upgrade Curator Framework to Latest Version (3.2.0),
STORM-2151,Upgrade hadoop 2.7.1,
STORM-2150,ShellBolt raise subprocess heartbeat timeout Exception,"I've got a simple topology running with Storm 1.0.1. The topology consists of a KafkaSpout and several python multilang ShellBolt. I frequently got the following exceptions. 

{code}
java.lang.RuntimeException: subprocess heartbeat timeout at org.apache.storm.task.ShellBolt$BoltHeartbeatTimerTask.run(ShellBolt.java:322) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)
{code}

More information here:
1. Topology run with ACK mode.
2. Topology had 40 workers.
3. Topology emitted about 10 milliom tuples every 10 minutes. 


Every time subprocess heartbeat timeout, workers would restart and python processes exited with exitCode:-1, which affected processing capacity and stability of the topology. 

I've checked some related issues from Storm Jira. I first found STORM-1946 reported a bug related to this problem and said bug had been fixed in Storm 1.0.2. However I got the same exception even after I upgraded Storm to 1.0.2.

I checked other related issues. Let's look at history of this problem.
DashengJu first reported this problem with Non-ACK mode in STORM-738. STORM-742 discussed the approach of this problem with ACK mode, and it seemed that bug had been fixed in 0.10.0. I don't know whether this patch is included in storm-1.x branch. In a word, this problem still exists in the latest stable version."
STORM-2149,[Storm SQL] Schema support on input format and output format,"Though Storm SQL receives INPUTFORMAT and OUTPUTFORMAT when defining TABLE, Storm SQL actually doesn't use that and assumes we only use JSON as format.

We definitely need to support various schema types, including CSV, TSV, and at least Avro (which SQE is providing), and others (added in future).

Since there're several schemas to support, and maybe more, I'll create this as epic issue."
STORM-2147,[Storm SQL] Support automatic spout parallelism based on DataSource metadata,"It would be better to receive metadata from Data Source, especially producer which can give some hints to optimize. 
A notable kind of hint is parallelism hint. In storm-kafka we know that normally it's best to set parallelism to same as topic's partition count so that Spouts can pull the data from all partitions in parallel.

We can apply non-query optimizations start from here."
STORM-2146,Storm SQL - Support JOIN between Streaming data source and State data source in Trident mode,"This issue is only targeted to Trident mode.

This issue also handles drop supporting join between Streaming data source and Streaming data source since the join is based on micro-batch so result is non-deterministic which doesn't make sense.

This issue tracks supporting join between Streaming data source and State data source. 

Following works should be done:
- create interface for State Table which is able to query value based on key
- define DDL for State data source
- modify visitTableScan in TridentLogicalPlanCompiler to handle State data source
- modify visitJoin in TridentLogicalPlanCompiler
-- check join condition: first one is Streaming and next one is State
--- table order can be reversible
--- only simple equi-join is supported (support compose key?)
-- query to State via join key and join Streaming row and the value
- address unit tests
- update document "
STORM-2143,Storm SQL provider implementations should convert field names as uppercased,"When getting column informations from Calcite, they are not case sensitive and all uppercased. 
Since Storm field name is case sensitive, we should respect 'case insensitive' while handling tuples. If producer just creates tuples with field names to uppercase before filtering and publishing, that makes life much easier.

NOTE: This should be documented so that following implementations can follow up."
STORM-2140,Track file offsets instead of line number,HDFS TextFileReader track file using line number and char number to replace it with file offset . 
STORM-2138,java.io.FileNotFoundException: stormconf.ser does not exist,"We are seeing problems in our storm topology whereby all our workers crash.

The errors we see are

2016-10-07 09:49:33.599 o.a.s.d.supervisor [ERROR] Error on initialization of server mk-supervisor
java.io.FileNotFoundException: File '/opt/storm_local/supervisor/stormdist/production_2016_09_13-1-1475831938/stormconf.ser' does not exist
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292)
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815)
        at org.apache.storm.config$read_supervisor_storm_conf_given_path.invoke(config.clj:142)
        at org.apache.storm.config$read_supervisor_storm_conf.invoke(config.clj:221)
        at org.apache.storm.daemon.supervisor$add_blob_references.invoke(supervisor.clj:495)
        at org.apache.storm.daemon.supervisor$fn__9307$exec_fn__2466__auto____9308.invoke(supervisor.clj:795)
        at clojure.lang.AFn.applyToHelper(AFn.java:160)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at clojure.core$apply.invoke(core.clj:630)
        at org.apache.storm.daemon.supervisor$fn__9307$mk_supervisor__9352.doInvoke(supervisor.clj:763)
        at clojure.lang.RestFn.invoke(RestFn.java:436)
        at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1200)
        at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1233)
        at clojure.lang.AFn.applyToHelper(AFn.java:152)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at org.apache.storm.daemon.supervisor.main(Unknown Source)
2016-10-07 09:49:33.608 o.a.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
        at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
        at clojure.lang.RestFn.invoke(RestFn.java:423)
        at org.apache.storm.daemon.supervisor$fn__9307$mk_supervisor__9352.doInvoke(supervisor.clj:763)
        at clojure.lang.RestFn.invoke(RestFn.java:436)
        at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1200)
        at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1233)
        at clojure.lang.AFn.applyToHelper(AFn.java:152)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at org.apache.storm.daemon.supervisor.main(Unknown Source)
2016-10-07 09:49:34.668 o.a.s.d.supervisor [INFO] Removing code for storm id production_2016_09_13-1-1475831938


We have looked at https://github.com/apache/storm/pull/418 and https://issues.apache.org/jira/browse/STORM-130, which both show the first issue as being fixed - however we are still experiencing it in 1.0.2. The changes from the fixing commit (https://github.com/apache/storm/pull/418/commits/ccd28f8a356f468e66865fa9d9901b0a2628ec74) don't seem to be in the current version of the file (https://github.com/apache/storm/blob/v1.0.2/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj).

We get this often when resubmitting a topology, and our only workaround is to stop the topology, delete the whole /opt/storm_local directory (which is our storm.local.dir) and resubmit the topology. Often, the workers seem to be looking for stormconf.ser in the local directory of an old topology that isn't even running at the time."
STORM-2137,how to make new issue about apache storm,how to make new issue about apache storm?
STORM-2132,NullPointerException in KafkaSpout,"Received a null pointer exception using storm-kafka-client.  Not sure what caused it, was just sitting idle in my IDE.  It did crash my topology.

Here's the consumer information:
topic 	0	1,546	2,802	2,798	4
topic 	1	1,663	2,856	2,856	0
topic 	2	1,671	3,031	3,022	9
topic 	3	1,648	2,760	2,760	0
topic 	4	1,618	2,828	2,824	4
topic 	5	1,537	2,599	2,595	4
topic 	6	1,469	2,522	2,522	0

java.lang.NullPointerException
	at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:249)
	at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:237)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:203)
	at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
[Thread-34-kafka-spout-executor[4 5]] ERROR org.apache.storm.daemon.executor - 
java.lang.NullPointerException
	at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:249)
	at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:237)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:203)
	at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
[Thread-34-kafka-spout-executor[4 5]] ERROR org.apache.storm.util - Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.worker$fn__8659$fn__8660.invoke(worker.clj:761)
	at org.apache.storm.daemon.executor$mk_executor_data$fn__7875$fn__7876.invoke(executor.clj:274)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:494)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)"
STORM-2130,1.0.x does not compile (BaseConfigurationDeclarer),Looks like we missed pulling something into 1.0.x-branch because if I use the version of storm-core/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java from 1.x-branch everything works.
STORM-2129,start storm application get CuratorConnectionLossException,"
32257 [Thread-32-kafka-reader-executor[4 4]] ERROR o.a.c.ConnectionState - Connection timed out for connection string (yj-es-6:2181,yj-es-5:2181) and timeout (15000) / elapsed (15084)
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197) [curator-client-2.10.0.jar:?]
	at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88) [curator-client-2.10.0.jar:?]
	at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:116) [curator-client-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:489) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108) [curator-client-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38) [curator-framework-2.10.0.jar:?]
	at org.apache.storm.kafka.DynamicBrokersReader.getNumPartitions(DynamicBrokersReader.java:111) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.DynamicBrokersReader.getBrokerInfo(DynamicBrokersReader.java:84) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.trident.ZkBrokerReader.<init>(ZkBrokerReader.java:44) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.KafkaUtils.makeBrokerReader(KafkaUtils.java:58) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.KafkaSpout.open(KafkaSpout.java:77) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.daemon.executor$fn__7990$fn__8005.invoke(executor.clj:604) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:482) [storm-core-1.0.2.jar:1.0.2]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_79]
32257 [Thread-30-kafka-reader-executor[5 5]] ERROR o.a.c.ConnectionState - Connection timed out for connection string (yj-es-6:2181,yj-es-5:2181) and timeout (15000) / elapsed (15083)
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197) [curator-client-2.10.0.jar:?]
	at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88) [curator-client-2.10.0.jar:?]
	at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:116) [curator-client-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:489) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108) [curator-client-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38) [curator-framework-2.10.0.jar:?]
	at org.apache.storm.kafka.DynamicBrokersReader.getNumPartitions(DynamicBrokersReader.java:111) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.DynamicBrokersReader.getBrokerInfo(DynamicBrokersReader.java:84) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.trident.ZkBrokerReader.<init>(ZkBrokerReader.java:44) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.KafkaUtils.makeBrokerReader(KafkaUtils.java:58) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.KafkaSpout.open(KafkaSpout.java:77) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.daemon.executor$fn__7990$fn__8005.invoke(executor.clj:604) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:482) [storm-core-1.0.2.jar:1.0.2]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_79]
32257 [Thread-24-kafka-reader-executor[6 6]] ERROR o.a.c.ConnectionState - Connection timed out for connection string (yj-es-6:2181,yj-es-5:2181) and timeout (15000) / elapsed (15084)
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197) [curator-client-2.10.0.jar:?]
	at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88) [curator-client-2.10.0.jar:?]
	at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:116) [curator-client-2.10.0.jar:?]
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:489) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108) [curator-client-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191) [curator-framework-2.10.0.jar:?]
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38) [curator-framework-2.10.0.jar:?]
	at org.apache.storm.kafka.DynamicBrokersReader.getNumPartitions(DynamicBrokersReader.java:111) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.DynamicBrokersReader.getBrokerInfo(DynamicBrokersReader.java:84) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.trident.ZkBrokerReader.<init>(ZkBrokerReader.java:44) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.KafkaUtils.makeBrokerReader(KafkaUtils.java:58) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.kafka.KafkaSpout.open(KafkaSpout.java:77) [storm-kafka-1.0.2.jar:1.0.2]
	at org.apache.storm.daemon.executor$fn__7990$fn__8005.invoke(executor.clj:604) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:482) [storm-core-1.0.2.jar:1.0.2]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_79]"
STORM-2123,Adding a ByteKeyValueScheme class,It is a common use case to send and receive byte key and values via Kafka. Unfortunately the storm-kafka package misses support for this. The pull request https://github.com/apache/storm/pull/1711 adds such a ByteKeyValueScheme class .
STORM-2121,StringKeyValueScheme doesn't override getOutputFields(),"In org.apache.storm.kafka StringKeyValueScheme extends StringScheme. However it doesn't override the method getOutputFields() from StringScheme 
{code}
public Fields getOutputFields() {
        return new Fields(STRING_SCHEME_KEY);
    }
{code}
And this method returns only one field, instead of two (one for key and one for value), which causes problems. 
It would be better to override the method getOutputFields() in StringKeyValueScheme with e.g. 
{code}
    @Override
    public Fields getOutputFields() {
        return new Fields(FieldNameBasedTupleToKafkaMapper.BOLT_KEY, FieldNameBasedTupleToKafkaMapper.BOLT_MESSAGE);
    }
{code}
The important thing is, that getOutputFields() should return two fields and not one. "
STORM-2116,[Storm SQL] Support 'CASE' statement,As we don't support it yet.
STORM-2114,[Storm SQL] Support 'Date' and 'Timestamp' literal,"We don't support it yet so we can't use '2016-01-01' as date literal and also  '2016-01-01 00:00:00' as timestamp literal. 
Adding it also would make testing functions with Date type easier. "
STORM-2113,[Storm SQL] 'OR' and 'AND' operators cannot handle more than 2 operands,"When we use 'IN' statement, Calcite is converting this statement of 'OR' with multiple arguments.

While testing I found current 'OR' implementation cannot handle more than 2 parameters, which effectively introduce a bug on 'IN' (and 'NOT IN').

It should be fixed to support 'IN' properly. Also need to consider applying 'short curcuit' on that."
STORM-2112,Catch up 'SQL Reference' page on Calcite to provide as many features as we can,"https://calcite.apache.org/docs/reference.html

Above page represents Calcite availability to parse and handle SQL statements and also types and built-in functions. We need to catch up this for providing as many feature as we can based on Calcite.

Note for contributors: RexImpTable, BuiltMethod, SqlFunctions on Calcite should be very helpful to implement missing functions, operators, and so on."
STORM-2111,[Storm SQL] support 'LIKE' and 'SIMILAR TO',"Currently Storm SQL doesn't support 'LIKE' and 'SIMILAR TO' keywords.
Its implementation is existing on SqlFunctions so what we need to do is just registering them to ImpTable and make some unit tests.

NOT LIKE and NOT SIMILAR TO is represented as NOT(LIKE) and NOT(SIMILAR TO) so it should be automatically supported after."
STORM-2108,Spout unable to recover after worker crashes...continuously see discarding messages errors...,"Hello:

We have a new situation that occurred after we upgraded to storm 1.0.2 (from 0.9.2). We had a worker crash due to a bug in our code that caused a stack overflow exception. The supervisor detected the issue and restarted the worker as expected.

After the worker crashed, the many of the tuples the spout sends out continuously time out and our throughput slows to a crawl. The spout seems to send out tuples until it hits the max spout pending. Then some of the tuples time out and it sends the next batch.

We saw this exception in the spout log when the worker crashed:

2016-09-21T01:54:32,749 [refresh-connections-timer] [org.apache.storm.messaging.netty.Client] [INFO]> closing Netty Client Netty-Client-/10.103.16.14:31437
2016-09-21T01:54:32,750 [refresh-connections-timer] [org.apache.storm.messaging.netty.Client] [INFO]> waiting up to 600000 ms to send 0 pending messages to Netty-Client-/10.103.16.14:31437
2016-09-21T01:55:35,925 [Netty-server-localhost-31009-worker-1] [org.apache.storm.messaging.netty.StormServerHandler] [ERROR]> server errors in handling the request
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_77]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_77]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_77]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_77]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) ~[?:1.8.0_77]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-1.0.2.jar:1.0.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
2

And now we just continuously see these messages in the spout logs:

2016-09-21T02:03:35,513 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,644 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,774 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,817 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,849 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,073 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,141 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,169 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,340 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,365 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,416 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,560 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,607 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,660 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,865 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,894 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,026 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,051 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,065 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,219 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed

The worker that died (10.103.16.14.31437) was restarted by the supervisor, but I don't see any log messages in the logs indicating that it is receiving any tuples. The ""is being closed"" messages in the spout logs make me think that storm has failed to close its connection.

This has happened to us repeatedly since the upgrade. Does anyone have any suggestions about how to fix this issue? Also, I originally thought it might be related to STORM-1560, but I don't see the exception that is mentioned in that ticket.

Thanks, and I would greatly appreciate any help
"
STORM-2107,Storm-kafka-client: Slow topology start breaks the Kafka Client with session timeouts ,"Specifically CommitFailedExceptions are being thrown when an attempt to commit the offset is made. The problem is detailed here: 

https://cwiki.apache.org/confluence/display/KAFKA/KIP-41%3A+KafkaConsumer+Max+Records

On topology start, the spout will start emitting tuples before all bolts are prepared. This means that tuples will hang around in the topology until everything is ready to start processing. This caused the max spout pending limit to be hit, and the spout to stop polling kafka for the duration of the timeout. It seems like there could be other use cases that cause this too. 

I propose we start polling on a timer kafka to avoid this.

"
STORM-2106,Storm Kafka Client is paused while failed tuples are replayed,"With the changes in STORM-2087, the kafka 10 spout will limited to emitting tuples that are within the poll() size for kafka. This means that if the first tuple in a batch from kafka is failed, the spout will not emit more than the size of the batch from kafka until the tuple is either processed successfully or given up on. This behavior is exacerbated by the exponential backoff retry policy.  

There probably needs to be bookkeeping for the next emittable offset."
STORM-2102,Introduce new sql external module: storm-sql-hbase,This issue tracks adding storm-sql-hbase as a new sql external module.
STORM-2096,Error creating jar file with maven,"I tried to alter the Wordcount file inside storm in order to make it take a file path externally in the command line. Then, I tried to make the jar file for operation using maven. 

First i ran mvn compile. The output is:-

[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 1.0.2
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 52 source files to /opt/storm/examples/storm-starter/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-starter ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 9.178 s
[INFO] Finished at: 2016-09-15T11:18:13+00:00
[INFO] Final Memory: 52M/691M
[INFO] ------------------------------------------------------------------------

The i ran mvn test. The output is:-

[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 1.0.2
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 52 source files to /opt/storm/examples/storm-starter/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /opt/storm/examples/storm-starter/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 8 source files to /opt/storm/examples/storm-starter/target/test-classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
  symbol:   class Rankings
  location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
  symbol:   class Rankings
  location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,52] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,44] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[52,9] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[65,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,25] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[89,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[93,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,25] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[97,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[122,9] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,56] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,94] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,22] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,87] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,13] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,51] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,89] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[153,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[174,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[188,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[203,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[228,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[232,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,5] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,23] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[INFO] 141 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.608 s
[INFO] Finished at: 2016-09-15T10:33:18+00:00
[INFO] Final Memory: 51M/706M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project storm-starter: Compilation failure: Compilation failure:
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,52] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,44] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[52,9] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[65,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,25] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[89,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[93,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,25] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[97,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[122,9] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,56] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,94] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,22] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,87] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,13] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,51] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,89] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[153,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[174,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[188,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[203,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[228,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[232,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,5] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,23] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException

I tried removing the rank files. Even that didn't rectify the problem. Please advice how I can make a jar file to run the modified wordcount file."
STORM-2094,RichSpoutBatch Executor doesn't refresh number of emitted items at collector reset,"Trident wraps IRichSpout  into a RichSpoutBatchExecutor which is a Trident spout.
Inside RichBatchSpoutExecutor is a for loop to emit batch of tuples:
for(int i=0; i<_maxBatchSize; i++) {
                _spout.nextTuple();
                if(_collector.numEmitted < i) {
                    break;
                }
            }

Due to numEmitted of CaptureCollector not being refreshed it carries the emitted items count from previous batch and thus causes spout to block for long durations even when it has nothing to emit."
STORM-2091,"storm-kafka-client, NoSuchMethod on KafkaConsumer.subscribe ","Hello,

I am currently using storm-kafka-client ""1.0.2"" with kafka-clients ""0.10.0.1"" and I am encountering the following stacktrace at runtime of my topology:

java.lang.NoSuchMethodError: org.apache.kafka.clients.consumer.KafkaConsumer.subscribe(Ljava/util/List;Lorg/apache/kafka/clients/consumer/ConsumerRebalanceListener;)V at org.apache.storm.kafka.spout.KafkaSpout.subscribeKafkaConsumer(KafkaSpout.java:350) at org.apache.storm.kafka.spout.KafkaSpout.activate(KafkaSpout.java:341) at org.apache.storm.daemon.executor$fn__7885$fn__7900$fn__7931.invoke(executor.clj:640) at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:745)

So I checked the source code and Indeed the subscribe method takes a collection of topics in addition of the listener: https://github.com/apache/kafka/blob/0.10.0.1/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L795

I though version 1.0.2 of storm-kafka-clients should work with kafka 0.10.0.1, do you have any idea ? Thanks."
STORM-2085,Remove guava from storm-core pom.,Guava is not used by storm-core. Remove it as a dependency from the pom.xml
STORM-2080,storm-submit-tools license check failure,
STORM-2077,KafkaSpout doesn't retry failed tuples,"KafkaSpout does not retry all failed tuples.

We used following Configuration:
        Map<String, Object> props = new HashMap<>();
        props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""c1"");
        props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, ByteArrayDeserializer.class.getName());
        props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, ByteArrayDeserializer.class.getName());
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker.bootstrapServer());

        KafkaSpoutStreams kafkaSpoutStreams = new KafkaSpoutStreams.Builder(FIELDS_KAFKA_EVENT, new String[]{""test-topic""}).build();

        KafkaSpoutTuplesBuilder<byte[], byte[]> kafkaSpoutTuplesBuilder = new KafkaSpoutTuplesBuilder.Builder<>(new KeyValueKafkaSpoutTupleBuilder(""test-topic"")).build();
        KafkaSpoutRetryService retryService = new KafkaSpoutLoggedRetryExponentialBackoff(KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.milliSeconds(1), KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.milliSeconds(1), 3, KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.seconds(1));

        KafkaSpoutConfig<byte[], byte[]> config = new KafkaSpoutConfig.Builder<>(props, kafkaSpoutStreams, kafkaSpoutTuplesBuilder, retryService)
                .setFirstPollOffsetStrategy(UNCOMMITTED_LATEST)
                .setMaxUncommittedOffsets(30)
                .setOffsetCommitPeriodMs(10)
                .setMaxRetries(3)
                .build();

kafkaSpout = new org.apache.storm.kafka.spout.KafkaSpout<>(config);


The downstream bolt fails every tuple and we expect, that those tuple will all be replayed. But that's not the case for every tuple."
STORM-2075,Storm SQL - expand supporting external components,"This epic tracks the effort of the phase III development of StormSQL.

For now Storm SQL only supports Kafka as a data source, which is limited for normal use cases. We would need to support others as well. Candidates are external modules.

And also consider supporting State since Trident provides a way to set / get in batch manner to only State. Current way (each) does insert a row 1 by 1.


|| module || producer || consumer || issue ||
| Cassandra | N/A | State | |
| Druid | N/A | State | |
| ElasticSearch | N/A | State | |
| EventHubs | Opaque/Transactional Spout | N/A | |
| HBase | N/A | State | STORM-2102 |
| HDFS | N/A | State | STORM-2082 |
| Hive | N/A | State | |
| JDBC | N/A | State | |
| Kafka | Opaque/Transactional Spout | State | STORM-2089 |
| Kafka-Client (New) | N/A | N/A | |
| Kinesis | N/A | N/A | |
| MongoDB | N/A | State | STORM-2103 |
| MQTT | N/A | BaseFunction | |
| OpenTSDB | N/A | State | |
| Redis | N/A | State | STORM-2099 |
| Solr | N/A | State | |
"
STORM-2073,Reduce multi-steps on visitProject into one,"In STORM-1434 we revamped the way to build Trident topology for Storm SQL.

While revamping visitProject(), it should handle multiple things (calculate expression, projection) and moreover Trident doesn't allow duplicated field name. So we end up having multiple steps - each -> project -> each -> project - which doesn't look good.

STORM-2072 is introducing a way to map / flatMap with specifying different output fields. With this, we can reduce visitProject() to have just 1 step instead of 4 steps."
STORM-2069,Wrong github url in tutorial page,"{noformat}
A ""stream grouping"" answers this question by telling Storm how to send tuples between sets of tasks. Before we dig into the different kinds of stream groupings, let's take a look at another topology from storm-starter. This WordCountTopology reads sentences off of a spout and streams out of WordCountBolt the total number of times it has seen that word before:

{noformat}

the word ""storm-starter"" is pointing to https://github.com/apache/storm/blob/1.0.1/examples/storm-starter, whereas the correct url is https://github.com/apache/storm/blob/v1.0.1/examples/storm-starter
"
STORM-2068,org.apache.storm.utils.NimbusLeaderNotFoundException,"my storm version is apache-storm-2.0.0-SNAPSHOT
from git clone ""storm github"", mvn clean install command
and, setting storm.yaml like bellow:

storm.zookeeper.servers:
 - ""NN""
 - ""DN01""
 - ""DN02""
 - ""DN03""
 - ""DN04""

nimbus.seeds: [""NN""]


supervisor.slots.ports:
 - 6700
 - 6701
 - 6702
 - 6703

ui.port: 8989

storm cluster is woring well. but, UI, storm jar command rise ERROR


org.apache.storm.utils.NimbusLeaderNotFoundException: Could not find leader nimbus from seed hosts [""NN""]. Did you specify a valid list of nimbus hosts for config nimbus.seeds?
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:106)
	at org.apache.storm.ui.core$all_topologies_summary.invoke(core.clj:508)
	at org.apache.storm.ui.core$fn__3916.invoke(core.clj:1141)
	at org.apache.storm.shade.compojure.core$make_route$fn__989.invoke(core.clj:100)
	at org.apache.storm.shade.compojure.core$if_route$fn__977.invoke(core.clj:46)
	at org.apache.storm.shade.compojure.core$if_method$fn__970.invoke(core.clj:31)
	at org.apache.storm.shade.compojure.core$routing$fn__995.invoke(core.clj:113)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__999.invoke(core.clj:118)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1956.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1491.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__1412.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__3226.invoke(helpers.clj:54)
	at org.apache.storm.ui.core$catch_errors$fn__4103.invoke(core.clj:1425)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2955.invoke(keyword_params.clj:35)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2998.invoke(nested_params.clj:84)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2927.invoke(params.clj:64)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1491.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__3213.invoke(flash.clj:35)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__3199.invoke(session.clj:98)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2821.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__2825.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
"
STORM-2065,Add tooltip descriptions for config keys to the UI,"As an admin/a user, I would like to know the purpose of various config settings I see on the UI, so that I can make the correct changes to my cluster/topology.



This could be accomplished with a simple annotation for each key in our config classes, similar to those we use for validation, but just a simple string.  The annotations could simply duplicate the text we already have in the javadoc comments.

The UI could send this text to the browser in as a tooltip pop-up when the mouse hovers over one of the config keys."
STORM-2062,Hive streaming doesn't support non string partition fields,"create hive table with an int partition column

CREATE TABLE CDRDWH.CDR_FACT (
geo_id int,
time_id smallint,
cust_id smallint,
vend_id smallint,
cust_rel_id smallint,
vend_rel_id smallint,
route tinyint,
connect boolean,
earlyEvent boolean,
Call_duration_cust double,
I_PDD double,
E_PDD double,
orig_number string,
term_number string
)
partitioned by (date_id int)
clustered by (geo_id, time_id) into 16 buckets
stored as ORC
tblproperties (""orc.compress""=""SNAPPY”);

When i try to stream my topolgy output to Hive I get the following exception:

11829 [Thread-31-hivewriter-executor[5 5]] ERROR o.a.s.d.executor - 
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
at org.apache.storm.tuple.TupleImpl.getStringByField(TupleImpl.java:153) ~[storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper.mapPartitions(DelimitedRecordHiveMapper.java:92) ~[storm-hive-1.0.1.jar:1.0.1]
at org.apache.storm.hive.bolt.HiveBolt.execute(HiveBolt.java:112) [storm-hive-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]


Line 92 of DelimtedRecordHiveMapper is attempting to access my integer field as a String and the subsequent exception is thrown
 @Override
    public List<String> mapPartitions(Tuple tuple) {
        List<String> partitionList = new ArrayList<String>();
        if(this.partitionFields != null) {
            for(String field: this.partitionFields) {
                partitionList.add(tuple.getStringByField(field));
            }
        }
        if (this.timeFormat != null) {
            partitionList.add(getPartitionsByTimeFormat());
        }
        return partitionList;
    }"
STORM-2061,Storm-Elasticsearch https client,"Storm-elasticsearch currently supports elasticsearch transportClient, however, there should be an ability to create httpconnection to bulk REST api provided by ES along with the ability to use https connection.

"
STORM-2060,Consolidate duplicated code in logviewer,lots of duplicated code in daemonlog-page and log-page functions
STORM-2053,Support connection between existing Trident Stream and Storm SQL (Input / Output) ,"Currently Storm SQL requires users to set input / output data sources on DDL which restricts whole topology constructed by only SQL statement.

It might be good to have hybrid construction of Trident topology via providing API for connecting Trident API and Storm SQL. (It would be a concatenation of Streams.)

Things to check: Trident doesn't preserve type information which can be blocker for this."
STORM-2051,Flux should support the builder pattern,"While trying to work with {{Flux}} and the {{storm-kafka-client}} package we noticed that they are incompatible, unfortunately, as the needed {{KafkaSpoutConfig}} is based on the builder pattern. Unless some hacky method is used it will not be possible to configure a {{KafkaSpout}} and instantiate/use it with a Flux-based topology.

Flux could be enhanced to support the builder pattern with the following yaml configuration as a proposal:

{code}
builder:
  - id: ""spoutConfigBuilder""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig.Builder""
    builderMethod: ""build""
    constructorArgs:
      - [...]
    properties:
      - [...]
    configMethods:
      - [...]
components:
  - id: ""spoutConfig""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig""
    builderRef: ""spoutConfigBuilder""
spouts:
  - id: ""kafkaSpout""
    className: ""org.apache.storm.kafka.spout.KafkaSpout""
    constructorArgs:
      - ref: ""spoutConfig""
{code}

Unfortunately, for now, we are busy with other tasks so we cannot work on a patch for Flux. But we thought it's better to report / suggest this enhancement nevertheless."
STORM-2049,MQTT Client ID Length Should Be Restrictive,"According the MQTT specification:
The Server *MUST* allow ClientIds which are between 1 and 23 UTF-8 encoded bytes in length, and that contain only the characters

""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"" [MQTT-3.1.3-5].

 

The Server *MAY* allow ClientId’s that contain more than 23 encoded bytes.

When using MQTT spout with message brokers that don't allow names longer than 23 characters, storm is unable to connect to the broker.  

Here's how storm constructs the client ID, which can cause names to be longer than 23 characters.
{noformat}String clientId = this.topologyName + ""-"" + this.context.getThisComponentId() + ""-"" +
                this.context.getThisTaskId();{noformat}

Options:
Restrict the final length of the client ID to be less than 23 characters
Allow setting the clientID using MQTTOptions
Attempt to connect, if fail - trim the length and try again.
"
STORM-2048,Refactor code blocks which are ported to for-loop to Java Stream API,"We just changed minimum requirement for master branch to Java 1.8 from STORM-2041. 

Thanks for the change we can change ported code block which was functional style to similar style again.

We could even broaden the boundary of this issue for applying other benefits from Java 8, or file separate issues."
STORM-2046,Errors when using TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE in local mode.,"When using a LocalCluster during tests, if {{TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE}} is specified, {{assert-can-serialize}} attempts to destructure a Java model object and throws, killing the worker. A minimal-ish case and the full logs are here: https://gist.github.com/ckolbeck/557734429e62b097efa9382a714122b0"
STORM-2044,Nimbus should not make assignments crazily when Pacemaker goes down and up,"        Now pacemaker is a stand-alone service and no HA is supported. When it goes down, all the workers's heartbeats will be lost. It will take a long time to recover even if pacemaker goes up immediately if there are dozens GB of heartbeats. During the time worker heartbeats are not restored completely, Nimbus will think these workers are dead because of heartbeats timeout and reassign these ""dead"" workers continuously until heartbeats restore to normal. So, during recovery time, many topologies will be reassigned continuously and the throughout will goes very down.  
        This is not acceptable. 
        So i think, pacemaker is not suitable for production if the problem above exists.
               i think several ways to solve this problem:
              1. pacemaker HA
              2. when pacemaker does down, notice nimbus not to reassign any more until it recover"
STORM-2043,Nimbus should not make assignments crazy when Pacemaker down,"When pacemaker goes down, all the heartbeats of workers are lost. These heartbeats will need a long time to recover even if pacemaker goes up immediately if it costs dozens of GB memory. During the time worker heartbeats are not complete，Nimbus will think the workers are died( heartbeat time out ),  and reassign these workers crazily. But actually the workers are healthy, the reassignment will move in cycles until pacemaker heartbeats recover. During this time, all the topologies's throughout will goes down. We should avoid this, because Pacemaker has no HA."
STORM-2036,Fix minor bug in RAS Tests,
STORM-2035,Surface supervisor configs to nimbus for serving to UI,"The idea for this came from [~kabhwan] in PR https://github.com/apache/storm/pull/1592.

It is to show in the supervisor page the host's config. This is not available to the UI backend or to nimbus right now, so it would mean a new communication path to obtain the configs.

Should this be stored in ZK? It is the only link currently, so either supervisor adds a new link or ZK gets the config."
STORM-2034,Remove Show System Stats button and refactor UI accordingly,"Removing the button and instead refactoring UI to distinguish between system vs. user stats gets us:

# no need to switch back and forth between showing or hiding system stats, it is all in one screen at all the time.
# less logic in core and stats dealing with the sys filtering
# consistency fixes that will come as a result of this (at times we hide system workers, but we don't hide system executors, etc.)"
STORM-2033,new Scheduler: UserDefinedScheduler,
STORM-2031,Multilang: support distribute cache API a.k.a BlobStore,"Distribute Cache API (BlobStore) comes in Storm 1.0.0 but there's no support for multi-lang.

It would be better to let multilang users enjoy the benefits together.

I just marked as 'Wish' since BlobStore implementation is fit for Java and it would be not easy to cover with only improving multilang protocol. BlobStore API exposes InputStream and OutputStream for downloading / creating & updating the contents so we should deal with this for other languages in order to get the feature."
STORM-2030,Multilang: support stateful bolt,"Stateful bolt with checkpointing feature comes in Storm 1.0.0 but there's no support for multi-lang.

It would be better to let multilang users enjoy the benefits together.

Implementation details and difficulty would be up to where we define and manage the State, Java (ShellBolt) vs Multi-lang."
STORM-2029,Multilang: support windowing feature,"Windowing feature comes in Storm 1.0.0 but there's no support for multi-lang.

It would be better to let multilang users enjoy the benefits together. 

Implementation should be fairly simple since there's tiny change in point of Bolt interface view."
STORM-2027,Possible Race Condition issue in SlidingWindow,"The function SlotBasedCounter#incrementCount() presents a bug. If 2 concurrent threads want to update the same counter, the result is different from the expected.
"
STORM-2025,dropping messages in withTumblingWindow,"when i use {{withTumblingWindow}} and process the input messages, if the processing time is longer than input rate, we will not get all input messages.

{code}
int count=0;
	@Override
	public void execute(TupleWindow inputWindow) {
		try {
			List<Event> windowEvenets = new ArrayList<>();
			for(Tuple tuple: inputWindow.get()) {
				count++;
				/* some operation here */
			}
			logger.info(count + ""======= Process event "");
			Thread.sleep(4000);
		}
		catch (Exception ex) {
			ex.printStackTrace();
		}
	}
{code}

The topology is as follow:
{code}
 TopologyBuilder builder = new TopologyBuilder();
            builder.setSpout(""KafkaSpout"", new KafkaSpout(kafkaConfig), 1);
            builder.setBolt(""WindowInputTest"", new WindowInputTest(zookeeperHosts).withTumblingWindow(new BaseWindowedBolt.Duration(4,TimeUnit.SECONDS)), 1).shuffleGrouping(""KafkaSpout"");
{code}"
STORM-2024,Add local store autocomplete to the logger name input field as a convenience (dynamic log level settings),"The UI should remember previous logger names I use when setting dynamic log levels, much like on form submit when the input fields are autocompleted by browsers.

In order to implement this in Storm UI, we'd have to use an autocomplete input field like Selectize, and store previous entries in Local Storage. These would trigger on the dynamic log level action (Add) so that when you add something, the value gets cached.

We'd then take that and prepulate the Selectize control as a convenience."
STORM-2019,NullPointerException in Kafka-Spout,"KafkaSpout reports following error:
java.lang.NullPointerException
        at java.util.TreeMap.rotateLeft(TreeMap.java:2220) ~[?:1.8.0_77]
        at java.util.TreeMap.fixAfterInsertion(TreeMap.java:2287) ~[?:1.8.0_77]
        at java.util.TreeMap.put(TreeMap.java:582) ~[?:1.8.0_77]
        at org.apache.storm.kafka.PartitionManager.fill(PartitionManager.java:235) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.next(PartitionManager.java:138) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) ~[stormjar.jar:?]
        at applications.spout.KafkaSpoutWrapper.nextTuple(KafkaSpoutWrapper.java:64) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7885$fn__7900$fn__7931.invoke(executor.clj:645) ~[storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
"
STORM-2015,logviewer does not download file when the directory is a symbolic link fails with 404 page not found,"logviewer does not download file when the directory is a symbolic link it fails with 404 page not found.

(defn download-log-file [fname req resp user ^String root-dir]
  (let [file (.getCanonicalFile (File. root-dir fname))]
    (if (.exists file)

      (-> (resp/response ""Page not found"")
          (resp/status 404)))))

Replace storm root-dir as an actual directory it succeeds to download the file.

Symbolic link for log locations is standard practice."
STORM-2013,Upgrade Netty to 3.10.6,"Since Netty 3.9 , it have fix some bugs and improve performance . "
STORM-2007,DRPC requests that take a long time to process fail with 500 error code,"This can be reproduced by altering a DRPC topology to sleep for 90 seconds before returning a result.   Hitting the DRPC server's corresponding endpoint produces the following result. 

{noformat}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=ISO-8859-1""/>
<title>Error 500 Server Error</title>
</head>
<body>
<h2>HTTP ERROR: 500</h2>
<p>Problem accessing /drpc/ppouloskTest/hello. Reason:
<pre>    Server Error</pre></p>
<hr /><i><small>Powered by Jetty://</small></i>
{noformat}"
STORM-2005,Getting metrics for client connection to Netty-Client-slave3/133.31.12.31:6700,storm stop working . in the log always record 
STORM-2004,Investigate throwing a better exception in the case of Security failures in Storm,"The Storm UI throws generic 500 error on ""SASL Authentication not open"" and other Thrift errors for calls via the UI. Investigate the https://raw.githubusercontent.com/apache/storm/master/storm-core/src/jvm/org/apache/storm/generated/Nimbus.java to see if a better return can be found."
STORM-2002,KafkaSpout data type error ,fix data type error in KafkaSpout  doSeek
STORM-2001,New Kafka Spout Consume Max Records,"Kafka 0.10 have add a new parameter (max.poll.records) that use to control the number of messages returned in a single call to poll(). It's useful to control topology QPS . 

[Implement max.poll.records for new consumer (KIP-41)|https://issues.apache.org/jira/browse/KAFKA-3007]
"
STORM-1998,Dynamic Topology Config and Resources Updates,"With this new feature, users will be able to update any topology config for any topology on the fly without having to restart their topology.  For users running the Resource Aware Scheduler (RAS), it will allow the user to dynamically scale-in and scale-out resource usage on a per component basis.  When RAS is used, this new feature will also allow users to dynamically change the scheduling strategy for a topology."
STORM-1996,Supplement Kafka Test Case,Storm Kafka Client's test package seems not a test case . 
STORM-1992,Deploy multilang-javascript code as node package,"Now that storm includes Flux, it is easier than ever to deploy a topology with javascript components. If the Bolt and Spout base classes defined in storm/multi-lang/javascript were available in a node.js package on https://www.npmjs.com/ it would allow node.js storm users to take advantage of node's built in package manager to develop their own bolts and spouts.

It would be relatively trivial to add some maven tasks to storm/multi-lang/javascript/pom.xml to take the storm.js resource and package it in a node module and submit it to npm.

This could be added to the pom as a separate profile so it wouldn't impact the normal storm build process.

This integration will also make it easier to add unit tests for storm.js

For additional background see this discussion: 

http://mail-archives.apache.org/mod_mbox/storm-dev/201607.mbox/%3CCAD8EKPHc6O1LCnoQUUoYoDuMQ3uSaNpD5gR4onK%2B0EL5_qcZ3Q%40mail.gmail.com%3E

If this sounds like a worthwhile addition to the project I would be happy to submit a PR.
"
STORM-1991,Support auto.commit.interval in Kafka Client,Support auto.commit.interval in Kafka Client
STORM-1990,Make some constant unvisible,"Some constant used as default config , make them private and unvisible ."
STORM-1986,Local BlobStore only lists blobs in local rather than all available blobs on nimbuses.,"This is follow up issue from [~revans2]'s comment on STORM-1977

https://github.com/apache/storm/pull/1574#issuecomment-233638403

Quote part of the comment describing bug:
{quote}
The one bug I saw while going through the code is that when we list keys, we are doing it only from the local storage, not from ZK.
{quote}

This is not same behavior for HDFS backed so it is definitely a bug which should be addressed."
STORM-1985,Provide a tool for showing and killing corrupted topology,"After STORM-1976, Nimbus doesn't clean up corrupted topologies.
(corrupted topology means the topology whose codes are not available on blobstore.)

Also after STORM-1977, no Nimbus is gaining leadership if one or more topologies are corrupted, which means all nimbuses will be no-op.

So we should provide a tool to kill specific topology without accessing leader nimbus (because there's no leader nimbus at that time). The tool should also determine which topologies are corrupted, and show its list or clean up automatically."
STORM-1984,Race during rebalance,"We have been seeing an issue with a storm cluster getting into a restart loop because of bad topology state saved in ZK.

On startup, we are seeing a rebalance timer being set with a time value of nil.

This rebalance was called during a startup state transition here..

https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L330-L336

The problem is that topology-action-options is nil in storm-base.  

(I added a temporary debug print)

2016-07-19 14:41:56.604 b.s.d.nimbus [INFO] In state-transitions #backtype.storm.daemon.common.StormBase{:storm-name ""test1"", :launch-time-secs 1468879726, :status {:type :rebalancing}, :num-workers 3, :component->executors {""__system"" 0, ""__acker"" 3, ""exclaim2"" 2, ""exclaim1"" 3, ""word"" 10}, :owner ""hadoopqa"", :topology-action-options nil, :prev-status {:type :active}}

If nimbus happens to crash during the rebalancing state, before the scheduler can reschedule the topology and then return it back to active or inactive, but after storm-base was set to nil here....

https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L292-L299

Then we get into a state where nimbus will crash repeatedly if supervised on startup.

We should remove the set of topology options to nil in do-rebalance, and / or ignore the rebalance on startup if the delay can't be read."
STORM-1983,"Topology Page: Visualization form is generated for each pushing of the 'Show Visualization' button, and only first one shows graph properly","In topology page, visualization form is generated for each pushing of the 'Show Visualization' button, and only first one shows graph properly.

I'll attach screenshot on this.
"
STORM-1982,Topology running Local Cluster is not properly destroyed,"The process which run LocalCluster is not properly destroyed.

I'll attach the console log after kill is triggered, and jstack dump."
STORM-1981,storm command hangs,"I ran the following command which threw an exception and never completed. I understand that the topology has some issues which caused this hang. The problem that I want to emphasize is the *hang*. We can't assume that user code will always be doing things correctly.
{code}
test@host:/grid/0/hadoopqe$ /usr/hdp/current/storm-client/bin/storm -c java.security.auth.login.config=/etc/storm/conf/client_jaas.conf -c storm.thrift.transport=org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin jar /usr/hdp/current/storm-client/contrib/storm-starter/storm-starter-topologies-1.0.1.jar org.apache.storm.starter.trident.TridentKafkaWordCount nat-d7-vnas-storm-5.openstacklocal:6667
Running: /usr/jdk64/jdk1.8.0_77/bin/java -server -Ddaemon.name= -Dstorm.options=java.security.auth.login.config%3D%2Fetc%2Fstorm%2Fconf%2Fclient_jaas.conf,storm.thrift.transport%3Dorg.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin -Dstorm.home=/storm -Dstorm.log.dir=/grid/0/log/storm -Djava.library.path=/usr/local/lib:/o\
pt/local/lib:/usr/lib -Dstorm.conf.file= -cp /storm/lib/slf4j-api-1.7.7.jar:/storm/lib/clojure-1.7.0.jar:/storm/lib/minlog-1.3.0.jar:/storm/lib/log4j-over-slf4j-1.6.6.jar:/storm/lib/ambari-metrics-storm-sink.jar:/storm/lib/reflectasm-1.10.1.jar:/storm/lib/ring-cors-0.1.5.jar:/storm/lib/disruptor-3.3.2.jar:/storm/lib/storm-rename-hack-1.0.1.jar:/storm/lib/log4j-api-2.1.jar:/storm/lib/log4j-slf4j-impl-2.1.jar:/storm/lib/zoo\
keeper.jar:/storm/lib/kryo-3.0.3.jar:/storm/lib/log4j-core-2.1.jar:/storm/lib/storm-core-1.0.1.jar:/storm/lib/servlet-api-2.5.jar:/storm/lib/objenesis-2.1.jar:/storm/lib/asm-5.0.3.jar:/storm/extlib/scala-library-2.10.4.jar:/storm/extlib/storm-kafka-1.0.1.jar:/storm/extlib/kafka_2.10-0.10.0.jar org.apache.storm.daemon.ClientJarTransformerRunner org.apache.storm.hack.StormShadeTransformer /usr/hdp/current/storm-client/contrib/storm-starter/storm-starter-topologies-1.0.1.jar /tmp/1bb42b0a4d2f11e6871dfa163e434686.jar
Running: /usr/jdk64/jdk1.8.0_77/bin/java -client -Ddaemon.name= -Dstorm.options=java.security.auth.login.config%3D%2Fetc%2Fstorm%2Fconf%2Fclient_jaas.conf,storm.thrift.transport%3Dorg.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin -Dstorm.home=/storm -Dstorm.log.dir=/grid/0/log/storm -Djava.library.path=/usr/local/lib:/o\
pt/local/lib:/usr/lib:/usr/hdp/current/storm-client/lib -Dstorm.conf.file= -cp /storm/lib/slf4j-api-1.7.7.jar:/storm/lib/clojure-1.7.0.jar:/storm/lib/minlog-1.3.0.jar:/storm/lib/log4j-over-slf4j-1.6.6.jar:/storm/lib/ambari-metrics-storm-sink.jar:/storm/lib/reflectasm-1.10.1.jar:/storm/lib/ring-cors-0.1.5.jar:/storm/lib/disruptor-3.3.2.jar:/storm/lib/storm-rename-hack-1.0.1.jar:/storm/lib/log4j-api-2.1.jar:/storm/lib/log4j-slf4j-impl-2.1.jar:/gri\
d/0/hdp/2.5.0.0-1016/storm/lib/zookeeper.jar:/storm/lib/kryo-3.0.3.jar:/storm/lib/log4j-core-2.1.jar:/storm/lib/storm-core-1.0.1.jar:/storm/lib/servlet-api-2.5.jar:/storm/lib/objenesis-2.1.jar:/storm/lib/asm-5.0.3.jar:/storm/extlib/scala-library-2.10.4.jar:/storm/extlib/storm-kafka-1.0.1.jar:/storm/extlib/kafka_2.10-0.10.0.jar:/tmp/1bb42b0a4d2f11e6871dfa163e434686.jar:/usr/hdp/current/storm-supervisor/conf:/storm/bin -Dstorm.jar=/tmp/1bb42b0a4d2f11e6871dfa163e434686.jar org.apache.storm.starter.trident.TridentKafkaWordCount nat-d7-vnas-storm-5.openstacklocal:6667
Using Kafka zookeeper url: nat-d7-vnas-storm-5.openstacklocal:6667 broker url: localhost:9092
Exception in thread ""main"" java.lang.ExceptionInInitializerError
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at clojure.lang.RT.classForName(RT.java:2154)
        at clojure.lang.RT.classForName(RT.java:2163)
        at clojure.lang.RT.loadClassForName(RT.java:2182)
        at clojure.lang.RT.load(RT.java:436)
        at clojure.lang.RT.load(RT.java:412)
        at clojure.core$load$fn__5448.invoke(core.clj:5866)
        at clojure.core$load.doInvoke(core.clj:5865)
...
...
...
        at clojure.lang.Var.invoke(Var.java:379)
        at org.apache.storm.LocalCluster.<clinit>(Unknown Source)
        at org.apache.storm.starter.trident.TridentKafkaWordCount.runMain(TridentKafkaWordCount.java:254)
        at org.apache.storm.starter.trident.TridentKafkaWordCount.main(TridentKafkaWordCount.java:240)
Caused by: java.lang.ClassNotFoundException: org.apache.ranger.authorization.storm.authorizer.RangerStormAuthorizer
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.storm.daemon.common$mk_authorization_handler.invoke(common.clj:421)
        at org.apache.storm.ui.core__init.load(Unknown Source)
        at org.apache.storm.ui.core__init.<clinit>(Unknown Source)
        ... 83 more
{code}"
STORM-1980,Command line arguments are not provided to shell/thrift topologies,"I have a non-JVM topology that I submit with the following command:
{code}
storm shell . /usr/bin/nodejs api/services/storm/agent.js 578c4b9315fe9c52562208cf
{code}

My argv when the topology is submitted might be as follows:
{code}
/usr/bin/nodejs
/tmp/jar/resources/api/services/storm/agent.js
578c4b9315fe9c52562208cf
local.streams.com
6627
/var/lib/storm/nimbus/inbox/stormjar-ee48aea5-a058-4232-93e2-69aa91dc816c.jar
{code}

However, when the topology is actually run, my command line arguments are missing:
{code}
/usr/bin/nodejs
/var/lib/storm/supervisor/stormdist/deployment-578c4b9315fe9c52562208cf-5-1468860387/resources/api/services/storm/agent.js
{code}
"
STORM-1978,Storm Druid Connector,Storm Bolt & Trident state implementation for Druid.
STORM-1975,Support default value for KafkaBolt,"Support acks , key and value serializer for kafka bolt"
STORM-1974,Using System.lineSeparator to replacement write a new line,Using System.lineSeparator to replacement write a new line . It will write message in once and reduce writer synchroniz .
STORM-1973,Using DefaultTopicSelector to replace StaticTopicSelector,"Replace StaticTopicSelector using DefaultTopicSelector , they are the same ."
STORM-1972,Storm throws java.lang.ClassNotFoundException on Bolt class,"I'm trying to debug very simple topology (1 spout 2 bolts)

public class JoinerTopologyTest {

public static void main(String[] args) throws IOException {
    Config conf = new Config();
    conf.setNumWorkers(5);
    conf.setDebug(true);

    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout(""SPOUT-1"",new MySpout(),1);
    builder.setBolt(""BOLT-1"",new Bolt1(), 3)
            .shuffleGrouping(""SPOUT-1"");
    builder.setBolt(""JOINER"", new JoinerBolt(),1)
            .shuffleGrouping(""BOLT-1"")
            .shuffleGrouping(""SPOUT-1"",""str1"");

    final LocalCluster cluster = new LocalCluster();
    cluster.submitTopology(""TOPO1"",conf,builder.createTopology());


    System.in.read();

    cluster.shutdown();
}
}

But when i run it from InteliJ IDEA i get:

java.lang.RuntimeException: java.lang.ClassNotFoundException: com.pixonic.zephyr.compaction.tests.Bolt1 at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:181) ~[storm-core-1.0.1.jar:1.0.1] at org.apache.storm.utils.Utils.getSetComponentObject(Utils.java:430) ~[storm-core-1.0.1.jar:1.0.1]
and

[Thread-15] ERROR o.a.s.d.worker - Error on initialization of server mk-worker java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.storm.daemon.acker at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:181) ~[storm-core-1.0.1.jar:1.0.1] at org.apache.storm.utils.Utils.getSetComponentObject(Utils.java:430) ~[storm-core-1.0.1.jar:1.0.1]
but same topology runs well in Cluster mode. PS in my pom.xml in debug mode i have:

    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>storm-core</artifactId>
        <version>1.0.1</version>
    </dependency>

project to reproduce bug: https://github.com/holinov/storm-101-localcluster/tree/master
"
STORM-1971,HDFS Timed Synchronous Policy,When the data need to be wrote to HDFS is not very large in quantity . We need a timed synchronous policy to flush cached date into HDFS periodically.
STORM-1967,Kafka 0.8 Incompatible with Storm 1.0.1,"Had a Storm Cluster deployed and functioning with storm 0.9.5. Updated the cluster and the topology to 1.0.1, leaving the same kafka version (0.8). Kafka Spout would not function, threw Kafka Buffer Underflow exceptions.

Updating to Kafka 0.9 fixed this issue, but I'm told I shouldn't have had to."
STORM-1965,Show warning message when dot ('.') is used for topology name or component name,"We're exposing metrics to metrics consumer, and most of them are storing metrics to time-series DB.

Well-known common rule of generating metric name for these DB is using dot ('.') to separate parts, so that it can be parsed easily when needed.
For example, graphite separates part of metric name to dot, and support wildcard for each part. Note that it doesn't support wildcard matching multiple parts.

Since topology name or component name having dots are not common case to use case of user, I think we can warn users to not having dots on their topology name or component name. We can even restrict to not having dots if we all are really OK with this."
STORM-1963,Replace Put add with addColumn,"HBase Put add() have deprecated , replace add() with addColumn()"
STORM-1958,storm-config.cmd doesn't handle spaces in JAVA_HOME,"We are currently upgrading the version of Storm we use in our environment to 1.0.1 (from 0.10.0). We have discovered that Storm does not start properly as JAVA_HOME has a space in it.

Investigating this, I have found that the main problem (in this case) seems to be in storm-config.cmd. In 0.10.0, line 128 contained:

{code}
set STORM_OPTS=-Dstorm.options= -Dstorm.home=%STORM_HOME% -Djava.library.path=%JAVA_LIBRARY_PATH%
{code}

The equivalent line in 1.0.1 (line 136) has:

{code}
set STORM_OPTS=%STORM_OPTS% -Dstorm.home=%STORM_HOME% -Djava.library.path=%JAVA_LIBRARY_PATH%;%JAVA_HOME%\bin;%JAVA_HOME%\lib;%JAVA_HOME%\jre\bin;%JAVA_HOME%\jre\lib
{code}

If JAVA_HOME has a space in it (as is frequently the case on Windows due to Java by default being installed under Program Files) this breaks the subsequent JVM command line.

This is an out of the box blocker to running Storm on Windows in commons configurations. I have not raised this as a blocker issue however, as there is a simple fix/workaround. We have changed storm-config.cmd in our local copy to add quotes around the java.library.path option:

{code}
set STORM_OPTS=%STORM_OPTS% -Dstorm.home=%STORM_HOME% ""-Djava.library.path=%JAVA_LIBRARY_PATH%;%JAVA_HOME%\bin;%JAVA_HOME%\lib;%JAVA_HOME%\jre\bin;%JAVA_HOME%\jre\lib""
{code}"
STORM-1957,Support Storm JDBC batch insert,"Batch insert support execute grouped SQL a batch and submit into one call . It can reduce the amount of communication , improving performance."
STORM-1955,Consider implementing Tool + ToolRunner for topologies,"I see that most of our example topologies have two issues:
1. They have lot of code repetition
2. Each has it's own way of parsing args
It will be good to have a uniform way of doing this in such a way that it avoids code duplication.
For e.g. Hadoop we have ToolRunner and Tool classes which do this.
https://hadoopi.wordpress.com/2013/06/05/hadoop-implementing-the-tool-interface-for-mapreduce-driver/"
STORM-1953,[storm-redis] Revisit Mappers to structurize and deprecate/remove unneeded methods,We need to revisit Mappers of storm-redis to structurize and deprecate/remove unneeded methods to get rid of redundant methods.
STORM-1952,Keeping topology code for supervisor until topology got killed,"It's based on review comment from [~sriharsha].
https://github.com/apache/storm/pull/1528/files#r69152524
Please feel free to change reporter if you would like to.

In supervisor we're removing topology code when assignments for that supervisor has gone.
But there's valid scenario to need to keep the topology code though assignments for that supervisor is none, for example, rebalancing.

So it would be better for supervisor to keep topology code until topology has been killed (and all topology workers assigned to that supervisor are also killed)."
STORM-1949,Backpressure can cause spout to stop emitting and stall topology,"Problem can be reproduced by this [Word count topology|https://github.com/hortonworks/storm/blob/perftopos1.x/examples/storm-starter/src/jvm/org/apache/storm/starter/perf/FileReadWordCountTopo.java]
within a IDE.
I ran it with 1 spout instance, 2 splitter bolt instances, 2 counter bolt instances.

The problem is more easily reproduced with WC topology as it causes an explosion of tuples due to splitting a sentence tuple into word tuples. As the bolts have to process more tuples than the  spout is producing, spout needs to operate slower.

The amount of time it takes for the topology to stall can vary.. but typically under 10 mins. 

*My theory:*  I suspect there is a race condition in the way ZK is being utilized to enable/disable back pressure. When congested (i.e pressure exceeds high water mark), the bolt's worker records this congested situation in ZK by creating a node. Once the congestion is reduced below the low water mark, it deletes this node. 
The spout's worker has setup a watch on the parent node, expecting a callback whenever there is change in the child nodes. On receiving the callback the spout's worker lists the parent node to check if there are 0 or more child nodes.... it is essentially trying to figure out the nature of state change in ZK to determine whether to throttle or not. Subsequently  it setsup another watch in ZK to keep an eye on future changes.

When there are multiple bolts, there can be rapid creation/deletion of these ZK nodes. Between the time the worker receives a callback and sets up the next watch.. many changes may have undergone in ZK which will go unnoticed by the spout. 

The condition that the bolts are no longer congested may not get noticed as a result of this."
STORM-1948,Add GC visualization to UI,"As a user, I would like worker JVM GC metrics presented in a visualization, so that I can quickly see and debug issues with heap utilization.

Perhaps something similar to what [GCViewer|https://github.com/chewiebug/GCViewer/wiki/Features] could be integrated in the UI."
STORM-1947,Show worker GC time on the UI,"It would be helpful if GC time was exposed on the UI, especially in a worker specific view."
STORM-1944,HLL support in storm core and trident,"HyperLogLog implementation for unique counts in core and trident.

http://research.google.com/pubs/pub40671.html
http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf"
STORM-1943,Support loading properties from a file,Support load properties from a file include config's argument . 
STORM-1940,Storm Topo is auto re-balance after ZK RECONNECTED,"I have a Topo with 2 workers at 2 Vm, while ZK RECONNECTED, Storm Topo will be auto-reblance. 
The log show NodeExists for /meta/712285. I guess it cause by: After reconnect successfully, TridentSpoutCoordinator create this node again, but this node is already created before the reconnect.
 Can we check if node exist first? Or not throw this exception to make whole Topo re-balance. 
{code}
06-29 05:54:37.515 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] shade.org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae1, negotiated timeout = 10000
06-29 05:54:37.515 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]-EventThread] apache.curator.framework.state.ConnectionStateManager [INFO] State change: RECONNECTED
06-29 05:54:37.519 [Thread-133-spout-DataKafkaSpout1466801942228-executor[154 154]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae5, negotiated timeout = 10000
06-29 05:54:37.519 [Thread-133-spout-DataKafkaSpout1466801942228-executor[154 154]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.524 [Thread-25-spout-DataKafkaSpout1466801942228-executor[156 156]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae4, negotiated timeout = 10000
06-29 05:54:37.524 [Thread-25-spout-DataKafkaSpout1466801942228-executor[156 156]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.528 [main-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] shade.org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7b556f0cc3a40896, negotiated timeout = 10000
06-29 05:54:37.528 [main-EventThread] apache.curator.framework.state.ConnectionStateManager [INFO] State change: RECONNECTED
06-29 05:54:37.528 [Thread-149-spout-DataKafkaSpout1466801942228-executor[160 160]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae3, negotiated timeout = 10000
06-29 05:54:37.528 [Thread-149-spout-DataKafkaSpout1466801942228-executor[160 160]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.536 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]] org.apache.storm.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:452) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80]
Caused by: java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:119) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.overrideState(RotatingTransactionalState.java:52) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:71) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more
Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:721) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:704) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:701) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:477) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:467) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.forPath(TransactionalState.java:83) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.createNode(TransactionalState.java:95) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:115) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.overrideState(RotatingTransactionalState.java:52) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:71) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more
{code}"
STORM-1938,Nimbus 's mk-assignments scheduler should act immediately whenever there are topology related tasks are submitted.,"Currently, nimbus’s mk-assignments task runs at configured `nimbus.monitor.freq.secs`scheduled intervals. Sometimes, this causes the pending topology related tasks to wait till next scheduled interval. This can be improved not to wait for next scheduled interval but as and when there are tasks submitted. This behavior can be based on a configuration to maintain backward compatibility. It gives flexibility not to read from zk when there are topology related tasks but at configured intervals."
STORM-1936,Support default value for WindowedBolt,
STORM-1935,RocksDB State Support,
STORM-1932,Change Duration's value data type ,BaseWindowedBolt Duration value is integer now . Use long to express time interval maybe better
STORM-1931,Share mapper and selector in Storm-Kafka,Storm Kafka's mapper and selector and Storm Kafka trident's mapper and selector are the same . I try to merge them into one .
STORM-1929,Check when create topology,"Add some check when create topology .

1. Spout and Bolt id shouldn't conflict

2. createTopology's spout and bolt set shouldn't empty ."
STORM-1926,Upgrade Jetty and Ring,Jetty 7 is EOL so we should upgrade to Jetty 9
STORM-1923,Storm site page not found ,[DaemonMetrics/Monitoring|http://storm.apache.org/releases/1.0.1/storm-metrics-profiling-internal-actions.html]  Not found 
STORM-1921,Update parallelism_hint date type to integer,update TopologyBuilder's parallelism_hint date type from Number to int
STORM-1918,Can storm pin executor threads to CPU cores?,"I want to manage the binding of Storm executor thread (NOT worker process) to CPU cores (NOT host nodes). 
I have to try to look into the related source code of Storm. Unfortunately, I’m very new to Clojure, and I don’t get how the executor thread is initiated. I assume if I would know that, I can manage the binding of it by relying on things like JNI.

Any helps are appreciated.
"
STORM-1917,Storm Redis TTL Control,add a new interface to control key ttl .
STORM-1912,Additions and Improvements for Trident RAS API,"Trident's RAS API does not honor the following config values:
{code}
topology.component.resources.onheap.memory.mb
topology.component.resources.offheap.memory.mb
topology.component.cpu.pcore.percent
{code}

Trident does not receive the user's config as part of its builder API, so it does not know the value of these. Instead of altering the existing API (we want to remain backwards-compatible), add some new methods for dealing with this.

There is also currently no way to set the master coord spouts' resources. "
STORM-1908,Support Storm HBase ZooKeeper Config ,"Support Storm HBase ZooKeeper Config . About zk host , port and parent ."
STORM-1905,Javascript API does not support reportError,"The javascript API does not support reportError, the python one for example does:
https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py#L135"
STORM-1904,Storm shell display summary info,"display storm cluster summary info about cluster , nimbus ,supervisor and history . "
STORM-1903,Intermittent Travis test failures in org.apache.storm.nimbus.LocalNimbusTest.testSubmitTopologyToLocalNimbus,"testSubmitTopologyToLocalNimbus fails with following error
{noformat}
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_getTopologyInfo(Nimbus.java:1182)
	at org.apache.storm.generated.Nimbus$Client.getTopologyInfo(Nimbus.java:1169)
	at org.apache.storm.utils.Utils.getTopologyInfo(Utils.java:1465)
	at org.apache.storm.LocalCluster$submit_hook.invoke(LocalCluster.clj:44)
	at org.apache.storm.LocalCluster$_submitTopology.invoke(LocalCluster.clj:52)
	at org.apache.storm.LocalCluster.submitTopology(Unknown Source)
	at org.apache.storm.nimbus.LocalNimbusTest.testSubmitTopologyToLocalNimbus(LocalNimbusTest.java:65)
{noformat}

The exception on server is 
{noformat}
283607 [pool-75-thread-3] ERROR o.a.t.s.AbstractNonblockingServer$FrameBuffer - Unexpected throwable while invoking!
java.lang.NullPointerException
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:26) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__4758$iter__4869__4873$fn__4874.invoke(nimbus.clj:1870) ~[classes/:?]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
	at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
	at clojure.core$dorun.invoke(core.clj:3009) ~[clojure-1.7.0.jar:?]
	at clojure.core$doall.invoke(core.clj:3025) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__4758.getTopologyInfoWithOpts(nimbus.clj:1868) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__4758.getTopologyInfo(nimbus.clj:1907) ~[classes/:?]
	at org.apache.storm.generated.Nimbus$Processor$getTopologyInfo.getResult(Nimbus.java:3748) ~[classes/:?]
	at org.apache.storm.generated.Nimbus$Processor$getTopologyInfo.getResult(Nimbus.java:3732) ~[classes/:?]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158) ~[classes/:?]
	at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [libthrift-0.9.3.jar:0.9.3]
	at org.apache.thrift.server.Invocation.run(Invocation.java:18) [libthrift-0.9.3.jar:0.9.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_31]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_31]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_31]
{noformat}"
STORM-1901,Avro Integration for Storm-Kafka,
STORM-1900,Log configuration with logback,"I am trying to use logback in my project and am getting this message:
{quote}
2016-06-14 16:55:56.945 STDERR [INFO] SLF4J: Class path contains multiple SLF4J bindings.
2016-06-14 16:55:56.997 STDERR [INFO] SLF4J: Found binding in [jar:file:/opt/apache-storm-0.10.1/lib/log4j-slf4j-impl-2.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: Found binding in [jar:file:/srv/storm/supervisor/stormdist/MatchIdentifiers-675106f-1465923070-5-1465923118/stormjar.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2016-06-14 16:56:03.647 STDERR [INFO] 2016-06-14 16:56:03,641 ERROR Logger contains an invalid element or attribute ""appender""{quote}

The solution slf4j suggests is to exclude slf4j from the dependency which in this case would be storm. But since storm includes slf4j in the classpath when starting the topology this does not work.


Is there a known way to fix this?
"
STORM-1898, MAX_BATCH_SIZE_CONF not working in Trident storm Spout,"Ideally Trident process should process tuples in Batch.

ex > https://github.com/apache/storm/blob/ab66003c18fe4f8c0926b3219408b735b2ce2adf/storm-core/src/jvm/org/apache/storm/trident/spout/RichSpoutBatchExecutor.java

there is a parameter called  MAX_BATCH_SIZE_CONF which limits the size of the batch.

This parameter is not present in TridentKafkaEmitter.

https://github.com/apache/storm/blob/1.x-branch/external/storm-kafka/src/jvm/org/apache/storm/kafka/trident/TridentKafkaEmitter.java


---------------
Problem is that now everytime the topology restarts it just fetches all messages from Kafka.

Could any one throw some idea on it, I certainly feel its a bug.
"
STORM-1897,"re-pattern file-path-separator problem with windows, breaks logviewer","re-pattern file-path-separator will cause errors in windows...

more specifically 
----
java.util.regex.PatternSyntaxException: Unexpected internal error near index 1
\

----

the ""\"" character (windows separator) is a reserved one in regex.  

hence the logviewer will not work on a windows machine...

A potential fix could be to define:

(defn file-path-separator-regex []
    (if on-windows?
        (re-pattern ""\\\\"")
        (re-pattern file-path-separator)))

and use this instead of ""re-pattern file-path-separator"" in logviewer.clj and config.clj

"
STORM-1896,HdfsSpout remove duplicated code,remove duplicated code in HdfsSpout open()
STORM-1894,storm-redis does not support a Redis cluster for state saving,"Working with Storm and stateful bolts we noticed that it is not possible to work with a Redis cluster at the moment. The problem is, that storm-redis requires that the configuration is of type {{JedisPoolConfig}} which only allows defining one host. If the given Redis instance is configured as a Redis cluster exceptions of type {{JedisMovedDataException}} might occur.
The configuration via {{JedisClusterConfig}} seems to provide support for a Redis cluster, but {{RedisKeyValueStateProvider}} does not handle it."
STORM-1892,class org.apache.storm.hdfs.spout.TextFileReader should be public,
STORM-1891,Incorporate trident kafka spout for the kafka offset lag tool,This is a follow up JIRA to support trident kafka spouts for tool introduced in https://issues.apache.org/jira/browse/STORM-1136
STORM-1889,Datatables error message displayed when viewing UI,"Updating to storm 1.0.1, running on Windows 7, I receive error messages from Datatables.
This occurs on the Topology Summary as well as the Component Summary for a spout/bolt

Example error: DataTables warning: table id=executor-stats-table - Requested unknown parameter '9' for row 0. For more information about this error, please see http://datatables.net/tn/4

If I edit index.html to remove the type: num targets, the errors go away.

For example.

  $.getJSON(""/api/v1/topology/summary"",function(response,status,jqXHR) {
      $.get(""/templates/index-page-template.html"", function(template) {
          topologySummary.append(Mustache.render($(template).filter(""#topology-summary-template"").html(),response));
          //name, owner, status, uptime, num workers, num executors, num tasks, replication count, assigned total mem, assigned total cpu, scheduler info
          dtAutoPage(""#topology-summary-table"", {
            columnDefs: [
              //{type: ""num"", targets: [4, 5, 6, 7, 8, 9]},
			  {type: ""num"", targets: []},
              {type: ""time-str"", targets: [3]}
            ]
          });
          $('#topology-summary [data-toggle=""tooltip""]').tooltip();
      });




"
STORM-1886,Extend KeyValueState interface with delete method,"Even if the implementation of checkpointing only uses the get/put methods of the KeyValueState interface, the existance of a delete method could be really useful in the general case.

I made a first implementation, what do you think about?"
STORM-1885,python script for squashing and merging prs,
STORM-1883,FileReader extends Closeable Interface,use Closeable Interface to decorate FileReader to support close()
STORM-1880,Support  EXISTS Command Storm-Redis,add exists command in storm-redis LookupBolt
STORM-1875,Separate Jedis/JedisCluster Config,Separate Jedis / JedisCluster to provide full operations for each environment to users . 
STORM-1871,Storm Alluxio integrate,[alluxio|http://alluxio.org/] is a memory speed virtual distributed storage system.Alluxio’s memory-centric architecture enables data access orders of magnitude faster than existing solutions.
STORM-1869,"Add ability for Kafka Spout to do limited retries on failed tuples, and redirect down a ""failed"" stream","h4. Summary

KafkaSpout provides no way to limit the number of times a tuple can fail within your topology.  I think it'd be great to allow configuring a hard limit to the number of times a tuple can be failed before it is no longer retried.

Additionally, it'd be fantastic if the spout could do a one-time emit of this tuple down a ""failed"" stream.  This stream would allow you to handle these failed tuples in some way -- persist the failed tuple to some other storage system to be replayed later?  logged?  ignored? tons of things you could possibly want to do with these."
STORM-1867,Storm Topology Freezes,"My storm topology freezes after few hours. I have a KafkaSpout, a parser bolt and an aggregator bolt.

After few hours I see KafkaSpout is not emitting anything. I checked the STORM UI, there were no errors. I also checked the kafka topic via kafka-console-consumer, I was able to consume but kafkaspout was emitting nothing. 

I checked worker.log and there were no issues.

Following are the last few lines of worker.log. 


2016-05-26 03:45:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:45:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:45:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:47:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:47:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:47:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:49:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:49:16.259 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:49:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:51:16.254 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:51:16.259 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:51:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:53:16.254 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:53:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:53:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:55:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:55:16.265 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:55:16.265 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:57:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:57:16.261 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:57:16.261 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:59:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:59:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:59:16.261 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
"
STORM-1863,Throw exception if messages fetched by storm-kafka is emtpy,"In kafka ConsumerIterator, there is some codes like this:

// if we just updated the current chunk and it is empty that means the fetch size is too small! if(currentDataChunk.messages.validBytes == 0)
 throw new MessageSizeTooLargeException(""Found a message larger than the maximum fetch size of this consumer on topic "" + ""%s partition %d at fetch offset %d. Increase the fetch size, or decrease the maximum message size the broker will allow."" .format(currentDataChunk.topicInfo.topic, currentDataChunk.topicInfo.partitionId, currentDataChunk.fetchOffset))

When ""fetch.message.max.bytes"" config is smaller than the actual message size in topic, ConsumerIterator will throw an exception to notify user.
But in storm-kafka, there is no such logic. And as a result, if KafkaConfig.fetchSizeBytes is smaller than actual message size, the topology will fetch no data but still be running.
To prevent this situation, we need throw MessageSizeTooLargeException as well."
STORM-1860,Simple UI Announcements Notification,"As a user of a storm cluster, I would like to be informed of current or scheduled maintenance on the cluster, so that I can plan my work & debug more efficiently.


This could be a really simple UI change.  The UI could look for a special file on the local disk. If found, it would display a notification near the top of the UI with the contents of this file.

It would be useful to announce an ongoing rolling upgrade of the cluster, scheduled future downtime, current known incidents, or other, even more general communications from administrators to users."
STORM-1858,KafkaBolt: sharing a single producer instance across threads,"According to the KafkaProducer javadoc:
{quote}The producer is thread safe and sharing a single producer instance across threads will generally be faster than having multiple instances.{quote}
Maybe we can change the implementation in KafkaBolt, just using a single producer instance in one worker/process."
STORM-1857,HDFS Topologies should support config parameters from a YAML file,HDFS test topologies should support specifying a yaml file for passing in config parameters (for eg. security related)
STORM-1856,Release Apache Storm 1.1.0,
STORM-1855,Release Apache Storm 1.0.2,
STORM-1854,Trident transactional spouts are broken in 1.0.x,"In the process of upgrading our Storm code from 0.10.0 to 1.0.0, I've run into an issue with TransactionalTridentKafkaSpout. When running one of our topologies I'm getting the following exception:

{code}
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.lang.Integer
	at org.apache.storm.trident.spout.PartitionedTridentSpoutExecutor$Coordinator.initializeTransaction(PartitionedTridentSpoutExecutor.java:55) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.trident.spout.PartitionedTridentSpoutExecutor$Coordinator.initializeTransaction(PartitionedTridentSpoutExecutor.java:43) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:70) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.0.jar:1.0.0]
{code}

The issue appears to be caused by a change in PartitionedTridentSpoutExecutor between the two versions, specifically this method:

1.0.0 - https://github.com/apache/storm/blob/v1.0.0/storm-core/src/jvm/org/apache/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L51

{code}
public Integer initializeTransaction(long txid, Integer prevMetadata, Integer currMetadata) {
    if(currMetadata!=null) {
        return currMetadata;
    } else {
        return _coordinator.getPartitionsForBatch();            
    }
}
{code}

0.10.0 - https://github.com/apache/storm/blob/v0.10.0/storm-core/src/jvm/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L51

{code}
public Object initializeTransaction(long txid, Object prevMetadata, Object currMetadata) {
    if(currMetadata!=null) {
        return currMetadata;
    } else {
        return _coordinator.getPartitionsForBatch();            
    }
}
{code}

This was introduced by: https://github.com/apache/storm/commit/9e4c3df17ffbc737210e606d3d8a9cdae8f86634

TransactionalTridentKafkaSpout uses List<GlobalPartitionInformation> for its metadata. Generally, transactional spouts should have metadata that is more complex than just an Integer. OpaquePartitionedTridentSpoutExecutor uses Object for its metadata and correctly handles the metadata used by OpaqueTridentKafkaSpout (List<GlobalPartitionInformation>).

It looks like reverting the metadata type for transactional spouts in PartitionedTridentSpoutExecutor should work, but I haven't tried this yet."
STORM-1852,Investigate using a ScheduledThreadPoolExecutor instead of a Timer+Executor in Disruptor Queue,"We use a DisruptorQueue with a Timer and a thread pool executor to schedule tasks periodically. 

https://github.com/apache/storm/blob/master/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java

This could potentially be replaced with a Scheduled Thread Pool executor to perform the job in a more first class way, http://stackoverflow.com/questions/409932/java-timer-vs-executorservice.
"
STORM-1847,add popular storm related projects(from github) in docs,
STORM-1846,Need a smoke test for the generated Python code,"This was inspired by STORM-1842. For that issue, simply attempting to import ttypes.py would've detected the error. I'm guessing there is no such test. It would be good to add one in order to help avoid problems like that in the future."
STORM-1845,use UTF-8 instead of default encoding,
STORM-1843,Unified API to address micro-batching and per tuple use cases,
STORM-1840,LocalCluster is not properly shutting down,In 1.0 after blobstore introduction LocalCluster shutdown is cleanly going through.
STORM-1838,[storm-kafka-client] the resumed OffsetEntry goes different,"There are no more new messages after consumer rejoins the group. And it turns out the old OffsetEntry instance kept commitOffsetsForAckedTuples failing.

the comment ""leave the acked offsets as they were to resume where it left off"" doesn't work for me, but the ackedMsgs goes different."
STORM-1836,An official Docker image,Here is the [PR|https://github.com/docker-library/official-images/pull/1641] which contains an official image for Storm. Would be great if someone takes a look at it and provide some feedback. Or maybe somebody is interested in collaboration on the image? Thanks!
STORM-1778,Scheme Extension Framework for KafkaDataSource and support for CSV format,
STORM-1777,Backport KafkaBolt from Storm 1.0.0 to 0.10.0 to support properties configuration method,"Storm KafkaBolt requires Map object supplied under the kafka.broker.properties which makes the configuration values global i.e. there can be only one set of KafkaBolts in a given topology.

This issue has already been resolved in Storm 1.0.0 by introducing the withProducerProperties method. 

This ticket is to back port those changes to 0.10.0 so that we can use the community version rather than project specific back ports."
STORM-1776,Error when processing event java.io.FileNotFoundException,"when i am trying to start storm cluster , i got the below error.
24628 [Thread-10] INFO  o.a.s.d.supervisor - Copying resources at jar:file:/D:/.m2/repository/org/apache/storm/flux-core/1.0.0/flux-core-1.0.0.jar!/resources to C:\Users\MAGESH~1\AppData\Local\Temp\1991ad4f-acbc-44ed-b07b-24643381f7a0\supervisor\stormdist\test-1-1462884218\resources
24631 [Thread-10] ERROR o.a.s.event - Error when processing event
java.io.FileNotFoundException: Source 'file:\D:\.m2\repository\org\apache\storm\flux-core\1.0.0\flux-core-1.0.0.jar!\resources' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1368) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1261) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1230) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$fn__9351.invoke(supervisor.clj:1194) ~[storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.MultiFn.invoke(MultiFn.java:243) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070$fn__9088.invoke(supervisor.clj:582) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070.invoke(supervisor.clj:581) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:40) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
24638 [Thread-8] INFO  o.a.s.d.supervisor - Copying resources at jar:file:/D:/.m2/repository/org/apache/storm/flux-core/1.0.0/flux-core-1.0.0.jar!/resources to C:\Users\MAGESH~1\AppData\Local\Temp\f4009546-87fe-4885-a0b0-cb775e8b784f\supervisor\stormdist\test-1-1462884218\resources
24638 [Thread-8] ERROR o.a.s.event - Error when processing event
java.io.FileNotFoundException: Source 'file:\D:\.m2\repository\org\apache\storm\flux-core\1.0.0\flux-core-1.0.0.jar!\resources' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1368) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1261) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1230) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$fn__9351.invoke(supervisor.clj:1194) ~[storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.MultiFn.invoke(MultiFn.java:243) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070$fn__9088.invoke(supervisor.clj:582) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070.invoke(supervisor.clj:581) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:40) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
24642 [Thread-10] ERROR o.a.s.util - Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:48) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
24643 [Thread-8] ERROR o.a.s.util - Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:48) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
"
STORM-1775,Generate StormParserImpl before maven building instead of in packaging time,"Just like genthrift.sh genrates the generated thrift-about java source files. I think it is better generate StormParserImpl.java before maven execution.

It can reduce the complexity of storm-sql."
STORM-1774,Generate StormParserImpl before maven execution ,"Just like genthrift.sh genrates the generated thrift-about java source files. I think it is better to generate StormParserImpl.java before maven execution.

It can reduce the complexity of storm-sql."
STORM-1770,Pluggable status storage in storm-Kafka,"Storm-Kafka spout store consumer offset in zookeeper . When a lot topology running in cluster , zookeeper snapshot frequently . This ticket is to make status storage pluggable , support store in zookeeper (default) and Redis ."
STORM-1768,Reduce noise in worker logs,"Much of the time when debugging a problematic topology, worker logs are filled with many statements like netty connection issues, so that more relevant information is harder to find.

Perhaps we can identify some of the noisiest logs and change their log level to DEBUG or TRACE."
STORM-1767,metrics log entries are being appended to root log,"Current setup of metrics logger ( {{storm/log4j2/worker.xml}}) uses fully qualified name of the class where the logging is happening from i.e `org.apache.storm.metric.LoggingMetricsConsumer`, which is problematic and does not achieve the original intent as stated by the METRICS appender defined in {{storm/log4j2/worker.xml}}.

Currently the metrics logger created explicitly by using the name above:
{{LoggerFactory.getLogger(""org.apache.storm.metric.LoggingMetricsConsumer"")}} or implicitly from within the {{LoggingMetricsConsumer}} by calling {{LoggerFactory.getLogger(LoggingMetricsConsumer.class)}} will be logging to **root** logger.

This happens because logger names use Java namespaces and as such create hierarchies. 

The solution is to name metrics logger outside of {{org.apache.storm.*}} namespace which is what is happening for all other non-root loggers defined within the {{storm/log4j2/worker.xml}} file. 

This will also mean a code change to {{LoggingMetricsConsumer}} class itself for it to use the logger with an explicit name matching the name defined in the {{worker.xml}} file.

The fix is easy. 
"
STORM-1765,KafkaBolt with new producer api should be part of storm-kaka-client,"During the discussion of storm-kaka-client we agreed on following
""We talked about this some more and it seems to me it would make more sense to leave this new spout in storm-kafka-client (or whatever you want to call it) and move the KafkaBolt which uses the new producer api over here also. That way this component only needs to depend on the new kafka-clients java api and not on the entire scala kafka core. We can make the old storm-kafka depend on this component so it still picks up the bolt so if anyone is using that its still works. We can deprecate the old KafkaSpout but keep it around for people using older versions of Kafka - tgravescs"""
STORM-1763,Add multiple trigger and eviction policy support for core windowing.,"Some of these APIs are described in https://issues.apache.org/jira/secure/attachment/12780751/StormTrident_windowing_support-676.pdf

This will be along the lines of Google cloud data flow trigger/eviction policies.
https://cloud.google.com/dataflow/model/windowing
https://cloud.google.com/dataflow/model/triggers"
STORM-1762,storm pom.xml miss some dependency,"Hi, all:
    I download storm v0.9.5 from github, and want to compile and install it with maven. Unfortunately, I got the following errors when I use command ""mvn clean install -DskipTests -X"":
    ...
   [ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.18:compile (compile-clojure) on project storm-core: Clojure failed. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.18:compile (compile-clojure) on project storm-core: Clojure failed.
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: Clojure failed.
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:453)
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:367)
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:344)
        at com.theoryinpractise.clojure.ClojureCompilerMojo.execute(ClojureCompilerMojo.java:47)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 19 more

    The trace log only shows something wrong in the process of clojure compile. To slove this problem I debug and found the deep cause: the storm-core pom.xml missing some dependency package, and I have made a path file."
STORM-1760,HiveState should retire idle or old writes with flushAndClose,
STORM-1759,Viewing logs from the Storm UI doesn't work in dockerized environment,"I run the Storm using the following docker-compose.yml

{code}
version: '2'

services:
    zookeeper:
        image: jplock/zookeeper:3.4.8
        restart: always

    nimbus:
        image: 31z4/storm:1.0.0
        command: nimbus -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - zookeeper
        restart: always
        ports:
            - 6627:6627
        volumes:
            - logs:/logs

    supervisor:
        image: 31z4/storm:1.0.0
        command: supervisor -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
        restart: always
        volumes:
            - logs:/logs

    logviewer:
        image: 31z4/storm:1.0.0
        command: logviewer -c storm.log.dir=""/logs""
        restart: always
        ports:
            - 8000:8000
        volumes:
            - logs:/logs

    ui:
        image: 31z4/storm:1.0.0
        command: ui -c storm.log.dir=""/logs"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
            - logviewer
        restart: always
        ports:
            - 8080:8080
        volumes:
            - logs:/log

volumes:
    logs: {}
{code}

And opening the logs from the Storm UI doesn't work because all links are pointing to different container ids as hosts.

I guess adding an ability to explicitly specify the logviewer host in the storm.yaml would solve the issue."
STORM-1758,Distributed log search doesn't work in dockerized environment,"I run the Storm using the following docker-compose.yml

{code}
version: '2'

services:
    zookeeper:
        image: jplock/zookeeper:3.4.8
        restart: always

    nimbus:
        image: 31z4/storm:1.0.0
        command: nimbus -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - zookeeper
        restart: always
        ports:
            - 6627:6627
        volumes:
            - logs:/logs

    supervisor:
        image: 31z4/storm:1.0.0
        command: supervisor -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
        restart: always
        volumes:
            - logs:/logs

    logviewer:
        image: 31z4/storm:1.0.0
        command: logviewer -c storm.log.dir=""/logs""
        restart: always
        ports:
            - 8000:8000
        volumes:
            - logs:/logs

    ui:
        image: 31z4/storm:1.0.0
        command: ui -c storm.log.dir=""/logs"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
            - logviewer
        restart: always
        ports:
            - 8080:8080
        volumes:
            - logs:/log

volumes:
    logs: {}
{code}

And distributed log search doesn't work because the Storm UI tries to access the logviewer by supervisor's container id as a host.

Here is the list of running containers
{code}
$ docker ps
7ae118eef55c        31z4/storm:1.0.0         ""bin/storm ui -c stor""   5 minutes ago       Up 5 minutes               0.0.0.0:8080->8080/tcp         stormdocker_ui_1
5a9101dc2510        31z4/storm:1.0.0         ""bin/storm supervisor""   5 minutes ago       Up 5 minutes                                              stormdocker_supervisor_1
4d954096cf18        31z4/storm:1.0.0         ""bin/storm nimbus -c ""   5 minutes ago       Up 5 minutes               0.0.0.0:6627->6627/tcp         stormdocker_nimbus_1
070080342c4f        31z4/storm:1.0.0         ""bin/storm logviewer ""   5 minutes ago       Up 5 minutes               0.0.0.0:8000->8000/tcp         stormdocker_logviewer_1
8650786a13cc        jplock/zookeeper:3.4.8   ""/opt/zookeeper/bin/z""   5 minutes ago       Up 5 minutes               2181/tcp, 2888/tcp, 3888/tcp   stormdocker_zookeeper_1
{code}

And here is what the Storm UI requests
{code}
curl 'http://5a9101dc2510:8000/search/topology-1-1462284216%2F6701%2Fworker.log?search-string=split&num-matches=1' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://192.168.99.100:8080/search_result.html?search=split&id=topology-1-1462284216&count=1' -H 'Origin: http://192.168.99.100:8080' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.86 Safari/537.36' --compressed
{code}

I guess adding an ability to explicitly specify the logviewer host in the storm.yaml would solve the issue."
STORM-1757,Apache Beam Runner for Storm,"This is a call for interested parties to collaborate on an Apache Beam [1] runner for Storm, and express their thoughts and opinions.

Given the addition of the Windowing API to Apache Storm, we should be able to map naturally to the Beam API. If not, it may be indicative of shortcomings of the Storm API that should be addressed.


[1] http://beam.incubator.apache.org"
STORM-1753,Add documentation for LoadAwareShuffleGrouping,
STORM-1752,Correct the nimbus.host property name in 0.10.0 docs,"http://storm.apache.org/releases/0.10.0/Setting-up-a-Storm-cluster.html

nimbus.seeds should be nimbus.host. "
STORM-1751,Remove references to external repositories in storm documentation,
STORM-1748,Better Trident Spout Names,"As a user, I would like to see more meaningful component names for spouts in trident topologies, so that I can make more sense of the topology functionality.

For example, automatically generated names like ""$mastercoord-bg2,3,4,5"" are not so useful for understanding."
STORM-1746,Don't error log NOT_LEADER_FOR_PARTITION in KafkaUtils,"NOT_LEADER_FOR_PARTITION may be returned in a fetch any time a partition in Kafka switches leader. Since it is transient most of the time, it should be downgraded to warning log level."
STORM-1744,Missing javadoc in Trident code,"Some or most of the core Trident classes don't have javadoc. It makes it really difficult to use.

http://storm.apache.org/releases/2.0.0-SNAPSHOT/javadocs/index.html

Examples:
TridentTopologyBuilder
IBackingMap"
STORM-1743,consumerAutoCommitMode not work in storm-kafka-client,"poll() checks the numUncommittedOffsets, however, it's never updated under consumerAutoCommitMode."
STORM-1740,Storm on X,"Storm should have a clean API to be able to get resources from another scheduler, like Mesos or Yarn."
STORM-1738,Add Storm alerting component (API and an implementation),
STORM-1734,ClassNotFound error when running storm-starter topologies in local mode,"The problem is described here: http://stackoverflow.com/questions/36649346/classnotfound-error-when-running-storm-starter-topologies-in-local-mode

Please help!"
STORM-1726,use Put#addColumn to replace the deprecated Put#add,
STORM-1722,Improve Storm UI,
STORM-1718,Trident API State Management Blog Post,"I have written a blog post based on my understanding of Trident State Management.  

https://chawlasumit.wordpress.com/2015/08/02/how-to-manage-state-in-trident-storm-topologies

Request experienced people to review it for correctness, and if possible include the url in Storm Trident Documentation.

"
STORM-1717,"Support Redis INRC , INRCBY and INRCBYFLOAT",to support redis incr incrby and incrbyfloat  
STORM-1716,Add some external Jedis pool config,add some jedis pool config 
STORM-1712,make storage plugin for transactional state,"As we know the transactional state must storage  in zk when we run trident topology. In fact I have packaged the TransactionalState to a plugin. We still storage the state to zk by default. But you can storage the transactional state to other places by set different plugin. And we now support storage transactional state to hbase, and so on. I want to hear your opinion. If Ok, I am pleasure to create the PR and merge the code."
STORM-1711,Kerberos principals gets mixed up while using storm-hive,Storm-hive uses UserGroupInformation.loginUserFromKeytab which updates the static variable that stores current UGI.
STORM-1708,Replay the kafka messages when no failed tuples.,"We build topology with OpaqueTridentKafkaSpout,  the configuration is :
        conf.setMaxSpoutPending(2048); 
        conf.setMessageTimeoutSecs(120);

that cause messages replayed lot of times, and there is no failed tuples in storm ui. 

After we set :
        conf.setMaxSpoutPending(10); 
        conf.setMessageTimeoutSecs(30);
everyting works fine. 

It confuse a lot , i think it's a major bug.



 "
STORM-1707,Improve supervisor latency by removing 2-min wait,"After launching workers, the supervisor waits up to 2 minutes synchronously for the workers to be ""launched"".

We should remove this, and instead keep track of launch time, making the ""killer"" function smart enough to determine the difference between a worker that's still launching, one that's timed out, etc."
STORM-1703,"In local mode, process is not shutting down clearly","Process is not shutting down clearly in local mode, but ‘Ctrl + C’ can terminate the process.

Will attach log file and jstack dump file."
STORM-1702,DirectGrouping ,"http://storm.apache.org/releases/0.10.0/index.html

The link to ""Direct Groupings"" (http://storm.apache.org/releases/0.10.0/Direct-groupings.html) is broken."
STORM-1701,Add simple JSON mapping to storm-hbase,"storm-hbase includes a way to map Storm fields to HBase CQs.  This adds a similar ability where a single field contains a flat JSON and the keys map to CQs.  The flat JSON must contain the row key.  

No intention of reverse mapping from HBaseLookUp as the existing HBaseMapper works well for this. "
STORM-1697,artifacts symlink not created ,"No artifacts symlink generated under worker's current directory. Gc log, jstack and heapdump will not be working.

2016-04-07 17:43:19.909 STDERR [INFO] Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file artifacts/gc.log due to No such file or directory
2016-04-07 17:43:19.913 STDERR [INFO]"
STORM-1692,HBaseSecurityUtil#login modifies the current UGI causing issues if two instances are running with different credentials,
STORM-1690,Backpressure flag not initialized correctly,"I heapdumped and found in different DisruptorQueues in a worker , the _enableBackpressure flags have both true and false, which cause topology to be blocked."
STORM-1689,set request headersize for logviewer,
STORM-1688,provide ParallismKillWorkerManager to shutdown workers  in parallel,
STORM-1686,Make local-grouping an independent option rather than just LOCAL_OR_SHUFFLE,"Currently, Storm distributes the tuples to bolts irrespective of their locality.
So if 1 spout and 5 bolts are running in the same process, storm does not give any preference to the local 5 bolts (except for LOCAL_OR_SHUFFLE grouping).

Due to this, there is a lot of inter-machine/inter-process communication for sending out the tuples.

It would be really good if local-mode can be made as an independent option (*preferLocalBolts*) rather than having just a single mode LOCAL_OR_SHUFFLE that can distribute locally.

At least for fields-grouping, the local-mode would make a lot of sense.

Currently, the alternatives to the local-mode-with-fields-grouping is:
# Run several topologies, each with 1 worker so as to limit the spouts/bolts to one machine only.
# Have another bolt between a spout and bolt that can be used for directing the tuple traffic by directed streams.

But both of these seem to be a lot of work for something that should be made an independent option - *preferLocalBolts*.

Also see the following SO content:
# [way-to-apply-multiple-groupings-in-storm|http://stackoverflow.com/questions/36368224/is-there-a-way-to-apply-multiple-groupings-in-storm/36374837#36374837]
# [fields-grouping-for-same-machine|http://stackoverflow.com/questions/35132136/storm-fields-grouping-for-same-machine?lq=1]"
STORM-1685,port local_supervisor.clj to java,
STORM-1684,New KafkaSpout should provide metrics,"From PR ConnieYang
""The previous KafkaSpout implementation publishes a kafkaOffset metrics to track spout lag, latest time offset and earliest time offset. """
STORM-1682,Kafka spout can lose partitions,"The KafkaSpout can lose partitions for a period, or hang because getBrokersInfo (https://github.com/apache/storm/blob/master/external/storm-kafka/src/jvm/org/apache/storm/kafka/DynamicBrokersReader.java#L77) may get a NoNodeException if there is no broker info in Zookeeper corresponding to the leader id in Zookeeper. When this error occurs, the spout ignores the partition until the next time getBrokersInfo is called, which isn't until the next time the spout gets an exception on fetch. If the timing is really bad, it might ignore all the partitions and never restart.

As far as I'm aware, Kafka doesn't update leader and brokerinfo atomically, so it's possible to get unlucky and hit the NoNodeException when a broker has just died.

I have a few suggestions for dealing with this. 

getBrokerInfo could simply retry the inner loop over partitions if it gets the NoNodeException (probably with a limit and a short sleep between attempts). If it fails repeatedly, the spout should be crashed.

Alternatively the DynamicBrokersReader could instead lookup all brokers in Zookeeper, create a consumer and send a TopicMetadataRequest on it. The response contains the leader for each partition and host/port for the relevant brokers.

Edit: I noticed that the spout periodically refreshes the brokers info, so the issue isn't as bad as I thought. I still think this change has value, since it avoids the spout temporarily dropping a partition."
STORM-1662,Reduce map lookups in send_to_eventlogger,Reducing map lookup in send_to_eventlogger can improve performance when when a spout emits in a tight loop.
STORM-1657,A new strategy in RAS that allows one worker to run executors from one component,"There are still lots of complaints from users and industry saying that Storm is hard to debug, mainly because Storm runs executors from different components (spout, bolt) in a single JVM process.
The original motivation for doing that is to allow inter-component communication to happen within a single process rather than over network socket as much as possible.

For small topology, this seems make sense, where we can place executors even within one or several worker. But for larger topology with more executors in each component, the saving in network transfer by using round-robin executor distribution to different workers is marginal. And the trade-off is that we will lose lots of debuggability, which makes problem-cracking difficult and prevents some potential users to start using Storm.

With RAS (STORM-894), it should be convenient for us to add a new strategy to allow one worker to only run executors (>= 1) from a single component, which will increase a few workers but not waste any memory /CPU resources.
By using this strategy (per-topology basis), it will be easier for users to find which component in a topology is having problems by checking their worker log, gc log and heap dump, without being bothered by noise made by other components' instances.
"
STORM-1656,Acking-Framework-implementation.md is not checked in the new documentation,Looks like Acking-framework-implementation was missed out while the documentation was moved to svn. I can't find it in the storm documentation and the link has been removed from the implementation-docs.html. 
STORM-1651,Add event time based support for trident windowing.,
STORM-1650,improve performance by XORShiftRandom,"'''Implement a random number generator based on the XORShift algorithm discovered by George Marsaglia. This RNG is observed 4.5 times faster than {@link Random} in benchmark, with the cost that abandon thread-safety. So it's recommended to create a new {@link XORShiftRandom} for each thread.'''"
STORM-1649,Optimize Kryo instaces creation in HBaseWindowsStore,
STORM-1647,AutoHBase doesn't send delegation token,"When submitting a topology, Nimbus appears to pick up the delegation tokens as expected (the second log line is a call I added for testing this):

{code:title=nimbus.log}
2016-03-22 12:54:55.159 o.a.s.h.s.AutoHBase [INFO] Logged into Hbase as principal = storm@MIST.COGNITIVESYSTEMS.COM
2016-03-22 12:54:55.160 o.a.s.h.s.AutoHBase [INFO] AutoHBase proxyUser: flurry@MIST.COGNITIVESYSTEMS.COM (auth:PROXY) via storm@MIST.COGNITIVESYSTEMS.COM (auth:KERBEROS)
2016-03-22 12:54:55.971 o.a.s.h.s.AutoHBase [INFO] Obtained HBase tokens, adding to user credentials.
{code}

However, when my bolt starts up, it attempts to write to HBASE as storm instead of flurry:

{code:title=Flurry-1-1458664951-worker-6700.log}
2016-03-22 16:48:10.506 b.s.d.executor [ERROR] 
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions (user=storm@MIST.COGNITIVESYSTEMS.COM, scope=########, family=##############, params=[table=########,family=########],action=WRITE)
{code}

if I go into SecurityAuth.audit, the HBASE regionserver receives a simple token instead of a proxy token:

{code:title=SecurityAuth.audit}
2016-03-22 16:55:20,903 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from ##.##.##.## port: 35551 with unknown version info
2016-03-22 16:55:20,903 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for storm@MIST.COGNITIVESYSTEMS.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingInterface
{code}

For reference, core-site.xml:

{code:title=core-site.xml}
hadoop.proxyuser.storm.groups = flurry
hadoop.proxyuser.storm.hosts = *
{code}

And storm.yaml:

{code:title=storm.yaml}
hbase.kerberos.principal : 'storm@MIST.COGNITIVESYSTEMS.COM'
hbase.keytab.file : '/etc/security/keytabs/storm.headless.keytab'
nimbus.autocredential.plugins.classes : ['org.apache.storm.hbase.security.AutoHBase']
nimbus.credential.renewers.classes : ['org.apache.storm.hbase.security.AutoHBase']
nimbus.credential.renewers.freq.secs : 82800
{code}"
STORM-1644,Shell component's executable can not be found,"Setting the working directory:
https://github.com/apache/storm/blob/a4f9f8bc5b4ca85de487a0a868e519ddcb94e852/storm-core/src/jvm/org/apache/storm/utils/ShellProcess.java#L74
 doesn't work for lookup of the executable:
http://stackoverflow.com/questions/9847242/processbuilder-cant-find-file
and results in ""CreateProcess error=2, The system cannot find the file specified"" "
STORM-1643,Performance Fix: Optimize clojure lookups related to throttling and stats,":key lookups in Clojure are expensive. And some of keys like :storm-conf are being looked up multiple times. Optimizing these by looking up once and reusing improves performance.
Will attach profiler screenshots demonstrating the before and after report."
STORM-1640,Provide an option to allow aggregating metrics before being reported,
STORM-1639,Log4j 2.1 doesn't effect when i modify the log file size or log index number in cluster.xml without restart sevice,
STORM-1638,Integration tests are failing on Windows,"Though I addressed STORM-1602, STORM-1629, STORM-1630, integration tests are still failing from Windows."
STORM-1629,Files/move doesn't work properly with non-empty directory in Windows,"Distributed version of download-storm-code uses Files#move().
It runs well on *Nix (including OSX) but fails on Windows.

Javadoc describes this behavior, please refer below link.
https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#move(java.nio.file.Path,%20java.nio.file.Path,%20java.nio.file.CopyOption...)

{quote}
When invoked to move a directory that is not empty then the directory is moved if it does not require moving the entries in the directory. For example, renaming a directory on the same FileStore will usually not require moving the entries in the directory. When moving a directory requires that its entries be moved then this method fails (by throwing an IOException). To move a file tree may involve copying rather than moving directories and this can be done using the copy method in conjunction with the Files.walkFileTree utility method.
{quote}

If directory is not empty, file system should treat ""move directory"" as ""rename"".
Unfortunately, file system on Windows 8 doesn't.

We should change the way to be compatible with both kinds of OS."
STORM-1628,Explore any optimizations to avoid storing replayed batches in trident windowing,Explore any optimizations to avoid storing replayed batches in trident windowing
STORM-1627,Handle emitting delayed triggered results in trident windowing operation.,
STORM-1626,HBaseBolt tuple counts too high in storm ui.,When a storm topology has an HbaseBolt the storm ui numbers for executed and acked tuples seem to be too high for HbaseBolt component. 
STORM-1619,Add the option of passing config directory apart from the file,"The error message, if configuration file doesn't exist, is also confusing. 
`Error: Cannot find configuration directory: /etc/storm/conf`

the binary is actually looking for a file. "
STORM-1616,Add RAS API for Trident,
STORM-1612,netty_unity_test uses hardcoded port ,netty_unit_test has a hardcoded port of 6700 in all of its test cases. It would be better to use an ephemeral port on systems running other tests / jobs to avoid the possibility of a port conflict.
STORM-1610,port pacemaker-state-factory-test to java,
STORM-1607,Add MongoMapState for supporting trident's exactly once semantics,
STORM-1600,Do not report errors when the worker shutdown is in progress,"Usually in a worker, some uncaught exception in an executor threads leads to process exit. Process exit is not instantaneous and it first triggers the shutdown. The shutdown initiation usually results in network connections closing e.g. zookeeper, hdfs in other threads causing other exceptions. These threads end up reporting their exceptions as well. It confuses the user who can these errors on UI but not the actual root cause of shutdown hidden beneath new errors. "
STORM-1599,Don't mark dependencies as provided unless they are in lib,"When we mark a dependency as provided it indicates the shade and assembly plugins to not include this particular dependency in the uber topology jar because it will be {{provided}} on the class path by the system.

We have been doing this for all of our kafka dependencies incorrectly, storm-cassandra does this for cassandra-driver-core, and storm-starter is doing it for storm-clojure as well.

This means that storm-starter does not have any version of kafka or storm-clojure packaged it the resulting jar and any example that uses kafka, TridentKafkaWordCount, will fail with missing class errors. 

storm-starter/pom.xml has should change its dependency on storm-kafka to be compile, and it should delete dependencies on kafka and kafka-clients as those should come from storm-kafka as transitive dependencies.

the main pom.xml should not have kafka-clients marked as provided in the dependency management section.

storm-kafka should remove its provided tag on kafka, and flux examples + storm-sql-kafka should remove dependencies on kafka and kafka-clients, and storm-kafka should not me marked as provided. 

the flux and sql code I am not as familiar with, but looking at them, and running `mvn dependecy:tree` and `mvn dependency:analyze` it looks like"
STORM-1598,Replace metric separator colon (:) with dot (.),Metric collection systems such as graphite work well with dot as a path separator in metric names. Right now we use <component>:<metric name> such as drpc:num-http-requests etc. This can be replaced with drpc.num-http-requests
STORM-1597,Provide a separate metric for timed out tuples in the spout,The metric may be useful to distinguish whether the failures are happening in bolt or due to delay in ack. 
STORM-1595,'Fail' messages get stuck somewhere ,"'Fail' acks seem to be getting stuck somewhere between the acker and the spout. 

After a long time - sometimes multiple minutes - the fails show up in the spout.
I tested this on master and 1.x-branch and it occurs in both places.
"
STORM-1593,Nimbus indicator for when a Topology finished processing all tuples,"Every time we want to update topologies, we routinely find ourselves waiting aimlessly for topologies to ""fully finish"" processing. We never truly know when a topology is actually still processing tuples, and when it's really done... Unless of course we wait for a full 10m window showing zeros in Nimbus's topology stats table.

I think it'd be beneficial to add some sort of a ""Green"" indicator in Nimbus, showing when a deactivated topology has ~0 tuples ringing through it. Would using the queue send/rcv population metric be correct for this?"
STORM-1591,[Storm-Kafka] Continuously processing un-emitted messages when retrying failed message,"This is an enhancement of failed message retrying logic in PartitionManager.java, for the failed msg, it will be fetched again for retrying in the fill() method. 

Current logic was to fetch a bulk of messages from the failed msg offset, and handle the failed msg, while discard the other un-failed msgs, which was kind of a waste, if those discarded msgs are un-emitted(offset > _emittedToOffset).

So, the enhance point is to handle the un-failed msgs if those msgs are not emitted, instead of discarding them. By changing code: 

{quote}
if (processingNewTuples || this._failedMsgRetryManager.shouldRetryMsg(cur_offset)) 
{quote}

to

{quote}
if (processingNewTuples || this._failedMsgRetryManager.shouldRetryMsg(cur_offset)) || cur_offset >= _emittedToOffset)
{quote}"
STORM-1589,port the webapp portion of drpc.clj over to java,
STORM-1584,Support UDF in storm-sql,
STORM-1583,storm-kafka module fails intermittently on java 8 in travis,Travis check is failing in many pull requests due to failure in storm-kafka module. VM crashes just after kafkaUtilsTest start to run. 
STORM-1582,asyncLoop will calls repeatedly when the return type of call only is Long,"It can repeatedly invoke fn.call when only the return type of call is Long. I think it maybe a bug.
  public static SmartThread asyncLoop(final Callable afn,
            boolean isDaemon, final Thread.UncaughtExceptionHandler eh,
            int priority, final boolean isFactory, boolean startImmediately,
            String threadName) {
        SmartThread thread = new SmartThread(new Runnable() {
            public void run() {
                Object s;
                try {
                    Callable fn = isFactory ? (Callable) afn.call() : afn;
                    while ((s = fn.call()) instanceof Long) {  //////????????
                        Time.sleepSecs((Long) s);
                    }
                }"
STORM-1580,Secure hdfs spout failed,"Some error occured when using secure hdfs spout:

""Login successful for user test@EXAMPLE.COM using keytab file /home/test/test.keytab

2016-02-26 10:33:14 o.a.h.i.Client [WARN] Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2016-02-26 10:33:14 o.a.h.i.Client [WARN] Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2016-02-26 10:33:14 o.a.h.i.r.RetryInvocationHandler [INFO] Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over hnn025/192.168.137.2:8020 after 1 fail over attempts. Trying to fail over immediately.
java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: ""HDD021/192.168.137.6""; destination host is: ""hnn025"":8020;"""
STORM-1577,Rename Utils.handeUncaughtException to avoid potential bug,"Utils.handeUncaughtException is inappropriately named.

 We need to rename handeUncaughtException to be more descriptive, because it is deceptive right now, and I think will cause bug while we translate code to java. Can we call it eatRuntimeExitOnOOM? "
STORM-1568,"Logviewer rest API for jstack, jmap and restart of a worker","To deliver a better user experience, we would want to serve jstack, jmap and restart worker request synchronously through a rest API in logviewer. STORM-1542 has more detailed discussion on this. "
STORM-1565,Multi-Lang Performance Improvements,"1. add _org.apache.storm.multilang.MessagePackSerializer_
2. change default ""topology.multilang.serializer"" to _MessagePackSerializer_

According to http://msgpack.org/ : It's like JSON, but fast and small.
{quote}
MessagePack is an efficient binary serialization format. It lets you exchange data among multiple languages like JSON. But it's faster and smaller. Small integers are encoded into a single byte, and typical short strings require only one extra byte in addition to the strings themselves.
{quote}"
STORM-1563,Replace Shutdownable to be AutoClosable,It will be good that we replace all the usages of Shutdownable in storm to be the java's conventional interface AutoClosable.
STORM-1562,Provide a pluggable ability to capture topology information on Nimbus (ISubmitterHook on Nimbus),"ISubmitterHook was introduced in Storm to enable a pluggable way of capturing information submitted in a topology. One use case which we have used it for is to capture metadata information and publish to governance stores like Apache Atlas. This works quite well for the intended use case. 

However, the fact that this runs on the client mode opens up the possibility that the configs for the hook can be overridden by a user either inadvertently or with malicious intent. Either is a problem from a use case such as governance. It feels that a client side solution would always be exposed to this problem. This JIRA is to discuss whether this can be moved to the Nimbus side where there would be more control administratively.

It is understood that moving such a plugin to Nimbus could add its own share of issues - hopefully we would be able to discuss and work through them, on the assumption that the use case is valid."
STORM-1560,Topology stops processing after Netty catches/swallows Throwable,"In some scenarios, netty connection problems can leave a topology in an unrecoverable state. The likely culprit is the Netty {{HashedWheelTimer}} class that contains the following code:

{code}
        public void expire() {
            if(this.compareAndSetState(0, 2)) {
                try {
                    this.task.run(this);
                } catch (Throwable var2) {
                    if(HashedWheelTimer.logger.isWarnEnabled()) {
                        HashedWheelTimer.logger.warn(""An exception was thrown by "" + TimerTask.class.getSimpleName() + '.', var2);
                    }
                }
            }
        }
{code}

The exception being swallowed can be seen below:

{code}
2016-02-18 08:46:59.116 o.a.s.m.n.Client [INFO] closing Netty Client Netty-Client-/192.168.202.6:6701
2016-02-18 08:46:59.173 o.a.s.m.n.Client [INFO] waiting up to 600000 ms to send 0 pending messages to Netty-Client-/192.168.202.6:6701
2016-02-18 08:46:59.271 STDIO [ERROR] Feb 18, 2016 8:46:59 AM org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer
WARNING: An exception was thrown by TimerTask.
java.lang.RuntimeException: Giving up to scheduleConnect to Netty-Client-/192.168.202.6:6701 after 44 failed attempts. 3 messages were lost
	at org.apache.storm.messaging.netty.Client$Connect.run(Client.java:573)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:546)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.notifyExpiredTimeouts(HashedWheelTimer.java:446)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:395)
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:745)
{code}

The netty client then never recovers, and the follows messages repeat forever:

{code}
2016-02-18 09:42:56.251 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:25.248 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:55.248 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:55.752 o.a.s.m.n.Client [ERROR] discarding 2 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:56.252 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:44:25.249 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
{code}
"
STORM-1557,trident get repeat data from kafka ,"When we used Trident API and set Config.TOPOLOGY_MAX_SPOUT_PENDING greater than 1, assuming 100, we found the txid was incontinuous.

That phenomenon has two effects:

* When using {color:red} OpaqueTridentKafkaSpout class {color}, we found that the {color:red} different txid got same offset which lead that we got repeated data value from kafka {color}. we printed some logs here:

{noformat}
11:10:13.516 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:96,offset:1805,nextOffset:1824]
11:10:13.567 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:142,offset:1824,nextOffset:1843]
11:10:13.619 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:97,offset:1824,nextOffset:1843]
11:10:13.670 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:98,offset:1843,nextOffset:1862]

please NOTICE that id 142 and 97 got same kafka offset.
{noformat}

* When using {color:red} TransactionalTridentKafkaSpout class {color}, we got different txid and continuance offset value but the process {color:red} speed would be slow {color}, because some unnecessary loops in function MasterBatchCoordinator.sync(). And the greater Config.TOPOLOGY_MAX_SPOUT_PENDING, the slower speed. we printed some logs here:

{noformat}
Config.TOPOLOGY_MAX_SPOUT_PENDING=100
Config.TOPOLOGY_TRIDENT_BATCH_EMIT_INTERVAL_MILLIS=50

14:05:00.337 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:135,offset:2546,nextOffset:2565]
14:05:00.483 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:136,offset:2565,nextOffset:2584]
14:05:00.495 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:197,offset:2584,nextOffset:2603]
14:05:03.550 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:198,offset:2603,nextOffset:2622]
14:05:03.593 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:199,offset:2622,nextOffset:2641]

please NOTICE the timestamp from id 136 to 198 and offset.
{noformat}


{noformat}
Config.TOPOLOGY_MAX_SPOUT_PENDING=1000
Config.TOPOLOGY_TRIDENT_BATCH_EMIT_INTERVAL_MILLIS=50
11:35:36.265 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:232,offset:228,nextOffset:247]
11:35:36.305 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:233,offset:247,nextOffset:266]
11:35:36.343 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:446,offset:266,nextOffset:285]
11:35:41.345 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:1266,offset:285,nextOffset:304]
11:35:47.063 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:1330,offset:304,nextOffset:323]
11:35:56.221 [Thread-15-spout0] ERROR s.kafka.trident.TridentKafkaEmitter - emit:[id:1447,offset:323,nextOffset:342]

please notice every log's timestamp,txid and offset.
{noformat}

We thought the txid's distribution algorithm needs to be with the continuous principle in MasterBatchCoordinator class. ONLY when the time windows and other condition is ready, the txid could be added.

{color:red}I submitted a solution on pull request #1041 {color}[https://github.com/apache/storm/pull/1041]"
STORM-1554,"Improvement on Storm's timer code by finding a better method to notify with simulate time, so we don't need this 1 second polling.",
STORM-1551,Wrong link to com.esotericsoftware.kryo.Serializer,"Page http://storm.apache.org/documentation/Serialization.html
has a link ""com.esotericsoftware.kryo.Serializer"" which points to http://code.google.com/p/kryo/source/browse/trunk/src/com/esotericsoftware/kryo/Serializer.java

It should point to https://github.com/EsotericSoftware/kryo/blob/master/src/com/esotericsoftware/kryo/Serializer.java"
STORM-1550,Fix storm.cmd to pass url-encoded options,"Windows 'storm.cmd' should url-encode options passed in storm.options similar to storm.py.

The regex that splits the options (org.apache.storm.utils.Utils) can be simplified to `split("","")` once this is done. The regex can currently produce undesired effect if invalid Json strings are passed via `storm.cmd` in windows."
STORM-1548,Writes for Pacemaker HA,
STORM-1547,Read Aggregations for Pacemaker HA,
STORM-1536,Eliminate or minimize use of deprecated TimeCacheMap,
STORM-1525,Topology visualization causes integer overlfow error,"We have seen the following exception when looking at topology visualization.  It looks like it was adding up the emitted tuples from a set of terminal bolts, so it should have been adding up a bunch of zeros.

{noformat}
java.lang.ArithmeticException: integer overflow
	at clojure.lang.Numbers.throwIntOverflow(Numbers.java:1424)
	at clojure.lang.Numbers.add(Numbers.java:1723)
	at clojure.lang.Numbers$LongOps.add(Numbers.java:447)
	at clojure.lang.Numbers.add(Numbers.java:126)
	at clojure.core$_PLUS_.invoke(core.clj:951)
	at clojure.core$merge_with$merge_entry__4318.invoke(core.clj:2773)
	at clojure.core$reduce1.invoke(core.clj:903)
	at clojure.core$merge_with$merge2__4320.invoke(core.clj:2776)
	at clojure.core$reduce1.invoke(core.clj:903)
	at clojure.core$reduce1.invoke(core.clj:894)
	at clojure.core$merge_with.doInvoke(core.clj:2777)
	at clojure.lang.RestFn.invoke(RestFn.java:439)
	at backtype.storm.stats$aggregate_counts$fn__2726.invoke(stats.clj:1369)
	at clojure.core$merge_with$merge_entry__4318.invoke(core.clj:2773)
	at clojure.core$reduce1.invoke(core.clj:903)
	at clojure.core$merge_with$merge2__4320.invoke(core.clj:2776)
	at clojure.lang.ArrayChunk.reduce(ArrayChunk.java:63)
	at clojure.core$reduce1.invoke(core.clj:901)
	at clojure.core$reduce1.invoke(core.clj:894)
	at clojure.core$merge_with.doInvoke(core.clj:2777)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:626)
	at backtype.storm.stats$aggregate_counts.invoke(stats.clj:1366)
	at backtype.storm.stats$aggregate_common_stats.invoke(stats.clj:1373)
	at backtype.storm.stats$aggregate_bolt_stats.invoke(stats.clj:1379)
	at backtype.storm.stats$bolt_streams_stats.invoke(stats.clj:1470)
	at backtype.storm.ui.core$visualization_data$iter__10652__10656$fn__10657.invoke(core.clj:221)
	at clojure.lang.LazySeq.sval(LazySeq.java:40)
	at clojure.lang.LazySeq.seq(LazySeq.java:49)
	at clojure.lang.RT.seq(RT.java:484)
	at clojure.core$seq.invoke(core.clj:133)
	at clojure.core$dorun.invoke(core.clj:2855)
	at clojure.core$doall.invoke(core.clj:2871)
	at backtype.storm.ui.core$visualization_data.invoke(core.clj:250)
	at backtype.storm.ui.core$build_visualization.invoke(core.clj:431)
	at backtype.storm.ui.core$fn__11091.invoke(core.clj:842)
	at org.apache.storm.shade.compojure.core$make_route$fn__8804.invoke(core.clj:93)
	at org.apache.storm.shade.compojure.core$if_route$fn__8792.invoke(core.clj:39)
	at org.apache.storm.shade.compojure.core$if_method$fn__8785.invoke(core.clj:24)
	at org.apache.storm.shade.compojure.core$routing$fn__8810.invoke(core.clj:106)
	at clojure.core$some.invoke(core.clj:2515)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:106)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:626)
	at org.apache.storm.shade.compojure.core$routes$fn__8814.invoke(core.clj:111)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__9325.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__10128.invoke(reload.clj:22)
	at backtype.storm.ui.helpers$requests_middleware$fn__9522.invoke(helpers.clj:46)
	at backtype.storm.ui.core$catch_errors$fn__11228.invoke(core.clj:1072)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__9258.invoke(keyword_params.clj:27)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__9297.invoke(nested_params.clj:65)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__9230.invoke(params.clj:55)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__9325.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__9506.invoke(flash.clj:14)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__9495.invoke(session.clj:43)
	at org.apache.storm.shade.ring.middleware.cookies$wrap_cookies$fn__9426.invoke(cookies.clj:160)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__9143.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__9147.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:652)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1317)
	at backtype.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at backtype.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1288)
	at yjava.servlet.FilterChainInvoker$ServletFilterChainInvoker.invoke(FilterChainInvoker.java:49)
	at yjava.servlet.filter.BouncerFilter.doFilter(BouncerFilter.java:231)
	at yjava.servlet.filter.BouncerFilter.doFilter(BouncerFilter.java:110)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1288)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1288)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

"
STORM-1514,add ability to configure read only option for storm UI,Today in a secure env one has to be logged in as storm before they can view the UI. It would be nice if one could configure read only access to specific users and/or groups that way users could view the progress being made by various topologies.
STORM-1512,how to execute one bolt after another when the input is taken from same spout.,"My requirement is to first execute one bolt and upon successful execution only next bolt have to execute and i am giving the input for the bolts from same spout.For that for second bolt i am using Thread.sleep() method, its working fine but have performance issues.Can anyone help me if there is any alternative for this problem."
STORM-1509,RAS Implementation does not show memory and cpu utilization,"Ran a default ResourceAwareSchedulerExampleTopology with config
scheduler.display.resource set to true. It does not show the adequate cpu and resource utilization"
STORM-1507,Getting the parallelism of components via thrift API after a rebalance/change in parallelism is incorrect,"When the parallelism of a component in a topology is changed, the thrift APIs to get the parallelism (""get_parallelism_hint()"") of components still return the original parallelism of the component and not the parallelism it changed to.  This due to the fact that the thrift object responsible for holding the information about a component only gets initialized once in nimbus.clj but never gets updated when a change in parallelism happens.  This may also cause the UI to not display the correct parallelism as well."
STORM-1506,It's better to  be Integer about port of STORM_ZOOKEEPER_PORT&TRANSACTIONAL_ZOOKEEPER_PORT ,It's better to replace Object by  Integer about port of STORM_ZOOKEEPER_PORT&TRANSACTIONAL_ZOOKEEPER_PORT
STORM-1502,Create per-version document scheme,"We want to modify our documentation scheme such that each version of storm has its own set of documentation published to the apache storm website. 

General consensus was that at least javadocs should be available per-version, but it is okay if parts of the process require manual intervention.

The full conversation about it can be found here:
http://mail-archives.apache.org/mod_mbox/storm-dev/201601.mbox/browser

"
STORM-1501, launch worker process exception will cause supervisor process exited,"[util.clj/async-loop | https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/util.clj#L474] default kill-fn will kill current process  

when supervisor use [util.clj/launch-process | https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/util.clj#L546] to launch worker process , if exeception occurs , supervisor process will exit."
STORM-1500,Couldn't extract resources in the supervisor,"i need this files to be extracted fromvar/storm/nimbus/stormdist/topology_name/stormjar.jar  to /var/storm/supervisor/tmp but i got in supervisor log file
[INFO] Could not extract resources from /var/storm/supervisor/tmp/f5989f74-aae5-43c8-8b41-056074cb7e55/stormjar.jar

i tried to exctract it manually but didn't fix ! how can i fix it ? "
STORM-1498,"In ConfigUtils.getLogMetaDataFile, file name split does not work with windows","In ConfigUtils.getLogMetaDataFile, ""fname.split(FILE_SEPARATOR)"" may have problem working on Windows."
STORM-1497,Repo Branches Cleanup ,"I've identified a bunch of branches that I think can go away:

Already merged:
STORM-633
STORM-648
STORM-651
STORM-1153-V1
better-trident-spouts
experimental
clojure1.3
clojure1.4
executors
gettuples
kryo-2.16
mesos
new-resource-scheduler
nimbus-ha-branch
scheduler
security
transactional-spout
ui-url

Branches not merged, but I think we can delete:
STORM-1040 (harschach)
PR_736 (ptgoetz)
nimbus-ha (nathanmarz)
debug (do not want)  (nathanmarz)
iso-scheduler (not merged but VERY old) (nathanmarz)
jarjar (not merged but VERY old) (nathanmarz)
statespout (not merged but VERY old) (nathanmarz)
std-redirect (not merged but VERY old) (nathanmarz)
superclojure (not merged but VERY old) (nathanmarz)
thunk (VERY old, don't want) (sritchie)"
STORM-1490,Update storm-merge.py to be full featured,"storm-merge.py should optionally do more then just print out the merge command.  

At a minimum it should

   1. update the branch you are trying to merge to
   2. create the branch integration branch (based off of the first branch)
   3. run the merge
   4. update the changelog (unless we just want this to go away)
   5. run some sanity tests (minimum build, ideally some tests and RAT too)
   6. merge the integration back to the main branch
   7. push the result back to apache

It would also really be nice if it could offer the ability to cherry-pick a pull request made on another branch.  Updating things as needed 

If we want the changelog to go away it would not be too hard to combine a JIRA query with a git log to regenerate that too.  But that might be a separate tool."
STORM-1489,Script for cleaner environment checking,"As a storm developer, I would like a common script that storm can execute to detect features of the environment (like OS), so that storm has cleaner code for enabling features at run-time.

See [original comment|https://github.com/apache/storm/pull/1012#discussion_r50280610]"
STORM-1487,UI Topology Page tooltips misplaced,"!screen-shot-tooltips.png|thumbnail!

Seems the placement is off.

(Note, the error timestamp in the year 1970 will be handled in a separate issue.)"
STORM-1477,STORM HDFS bolt must acknoledge tuples after rotation,"While performing rotation we call closeOutputFile method - so data is already in HDFS and tuples must be acknoledged. Otherwise, next sync operation could fail and mark thouse tuples as failed."
STORM-1475,storm-elasticsearch should support ES 2.X,
STORM-1474,Address remaining minor review comments for STORM-1199,"Address the last few pending review comments  from 
https://github.com/apache/storm/pull/936"
STORM-1471,Make FetchRequestBuilder.minBytes a configurable parameter in SpoutConfig,"We currently have an issue in our storm cluster where our Kafka brokers are under heavy load due to too many fetch requests from storm.  We've narrowed the problem to the way Fetch Requests are build in KafkaUtils.  When using the FetchRequestBuilder, storm provides overrides for all the properties except minBytes.  The default for that field is 0 (even though the Kafka default for the high-level consumer is 1).  When paired with a maxWait > 0, this creates a situation where the broker can immediately return a response without waiting (due to minBytes 0).  This puts a heavy load on the brokers and defeats the purpose of any long polling.

By making this a SpoutConfig option, it will allow the user to set that as appropriate for their situation."
STORM-1469,Unable to deploy large topologies on apache storm,"When deploying to a nimbus a topology which is larger in size >17MB, we get an exception. In storm 0.9.3 this could be mitigated by using the following config on the storm.yaml to increse the buffer size to handle the topology size. i.e. 50MB would be

nimbus.thrift.max_buffer_size: 50000000

This configuration does not resolve the issue in the master branch of storm and we cannot deploy topologies which are large in size.

Here is the log on the client side when attempting to deploy to the nimbus node:
java.lang.RuntimeException: org.apache.thrift7.transport.TTransportException
	at backtype.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:251) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:272) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:155) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at com.trustwave.siem.storm.topology.deployer.TopologyDeployer.deploy(TopologyDeployer.java:149) [siem-ng-storm-deployer-cloud.jar:]
	at com.trustwave.siem.storm.topology.deployer.TopologyDeployer.main(TopologyDeployer.java:87) [siem-ng-storm-deployer-cloud.jar:]
Caused by: org.apache.thrift7.transport.TTransportException
	at org.apache.thrift7.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TFramedTransport.read(TFramedTransport.java:101) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:77) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.generated.Nimbus$Client.recv_submitTopology(Nimbus.java:238) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.generated.Nimbus$Client.submitTopology(Nimbus.java:222) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:237) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	... 4 more

Here is the log on the server side (nimbus.log):

2016-01-13 10:48:07.206 o.a.s.d.nimbus [INFO] Cleaning inbox ... deleted: stormjar-c8666220-fa19-426b-a7e4-c62dfb57f1f0.jar
2016-01-13 10:55:09.823 o.a.s.d.nimbus [INFO] Uploading file from client to /var/storm-data/nimbus/inbox/stormjar-80ecdf05-6a25-4281-8c78-10062ac5e396.jar
2016-01-13 10:55:11.910 o.a.s.d.nimbus [INFO] Finished uploading file from client: /var/storm-data/nimbus/inbox/stormjar-80ecdf05-6a25-4281-8c78-10062ac5e396.jar
2016-01-13 10:55:12.084 o.a.t.s.AbstractNonblockingServer$FrameBuffer [WARN] Exception while invoking!
org.apache.thrift7.transport.TTransportException: Frame size (17435758) larger than max length (16384000)!
	at org.apache.thrift7.transport.TFramedTransport.readFrame(TFramedTransport.java:137)
	at org.apache.thrift7.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift7.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift7.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift7.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
	at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
	at org.apache.thrift7.server.Invocation.run(Invocation.java:18)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


"
STORM-1465,TridentOperationContext - no way to check if metric is already registered,"There's no way to call TopologyContext.getRegisteredMetricByName, to check if metric has already been registered, when using Trident API.

The registerMetric method has been already exposed to Trident, but other methods from TopologyContext are kept hidden.

Aside from that... Why does TridentOperationContext not extend TopologyContext? (They both implement the same interface - IMetricsContext and are used in the same way)."
STORM-1460,add storm-tachyon connector,Tachyon is a memory-centric distributed storage system.
STORM-1456,Cannot check for ack or fail in FixedTupleSpout,"The FixedTupleSpout class has two static methods for getting the number of acked or failed tuples.  It takes a single parameter, which is the stormId, to identify which instance of the class is being checked.

The problem is that the id is auto generated in the constructor, set to a private member and has no accessor.  Thus you cannot get the stormId to use in the static methods.

I would recommend simply making the methods non static and removing the parameter.  This would be more consistent with the *getCompleted()* method."
STORM-1454,UI cannot handle topology names with spaces,"If I submit a topology with an name that contains spaces (eg, ""Linear Road Benchmark"", I cannot access the detailed topology view. If I click on the topology name in the Web UI, I get the following:

Internal Server Error
{noformat}
NotAliveException(msg:Linear+Road+Benchmark-3-1452252242)
	at backtype.storm.generated.Nimbus$getTopologyInfo_result.read(Nimbus.java:11347)
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:78)
	at backtype.storm.generated.Nimbus$Client.recv_getTopologyInfo(Nimbus.java:491)
	at backtype.storm.generated.Nimbus$Client.getTopologyInfo(Nimbus.java:478)
	at backtype.storm.ui.core$topology_page.invoke(core.clj:628)
	at backtype.storm.ui.core$fn__8020.invoke(core.clj:853)
	at compojure.core$make_route$fn__6199.invoke(core.clj:93)
	at compojure.core$if_route$fn__6187.invoke(core.clj:39)
	at compojure.core$if_method$fn__6180.invoke(core.clj:24)
	at compojure.core$routing$fn__6205.invoke(core.clj:106)
	at clojure.core$some.invoke(core.clj:2443)
	at compojure.core$routing.doInvoke(core.clj:106)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:619)
	at compojure.core$routes$fn__6209.invoke(core.clj:111)
	at ring.middleware.reload$wrap_reload$fn__6234.invoke(reload.clj:14)
	at backtype.storm.ui.core$catch_errors$fn__8059.invoke(core.clj:909)
	at ring.middleware.keyword_params$wrap_keyword_params$fn__6876.invoke(keyword_params.clj:27)
	at ring.middleware.nested_params$wrap_nested_params$fn__6915.invoke(nested_params.clj:65)
	at ring.middleware.params$wrap_params$fn__6848.invoke(params.clj:55)
	at ring.middleware.multipart_params$wrap_multipart_params$fn__6943.invoke(multipart_params.clj:103)
	at ring.middleware.flash$wrap_flash$fn__7124.invoke(flash.clj:14)
	at ring.middleware.session$wrap_session$fn__7113.invoke(session.clj:43)
	at ring.middleware.cookies$wrap_cookies$fn__7044.invoke(cookies.clj:160)
	at ring.adapter.jetty$proxy_handler$fn__7324.invoke(jetty.clj:16)
	at ring.adapter.jetty.proxy$org.mortbay.jetty.handler.AbstractHandler$0.handle(Unknown Source)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}

The link says: http://dbis71:8080/topology.html?id=Linear+Road+Benchmark-5-1452255348
If I replace the ""+"" with spaces manually I can access the page. However, I cannot ""kill"" the topology -- a click on ""kill"" has no effect."
STORM-1451,Storm topology submission can take upto 5 minutes in HA mode when zookeeper reconnects. Nimbus discovery can fail when zookeeper reconnect happens.,"We discovered couple of issues when testing storm under vagrant clusters.
1. When a nimbus zookeeper connection is dropped and reconnected the ephemeral entry for that host under /nimbuses gets deleted and is not auto recreated when reconnection happens. This means even though nimbus is up no client will be able to actually discover it. To address this issue we now have a listener that listens for RECONNECT events and recreates the entry.
2. Zookeeper is eventual consistent when multiple clients are involved. In practice we did not notice this issue but in the vagrant cluster due to resource constrained it was pretty evident that updates created by leader nimbuses were not observed by other nimbus host unless they waited for a few second. Due to this topology submission can take upto 5 minutes which is super bad user experience."
STORM-1448,How to return objects in declare output fields,"I was working on twitter data using kafka-storm. I was using deserialize method to parse the twitter data using storm spout , here in parser I was facing a problem of returning all the objects. What I have done is I added them in a list and returned the list. Up to that it's working fine but, while coming to getOutputFields() it showing runtime error i.e., 

ERROR : java.lang.IllegalArgumentException: Tuple created with wrong number of fields. Expected 6 fields but got 140 fields at backtype.storm.tuple.TupleImpl.<init>(TupleImpl.java:58) at backtype.storm.daemon.executor$fn_5624$fn5639$send_spout_msg5658.invoke(executor.clj:529) at backtype.storm.daemon.executor$fn5624$fn$reify5668.emit(executor.clj:568) at backtype.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:49) at backtype.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:63) at storm.kafka.PartitionManager.next(PartitionManager.java:141) at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:141) at backtype.storm.daemon.executor$fn5624$fn5639$fn5670.invoke(executor.clj:607) at backtype.storm.util$async_loop$fn_545.invoke(util.clj:479) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:745) 


I was returning 6 objects but it showing that its getting 60 fields . How to solve that ? Can anyone suggest me the solution ?
Below is the program which I am working on .

Program : 

package Demo;

import java.sql.Timestamp;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.io.IOException;
import java.util.Properties;
import java.util.*;
import twitter4j.JSONArray;
import twitter4j.JSONObject;
import twitter4j.JSONObjectType;
import java.io.UnsupportedEncodingException;
import backtype.storm.spout.Scheme;
import backtype.storm.spout.RawScheme;
import backtype.storm.spout.SpoutOutputCollector;
import backtype.storm.task.TopologyContext;
import backtype.storm.topology.OutputFieldsDeclarer;
import backtype.storm.topology.base.BaseRichSpout;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Values;

public class TwitterTweet implements Scheme{

	//twitter_tweets fields

	String created_at;
	String id;
	String id_str;
	String text;
	String source;
	String truncated;
	
	int i=0;int j=0;

	@Override
	public List<Object> deserialize(final byte[] bytes) {

		List<Object> list = new ArrayList<Object>() {
			{
	try{
		String twitterEvent = new String(bytes, ""UTF-8"");
	
	 JSONArray JSON = new JSONArray(twitterEvent); // kafka topic name(twitterEvent)
     	for(i=0;i<JSON.length();i++)
        {
            JSONObject object_tweet=JSON.getJSONObject(i);
    			
           	//Tweet status 		 		
 		
		try{

	 	add(created_at=object_tweet.getString(""created_at""));
	 	add(id=object_tweet.getString(""id""));
		add(id_str=object_tweet.getString(""id_str""));
		add(text=object_tweet.getString(""text""));
		add(source=object_tweet.getString(""source""));
		add(truncated=object_tweet.getString(""truncated""));
		
			}catch(Exception e){}

		}//JSON main_array for close

	}catch(Exception e){} // UTF- try close

	}
	};

return list;

     } //deserialize method close

public Fields getOutputFields() {
	return new Fields (""created_at"",""id"",""id_str"",""text"",""source"",""truncated"");

		} // getOutputFields() method close

	}  //class close

"
STORM-1447,add storm-flume connector,"flume --> storm FlumeSpout
storm FlumeBolt --> flume
"
STORM-1441,Random test failure on nimbus-auth-test,"{code}
Running backtype.storm.security.auth.nimbus-auth-test
63484 [pool-937-thread-1-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
63485 [main] INFO  b.s.d.nimbus - Shutting down master
63487 [main] INFO  b.s.zookeeper - closing zookeeper connection of leader elector.
63489 [main] INFO  b.s.d.nimbus - Shut down master
63490 [pool-937-thread-1] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
63495 [main] INFO  b.s.testing - Shutting down in process zookeeper
63499 [main] INFO  b.s.testing - Done shutting down in process zookeeper
63499 [main] INFO  b.s.testing - Deleting temporary path /tmp/7268ca66-4258-407e-8920-33e611eed59b
63500 [main] INFO  b.s.testing - Deleting temporary path /tmp/c486768a-2c3d-4db3-8711-23770759444a
63774 [main] INFO  b.s.d.nimbus - Shutting down master
63825 [main] INFO  b.s.zookeeper - closing zookeeper connection of leader elector.
63827 [pool-957-thread-1-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
63828 [main] INFO  b.s.d.nimbus - Shut down master
63828 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2000] WARN  o.a.z.s.NIOServerCnxn - caught end of stream exception
org.apache.zookeeper.server.ServerCnxn$EndOfStreamException: Unable to read additional data from client sessionid 0x151a5a7ecc20000, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_31]
63856 [main] INFO  b.s.testing - Shutting down in process zookeeper
63857 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2000] WARN  o.a.z.s.NIOServerCnxnFactory - Ignoring unexpected runtime exception
java.nio.channels.CancelledKeyException
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) ~[?:1.8.0_31]
	at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:87) ~[?:1.8.0_31]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:187) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_31]
63862 [SyncThread:0] ERROR o.a.z.s.NIOServerCnxn - Unexpected Exception: 
java.nio.channels.CancelledKeyException
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) ~[?:1.8.0_31]
	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77) ~[?:1.8.0_31]
	at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:151) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:404) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.server.SyncRequestProcessor.flush(SyncRequestProcessor.java:200) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:131) [zookeeper-3.4.6.jar:3.4.6-1569965]
63864 [main] INFO  b.s.testing - Done shutting down in process zookeeper
63864 [main] INFO  b.s.testing - Deleting temporary path /tmp/4ff1e17c-3996-422d-822d-d15fa28a08c8
63865 [main] INFO  b.s.testing - Deleting temporary path /tmp/1efc1124-61de-4a8f-a4a6-d0d1ac0b64f6
Tests run: 5, Passed: 37, Failures: 2, Errors: 0
{code}"
STORM-1440,Random test failure on storm-cassandra: AlreadyExistsException: Keyspace weather already exists,"{code}
Running org.apache.storm.cassandra.DynamicStatementBuilderTest
Tests run: 7, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 19.203 sec <<< FAILURE! - in org.apache.storm.cassandra.DynamicStatementBuilderTest
shouldBuildStaticInsertStatementGivenNoKeyspaceAllMapper(org.apache.storm.cassandra.DynamicStatementBuilderTest)  Time elapsed: 12.31 sec  <<< ERROR!
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9142 (com.datastax.driver.core.OperationTimedOutException: [/127.0.0.1:9142] Operation timed out))
	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:62)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9142 (com.datastax.driver.core.OperationTimedOutException: [/127.0.0.1:9142] Operation timed out))
	at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:216)
	at com.datastax.driver.core.RequestHandler.access$900(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.sendRequest(RequestHandler.java:276)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution$1.run(RequestHandler.java:374)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
shouldBuildStaticUnloggedBatchStatementGivenNoKeyspaceAndWithFieldsMapper(org.apache.storm.cassandra.DynamicStatementBuilderTest)  Time elapsed: 0.001 sec  <<< ERROR!
com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:69)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:104)
	at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:118)
	at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183)
	at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:573)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:913)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:69)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:213)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:204)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
shouldBuildStaticInsertStatementGivenKeyspaceAndAllMapper(org.apache.storm.cassandra.DynamicStatementBuilderTest)  Time elapsed: 0.091 sec  <<< ERROR!
com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:69)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:104)
	at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:118)
	at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183)
	at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:573)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:913)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:69)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:213)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:204)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
shouldBuildStaticBoundStatement(org.apache.storm.cassandra.DynamicStatementBuilderTest)  Time elapsed: 0.244 sec  <<< ERROR!
com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:69)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:104)
	at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:118)
	at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183)
	at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:573)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:913)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:69)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:213)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:204)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)

classname: org.apache.storm.cassandra.DynamicStatementBuilderTest / testname: shouldBuildStaticInsertStatementGivenNoKeyspaceAllMapper
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9142 (com.datastax.driver.core.OperationTimedOutException: [/127.0.0.1:9142] Operation timed out))
	at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:62)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9142 (com.datastax.driver.core.OperationTimedOutException: [/127.0.0.1:9142] Operation timed out))
	at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:216)
	at com.datastax.driver.core.RequestHandler.access$900(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.sendRequest(RequestHandler.java:276)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution$1.run(RequestHandler.java:374)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
-------------------- system-out --------------------
6736 [main] INFO  c.d.d.c.p.DCAwareRoundRobinPolicy - Using data-center name 'datacenter1' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
6736 [main] INFO  c.d.d.c.Cluster - New Cassandra host /127.0.0.1:9142 added
6756 [SharedPool-Worker-2] INFO  o.a.c.s.MigrationManager - Drop Keyspace 'weather'
6760 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of schema_keyspaces: 155 (0%) on-heap, 0 (0%) off-heap
6760 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Writing Memtable-schema_keyspaces@1570950213(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.cassandra.DynamicStatementBuilderTest / testname: shouldBuildStaticUnloggedBatchStatementGivenNoKeyspaceAndWithFieldsMapper
com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:69)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:104)
	at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:118)
	at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183)
	at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:573)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:913)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:69)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:213)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:204)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
-------------------- system-out --------------------
18858 [main] INFO  c.d.d.c.p.DCAwareRoundRobinPolicy - Using data-center name 'datacenter1' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
18858 [main] INFO  c.d.d.c.Cluster - New Cassandra host /127.0.0.1:9142 added
--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.cassandra.DynamicStatementBuilderTest / testname: shouldBuildStaticInsertStatementGivenKeyspaceAndAllMapper
com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:69)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:104)
	at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:118)
	at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183)
	at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:573)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:913)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:69)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:213)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:204)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
-------------------- system-out --------------------
18976 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-11-Data.db (33 bytes) for commitlog position ReplayPosition(segmentId=1451364264392, position=9114)
18979 [main] INFO  c.d.d.c.p.DCAwareRoundRobinPolicy - Using data-center name 'datacenter1' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
18980 [main] INFO  c.d.d.c.Cluster - New Cassandra host /127.0.0.1:9142 added
--------------------------------------------------
--------------------------------------------------
classname: org.apache.storm.cassandra.DynamicStatementBuilderTest / testname: shouldBuildStaticBoundStatement
com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:269)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:183)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
	at org.cassandraunit.CQLDataLoader.initKeyspaceContext(CQLDataLoader.java:69)
	at org.cassandraunit.CQLDataLoader.load(CQLDataLoader.java:31)
	at org.cassandraunit.CassandraCQLUnit.load(CassandraCQLUnit.java:51)
	at org.cassandraunit.BaseCassandraUnit.before(BaseCassandraUnit.java:32)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.exceptions.AlreadyExistsException.copy(AlreadyExistsException.java:85)
	at com.datastax.driver.core.Responses$Error.asException(Responses.java:104)
	at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:118)
	at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183)
	at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:45)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:748)
	at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:573)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:991)
	at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:913)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.datastax.driver.core.exceptions.AlreadyExistsException: Keyspace weather already exists
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:69)
	at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:213)
	at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:204)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)
	at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
-------------------- system-out --------------------
19062 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of schema_columnfamilies: 155 (0%) on-heap, 0 (0%) off-heap
19063 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Writing Memtable-schema_columnfamilies@614174029(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
19103 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-8-Data.db (33 bytes) for commitlog position ReplayPosition(segmentId=1451364264392, position=9122)
19104 [CompactionExecutor:2] INFO  o.a.c.d.c.CompactionTask - Compacting [SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-6-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-5-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-7-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-8-Data.db')]
19108 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of schema_columns: 155 (0%) on-heap, 0 (0%) off-heap
19108 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Writing Memtable-schema_columns@1627156840(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
19131 [main] INFO  c.d.d.c.p.DCAwareRoundRobinPolicy - Using data-center name 'datacenter1' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)
19131 [main] INFO  c.d.d.c.Cluster - New Cassandra host /127.0.0.1:9142 added
19152 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-8-Data.db (33 bytes) for commitlog position ReplayPosition(segmentId=1451364264392, position=9122)
19153 [CompactionExecutor:1] INFO  o.a.c.d.c.CompactionTask - Compacting [SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-6-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-5-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-7-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-8-Data.db')]
19163 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of schema_triggers: 155 (0%) on-heap, 0 (0%) off-heap
19163 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Writing Memtable-schema_triggers@1163581484(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
19189 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_triggers-0359bc7171233ee19a4ab9dfb11fc125/system-schema_triggers-ka-3-Data.db (33 bytes) for commitlog position ReplayPosition(segmentId=1451364264392, position=9122)
19192 [CompactionExecutor:2] INFO  o.a.c.d.c.CompactionTask - Compacted 4 sstables to [target/embeddedCassandra/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-9,].  8,698 bytes to 6,907 (~79% of original) in 86ms = 0.076593MB/s.  6 total partitions merged to 3.  Partition merge counts were {1:2, 4:1, }
19196 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of schema_usertypes: 155 (0%) on-heap, 0 (0%) off-heap
19196 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Writing Memtable-schema_usertypes@53100229(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
19201 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/schema_usertypes-3aa752254f82350b8d5c430fa221fa0a/system-schema_usertypes-ka-3-Data.db (33 bytes) for commitlog position ReplayPosition(segmentId=1451364264392, position=9607)
19205 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of local: 881 (0%) on-heap, 0 (0%) off-heap
19206 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Writing Memtable-local@77342413(126 serialized bytes, 9 ops, 0%/0% of on/off-heap limit)
19212 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-8-Data.db (133 bytes) for commitlog position ReplayPosition(segmentId=1451364264392, position=9720)
19213 [CompactionExecutor:2] INFO  o.a.c.d.c.CompactionTask - Compacting [SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-8-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-6-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-7-Data.db'), SSTableReader(path='/home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-5-Data.db')]
19217 [MigrationStage:1] INFO  o.a.c.c.QueryProcessor - Table weather.station was dropped, invalidating related prepared statements
19228 [cluster6-nio-worker-0] WARN  c.d.d.c.Cluster - Received a DROPPED notification for table weather.station, but this keyspace is unknown in our metadata
19229 [cluster5-nio-worker-0] WARN  c.d.d.c.Cluster - Received a DROPPED notification for table weather.station, but this keyspace is unknown in our metadata
19229 [cluster4-nio-worker-0] WARN  c.d.d.c.Cluster - Received a DROPPED notification for table weather.station, but this keyspace is unknown in our metadata
19233 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of IndexInfo: 155 (0%) on-heap, 0 (0%) off-heap
19233 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of compaction_history: 5552 (0%) on-heap, 0 (0%) off-heap
19234 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Writing Memtable-IndexInfo@1218571140(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
19235 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Writing Memtable-compaction_history@2121092188(1124 serialized bytes, 40 ops, 0%/0% of on/off-heap limit)
19238 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/IndexInfo-9f5c6374d48532299a0a5094af9ad1e3/system-IndexInfo-ka-2-Data.db (33 bytes) for commitlog position ReplayPosition(segmentId=1451364264393, position=24)
19267 [MemtableFlushWriter:1] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/system-compaction_history-ka-2-Data.db (547 bytes) for commitlog position ReplayPosition(segmentId=1451364264393, position=24)
19270 [MigrationStage:1] INFO  o.a.c.d.ColumnFamilyStore - Enqueuing flush of local: 357 (0%) on-heap, 0 (0%) off-heap
19270 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Writing Memtable-local@1386225500(40 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
19279 [MemtableFlushWriter:2] INFO  o.a.c.d.Memtable - Completed flushing /home/travis/build/apache/storm/external/storm-cassandra/target/embeddedCassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-10-Data.db (80 bytes) for commitlog position ReplayPosition(segmentId=1451364264393, position=137)
19279 [MigrationStage:1] INFO  o.a.c.c.QueryProcessor - Table weather.temperature was dropped, invalidating related prepared statements
19280 [CompactionExecutor:2] INFO  o.a.c.d.c.CompactionTask - Compacted 4 sstables to [target/embeddedCassandra/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-9,].  873 bytes to 545 (~62% of original) in 58ms = 0.008961MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
{code}"
STORM-1439,Random test failures on integration-test (can't see the details),"{code}
Running integration.backtype.storm.integration-test
291683 [main] INFO  b.s.d.nimbus - Shutting down master
291686 [main] INFO  b.s.zookeeper - closing zookeeper connection of leader elector.
291687 [main] INFO  b.s.d.nimbus - Shut down master
291689 [main] INFO  b.s.d.supervisor - Shutting down supervisor 06797c28-aeda-408b-aa41-de43bac0caf8
291690 [Thread-2284] INFO  b.s.event - Event manager interrupted
291690 [Thread-2285] INFO  b.s.event - Event manager interrupted
291691 [main] INFO  b.s.d.supervisor - Shutting down supervisor 78984e56-df4f-412f-91c4-ba1142b53452
291692 [Thread-2286] INFO  b.s.event - Event manager interrupted
291692 [Thread-2287] INFO  b.s.event - Event manager interrupted
291693 [main] INFO  b.s.testing - Shutting down in process zookeeper
291694 [main] INFO  b.s.testing - Done shutting down in process zookeeper
291694 [main] INFO  b.s.testing - Deleting temporary path /tmp/bd479e33-beef-4d52-ae3b-9e8e4d49e5ed
291696 [main] INFO  b.s.testing - Deleting temporary path /tmp/99289814-f5e8-4697-858c-a3f150cdbd82
291697 [main] INFO  b.s.testing - Deleting temporary path /tmp/bb176401-539d-45ab-b98d-8e343ec6801f
291699 [main] INFO  b.s.testing - Deleting temporary path /tmp/d183fc0f-745c-4be8-82d3-0f206c9cd3e8
Tests run: 17, Passed: 44, Failures: 1, Errors: 0
{code}"
STORM-1438,Random test failures on TridentKafkaTest : No output has been received in the last 10 minutes,"{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running storm.kafka.TridentKafkaTest

...

No output has been received in the last 10 minutes, this potentially indicates a stalled build or something wrong with the build itself.

The build has been terminated
{code}"
STORM-1437,Random test failures on test-load at netty-unit-test,"test-load at netty-unit-test occasionally fails. 

{code}
Running backtype.storm.messaging.netty-unit-test
Tests run: 5, Passed: 20, Failures: 0, Errors: 1

--------------------------------------------------
classname: backtype.storm.messaging.netty-unit-test / testname: test-load
Uncaught exception, not in assertion.
expected: nil
  actual: java.lang.AssertionError: Test timed out (5000ms) (empty? (.getLoad client [(int 1) (int 2)]))
 at backtype.storm.messaging.netty_unit_test$test_load_fn.invoke (netty_unit_test.clj:115)
    backtype.storm.messaging.netty_unit_test/fn (netty_unit_test.clj:144)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900$fn__10901.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval10893.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
      at: test_runner.clj:105
-------------------- system-out --------------------
30464 [main] INFO  b.s.m.netty-unit-test - 2 test load
30464 [main] INFO  b.s.m.TransportFactory - Storm peer transport plugin:backtype.storm.messaging.netty.Context
30474 [main] INFO  b.s.m.n.Server - Create Netty Server Netty-server-localhost-6700, buffer_size: 1024, maxWorkers: 1
30476 [main] INFO  b.s.m.n.Client - creating Netty Client, connecting to localhost:6700, bufferSize: 1024
30476 [main] INFO  b.s.m.netty-unit-test - Waiting until all Netty connections are ready...
30588 [main] INFO  b.s.m.netty-unit-test - All Netty connections are ready
30603 [main] INFO  b.s.m.n.Client - closing Netty Client Netty-Client-localhost/127.0.0.1:6700
30603 [main] INFO  b.s.m.n.Client - waiting up to 600000 ms to send 0 pending messages to Netty-Client-localhost/127.0.0.1:6700
30605 [main] INFO  b.s.m.netty-unit-test - 2 test load
30605 [main] INFO  b.s.m.TransportFactory - Storm peer transport plugin:backtype.storm.messaging.netty.Context
30609 [main] INFO  b.s.m.n.Server - Create Netty Server Netty-server-localhost-6700, buffer_size: 1024, maxWorkers: 1
30610 [main] INFO  b.s.m.n.Client - creating Netty Client, connecting to localhost:6700, bufferSize: 1024
30610 [main] INFO  b.s.m.netty-unit-test - Waiting until all Netty connections are ready...
30722 [main] INFO  b.s.m.netty-unit-test - All Netty connections are ready
30799 [Thread-147-SendThread(localhost:2000)] WARN  o.a.z.ClientCnxn - Session 0x151ec0a79640011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
32859 [Thread-147-SendThread(localhost:2000)] WARN  o.a.z.ClientCnxn - Session 0x151ec0a79640011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
34645 [Thread-147-SendThread(localhost:2000)] WARN  o.a.z.ClientCnxn - Session 0x151ec0a79640011 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
35727 [main] INFO  b.s.m.netty-unit-test - Condition (empty? (.getLoad client [(int 1) (int 2)])) not met in 5000ms
35727 [main] INFO  b.s.m.netty-unit-test - ""pool-305-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:483)
        at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:392)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at java.lang.Thread.run(Thread.java:745)
""client-schedule-service-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:483)
        at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:392)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at java.lang.Thread.run(Thread.java:745)
""Netty-server-localhost-6700-boss-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""Netty-server-localhost-6700-worker-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""client-boss-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""client-worker-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""Thread-147-EventThread"" 
   java.lang.Thread.State: WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:494)
""Thread-147-SendThread(localhost:2000)"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.zookeeper.client.StaticHostProvider.next(StaticHostProvider.java:101)
        at org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:940)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1003)
""pool-299-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""Curator-Framework-0"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.DelayQueue.take(DelayQueue.java:223)
        at java.util.concurrent.DelayQueue.take(DelayQueue.java:70)
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:795)
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:63)
        at org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:267)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""Curator-ConnectionStateManager-0"" 
   java.lang.Thread.State: WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
        at org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:245)
        at org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
        at org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-254-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""pool-211-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""Thread-123"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-121"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-120"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""pool-168-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-167-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""Thread-77"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-75"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-74"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-72"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-7"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-6"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-5"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-4"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-3"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-2"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-117-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-116-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""metric/stat timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
""disruptor-flush-trigger"" 
   java.lang.Thread.State: WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.util.TimerThread.mainLoop(Timer.java:526)
        at java.util.TimerThread.run(Timer.java:505)
""Thread-29"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-24"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-19"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-17"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-16"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-14"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""clojure-agent-send-off-pool-0"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""FileBlobStore cleanup thread"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
""Thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Signal Dispatcher"" 
   java.lang.Thread.State: RUNNABLE
""Finalizer"" 
   java.lang.Thread.State: WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:142)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:158)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
""Reference Handler"" 
   java.lang.Thread.State: WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157)
""main"" 
   java.lang.Thread.State: RUNNABLE
        at sun.management.ThreadImpl.getThreadInfo1(Native Method)
        at sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
        at backtype.storm.utils.Utils.threadDump(Utils.java:1134)
        at backtype.storm.messaging.netty_unit_test$test_load_fn.invoke(netty_unit_test.clj:115)
        at backtype.storm.messaging.netty_unit_test$fn__938.invoke(netty_unit_test.clj:144)
        at clojure.test$test_var$fn__7670.invoke(test.clj:704)
        at clojure.test$test_var.invoke(test.clj:704)
        at clojure.test$test_vars$fn__7692$fn__7697.invoke(test.clj:722)
        at clojure.test$default_fixture.invoke(test.clj:674)
        at clojure.test$test_vars$fn__7692.invoke(test.clj:722)
        at clojure.test$default_fixture.invoke(test.clj:674)
        at clojure.test$test_vars.invoke(test.clj:718)
        at clojure.test$test_all_vars.invoke(test.clj:728)
        at clojure.test$test_ns.invoke(test.clj:747)
        at clojure.core$map$fn__4553.invoke(core.clj:2624)
        at clojure.lang.LazySeq.sval(LazySeq.java:40)
        at clojure.lang.LazySeq.seq(LazySeq.java:49)
        at clojure.lang.Cons.next(Cons.java:39)
        at clojure.lang.RT.boundedLength(RT.java:1735)
        at clojure.lang.RestFn.applyTo(RestFn.java:130)
        at clojure.core$apply.invoke(core.clj:632)
        at clojure.test$run_tests.doInvoke(test.clj:762)
        at clojure.lang.RestFn.invoke(RestFn.java:408)
        at org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900$fn__10901.invoke(test_runner.clj:107)
        at org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900.invoke(test_runner.clj:53)
        at org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899.invoke(test_runner.clj:52)
        at clojure.lang.LazySeq.sval(LazySeq.java:40)
        at clojure.lang.LazySeq.seq(LazySeq.java:49)
        at clojure.lang.RT.seq(RT.java:507)
        at clojure.core$seq__4128.invoke(core.clj:137)
        at clojure.core$dorun.invoke(core.clj:3009)
        at org.apache.storm.testrunner$eval10893.invoke(test_runner.clj:52)
        at clojure.lang.Compiler.eval(Compiler.java:6782)
        at clojure.lang.Compiler.load(Compiler.java:7227)
        at clojure.lang.Compiler.loadFile(Compiler.java:7165)
        at clojure.main$load_script.invoke(main.clj:275)
        at clojure.main$script_opt.invoke(main.clj:337)
        at clojure.main$main.doInvoke(main.clj:421)
        at clojure.lang.RestFn.invoke(RestFn.java:421)
        at clojure.lang.Var.invoke(Var.java:383)
        at clojure.lang.AFn.applyToHelper(AFn.java:156)
        at clojure.lang.Var.applyTo(Var.java:700)
        at clojure.main.main(main.java:37)
{code}

and another stack trace (a bit different)

{code}
classname: backtype.storm.messaging.netty-unit-test / testname: test-load
Uncaught exception, not in assertion.
expected: nil
  actual: java.lang.AssertionError: Test timed out (5000ms) (empty? (.getLoad client [(int 1) (int 2)]))
 at backtype.storm.messaging.netty_unit_test$test_load_fn.invoke (netty_unit_test.clj:115)
    backtype.storm.messaging.netty_unit_test/fn (netty_unit_test.clj:144)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900$fn__10901.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval10893.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
      at: test_runner.clj:105
-------------------- system-out --------------------
35043 [main] INFO  b.s.m.netty-unit-test - 2 test load
35044 [main] INFO  b.s.m.TransportFactory - Storm peer transport plugin:backtype.storm.messaging.netty.Context
35049 [main] INFO  b.s.m.n.Server - Create Netty Server Netty-server-localhost-6700, buffer_size: 1024, maxWorkers: 1
35050 [main] INFO  b.s.m.n.Client - creating Netty Client, connecting to localhost:6700, bufferSize: 1024
35051 [main] INFO  b.s.m.netty-unit-test - Waiting until all Netty connections are ready...
35173 [main] INFO  b.s.m.netty-unit-test - All Netty connections are ready
35188 [main] INFO  b.s.m.n.Client - closing Netty Client Netty-Client-localhost/127.0.0.1:6700
35188 [main] INFO  b.s.m.n.Client - waiting up to 600000 ms to send 0 pending messages to Netty-Client-localhost/127.0.0.1:6700
35192 [main] INFO  b.s.m.netty-unit-test - 2 test load
35192 [main] INFO  b.s.m.TransportFactory - Storm peer transport plugin:backtype.storm.messaging.netty.Context
35213 [main] INFO  b.s.m.n.Server - Create Netty Server Netty-server-localhost-6700, buffer_size: 1024, maxWorkers: 1
35213 [main] INFO  b.s.m.n.Client - creating Netty Client, connecting to localhost:6700, bufferSize: 1024
35214 [main] INFO  b.s.m.netty-unit-test - Waiting until all Netty connections are ready...
35330 [main] INFO  b.s.m.netty-unit-test - All Netty connections are ready
35344 [client-worker-1] INFO  b.s.m.n.StormClientHandler - Connection to localhost/127.0.0.1:6700 failed:
java.lang.ClassCastException: java.util.ArrayList cannot be cast to backtype.storm.messaging.netty.SaslMessageToken
	at backtype.storm.messaging.netty.SaslStormClientHandler.messageReceived(SaslStormClientHandler.java:115) ~[classes/:?]
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[netty-3.9.0.Final.jar:?]
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[netty-3.9.0.Final.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_31]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_31]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_31]
40352 [main] INFO  b.s.m.netty-unit-test - Condition (empty? (.getLoad client [(int 1) (int 2)])) not met in 5000ms
40352 [main] INFO  b.s.m.netty-unit-test - ""pool-303-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:483)
        at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:392)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at java.lang.Thread.run(Thread.java:745)
""client-schedule-service-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:483)
        at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:392)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at java.lang.Thread.run(Thread.java:745)
""Netty-server-localhost-6700-boss-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""Netty-server-localhost-6700-worker-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""client-boss-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""client-worker-1"" 
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-297-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""Thread-169"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""pool-254-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""Thread-143"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""pool-211-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""pool-168-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-167-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""Thread-75"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""pool-117-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-8"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-7"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-6"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-5"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-4"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-3"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-2"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""pool-116-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""reset-log-levels-timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.timer$mk_timer$fn__4208$fn__4209.invoke(timer.clj:60)
        at backtype.storm.timer$mk_timer$fn__4208.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
""pool-113-thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""metric/stat timer"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
""disruptor-flush-trigger"" 
   java.lang.Thread.State: WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.util.TimerThread.mainLoop(Timer.java:526)
        at java.util.TimerThread.run(Timer.java:505)
""Thread-19"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-17"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-16"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Thread-14"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""clojure-agent-send-off-pool-0"" 
   java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1066)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
""FileBlobStore cleanup thread"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
""Thread-1"" 
   java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.utils.Time.sleepUntil(Time.java:86)
        at backtype.storm.utils.Time.sleep(Time.java:91)
        at backtype.storm.utils.TimeCacheMap$1.run(TimeCacheMap.java:61)
        at java.lang.Thread.run(Thread.java:745)
""Signal Dispatcher"" 
   java.lang.Thread.State: RUNNABLE
""Finalizer"" 
   java.lang.Thread.State: WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:142)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:158)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
""Reference Handler"" 
   java.lang.Thread.State: WAITING
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157)
""main"" 
   java.lang.Thread.State: RUNNABLE
        at sun.management.ThreadImpl.getThreadInfo1(Native Method)
        at sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:174)
        at backtype.storm.utils.Utils.threadDump(Utils.java:1134)
        at backtype.storm.messaging.netty_unit_test$test_load_fn.invoke(netty_unit_test.clj:115)
        at backtype.storm.messaging.netty_unit_test$fn__938.invoke(netty_unit_test.clj:144)
        at clojure.test$test_var$fn__7670.invoke(test.clj:704)
        at clojure.test$test_var.invoke(test.clj:704)
        at clojure.test$test_vars$fn__7692$fn__7697.invoke(test.clj:722)
        at clojure.test$default_fixture.invoke(test.clj:674)
        at clojure.test$test_vars$fn__7692.invoke(test.clj:722)
        at clojure.test$default_fixture.invoke(test.clj:674)
        at clojure.test$test_vars.invoke(test.clj:718)
        at clojure.test$test_all_vars.invoke(test.clj:728)
        at clojure.test$test_ns.invoke(test.clj:747)
        at clojure.core$map$fn__4553.invoke(core.clj:2624)
        at clojure.lang.LazySeq.sval(LazySeq.java:40)
        at clojure.lang.LazySeq.seq(LazySeq.java:49)
        at clojure.lang.Cons.next(Cons.java:39)
        at clojure.lang.RT.boundedLength(RT.java:1735)
        at clojure.lang.RestFn.applyTo(RestFn.java:130)
        at clojure.core$apply.invoke(core.clj:632)
        at clojure.test$run_tests.doInvoke(test.clj:762)
        at clojure.lang.RestFn.invoke(RestFn.java:408)
        at org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900$fn__10901.invoke(test_runner.clj:107)
        at org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899$fn__10900.invoke(test_runner.clj:53)
        at org.apache.storm.testrunner$eval10893$iter__10894__10898$fn__10899.invoke(test_runner.clj:52)
        at clojure.lang.LazySeq.sval(LazySeq.java:40)
        at clojure.lang.LazySeq.seq(LazySeq.java:49)
        at clojure.lang.RT.seq(RT.java:507)
        at clojure.core$seq__4128.invoke(core.clj:137)
        at clojure.core$dorun.invoke(core.clj:3009)
        at org.apache.storm.testrunner$eval10893.invoke(test_runner.clj:52)
        at clojure.lang.Compiler.eval(Compiler.java:6782)
        at clojure.lang.Compiler.load(Compiler.java:7227)
        at clojure.lang.Compiler.loadFile(Compiler.java:7165)
        at clojure.main$load_script.invoke(main.clj:275)
        at clojure.main$script_opt.invoke(main.clj:337)
        at clojure.main$main.doInvoke(main.clj:421)
        at clojure.lang.RestFn.invoke(RestFn.java:421)
        at clojure.lang.Var.invoke(Var.java:383)
        at clojure.lang.AFn.applyToHelper(AFn.java:156)
        at clojure.lang.Var.applyTo(Var.java:700)
        at clojure.main.main(main.java:37)
{code}"
STORM-1433,StormSQL Phase II,This epic tracks the effort of the phase II development of StormSQL.
STORM-1431,Still hasn't start !! ,"i got in the supervisor log file 
b.s.d.supervisor [INFO] 796a6c02-e540-4906-be38-3eefeab5094a still hasn't started
How can i know Why worker still hasn't start ? 

and someone told me that this format wrong it should be like 
Worker with ID 82d26d0c-e6c0-4589-bb5a-49c76d51cb06 still hasn't started"" and not ""82d26d0c-e6c0-4589-bb5a-49c76d51cb06 still hasn't started

How can i fix it ?
"
STORM-1428,Nimbus fails to start after adding consumer metrics,"Setup
* Storm v0.10.0 and 0.9.2-incubating
* Setup nimbus and supervisor on the same node
* Storm configuration -- https://gist.github.com/thedebugger/a9b54de54ed38ae00cd4#file-storm-yaml

Steps to reproduce
1) Start nimbus without Logging consumer metrics
2) Start supervisor on the 
3) Submit ExclamationToplogy in the cluster mode
4) Stop nimbus
5) Add Logging consumer metric in storm.yaml
5) Start nimbus. It doesn't start. 
6) Check the error in the logs -- https://gist.github.com/thedebugger/a9b54de54ed38ae00cd4

After looking at the code, I figured out that Nimbus and metric consumer works fine when there is no storm cluster state i.e. if it is enabled in the first run.

Let me know if more information is required."
STORM-1421,Couldn't extract resources in the supervisor ,"i found in the supervisor log file that couldn't extract resources from /supervisor/tmp/7c3f723c-6b7f-4919-a01c-5a87b872dda7/stormjar.jar
then got still hasn't start 

please tell me which document or information i can share it to help in fixing 

i searched more to fix it but couldn't and some people told me that it's a bug in storm 

i hope i can help here to fix it fastly beacuse i'm on this error months and my project stopped until i fix this error 


i'm using storm-0.8.2 
i ran this topology in local mode sucessfully but when i tried to submit it i got above error 
i'm trying now to submit it in real cluster 
so i have 2 laptops one of them 1 ""server"" and other 2 i installed storm and zookeeper in both laptop and installed ssh in server laptop 

i started to connect 2 from server and start zookeeper then ran nimbus in server 1 but supervisor as i read i ran it in both but still error and i tried to ran supervisor in 2  only got same error 

this is my configuration of zookeeper in both lap 

tickTime=2000
dataDir=/var/zookeeper
clientPort=2181
initLimit=5
syncLimit=2

and this is for storm.yaml 
storm.zookeeper.servers:
    - ""ip address if laptop 2 ""

nimbus.host: ""ip address of laptop 1""

storm.zookeeper.port: 2181

storm.local.dir: ""/var/storms""

java.library.path: ""/usr/lib/jvm/java-6-openjdk-amd64:/usr/local/lib""

nimbus.thrift.port: 6627

supervisor.childopts: ""-Djava.net.preferIPv4Stack=true""

nimbus.childopts: ""-Xmx1024m -Djava.net.preferIPv4Stack=true""

worker.childopts: ""-Xmx768m -Djava.net.preferIPv4Stack=true""

ui.childopts: ""-Xmx768m -Djava.net.preferIPv4Stack=true""

supervisor.slots.ports:
    
    - 6700
    - 6701
    - 6702
    - 6703

/etc/hosts 
# 127.0.0.1   localhost
my ip address   user-Lenovo
#127.0.1.1    localhost2


"
STORM-1410,Add tests to cover multilang functionality,"STORM-1401 proposed the removal of multilang-test, which had been causing problems with the travis-ci builds.

These tests launched topologies, waited, and stopped them.  A pass was the absence of a crash.  If they crashed, they stopped the tests and produced a ton of topology.debug ouput that was not helpful.

We should write new unit tests and/or integration tests that cover JavaScript, Ruby, and Python that are in storm-multilang, and we should also cover the Shell* pieces that are in storm-core."
STORM-1392,Storm Cassandra Test Timeouts,"Noticed the following error in one of the travis-ci test runs.  If it makes sense, we should adjust the test timeout so that this does not fail as often.

In org.apache.storm.cassandra.DynamicStatementBuilderTest

{noformat}
java.lang.AssertionError: Cassandra daemon did not start within timeout
{noformat}


This is annoying because test for unrelated changes can fail, causing confusion."
STORM-1391,Logviewer doesn't close the file stream after reading metafile,"File handle is not released after reading metafile, even the metafile has been deleted by log-cleaner"
STORM-1384,util [INFO] Halting process,"i'm using storm-0.8.2 with zookeeper-3.4.7 
i submitted topology but i got in the supervisor log file 
still hasn't start 
and in the worker log file 
[ERROR] Async loop died!
org.zeromq.ZMQException: Address already in use(0x62)
	at org.zeromq.ZMQ$Socket.bind(Native Method)
	at zilch.mq$bind.invoke(mq.clj:69)
	at backtype.storm.messaging.zmq.ZMQContext.bind(zmq.clj:57)
	at backtype.storm.messaging.loader$launch_receive_thread_BANG_$fn__1629.invoke(loader.clj:26)
	at backtype.storm.util$async_loop$fn__465.invoke(util.clj:375)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:701)
2015-12-10 00:43:01 util [INFO] Halting process: 

i tried to use another version like storm-0.9.5 but got in the worker that it cannot bind port 6703 

how can i fix it ? "
STORM-1382,Netty Client connection failure error message is too alarming,"The error message printed when a netty-client cannot connect to another worker is worded in a way that our users are interpreting as a failure with storm.

There are times, such as at topology launch when such messages are normal as not all of the workers have been launched on all of the supervisors yet.

Other times, it is indicative of a failure (uncaught exception, OOM) on another worker, but the end user believes that this client is failing, due to the error message.

eg:

{noformat}
2015-12-03 12:28:53.338 b.s.m.n.Client [ERROR] connection attempt 10 to Netty-Client-host1.grid.myco.com/10.1.2.3:6710 failed: java.net.ConnectException: Connection refused: host1.grid.myco.com/10.1.2.3:6710
{noformat}

We should change the message to be more informative to our end users as to what happened, and it should not be an ERROR, but a Warning, as there are occasions when one would expect to see this."
STORM-1380,Automatic Tuple Compression for Worker / Worker connections,"We have seen issues with topologies that are resource constrained by network capacity much more than cpu constrained.  They are sending large numbers of large tuples worker to worker and hitting bandwidth limitations inter-node, intra-rack, and inter-rack.

In situations like this, it would be useful to have an option for a topology to automatically compress any data leaving a worker, and automatically decompress it as it arrives based on a topology setting."
STORM-1368,"For secure cluster, heapdump file lacks of group read permissions for UI download","In Secure storm, Jstack, gc and other log files have read permission for group. However, heapdump generated from OOM or User Dynamic Profiling has no read permissions for group user because JVM hard-coded it in this way. 

HTTP ERROR: 500
Problem accessing /download/Penguin-151202-234121-19-1449099695%2F6701%2Frecording-28103-20151203-184734.bin. Reason:
Server Error


We need to fix it to enable user to download heapdump from UI.

-rw------- 1 zhuol  gstorm 3664982 Dec  3 19:37 /home/y/var/storm/workers-artifacts/wc2-38-1449171210/6702/java_pid24691.hprof
-rw-r----- 1 zhuol  gstorm    7597 Dec  3 19:37 /home/y/var/storm/workers-artifacts/wc2-38-14491712 


{code}
4373 // create binary file, rewriting existing file if required
 4374 int os::create_binary_file(const char* path, bool rewrite_existing) {
 4375   int oflags = O_WRONLY | O_CREAT;
 4376   if (!rewrite_existing) {
 4377     oflags |= O_EXCL;
 4378   }
 4379   return ::open64(path, oflags, S_IREAD | S_IWRITE);
 4380 }
{code}"
STORM-1367,Issue with OpaqueTridentKafkaSpout - TridentKafkaConfig getting (java.lang.RuntimeException: kafka.common.OffsetOutOfRangeException),"I'm using trident topology with OpaqueTridentKafkaSpout.

Code snippet of TridentKafkaConfig i’m using :-

OpaqueTridentKafkaSpout kafkaSpout = null;
TridentKafkaConfig spoutConfig = new TridentKafkaConfig(new ZkHosts(""xxx.x.x.9:2181,xxx.x.x.1:2181,xxx.x.x.2:2181""), ""topic_name"");
			spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
			spoutConfig.forceFromStart = true;
                        spoutConfig.fetchSizeBytes = 147483600;
			kafkaSpout = new OpaqueTridentKafkaSpout(spoutConfig);

I get this runtime exception from one of the workers :-

java.lang.RuntimeException: storm.kafka.UpdateOffsetException at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:745) Caused by: storm.kafka.UpdateOffsetException at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:186) at storm.kafka.trident.TridentKafkaEmitter.fetchMessages(TridentKafkaEmitter.java:132) at storm.kafka.trident.TridentKafkaEmitter.doEmitNewPartitionBatch(TridentKafkaEmitter.java:113) at storm.kafka.trident.TridentKafkaEmitter.failFastEmitNewPartitionBatch(TridentKafkaEmitter.java:72) at storm.kafka.trident.TridentKafkaEmitter.emitNewPartitionBatch(TridentKafkaEmitter.java:79) at storm.kafka.trident.TridentKafkaEmitter.access$000(TridentKafkaEmitter.java:46) at storm.kafka.trident.TridentKafkaEmitter$1.emitPartitionBatch(TridentKafkaEmitter.java:204) at storm.kafka.trident.TridentKafkaEmitter$1.emitPartitionBatch(TridentKafkaEmitter.java:194) at storm.trident.spout.OpaquePartitionedTridentSpoutExecutor$Emitter.emitBatch(OpaquePartitionedTridentSpoutExecutor.java:127) at storm.trident.spout.TridentSpoutExecutor.execute(TridentSpoutExecutor.java:82) at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:370) at backtype.storm.daemon.executor$fn__5694$tuple_action_fn__5696.invoke(executor.clj:690) at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:436) at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:127) ... 6 more

But when i set spoutConfig.forceFromStart = true... It works fine for a while and then fails throwing this exception. I need a trident topology to give out accurate exactly-once processing even when the topology is restarted. 

As per some posts, I have tried setting spoutConfig :-

spoutConfig.maxOffsetBehind = Long.MAX_VALUE;

spoutConfig.startOffsetTime = kafka.api.OffsetRequest.EarliestTime();

My Kafka retention time is default - 128 Hours i.e. 7 Days and kafka producer is sending 6800 messages/second to Storm/Trident topology. I have gone through most of the posts, but none of them seem to solve this issue. 
"
STORM-1362,Travis should run at least the multilang-test using Python2 and 3,"Adjust the Travis config to run the backtype.storm.multilang-test clj test against both Python 2.x and 3.x separately.  Perhaps even consider testing against all the feature releases above a minimum 2.x version (e.g. 2.6, 2.7, ... 3.4, 3.5), assuming that only the multilang-test is what's being executed on each version.

Once this testing scheme establishes which versions are usable, the Storm docs should be updated to reflect it."
STORM-1360,storm.py issues with python 3,"Python 3.x doesn't like the ""print without ()"" and ""except with comma"" syntax."
STORM-1358,Porting JStorm multi-thread mode of spout to Storm,"There are two modes of spout, ""single-thread"" and ""multi-thread"" in JStorm. The ""single-thread"" mode is simliar to Storm while the ""multi-thread"" mode separates the processing of ack/fail and nextTuple to two threads. It means we can stay in nextTuple for a long time without any side effect on ack/fail. 
Let's think about an example of kafka spout. We can initiate a consumer thread for kafka when initialization of spout. Then the comsumer starts to pull events from kafka and pulish the retreived events into a local queue. At meantime, nextTuple waits to read at this queue. If any available events, nextTuple will get notification faster and flush them to downstream. This model could probably introduce better performance compared with ""single-thread"" mode.

For this mode, the max pending configuration of spout might not be useful as expectation. It depends on how long we stay in nextTuple. But backpressure is a good choice to resolve flow control problem."
STORM-1357,Support writing to Kafka streams in Storm SQL,This jira proposes to add supports to write SQL results to Kafka streams.
STORM-1356,StormSQL Explain Execute Plan ,StormSQL  maybe need a explain method use to show SQL's topology component structure  looks like MySQL and any other RDBMS .  
STORM-1355,Storm Kafka Sport Can't Emit Message,Using Kafka-Spout reading a topic after some time spout can't read data any more 
STORM-1351,Storm spouts and bolts need a way to communicate problems back to toplogy runner,"A spout can be having a problem generating a  tuple in nextTuple()  because 
 -a) there is no data to generate currently 
 - b) there is some I/O  issues it is experiencing

If the spout returns immediately from the nextTuple() call then the nextTuple() will be invoked immediately leading to CPU spike. The CPU spike would last till the situation is remedied by new coming data or the i/o issues getting resolved.

Currently to work around this, the spouts will have to implement a exponential backoff mechanism internally. There are two problems with this:
 - each spout needs to implement this backoff logic
 - since nextTuple() has an internal sleep and takes longer to return, the latency metrics computation gets thrown off


*Thoughts for Solution:*
The spout should be able to indicate a 'no data',  'experiencing error' or 'all good' status back to the caller of nextTuple() so that the right backoff logic can kick in.

- The most natural way to do this is using the return type of the nextTuple method. Currently nextTuple() returns void.  However, this will break source and binary compat since the new storm will not be able to invoke the methods on the unmodified spouts. This breaking change can only be considered as an option only prior to v1.0. 

- Alternatively this can be done by providing an additional method on the collector to indicate the condition to the topology runner. The spout can invoke this explicitly. the metrics can then also account for 'no data' and 'error attempts'

- Alternatively - The toplogy  runner may just examine the collector if there was new data generated by the nextTuple() call. In this case it cannot distinguish between errors v/s no incoming data. "
STORM-1350,Unable to build: Too many files with unapproved licenses (RAT exception),"Issue when making build from the master branch. 

[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project storm: Too many files with unapproved license: 7 See RAT report in: /Users/myusername/Documents/apache-storm/target/rat.txt -> [Help 1]

Simple workaround is to build with maven flag to skip rat check:
mvn clean install -Drat.skip=true"
STORM-1346,upgrade topology CLI tool ,
STORM-1345,"Thrift, nimbus ,zookeeper, supervisor and worker changes to support update topology.",
STORM-1343,Add a matrices for Trident which actually counts number of messages processed.,"If we keep trident topology running without pumping any messages in Kafka. Storm UI still show increased count. After some investigation we found one blog mentioning that its not actual processed message count  https://github.com/miguno/kafka-storm-starter/issues/5

As user, its very confusing what is this count and some time it gets misinterpreted. We have seen core storm is showing the count correctly. As trident is abstract level of core storm can't we use those matrices? 

We also found a blog where user can manually add the matrices to his code which will come up to the Storm UI. http://www.bigdata-cookbook.com/post/72320512609/storm-metrics-how-to 

Can we implement processed tuple matrices in trident directly? That will helful to end user in understanding what is topology doing & what needs to done to increase EPS of particular component. "
STORM-1342,support multiple logviewers per host for container-isolated worker logs,"h3. Storm-on-Mesos Worker Logs are in varying directories
When using [storm-on-mesos|https://github.com/mesos/storm] with cgroups, each topology's workers are isolated into separate containers.  By default the worker logs will be saved into container-specific sandbox directories.  These directories are also topology-specific by definition, because, as just stated, the containers are specific to each topology.

h3. Problem: Storm supports 1-and-only-1 Logviewer per Worker Host
A challenge with this different way of running Storm is that the [Storm logviewer|https://github.com/apache/storm/blob/768a85926373355c15cc139fd86268916abc6850/docs/_posts/2013-12-08-storm090-released.md#log-viewer-ui] runs as a single instance on each worker host.   This doesn't play well with having the topology worker logs in separate per-topology containers.  The one logviewer doesn't know about the various sandbox directories that the Storm Workers are writing to.  And if we just spawned new logviewers for each container, the problem is that the Storm UI only knows about 1 global port the logviewer, so you cannot just direct.

These problems are documented (or linked to) from [Issue #6 in the storm-on-mesos project|https://github.com/mesos/storm/issues/6]

h3. Possible Solutions I can envision
# configure the Storm workers to write to log directories that exist on the raw host outside of the container sandbox, and run a single logviewer on a host, which serves up the contents of that directory.
#* violates one of the basic reasons for using containers: isolation.
#* also prevents a standard use case for Mesos: running more than 1 instance of a Mesos Framework (e.g., ""Storm Cluster"") at once on same Mesos Cluster. e.g., for Blue-Green deployments.
#* a variation on this proposal is to somehow expose the sandbox dirs of all storm containers to this singleton logviewer process (still has above problems)
# launch a separate logviewer in each container, and somehow register those logviewers with Storm such that Storm knows for a given host which logviewer port is assigned to a given topology.
#* this is the proposed solution
# launch a separate logviewer in a separate container, somehow exposing the logs from these other containers to the host-global logviewer.

h3. Storm Changes for the Proposed Solution

Nimbus or ZooKeeper could serve as a registrar, recording the association between a slot (host + worker port) and the logviewer port that is serving the workers logs. And the Storm-on-Mesos framework could update this registry when launching a new worker.  (This proposal definitely calls for thorough vetting and thinking.)

h3. Storm-on-Mesos Framework Changes for the Proposed Solution

Along with the interaction with the ""registrar"" proposed above, the storm-on-mesos framework can be enhanced to launch multiple logviewers on a given worker host, where each logviewer is dedicated to serving the worker logs from a specific topology's container/sandbox directory.  This would be done by launching a logviewer process within the topology's container, and assigning it an arbitrary listening port that has been determined dynamically through mesos (which treats ports as one of the schedulable resource primitives of a worker host).  [Code implementing this logviewer-port-allocation logic already exists|https://github.com/mesos/storm/commit/af8c49beac04b530c33c1401c829caaa8e368a35], but [that specific portion of the code was reverted|https://github.com/mesos/storm/commit/dc3eee0f0e9c06f6da7b2fe697a8e4fc05b5227e] because of the issues that inspired this ticket."
STORM-1340,Use Travis-CI build matrix to improve test execution times,"Travis-CI provides a 'build-matrix' that we can use to parallelize the unit tests of various components in storm.
It also provides a caching mechanism that can reduce the overhead of starting up a test container."
STORM-1339,Evaluate/Port JStorm UI Elements,"The JStorm UI has some nice features, especially graphing in the newest version (Not in our repo)

I would be nice to evaluate not just their features, but get a real UI designer to evaluate the user experience and make something that is useful and intuitive for our users."
STORM-1338,Evaluate/Port JStorm worker classloader,"Need to see if we want to continue with shading for classpath isolation, or do we want to adopt the JStorm worker classloader. "
STORM-1337,Evalute/port Jstorm logging,"    Supports user-defined configuration of log
    Supports both logback and log4j

Need to evaluate differences and see what we want long term.  Also don't want to break dynamic log reconfiguration."
STORM-1335,Evaluate/port JStorm update_topolgy command,"Update jars and configuration dynamically for a topology, without stopping the topology. (How is this different from restart command?) 

This should be evaluated with how this will impact the blob-store feature."
STORM-1334,Evaluate/Port JStorm restart command,"Restart a topology. Besides restart, this command can also update the topology configuration."
STORM-1333,Evaluate/port Jstorm metricsMonitor command,"Allows toggling on/off some metrics which may impact topology performance

Evaluate the best way to work this into the current storm code.  We may need to do something with TopologyMaster to make it work well."
STORM-1332,Evaluate/port jstorm zktool,"JStorm:
Supports some ZooKeeper operations, e.g. ""list"", ""read""…

Will need to be evaluated for security.  Perhaps we can combine this with the heartbeats command to provide a way to explore stored state in a safe debuggable way."
STORM-1331,evalute/port jstorm list command,"JStorm:
List information of all topologies, all supervisors, and JStorm version

More info is good, but we want it human readable too. Possibly suing multiple commands instead of just one.  Perhaps with a machine readable option too."
STORM-1330,Evaluate Port JStorm advanced Rebalancing,"JStorm:
Besides rebalance, scale-out/in by updating the number of workers, ackers, spouts & bolts dynamically without stopping topology. Routing is updated dynamically within upstream components.

dynamic routing with some groupings is difficult to get right when there is state, we need to be sure this is well documented, and might want to disallow it for some groupings without a force flag."
STORM-1329,Evaluate/Port JStorm metrics system,"Storm:
IMetric and IMetricConsumer

JStorm:
    All levels of metrics, including stream metrics, task metrics, component metrics, topology metrics, even cluster metrics, are sampled & calculated. Some metrics, e.g. """"tuple life cycle"""", are very useful for debugging and finding the hotspots of a topology.
    Support full metrics data. Previous metric system can only display mean value of meters/histograms, the new metric system can display m1, m5, m15 of meters, and common percentiles of histograms.
    Use new metrics windows, the mininum metric window is 1 minute, thus we can see the metrics data every single minute.
    Supplies a metric uploader interface, third-party companies can easily build their own metric systems based on the historic metric data.

Ideally we should have a way to display most/system metrics in the UI.  IMetric is too generic to make this happen, but we cannot completely drop support for it.  But perhaps we need to depricate it if the JStorm metrics are much better."
STORM-1328,Evaluate/Port JStorm tuple groupings,"Storm:
Load aware balancing in shuffle grouping

JStorm:
    Has a ""localfirst"" grouping that causes tuples to be sent to the tasks in the same worker by default. But if the load of all local tasks is high, the tuples will be sent out to remote tasks.
    Improve localOrShuffle grouping from Storm. In Storm's localOrShuffle grouping the definition of ""local"" is local within the same worker process. i.e., if there is a bolt that the component can send to in the current worker process it will send the tuples there. If there is not one, it will do round robin between all of the instances of that bolt no matter which hosts they are on. JStorm has extended that so that other workers/JVMs on the same host are considered ""local"" as well, taking into account the load of the network connections on the local worker.

We should look at combining both of these to have shuffle look at both distance and load to decide where to send a tuple, in addition the the new JStorm localfirst grouping."
STORM-1327,Evaluate/Port JStorm tuple batching,"Storm:
Batching in DisruptorQueue

JStorm:
Do batch before sending tuple to transfer queue and support for adjusting the batch size dynamically according to samples of actual batch size sent out for past intervals.

Should evaluate both implementations, and see which is better for performance, and possible if we can/should move some of the dynamic batching logic into disruptor."
STORM-1326,Evaluate/port JStorm message processing,"Storm:
Deserialization happens on the netty thread

Serialization happens after the send queue when batching is happening.

JStorm:
    Add receiving and transferring queue/thread for each task to make deserialization and serialization asynchronously
    Remove receiving and transferring thread on worker level to avoid unnecessary locks and to shorten the message processing phase

The two sound equivalent now, but we should talk to see if there are other optimizations needed."
STORM-1325,Port jstorm execution thread monitoring to storm,"Monitors the status of the execute thread of tasks. It is effective to find the slow bolt in a topology, and potentially uncovers deadlock as well."
STORM-1324,Evaluate JStorm backpressure implementaion,"Both storm and jstorm support automatic backpressure.

The two sound fairly equivalent, but we should talk to see if there are other optimizations needed, especially support for adjusting the backpressure while the topology is live which jstorm currently supports."
STORM-1323,Explore support for JStorm TopologyMaster,"JStorm uses a TopologyMaster to reduce the load on zookeeper and to provide coordination within the topology.

Need to evaluate how this impacts storm architecture especially around security, and port it over, if it makes since."
STORM-1322,Eveluate adopting JStorm TopologyStructure,"JStorm has merged executors and tasks.

We need to evaluate if adopting their structure will add enough benefit to developers/performance that we can drop it.  Probably need resource aware re-balancing or Jstorm rebalancing that can support changing parallelism before this can happen."
STORM-1321,Evaluate/Port JStorm nimbus HA,The JStorm nimbus HA has been in production longer then the storm nimbus HA has.  we should evaluate the pros and cons of switching all or in part to the JStorm version.
STORM-1320,Port JStorm scheduler to storm,Port the JStorm scheduler to storm core.
STORM-1319,Port JStorm features to storm,This is a rollup jira for tracking porting jstorm features to storm.  This should not be a blind port.  We should evaluate each feature and compare it to what storm already has to see what parts of the feature should be ported over.
STORM-1317,port storm.trident.tuple-test to java,test trident tuple
STORM-1316,port storm.trident.state-test to java,Test some of trident state
STORM-1315,port storm.trident.integration-test to java,Run trident topolgies
STORM-1314,port storm.trident.testing to java,helper functions for testing trident
STORM-1313,port backtype.storm.versioned-store-test to java,Test VersionedStore
STORM-1310,port backtype.storm.tuple-test to java,
STORM-1309,port  backtype.storm.transactional-test to java,Test transactional topologies
STORM-1305,port backtype.storm.supervisor-test to java,Test the supervisor
STORM-1304,port  backtype.storm.submitter-test to java,Test ZookeeperAuthentication payload generation that is a part of StormSubmitter
STORM-1303,port  backtype.storm.security.auth.nimbus-auth-test to java,Test Nimbus Authorization
STORM-1302,port  backtype.storm.security.auth.drpc-auth-test to java,Test DRPC Authorization
STORM-1301,port  backtype.storm.security.auth.auth-test to java,Test Authentication and Authorization
STORM-1299,port  backtype.storm.scheduler.multitenant-scheduler-test to java ,Test Multi-tenant scheduler
STORM-1298,port  backtype.storm.scheduler-test to java,Test even scheduler
STORM-1297,port  backtype.storm.nimbus-test to java,test that nimbus works
STORM-1296,port  backtype.storm.multilang-test to java and move out of storm-core,"Runs a few topolgies using the multi-lang feature.  These should be moved to be a part of the multi-lang jars, and fixed to actually test something."
STORM-1295,port  backtype.storm.metrics-test to java,Test for the metrics system
STORM-1293,port  backtype.storm.messaging.netty-integration-test to java,Integration tests for netty messaging layer
STORM-1288,port  backtype.storm.grouping-test to java,Test groupings
STORM-1287,port backtype.storm.drpc-test to java,Test DPRC
STORM-1284,port backtype.storm.cluster-test to java,Tests for backtype.storm.cluster
STORM-1243,port backtype.storm.command.healthcheck to java,
STORM-1241,port  backtype.storm.security.auth.auto-login-module-test to java,junit migration
STORM-1225,storm-core java migration,"This is a rollup ticket for tracking the migration of the storm-core codebase from being prominently clojure based to being mostly java based as a step to ease the jstorm merger.

All JIRA that are a part of this will be labeled with jstorm-merger and java-migration to make queries simpler.

Dependencies should also be tracked as much as possible."
STORM-1224,ShellBolt only increases memory usage when subprocess doesn't send ack / fail,"In order to handle ack / fail easily, ShellBolt stores generated id and tuple to Map, and remove it when subprocess passes ack or fail to that id.

In other word, if users doesn't let subprocess pass ack or fail, _input Map only increases and never decreases. It would make memory issue when each tuple is big or there're a lot of tuples flowing."
STORM-1222,Support Kafka as external tables in StormSQL,This jira propose to support Kafka as both the data ingress and egress point in StormSQL.
STORM-1221,Create a common interface for all Trident spout,"Currently {{IBatchSpout}}, {{IOpaquePartitionSpout}}}, {{ITridentSpout}} and {{IPartitionedSpout}} are all top-level interfaces. The lowest common ancestor of their parent interface is {{Serializable}}. There is no clear, single interface in Trident to mark these class are spouts / data sources. The abstraction is beneficial when components on top of Trident (e.g. StormSQL) need a notion of data source.

This jira proposes to tags all the spouts above with a common marker interface."
STORM-1220,Avoid double copying in the Kafka spout,"Currently the kafka spout takes a {{ByteBuffer}} from Kafka. However, the serialization scheme takes a {{byte[]}} array as input. Therefore the current implementation copies the {{ByteBuffer}} to a new {{byte[]}} array in order to hook everything together.

This jira proposes to changes the interfaces of serialization scheme to avoid copying the data twice in the spout."
STORM-1216,button to kill all topologies in Storm UI,"In the Storm-on-Mesos project we had a [request to have an ability to ""shut down the storm cluster"" via a UI button|https://github.com/mesos/storm/issues/46].   That could be accomplished via a button in the Storm UI to kill all of the topologies.

I understand if this is viewed as an undesirable feature, but I just wanted to document the request."
STORM-1214,Trident API Improvements,"There are a few idiosyncrasies in the Trident API that can sometimes trip developers up (e.g. when and how to set the parallelism of components). There are also a few areas where the API could be made slightly more intuitive (e.g. add Java 8 streams-like methods like {{filter()}}, {{map()}}, {{flatMap()}}, etc.).

Some of these concerns can be addressed through documentation, and some by altering the API. Since we are approaching a 1.0 release, it would be good to address any API changes before a major release.

The goal of this JIRA is to identify specific areas of improvement and formulate an implementation that addresses them."
STORM-1212,Add synchronous invocation support for Cassandra connector.,
STORM-1209,Generalize StringMultiSchemeWithTopic,"STORM-817 allows a KafkaSpout to listen to multiple topics.

In this scenario, it makes sense to append the kafka topic to the tuples sent to storm.

This is partially implemented as `StringMultiSchemeWithTopic`. The idea is great, but not flexible enough, since it only works for Strings.

This should be generalized to work with any Scheme.

My proposed solution is available on https://github.com/apache/storm/pull/883"
STORM-1205,Make nimbus independent of sync thread to download blobs in nimbus ha and make the blobstore handle it,"The nimbus handles the syncing of blobs logic. It will be nice in to move the sync thread in nimbus for ha from nimbus to blobstore, making the blobstore truly independent entity"
STORM-1201,Support distributed deployment in StormSQL,"StormSQL compiles the SQL statements into Java classes. The Trident topology executes these classes in order to execute the SQL statements.

These classes need to be properly distributed through Nimbus so that the topology can be run in distributed mode."
STORM-1200,Support collations of primary keys,This jira proposes to add support of specifying collations of primary keys. Collations provide information on uniqueness and monotonicity of columns. The information is essential to implement aggregation functions over streaming data.
STORM-1197,Migrate Travis-CI to container-based builds.,http://docs.travis-ci.com/user/migrating-from-legacy/
STORM-1194,java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.Log4jLoggerFactory,"can not consume topic from Kafka using storm-kafka, failed with 

java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.Log4jLoggerFactory
	at org.apache.log4j.Logger.getLogger(Logger.java:39) ~[log4j-over-slf4j-1.6.6.jar:1.6.6]
	at kafka.utils.Logging$class.logger(Logging.scala:24) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger$lzycompute(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.utils.Logging$class.debug(Logging.scala:51) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.debug(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:64) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:44) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOrMakeConnection(SimpleConsumer.scala:142) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:69) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:124) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[kafka_2.10-0.8.1.1.jar:na]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:77) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:67) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.PartitionManager.<init>(PartitionManager.java:83) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) ~[storm-kafka-0.9.5.jar:0.9.5]
	at backtype.storm.daemon.executor$fn__3371$fn__3386$fn__3415.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.util$async_loop$fn__460.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
67167 [Thread-20-spout] ERROR backtype.storm.daemon.executor - 
java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.Log4jLoggerFactory
	at org.apache.log4j.Logger.getLogger(Logger.java:39) ~[log4j-over-slf4j-1.6.6.jar:1.6.6]
	at kafka.utils.Logging$class.logger(Logging.scala:24) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger$lzycompute(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.utils.Logging$class.debug(Logging.scala:51) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.debug(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:64) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:44) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOrMakeConnection(SimpleConsumer.scala:142) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:69) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:124) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[kafka_2.10-0.8.1.1.jar:na]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:77) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:67) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.PartitionManager.<init>(PartitionManager.java:83) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) ~[storm-kafka-0.9.5.jar:0.9.5]
	at backtype.storm.daemon.executor$fn__3371$fn__3386$fn__3415.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.util$async_loop$fn__460.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
67235 [Thread-20-spout] ERROR backtype.storm.util - Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
	at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$fn__4694$fn__4695.invoke(worker.clj:493) [storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.daemon.executor$mk_executor_data$fn__3272$fn__3273.invoke(executor.clj:240) [storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.util$async_loop$fn__460.invoke(util.clj:473) [storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]



Here's the pom.xml

ack=true</test.extra.args>

    <!-- dependency versions -->
    <clojure.version>1.5.1</clojure.version>
    <compojure.version>1.1.3</compojure.version>
    <hiccup.version>0.3.6</hiccup.version>
    <commons-io.version>2.4</commons-io.version>
    <commons-lang.version>2.5</commons-lang.version>
    <commons-exec.version>1.1</commons-exec.version>
    <curator.version>2.5.0</curator.version>
    <json-simple.version>1.1</json-simple.version>
    <ring.version>0.3.11</ring.version>
    <clojure.tools.logging.version>0.2.3</clojure.tools.logging.version>
    <clojure.math.numeric-tower.version>0.0.1</clojure.math.numeric-tower.version>
    <carbonite.version>1.4.0</carbonite.version>
    <httpclient.version>4.3.3</httpclient.version>
    <clojure.tools.cli.version>0.2.4</clojure.tools.cli.version>
    <disruptor.version>2.10.1</disruptor.version>
    <jgrapht.version>0.9.0</jgrapht.version>
    <guava.version>16.0.1</guava.version>
    <logback-classic.version>1.0.13</logback-classic.version>
    <mockito.version>1.9.5</mockito.version>
    <storm-core.version>0.9.5</storm-core.version>
    <storm-kafka.version>0.9.5</storm-kafka.version>
    <snakeyaml.version>1.16</snakeyaml.version>
    <storm.crawler.core.version>0.7</storm.crawler.core.version>

  </properties>

  <build>
    <sourceDirectory>src/jvm</sourceDirectory>
    <testSourceDirectory>test/jvm</testSourceDirectory>
    <resources>
      <resource>
        <directory>${basedir}/multilang</directory>
      </resource>
    </resources>

    <plugins>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
      </plugin>

      <!--
        Bind the maven-assembly-plugin to the package phase
        this will create a jar file without the storm dependencies
        suitable for deployment to a cluster.
       -->
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
          <archive>
            <manifest>
              <mainClass />
            </manifest>
          </archive>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.3.2</version>
        <executions>
          <execution>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>java</executable>
          <includeProjectDependencies>true</includeProjectDependencies>
          <includePluginDependencies>false</includePluginDependencies>
          <classpathScope>compile</classpathScope>
          <killAfter>-1</killAfter>
          <mainClass>${storm.topology}</mainClass>
        </configuration>
      </plugin>
    </plugins>
  </build>

  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.testng</groupId>
      <artifactId>testng</artifactId>
      <version>6.8.5</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-all</artifactId>
      <version>${mockito.version}</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.easytesting</groupId>
      <artifactId>fest-assert-core</artifactId>
      <version>2.0M8</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.jmock</groupId>
      <artifactId>jmock</artifactId>
      <version>2.6.0</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.twitter4j</groupId>
      <artifactId>twitter4j-stream</artifactId>
      <version>3.0.3</version>
    </dependency>

    <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka_2.10</artifactId>
      <version>0.8.1.1</version>
      <exclusions>
        <exclusion>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
        </exclusion>
        <exclusion>
          <groupId>log4j</groupId>
          <artifactId>log4j</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

    <dependency>
      <groupId>org.apache.storm</groupId>
      <artifactId>storm-core</artifactId>
      <version>${storm-core.version}</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.storm</groupId>
      <artifactId>storm-kafka</artifactId>
      <version>${storm-kafka.version}</version>
      <exclusions>
        <exclusion>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
        </exclusion>

        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-simple</artifactId>
        </exclusion>

        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-sl4j12</artifactId>
        </exclusion>

      </exclusions>
    </dependency>

    <dependency>
      <groupId>org.apache.zookeeper</groupId>
      <artifactId>zookeeper</artifactId>
      <version>3.4.6</version>
      <exclusions>
        <exclusion>
          <groupId>com.sun.jmx</groupId>
          <artifactId>jmxri</artifactId>
        </exclusion>

        <exclusion>
          <groupId>com.sun.jdmk</groupId>
          <artifactId>jmxtools</artifactId>
        </exclusion>

        <exclusion>
          <groupId>javax.jms</groupId>
          <artifactId>jms</artifactId>
        </exclusion>

      </exclusions>
    </dependency>

    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.2.1</version>
    </dependency>
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
      <version>${guava.version}</version>
    </dependency>
    <dependency>
      <groupId>org.yaml</groupId>
      <artifactId>snakeyaml</artifactId>
      <version>${snakeyaml.version}</version>
    </dependency>
    <dependency>
      <groupId>com.digitalpebble</groupId>
      <artifactId>storm-crawler-core</artifactId>
      <version>${storm.crawler.core.version}</version>
    </dependency>

    <dependency>
      <groupId>com.netflix.curator</groupId>
      <artifactId>curator-test</artifactId>
      <version>1.2.5</version>

      <exclusions>
        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
        <exclusion>
          <groupId>log4j</groupId>
          <artifactId>log4j</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

  </dependencies>"
STORM-1193,supervisor-test error creating symlinks on windows,"The test-worker-launch-command-run-as-user testcase errors rather than fails, due to what seems to be a privilege issue with creating a symlink.

{quote}
5701 \[main] INFO  b.s.d.supervisor - Creating symlinks for worker-id: fake-worker-id storm-id: fake-storm-id to its port artifacts directory

Uncaught exception, not in assertion.

expected: nil
actual: java.nio.file.FileSystemException: C:\path\to\Temp\storm-local8914443830909286033\workers\fake-worker-id\artifacts: A required privilege is not held by the client.

 at sun.nio.fs.WindowsException.translateToIOException (WindowsException.java:86)
    sun.nio.fs.WindowsException.rethrowAsIOException (WindowsException.java:97)
    sun.nio.fs.WindowsException.rethrowAsIOException (WindowsException.java:102)
    sun.nio.fs.WindowsFileSystemProvider.createSymbolicLink (WindowsFileSystemProvider.java:577)
    java.nio.file.Files.createSymbolicLink (Files.java:994)
    backtype.storm.util$create_symlink_BANG_.invoke (util.clj:604)
    backtype.storm.daemon.supervisor$create_artifacts_link.invoke (supervisor.clj:811)
    backtype.storm.daemon.supervisor/fn (supervisor.clj:898)
    clojure.lang.MultiFn.invoke (MultiFn.java:251)
    backtype.storm.supervisor_test$fn__498$fn__501$fn__502$fn__503.invoke (supervisor_test.clj:472)
    clojure.core$with_redefs_fn.invoke (core.clj:7209)
    backtype.storm.supervisor_test$fn__498$fn__501$fn__502.invoke (supervisor_test.clj:462)
    backtype.storm.supervisor_test$fn__498$fn__501.invoke (supervisor_test.clj:462)
    backtype.storm.supervisor_test/fn (supervisor_test.clj:451)
{quote}

I don't know if the testcase itself should be catching the error and presenting it as a failed test, or if the original code should be catching it and reacting.

What's odd here is that if this is using my own user account and privileges, then it should work, because my account _does_ have permission to create symlinks.

Tested against master branch (c12e28c829)"
STORM-1192,0.11.0-SNAPSHOT ShellBolt not compatible with 0.10.x,"I get java serialization errors if I try to submit a topology from a 0.10.1 client to a 0.11.0-SNAPSHOT cluster that uses a shell bolt (WordCountTopology).

We should look at see what is and is not backwards compatible around the serialization and hopefully if everything is still compatible just hardcode the version.  We should do this for the ShellSpout as well."
STORM-1188,PartialKeyGrouping missing from storm.thrift (and can't use it via custom_object),"I'm working on a Python DSL for Storm to add to streamparse, and as part of it I realized that the new partial key grouping was never added to the Grouping struct in storm.thrift, so it's not usable outside of JVM-based topology definitions (at least not easily).  My initial thought was to just use Grouping.custom_object, but the PartialKeyGrouping constructor takes a Fields object, which isn't a type defined in storm.thrift, so I can't use it.

The fields grouping explicitly takes a list of strings in storm.thrift, so it would seem PartialKeyGrouping needs to be added in the same way."
STORM-1186,Storm System tests,"Framework to spin up storm cluster along with hdfs, hive etc.. to test storm and also storm connectors"
STORM-1184,Update storm.yaml with HA configuration,replace nimbus.host with nimbus.seeds
STORM-1183,Intermittent testInOrder failures seen in Travis builds,"DisruptorQueueTests::testInOrder has some assertions that pay attention to whether or not producer instances shut down quickly.  

(https://travis-ci.org/apache/storm/jobs/89506537)

Since the test case itself focuses the ordering of processing rather than speed, I'd suggest bumping TIMEOUT up from the current 1s to 10s... maybe consistent failures with such a large timeout would be more useful as visible test failures."
STORM-1181,Compile SQLs into Tridient topology and execute them in LocalCluster,This jira tracks the effort of compiling SQL to Trident topology to enable users to run StormSQL at production scale.
STORM-1178,Investigate whether we need to support additional metrics reporter plugins for Storm Metrics,"A set of metrics was added to profile various internal functions in Storm.  However, since we shade our packages, if a user wants to use a reporter plugin to collect the metrics, the plugin might not work correctly.  We need to figure out if we want to support additional metrics reporters since currently using JMX to collect the metrics is supported"
STORM-1177,Build fails because DISCLAIMER file is missing,"Commit #4a57e500b3e0c3f3256e776357d1e1de1c7f5e49 removed the DISCLAIMER file, causing `mvn package` to fail."
STORM-1174,Divide ACL struct into a union of Others and Users struct for Dist Cache,
STORM-1173,Support string operations in StormSQL,This jira tracks the effort of implementing string operations in StormSQL.
STORM-1172,Improve clojure.test handling of stdout/stderr to match JUnit behavior,It would be nice if clojure.test would include stdout/stderr in the test reports only when there is an error or failure.
STORM-1171,UI: number of workers possibly inaccurate after rebalance,"1. Describe observed behavior.

Topology Page shows the number of workers the topology was submitted with, but if it has been rebalanced since this number can be wrong.

2. What is the expected behavior?

Topology page should show current counts, not original counts.

3. Outline the steps to reproduce the problem.

Launch a topology with 3 workers. Rebalance -n 1, load cluster summary page and topology summary page to compare.

Check that the number of executors is correct. It is likely that they are wrong.
"
STORM-1169,Validate file ownership before setting up stormdist dir,"Created for [TODO comment|https://github.com/apache/storm/blob/f3568d73c8832cdf2a6a6ec06b929c8b7bb96c10/storm-core/src/native/worker-launcher/impl/worker-launcher.c#L494] in the code.

"
STORM-1166,Allow Groups in ACL for Dist Cache,
STORM-1159,Support nullable operations in StormSQL,This jira tracks the efforts of supporting nullable types in StormSQL.
STORM-1149,Support pluggable data sources in CREATE TABLE,"This jira proposes to implement supports for external data sources for StormSQL so that StormSQL can process real-world data-set.

The external data sources will be specified through the {{CREATE TABLE}} statement. "
STORM-1139,Issues regarding storm-postgresql interface,"hai 
      I am trying to write storm bolt to insert data in postgesql DB.But i am facing issues like java.io.NotSerializableException:org.postgresql.jdbc4.Jdbc4Connection.
Can anyone provide me full code for storm bolt that can insert data into postgres database."
STORM-1137,loading data into postgresdb,"I am trying to store the streaming tweets coming from storm bolt into the postgresql database. For that i need full coding to do that job. i.e., to store the data coming from the storm bolt to postgresql database and i need pom.xml file too. Can you please help me to solve this issue."
STORM-1135,Support pluggable data sources in CREATE TABLE,This jira proposes to support pluggable data sources in CREATE TABLE so that StormSQL can consume external data.
STORM-1133,loading data into postgresdb,"I have written PsqlBolt to load the tables into postgres. I am able to connect to postgresdb but unable to load the data.
PROBLEM : Want to insert data into those tables through the same bolt.
I have tried ""copy"" command to do it. But it doesn't work.

I want help to insert data into postgres tables by writing the POSTGRESQLBOLT in STORM.

REASON : I want to give STORM-POSTGRESQL connection.

NOTE : I want full code to insert like topology,spout,bolts.

I am running this in SANDBOX i.e., [ root@sandbox~]
Using Linux-Ubuntu as OS (15.04)"
STORM-1132,Loading data into postgresdb through postgresql bolt,
STORM-1131,Kafka Spout can get into a loop of refreshing offset when there are failed messages,"When Kafka spout offset is out of range we update _emittedToOffset
https://github.com/hortonworks/storm/blob/2.3-maint/external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java#L172
and return immediately.
If there are failed messages https://github.com/hortonworks/storm/blob/2.3-maint/external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java#L162
we never update the local offset to _emittedToOffset. 

We should discard failedMessages if there is offsetOutOfexception as there is no point in keeping these messages around. "
STORM-1130,"Support AND, OR and NOT operators in StormSQL","This jira proposes to compile AND, OR, and NOT operators to Java source code."
STORM-1129,Storm should use topology name instead of ids for url in storm UI.,"Currently, in storm UI details about a topology can be viewed at a URL which has a topology id as a query parameter. When a topology is updated and  redeployed a new id is assigned by storm and existing URL(and any bookmarks relying on it) for the topology do not work since the id has changed. We should change it so that topology name is used instead of id."
STORM-1124,'schema' in ui core should be 'scheme',
STORM-1123,TupleImpl - Unnecessary variable initialization,"Within the backtype.storm.tuple.TupleImpl class, the following variables are set to null in the constructor or variable definition.  This is not necessary as they are null by default per the Java language specification.

_processSampleStartTime 
_executeSampleStartTime 
_meta 

Simple optimization."
STORM-1116,"storm-hbase doesn't create a table for you, it would be great to have an option to create a table and optionally pre-split the table by passing a splitting algorithm to it.","HBase has a utility to create and pre-split a table.
hbase org.apache.hadoop.hbase.util.RegionSplitter table_name HexStringSplit -c 30 -f cf

The other algorithm choice is UniformSplit. User can also implement a custom algorithm. It would be great to be able to pass an option to create a table, perhaps in the prepare() method as well as pre-split the table based on the passed algorithm.

Something like this ""hbaseMapper().withCreateTable(numRegions, columnFamily, algorithm) where algorithm is UniformSplit, HexStringSplit and/or a custom algorithm implementation"
STORM-1113,Add executor id to thread name,"It would be useful in debugging worker that have lots of instances of a given component if the executor id were a part of the thread name.

When using jstack on a worker with 7 instances of a spout, two of which were failing, I had a hard time figuring out which thread corresponded to which instance of the spout."
STORM-1109,Worker exists frequently due to java.net.SocketTimeoutException,"One of the supervisor is exiting frequently , following is the log

015-10-13T20:57:20.245+0530 k.c.SimpleConsumer [INFO] Reconnect due to socket error: null
2015-10-13T20:57:30.284+0530 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.net.SocketTimeoutException
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:146) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.fill(PartitionManager.java:134) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.next(PartitionManager.java:108) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:123) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__6579$fn__6594$fn__6623.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$async_loop$fn__459.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
Caused by: java.net.SocketTimeoutException: null
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:229) ~[na:1.7.0_67]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[na:1.7.0_67]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[na:1.7.0_67]
        at kafka.utils.Utils$.read(Utils.scala:395) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54) ~[stormjar.jar:na]
        at kafka.network.Receive$class.readCompletely(Transmission.scala:56) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) ~[stormjar.jar:na]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:100) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:81) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:71) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:108) ~[stormjar.jar:na]
        at kafka.javaapi.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:48) ~[stormjar.jar:na]
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:141) ~[stormjar.jar:na]
        ... 7 common frames omitted
2015-10-13T20:57:30.285+0530 b.s.d.executor [ERROR]
java.lang.RuntimeException: java.net.SocketTimeoutException
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:146) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.fill(PartitionManager.java:134) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.next(PartitionManager.java:108) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:123) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__6579$fn__6594$fn__6623.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$async_loop$fn__459.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
Caused by: java.net.SocketTimeoutException: null
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:229) ~[na:1.7.0_67]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[na:1.7.0_67]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[na:1.7.0_67]
        at kafka.utils.Utils$.read(Utils.scala:395) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54) ~[stormjar.jar:na]
        at kafka.network.Receive$class.readCompletely(Transmission.scala:56) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) ~[stormjar.jar:na]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:100) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:81) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:71) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:108) ~[stormjar.jar:na]
        at kafka.javaapi.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:48) ~[stormjar.jar:na]
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:141) ~[stormjar.jar:na]
        ... 7 common frames omitted
2015-10-13T20:57:47.906+0530 b.s.util [ERROR] Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$fn__7028$fn__7029.invoke(worker.clj:497) [storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.executor$mk_executor_data$fn__6480$fn__6481.invoke(executor.clj:240) [storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$async_loop$fn__459.invoke(util.clj:473) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
2015-10-13T20:57:47.907+0530 b.s.d.worker [INFO] Shutting down worker


this is usually accompanied by the following error is supervisor.

2015-10-13T20:58:21.858+0530 b.s.d.supervisor [INFO] Shutting down fa862fbe-bfd2-4b53-9abb-cd951303ecb8:42ccaf83-6a17-4dcb-92e6-47416486806d
2015-10-13T20:58:21.869+0530 b.s.event [ERROR] Error when processing event
java.io.IOException: . doesn't exist.
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:157) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:147) ~[commons-exec-1.1.jar:1.1]
        at backtype.storm.util$exec_command_BANG_.invoke(util.clj:386) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$send_signal_to_process.invoke(util.clj:415) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$kill_process_with_sig_term.invoke(util.clj:426) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:197) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:267) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2625.invoke(event.clj:40) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
2015-10-13T20:58:21.877+0530 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2625.invoke(event.clj:48) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]


"
STORM-1107,Remove deprecated Config STORM_MESSAGING_NETTY_MAX_RETRIES and fix Netty Client backoff calculations,"Since Netty Client should not limit retry attempts, we should not use deprecated STORM_MESSAGING_NETTY_MAX_RETRIES configuration for  backoff calculation "
STORM-1105,Compile the logical plans into LLVM functions,This jira tracks the effort on compiling the stages of the logical plans into LLVM  functions.
STORM-1101,test-retry-read-assignments in backtype.storm.supervisor-test fails,"https://travis-ci.org/mjsax/storm/builds/84478972

{noformat}
ava.lang.RuntimeException: Should not have multiple topologies assigned to one port
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.7.0_76]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[?:1.7.0_76]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.7.0_76]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[?:1.7.0_76]
	at clojure.lang.Reflector.invokeConstructor(Reflector.java:180) ~[clojure-1.7.0.jar:?]
	at backtype.storm.util$throw_runtime.doInvoke(util.clj:845) ~[classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:408) ~[clojure-1.7.0.jar:?]
	at backtype.storm.daemon.supervisor$read_assignments$fn__9770.doInvoke(supervisor.clj:84) ~[classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:421) ~[clojure-1.7.0.jar:?]
	at clojure.core$merge_with$merge_entry__4649.invoke(core.clj:2932) ~[clojure-1.7.0.jar:?]
	at clojure.core$reduce1.invoke(core.clj:909) ~[clojure-1.7.0.jar:?]
	at clojure.core$merge_with$merge2__4651.invoke(core.clj:2935) ~[clojure-1.7.0.jar:?]
	at clojure.core$reduce1.invoke(core.clj:909) ~[clojure-1.7.0.jar:?]
	at clojure.core$reduce1.invoke(core.clj:900) ~[clojure-1.7.0.jar:?]
	at clojure.core$merge_with.doInvoke(core.clj:2936) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.applyTo(RestFn.java:139) ~[clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:632) ~[clojure-1.7.0.jar:?]
	at backtype.storm.daemon.supervisor$read_assignments.invoke(supervisor.clj:84) ~[classes/:?]
	at backtype.storm.daemon.supervisor$read_assignments.invoke(supervisor.clj:86) ~[classes/:?]
	at backtype.storm.daemon.supervisor$mk_synchronize_supervisor$this__10013.invoke(supervisor.clj:449) ~[classes/:?]
	at backtype.storm.event$event_manager$fn__9629.invoke(event.clj:40) [classes/:?]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
{noformat}"
STORM-1100,TNonblockingServer [ERROR] Unexpected exception while invoking!,"I'm new to storm and first time to write issue here so Sorry if i made any mistake 

when i tried to submit topology spout didn't emit anything (ZERO)
then found in Supervisor log file that hasn't still start 
and in Worker log file found this 
Error on initialization of server mk-worker java.io.IOException: No such file or directory at java.io.UnixFileSystem.createFileExclusively(Native Method) at java.io.File.createNewFile(Unknown Source) at backtype.storm.util$touch.invoke(util.clj:432) at backtype.storm.daemon.worker$fn__4348$exec_fn__1228__auto____4349.invoke(worker.clj:331) at clojure.lang.AFn.applyToHelper(AFn.java:185) at clojure.lang.AFn.applyTo(AFn.java:151) at clojure.core$apply.invoke(core.clj:601) at backtype.storm.daemon.worker$fn__4348$mk_worker__4404.doInvoke(worker.clj:323) at clojure.lang.RestFn.invoke(RestFn.java:512) at backtype.storm.daemon.worker$_main.invoke(worker.clj:433) at clojure.lang.AFn.applyToHelper(AFn.java:172) at clojure.lang.AFn.applyTo(AFn.java:151) at backtype.storm.daemon.worker.main(Unknown Source) 2015-10-09 02:58:47 util [INFO] Halting process: (""Error on initialization"")

and after few minutes in Nimbus log file 
TNonblockingServer [ERROR] Unexpected exception while invoking! java.io.FileNotFoundException: File 'storm-local/nimbus/stormdist/fsd-1-1444356616/stormcode.ser' does not exist at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:137) at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1135) at backtype.storm.daemon.nimbus$read_storm_topology.invoke(nimbus.clj:305) at backtype.storm.daemon.nimbus$fn__3592$exec_fn__1228__auto__$reify__3605.getTopologyInfo(nimbus.clj:1066)  
"
STORM-1097,Compile logical plans to Java source code,This jira tracks the changes to compile logical plans from Calcite to Java source code.
STORM-1092,Writing issue on https://storm.apache.org/about/fault-tolerant.html,"Hi,
When I browse fault-tolerant part on https://storm.apache.org/about/fault-tolerant.html. I see ""If a node dies, the worker will be restarted on another node."" It makes me confused, so I go to https://storm.apache.org/documentation/Fault-tolerance.html
I see below part
What happens when a node dies?
The tasks assigned to that machine will time-out and Nimbus will reassign those tasks to other machines.
I think this Q&A makes sense, so it's better to change ""If a node dies, the worker will be restarted on another node."" to ""If a node dies, the tasks will be restarted on another node.""
Thanks,
Nick
"
STORM-1089,Add tool for recording and playback of storm traces,"We currently report information about latency and throughput of tuples flowing through storm. It would really be nice to have a tool that can gather this information over a period of time, anonymizes it if needed, and records it to a file.

Then have another tool that will replay that data so we can test how changes to storm would impact a given topology or set of topologies.

It would be great if we could capture the entire cluster, And potentially add in more details around CPU, memory, Network, tuple size, etc as we are able to gather more information."
STORM-1088,Reflection in disruptor/publish,This can slow things down if publishing a lot of tuples.
STORM-1085,"Compile comparison, arithmetic, and field reference expressions","This jira tracks the effort of implementing the comparison, arithmetic and the field reference expressions in the expression compiler."
STORM-1083,Run maven in batch mode to avoid excessive loggin in travis CI,"Currently the travis CI log prints out excessive logging messages on the progress of downloading jars which makes it difficult to debug build failures.

This jira proposes to run maven in batch mode to avoid the excessive logging messages."
STORM-1081,Import json11 and gmock into the storm-sql project,This jira proposes to include json11 and gmock into the storm-sql project to facilitate later development.
STORM-1080,Compiler SQL literals to LLVM constants,"This jira tracks the effort of creating a expression compiler to compile SQL literals down to constants that are represented in LLVM IR.
"
STORM-1077,"Trident tutorial page in ""Documentation/Trident"" shows old UI.","https://storm.apache.org/documentation/Trident-tutorial.html has old UI
"
STORM-1076,"""Lifecycle of a trident tuple"" link in Intermediate section of Documentation is pointing to ""Life cycle of a topology""","""Lifecycle of a trident tuple"" link in ""Intermediate"" section of Documentation is pointing to ""Life cycle of a topology"" document.
"
STORM-1072,Nimbus gives incomplete cluster data to scheduler (hides dead worker slots),"1. Describe observed behavior.

Certain slots that have been assigned but have workers that have not yet sent a heartbeat are treated as ""dead"" slots, and these are not included in the cluster summary data that is passed to the scheduler.

[link|https://github.com/apache/storm/blob/8dd9e6e213210009968f39483cb69f271b2e8415/storm-core/src/clj/backtype/storm/daemon/nimbus.clj#L527] to nimbus code

For topologies whose payload is very large, this can result in scheduler results that never quite converge due to some of the slots not appearing on each call to schedule()


2. What is the expected behavior?

Nimbus may be too smart here: it seems better to give the full cluster information to the scheduler and let the scheduler make the appropriate decision about how to handle workers that are not yet up.

3. Outline the steps to reproduce the problem.

Either launch a topology with a very large jar file that takes minutes to download, or simulate by adding a sleep to the supervisor code just after the jar is downloaded.  This will cause a significant delay before the worker is up and heartbeating in.  On each scheduling run, such slots will not even be present for the scheduler logic.
"
STORM-1071,Task Message format to include source task id.,"As part of gathering inputs for Resource Aware Scheduler, understanding the tuple network characteristics between tasks would be helpful. 
The present implementation of TaskMessage includes only the destination task id and message. Including the source task id to it would help us understand the task-task (component/component) network bandwidth characteristics.  It shall also avoid the deserialization of message to retrieve the source task id from the serialized tuple message.
"
STORM-1070,Additional JIRA Components,"Create additional JIRA components in Storm project:

storm-core
storm-multilang
storm-nimbus
storm-supervisor
storm-drpc
storm-elasticsearch
storm-hive
storm-jdbc
storm-solr
storm-examples (for storm-starter and others)
storm-ui

For consideration:
documentation
build
website
security

Would also consider making component required when creating new bugs.  Default would be storm-core.

"
STORM-1067,Run Travis-CI to in containers,"Currently the Travis-CI of Storm is running on VM. It is possible to run Travis-CI inside containers for better performance, which lower the turn around time for patches:

http://docs.travis-ci.com/user/migrating-from-legacy/?utm_source=legacy-notice&utm_medium=banner&utm_campaign=legacy-upgrade"
STORM-1065,storm-kafka : kafka-partition can not find leader in zookeeper ,"If the Kafka cluster is not consistent with the zookeeper data, the partition can not find leader in zookeeper.In storm-kafka, it throws runtime exception""No leader found for partition"",it is not friendly.
Suggestion:
If there is no leader partition in zookeeper,don't add the partition to GlobalPartitionInformation object,instead of throwing runtime exception."
STORM-1064,storm-kafka : kafka-partition is not leader,"If the Kafka cluster is not consistent with the zookeeper data, the partition can not find leader in zookeeper.In storm-kafka, it throws runtime exception""No leader found for partition"",it is not friendly.
Suggestion:
If there is no leader partition in zookeeper,don't add the partition to GlobalPartitionInformation object,instead of throwing runtime exception."
STORM-1062,Establish the basic structure of the code generator,This jira tracks the effort of creating the basic structures of the code generator in Storm-SQL.
STORM-1061,Create integration tests around ignoreZkOffsets config for kafka spout - core and trident,Related to STORM-1017 changes. Add tests to make sure that ignoreZkOffsets config is used only user kills and resubmits the topology and not when a worker is crashed and restarted.
STORM-1060,Serialize Calcite plans into JSON format,This jira proposes to serialize the information generated by Calcite into JSON format so that it can be consumed in the later stages of the compilation pipeline.
STORM-1057,Add throughput metric to spout/bolt and display them on web ui,"Throughput is a fundamental metric to reasoning about the performance bottleneck of a topology. Displaying the throughputs of components and tasks on the web ui could greatly facilitate the user identifying the performance bottleneck and checking whether the the workload among components and tasks are balanced. 

What to do:
1. Measure the throughput of each spout/bolt.
2. Display the throughput metrics on web UI."
STORM-1049,Document central classes of storm-kafka,"The `storm-kafka` project currently is very hard to use because central classes, like `KafkaSpout` and `PartitionManager` are completely documentation-free. At least these classes should be well documented, so that it's possible to guess from the code what important methods do."
STORM-1047,document internals of bin/storm.py,"The `python` script `bin/storm.py` is completely undocumented regarding its internals. Function comments only include a command line interface often omitting an explanation of arguments and their default values (e.g. it should be clear why the default value of `klass` of `nimbus` is `""backtype.storm.daemon.nimbus""` because that doesn't make sense to someone unfamiliar with the storm-core implementation).

Also explanations like ""Launches the nimbus daemon. [...]"" (again `nimbus` function) is good for a command line API doc, but insufficient for a function documentation (should mention that it starts a `java` process and passes `klass` as class name to it).

How does the script use `lib/`, `extlib/` and `extlib-daemon`? It's too complex to squeeze this info out of the source code."
STORM-1045,Parse CREATE TABLE statements,This jira tracks the effort of creating a SQL parser for STORM-1040
STORM-1044,Setting dop to zero does not raise an error,"While I did some testing (with automatic topology plugging code) I set the dop of all spouts and bolts to zero. I submitted the topology to {{LocalCluster}} and did not get any error.

It took me a while to figure out that the wrongly specified dop were the reason for an empty result of the test. From a user point of view, it would be nice to raise an exception when the {{parallelism_hint}} is smaller than 1."
STORM-1043,Concurrent access to state on local FS by multiple supervisors,"Hi,

we are running storm-mesos cluster and occassionaly workers die or are ""lost"" in mesos. When this happens it often coincides with errors in logs related to supervisors local state.

By looking at the storm code it seems this might be caused by the way how multiple supervisor processes access the local state in the same directory via VersionedStore.

For example: https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/daemon/supervisor.clj#L434

Here every supervisor does this concurrently:
1. reads latest state from FS
2. possibly updates the state
3. writes the new version of the state

Some updates could be lost if there are 2+ supervisors and they execute above steps concurrently - then only the updates from last supervisor would remain on the last state version on the disk.

We observed local state changes quite often (seconds), so the likelihood of this concurrency issue occurring is high.

Some examples of exeptions:
------------------------------------------
java.lang.RuntimeException: Version already exists or data already exists
at backtype.storm.utils.VersionedStore.createVersion(VersionedStore.java:85) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.VersionedStore.createVersion(VersionedStore.java:79) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.persist(LocalState.java:101) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.put(LocalState.java:82) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.put(LocalState.java:76) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.daemon.supervisor$mk_synchronize_supervisor$this7400.invoke(supervisor.clj:382) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.event$event_manager$fn2625.invoke(event.clj:40) ~[storm-core-0.9.5.jar:0.9.5]
at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]

---------------------------------------
java.io.FileNotFoundException: File '/var/lib/storm/supervisor/localstate/1441034838231' does not exist
at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
at backtype.storm.utils.LocalState.deserializeLatestVersion(LocalState.java:61) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.snapshot(LocalState.java:47) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.get(LocalState.java:72) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:234) ~[storm-core-0.9.5.jar:0.9.5]
at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
at clojure.core$partial$fn4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
at backtype.storm.event$event_manager$fn2625.invoke(event.clj:40) ~[storm-core-0.9.5.jar:0.9.5]
at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]
-----------------------------------------"
STORM-1041,Topology with kafka spout stops processing,"Topology:
 KafkaSpout (1 task/executor) -> bolt that does grouping (1 task/executor) -> bolt that does processing (176 tasks/executors)
 8 workers
 Using Netty

Sometimes when a worker dies (we've seen it happen due to an OOM or load from a co-located worker) it will try to restart on the same node, then 20s later shutdown and start on another node.

{code}
2015-09-10 08:05:41,131 -0700 INFO        backtype.storm.daemon.supervisor:0 - Launching worker with assignment #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""NoticeProcessorTopology-368-1441856754"", :executors ([9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225])} for this supervisor 8a845b9b-adaa-4943-b6a6-68fdadcc5146 on port 6701 with id 42a499b2-2c5c-43c2-be8a-a5b3f4f8a99e
2015-09-10 08:05:39,953 -0700 INFO        backtype.storm.daemon.supervisor:0 - Shutting down and clearing state for id 39c28ee2-abf9-4834-8b1f-0bd6933412e8. Current supervisor time: 1441897539. State: :disallowed, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1441897539, :storm-id ""NoticeProcessorTopology-368-1441856754"", :executors #{[9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [-1 -1] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225]}, :port 6700}
2015-09-10 08:05:22,693 -0700 INFO        backtype.storm.daemon.supervisor:0 - Launching worker with assignment #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""NoticeProcessorTopology-368-1441856754"", :executors ([9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225])} for this supervisor f26e1fae-03bd-4fa8-9868-6a54993f3c5d on port 6700 with id 39c28ee2-abf9-4834-8b1f-0bd6933412e8
2015-09-10 08:05:21,588 -0700 INFO        backtype.storm.daemon.supervisor:0 - Shutting down and clearing state for id 4f0e4c22-6ccc-4d78-a20f-88bffb8def1d. Current supervisor time: 1441897521. State: :timed-out, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1441897490, :storm-id ""NoticeProcessorTopology-368-1441856754"", :executors #{[9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [-1 -1] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225]}, :port 6700}
{code}

While the worker was dead and then killed, other workers have had netty drop messages. In theory these messages should timeout and be replayed. Our message timeout is 30s. 

{code}
2015-09-10 08:05:50,914 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone33-prod.amz.relateiq.com/10.30.101.36:6701
2015-09-10 08:05:44,904 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone33-prod.amz.relateiq.com/10.30.101.36:6701
2015-09-10 08:05:43,902 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone39-prod.amz.relateiq.com/10.30.101.5:6700
2015-09-10 08:05:27,873 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone39-prod.amz.relateiq.com/10.30.101.5:6700
2015-09-10 08:05:27,873 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone39-prod.amz.relateiq.com/10.30.101.5:6700
{code}

However these messages never timeout, and the MAX_SPOUT_PENDING has been reached, so no more tuples are emitted/processed.

"
STORM-1040,SQL support for Storm Phase I - Supporting filtering and projection SQL queries,"SQL is one of the more popular languages in business intelligence. Instead of writing Storm topology directly, having supports to compile SQL down to topology will accelerate the development and deployment of real-time streaming solutions."
STORM-1039,"Storm Web UI gives 500 error: Remove commons-codec shading, commons-codec is used by hadoop authentication library which is used during ui authentication in secured environment.",
STORM-1035,do not remove storm-code in supervisor until kill job,
STORM-1034,Create Fluent API for Kafka - SpoutConfig & TridentKafkaConfig ,"Create Fluent Java API for construction of SpoutConfig & TridentSpountConfig.  At present both of these classes have public instance variables and as such instantiation and validation is poor.

The use of a Fluent Java API will provide cleaner validation and the opportunity to deprecate properties while maintaining API compatibility.

Example:

{code:title=Bar.java|borderStyle=solid}
SpoutConfig simpleConfig = new SpountConfigBuilder()
                                                        .topicName(""topicA"")
                                                        .kafkaBrokerHosts(""hostname:2181"")
                                                        .stormClientID(""storm-client-id"")
                                                         .schema(new SchemeAsMultiScheme ())
                                                         .build();
{code}
 "
STORM-1031,Cleanup for temporary directories during unit tests,"During execution of unit tests, temporary directories are created in the users TEMP/TMP directory.  The clean process does not remove these as it has no knowledge of the directory names.  Given the size of these scratch directories, it will eventually fill up the user's hard drive.

I believe this is caused by the clojure-maven-plugin but have not confirmed.

An acceptable work around would be to at least prefix the temporary directories so they could be safely deleted.
"
STORM-1028,Eventhub spout meta data,"Event hub (and Kafka) play well into event source architectures as event ingest point for later Storm processing to downstream stateful consumers.

Advanced event stream processing, such as replaying parts of a stream, requires that the downstream consumers can synchronise different ""stream runs"" to their stateful view, which itself can be seen as an aggregation of all previous events. To set up the right context for re-processing the stream in a deterministic way, they need to sync their view with the incoming old data. To be able to do this, they need knowledge of the event sequenceNumber and partition.

For example, if you have a bolt that calculates total_order_amount for a stream of orders, and emits order tuples with the total_order_amount calculated for all previous orders, replaying an order event should not change total_order_amount. I.e. orders with a higher sequenceNumber than the order being processed should not be included in total_order_amount.

This synchronisation can be achieved if the bolt has access to the parition and sequenceNumber from eventHub."
STORM-1023,Nimbus server hogs 100% CPU and clients are stuck ,"Testing environment is Storm 0.9.5 / thrift java 0.7.
Test scenario: 
  Deploy storm topology in loop.
  When nimbus cleanup timeout is reached, an error is thrown by thrift server: 
  ""Exception while invoking ..."" ... TException

Test result:
  Thrift java server in nimbus goes 100% CPU in infinite loop in:

jstack:
{code}
""Thread-5"" prio=10 tid=0x00007fb134aab800 nid=0x6767 runnable [0x00007fb129c9b000]
   java.lang.Thread.State: RUNNABLE
                                      at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
                                      at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
                                      at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
                                      at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
...
at org.apache.thrift7.server.TNonblockingServer$SelectThread.select(TNonblockingServer.java:284) 
{code}

strace:
{code}
epoll_wait(70, {{EPOLLIN, {u32=866, u64=866}}, {EPOLLIN, {u32=876, u64=876}}}, 4096, 4294967295) = 2
{code}

Investigation and tests show that:
Any Exception thrown during the processor execution will bypass the call to {code} responseReady() {code} and will cause the counter {code}       readBufferBytesAllocated.addAndGet(-buffer_.array().length); {code} not to be decremented by the size of the request buffer.

After a bunch of failed requests, this counter almost reaches the max value MAX_READ_BUFFER_BYTES causing any subsequent request to be delayed forever because the following test in {code} read() {code}:
{code}           if (readBufferBytesAllocated.get() + frameSize > MAX_READ_BUFFER_BYTES)  {code} is always true.

At the end, the server thread loops in select() which immediately wakes up for read() since the content of the socket was never drained.

This loops forever between select and read() method above causing a 100% CPU on server thread.
Moreover, all client requests are stuck forever.

Example of failed request:
{code}
2015-09-01T12:19:35.954+0200 b.s.d.nimbus [WARN] Topology submission exception. (topology name='mytopology') #<IllegalArgumentException java.lang.IllegalArgumentException: /opt/SPE/share/stor
m/storm/local/nimbus/inbox/stormjar-3f8f3ba7-5420-4773-af24-bfa294cceb79.jar to copy to /opt/SPE/share/storm/storm/local/nimbus/stormdist/mytopology-87-1441102775 does not exist!>
2015-09-01T12:19:35.955+0200 o.a.t.s.TNonblockingServer [ERROR] Unexpected exception while invoking!
java.lang.IllegalArgumentException: /opt/SPE/share/storm/storm/local/nimbus/inbox/stormjar-3f8f3ba7-5420-4773-af24-bfa294cceb79.jar to copy to /opt/SPE/share/storm/storm/local/nimbus/stormdis
t/mytopology-87-1441102775 does not exist!
        at backtype.storm.daemon.nimbus$fn__3827.invoke(nimbus.clj:1173) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.MultiFn.invoke(MultiFn.java:236) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.nimbus$setup_storm_code.invoke(nimbus.clj:307) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.nimbus$fn__3724$exec_fn__1103__auto__$reify__3737.submitTopologyWithOpts(nimbus.clj:953) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.nimbus$fn__3724$exec_fn__1103__auto__$reify__3737.submitTopology(nimbus.clj:966) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.generated.Nimbus$Processor$submitTopology.getResult(Nimbus.java:1240) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.generated.Nimbus$Processor$submitTopology.getResult(Nimbus.java:1228) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:32) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:34) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:632) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.server.THsHaServer$Invocation.run(THsHaServer.java:201) [storm-core-0.9.5.jar:0.9.5]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
{code} "
STORM-1022,disconnectiong between workers,"We upgraded to 0.9.5 ando ran into the following exception. The supervisors did go down:

1 caution in our upgrade is we started a new nimbus, without any supervisors attached. Then we deployed topologies (from CICD). Next we build new supervisors and the supervisors will start on startup. However, in between the network service is restarted (due to hostname changed during the build <- chef). Just wanna throw this out in case this makes a difference.

In other word, it could be that supervisors started, picked up work,  then network restarted. 

{code}
SEVERE: RuntimeException while executing runnable org.apache.storm.guava.util.concurrent.Futures$4@445058b with executor org.apache.storm.guava.util.concurrent.MoreExecutors$SameThreadExecutorService@691bc565
java.lang.RuntimeException: Failed to connect to Netty-Client-usw2b-grunt-drone32-prod.amz.relateiq.com/10.30.103.202:6700
at backtype.storm.messaging.netty.Client.connect(Client.java:308)
at backtype.storm.messaging.netty.Client.access$1100(Client.java:78)
at backtype.storm.messaging.netty.Client$2.reconnectAgain(Client.java:297)
at backtype.storm.messaging.netty.Client$2.onSuccess(Client.java:283)
at backtype.storm.messaging.netty.Client$2.onSuccess(Client.java:275)
at org.apache.storm.guava.util.concurrent.Futures$4.run(Futures.java:1181)
at org.apache.storm.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
at org.apache.storm.guava.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
at org.apache.storm.guava.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
at org.apache.storm.guava.util.concurrent.ListenableFutureTask.done(ListenableFutureTask.java:91)
at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:384)
at java.util.concurrent.FutureTask.set(FutureTask.java:233)
at java.util.concurrent.FutureTask.run(FutureTask.java:274)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Giving up to connect to Netty-Client-usw2b-grunt-drone32-prod.amz.relateiq.com/10.30.103.202:6700 after 102 failed attempts
at backtype.storm.messaging.netty.Client.connect(Client.java:303)
{code}"
STORM-1021,HeartbeatExecutorService issue in Shell Spout\Bolt,"ShellSpout class (and it seems that this touches ShellBolt as well) doesn't restart when hearbeat timeout occurs. To reproduce this bug you should do the following:
1. Set supervisor.worker.timeout.secs property to e.g. 1;
2. Create a shell spout (as a standalone application, not Java class) that hangs for more than 1 second and doesn't respond on heartbeat messages, e.g. Thread.Sleep(5000);
3. After timeout Storm will try to kill the shell spout process with calling die function:
https://github.com/apache/storm/blob/v0.10.0-beta/storm-core/src/jvm/backtype/storm/spout/ShellSpout.java#L237
4. The ""die"" function will call heartBeatExecutorService.shutdownNow() function that raises InterruptedException, which is not caughted by the calling thread. In a result topology stops working properly, however you may see it in ./storm list.

I'm not Java developer and thus I'm not sure whether code below is valid, however it seems to fix the problem:

    private void die(Throwable exception) {
        heartBeatExecutorService.shutdownNow();
        try {
            heartBeatExecutorService.awaitTermination(5, TimeUnit.SECONDS);
        } catch (InterruptedException e) {
            LOG.error(""await catch "", e);
        }

        _collector.reportError(exception);
        _process.destroy();
        System.exit(11);
    }"
STORM-1019,Added missing dependency version to use of org.codehaus.mojo:make-maven-plugin ,Travis-CI reports critical warning that Maven plugin is missing version information.
STORM-1018,drpc https with keystore / password fails,"When running a drpc daemon configured to bring up https on a port with a given keystore, the daemon launches, but does not bind to either the http or https ports.  There is no indication in the drpc log what, if anything, is failing.

When launching the daemon (unsupervised) I saw a prompt for the jetty keystore password, which may be an indication into what is going on.

{noformat}
myhost:apache-storm-0.11.0-SNAPSHOT ppoulosk$ bin/storm drpc &
[4] 48845
myhost:apache-storm-0.11.0-SNAPSHOT ppoulosk$ Running: /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/bin/java -server -Ddaemon.name=drpc -Dstorm.options= -Dstorm.home=/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT -Dstorm.log.dir=/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/logs -Djava.library.path=/usr/local/lib -Dstorm.conf.file= -cp /Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/asm-4.0.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/cheshire-5.3.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/clj-stacktrace-0.2.7.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/clj-time-0.8.0.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/clojure-1.6.0.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/clout-1.0.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/compojure-1.1.3.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/core.incubator-0.1.0.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/disruptor-2.10.4.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/hiccup-0.3.6.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/jackson-core-2.3.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/jackson-dataformat-smile-2.3.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/java.classpath-0.2.2.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/javax.servlet-2.5.0.v201103041518.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/kryo-2.21.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/log4j-api-2.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/log4j-core-2.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/log4j-over-slf4j-1.6.6.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/log4j-slf4j-impl-2.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/minlog-1.2.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/ns-tracker-0.2.2.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/reflectasm-1.07-shaded.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/ring-core-1.1.5.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/ring-devel-1.3.0.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/ring-jetty-adapter-1.3.0.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/ring-json-0.3.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/ring-servlet-1.3.0.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/servlet-api-2.5.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/slf4j-api-1.7.7.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/storm-core-0.11.0-SNAPSHOT.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/tigris-0.1.1.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/tools.logging-0.2.3.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/lib/tools.namespace-0.2.4.jar:/Users/ppoulosk/workspace/ppoulosk-storm/storm-dist/binary/target/apache-storm-0.11.0-SNAPSHOT/conf -Xmx768m -Dlogfile.name=drpc.log -Dlog4j.configurationFile=log4j2/cluster.xml backtype.storm.daemon.drpc
org.apache.storm.jetty.ssl.keypassword : password
{noformat}

I used the following conf to test.
{noformat}
myhost:apache-storm-0.11.0-SNAPSHOT ppoulosk$ cat conf/storm.yaml 
storm.zookeeper.servers:
     - ""localhost""
storm.zookeeper.port: 2181
nimbus.host: ""localhost""
storm.local.dir: ""/var/stormtmp""
java.library.path: ""/usr/local/lib""
supervisor.slots.ports:
     - 6700
     - 6701
     - 6702
     - 6703
worker.childopts: ""-Xmx768m""
nimbus.childopts: ""-Xmx512m""
supervisor.childopts: ""-Xmx256m""
drpc.servers:
    - ""localhost""
drpc.https.keystore.password: ""password""
drpc.worker.threads: 128
drpc.https.port: 4949
drpc.port: 50570
drpc.https.keystore.path: ""/Users/ppoulosk/testkeystore""
drpc.http.port: 4080
drpc.invocations.port: 50571
drpc.https.keystore.type: ""PKCS12""
{noformat}

"
STORM-1015,Store Kafka offsets with Kafka's consumer offset management api,"Current Kafka spout stores the offsets (and some other states) inside ZK with its proprietary format. This does not work well with other Kafka offset monitoring tools such as Burrow, KafkaOffsetMonitor etc. In addition, the performance does not scale well compared with offsets managed by Kafka's built-in offset management api. I have added a new option for Kafka to store the same data using Kafka's built-in offset management capability. The change is completely backward compatible with the current ZK storage option. The feature can be turned on by a single configuration option. Hope this will help people who wants to explore the option of using Kafka's built-in offset management api.

References:

https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka
https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-OffsetCommit/FetchAPI

-thanks"
STORM-1010,Each KafkaBolt could have a specified properties.,"Now KafkaBolt producer properties could only be set by
conf.put(KafkaBolt.KAFKA_BROKER_PROPERTIES, props);
which means all bolts in a topology use the same version of properties.
A KafkaBolt may need it's own properties sometimes."
STORM-1006,Storm is not garbage collecting the messages (causing memory hit),"We are reading whole file in memory around 5 MB, which is send through Kafaka to Storm. In next bolt, we performs the operation on file and sends out tuple to next bolt. After profiling we found that file (bytes of file) does not get garbage collected. So after further investigation we found that  backtype.storm.coordination.CoordinatedBolt.CoordinatedOutputCollector.emit(String, Collection<Tuple>, List<Object>) API gets the first object and use it for tracking :(. Can you confirm reason behind this? Is there any way we can send different unique id as first element in list or the unique id of tuple used as indicator.

However, for time being we have made changes in schema assigned to KafkaSpout, so that it will parse the file and send out list of values.

If you below code CoordinatedBolt, ""Object id = tuple.getValue(0);” takes the 1st element from tuple instead of taking id of tuple. This ""id"" is then saved to _tracked hashhMap(TimeCache). In our case the 0th element is files byte data. This gets stored in the _tracked map till tree of tuple doesn’t get complete. As we are processing huge data we run outofMemory issue.

Code:

public void execute(Tuple tuple) {

        *Object id = tuple.getValue(0);*

        TrackingInfo track;

        TupleType type = getTupleType(tuple);

        synchronized(_tracked) {

            track = _tracked.get(id);

            if(track==null) {

                track = new TrackingInfo();

                if(_idStreamSpec==null) track.receivedId = true;

                _tracked.put(id, track);*

            }

        }



        if(type==TupleType.ID) {

            synchronized(_tracked) {

                track.receivedId = true;

            }

            checkFinishId(tuple, type);

        } else if(type==TupleType.COORD) {

            int count = (Integer) tuple.getValue(1);

            synchronized(_tracked) {

                track.reportCount++;

                track.expectedTupleCount+=count;

            }

            checkFinishId(tuple, type);

        } else {

            synchronized(_tracked) {

                _delegate.execute(tuple);

            }

        }

    }


"
STORM-1002,Remove backtype.storm.Transactional package,"Trident seems to replicate the batch processing similar to transactional package and it seems few of the classes are deprecated. Hence, it would be nice if we actually remove the transactional package. Also it would be nice to document the Trident functionality in detail."
STORM-999,Add support for user specified UGI - (UserGroupInformation) for storm hive connector,"In a non-secure environment, Storm Hive component that provides interaction with Hive from storm currently does that as the user storm with which the worker process had been started. We want to allow the component to interact with Hive as the user provided instead of user running the worker process"
STORM-998,Add support for user specified UGI - (UserGroupInformation) for storm hbase connector,"In a non-secure environment, Storm HBase component that provides interaction with HBase from storm currently does that as the user storm with which the worker process had been started. We want to allow the component to interact with HBase as the user provided instead of user running the worker process"
STORM-997,Add support for user specified UGI - (UserGroupInformation) for storm hdfs connector,"In a non-secure environment, Storm HDFS component that provides interaction with HDFS from storm currently does that as the user storm with which the worker process had been started. We want to allow the component to interact with hdfs as the user provided instead of user running the worker process"
STORM-995,ILocalCluster topology methods can throw NotAliveException,"The getTopology*(String) methods in ILocalCluster can all throw the checked exception NotAliveException through nimbus, but it's not declared in the method signatures, which confuses the Java compiler. They are declared correctly in Nimbus.Iface.

Affected methods - getTopologyConf, getTopology, getTopologyInfo"
STORM-989,JDBC state can not support transactional,"the class JdbcState in this module storm/external/storm-jdbc does not distinguish between opaque transactional state, transactional state, and non-transactional state, it can not implement the transaction executed only once."
STORM-987,Document how to recover TopicOffsetOutOfRange exception in Storm-Kafka Connector,KafkaSpout can land in TopicOffsetOutOfRangeException: Got fetch request with offset out of range . We need to document how to recover from this case.
STORM-985, provide .editorconfig with git pre-commit hooks to check code style before commit,
STORM-984,Storm UI redesign,
STORM-983,"If stormconf.ser is not downloaded correctly, the supervisor will bounce for ever","If the stormconf.ser is corrupted, and for whatever reason exists with size 0, the following function returns with an EOF exception, which crashes the Supervisor. Upon restart, supervisor tries to read it again, which results in another crash, etc. 

https://github.com/apache/storm/blob/master/storm-core/src/jvm/backtype/storm/utils/Utils.java#L155"
STORM-982,Random test failures on backtype.storm.grouping-test,"{code}
Error parsing /home/travis/build/HeartSaVioR/storm/storm-core/target/test-reports/backtype.storm.grouping-test.xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<testsuites>
    <testsuite package=""backtype.storm"" name=""grouping-test"">
        <testcase name=""test-custom-groupings"" classname=""backtype.storm.grouping-test"">
            <system-out>
<![CDATA[119127 [main] INFO  b.s.zookeeper - Starting inprocess zookeeper at port 2002 and dir /tmp/da732832-bada-4305-9132-e4b0e2de0e5b
119130 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
119139 [main] INFO  b.s.d.nimbus - Starting Nimbus with conf {""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""local"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 3773, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 0, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2002, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" false, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" true, ""drpc.authorizer.acl.filename"" ""drpc-auth-acl.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""/tmp/855275ea-e2eb-456c-a8f0-50fcc178969f"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""drpc.port"" 3772, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 6627, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" [6700 6701 6702 6703], ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""ui.header.buffer.bytes"" 4096, ""topology.disruptor.wait.timeout.millis"" 1000, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 50, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
119139 [main] INFO  b.s.d.nimbus - Using default scheduler
119140 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119146 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119146 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
119150 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119152 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119158 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119160 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119161 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
119162 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2002] WARN  o.a.z.s.NIOServerCnxn - caught end of stream exception
org.apache.zookeeper.server.ServerCnxn$EndOfStreamException: Unable to read additional data from client sessionid 0x14f1cc844830002, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
119162 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119164 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119164 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119166 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119166 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
119168 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119170 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119174 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""local"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 3773, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 0, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2002, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" false, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" true, ""drpc.authorizer.acl.filename"" ""drpc-auth-acl.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""/tmp/b74f8021-d3c3-48ff-a172-bb649daedd96"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""drpc.port"" 3772, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 6627, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" (1024 1025 1026), ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""ui.header.buffer.bytes"" 4096, ""topology.disruptor.wait.timeout.millis"" 1000, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 50, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
119176 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119177 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119178 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
119179 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119181 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119186 [main] INFO  b.s.d.supervisor - Starting supervisor with id a675baeb-c40f-4589-a33b-d828ea24e42b at host localhost
119188 [main] INFO  b.s.d.supervisor - Starting Supervisor with conf {""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""local"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 3773, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 0, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2002, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" false, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" true, ""drpc.authorizer.acl.filename"" ""drpc-auth-acl.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""/tmp/94b40e92-cc7a-4bcb-aab3-cb6be4f31a87"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""drpc.port"" 3772, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 6627, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" (1027 1028 1029), ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""ui.header.buffer.bytes"" 4096, ""topology.disruptor.wait.timeout.millis"" 1000, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 50, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
119189 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119191 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119191 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
119193 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119195 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119199 [main] INFO  b.s.d.supervisor - Starting supervisor with id 392aa8b2-1060-4cc0-b938-8dfe090ce254 at host localhost
119213 [main] INFO  b.s.d.nimbus - [req 1] Access from:  principal: op:submitTopology
119217 [main] INFO  b.s.d.nimbus - Received topology submission for topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef with conf {""topology.max.task.parallelism"" nil, ""topology.submitter.principal"" """", ""topology.acker.executors"" nil, ""storm.zookeeper.superACL"" nil, ""topology.users"" (), ""topology.submitter.user"" """", ""topology.kryo.register"" nil, ""topology.kryo.decorators"" (), ""storm.id"" ""topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0"", ""topology.name"" ""topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef""}
119220 [main] INFO  b.s.d.nimbus - nimbus file location:/tmp/855275ea-e2eb-456c-a8f0-50fcc178969f/nimbus/stormdist/topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0
119228 [main] INFO  b.s.d.nimbus - Activating topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef: topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0
119250 [main] INFO  b.s.s.EvenScheduler - Available slots: ([""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1025] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1026] [""392aa8b2-1060-4cc0-b938-8dfe090ce254"" 1027] [""392aa8b2-1060-4cc0-b938-8dfe090ce254"" 1028] [""392aa8b2-1060-4cc0-b938-8dfe090ce254"" 1029])
119253 [main] INFO  b.s.d.nimbus - Setting new assignment for topology id topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0: #backtype.storm.daemon.common.Assignment{:master-code-dir ""/tmp/855275ea-e2eb-456c-a8f0-50fcc178969f/nimbus/stormdist/topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0"", :node->host {""a675baeb-c40f-4589-a33b-d828ea24e42b"" ""localhost""}, :executor->node+port {[8 8] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [12 12] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [2 2] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [7 7] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [3 3] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [1 1] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [6 6] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [9 9] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [11 11] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [13 13] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [5 5] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [10 10] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024], [4 4] [""a675baeb-c40f-4589-a33b-d828ea24e42b"" 1024]}, :executor->start-time-secs {[8 8] 0, [12 12] 0, [2 2] 0, [7 7] 0, [3 3] 0, [1 1] 0, [6 6] 0, [9 9] 0, [11 11] 0, [13 13] 0, [5 5] 0, [10 10] 0, [4 4] 0}}
119271 [Thread-745] INFO  b.s.d.supervisor - Copying resources at file:/home/travis/build/HeartSaVioR/storm/storm-core/target/test-classes/resources to /tmp/b74f8021-d3c3-48ff-a172-bb649daedd96/supervisor/stormdist/topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0/resources
119276 [Thread-746] INFO  b.s.d.supervisor - Launching worker with assignment {:storm-id ""topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0"", :executors [[8 8] [12 12] [2 2] [7 7] [3 3] [1 1] [6 6] [9 9] [11 11] [13 13] [5 5] [10 10] [4 4]]} for this supervisor a675baeb-c40f-4589-a33b-d828ea24e42b on port 1024 with id a1dfcef2-be80-43b9-841f-93a170d3f96a
119277 [Thread-746] INFO  b.s.d.worker - Launching worker for topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0 on a675baeb-c40f-4589-a33b-d828ea24e42b:1024 with id a1dfcef2-be80-43b9-841f-93a170d3f96a and conf {""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""local"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 3773, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 0, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2002, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" false, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" true, ""drpc.authorizer.acl.filename"" ""drpc-auth-acl.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""/tmp/b74f8021-d3c3-48ff-a172-bb649daedd96"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""drpc.port"" 3772, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 6627, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" (1024 1025 1026), ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""ui.header.buffer.bytes"" 4096, ""topology.disruptor.wait.timeout.millis"" 1000, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 50, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
119279 [Thread-746] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119281 [Thread-746-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119281 [Thread-746-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
119283 [Thread-746] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
119285 [Thread-746-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
119289 [Thread-746] INFO  b.s.s.a.AuthUtils - Got AutoCreds []
119289 [Thread-746] INFO  b.s.d.worker - Reading Assignments.
119304 [Thread-746] INFO  b.s.d.worker - Launching receive-thread for a675baeb-c40f-4589-a33b-d828ea24e42b:1024
119304 [Thread-749-worker-receiver-thread-0] INFO  b.s.m.loader - Starting receive-thread: [stormId: topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0, port: 1024, thread-id: 0 ]
119316 [Thread-746] INFO  b.s.d.executor - Loading executor 3:[8 8]
119318 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 3:[8 8]
119319 [Thread-746] INFO  b.s.d.executor - Finished loading executor 3:[8 8]
119324 [Thread-746] INFO  b.s.d.executor - Loading executor 7db2538f-2c23-487a-9a75-1afda66c6134:[12 12]
119324 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 7db2538f-2c23-487a-9a75-1afda66c6134:[12 12]
119326 [Thread-746] INFO  b.s.d.executor - Finished loading executor 7db2538f-2c23-487a-9a75-1afda66c6134:[12 12]
119330 [Thread-746] INFO  b.s.d.executor - Loading executor 2:[2 2]
119332 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 2:[2 2]
119333 [Thread-746] INFO  b.s.d.executor - Finished loading executor 2:[2 2]
119337 [Thread-746] INFO  b.s.d.executor - Loading executor 3:[7 7]
119339 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 3:[7 7]
119341 [Thread-746] INFO  b.s.d.executor - Finished loading executor 3:[7 7]
119345 [Thread-746] INFO  b.s.d.executor - Loading executor 2:[3 3]
119346 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 2:[3 3]
119348 [Thread-746] INFO  b.s.d.executor - Finished loading executor 2:[3 3]
119355 [Thread-746] INFO  b.s.d.executor - Loading executor 1:[1 1]
119356 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 1:[1 1]
119359 [Thread-746] INFO  b.s.d.executor - Timeouts disabled for executor 1:[1 1]
119359 [Thread-746] INFO  b.s.d.executor - Finished loading executor 1:[1 1]
119363 [Thread-746] INFO  b.s.d.executor - Loading executor 3:[6 6]
119365 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 3:[6 6]
119367 [Thread-746] INFO  b.s.d.executor - Finished loading executor 3:[6 6]
119371 [Thread-746] INFO  b.s.d.executor - Loading executor 3:[9 9]
119373 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 3:[9 9]
119374 [Thread-746] INFO  b.s.d.executor - Finished loading executor 3:[9 9]
119379 [Thread-746] INFO  b.s.d.executor - Loading executor 3:[11 11]
119380 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 3:[11 11]
119382 [Thread-746] INFO  b.s.d.executor - Finished loading executor 3:[11 11]
119386 [Thread-746] INFO  b.s.d.executor - Loading executor __system:[-1 -1]
119386 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks __system:[-1 -1]
119388 [Thread-746] INFO  b.s.d.executor - Finished loading executor __system:[-1 -1]
119391 [Thread-746] INFO  b.s.d.executor - Loading executor __acker:[13 13]
119392 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks __acker:[13 13]
119393 [Thread-746] INFO  b.s.d.executor - Timeouts disabled for executor __acker:[13 13]
119394 [Thread-746] INFO  b.s.d.executor - Finished loading executor __acker:[13 13]
119397 [Thread-746] INFO  b.s.d.executor - Loading executor 2:[5 5]
119399 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 2:[5 5]
119401 [Thread-746] INFO  b.s.d.executor - Finished loading executor 2:[5 5]
119405 [Thread-746] INFO  b.s.d.executor - Loading executor 3:[10 10]
119406 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 3:[10 10]
119408 [Thread-746] INFO  b.s.d.executor - Finished loading executor 3:[10 10]
119412 [Thread-746] INFO  b.s.d.executor - Loading executor 2:[4 4]
119414 [Thread-746] INFO  b.s.d.executor - Loaded executor tasks 2:[4 4]
119416 [Thread-746] INFO  b.s.d.executor - Finished loading executor 2:[4 4]
119419 [Thread-746] INFO  b.s.d.worker - Worker has topology config {""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""local"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 3773, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 0, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2002, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" false, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""topology.submitter.principal"" """", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" true, ""drpc.authorizer.acl.filename"" ""drpc-auth-acl.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""/tmp/b74f8021-d3c3-48ff-a172-bb649daedd96"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""storm.zookeeper.superACL"" nil, ""drpc.port"" 3772, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 6627, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" (1024 1025 1026), ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""topology.users"" [], ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.submitter.user"" """", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""topology.kryo.register"" nil, ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""ui.header.buffer.bytes"" 4096, ""topology.disruptor.wait.timeout.millis"" 1000, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 50, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""topology.kryo.decorators"" [], ""storm.id"" ""topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0"", ""topology.name"" ""topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef"", ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
119419 [Thread-746] INFO  b.s.d.worker - Worker a1dfcef2-be80-43b9-841f-93a170d3f96a for storm topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0 on a675baeb-c40f-4589-a33b-d828ea24e42b:1024 has finished loading
119419 [Thread-746] INFO  b.s.config - SET worker-user a1dfcef2-be80-43b9-841f-93a170d3f96a 
119438 [refresh-active-timer] INFO  b.s.d.worker - All connections are ready for worker a675baeb-c40f-4589-a33b-d828ea24e42b:1024 with id a1dfcef2-be80-43b9-841f-93a170d3f96a
119441 [Thread-757-3] INFO  b.s.d.executor - Preparing bolt 3:(7)
119443 [Thread-757-3] INFO  b.s.d.executor - Prepared bolt 3:(7)
119448 [Thread-759-2] INFO  b.s.d.executor - Preparing bolt 2:(3)
119450 [Thread-759-2] INFO  b.s.d.executor - Prepared bolt 2:(3)
119459 [Thread-761-1] INFO  b.s.d.executor - Opening spout 1:(1)
119459 [Thread-761-1] INFO  b.s.d.executor - Opened spout 1:(1)
119460 [Thread-761-1] INFO  b.s.d.executor - Activating spout 1:(1)
119467 [Thread-763-3] INFO  b.s.d.executor - Preparing bolt 3:(6)
119469 [Thread-763-3] INFO  b.s.d.executor - Prepared bolt 3:(6)
119475 [Thread-765-3] INFO  b.s.d.executor - Preparing bolt 3:(9)
119476 [Thread-765-3] INFO  b.s.d.executor - Prepared bolt 3:(9)
119482 [Thread-767-3] INFO  b.s.d.executor - Preparing bolt 3:(11)
119483 [Thread-767-3] INFO  b.s.d.executor - Prepared bolt 3:(11)
119488 [Thread-769-__system] INFO  b.s.d.executor - Preparing bolt __system:(-1)
119488 [Thread-769-__system] INFO  b.s.d.executor - Prepared bolt __system:(-1)
119494 [Thread-771-__acker] INFO  b.s.d.executor - Preparing bolt __acker:(13)
119494 [Thread-771-__acker] INFO  b.s.d.executor - Prepared bolt __acker:(13)
119501 [Thread-773-2] INFO  b.s.d.executor - Preparing bolt 2:(5)
119502 [Thread-773-2] INFO  b.s.d.executor - Prepared bolt 2:(5)
119508 [Thread-775-3] INFO  b.s.d.executor - Preparing bolt 3:(10)
119510 [Thread-775-3] INFO  b.s.d.executor - Prepared bolt 3:(10)
119516 [Thread-777-2] INFO  b.s.d.executor - Preparing bolt 2:(4)
119518 [Thread-777-2] INFO  b.s.d.executor - Prepared bolt 2:(4)
119520 [Thread-751-3] INFO  b.s.d.executor - Preparing bolt 3:(8)
119521 [Thread-751-3] INFO  b.s.d.executor - Prepared bolt 3:(8)
119526 [Thread-753-7db2538f-2c23-487a-9a75-1afda66c6134] INFO  b.s.d.executor - Preparing bolt 7db2538f-2c23-487a-9a75-1afda66c6134:(12)
119527 [Thread-753-7db2538f-2c23-487a-9a75-1afda66c6134] INFO  b.s.d.executor - Prepared bolt 7db2538f-2c23-487a-9a75-1afda66c6134:(12)
119534 [Thread-755-2] INFO  b.s.d.executor - Preparing bolt 2:(2)
119535 [Thread-755-2] INFO  b.s.d.executor - Prepared bolt 2:(2)
119583 [main] INFO  b.s.d.nimbus - [req 1] Access from:  principal: op:killTopology
119586 [main] INFO  b.s.d.nimbus - Delaying event :remove for 0 secs for topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0
119593 [timer] INFO  b.s.d.nimbus - Killing topology: topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0
119596 [Thread-745] INFO  b.s.d.supervisor - Removing code for storm id topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0
119600 [Thread-746] INFO  b.s.d.supervisor - Shutting down and clearing state for id a1dfcef2-be80-43b9-841f-93a170d3f96a. Current supervisor time: 11. State: :disallowed, Heartbeat: {:time-secs 11, :storm-id ""topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0"", :executors [[8 8] [12 12] [2 2] [7 7] [3 3] [1 1] [6 6] [9 9] [11 11] [-1 -1] [13 13] [5 5] [10 10] [4 4]], :port 1024}
119600 [Thread-746] INFO  b.s.d.supervisor - Shutting down a675baeb-c40f-4589-a33b-d828ea24e42b:a1dfcef2-be80-43b9-841f-93a170d3f96a
119600 [Thread-746] INFO  b.s.config - GET worker-user a1dfcef2-be80-43b9-841f-93a170d3f96a
119600 [Thread-746] INFO  b.s.process-simulator - Killing process 49e76a1a-4cc0-4037-af28-2dc46cdf36c5
119600 [Thread-746] INFO  b.s.d.worker - Shutting down worker topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0 a675baeb-c40f-4589-a33b-d828ea24e42b 1024
119600 [Thread-746] INFO  b.s.d.worker - Shutting down receive thread
119601 [Thread-746] INFO  b.s.m.loader - Shutting down receiving-thread: [topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0, 1024]
119601 [Thread-746] INFO  b.s.m.loader - Waiting for receiving-thread:[topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0, 1024] to die
119601 [Thread-746] INFO  b.s.m.loader - Shutdown receiving-thread: [topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0, 1024]
119601 [Thread-749-worker-receiver-thread-0] INFO  b.s.m.loader - Receiving-thread:[topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0, 1024] received shutdown notice
119601 [Thread-746] INFO  b.s.d.worker - Shut down receive thread
119601 [Thread-746] INFO  b.s.d.worker - Terminating messaging context
119601 [Thread-746] INFO  b.s.d.worker - Shutting down executors
119601 [Thread-746] INFO  b.s.d.executor - Shutting down executor 3:[8 8]
119601 [Thread-751-3] INFO  b.s.util - Async loop interrupted!
119601 [Thread-750-disruptor-executor[8 8]-send-queue] INFO  b.s.util - Async loop interrupted!
119601 [Thread-746] INFO  b.s.d.executor - Shut down executor 3:[8 8]
119603 [Thread-746] INFO  b.s.d.executor - Shutting down executor 7db2538f-2c23-487a-9a75-1afda66c6134:[12 12]
119603 [Thread-753-7db2538f-2c23-487a-9a75-1afda66c6134] INFO  b.s.util - Async loop interrupted!
119603 [Thread-752-disruptor-executor[12 12]-send-queue] INFO  b.s.util - Async loop interrupted!
119603 [Thread-746] INFO  b.s.d.executor - Shut down executor 7db2538f-2c23-487a-9a75-1afda66c6134:[12 12]
119603 [Thread-746] INFO  b.s.d.executor - Shutting down executor 2:[2 2]
119603 [Thread-754-disruptor-executor[2 2]-send-queue] INFO  b.s.util - Async loop interrupted!
119603 [Thread-755-2] INFO  b.s.util - Async loop interrupted!
119604 [Thread-746] INFO  b.s.d.executor - Shut down executor 2:[2 2]
119604 [Thread-746] INFO  b.s.d.executor - Shutting down executor 3:[7 7]
119604 [Thread-756-disruptor-executor[7 7]-send-queue] INFO  b.s.util - Async loop interrupted!
119604 [Thread-757-3] INFO  b.s.util - Async loop interrupted!
119604 [Thread-746] INFO  b.s.d.executor - Shut down executor 3:[7 7]
119604 [Thread-746] INFO  b.s.d.executor - Shutting down executor 2:[3 3]
119605 [Thread-759-2] INFO  b.s.util - Async loop interrupted!
119605 [Thread-758-disruptor-executor[3 3]-send-queue] INFO  b.s.util - Async loop interrupted!
119605 [Thread-746] INFO  b.s.d.executor - Shut down executor 2:[3 3]
119605 [Thread-746] INFO  b.s.d.executor - Shutting down executor 1:[1 1]
119605 [Thread-760-disruptor-executor[1 1]-send-queue] INFO  b.s.util - Async loop interrupted!
119605 [Thread-761-1] INFO  b.s.util - Async loop interrupted!
119605 [Thread-746] INFO  b.s.d.executor - Shut down executor 1:[1 1]
119605 [Thread-746] INFO  b.s.d.executor - Shutting down executor 3:[6 6]
119606 [timer] INFO  b.s.d.nimbus - Cleaning up topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0
119606 [Thread-762-disruptor-executor[6 6]-send-queue] INFO  b.s.util - Async loop interrupted!
119606 [Thread-763-3] INFO  b.s.util - Async loop interrupted!
119606 [Thread-746] INFO  b.s.d.executor - Shut down executor 3:[6 6]
119606 [Thread-746] INFO  b.s.d.executor - Shutting down executor 3:[9 9]
119606 [Thread-765-3] INFO  b.s.util - Async loop interrupted!
119606 [Thread-764-disruptor-executor[9 9]-send-queue] INFO  b.s.util - Async loop interrupted!
119607 [Thread-746] INFO  b.s.d.executor - Shut down executor 3:[9 9]
119607 [Thread-746] INFO  b.s.d.executor - Shutting down executor 3:[11 11]
119607 [Thread-767-3] INFO  b.s.util - Async loop interrupted!
119607 [Thread-766-disruptor-executor[11 11]-send-queue] INFO  b.s.util - Async loop interrupted!
119607 [Thread-746] INFO  b.s.d.executor - Shut down executor 3:[11 11]
119607 [Thread-746] INFO  b.s.d.executor - Shutting down executor __system:[-1 -1]
119607 [Thread-769-__system] INFO  b.s.util - Async loop interrupted!
119607 [Thread-768-disruptor-executor[-1 -1]-send-queue] INFO  b.s.util - Async loop interrupted!
119608 [Thread-746] INFO  b.s.d.executor - Shut down executor __system:[-1 -1]
119608 [Thread-746] INFO  b.s.d.executor - Shutting down executor __acker:[13 13]
119608 [Thread-771-__acker] INFO  b.s.util - Async loop interrupted!
119608 [Thread-770-disruptor-executor[13 13]-send-queue] INFO  b.s.util - Async loop interrupted!
119608 [Thread-746] INFO  b.s.d.executor - Shut down executor __acker:[13 13]
119608 [Thread-746] INFO  b.s.d.executor - Shutting down executor 2:[5 5]
119608 [Thread-772-disruptor-executor[5 5]-send-queue] INFO  b.s.util - Async loop interrupted!
119608 [Thread-773-2] INFO  b.s.util - Async loop interrupted!
119608 [Thread-746] INFO  b.s.d.executor - Shut down executor 2:[5 5]
119609 [Thread-746] INFO  b.s.d.executor - Shutting down executor 3:[10 10]
119609 [Thread-775-3] INFO  b.s.util - Async loop interrupted!
119609 [Thread-774-disruptor-executor[10 10]-send-queue] INFO  b.s.util - Async loop interrupted!
119610 [Thread-746] INFO  b.s.d.executor - Shut down executor 3:[10 10]
119610 [Thread-746] INFO  b.s.d.executor - Shutting down executor 2:[4 4]
119610 [Thread-777-2] INFO  b.s.util - Async loop interrupted!
119610 [Thread-776-disruptor-executor[4 4]-send-queue] INFO  b.s.util - Async loop interrupted!
119610 [Thread-746] INFO  b.s.d.executor - Shut down executor 2:[4 4]
119610 [Thread-746] INFO  b.s.d.worker - Shut down executors
119610 [Thread-746] INFO  b.s.d.worker - Shutting down transfer thread
119611 [Thread-778-disruptor-worker-transfer-queue] INFO  b.s.util - Async loop interrupted!
119611 [Thread-746] INFO  b.s.d.worker - Shut down transfer thread
119608 [executor-heartbeat-timer] ERROR b.s.d.worker - Error when processing event
java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /workerbeats/topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0/a675baeb-c40f-4589-a33b-d828ea24e42b-1024
	at backtype.storm.util$wrap_in_runtime.invoke(util.clj:48) ~[classes/:?]
	at backtype.storm.zookeeper$set_data.invoke(zookeeper.clj:177) ~[classes/:?]
	at backtype.storm.cluster$mk_distributed_cluster_state$reify__5018.set_data(cluster.clj:103) ~[classes/:?]
	at backtype.storm.cluster$mk_storm_cluster_state$reify__5558.worker_heartbeat_BANG_(cluster.clj:376) ~[classes/:?]
	at sun.reflect.GeneratedMethodAccessor117.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_76]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[?:1.7.0_76]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
	at backtype.storm.daemon.worker$do_executor_heartbeats.doInvoke(worker.clj:66) ~[classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:439) ~[clojure-1.6.0.jar:?]
	at backtype.storm.daemon.worker$fn__7341$exec_fn__1689__auto__$reify__7343$fn__7346.invoke(worker.clj:439) ~[classes/:?]
	at backtype.storm.timer$schedule_recurring$this__4173.invoke(timer.clj:99) ~[classes/:?]
	at backtype.storm.timer$mk_timer$fn__4156$fn__4157.invoke(timer.clj:50) [classes/:?]
	at backtype.storm.timer$mk_timer$fn__4156.invoke(timer.clj:42) [classes/:?]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /workerbeats/topologytest-b78aef30-864a-4b00-a92b-fe20712f44ef-1-0/a675baeb-c40f-4589-a33b-d828ea24e42b-1024
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:260) ~[curator-framework-2.5.0.jar:?]
	at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:256) ~[curator-framework-2.5.0.jar:?]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.5.0.jar:?]
	at org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:252) ~[curator-framework-2.5.0.jar:?]
	at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:239) ~[curator-framework-2.5.0.jar:?]
	at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:39) ~[curator-framework-2.5.0.jar:?]
	at backtype.storm.zookeeper$set_data.invoke(zookeeper.clj:176) ~[classes/:?]
	... 15 more
119617 [executor-heartbeat-timer] ERROR b.s.util - Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:332) [classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:?]
	at backtype.storm.daemon.worker$mk_halting_timer$fn__7151.invoke(worker.clj:190) [classes/:?]
	at backtype.storm.timer$mk_timer$fn__4156$fn__4157.invoke(timer.clj:68) [classes/:?]
	at backtype.storm.timer$mk_timer$fn__4156.invoke(timer.clj:42) [classes/:?]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
{code}"
STORM-981,Random test failures on backtype.storm.security.auth.drpc-auth-test,"{code}
--------------------------------------------------
Error parsing /home/travis/build/HeartSaVioR/storm/storm-core/target/test-reports/backtype.storm.security.auth.drpc-auth-test.xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<testsuites>
    <testsuite package=""backtype.storm.security.auth"" name=""drpc-auth-test"">
        <testcase name=""deny-drpc-digest-test"" classname=""backtype.storm.security.auth.drpc-auth-test"">
            <system-out>
<![CDATA[151632 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
151639 [main] INFO  b.s.s.a.drpc-auth-test - storm conf:{""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""distributed"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 33709, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.digest.DigestSaslTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 5000, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2181, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" true, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""drpc.authorizer"" ""backtype.storm.security.auth.authorizer.DenyAuthorizer"", ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" false, ""drpc.authorizer.acl.filename"" ""drpc-auth-acl.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""storm-local"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""drpc.port"" 33708, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 6627, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" [6700 6701 6702 6703], ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""java.security.auth.login.config"" ""test/clj/backtype/storm/security/auth/jaas_digest.conf"", ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""ui.header.buffer.bytes"" 4096, ""topology.disruptor.wait.timeout.millis"" 1000, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 500, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
151639 [main] INFO  b.s.s.a.drpc-auth-test - Starting DRPC invocation server ...
151640 [Thread-1350] INFO  b.s.s.a.d.DigestSaslTransportPlugin - SASL DIGEST-MD5 transport factory will be used
151696 [Thread-1203-SendThread(localhost:2000)] WARN  o.a.z.ClientCnxn - Session 0x14f1c8c8933000a for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_31]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716) ~[?:1.8.0_31]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [zookeeper-3.4.6.jar:3.4.6-1569965]
151740 [main] INFO  b.s.s.a.drpc-auth-test - Starting DRPC handler server ...
151741 [Thread-1351] INFO  b.s.s.a.d.DigestSaslTransportPlugin - SASL DIGEST-MD5 transport factory will be used
151845 [pool-1377-thread-1] INFO  b.s.s.a.d.ServerCallbackHandler - Successfully authenticated client: authenticationID = bob authorizationID = bob
151848 [pool-1376-thread-1] INFO  b.s.s.a.d.ServerCallbackHandler - Successfully authenticated client: authenticationID = bob authorizationID = bob
151855 [pool-1377-thread-1] INFO  b.s.s.a.a.DenyAuthorizer - [req 124] Access  from: /127.0.0.1 principal:bob op:execute topoology:null
151862 [pool-1376-thread-1] INFO  b.s.s.a.a.DenyAuthorizer - [req 125] Access  from: /127.0.0.1 principal:bob op:fetchRequest topoology:null
151864 [main] INFO  b.s.s.a.drpc-auth-test - Stopping DRPC servers ...
]]>            </system-out>
        </testcase>
        <testcase name=""drpc-per-function-auth-non-strict-test"" classname=""backtype.storm.security.auth.drpc-auth-test"">
            <system-out>
<![CDATA[151868 [main] INFO  b.s.s.a.drpc-auth-test - storm conf:{""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""distributed"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 38836, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.digest.DigestSaslTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 5000, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2181, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" true, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""drpc.authorizer"" ""backtype.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer"", ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" false, ""drpc.authorizer.acl.filename"" ""drpc-simple-acl-test-scenario.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""storm-local"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""drpc.port"" 38835, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 6627, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" [6700 6701 6702 6703], ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""java.security.auth.login.config"" ""test/clj/backtype/storm/security/auth/drpc-auth-server.jaas"", ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""ui.header.buffer.bytes"" 4096, ""topology.disruptor.wait.timeout.millis"" 1000, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 500, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
151868 [main] INFO  b.s.s.a.drpc-auth-test - Starting DRPC invocation server ...
151869 [Thread-1354] INFO  b.s.s.a.d.DigestSaslTransportPlugin - SASL DIGEST-MD5 transport factory will be used
151869 [Thread-1354] ERROR b.s.s.a.ThriftServer - ThriftServer is being stopped due to: org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:38836.
org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:38836.
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:109) ~[libthrift-0.9.2.jar:0.9.2]
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:91) ~[libthrift-0.9.2.jar:0.9.2]
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:83) ~[libthrift-0.9.2.jar:0.9.2]
	at org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:76) ~[libthrift-0.9.2.jar:0.9.2]
	at backtype.storm.security.auth.SaslTransportPlugin.getServer(SaslTransportPlugin.java:72) ~[classes/:?]
	at backtype.storm.security.auth.ThriftServer.serve(ThriftServer.java:70) [classes/:?]
	at backtype.storm.security.auth.drpc_auth_test$launch_server$fn__6974.invoke(drpc_auth_test.clj:52) [?:?]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_31]

--------------------------------------------------
{code}"
STORM-980,Re-include storm-kafka tests from Travis CI build,"Recently [~zhuoliu] reported that randome unit tests failures on storm-kafka may be resolved with STORM-826.

I rebuilt ""test-kafka-test"" branch 5 times in a row (tested with jdk7, jdk8, so storm-kafka unit tests were run 10 times), and I can see there's no longer storm-kafka's random failure.

It is broken at 6th trial, but I've met random failure of storm-core first, so we're leaving decision whether we ignore low probabilities of random failures or not."
STORM-979,[storm-elasticsearch] Introduces BaseQueryFunction to query to ES while using Trident,"storm-elasticsearch has features on storing document, not querying something.
It would be better to have BaseQueryFunction for querying to ES and emit matched documents, as other external modules did."
STORM-970,UT messaging_test.clj#test-receiver-message-order build failed ,"the CI always build error recently and the failure looks really spurious
Then I build the tests locally and find : 

{code}
➜  storm git:(master) dev-tools/test-ns.py backtype.storm.messaging-test
...
[main] INFO  b.s.u.Utils - Using defaults.yaml from resources
Running backtype.storm.messaging-test
Tests run: 2, Passed: 2, Failures: 0, Errors: 1    (!!!Error occurred but travis-ci does not push this info out)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 39.593 s
[INFO] Finished at: 2015-08-05T15:20:26+08:00
[INFO] Final Memory: 27M/205M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.7.1:test (test-clojure) on project storm-core: Clojure failed. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.7.1:test (test-clojure) on project storm-core: Clojure failed.
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: Clojure failed.
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:464)
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:366)
        at com.theoryinpractise.clojure.ClojureRunTestWithJUnitMojo.execute(ClojureRunTestWithJUnitMojo.java:138)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 20 more
{code}

Seems like something went wrong of UT
{code}
 messaging_test.clj#test-receiver-message-order 
{code}
https://github.com/apache/storm/blob/master/storm-core/test/clj/backtype/storm/messaging_test.clj#L85"
STORM-963,Frozen topology (KafkaSpout + Multilang bolt),"Hi,

We've got a pretty simple topology running with Storm 0.9.5 (tried also with 0.9.4 and 0.9.6-INCUBATING) in a 3 machine cluster:

{code}kafkaSpout (3) -----> processBolt (12){code}

Some info:
- kafkaSpout reads from a topic with 3 partitions and 2 replications
- processBolt iterates throught the message and saves the results in MongoDB
- processBolt is implemented in Python and has a storm.log(""I'm doing something"") just to add a simple debug message in the logs
- The messages can be quite big (~25-40 MB) and are in JSON format
- The kafka topic has a retention of 2 hours
- We use the same ZooKeeper cluster to both Kafka and Storm

The topology gets frozen after several hours (not days) running. We don't see any message in the logs... In fact, the periodic message from s.k.KafkaUtils and s.k.ZkCoordinator disapears. As you can imagine, the message from the Bolt also dissapears. Logs are copy/pasted further on. If we redeploy the topology everything starts to work again until it becomes frozen again.

Our kafkaSpout config is:

{code}
ZkHosts zkHosts = new ZkHosts(""zkhost01:2181,zkhost02:2181,zkhost03:2181"");
SpoutConfig kafkaConfig = new SpoutConfig(zkHosts, ""topic"", ""/topic/ourclientid"", ""ourclientid"");
kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
kafkaConfig.fetchSizeBytes = 50*1024*1024;
kafkaConfig.bufferSizeBytes = 50*1024*1024;
{code}

We've also tried setting the following options

{code}
kafkaConfig.forceFromStart = true;
kafkaConfig.startOffsetTime = kafka.api.OffsetRequest.EarliestTime(); // Also with kafka.api.OffsetRequest.LatestTime();
kafkaConfig.useStartOffsetTimeIfOffsetOutOfRange = true;
{code}

Right now the topology is running without acking the messages since there's a bug in kafkaSpout with failed messages and deleted offsets in Kafka.

This is what can be seen in the logs in one of the workers:

{code}
2015-07-23T12:37:38.008+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:37:39.079+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:37:51.013+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:37:51.091+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:38:02.684+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:38:02.687+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:38:02.687+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2, partition=1}]
2015-07-23T12:38:02.687+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:38:02.687+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:38:02.687+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:38:09.012+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:38:41.878+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:39:02.688+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:39:02.691+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:39:02.691+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:39:02.691+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:39:02.691+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:39:02.691+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:40:02.692+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:40:02.695+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:40:02.695+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:40:02.695+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:40:02.695+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:40:02.695+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:41:02.696+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:41:02.699+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:41:02.699+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:41:02.699+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:41:02.699+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:41:02.699+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:42:02.735+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:42:02.737+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:42:02.737+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:42:02.737+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:42:02.737+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:42:02.737+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
{code}

and then it becomes frozen. Nothing is written into the nimbus log. We've checked the offsets in ZooKeeper and they're not updated:

{code}
{""topology"":{""id"":""218e58a5-6bfb-4b32-ae89-f3afa19306e1"",""name"":""our-topology""},""offset"":12047144,""partition"":1,""broker"":{""host"":""kafka2"",""port"":9092},""topic"":""topic""}
cZxid = 0x100028958
ctime = Wed Jul 01 12:22:36 CEST 2015
mZxid = 0x100518527
mtime = Thu Jul 23 12:42:41 CEST 2015
pZxid = 0x100028958
cversion = 0
dataVersion = 446913
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 183
numChildren = 0
{code}


Any ideas of what we could be missing?

PS: This was sent to the Storm user's mailing list and got 0 replies :\"
STORM-962,travis user's $SHELL env  is /bin/sh but should be /bin/bash,"https://travis-ci.org/apache/storm/jobs/72794573#L405

{code}
[INFO] /bin/sh: 14: /home/travis/.jdk_switcher_rc: [[: not found
[INFO] /bin/sh: 22: /home/travis/.jdk_switcher_rc: [[: not found
[INFO] /bin/sh: 32: /home/travis/.jdk_switcher_rc: [[: not found
[INFO] /bin/sh: 42: /home/travis/.jdk_switcher_rc: [[: not found
[INFO] /bin/sh: 3: /etc/profile.d/rvm.sh: [[: not found
[INFO] checking for gcc... gcc
{code}

change $SHELL :
{code}
chsh -s /bin/bash
{code}"
STORM-961,Investigate adding squall and trident-ml as modules,Add squall(https://github.com/epfldata/squall) as Storm-SQL and trident-ml(https://github.com/pmerienne/trident-ml) as Storm-ML  to Storm External Modules.
STORM-957,How to write Python sport/bolts to send data from Kafka via Storm to HDFS?,"We want to send some logs via Flume > Kafka > Storm > HDFS

1. Is there a sprout/bolt code available in Python to send loglines coming to Kafka queue to send/write to HDFS via Storm?

2. How much code do we have to write to integrate Kafka Storm to HDFS?"
STORM-956,"When the execute() or nextTuple() hang on external resources, stop the Worker's heartbeat","Sometimes the work threads produced by mk-threads in executor.clj hang on external resources or other unknown reasons. This makes the workers stop processing the tuples.  I think it is better to kill this worker to resolve the ""hang"". I plan to :
1. like `setup-ticks`, send a system-tick to receive-queue
2. the tuple-action-fn deal with this system-tick and remember the time that processes this tuple in the executor-data
3. when worker do local heartbeat, check the time the executor writes to executor-data. If the time is long from current (for example, 3 minutes), the worker does not do the heartbeat.  So the supervisor could deal with this problem."
STORM-955,replace all  backtype.storm.daemon.common.clj#StormBase with backtype.storm.generated.StormBase,
STORM-953,Acknowledging Unanchored tuple causes freeze in toplogy,"I am running a topology in the 6 node cluster on RHEL 5 and Oracle JDK 1.7. Schematic for topology as attached, there are 6 workers for spout and bolts. Topology picks up message from JMS and sends out message to bolts. Each bolt does some processing and sends out next set of tuple to downstream for further processing. All bolts are in shuffle grouping. There are two special bolts 
1. Log 
2. Ticket
Log bolt receive same tuple from each bolt for logging purpose but these tuples are unanchored. Similarly Ticket bolt receive tuple that are failed due to business logic, these are also unanchored. Bolt1 to Bolt4 and anchored tuple since we wanted message guarantee for these tuples. Each bolt receive one tuple and emit one data tuple from Bolt1 to Bolt4. 

I have observed that we were acking some unanchored tuple in Log and Ticket bolts, and it was causing topology freeze after sometime (10 mins when worker jvm heap size is set to 2 GB , 2 days when worker jvm heap was set to 16 GB). These freeze essentially slows down message read from JMS. After commenting out acking I didnt observe any slow down/freeze.
"
STORM-952,Add name for the threads in Storm process(nimbus  supervisor worker and drpc),
STORM-947,replace all  `backtype.storm.scheduler.ExecutorDetails`  with `backtype.storm.generated.ExecutorInfo `,"replace all  
{code}
backtype.storm.scheduler.ExecutorDetails
{code}  
with 
{code}
backtype.storm.generated.ExecutorInfo 
{code}"
STORM-946,We should remove Closed Client form cached-node+port->socket in worker,"The client may be Closed status after reconnect failed, and we will remove closed client from Context to escape memory leak.
But there is also reference for the closed Client in cached-node+port->socket in worker, for this reason we should also remove closed Client from cached-node+port->socket.  

Meanwhile there is another reason for us to do so. Think about this situation: worker A connect to worker B1 B2, but for some reason worker B1 B2 died at the same, then nimbus reschedule worker B1 B1. And new B1 B2 may partly rescheduled at the some host:port as old B1 B2, that is (old B1: host1+port1, old B2: host2+port2, new B1: host2+port2, new B2: host3+port3). Worker A realized worker B1 B2 died and start reconnect to worker B1 B2, but before new worker B1 and old B2 have the same host+port, and by the current logic, we will remove old B1 Client and and create new Client for new worker B2, and do nothing to old B2 and new B1 because they have the same host+port. This will result the topology stop processing tuples. Once we remove closed Client from cached-node+port->socket before refresh-connections, this  will not happen again."
STORM-941,Add storm-on-yarn to Storm External Modules,Add storm-yarn to Storm external
STORM-940,Lost messages with netty,"We have a topology that mysteriously stopped several times. On the most recent occasion it had a max spout pending of 4 and there were 4 transactions in zookeeper. We are using transactional topologies and the final bolt in the topology is a global grouping with only one task. We have verified that the first transaction finished successfully right up to the last line of the finish batch, but the tx is still in zookeeper and all of the other transactions are waiting on it. I know transactional topologies have been deprecated, but regardless this shouldn't happen unless the acks are being dropped. We have had a bunch of similar issues since moving to the netty based versions and I think there is a serious reliability issue with them. Sorry I can't provide code to replicate the issue because the problem is intermittent. "
STORM-939,"Storm Hive,Hbase, Hadoop connector should support user specified UGI in non-secure env","hadoop UGI can be configured in non-secure env as well. We should provide this capability for hdfs, hive, hbase connectors."
STORM-936,Modify workerlauncher to source a prescript with env settings,"All workers before being launched should optionally source a system defined shell script (e.g., cgroup configuration script). We need to be careful we don't break windows support."
STORM-932,Trident RichSpoutBatchExecutor  emit batch,"While working on support to make pending tuple count available for Spouts, we noticed that the emitBatch code actually waits for two tuple misses to break of the loop instead of one. 

```
            for(int i=0; i<_maxBatchSize; i++) {
                _spout.nextTuple();
                if(_collector.numEmitted < i) {
                    break;
                }
            }
            idsMap.put(txid, _collector.ids);
```
As the numEmitted is incremented for every tuple emitted, the condition to check for missed tuple could have been better put as 
_collector.numEmitted == i instead. This would break the loop when a single tuple in the batch missed proper emit. 
Kindly confirm if this behaviour is as expected.  
"
STORM-929,High CPU usage when bolt idle due to short disruptor queue wait time,"I'm running topology which has large num of executors (500) on storm 0.9.3. I find the CPU usage over 100% when topology idle. And half of the CPU usage is from kernel. I look into CPU utilization of worker process and find most of threads wait on:
     com.lmax.disruptor.BlockingWaitStrategy.waitFor(long, com.lmax.disruptor.Sequence, com.lmax.disruptor.Sequence[], com.lmax.disruptor.SequenceBarrier, long, java.util.concurrent.TimeUnit) 

I use Storm starter topology (wordcounter) to reproduce this issue. I change the sleep time of spout to 10s and executor num of  bolt to 500. So there was effectively no task to do. Again the CPU usage comes to 100% and half from  kernel. I think this may caused by frequently switching thread context due to short disruptor queue wait time."
STORM-927,Storm.cmd on Windows shouldn't exit until deployment complete,"The storm.cmd script returns before the topology has been deployed. This causes problems when attempting to automate deployments, as detecting failures is made difficult.

It looks like this is because all storm command are ultimately invoked via ""start /b"" unless STORM_DEBUG is set. Attempting to workaround by setting STORM_DEBUG doesn't work, as the fix in STORM-322 hasn't been applied to the debug command.

It looks like a relatively simple fix. Apply the STORM-322 fix to the debug route, and make the jar command use it by default."
STORM-923,(Security) Add AutoHDFS like credential fetching for the storm-hive bolt,
STORM-920,Should we  remove duplicated classes/code ?,"See:
{code}
backtype.storm.generated.ExecutorInfo vs backtype.storm.scheduler.ExecutorDetails
backtype.storm.generated.NodeInfo vs backtype.storm.scheduler.WorkerSlot
backtype.storm.generated.SupervisorInfo vs backtype.storm.scheduler.SupervisorDetails
backtype.storm.generated.StormBase vs backtype.storm.daemon.common.clj#StormBase
backtype.storm.generated.ExecutorStats vs backtype.storm.daemon.common.clj#ExecutorStats
backtype.storm.generated.SupervisorInfo vs backtype.storm.daemon.common.clj#SupervisorInfo
backtype.storm.generated.Assignment vs backtype.storm.daemon.common.clj#Assignment
{code}"
STORM-919,Gathering worker and supervisor process information (CPU/Memory),"It would be useful to have supervisor and worker process related information such as %cpu utilization, JVM memory and network bandwidth available to NIMBUS which would be useful for resource aware scheduler implementation later on. As a beginning, the information can be piggybacked on the existing heartbeats into the ZK or to the pacemaker as required. 
Related JIRAs
STORM-177
STORM-891
STORM-899
"
STORM-917,Ability to emit messages from a kafka spout to a specific stream ,"Today even if we have multiple spouts in storm they will all emit on the same ""default"" stream. Having an ability to configure the stream that a kafka spout emits to would be great. By default should emit to the ""default"" stream"
STORM-916,test-noop-authorization-w-simple-transport in nimbus-auth-test could throws SocketTimeoutException,"https://travis-ci.org/apache/storm/jobs/68401296
Relevant PR is https://github.com/apache/storm/pull/593

One of two build is failed for below reason.

Checking ./storm-core/target/test-reports/backtype.storm.security.auth.nimbus-auth-test.xml
--------------------------------------------------
classname: backtype.storm.security.auth.nimbus-auth-test / testname: test-noop-authorization-w-simple-transport
Uncaught exception, not in assertion.
expected: nil
  actual: java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException
 at backtype.storm.security.auth.TBackoffConnect.retryNext (TBackoffConnect.java:59)
    backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry (TBackoffConnect.java:51)
    backtype.storm.security.auth.ThriftClient.reconnect (ThriftClient.java:103)
    backtype.storm.security.auth.ThriftClient.<init> (ThriftClient.java:72)
    backtype.storm.utils.NimbusClient.<init> (NimbusClient.java:64)
    backtype.storm.security.auth.nimbus_auth_test/fn (nimbus_auth_test.clj:77)
    clojure.test$test_var$fn__7187.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7209$fn__7214.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7209.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4245.invoke (core.clj:2559)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1654)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:626)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798$fn__8799$fn__8800.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798$fn__8799.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:484)
    clojure.core$seq.invoke (core.clj:133)
    clojure.core$dorun.invoke (core.clj:2855)
    org.apache.storm.testrunner$eval8792.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6703)
    clojure.lang.Compiler.load (Compiler.java:7130)
    clojure.lang.Compiler.loadFile (Compiler.java:7086)
    clojure.main$load_script.invoke (main.clj:274)
    clojure.main$script_opt.invoke (main.clj:336)
    clojure.main$main.doInvoke (main.clj:420)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException
 at org.apache.thrift.transport.TSocket.open (TSocket.java:187)
    org.apache.thrift.transport.TFramedTransport.open (TFramedTransport.java:81)
    backtype.storm.security.auth.SimpleTransportPlugin.connect (SimpleTransportPlugin.java:103)
    backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry (TBackoffConnect.java:48)
    backtype.storm.security.auth.ThriftClient.reconnect (ThriftClient.java:103)
    backtype.storm.security.auth.ThriftClient.<init> (ThriftClient.java:72)
    backtype.storm.utils.NimbusClient.<init> (NimbusClient.java:64)
    backtype.storm.security.auth.nimbus_auth_test/fn (nimbus_auth_test.clj:77)
    clojure.test$test_var$fn__7187.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7209$fn__7214.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7209.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4245.invoke (core.clj:2559)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1654)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:626)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798$fn__8799$fn__8800.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798$fn__8799.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:484)
    clojure.core$seq.invoke (core.clj:133)
    clojure.core$dorun.invoke (core.clj:2855)
    org.apache.storm.testrunner$eval8792.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6703)
    clojure.lang.Compiler.load (Compiler.java:7130)
    clojure.lang.Compiler.loadFile (Compiler.java:7086)
    clojure.main$load_script.invoke (main.clj:274)
    clojure.main$script_opt.invoke (main.clj:336)
    clojure.main$main.doInvoke (main.clj:420)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
Caused by: java.net.SocketTimeoutException: null
 at java.net.SocksSocketImpl.remainingMillis (SocksSocketImpl.java:111)
    java.net.SocksSocketImpl.connect (SocksSocketImpl.java:392)
    java.net.Socket.connect (Socket.java:589)
    org.apache.thrift.transport.TSocket.open (TSocket.java:182)
    org.apache.thrift.transport.TFramedTransport.open (TFramedTransport.java:81)
    backtype.storm.security.auth.SimpleTransportPlugin.connect (SimpleTransportPlugin.java:103)
    backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry (TBackoffConnect.java:48)
    backtype.storm.security.auth.ThriftClient.reconnect (ThriftClient.java:103)
    backtype.storm.security.auth.ThriftClient.<init> (ThriftClient.java:72)
    backtype.storm.utils.NimbusClient.<init> (NimbusClient.java:64)
    backtype.storm.security.auth.nimbus_auth_test/fn (nimbus_auth_test.clj:77)
    clojure.test$test_var$fn__7187.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7209$fn__7214.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7209.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4245.invoke (core.clj:2559)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1654)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:626)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798$fn__8799$fn__8800.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798$fn__8799.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval8792$iter__8793__8797$fn__8798.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:484)
    clojure.core$seq.invoke (core.clj:133)
    clojure.core$dorun.invoke (core.clj:2855)
    org.apache.storm.testrunner$eval8792.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6703)
    clojure.lang.Compiler.load (Compiler.java:7130)
    clojure.lang.Compiler.loadFile (Compiler.java:7086)
    clojure.main$load_script.invoke (main.clj:274)
    clojure.main$script_opt.invoke (main.clj:336)
    clojure.main$main.doInvoke (main.clj:420)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)

      at: test_runner.clj:105

{noformat}
-------------------- system-out --------------------

93518 [main] INFO  b.s.zookeeper - Starting inprocess zookeeper at port 2000 and dir /tmp/2e907bc8-728a-4264-9633-8286457416f9
93520 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
93527 [main] INFO  b.s.d.nimbus - Starting Nimbus with conf {""topology.builtin.metrics.bucket.size.secs"" 60, ""nimbus.childopts"" ""-Xmx1024m"", ""ui.filter.params"" nil, ""storm.cluster.mode"" ""local"", ""storm.messaging.netty.client_worker_threads"" 1, ""supervisor.run.worker.as.user"" false, ""topology.max.task.parallelism"" nil, ""zmq.threads"" 1, ""storm.group.mapping.service"" ""backtype.storm.security.auth.ShellBasedGroupsMapping"", ""transactional.zookeeper.root"" ""/transactional"", ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""drpc.invocations.port"" 3773, ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""storm.messaging.netty.server_worker_threads"" 1, ""topology.max.error.report.per.interval"" 5, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""zmq.hwm"" 0, ""storm.principal.tolocal"" ""backtype.storm.security.auth.DefaultPrincipalToLocal"", ""supervisor.worker.shutdown.sleep.secs"" 1, ""storm.zookeeper.retry.times"" 5, ""ui.actions.enabled"" true, ""zmq.linger.millis"" 0, ""supervisor.enable"" true, ""topology.stats.sample.rate"" 0.05, ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2000, ""supervisor.heartbeat.frequency.secs"" 5, ""topology.enable.message.timeouts"" false, ""drpc.worker.threads"" 64, ""drpc.queue.size"" 128, ""drpc.https.keystore.password"" """", ""logviewer.port"" 8000, ""nimbus.reassign"" true, ""topology.executor.send.buffer.size"" 1024, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""ui.host"" ""0.0.0.0"", ""storm.nimbus.retry.interval.millis"" 2000, ""nimbus.inbox.jar.expiration.secs"" 3600, ""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.acker.executors"" nil, ""topology.fall.back.on.java.serialization"" true, ""storm.zookeeper.servers"" [""localhost""], ""nimbus.thrift.threads"" 64, ""logviewer.cleanup.age.mins"" 10080, ""topology.worker.childopts"" nil, ""topology.classpath"" nil, ""supervisor.monitor.frequency.secs"" 3, ""nimbus.credential.renewers.freq.secs"" 600, ""topology.skip.missing.kryo.registrations"" true, ""drpc.authorizer.acl.filename"" ""drpc-auth-acl.yaml"", ""storm.group.mapping.service.cache.duration.secs"" 120, ""topology.testing.always.try.serialize"" false, ""nimbus.monitor.freq.secs"" 10, ""supervisor.supervisors"" [], ""topology.tasks"" nil, ""topology.bolts.outgoing.overflow.buffer.enable"" false, ""storm.messaging.netty.socket.backlog"" 500, ""topology.workers"" 1, ""storm.local.dir"" ""/tmp/f9d407e6-abcf-4432-9ddb-052e0391d1e4"", ""worker.childopts"" ""-Xmx768m"", ""storm.auth.simple-white-list.users"" [], ""topology.message.timeout.secs"" 30, ""topology.state.synchronization.timeout.secs"" 60, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""supervisor.supervisors.commands"" [], ""logviewer.childopts"" ""-Xmx128m"", ""topology.environment"" nil, ""topology.debug"" false, ""storm.messaging.netty.max_retries"" 300, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""drpc.childopts"" ""-Xmx768m"", ""drpc.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.connection.timeout"" 15000, ""storm.zookeeper.auth.user"" nil, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.GzipThriftSerializationDelegate"", ""topology.max.spout.pending"" nil, ""nimbus.supervisor.timeout.secs"" 60, ""nimbus.task.timeout.secs"" 30, ""drpc.port"" 3772, ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""nimbus.thrift.port"" 60040, ""storm.auth.simple-acl.admins"" [], ""storm.nimbus.retry.times"" 5, ""supervisor.worker.start.timeout.secs"" 120, ""storm.zookeeper.retry.interval"" 1000, ""logs.users"" nil, ""transactional.zookeeper.port"" nil, ""drpc.max_buffer_size"" 1048576, ""task.credentials.poll.secs"" 30, ""drpc.https.keystore.type"" ""JKS"", ""topology.worker.receiver.thread.count"" 1, ""supervisor.slots.ports"" [6700 6701 6702 6703], ""topology.transfer.buffer.size"" 1024, ""topology.worker.shared.thread.pool.size"" 4, ""drpc.authorizer.acl.strict"" false, ""nimbus.file.copy.expiration.secs"" 600, ""topology.executor.receive.buffer.size"" 1024, ""nimbus.task.launch.secs"" 120, ""storm.local.mode.zmq"" false, ""storm.messaging.netty.buffer_size"" 5242880, ""worker.heartbeat.frequency.secs"" 1, ""ui.http.creds.plugin"" ""backtype.storm.security.auth.DefaultHttpCredentialsPlugin"", ""storm.zookeeper.root"" ""/storm"", ""topology.tick.tuple.freq.secs"" nil, ""drpc.https.port"" -1, ""task.refresh.poll.secs"" 10, ""task.heartbeat.frequency.secs"" 3, ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.http.port"" 3774, ""topology.error.throttle.interval.secs"" 10, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""storm.messaging.netty.authentication"" false, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""worker.gc.childopts"" """", ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""nimbus.cleanup.inbox.freq.secs"" 600, ""ui.users"" nil, ""transactional.zookeeper.servers"" nil, ""supervisor.worker.timeout.secs"" 30, ""storm.zookeeper.auth.password"" nil, ""supervisor.childopts"" ""-Xmx256m"", ""ui.filter"" nil, ""nimbus.authorizer"" ""backtype.storm.security.auth.authorizer.NoopAuthorizer"", ""ui.header.buffer.bytes"" 4096, ""storm.messaging.netty.flush.check.interval.ms"" 10, ""storm.nimbus.retry.intervalceiling.millis"" 60000, ""topology.trident.batch.emit.interval.millis"" 50, ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""storm.auth.simple-acl.users"" [], ""drpc.invocations.threads"" 64, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""ui.port"" 8080, ""storm.messaging.netty.transfer.batch.size"" 262144, ""logviewer.appender.name"" ""A1"", ""nimbus.thrift.max_buffer_size"" 1048576, ""nimbus.host"" ""localhost"", ""storm.auth.simple-acl.users.commands"" [], ""drpc.request.timeout.secs"" 600}
93528 [main] INFO  b.s.d.nimbus - Using default scheduler
93555 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]
93555 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
93623 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
93841 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
94898 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]
94898 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
94898 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2000] WARN  o.a.z.s.NIOServerCnxn - caught end of stream exception
org.apache.zookeeper.server.ServerCnxn$EndOfStreamException: Unable to read additional data from client sessionid 0x14e2d47005c0000, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208) [zookeeper-3.4.6.jar:3.4.6-1569965]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_31]
94907 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
94976 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]
94977 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
94992 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
94992 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
95050 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]
95059 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
95064 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]
95064 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
95115 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
95131 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
95138 [main-EventThread] INFO  b.s.zookeeper - Zookeeper state update: :connected:none
95149 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [1000] the maxSleepTimeMs [30000] the maxRetries [5]
95150 [main] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
95161 [main-EventThread] INFO  o.a.c.f.s.ConnectionStateManager - State change: CONNECTED
95574 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
95650 [main] INFO  b.s.u.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [2000] the maxSleepTimeMs [60000] the maxRetries [0]
            
--------------------------------------------------
{noformat}"
STORM-915,Track and resolve any further random tests failure,"Though we exclude storm-kafka build from Travis CI, we met some other random test failures on storm-core.
We can try to track these and file to JIRA (subtask of this issue), and resolve them.

Maybe we don't want to close this issue cause we're modifying codes every time and it could make strange random test failures so there's no finish line here."
STORM-914,AutoHDFS should also update hdfs.config into Configuration,"When storm user a specify hdfs cluster, he will put some config such as namenode.host into hdfs.config when submit storm topology, and when HdfsBolt create HdfsConfiguration, it will use hdfs.config update HDFSConfiguration, then HDFSBolt will visit the right hdfs cluster. But in AutoHDFS's renew and getHadoopCredentials, Configuration not for the HDFS that storm user specify, we should update all the Configuration with hdfs.config."
STORM-911,Have nimbus request resources from and run workers on external systems,"We need a framework in place so nimbus, and the scheduler in particular can request resources from both dedicated hosts, but also external systems, like YARN, Mesos, Openstack and Kubernetes."
STORM-909,Automatic Black Listing of bad nodes,"We should be able to detect and monitor the failure rate of workers on nodes, and come up with a few different probabilities.  How likely is it that this worker will fail on this particular node in the next n mins.  How likely is it that all workers will fail on this particular node in the next n mins.  How likely is it that this worker will fail on any node in the next n mins.

With these we should be able to detect bad nodes and blacklist them, and ideally trigger external systems that can take actions to try and fix the nodes.  We should also be able to detect topologies that have bugs in the common case warn them, and in the worst case stop trying to run them."
STORM-908,AWS workers can't communicate due to Netty-Client hostname resolution,"Observed that there is some kind of problem blocking communication between workers.  I think manually editing the /etc/hosts file is a workaround.
ubuntu@ip-10-9-255-123
/home/ubuntu/apache-storm-0.9.4/logs/worker-6714.log
2015-06-24T07:13:07.856+0000 b.s.m.n.Client [INFO] connection attempt 24 to Netty-Client-ip-10-9-255-20.us-west-2.compute.internal/10.9.255.20:6711 scheduled to run in 387 ms
2015-06-24T07:13:08.244+0000 b.s.m.n.Client [ERROR] connection attempt 24 to Netty-Client-ip-10-9-255-20.us-west-2.compute.internal/10.9.255.20:6711 failed: java.lang.RuntimeException: Returned channel was actually not established
 "
STORM-907,More Intellegent Backpressure,"As a follow on to STORM-886 we should look at ways to automatically adjust the back-pressure to meet SLAs.

Using an SLA latency goal for a topology from something like STORM-815 or STORM-594 we should be able to come up with a time budget for each stage in a DAG.  We will have to play games if there are loops in a topology, but it should be doable.  We could then adjust the budget based off of actual latency numbers measured while running.  Then throttle the upstream bolts/spouts so that the predicted time spent in the downstream queue will be under the budget for that stage.

I see this as a complement to the bang-bang controller from STORM-886 "
STORM-906,"Flux ""--local --zookeeper"" dose not work","[description]
-z,--zookeeper <host:port>
When running in local mode, use the ZooKeeper at the specified <host>:<port>  instead of the in-process ZooKeeper.
But when using zookeeper string like 'host-A:2181,host-B:2181,host-C:2181', Flux dose not work well.

[command]
{code}
storm jar topo.jar org.apache.storm.flux.Flux --local --zookeeper host-A:2181,host-B:2181,host-C:2181 topo.yaml
{code}

[code]
Flux.java:
{code}
...
if(zkStr.contains("":"")){
                        String[] hostPort = zkStr.split("":"");
                        zkHost = hostPort[0];
                        zkPort = hostPort.length > 1 ? Long.parseLong(hostPort[1]) : DEFAULT_ZK_PORT;

 }
...
{code}"
STORM-905,Look into supporting Cloud Dataflow streaming API,"We should look at supporting the Google Cloud Dataflow Streaming API through storm

https://github.com/GoogleCloudPlatform/DataflowJavaSDK"
STORM-904,move storm bin commands to java and provide appropriate bindings for windows and linux,Currently we have python and .cmd implementation for windows. This is becoming increasing difficult upkeep both versions. Lets make all the main code of starting daemons etc. to java and provider wrapper scripts in shell and batch for linux and windows respectively. 
STORM-903,fix class missing when ui.filter set to org.apache.hadoop.security.authentication.server.AuthenticationFilter,"When I set ui.filter=org.apache.hadoop.security.authentication.server.AuthenticationFilter, there are java.lang.NoClassDefFoundError about the ui:
```
org/apache/commons/codec/binary/Base64
java.lang.NoClassDefFoundError:
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:305) ~[hadoop-auth-2.4.0.jar:?]
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:357) ~[hadoop-auth-2.4.0.jar:?]
        at org.apache.storm.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.Server.handle(Server.java:369) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.http.HttpParser.parseNext(HttpParser.java:644) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.ssl.SslSocketConnector$SslConnectorEndPoint.run(SslSocketConnector.java:670) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:662) [?:1.6.0_37]
Caused by: java.lang.ClassNotFoundException: org.apache.commons.codec.binary.Base64
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202) ~[?:1.6.0_37]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.6.0_37]
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190) ~[?:1.6.0_37]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306) ~[?:1.6.0_37]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) ~[?:1.6.0_37]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247) ~[?:1.6.0_37]
        ... 25 more
```

This is because missing commons-code."
STORM-900,Provide Metrics Feedback API to Resource Aware Scheduler,"The Resource Aware scheduler should be able to use metrics to determine if it needs to reschedule things to be more ideal.  This is to provide that API to the scheduler, but not necessarily to act on those metrics yet.

This especially relates to STORM-899, but is not necessarily blocked by it."
STORM-899,CPU/Memory/Network metrics collection,"There are many use cases where it would be great to know how much memory/cpu/network a particular bolt/spout is using, and how much of that network is being used to send data to specific bolt instances downstream.

This can be used to improve the scheduling of a topology, and also to provide automatic elasticity to a topology.

Collecting these metrics is not too difficult, but being able to get them on a per bolt.spout basis is difficult.  Perhaps STORM-891 will make it simple for us to support this.

These metrics should flow back to nimbus."
STORM-897,Add Multi-tenant scheduler as a resource aware strategy,"Add a Resource aware scheduler strategy that is compatible with the multi-tenant scheduler.  This scheduler should use default CPU/network resources, but allocate memory based off of what the worker.childopts and topology.worker.childopts are set to."
STORM-896,Make Resource Aware strategy configurable per topology ,"Different topologies may want to use different strategies to place their bolts and spouts on to the nodes.  Making it configurable per topology allows for the topologies to find one that works better for them, and more importantly it gives them an upgrade path from a previous scheduler that is not resource aware, and can be running in a modified form as a strategy and strategies that are.  "
STORM-895,Explore different Resource Aware scheduling strategies,"optimal scheduling in by itself an NP Hard problem.  Add in the fact that users can launch/kill topologies at any point in time is now the halting problem too.  So the best we can do is use heuristics to try and get decent results.

We should explore different heuristics around placing bolts/spout on different nodes in a cluster to try and see which ones will work best for different situations."
STORM-893,Resource Aware Scheduling,"At Yahoo we have been working on resource aware scheduling in storm, based off of some work done in academia.  This rollup ticket is to track the complete project.  With several sub tasks.  Some that are already done and need to be pushed back, and others that we have not started on yet."
STORM-892,Worker Metrics Collection Improvements,"Heron has a dedicated metrics collection process per host/topology.  In our current setup metrics collection is not always consistent, especially on systems  and bolts that are overloaded.  We should explore if the benefits of collecting the metrics differently and decide if there is a better way to do it.  Especially after STORM-885, we could potentially use it as a starting point for this process and have it pre-aggregate the metrics/heartbeats that are sent to nimbus for scheduling purposes, while forwarding on other metrics to different systems for alerting and monitoring."
STORM-891,Should we have one JVM per bolt/spout?,"In Heron each bolt/spout instance runs in a separate JVM instance.  This reduces the failure domain for each bolt/spout to itself, and also would allow for finer grained measurements of resource usage, which would be very useful in resource aware scheduling and elasticity. 

We should explore what performance impact this might have on storm, and what it would take to truly support it."
STORM-890,Should we move the worker to java?,"Both JStorm and Heron have the worker running in pure java with noticable performance improvements.  Java is also a lot easier for new developers to pick up on.  We should profile the worker and at a minimum move slow portions to java, and possible most/all of it."
STORM-889,Worker Performance Optimizations,The Heron paper showed that there is a lot of potential for optimization in storm.  We should explore what Heron did around the worker and see if it can be a benefit to us.
STORM-888,Nimbus Scaling: very large topologies,"While doing load testing of a heartbeat server implementation we found that nimbus started to struggle when scheduling a very large highly connected topology.

[~knusbaum] perhaps you can give some details on the topology you used, but it was three levels spout, and two levels of bolts.  The bolts were very highly connected with separate streams for each connection.

We should profile nimbus while it is doing this and see if we can determine the bottleneck to fix it."
STORM-887,Nimbus Scaling: Thousands of topologies,"As more and more topologies are launched interacting with nimbus to start, kill, rebalance topologies get slower and slower.  Anecdotally at 500 nodes it was upwards of 30 seconds to launch a topology.  We should analyze why this is happening and determine if it is still an issue, and if so what is causing it, then hopefully fix it."
STORM-884,Dist Cache: don't let blobstore users remove themselves from ACL,"We shouldn't let a blob store user remove read/write/admin privileges for themselves from an ACL, just so a typo cannot make the blob unrecoverable without an admin."
STORM-883,Dist Cache: Have supervisor throttle downloads to a set speed,To avoid overloading the network of a supervisor we should limit the average speed that data is downloaded for the dist cache.
STORM-882,Dist Cache: Have HDFS blob store download starting at a random block,"To reduce the possibility of overloading a datanode when downloading large dist cache items we should start downloading the blob at a random block, and then loop back to the beginning of the file to finish.  STORM-881 provides randomizing the order of the files downloaded, but on an update of a large blob this may become important."
STORM-881,Dist Cache: Randomize the order of blobs downloaded,For a large topology we can easily do a DDOS on a single node if all of the supervisors are in sync as they try to download blobs at the same time.  It would be good to randomize the order of the blobs being downloaded so we are not likely to all be hitting the same nodes at the exact same time.
STORM-880,Dist Cache: allow files already in HDFS to be used by HDFS blob store,"When operating on a shared cluster it would be nice to write data directly to hdfs instead of using the blob store API.  To make this work we would have to allow special keys for read that would be the full path to the HDFS file, and we would need to be extra careful that we access the file as the user trying to read it, instead of as the supervisor user so that permissions are setup properly."
STORM-879,Dist Cache: Quota/Usage Metrics Support,"In a multi-tenant environment it really would be nice to have some sort of quotas for the blob store.  Per User quotas are a hard because we only support ACLs, so being able to associate it with an owner is difficult.  Perhaps the best thing to do is to limit the total blob store size, and the size of individual items.  But also keep track of metrics on how frequently a blob store entry is used, any by whom.  That way the cluster owner can know what unused items to that can be deleted."
STORM-878,Dist Cache: UI Displays current version of blobs on supervisor,It would really be nice to have a way to track the version of a dist cache item that is currently on a node.  That way they know if it has been updated or not for debugging purposes. I'm not sure how much work this really would end up being.  If there are not very many dist cache items then we can possibly include it in the heartbeat.  If there are lots we may need to figure out another way to get the information less frequently.
STORM-877,Dist Cache: Worker API to know when blob has changed,"End users need a good way to detect that a dist cache item has changed, so that if they have anything cached."
STORM-875,404 Not Found responded when issuing request '/api/v1/token',"I want to upload a topology via a new added REST service '/api/v1/uploadTopology' introduced since 0.10. STORM-615 mentions a ring-session token be required before uploading the topology. Here is what I get when issuing command 'curl -i --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://localhost:8080/api/v1/token' in a shell:

HTTP/1.1 404 Not Found
Date: Thu, 18 Jun 2015 08:52:48 GMT
Content-Type: text/html; charset=utf-8
Content-Length: 14
Server: Jetty(6.1.26)

Page not found
 
That blocks me from submitting topologies via the exposed REST interface."
STORM-870,Tuples emitted just before spout restarts are not acked/failed,"The tuples emitted just before the spout restarts are never acked/failed after the tuple tree completes. The spout never gets notified but the worker log for the spout has the below log during its restart.

2015-06-18T14:47:01.430+0000 b.s.m.n.Client [ERROR] connection to Netty-Client-localhost/127.0.0.1:6701 is unavailable
2015-06-18T14:47:01.433+0000 b.s.m.n.Client [DEBUG] successfully connected to localhost/127.0.0.1:6701, [id: 0x9a243253, /127.0.0.1:36638 => localhost/127.0.0.1:6701] [attempt 1]
2015-06-18T14:47:01.433+0000 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-localhost/127.0.0.1:6701
"
STORM-869,kafka spout cannot fetch message if log size is above fetchSizeBytes,"let's say maxFetchSizeBytes is set to 1 megabytes, then if there exists a message that is bigger than 1 m, kafka spout just hangs and become inactive.
This is both happening in Kafka spout for bolt/spout topology and also in trident spouts."
STORM-868,Nimbus not starting Storm supervisor running on another machine,"We have multi-machine storm cluster. Over storm UI we can see total number of supervisor equal to total number of machine. I thought, nimbus should restart supervisor of another machine of same cluster when it die. Nimbus is able to restart supervisor of same machine but not able restart of another machine. Please advise what is missing ?"
STORM-865,Reflection in send path,"I was doing some profiling and found that reflection was happening in the send path, and was having a significant impact to performance.

java.lang.Class.getMethods()
clojure.lang.Reflector.getMethods()
clojure.lang.Reflector.invokeInstanceMethod()
backtype.storm.daemon.worker$mk_transfer_fn$transfer_fn__3760.invoke()
backtype.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__3411.invoke()

I'm not totally sure which line it is coming from, but the only java method call that does not have a type hint already appears to be
https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/daemon/executor.clj#L279
"
STORM-863,UI report TTransportException,
STORM-861,"Rebalance REST API : ""executors"" in rebalanceOptions doesn't seem to be reflected while rebalancing","There's one topology in cluster, and it is storm.starter.RollingTopWords from storm-starter.
https://github.com/apache/storm/blob/master/examples/storm-starter/src/jvm/storm/starter/RollingTopWords.java

I just ran rebalance via REST API, and found something is not working properly.

{noformat}
curl -X POST -H ""Content-Type: application/json"" -d '{""rebalanceOptions"": {""numWorkers"": 2, ""executors"": {""wordGenerator"": 10, ""counter"": 7, ""intermediateRanker"": 10}}}' http://localhost:8080/api/v1/topology/production-topology-4-1433927082/rebalance/50
{noformat}

It changes number of workers to 2, but it doesn't change any components'  executor count.

Sample of rebalance REST API from document is here.
{noformat}
curl  -i -b ~/cookiejar.txt -c ~/cookiejar.txt -X POST  
-H ""Content-Type: application/json"" 
-d  '{""rebalanceOptions"": {""numWorkers"": 2, ""executors"": { ""spout"" : ""5"", ""split"": 7, ""count"": 5 }}, ""callback"":""foo""}' 
http://localhost:8080/api/v1/topology/wordcount-1-1420308665/rebalance/0
{noformat}"
STORM-858,nimbus start failed due to ThriftServer startup NPE,"When I run storm with master, I got NPE when nimbus start in ThriftServer
```
5213 [main] ERROR b.s.s.a.ThriftServer - ThriftServer is being stopped due to: java.lang.NullPointerException
java.lang.NullPointerException
        at backtype.storm.security.auth.AuthUtils.get(AuthUtils.java:271) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.getServerTransportFactory(KerberosSaslTransportPlugin.java:77) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.security.auth.SaslTransportPlugin.getServer(SaslTransportPlugin.java:71) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.security.auth.ThriftServer.serve(ThriftServer.java:70) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:1370) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.daemon.nimbus$_launch.invoke(nimbus.clj:1394) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.daemon.nimbus$_main.invoke(nimbus.clj:1417) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at clojure.lang.AFn.applyToHelper(AFn.java:152) [clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.nimbus.main(Unknown Source) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
```
After add some debug log, I found that in AuthUtils::getConfiguration, It get ""java.security.auth.login.config"" from storm_conf, but there is never someone had put ""java.security.auth.login.config"" to storm_config,(https://github.com/apache/storm/blob/master/storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java#L55), and then login_conf will always be null, so (https://github.com/apache/storm/blob/master/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java#L77) will got NPE;
"
STORM-855,Add tuple batching,"In order to increase Storm's throughput, multiple tuples can be grouped together in a batch of tuples (ie, fat-tuple) and transfered from producer to consumer at once.

The initial idea is taken from https://github.com/mjsax/aeolus. However, we aim to integrate this feature deep into the system (in contrast to building it on top), what has multiple advantages:
  - batching can be even more transparent to the user (eg, no extra direct-streams needed to mimic Storm's data distribution patterns)
  - fault-tolerance (anchoring/acking) can be done on a tuple granularity (not on a batch granularity, what leads to much more replayed tuples -- and result duplicates -- in case of failure)

The aim is to extend TopologyBuilder interface with an additional parameter 'batch_size' to expose this feature to the user. Per default, batching will be disabled.

This batching feature has pure tuple transport purpose, ie, tuple-by-tuple processing semantics are preserved. An output batch is assembled at the producer and completely disassembled at the consumer. The consumer output can be batched again, however, independent of batched or non-batched input. Thus, batches can be of different size for each producer-consumer pair. Furthermore, consumers can receive batches of different size from different producers (including regular non batched input)."
STORM-852,Direct use of org.apache.log4j logger,"The following Java classes directly import and use Log4j APIs when they should be using the SLF4J Logger class.

storm-core:
  BlowfishTupleSerializer
  ShellProcess

storm-starter:
  RollingTopWords
  SkewedRollingTopWords
  IntermediateRankingsBolt
  AbstractRankerBolt
  TotalRankingsBolt
  RollingCountBolt
  RollingCountAggBolt



"
STORM-846,ShellBolt throws java.lang.RuntimeException: Anchored onto <TupleId> after ack/fail,"charlie quillard reports about strange behavior on multilang bolt.

{quote}
I have a problem with my shellbolt, which uses a cpp bolt(with this wrapper : http://demeter.inf.ed.ac.uk/cross/stormcpp.html ) with the multilang module. This one dies because the shellBolt's attribute ""_inputs"" is empty and call the next runtime exception : ""Anchored onto #Anchor after ack/fail"" .

The log: http://pastebin.com/pG9mHt8X
{quote}

"
STORM-844,[storm-redis] access modifier for fields seems too open,"Some classes in storm-redis have fields which are having too open access modifier.
One of these is Options class, though we can treat it to data structure and allow public fields.
(Better to have builder class?)"
STORM-840,My supervisor crashes when I kill a topology,"Hello,
I run 3 topologies inside my cluster.
Sometimes, when I kill one of them (not one specific). One supervisor goes down and restart. After few restart, it become stable.
The topology process is in ""Zombie state"" in the process list.

In version 0.9.3, all the supervisors crashed and couldn't restart. To resolve this, I had to ""rm -fr <storm-local-dir>/workers/""
So I migrate to 0.9.4 (I thought that was STORM-682).

Now it continues but no all the times, but occasionally.

I have these logs inside supervisor.log:
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Removing code for storm id nlp-11-1432906756
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Removing code for storm id nlp-11-1432906756
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down and clearing state for id 355af307-fafc-43a8-865d-0dfbf9baee33. Current supervisor time: 1432911702. State: :disallowed, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1432911702, :storm-id ""nlp-11-1432906756"", :executors #{[2 2] [3 3] [-1 -1] [1 1]}, :port 6700}
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down and clearing state for id 355af307-fafc-43a8-865d-0dfbf9baee33. Current supervisor time: 1432911702. State: :disallowed, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1432911702, :storm-id ""nlp-11-1432906756"", :executors #{[2 2] [3 3] [-1 -1] [1 1]}, :port 6700}
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down 90f0964b-c48c-4cbc-9d1c-57119c56e99c:355af307-fafc-43a8-865d-0dfbf9baee33
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down 90f0964b-c48c-4cbc-9d1c-57119c56e99c:355af307-fafc-43a8-865d-0dfbf9baee33
2015-05-29 15:01:42 b.s.event [ERROR] Error when processing event
java.io.IOException: Cannot run program ""kill"" (in directory "".""): error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) ~[na:1.8.0_45]
        at java.lang.Runtime.exec(Runtime.java:620) ~[na:1.8.0_45]
        at org.apache.commons.exec.launcher.Java13CommandLauncher.exec(Java13CommandLauncher.java:58) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.launch(DefaultExecutor.java:254) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:319) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:160) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:147) ~[commons-exec-1.1.jar:1.1]
        at backtype.storm.util$exec_command_BANG_.invoke(util.clj:386) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$send_signal_to_process.invoke(util.clj:415) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$kill_process_with_sig_term.invoke(util.clj:426) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:197) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:267) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:40) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.forkAndExec(Native Method) ~[na:1.8.0_45]
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:248) ~[na:1.8.0_45]
        at java.lang.ProcessImpl.start(ProcessImpl.java:134) ~[na:1.8.0_45]
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ~[na:1.8.0_45]
        ... 19 common frames omitted
2015-05-29 15:01:42 b.s.event [ERROR] Error when processing event
java.io.IOException: Cannot run program ""kill"" (in directory "".""): error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) ~[na:1.8.0_45]
        at java.lang.Runtime.exec(Runtime.java:620) ~[na:1.8.0_45]
        at org.apache.commons.exec.launcher.Java13CommandLauncher.exec(Java13CommandLauncher.java:58) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.launch(DefaultExecutor.java:254) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:319) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:160) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:147) ~[commons-exec-1.1.jar:1.1]
        at backtype.storm.util$exec_command_BANG_.invoke(util.clj:386) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$send_signal_to_process.invoke(util.clj:415) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$kill_process_with_sig_term.invoke(util.clj:426) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:197) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:267) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:40) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.forkAndExec(Native Method) ~[na:1.8.0_45]
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:248) ~[na:1.8.0_45]
        at java.lang.ProcessImpl.start(ProcessImpl.java:134) ~[na:1.8.0_45]
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ~[na:1.8.0_45]
        ... 19 common frames omitted
2015-05-29 15:01:42 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:48) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
2015-05-29 15:01:42 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:48) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down supervisor 90f0964b-c48c-4cbc-9d1c-57119c56e99c
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down supervisor 90f0964b-c48c-4cbc-9d1c-57119c56e99c
2015-05-29 15:01:42 b.s.event [INFO] Event manager interrupted
2015-05-29 15:01:42 b.s.event [INFO] Event manager interrupted
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:host.name=storm-supervisor-01
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.version=1.8.0_45
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.vendor=Oracle Corporation
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.home=/usr/lib/jvm/jre-8-oracle-x64/jre
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.class.path=/usr/share/apache-storm-0.9.4/lib/zookeeper-3.4.6.jar:/usr/share/apache-storm-0.9.4/lib/hiccup-0.3.6.jar:/usr/share/apache-storm-0.9.4/lib/chill-java-0.3.5.jar:/usr/share/apache-storm-0.9.4/lib/commons-exec-1.1.jar:/usr/share/apache-storm-0.9.4/lib/tools.macro-0.1.0.jar:/usr/share/apache-storm-0.9.4/lib/jgrapht-core-0.9.0.jar:/usr/share/apache-storm-0.9.4/lib/ring-servlet-0.3.11.jar:/usr/share/apache-storm-0.9.4/lib/clout-1.0.1.jar:/usr/share/apache-storm-0.9.4/lib/storm-core-0.9.4.jar:/usr/share/apache-storm-0.9.4/lib/asm-4.0.jar:/usr/share/apache-storm-0.9.4/lib/tools.cli-0.2.4.jar:/usr/share/apache-storm-0.9.4/lib/disruptor-2.10.1.jar:/usr/share/apache-storm-0.9.4/lib/log4j-over-slf4j-1.6.6.jar:/usr/share/apache-storm-0.9.4/lib/clj-time-0.4.1.jar:/usr/share/apache-storm-0.9.4/lib/slf4j-api-1.7.5.jar:/usr/share/apache-storm-0.9.4/lib/clojure-1.5.1.jar:/usr/share/apache-storm-0.9.4/lib/core.incubator-0.1.0.jar:/usr/share/apache-storm-0.9.4/lib/json-simple-1.1.jar:/usr/share/apache-storm-0.9.4/lib/logback-classic-1.0.13.jar:/usr/share/apache-storm-0.9.4/lib/servlet-api-2.5.jar:/usr/share/apache-storm-0.9.4/lib/logback-core-1.0.13.jar:/usr/share/apache-storm-0.9.4/lib/jetty-6.1.26.jar:/usr/share/apache-storm-0.9.4/lib/clj-stacktrace-0.2.2.jar:/usr/share/apache-storm-0.9.4/lib/ring-devel-0.3.11.jar:/usr/share/apache-storm-0.9.4/lib/minlog-1.2.jar:/usr/share/apache-storm-0.9.4/lib/kryo-2.21.jar:/usr/share/apache-storm-0.9.4/lib/compojure-1.1.3.jar:/usr/share/apache-storm-0.9.4/lib/commons-codec-1.6.jar:/usr/share/apache-storm-0.9.4/lib/tools.logging-0.2.3.jar:/usr/share/apache-storm-0.9.4/lib/ring-jetty-adapter-0.3.11.jar:/usr/share/apache-storm-0.9.4/lib/jetty-util-6.1.26.jar:/usr/share/apache-storm-0.9.4/lib/joda-time-2.0.jar:/usr/share/apache-storm-0.9.4/lib/jline-2.11.jar:/usr/share/apache-storm-0.9.4/lib/commons-logging-1.1.3.jar:/usr/share/apache-storm-0.9.4/lib/reflectasm-1.07-shaded.jar:/usr/share/apache-storm-0.9.4/lib/carbonite-1.4.0.jar:/usr/share/apache-storm-0.9.4/lib/snakeyaml-1.11.jar:/usr/share/apache-storm-0.9.4/lib/objenesis-1.2.jar:/usr/share/apache-storm-0.9.4/lib/ring-core-1.1.5.jar:/usr/share/apache-storm-0.9.4/lib/commons-io-2.4.jar:/usr/share/apache-storm-0.9.4/lib/commons-fileupload-1.2.1.jar:/usr/share/apache-storm-0.9.4/lib/math.numeric-tower-0.0.1.jar:/usr/share/apache-storm-0.9.4/lib/commons-lang-2.5.jar:/usr/share/apache-storm-0.9.4/conf
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.library.path=/usr/local/lib:/opt/local/lib:/usr/lib
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.io.tmpdir=/tmp
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.compiler=<NA>
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:os.name=Linux
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:os.arch=amd64
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:os.version=3.16.0-0.bpo.4-amd64
...
"
STORM-838,Upgrade Jackson,"Currently the project is using a very old Jackson version (2.3.1). There are already many new (and better versions:

http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22com.fasterxml.jackson.core%22%20AND%20a%3A%22jackson-core%22

So it'd be great to upgrade to 2.5.3.

I can provide a patch if you agree on the request."
STORM-836,DPRC request failed after DRPC server restarted,"When the DRPC server was killed and then restarted, the DRPC client would throw a DRPCExecutionException (msg:Request failed) for a while, off and on.

Here is my client code:
{code:java}
DRPCClient client = new DRPCClient(""hd181"", 3772);

for (int i = 0; i < 100; ++i) {
	System.out.println(client.execute(""drpcFunc"", ""aaa""));
}
{code}

and the error messages:
{code}
Exclamation-6:aaa!!!
Exception in thread ""main"" DRPCExecutionException(msg:Request failed)
	at backtype.storm.generated.DistributedRPC$execute_result.read(DistributedRPC.java:904)
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:78)
	at backtype.storm.generated.DistributedRPC$Client.recv_execute(DistributedRPC.java:92)
	at backtype.storm.generated.DistributedRPC$Client.execute(DistributedRPC.java:78)
	at backtype.storm.utils.DRPCClient.execute(DRPCClient.java:71)
	at com.enjoyor.storm.kafka.example.DRPCTest.main(DRPCTest.java:18)
{code}

As you can see, the client can get few valid results at beginning and then get a failed request.

This problem would last about 10 minutes after restarting the server, and then the server would be back to normal."
STORM-834,Class cast exception Object to Iterable,"This exception is happening after some hours of running. Unfortunately, nothing in the trace suggests the source of the error for further investigation.

2015-05-23T13:24:22.343-0400 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.daemon.executor$fn__3441$fn__3453$fn__3500.invoke(executor.clj:748) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.util$async_loop$fn__464.invoke(util.clj:463) ~[storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
Caused by: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
        at backtype.storm.util$get_iterator.invoke(util.clj:867) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__3364.invoke(executor.clj:397) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.disruptor$clojure_handler$reify__1447.onEvent(disruptor.clj:58) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:120) ~[storm-core-0.9.3.jar:0.9.3]
        ... 6 common frames omitted
"
STORM-830,Integer value of backtype.storm.Config is converted to Long during runtime.,"refer to https://github.com/StarWindMoonCloud/storm-test
When run ProcessBoltUnitTest, it's expected.
But ""java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer"" showed up when running TestTopologyJava"
STORM-828,"HdfsBolt takes a lot of configuration, need good defaults","The following is code from https://github.com/apache/storm/blob/master/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/HdfsFileTopology.java representing the amount of configuration required to use the HdfsBolt. 
{code}
        // sync the filesystem after every 1k tuples
        SyncPolicy syncPolicy = new CountSyncPolicy(1000);

        // rotate files every 1 min
        FileRotationPolicy rotationPolicy = new TimedRotationPolicy(1.0f, TimedRotationPolicy.TimeUnit.MINUTES);

        FileNameFormat fileNameFormat = new DefaultFileNameFormat()
                .withPath(""/tmp/foo/"")
                .withExtension("".txt"");

        RecordFormat format = new DelimitedRecordFormat()
                .withFieldDelimiter(""|"");

        Yaml yaml = new Yaml();
        InputStream in = new FileInputStream(args[1]);
        Map<String, Object> yamlConf = (Map<String, Object>) yaml.load(in);
        in.close();
        config.put(""hdfs.config"", yamlConf);

        HdfsBolt bolt = new HdfsBolt()
                .withConfigKey(""hdfs.config"")
                .withFsUrl(args[0])
                .withFileNameFormat(fileNameFormat)
                .withRecordFormat(format)
                .withRotationPolicy(rotationPolicy)
                .withSyncPolicy(syncPolicy)
                .addRotationAction(new MoveFileAction().toDestination(""/tmp/dest2/""));
{code}

This is way too much.  If it were just an example showing all of the possibilities that would be OK but of the 8 lines used in the construction of the bolt, 5 of them are required or the bolt will blow up at run time.  We should provide reasonable defaults for everything that can have a reasonable default.  And required parameters should be passed in through the constructor, not as builder arguments.  I realize we need to maintain backwards compatibility so we may need some new Bolt definitions.

{code}
HdfsTSVBolt bolt = new HdfsTSVBolt(outputDir);
{code}

If someone wanted to sync every 100 records instead of every 1000 we could do

{code}
TSVFileBolt bolt = new TSVFileBolt(outputDir).withSyncPolicy(new CountSyncPolicy(100))
{code}

I would like to see a base HdfsFileBolt that requires a record format, and an output directory.  It would have defaults for everything else.  Then we could have a TSVFileBolt and CSVFileBolt subclass it and ideally SequenceFileBolt as well."
STORM-825,"As a storm developer I’d like consistent, structured exception handling",
STORM-824,As a storm developer I’d like all loggers to be private,
STORM-823,As a storm developer I’d like to use ITuple interface consistently to avoid duplication ,
STORM-817,Kafka Wildcard Topic Support,"Creating a feature request for supporting Wildcard Topic's for Kafka Spout.  

We want to be able to run a aggregation stream for data coming from all tenants. Tenants get added dynamically. So new kafka topics get created. All the topics will be matching a regex pattern. 
example:
clickstream:tenant1:log
clickstream:tenant2:log
clickstream:tenant3:log
Storm code should be able to perform auto-discovery, and should be able to to fetch from newly created topics in run time.

"
STORM-815,Auto tuning topologies based on user submitted SLA,"Hi all – not sure if this is the place for this or not, so please feel free to move this ticket around.

I have been working on making Storm SLA aware and wanted to create a ticket focused on auto-tuning of submitted topologies. Having talked with [~revans2] , I think this feature will go a long way in improving the usability of Storm.

This will be particularly useful as more abstractions are developed on top of Storm and developers move further away from the internals of topologies with systems like Summingbird (https://github.com/twitter/summingbird). Storm already has the majority of the metrics required to made decisions regarding the stability of a given stream in Zookeeper; these can be used to decide if a given SLA is being met.

The system I'm working on functions externally to Storm, getting measurements from the Thrift interface and then performing parallelism level tuning through the “rebalance” command. If the community is interested, I can upload a document in the upcoming weeks highlighting the challenges and benefits of an auto-tuning system in Storm and that can serve as a first start for developing an auto-tuning scheduler.
"
STORM-814,Kafka Spout performance,"I am running few test for storm topology with kafka. 

Created a topic with 16 partitions and emitted 10k messages/sec with each message 1k size.

When I set the kafka spout parallelism to 1, I am getting latency of 8.9 ms

but  when i increase the parallelism to 8, I am getting latency of 1.2 sec.

Settings : 
MAX_SPOUT SPENDING set to 1000-5000

String zkConnString = ""zookeeper1:2181"";
        String kafkaTopic = ""messages"";
        String zkRootPath = ""/kafkaStorm"";
        String zkOffSetID = ""kafka"";
"
STORM-812,Create Testing#createLocalCluster,"Currently, there is a method called {{Testing#withLocalCluster}} that allows one to customize the configuration of a {{LocalCluster}} that isn't possible using {{new LocalCluster}}. In fact, as far as I can tell this is the only way to specify the number supervisors and number of workers per supervisor. However, this method controls the life-cycle of the cluster and thus is very inconvenient when that is not desired. This has been discussed in 

http://storm-user.narkive.com/FPgchFEL/localcluster-topology-not-working-is-there-a-max-number-of-topologies-to-deploy

The workaround required is less than ideal:

{code}
  @NonNull
  public static LocalCluster createLocalCluster(String zkHost, long zkPort) {
    val daemonConf = new Config();
    daemonConf.put(Config.TOPOLOGY_ACKER_EXECUTORS, 0);
    daemonConf.put(Config.STORM_ZOOKEEPER_SERVERS, ImmutableList.of(zkHost));
    daemonConf.put(Config.STORM_ZOOKEEPER_PORT, zkPort);

    val clusterParams = new MkClusterParam();
    clusterParams.setSupervisors(5);
    clusterParams.setPortsPerSupervisor(5);
    clusterParams.setDaemonConf(daemonConf);

    return createLocalCluster(clusterParams);
  }

  /**
   * Hack to override local cluster workers.
   */
  @SneakyThrows
  private static LocalCluster createLocalCluster(final MkClusterParam clusterParams) {
    val reference = new AtomicReference<LocalCluster>();
    val latch = new CountDownLatch(1);
    val thread = new Thread(new Runnable() {

      @Override
      public void run() {
        Testing.withLocalCluster(clusterParams,
            new TestJob() {

              @Override
              @SneakyThrows
              public void run(final ILocalCluster cluster) {
                reference.set((LocalCluster) cluster);
                latch.countDown();

                // Wait forever
                synchronized (this) {
                  while (true) {
                    this.wait();
                  }
                }
              }

            });

      }

    });

    thread.setDaemon(true);
    thread.start();

    latch.await();
    return reference.get();
  }

}
{code}

For greater flexibility, a {{createLocalCluster}} should be created."
STORM-805,Supervisor crashes with 'org.apache.commons.io.FileExistsException: Destination already exists' error,"Version : 0.9.4

All of a sudden supervisor crashed with error log below.

2015-04-29 16:48:24 b.s.d.supervisor [INFO] 6d035728-bb86-486e-847e-94cd193a51db still hasn't started
2015-04-29 16:48:24 b.s.event [ERROR] Error when processing event
org.apache.commons.io.FileExistsException: Destination '/somepath/storm-6-1430100813' already exists
        at org.apache.commons.io.FileUtils.moveDirectory(FileUtils.java:2748) ~[commons-io-2.4.jar:2.4]
        at backtype.storm.daemon.supervisor$fn__5554.invoke(supervisor.clj:489) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.MultiFn.invoke(MultiFn.java:241) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.supervisor$mk_synchronize_supervisor$this__5474.invoke(supervisor.clj:374) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:40) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
2015-04-29 16:48:24 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:48) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
2015-04-29 16:48:24 b.s.d.supervisor [INFO] Shutting down supervisor 4f00962b-2af0-4922-97f8-142db00fbddf"
STORM-802,Multilang tests throw NoOutputException with Travis CI,"Currently Storm build tests are passed via Travis CI, but if we take a look to output log, Multilang tests doesn't work correctly.

https://travis-ci.org/apache/storm/jobs/60254925
https://s3.amazonaws.com/archive.travis-ci.org/jobs/60254925/log.txt

You can find ""PIPE to subprocess"" and got it.

Travis CI installs recent ruby and nodejs at startup, so it isn't about language version."
STORM-798,storm-kafka tests are failing intermittently,"I'm trying to adopt Travis CI and observed failures about storm-kafka test.

https://travis-ci.org/apache/storm/jobs/59795630

Full log is here.
https://s3.amazonaws.com/archive.travis-ci.org/jobs/59795629/log.txt

{noformat}
Tests in error: 
  testKeyValue(storm.kafka.TridentKafkaTest): Could not send message with key = key-123 to topic = test
  generateTuplesWithKeyAndKeyValueScheme(storm.kafka.KafkaUtilsTest): Error fetching data from [Partition{host=localhost:52047, partition=0}] for topic [testTopic]: [OFFSET_OUT_OF_RANGE]
{noformat}

Please note that tests run with VM, which could be slow at moment or while testing.
It would be better to let tests pass from slow box without random failure."
STORM-792,Missing documentation in backtype.storm.generated.Nimbus,Explain the difference between Nimbus$Client.getTopology(String id) and Nimbus$Client.getUserTopology(String id)
STORM-785,Have UI know about trident and display things cleanly,"The current UI does not know about trident and so being able to understand where your code is running, and what is happening can be very confusing.  It would be great to update the UI, and possible some other parts of core storm itself, to better display trident topologies."
STORM-784,Trident Docs: javadoc and markdown for bultin operations,"There are a number of very useful bultin trident operations that are not documented in the markdown trident documentation (just mentioning they exist and the package they are in might be enough).

Also it would be good to add javadocs for all of them.  They are fairly obvious what they do from reading the code, but it would be nice to not require end users to read the code for them. "
STORM-783,Trident Docs: javadoc for operations,"Most of the operation Interfaces and base classes are very straight forward, but none of them have javadocs which makes working with them using and IDE less useful.  It would be good to describe what each of the Operation types does, how to use it, perhaps pointing to some examples of how it is used, and also for more complex ones like the aggregations what context each of the methods are called in."
STORM-782,Trident Docs: javadoc for IBackingMap,"IBackingMap is a simple interface, but there is not documentation for it at all.  It would be nice to know basics about how it works, and how it fits into other things like OpaqueMap, MicrobatchIbackingMap, CachedMap, etc.  the other classes may need some documentation updates too.

It would also be nice to explain what context the methods are called in.  Are they serialized out through java serialization and there could be multiple instances of it running?  How does all of this work?"
STORM-781,Trident Docs: javadoc MemoryMapState,MemoryMapState is what everyone reads to try and understand how states work.  we should have really good javadocs and comments describing how it works so when devs look at it they quickly understand what it is doing and why.
STORM-780,Trident Docs: javadoc for IPartitionedTridentSpout,"Improve the documentation for the spout interface to describe when methods are called, who calls them, what each method should do and what the return values should be like"
STORM-779,Trident Docs: javadoc for IOpaquePartitionedTridentSpout,"Improve the javadocs for the IOpaquePartitionedTridentSpout.  Describe better how metadata is handled and document each api to say when it is called, who calls it what should happen in the method and what the return values should be like."
STORM-778,Trident Docs: javadoc for ITridentSpout,"Add javadocs for ITridentSpout describing what each method should do, when it is called, and what the return values mean."
STORM-777,Trident Docs: debugging,"It would be great to have documentation about how to debug a trident topology, when it is not working as expected, performance tuning is separate from this. "
STORM-776,Trident Docs: Tuning and Optimizations,It would be great to have a page that could talk a bit about how to tune a trident topology and optimize.
STORM-775,Trident Docs: commits and coord,How coordinators work and how commits happen is not really documented anywhere right now.  It would be good to have a complete explanation about how coordinators work and how commits are coordinated within a trident topology.
STORM-774,Improve Trident Documentation,"The documentation for trident is lacking in a lot of ways.  The goal for this umbrella ticket is to have several different tasks underneath it to make writing, debugging and tuning a trident topology/spout/state as straight forwards as possible. 

For some of this we need to add javadocs to a lot of the Trident interfaces and base classes.  For other things we need to document with examples how these things work."
STORM-771,Authentication with Kerberos,"I am using Storm in a Kerberized Cluster. 
There is an user ""Robin"" in the Storm server. And I follow the steps below to generate keytab for Robin.
{noformat}
# /usr/sbin/kadmin.local
# kadmin.local: addprinc -randkey Robin@EXAMPLE.COM
# kadmin.local: xst -norandkey -k Robin.keytab Robin
# scp Robin.keytab Robin@storm_server:/home/Robin
{noformat}
After these, I login the Storm server as Robin. And authenticate Robin with his own keytab(Robin.keytab)
{noformat}
# kinit -k -t Robin.keytab Robin
{noformat}
The output of klist is
{noformat}
Ticket cache: FILE:/tmp/krb5cc_1006
Default principal: Robin@EXAMPLE.COM

Valid starting     Expires            Service principal
04/15/15 11:34:19  04/16/15 11:34:19  krbtgt/EXAMPLE.COM@EXAMPLE.COM
        renew until 04/15/15 11:34:19
{noformat}

But there was an authentication error occurred when I executed 
{noformat}
#storm list
{noformat}
The error was
{noformat}
Exception in thread ""main"" java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:108)
        at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48)
        at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:97)
        at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:66)
        at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:47)
        at backtype.storm.thrift$nimbus_client_and_conn.invoke(thrift.clj:71)
        at backtype.storm.command.list$_main.invoke(list.clj:22)
        at clojure.lang.AFn.applyToHelper(AFn.java:159)
        at clojure.lang.AFn.applyTo(AFn.java:151)
        at backtype.storm.command.list.main(Unknown Source)
Caused by: javax.security.auth.login.LoginException: No password provided
        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:878)
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719)
{noformat}

Here is my Kerberos settings in ""storm.yaml""
{noformat}
storm.principal.tolocal: ""backtype.storm.security.auth.KerberosPrincipalToLocal""
storm.zookeeper.superACL: ""sasl:storm""
java.security.auth.login.config: ""/etc/storm/conf/storm_jaas.conf""
nimbus.admins:
  - ""storm""
nimbus.supervisor.users:
  - ""storm""
nimbus.authorizer: ""backtype.storm.security.auth.authorizer.SimpleACLAuthorizer""
drpc.authorizer: ""backtype.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer""

ui.filter: ""org.apache.hadoop.security.authentication.server.AuthenticationFilter""
ui.filter.params:
  ""type"": ""kerberos""
  ""kerberos.principal"": ""HTTP/slave""
  ""kerberos.keytab"": ""/etc/security/keytabs/spnego.service.keytab""
  ""kerberos.name.rules"": ""DEFAULT""
supervisor.enable: true
{noformat}
And ""storm_jaas.conf""
{noformat}
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab=""/etc/security/keytabs/nimbus.service.keytab""
   storeKey=true
   useTicketCache=false
   principal=""nimbus/slave@EXAMPLE.COM"";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab=""/etc/security/keytabs/storm.service.keytab""
   storeKey=true
   useTicketCache=false
   serviceName=""nimbus""
   principal=""storm@EXAMPLE.COM"";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab=""/etc/security/keytabs/storm.service.keytab""
   storeKey=true
   useTicketCache=false
   serviceName=""zookeeper""
   principal=""storm@EXAMPLE.COM"";
};
{noformat}

By the way, the cluster is installed via Ambari 1.7.

Thanks in advanced."
STORM-770,NullPointerException in consumeBatchToCursor,"We got the following exception after our topology had been up for ~2 days, and I was wondering if it might be related. 
Looks like ""task"" in ""mk-transfer-fn"" is null, making ""(.add remote (TaskMessage. task (.serialize serializer tuple)))"" fail on NPE (worker.clj:128, storm-core-0.9.2-incubating.jar)

java.lang.RuntimeException: java.lang.NullPointerException
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$consume_loop_STAR_$fn__758.invoke(disruptor.clj:94) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.7.0_72]
Caused by: java.lang.NullPointerException: null
at clojure.lang.RT.intCast(RT.java:1087) ~[clojure-1.5.1.jar:na]
at backtype.storm.daemon.worker$mk_transfer_fn$fn__5748.invoke(worker.clj:128) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.daemon.executor$start_batch_transfer_GT_worker_handler_BANG$fn__5483.invoke(executor.clj:256) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$clojure_handler$reify__745.onEvent(disruptor.clj:58) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
... 6 common frames omitted,java.lang.RuntimeException: java.lang.NullPointerException

Any ideas?

P.S.
Also saw it here: 
http://mail-archives.apache.org/mod_mbox/storm-user/201501.mbox/%3CCABcMBhCusXXU=V1e66wfUATGYH1euQnd1SiOG65-Tp8xLWx0ww@mail.gmail.com%3E

https://mail-archives.apache.org/mod_mbox/storm-user/201408.mbox/%3CCAJuQM_4KXHSH2_X08ujuQR76m2C+Dswp0fCiJBmfCAeyqgsFHQ@mail.gmail.com%3E

Comment from Bobby
http://mail-archives.apache.org/mod_mbox/storm-user/201501.mbox/%3C574363643.2791948.1420470097280.JavaMail.yahoo@jws10027.mail.ne1.yahoo.com%3E

{quote}
What version of storm are you using?  Are any of the bolts shell bolts?  There is a known
issue where this can happen if two shell bolts share an executor, because they are multi-threaded. 
- Bobby
{quote}"
STORM-769,Storm can not identify JAVA_HOME if the of java home contains a space,"I installed java 1.7.0_75 in windows 7 through .exe file and setted the JAVA_HOME path which is ""D:\Program Files\Java\jdk1.7.0_75"".
Run the ""java -version"" command to make sure the jdk is ok.
After I setted up all environment variable about storm, I run the command ""storm version""
the terminate shows ""Error: JAVA_HOME is incorrectly set.""
So I delete following code in storm.cmd file
""
if not defined JAVA_HOME (
  set JAVA_HOME=c:\apps\java\openjdk7
)

if not exist %JAVA_HOME%\bin\java.exe (
  echo Error: JAVA_HOME is incorrectly set.
  goto :eof
)
""
and run ""storm version"" command again,
it shows that 'D:\Program' is not recognized as an internal or external command,operable program or batch file.
The storm.cmd script can not deal with a JAVA_HOME path which contains a space like ""D:\Program Files\Java\jdk1.7.0_75"".
"
STORM-767,Unresolved dependency in storm-hive,"Building storm-hive fails due to unresolved dependencies about {{pentaho-aggdesigner}}.

{code}
[ERROR] Failed to execute goal on project storm-hive: Could not resolve dependencies for project org.apache.storm:storm-hive:jar:0.11.0-SNAPSHOT: Could not find artifact org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.3-jhyde in repo.jenkins-ci.org (http://repo.jenkins-ci.org/public/) -> [Help 1]
{code}
"
STORM-759,Storm target for waiting for remote java debugger,"It would be really nice if there was a storm target like ""jar"", but one that would pause and wait for a remote java debugger so you could debug your topology while running locally.

Incidentally, here's a more or less trivial fix for this:
https://github.com/apache/storm/pull/510"
STORM-758,Very busy ShellBolt subprocess with Non-ACK mode cannot respond heartbeat just in time,"As [~dashengju] stated from STORM-738, very busy ShellBolt subprocess cannot respond heartbeat just in time.
Actually it's by design constraint (more details are on STORM-513 or STORM-738), but it's better to find a way to avoid.

My approach with ACK mode is here, STORM-742
This issue points Non-ACK mode. "
STORM-755,An exception occured while executing the Java class. fyp-storm-try.src.jvm.Topology ,"ubuntu@ip-10-0-0-101:~/storm/examples/fyp-storm-try$ sudo mvn exec:java -D storm.topology=fyp-storm-try.src.jvm.Topology
[INFO] Scanning for projects...
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:fyp-storm-try:jar:0.9.4
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-javadoc-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 694, column 21
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 660, column 21
[WARNING]
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING]
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING]
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building fyp-storm-try 0.9.4
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] >>> exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try >>>
[INFO]
[INFO] <<< exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try <<<
[INFO]
[INFO] --- exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try ---
[WARNING]
java.lang.ClassNotFoundException: fyp-storm-try.src.jvm.Topology
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:285)
        at java.lang.Thread.run(Thread.java:745)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.838s
[INFO] Finished at: Sun Apr 05 09:09:28 UTC 2015
[INFO] Final Memory: 9M/22M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project fyp-storm-try: An exception occured while executing the Java class. fyp-storm-try.src.jvm.Topology -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException


Below are my pom.xml:
?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
  <modelVersion>4.0.0</modelVersion>
  <parent>
      <artifactId>storm</artifactId>
      <groupId>org.apache.storm</groupId>
      <version>0.9.4</version>
      <relativePath>../../pom.xml</relativePath>
  </parent>

  <groupId>org.apache.storm</groupId>
  <artifactId>fyp-storm-try</artifactId>
  <packaging>jar</packaging>

  <name>fyp-storm-try</name>


  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.testng</groupId>
      <artifactId>testng</artifactId>
      <version>6.8.5</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-all</artifactId>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.easytesting</groupId>
      <artifactId>fest-assert-core</artifactId>
      <version>2.0M8</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.jmock</groupId>
      <artifactId>jmock</artifactId>
      <version>2.6.0</version>
      <scope>test</scope>
    </dependency>
    <dependency>
     <groupId>org.twitter4j</groupId>
     <artifactId>twitter4j-stream</artifactId>
     <version>3.0.3</version>
    </dependency>
    <dependency>
      <groupId>org.apache.storm</groupId>
      <artifactId>storm-core</artifactId>
      <version>${project.version}</version>
      <!-- keep storm out of the jar-with-dependencies -->
      <scope>provided</scope>
    </dependency>
    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.2.1</version>
    </dependency>
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
    </dependency>
  </dependencies>

  <build>
    <sourceDirectory>src/jvm</sourceDirectory>
    <testSourceDirectory>test/jvm</testSourceDirectory>
    <resources>
      <resource>
        <directory>${basedir}/multilang</directory>
      </resource>
    </resources>

    <plugins>
      <!--
        Bind the maven-assembly-plugin to the package phase
        this will create a jar file without the storm dependencies
        suitable for deployment to a cluster.
       -->
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
          <archive>
            <manifest>
              <mainClass />
            </manifest>
          </archive>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>com.theoryinpractise</groupId>
        <artifactId>clojure-maven-plugin</artifactId>
        <extensions>true</extensions>
        <configuration>
          <sourceDirectories>
            <sourceDirectory>src/clj</sourceDirectory>
          </sourceDirectories>
        </configuration>
        <executions>
          <execution>
            <id>compile</id>
            <phase>compile</phase>
            <goals>
              <goal>compile</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.2.1</version>
        <executions>
          <execution>
		   <phase>test</phase>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>java</executable>
          <includeProjectDependencies>true</includeProjectDependencies>
          <includePluginDependencies>false</includePluginDependencies>
          <classpathScope>compile</classpathScope>
          <mainClass>${storm.topology}</mainClass>
        </configuration>
      </plugin>
    </plugins>
  </build>
</project>
"
STORM-754,Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project fyp-storm-try: The parameters 'mainClass' for goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java are missing or invalid -> [Help 1],"ubuntu@ip-10-0-0-101:~/storm/examples/fyp-storm-try$ sudo mvn compile exec:java  
-D storm.topology=fyp.storm.try.Topology
[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:fyp-storm-try:jar:0.9.4
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-javadoc-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 694, column 21
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 660, column 21
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building fyp-storm-try 0.9.4
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ fyp-storm-try ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ fyp-storm-try ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/ubuntu/storm/examples/fyp-storm-try/multilang
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ fyp-storm-try ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 5 source files to /home/ubuntu/storm/examples/fyp-storm-try/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.3.18:compile (compile) @ fyp-storm-try ---
[INFO] 
[INFO] >>> exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try >>>
[INFO] 
[INFO] <<< exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try <<<
[INFO] 
[INFO] --- exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.364s
[INFO] Finished at: Wed Apr 01 12:43:59 UTC 2015
[INFO] Final Memory: 19M/51M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project fyp-storm-try: The parameters 'mainClass' for goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java are missing or invalid -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginParameterException"
STORM-751,Move javadoc aggregate to site phase.,"{code}

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm ............................................. FAILURE [5.904s]
[INFO] maven-shade-clojure-transformer ................... SKIPPED
[INFO] storm-maven-plugins ............................... SKIPPED
[INFO] Storm Core ........................................ SKIPPED
[INFO] storm-starter ..................................... SKIPPED
[INFO] storm-kafka ....................................... SKIPPED
[INFO] storm-hdfs ........................................ SKIPPED
[INFO] storm-hbase ....................................... SKIPPED
[INFO] storm-hive ........................................ SKIPPED
[INFO] storm-jdbc ........................................ SKIPPED
[INFO] storm-redis ....................................... SKIPPED
[INFO] Storm JMS ......................................... SKIPPED
[INFO] Storm Cassandra Support ........................... SKIPPED
[INFO] Storm Binary Distribution ......................... SKIPPED
[INFO] Storm Source Distribution ......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 6.826s
[INFO] Finished at: Fri Apr 03 14:12:15 EDT 2015
[INFO] Final Memory: 40M/2474M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project storm-kafka: Could not resolve dependencies for project org.apache.storm:storm-kafka:jar:0.10.0.2.3.0.0-1493: Could not find artifact org.apache.storm:storm-core:jar:0.10.0.2.3.0.0-1493 in public

{code}"
STORM-745,Second Commandline Parameter passed to the main class is skipped when run in windows,"Always the second parameter is getting skipped.

E:\target>storm jar StormZeroMQ.jar com.wipro.bdas.zeromq.ZMQTopology value1 value2 value3 value4 value5
Output
I=0 value=value1
I=1 value=value3
I=2 value=value4
I=3 value=value5

public class ZMQTopology {
public static void main(String[] args) throws AlreadyAliveException,
                                                InvalidTopologyException {

                            for(int i=0;i<(args.length);i++)
                            {   System.out.println(""I="" +i+ "" value=""+args[i]);
                            }

I am using the apache storm pre-built for windows.

After some amount of debugging I could find that it happens only with windows machine . I was able to reproduce the error in 2 windows machine. With both  0.9.3 and 0.9.4 .In Linux machine I could see command line parameters working perfectly.
"
STORM-744,downstream worker fail may lead to worker oom without limit pending TaskMessage  count,"storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
message_queue[i] = new LinkedBlockingQueue<ArrayList<TaskMessage>>(); // here
   "
STORM-743,Invalid JSON produced when serializing TaskInfo and DataPoint types,"In experimenting with a pure Node.js topology I attempted to write a simple logging consumer Bolt by merely observing the `__metrics` streams from my other existing components.

In doing so I observed that the message sent to my tasks is not valid json. For example:
```
{
    ""comp"": ""changes"",
    ""tuple"": [backtype.storm.metric.api.IMetricsConsumer$TaskInfo@28b63f13,[[__emit-count = {}],[__process-latency = {}],[__receive = {read_pos=0, write_pos=1, capacity=1024, population=1}],[__ack-count = {}],[__transfer-count = {}],[__execute-latency = {}],[__fail-count = {}],[__sendqueue = {read_pos=-1, write_pos=-1, capacity=1024, population=0}],[__execute-count = {}]]],
    ""task"":4,
    ""stream"":""__metrics"",
    ""id"":""1702055549821416448""
}
```

This seems to be able to be tracked directly to bad JSON serialization of TaskInfo and the collection of Datapoint [1].

(Thanks!)

[1] https://github.com/nathanmarz/storm/blob/cdb116e942666973bc4eaa0df098d5bab82739e7/storm-core/src/jvm/backtype/storm/metric/api/IMetricsConsumer.java"
STORM-739,If a worker dies supervisor clears local topology jar and re-downloads from nimbus,Currently if a worker dies supervisor clears the local state including the supervisor's local topology jar. This is causing supervisor to re-download the topology jar from nimbus which can bring down nimbus if a topology has exception causing it to kill worker jvms and this can get into a loop worker dies-> supervisor clears the state- > supervisor downloads jar from nimbus . This will increase the load on the nimbus and there is no need to clear the supervisor state.
STORM-738,Multilang needs Overflow-Control mechanism and HeartBeat timeout problem,"hi, all

we have a topology, which have 3 components(spout->parser->saver) and the parser is Multilang bolt with python. We do not use ACK mechanism.

we found 2 problems with Mutilang python script.
1) the parser python scripts may hold too many tuples and consume too many memory;
2) with MultiLang heartbeat mechanism described by  https://issues.apache.org/jira/browse/STORM-513, the python script always timeout to heartbeat, even when the parser bolt is normal, cause supervisor to restart itself.

!storm_multilang.png!

ShellBolt process === Father-Process
PythonScript process === Child-Process

The reason is :
1) when topology do not use ACK mechanism, the spout do not have Overflow-control ability, if the stream have too many tuples comes,  spout will send all the tuples to parser's ShellBolt process(Father-Process);
2) parser's ShellBolt process just put the tuples to _pendingWrites queue, if the _pendingWrites queue does not have limit;
3) parser's PythonScript process(Child-Process) call readMsg() to read a tuple from STDIN, handle the tuple, and emit a new tuple to its father process through STDOUT, and then call readTaskIds() from STDIN.  Because Father-Process's queue already have too many other tuples, Child-Process will read all the tuples to pending_commands, util received TaskIds.
4) so Child-Process process's pending_commands may contains too many tuples and consume too many memory.

As to heartbeat, because there are too many pending_commands need Child-Process to handle, and Child-Process's every emit operation will need more I/O read operations from STDIN. It may need 10 seconds to handle one tuple, and this will cause the heartbeat tuple not handle quickly, and timeout will happen.

Even if Father-Process's _pendingWrites have limits, for example 1000, Child-Process may needs 1000 x 1000 read operations then it can handle the heartbeat tuple.

[~revans2] [~kabhwan] this related to Multilang and heartbeat, please help to confirm the two problems.

I think Father-Process and Child-Process need Overflow-Control Protocol to control the python script's memory usage.
And heartbeat tuple needs a separate queue(pending_heartbeats), and Child-Process handle heartbeat tuple at high priority. [~kabhwan] wish to hear your opinion.
"
STORM-736,Add a RESTful API to  print all of the thread's information and stack traces of Nimbus/Supervisor/Worker Process,
STORM-733,ShellBolts that don't respond to heartbeats are not being killed,"In cases where a multilang bolt is stuck (say, an infinite loop), the heartbeats are supposed to detect the issue and kill the supervisor process.

In 0.9.3 this doesn't happen due to backtype.storm.utils.ShellProcess.getErrorsString() call in ShellBolt.die()

This call, which in turn executes IOUtils.toString(processErrorStream) will block the thread until process exits. Heartbeat flow should not assume process had exited."
STORM-732,PartitionManager should remove those messages not exists in kafka from waiting queue,
STORM-725,Topology does not start,"I am running a topology with 1 Nimbus, 4 supervisor and 3 zookeeper in production environment. Most of the time when I start the storm cluster and deploy the topology, none of spouts or bolts get activated. Even though I Storm UI shows everything is running and in Active state; however, spout is not emitting any messages. I checked the logs on all nodes including nimbus, there are no errors or exceptions. In Storm UI -> Topology Summary, all the starts shows 0.

I have to restart the whole cluster few times before topology actually starts working.

I am using storm storm-0.9.0-rc3 with zookeeper zookeeper-3.4.5 on JDK 1.6.

Any Idea why is this happening? Why do I need to start the storm cluster i
"
STORM-722,[storm-redis] Apply various Redis data types to Trident,"Since we introduced supporting various data types to storm-redis, it would be better to apply it to storm-redis Trident to enjoy same benefits.

For more details, please see STORM-691."
STORM-720,Storm.cmd should return ERRORLEVEL before exiting main block,"This JIRA is for a very small PR that I will post soon (attached patch)

Issue: The Storm.cmd does not exit with an ErrorLevel.

Impact: Any automation via windows banks on return or exit code from the program to determine success.

When can this occur: Passing wrong arguments (like wrong class name or bad topology builder code) to java will result in error and any automation program will check on the exit code.

FYI @ [~harsha_ch] [~shanyu]"
STORM-719,Trident name space for spout in zookeeper should be separated by topology name,Trident name space for spout in zk doesn't use topology name this can result in conflict if another topology uses the same spout name.
STORM-717,Proposal: JStorm contribution from Alibaba,"This is a umbrella case for the code contribution from Jstorm project. Please follow in this root jira to raise concerns and make discussions. All discussion in dev list or votes shall have a link posted in this unbrella jira.

The suggested steps and principals:

Principal for the JStorm contribution
1.      All job jars in Storm can be migrated directly without recompilation.
2.      Functions of JStorm will be merged into Storm.
3.      Most user facing function or user experience of Storm will be retained.
4.      There will be performance improvement.
5.      The process should be Smooth with incremental and tractable progress.

Need consense and further discussion
What is the best approach to evolve Clojure and Java(need more discussion on this) the two languages in future? More on Java?

Proposed Steps:
1.      IP donation
We should follow process of http://incubator.apache.org/ip-clearance/, 

a)      Identity the code base,  which tag version to choose as a basis. MD5 for the code base.
b)       IP Plan (copyright)
c)       distribution right, ICLA

2.      Determine the folder layout required for Storm, so that we can then easily break the effort to module level. 

3.      Continue the discussion and get a consensus of everyone in community about relation of Clojure and Java.

4.      Define process
a)       whether to create a Branch?
b)       What is the target Feature list?
c)        What is the target feature Order?
d)       What is the target Time line?
e)       What is the Target Version?


References:
Jstorm site: https://github.com/alibaba/jstorm
Original Discussion about JStorm and Storm: http://mail-archives.apache.org/mod_mbox/storm-dev/201410.mbox/%3C005701cfef39%245efd8100%241cf88300%24%40alibaba-inc.com%3E"
STORM-710,bin/storm command list out all the classes in the output for a command,"when running bin/storm list command or any other command it outputs
Running: /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/bin/java -client -Dstorm.options= -Dstorm.home=/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT -Dstorm.log.dir=/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/asm-4.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/carbonite-1.4.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/chill-java-0.3.5.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/clj-stacktrace-0.2.7.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/clj-time-0.8.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/clojure-1.6.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/clout-1.0.1.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/commons-codec-1.6.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/commons-exec-1.1.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/commons-fileupload-1.2.1.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/commons-io-2.4.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/commons-lang-2.5.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/commons-logging-1.1.3.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/compojure-1.1.3.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/core.incubator-0.1.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/crypto-equality-1.0.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/crypto-random-1.2.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/disruptor-2.10.1.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/hadoop-auth-2.4.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/hiccup-0.3.6.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/httpclient-4.3.3.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/httpcore-4.3.2.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/java.classpath-0.2.2.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/javax.servlet-2.5.0.v201103041518.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-client-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-continuation-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-http-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-io-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-security-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-server-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-servlet-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-servlets-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jetty-util-7.6.13.v20130916.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/jgrapht-core-0.9.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/joda-time-2.3.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/json-simple-1.1.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/kryo-2.21.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/log4j-over-slf4j-1.6.6.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/logback-classic-1.0.13.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/logback-core-1.0.13.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/math.numeric-tower-0.0.1.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/minlog-1.2.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/ns-tracker-0.2.2.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/objenesis-1.2.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/reflectasm-1.07-shaded.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/ring-anti-forgery-1.0.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/ring-core-1.1.5.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/ring-devel-1.3.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/ring-jetty-adapter-1.3.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/ring-servlet-1.3.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/servlet-api-2.5.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/slf4j-api-1.7.5.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/snakeyaml-1.11.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/storm-core-0.10.0-SNAPSHOT.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/tools.cli-0.2.4.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/tools.logging-0.2.3.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/tools.macro-0.1.0.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/lib/tools.namespace-0.2.4.jar:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/conf:/Users/schintalapani/build/apache-storm-0.10.0-SNAPSHOT/bin backtype.storm.command.list

This is distracting from actual output of a command. We should remove this output for user command such as list, jar, activate, deactivate, kill, rebalance but keep this for running storm daemons supervisor, ui, nimbus, drpc, logviewer. "
STORM-709,Received unexpected tuple,"Trident Topology using 2 OpaqueTridentKafkaSpouts and a TridentKafkaState.  Everything works great in production storm topology, but when running in local mode from an IDE receiving this error on startup:

{code}
java.lang.RuntimeException: java.lang.RuntimeException: Received unexpected tuple source: $mastercoord-bg1:2, stream: $commit, id: {-4957901903366351898=6364388931843393707}, [1:0]
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$fn__4606$fn__4619$fn__4670.invoke(executor.clj:806) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.util$async_loop$fn__543.invoke(util.clj:475) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:na]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
Caused by: java.lang.RuntimeException: Received unexpected tuple source: $mastercoord-bg1:2, stream: $commit, id: {-4957901903366351898=6364388931843393707}, [1:0]
    at storm.trident.planner.SubtopologyBolt.execute(SubtopologyBolt.java:144) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:369) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$fn__4606$tuple_action_fn__4608.invoke(executor.clj:668) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$mk_task_receiver$fn__4529.invoke(executor.clj:424) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.disruptor$clojure_handler$reify__1229.onEvent(disruptor.clj:58) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    ... 6 common frames omitted
{code}

Also receiving this error in the same dump:
{code}
java.lang.RuntimeException: org.apache.storm.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /errors/<topology-name>/<bolt-name>-last-error
    at backtype.storm.util$wrap_in_runtime.invoke(util.clj:48) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.zookeeper$create_node.invoke(zookeeper.clj:92) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.cluster$mk_distributed_cluster_state$reify__2234.set_data(cluster.clj:104) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.cluster$mk_storm_cluster_state$reify__2774.report_error(cluster.clj:450) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$throttled_report_error_fn$fn__4385.invoke(executor.clj:191) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$mk_executor_data$fn__4439$fn__4440.invoke(executor.clj:253) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.util$async_loop$fn__543.invoke(util.clj:485) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.6.0.jar:na]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_31]
Caused by: org.apache.storm.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /errors/<topology-name>/<bolt-name>-last-error
    at org.apache.storm.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:239) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:193) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) ~[na:na]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_31]
    at java.lang.reflect.Method.invoke(Method.java:483) ~[na:1.8.0_31]
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:na]
    at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:na]
    at backtype.storm.zookeeper$create_node.invoke(zookeeper.clj:91) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    ... 7 common frames omitted
{code}"
STORM-707,Client (Netty): improve logging to help troubleshooting connection woes,The current code in Client.java has a few places where we lack proper logging to troubleshoot connection issues (like we had to do for STORM-329).
STORM-705,Add BigPetStore stream to examples/,"I work on the Apache BigTop project (bigpetstore-transaction-queue), and we curate an internal data generator which is ideally suited for testing data streams. 
We could directly add this into the examples/ sections, as this will give users a less abstract and more universal entry point to building storm topologies."
STORM-701,provide eclipse plugin for creating/monitoring/debugging  user topology,"provide eclipse plugin for debugging user topology, submit topologies   and monitor their execution.
https://github.com/caofangkun/storm-eclipse-plugin
!New Storm Topology Project.jpg!
!New Storm Topology Project-02.jpg!
!New Storm Topology Project-03.jpg!

"
STORM-700,Add a hint about auto-generation in java source,"Apparently some java classes in `storm-core` are created by a code generator<ref>https://github.com/apache/storm/pull/460</ref>, but there's no hint in the generated code about that which would be extremely helpful."
STORM-698,Jackson incompatibility ring-json,"ring-json -> com.fasterxml.jackson.core:jackson-core:2.3.1 incompatibility

{code}
2015-03-04 12:34:35 b.s.d.worker [ERROR] Error on initialization of server mk-worker
java.lang.RuntimeException: java.io.InvalidClassException: com.fasterxml.jackson.core.JsonFactory; local class incompatible: stream classdesc serialVersionUID = 3306684576057132431, local class serialVersionUID = 3194418244231611666
  at backtype.storm.serialization.DefaultSerializationDelegate.deserialize(DefaultSerializationDelegate.java:56) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.utils.Utils.deserialize(Utils.java:95) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.utils.Utils.getSetComponentObject(Utils.java:234) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$get_task_object.invoke(task.clj:79) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$mk_task_data$fn__4304.invoke(task.clj:181) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.util$assoc_apply_self.invoke(util.clj:894) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$mk_task_data.invoke(task.clj:174) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$mk_task.invoke(task.clj:185) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.executor$mk_executor$fn__4499.invoke(executor.clj:337) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.core$map$fn__4245.invoke(core.clj:2557) ~[clojure-1.6.0.jar:na]
  at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.6.0.jar:na]
  at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.6.0.jar:na]
  at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.6.0.jar:na]
  at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.6.0.jar:na]
  at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.6.0.jar:na]
  at clojure.core.protocols$fn__6078.invoke(protocols.clj:54) ~[clojure-1.6.0.jar:na]
  at clojure.core.protocols$fn__6031$G__6026__6044.invoke(protocols.clj:13) ~[clojure-1.6.0.jar:na]
  at clojure.core$reduce.invoke(core.clj:6289) ~[clojure-1.6.0.jar:na]
  at clojure.core$into.invoke(core.clj:6341) ~[clojure-1.6.0.jar:na]
  at backtype.storm.daemon.executor$mk_executor.invoke(executor.clj:337) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.worker$fn__5031$exec_fn__1694__auto__$reify__5033$iter__5038__5042$fn__5043.invoke(worker.clj:459) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.6.0.jar:na]
  at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.6.0.jar:na]
  at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.6.0.jar:na]
  at clojure.lang.RT.next(RT.java:598) ~[clojure-1.6.0.jar:na]
  at clojure.core$next.invoke(core.clj:64) ~[clojure-1.6.0.jar:na]
  at clojure.core$dorun.invoke(core.clj:2856) ~[clojure-1.6.0.jar:na]
  at clojure.core$doall.invoke(core.clj:2871) ~[clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$fn__5031$exec_fn__1694__auto__$reify__5033.run(worker.clj:459) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at java.security.AccessController.doPrivileged(Native Method) ~[na:1.8.0_31]
  at javax.security.auth.Subject.doAs(Subject.java:422) ~[na:1.8.0_31]
  at backtype.storm.daemon.worker$fn__5031$exec_fn__1694__auto____5032.invoke(worker.clj:433) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.AFn.applyToHelper(AFn.java:178) [clojure-1.6.0.jar:na]
  at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:na]
  at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$fn__5031$mk_worker__5108.doInvoke(worker.clj:416) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$_main.invoke(worker.clj:548) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:na]
  at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
Caused by: java.io.InvalidClassException: com.fasterxml.jackson.core.JsonFactory; local class incompatible: stream classdesc serialVersionUID = 3306684576057132431, local class serialVersionUID = 3194418244231611666
  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:621) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371) ~[na:1.8.0_31]
  at backtype.storm.serialization.DefaultSerializationDelegate.deserialize(DefaultSerializationDelegate.java:52) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  ... 40 common frames omitted
2015-03-04 12:34:35 b.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
  at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:332) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$fn__5031$mk_worker__5108.doInvoke(worker.clj:416) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$_main.invoke(worker.clj:548) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:na]
  at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
2015-03-04 12:34:39 o.a.s.z.ZooKeeper [INFO] Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
{code}
"
STORM-693,KafkaBolt exception handling improvement,"Within the KafkaBolt execute method, an error message is logged if any sort of error occurs communicating with Kafka.  Unfortunately the input is still acknowledged.

Upon review of the HdfsBolt & HiveBolt, I believe the exception handling block should include the following two lines:

            this.collector.reportError(ex);
            this.collector.fail(input);
"
STORM-692,KakfaSpout storage of offsets in Zookeeper lost,"Constructor for SpoutConfig and inadequate documentation in storm-kafka/README.md leads to lost persistence of Kafka offset information.

The SpoutConfig class constructor takes four parameters:

hosts - Zookeeper host & root path for Kafka 
topic - Kafka topic name
zkRoot - Zookeeper path for Kafka Spout offset
id -- Identifier for KafkaSpout 

Unfortunately it also exposes two public instance variables that must also be set: zkServers & zkPort.  If these two variables are not set it uses Storm default values(localhost & 2000) which cause the offset information to be lost.

Suggest we replace the existing constructor with one that supplies all 6 required values."
STORM-677,Maximum retries strategy may cause data loss,"h3. Background

Storm currently supports the configuration setting storm.messaging.netty.max_retries.  This setting is supposed to limit the number of reconnection attempts a Netty client will perform in case of a connection loss.

Unfortunately users have run into situations where this behavior will result in data loss:

{quote}
https://github.com/apache/storm/pull/429/files#r24681006

This could be a separate JIRA, but we ran into a situation where we hit the maximum number of reconnection attempts, and the exception was eaten because it was thrown from a background thread and it just killed the background thread. This code appears to do the same thing.
{quote}

The problem can be summarized by the following example:  Once a Netty client hits the maximum number of connection retries, it will stop trying to reconnect (as intended) but will also continue to run forever without being able to send any messages to its designated remote targets.  At this point data will be lost because any messages that the Netty client is supposed to send will be dropped (by design).  And since the Netty client is still alive and thus considered ""functional"", Storm is not able to do something about this data loss situation.

For a more detailed description please take a look at the discussion in https://github.com/apache/storm/pull/429/files#r24742354.

h3. Possible solutions

(Most of this section is copy-pasted from an [earlier discussion on this problem|https://github.com/apache/storm/pull/429/files#r24742354].)

There are at least three approaches we may consider:

# Let the Netty client die if max retries is reached, so that the Storm task has the chance to re-create a client and thus break out of the client's discard-messages-forever state.
# Let the ""parent"" Storm task die if (one of its possibly many) Netty clients dies, so that by restarting the task we'll also get a new Netty client.
# Remove the max retries semantics as well as the corresponding setting from Storm's configuration. Here, a Netty client will continue to reconnect to a remote destination forever. The possible negative impact of these reconnects (e.g. number of TCP connection attempts in a cluster) are kept in check by our exponential backoff policy for such connection retries.

My personal opinion on these three approaches:

- I do not like (1) because I feel it introduces potentially confusing semantics: We keep having a max retries setting, but it is not really a hard limit anymore. It rather becomes a ""max retries until we recreate a Netty client"", and would also reset any exponential backoff strategy of the ""previous"" Netty client instance (cf. StormBoundedExponentialBackoffRetry). If we do want such resets (but I don't think we do at this point), then a cleaner approach would be to implement such resetting inside the retry policy (again, cf. StormBoundedExponentialBackoffRetry).
- I do not like (2) because a single ""bad"" Netty client would be able to take down a Storm task, which among other things would also impact any other, working Netty clients of the Storm task.
- Option (3) seems a reasonable approach, although it breaks backwards compatibility with regard to Storm's configuration (because we'd now ignore storm.messaging.netty.max_retries).


Here's initial feedback from other developers:

{quote}
https://github.com/apache/storm/pull/429/files#r24824540

revans2: I personally prefer option 3, no maximum number of reconnection attempts. Having the client decide that it is done, before nimbus does feels like it is asking for trouble.
{quote}

{quote}
https://github.com/ptgoetz

ptgoetz: I'm in favor of option 3 as well. I'm not that concerned about storm.messaging.netty.max_retries being ignored. We could probably just log a warning that that configuration option is deprecated and will be ignored if the value is set.
{quote}

{quote}
https://github.com/apache/storm/pull/429#issuecomment-74914806

nathanmarz: Nimbus only knows a worker is having trouble when it stops sending heartbeats. If a worker gets into a bad state, the worst thing to do is have it continue trying to limp along in that bad state. It should instead suicide as quickly as possible. It seems counterintuitive, but this aggressive suiciding behavior actually makes things more robust as it prevents processes from getting into weird, potentially undefined states. This has been a crucial design principle in Storm from the beginning. One consequence of it is that any crucial system thread that receives an unrecoverable exception must suicide the process rather than die quietly.

For the connection retry problem, it's a tricky situation since it may not be able to connect because the other worker is still getting set up. So the retry policy should be somehow related to the launch timeouts for worker processes specified in the configuration. Not being able to connect after the launch timeout + a certain number of attempts + a buffer period would certainly qualify as a weird state, so the process should suicide in that case. Suiciding and restarting gets the worker back to a known state.

So in this case, I am heavily in favor of Option 2. I don't care about killing the other tasks in the worker because this is a rare situation. It is infinitely more important to get the worker back to a known, robust state than risk leaving it in a weird state permanently.
{quote}

If we decide to go with option 3, then the essence of the fix is the following modification of Client.java:

{code}
    private boolean reconnectingAllowed() {
        // BEFORE:
        // return !closing && connectionAttempts.get() <= (maxReconnectionAttempts + 1);
        return !closing;
    }
{code}"
STORM-674,Add chaos monkey testing to storm,"Storm is a distributed processing system and  as we add new features like Nimbus HA  its not always easy to test the correctness and robustness of storm under failure scenarios across multiple hosts.Ideally storm will have a suit of chaos monkey tests that can be executed to gain confidence that any new features/ or changes to these new features in future has not affected storm's robustness. 

We could add simple kill, restart(numSecsToWait) thrift APIs that can be called  remotely to kill or restart nimbus/supervisor/workers to begin with. Alternatively we could evaluate some frameworks out there for ingesting host/network failures.
"
STORM-671,Measure tuple serialization/deserialization latency.,"Some times the serialization/deserialization cost can be very high, and it is not currently measured anywhere in storm.  We should measure it, at least in a similar way to how we do execute and process latency."
STORM-668, Generate storm-ui war using maven and deployed on TomCat,"$ mvn package -Dmaven.test.skip=true -P dist -Dwar

generate storm-ui.war "
STORM-666,Create a CONTRIBUTING file which explains how to edit http://storm.apache.org/documentation,"It should be clear from a `CONTRIBUTING` file how to contribute to every part of the project, including the task mentioned in the title."
STORM-665,Lower entry barrier for understanding storm's stream management,"Due to the fact that storm is a stream processing framework, it's necessary to explain the handling of streams in a proper documentation chapter. Currently the index at http://storm.apache.org/doc-index.html doesn't even contain the word.

An example of missing infos includes the usage of explicit stream ids and omitting them (e.g. in `BoltDeclarer.xyGrouping`). Question which arise and are not answered: How to declare a stream (object) explicitly? Which one is used when an explicit id is omitted?
"
STORM-664,Link to inexisting wiki on documentation site,http://storm.apache.org/documentation.html links to the wiki of https://github.com/apache/storm which doesn't exist and the `README` doesn't mention the word.
STORM-662,java.lang.OutOfMemoryError: Direct buffer memory ,"I'm using Storm 0.9.3 and facing this error in running topology.

Below is our custom configuration but it's pretty small.

supervisor.slots.ports:
    - 6700
worker.childopts:
    -Xmx4g -Xms4g
    -server -Djava.net.preferIPv4Stack=true
topology.worker.shared.thread.pool.size: 512
storm.messaging.netty.max_retries: 100

Feb 08, 2015 5:17:05 PM org.apache.storm.netty.channel.socket.nio.AbstractNioSelector WARNING: Unexpected exception in the selector loop. java.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:658) at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306) at org.apache.storm.netty.channel.socket.nio.SocketReceiveBufferAllocator.newBuffer(SocketReceiveBufferAllocator.java:64) at org.apache.storm.netty.channel.socket.nio.SocketReceiveBufferAllocator.get(SocketReceiveBufferAllocator.java:44) at org.apache.storm.netty.channel.socket.nio.NioWorker.read(NioWorker.java:62) at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) at org.apache.storm.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) at org.apache.storm.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) at org.apache.storm.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) at org.apache.storm.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)"
STORM-661,java.io.FileNotFoundException: File 'stormconf.ser' does not exist,"I'm using 0.9.3 and facing this error.

I heard that it's solved in 0.9.0 something but still it's happening.

It's critical since if this happen, supervisor also dies and does not automatically run again.

we've been using Storm from 0.9.2.

2015-02-08 17:22:16 b.s.event [ERROR] Error when processing event
java.io.FileNotFoundException: File 'storm-local/supervisor/stormdist/indexer-rescue-135-1422240287/stormconf.ser' does not exist
        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
        at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
        at backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:212) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.daemon.supervisor$fn__4256.invoke(supervisor.clj:509) ~[storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.MultiFn.invoke(MultiFn.java:241) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.supervisor$sync_processes$iter__4116__4120$fn__4121.invoke(supervisor.clj:285) ~[storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.LazySeq.sval(LazySeq.java:42) ~[clojure-1.5.1.jar:na]
        at clojure.lang.LazySeq.seq(LazySeq.java:60) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.5.1.jar:na]
        at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.5.1.jar:na]
        at clojure.core$dorun.invoke(core.clj:2780) ~[clojure-1.5.1.jar:na]
        at clojure.core$doall.invoke(core.clj:2796) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:273) ~[storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2467.invoke(event.clj:40) ~[storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]
2015-02-08 17:22:16 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2467.invoke(event.clj:48) [storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]
2015-02-08 17:22:16 b.s.d.supervisor [INFO] Shutting down supervisor 5fbb3cd9-0cd9-4a4c-9f17-3991928ea716"
STORM-660, IndexOutOfBoundsException with  shuffle grouping and a large number of paralleliziation hint,"Storm throws  IndexOutOfBoundsException error once in a while, when a bolt with shuffle grouping to is linked its predecessor bolt.especially with a high paralleliziation hint.
This was reported as a bug with the previous versions of storm as well.

http://stackoverflow.com/questions/19632287/indexoutofboundsexception-in-shuffle-grouping
"
STORM-658,config topology.acker.executors default value is null and then should not start acker bolts,"See Code:
https://github.com/caofangkun/apache-storm/blob/master/storm-core/src/clj/backtype/storm/daemon/common.clj#L315
Config ""topology.acker.executors"" default value is null
then acker executors will be same as ""topology.workers""
$ storm jar storm-starter-0.10.0-SNAPSHOT.jar storm.starter.ExclamationTopology ExclamationTopology
Executors show up as 18 executors = 10(word) + 3(exclaim1) + 2(exclaim2) + 3(acker bolt)
But the 3 acker bolt executors will not be used."
STORM-650,Storm-Kafka Refactoring and Improvements,"This is intended to be a parent/umbrella JIRA covering a number of efforts/suggestions aimed at improving the storm-kafka module. The goal is to facilitate communication and collaboration by providing a central point for discussion and coordination.

The first phase should be to identify and agree upon a list of high-level points we would like to address. Once that is complete, we can move on to implementation/design discussions, followed by an implementation plan, division of labor, etc.

A non-exhaustive, initial list of items follows. New/additional thoughts can be proposed in the comments.

* Improve API for Specifying the Kafka Starting point
Configuring the kafka spout's starting position (e.g. forceFromStart=true) is a common source of confusion. This should be refactored to provide an easy to understand, unambiguous API for configuring this property.

* Use Kafka APIs Instead of Internal ZK Metadata (STORM-590)
Currently the Kafka spout relies on reading Kafka's internal metadata from zookeeper. This should be refactored to use the Kafka Consumer API to protect against changes to the internal metadata format stored in ZK.

* Improve Error Handling
There are a number of failure scenarios with the kafka spout that users may want to react to differently based on their use case. Add a failure handler API that allows users to implement and/or plug in alternative failure handling implementations. It is assumed that default (sane) implementations would be included and configured by default.

* Configuration/General Refactoring (BrokerHosts, etc.) (STORM-631)
(need to flesh this out better) Reduce unnecessary marker interfaces/""instance of"" checks. Unify configuration of core storm/trident spout implementations.

* Kafka Spout doesn't pick up from the beginning of the queue unless forceFromStart specified (STORM-563)

Discussion Items:

* How important is backward compatibility?

"
STORM-647,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-646,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-645,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-644,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-642,Add Storm Benchmarking Framework,"(This is intended to be a parent for a multi-task JIRA)

Add a framework for benchmarking Storm topologies and clusters that would capture basic throughput (messages/sec., MB/sec.) and latency metrics.

Allow for the definition of named ""benchmark suites"" in a configuration file (i.e. YAML) that would define multiple runs of the same benchmark with different tuning parameters (# workers, parallelism hints, etc.). This would allow for easy comparison of different runs to show how different tuning options affect overall performance.

Develop and benchmark both Core Storm and Trident Topologies to illustrate the difference in performance characteristics. Benchmark multiple trident constructs (joins, aggregations, partition(Persist), etc.) to illustrate performance implications.

Document results in a blog post series ""Benchmarking Storm"" on storm.apache.org/blog."
STORM-639,storm-maven-plugin not found,"storm-maven-plugin is required by storm-core, but it cannot be found.

```
[ERROR] Error resolving version for plugin 'org.apache.storm:storm-maven-plugins' from the repositories [local (/Users/sasakikai/.m2/repository), central (https://repo.maven.apache.org/maven2)]: Plugin not found in any plugin repository -> [Help 1]
org.apache.maven.plugin.version.PluginVersionResolutionException: Error resolving version for plugin 'org.apache.storm:storm-maven-plugins' from the repositories [local (/Users/sasakikai/.m2/repository), central (https://repo.maven.apache.org/maven2)]: Plugin not found in any plugin repository
        at org.apache.maven.plugin.version.internal.DefaultPluginVersionResolver.selectVersion(DefaultPluginVersionResolver.java:236)
        at org.apache.maven.plugin.version.internal.DefaultPluginVersionResolver.resolveFromRepository(DefaultPluginVersionResolver.java:148)
        at org.apache.maven.plugin.version.internal.DefaultPluginVersionResolver.resolve(DefaultPluginVersionResolver.java:96)
        at org.apache.maven.lifecycle.internal.LifecyclePluginResolver.resolveMissingPluginVersions(LifecyclePluginResolver.java:71)
        at org.apache.maven.lifecycle.internal.DefaultLifecycleExecutionPlanCalculator.calculateExecutionPlan(DefaultLifecycleExecutionPlanCalculator.java:116)
        at org.apache.maven.lifecycle.internal.DefaultLifecycleExecutionPlanCalculator.calculateExecutionPlan(DefaultLifecycleExecutionPlanCalculator.java:135)
        at org.apache.maven.lifecycle.internal.builder.BuilderCommon.resolveBuildPlan(BuilderCommon.java:97)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:109)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:347)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:154)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:582)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR]
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
```

This plugin should be installed in local repository before compile storm-core."
STORM-638,UI should show up  process-id of the Worker to which an Executor is assigned,
STORM-631,Simplifying kafka connector code,"The current kafka connector was written when kafka did not support any APIs to expose its internal structures. We already have STORM-590 to modify the kafka connector to new kafka meta APIS. 

The current codebase uses some marker interfaces to support reading kafka partition/broker info from zookeeper or for users to specify the mapping them self. Adding one more layer that reads this info using kakfa APIs would reduce readability. Because we are using marker interfaces there are bunch of places that has if statements with instanceof checks and each one creates a separate code path to trace. We should delete these interfaces and their implementation in favor of a single way to get kafka topic information, using kafka APIs. This will be a backward incompatible change but should make future changes and usage easy.

Current kafka connector also has 2 separate configs for trident and core topologies. This configurations extend from a common interface but the extensions are not interchangeable. I think it is intuitive to have a single config with sane defaults for both core and trident spouts.




"
STORM-628,Storm-HBase add support to WriteBuffer/setAutoFlush,"The default value for ""autoflush"" in HTable is true. We should support our user to enable HBase writebuffer on the client side, by add a new configuration ""storm.hbase.table.autoflush"".

 "
STORM-622,saveVersion.sh should work on all platforms,
STORM-619,add supervisor page to show workers running detail informations,"add supervisor page to show workers running detail information

show workers details like:
1: runging taskids 
2: uptime secs
3: topologyid
4: worker port 
5: error log link"
STORM-618, Add spoutconfig option to make kafka spout process messages at most once.,"While it's nice for kafka spout to push failed tuple back into a sorted set and try to process it again, this way of guaranteed message processing sometimes makes situation pretty bad when a failed tuple repeatedly fails in downstream bolts since PartitionManager#fill method tries to fetch from that offset repeatedly.

This is a corresponding code snippet.

    private void fill() {
...
        if (had_failed) {
            offset = failed.first();
        } else {
            offset = _emittedToOffset;
        }
...
            msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
...

So there should be an option for a developer to decide if he wants to process failed tuple again or just skip failed tuple. One of the best thing of Storm is that spout together with trident can be implemented to guarantee at-least-once,exactly-once and at-most-once message processing."
STORM-615,Add REST API to upload topology,Add REST api /api/v1/submitTopology to upload topology jars and config using REST api.
STORM-614,storm-core mvn artifacts dependencies are not downloaded automatically,"I added 'storm-core' artefact to my gradle project. Gradle couldn't download roughly half of artefact dependencies (clj-time, ring-servlet and others), because it couldn't find them. pom.xml of 'storm-core' doesn't contain any links to any external repos.

Problem is: these dependencies are stored in 'clojars' repo, and during build of all storm, dependent projects use links from root pom.xml, and thus when 'storm-core' artefact is deployed to maven central it is broken by default. 

Suggestions to solving are: 
1) Update documentation on storm.apache.org (in Downloads section)
2) Adding 'clojars' repo url to 'storm-core' pom.xml
"
STORM-606,Attempting to call unbound fn during bolt prepare,"
We had a bunch of topologies running very well under Storm 0.8.2 until last
week when we switched to storm 0.9.2-incubating. We use the clojure DSL,
and clojure 1.5.1 (only).

Since the change, we have a large topology (about 30 bolts, parallellism=10
or 20 per bolt, total 372 tasks on 10 workers) that fails on startup with
several bolts showing the exception :

java.lang.RuntimeException: java.lang.IllegalStateException: Attempting to
call unbound fn: #'entry-dedup.bolt/dedup__ at
backtype.storm.clojure.ClojureBolt.prepare(ClojureBolt.java:77) ...

This can occur on one or several bolts at random and is not consistent
between restarts.

The topology is indeed quite long to initialize (a dozen seconds) due to several models being loaded but this was OK in 0.8.2.

Another (shorter) topology works most of the time but shows this behaviour
on some restarts sometimes.

We found a workaround that works most of the time : start the topology in
the INACTIVE state, then wait 200 seconds, then activate it. But this
doesn't really solve our problem because sometimes Storm tries to rebalance
the topologies by itself and reassigns the topology without our little trick, effectively crashing them.

The same behavior is present with storm 0.9.3.

So maybe something changed in storm that introduces a kind of race
condition during initializaion of some bolts on larger topologies ? Maybe this is a consequence to the switch to Netty ?"
STORM-605,Attempting to call unbound fn during bolt prepare,"
We had a bunch of topologies running very well under Storm 0.8.2 until last
week when we switched to storm 0.9.2-incubating. We use the clojure DSL,
and clojure 1.5.1 (only).

Since the change, we have a large topology (about 30 bolts, parallellism=10
or 20 per bolt, total 372 tasks on 10 workers) that fails on startup with
several bolts showing the exception :

java.lang.RuntimeException: java.lang.IllegalStateException: Attempting to
call unbound fn: #'entry-dedup.bolt/dedup__ at
backtype.storm.clojure.ClojureBolt.prepare(ClojureBolt.java:77) ...

This can occur on one or several bolts at random and is not consistent
between restarts.

The topology is indeed quite long to initialize (a dozen seconds) due to several models being loaded but this was OK in 0.8.2.

Another (shorter) topology works most of the time but shows this behaviour
on some restarts sometimes.

We found a workaround that works most of the time : start the topology in
the INACTIVE state, then wait 200 seconds, then activate it. But this
doesn't really solve our problem because sometimes Storm tries to rebalance
the topologies by itself and reassigns the topology without our little trick, effectively crashing them.

The same behavior is present with storm 0.9.3.

So maybe something changed in storm that introduces a kind of race
condition during initializaion of some bolts on larger topologies ? Maybe this is a consequence to the switch to Netty ?"
STORM-604,Storm Bug List,
STORM-598,Newly submitted topologies do not show up on the storm ui cluser page when the Storm Cluster run out of worker slots ,"1: Set up a Storm Cluster with 1 Supervisor(4 worker slots)
2: submit topologyA and use 4 workers 
3: submit topologyB with 4 workers 

topologyB does not but should show up on the storm ui cluster page.


See Code Line 232：
https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/daemon/nimbus.clj#L1232

If assignment is null, just new Assignmnet with 0 workers and 0 tasks? 
{code:title=nimbus.clj|borderStyle=solid}
Index: src/clj/backtype/storm/daemon/nimbus.clj
===================================================================
--- src/clj/backtype/storm/daemon/nimbus.clj	(revision 4324)
+++ src/clj/backtype/storm/daemon/nimbus.clj	(working copy)
@@ -1230,7 +1230,9 @@
               bases (topology-bases storm-cluster-state)
               topology-summaries (dofor [[id base] bases :when base]
 	                                  (let [assignment (.assignment-info storm-cluster-state id nil)
-                                                topo-summ (TopologySummary. id
+                                                topo-summ ( if (nil? assignment) 
+                                                           (TopologySummary. id  (:storm-name base) 0 0 0  (time-delta (:launch-time-secs base)) (extract-status-str base))
+                                                           (TopologySummary. id
                                                             (:storm-name base)
                                                             (->> (:executor->node+port assignment)
                                                                  keys
@@ -1244,7 +1246,7 @@
                                                                  set
                                                                  count)
                                                             (time-delta (:launch-time-secs base))
-                                                            (extract-status-str base))]
+                                                            (extract-status-str base)))]
                                                (when-let [owner (:owner base)] (.set_owner topo-summ owner))
                                                (when-let [sched-status (.get @(:id->sched-status nimbus) id)] (.set_sched_status topo-summ sched-status))
                                                topo-summ
{code}
"
STORM-597,spout not receve the message acker,"I have one spout and one bolt.
The program always runs greate. But sometimes the spout do not receive the ack and at debug mode the bolt have do the ack correctly.

LOG:
###1 Normally the last line says the spout has received acker.

2014-12-21 02:46:15 c.s.b.s.b.s.BaseTaskScheduleSpout [INFO] check time begin at :2014-12-21 02:46:15   end
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting: task-spout default [1419100920000, 1419100980000]
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting: task-spout __ack_init [3492883688624743717 -7308083944080730032 10]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: default, id: {3492883688624743717=-7308083944080730032}, [1419100920000, 1419100980000]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: __ack_init, id: {}, [3492883688624743717 -7308083944080730032 10]
2014-12-21 02:46:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        scan    0       cost    3 ms    time    2014-12-21 02:42:00     to      2014-12-21 02:43:00     row     null    to      null
2014-12-21 02:46:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        run final success
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting: task-bolt __ack_ack [3492883688624743717 -7308083944080730032]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: task-bolt:7, stream: __ack_ack, id: {}, [3492883688624743717 -7308083944080730032]
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting direct: 10; __acker __ack_ack [3492883688624743717]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: __acker:1, stream: __ack_ack, id: {}, [3492883688624743717]
2014-12-21 02:46:15 b.s.d.executor [INFO] Acking message 1419100920000,1419100980000,60000,prmt_user_tracer,true,1419101175675,from:2014-12-21 02:42:00,to:2014-12-21 02:43:00,execute:2014-12-21 02:46:15



###2.bug case, the bolt ack is stopping at ""Emitting direct: 10; __acker __ack_ack [-5576733886177167329]"". And the spout will not receve the ack of that message.

2014-12-21 02:45:15 c.s.b.s.b.s.BaseTaskScheduleSpout [INFO] begin put task     1419100860000,1419100920000,60000,prmt_user_tracer,true,1419101115112,from:2014-12-21 02:41:00,to:2014-12-21 02:42:00,execute:2014-12-21 02:45:15
2014-12-21 02:45:15 c.s.b.s.b.s.BaseTaskScheduleSpout [INFO] check time begin at :2014-12-21 02:45:14   end
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting: task-spout default [1419100860000, 1419100920000]
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting: task-spout __ack_init [-5576733886177167329 6447998047515384780 10]
2014-12-21 02:45:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: default, id: {-5576733886177167329=6447998047515384780}, [1419100860000, 1419100920000]
2014-12-21 02:45:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: __ack_init, id: {}, [-5576733886177167329 6447998047515384780 10]
2014-12-21 02:45:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        scan    0       cost    4 ms    time    2014-12-21 02:41:00     to      2014-12-21 02:42:00     row     null    to      null
2014-12-21 02:45:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        run final success
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting: task-bolt __ack_ack [-5576733886177167329 6447998047515384780]
2014-12-21 02:45:15 b.s.d.executor [INFO] Processing received message source: task-bolt:5, stream: __ack_ack, id: {}, [-5576733886177167329 6447998047515384780]
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting direct: 10; __acker __ack_ack [-5576733886177167329]

"
STORM-594,Auto-Scaling Resources in a Topology,"A useful feature missing in Storm topologies is the ability to auto-scale resources, based on a pre-configured metric. The feature proposed here aims to build such a auto-scaling mechanism using a feedback system. A brief overview of the feature is provided here. The finer details of the required components and the scaling algorithm (uses a Feedback System) are provided in the PDFs attached.

Brief Overview:
Topologies may get created with or (ideally) without parallelism hints and tasks in their bolts and spouts, before submitting them, If auto-scaling is set in the topology (using a Boolean flag), the topology will also get submitted to the auto-scale module.
The auto-scale module will read a pre-configured metric (threshold/min) from a configuration file. Using this value, the topology's resources will be modified till the threshold is reached. At each stage in the auto-scale module's execution, feedback from the previous execution will be used to tune the resources.

The systems that need to be in place to achieve this are:
1. Metrics which provide the current threshold (no: of acks per minute) for a topology's spouts and bolts.
2. Access to Storm's CLI tool which can change a topology's resources are runtime.
3. A new java or clojure module which runs within the Nimbus daemon or in parallel to it. This will be the auto-scale module.

Limitations: (This is not an exhaustive list. More will be added as the design matures. Also, some of the points here may get resolved)
To test the feature there will be a number of limitations in the first release. As the feature matures, it will be allowed to scale more
1. The auto-scale module will be limited to a few topologies (maybe 4 or 5 at maximum)
2. New bolts will not be added to scale a topology. This feature will be limited to increasing the resources within the existing topology.
3. Topology resources will not be decreased when it is running at more than the required number (except for a few cases)
4. This feature will work only for long-running topologies where the input threshold can become equal to or greater than the required threshold

"
STORM-593,No need of rwlock for clojure atom ,"cached-node+port->socket in worker-data is atom, there on need for rwlock endpoint-socket-lock to protect cached-node+port->socket. And after use rwlock, there will be competition between refresh-connections and message send."
STORM-592,"Update stats.clj ""rolling-window-set"" function, exchange the real argument ""num-buckets"" and ""s"" of ""rolling-window"" function","(defn rolling-window-set [updater merger extractor num-buckets & bucket-sizes]
  (RollingWindowSet. updater extractor (dofor [s bucket-sizes] (rolling-window updater merger extractor s num-buckets)) nil)
  )

(defrecord RollingWindow [updater merger extractor bucket-size-secs num-buckets buckets]) 

if not exchange the real argument ”num-buckets“ and ""s"" of “rolling-window” function, then the ""bucket-size-secs"" of RollingWindow is 30/540/4320, and the ""num-buckets"" of RollingWindow is 20

I think that the ""bucket-size-secs"" of RollingWindow is 20, and the ""num-buckets"" of RollingWindow is 30/540/4320."
STORM-591,logback.xml in store-core,"logback.xml is now being included in storm-core.jar.

This causes issues for any application that might include storm-core in it's classpath, having to override the config file location."
STORM-590,KafkaSpout should use kafka consumer api,"Following below ticket
https://github.com/apache/storm/pull/338

KafkaSpout uses kakfa internal data included zk nodes. However it should be changed to get these data from kafka consumer api provided kafka project.

"
STORM-589,Suboptimal default worker hb timeouts for nimbus & supervisor,"Both worker heartbeat timeouts for nimbus and supervisor are set to 30 seconds by default:

https://github.com/apache/storm/blob/3bbdc166bda7fb1a39b6906eda40da9bc83d5d4c/conf/defaults.yaml#L58

https://github.com/apache/storm/blob/3bbdc166bda7fb1a39b6906eda40da9bc83d5d4c/conf/defaults.yaml#L118

This means that it is when a worker dies in relation to its heartbeats that would determine whether the supervisor relaunches it or nimbus reassigns it.

If the supervisor heartbeat is found to have timed out first, it is relaunched.  If the nimbus heartbeat is found to have timed out first, it is rescheduled.

We may want the nimbus time-out to be larger than the supervisor time-out, to give the supervisor a chance to relaunch the worker before nimbus re-assigns it.

As always, users administrating clusters are encouraged to set these as needed."
STORM-588,Executor-Level Rebalance Mechanism,"I. The motivation

The current rebalance mechanism is implemented on the worker level. When rebalance operation is triggered (e.g. by adding/removing a worker), storm kills all the workers with different assignment. It means the rebalance operation has to kill certain running workers and launches them according to the new assignment. The advantage of the mechanism is the simplicity of the implementation, but possibly incurs _huge_ overhead. Actually, the restarting latency is usually more than one second, making the system almost impossible to recover under high incoming data stream rate. No system administrator dares to call rebalance, especially when the system is overloaded! To bring back the real benefits of rebalancing operation, we believe it is important to address the following problems:

*1. Resource wastage and additional initialization cost*: In most cases, the changes on worker’s assignment (if not killed) only affect a small fraction of running executors on it. Only part of them needs to be migrated or created, while the remaining can keep running on the same worker. The current implementation, however, forcefully restarts all the executors, and calls unnecessary initializations (i.e. call Bolt.prepare() and Spout.prepare()) to most of the running tasks. It not only wastes the computation resources of unaffected executors, but also amplifies the initialization costs under certain condition, e.g. index load in the bolt.

*2. Restarting workers causes avoidable in-memory data loss*: Currently, a supervisor uses “kill -9” command to kill its correspondent worker. Consequently, all the tasks on this worker have no chance to save the task data. The running states of the workers, including important information when resuming its duty, are simply lost, potentially causing unnecessary recomputation on the states.

*3. JVM restart cost, long duration and lost of HotSpot optimizations*: Restarting a JVM involves a long initialization procedure, and loses all the runtime optimizations available for the application byte-code. As far as we know, the HotSpot JVM is capable of detecting the performance-critical sections in the code and dynamically translates the Java byte codes of these hot spots into native machine code. In particular, tasks that are CPU-bound can greatly benefit from this feature. If we directly kill the worker, all the advantages of these features are lost.

II. Proposed solutions

1. At the supervisor side:
The current supervisor implementation periodically calls the “sync-processes” function to check whether a live worker should be killed: (1) the mapping relationship between the worker and the topology has changed (e.g. this worker is re-assigned to another topology or the serving topology is killed); (2) the worker’s assignment has updated (e.g. the parallelism of some bolts increases/decreases). 

In order to reuse the worker’s JVM instance as much as possible, we propose that we do not kill the workers mentioned in condition (2), but only kill those that do not belong to the topology anymore (condition (1)).

2. At the worker side: 
Because of the reuse of the JVM instance, workers needs to periodically synchronize its assigned executors. To achieve this, a new thread which is similar to the existing “refresh-connections” is launched, to kill the non-existing executors, and to start newly assigned ones. Note that, in practice, the “refresh-connections“ threads already retrieves the assignment information from the ZK, and this information can be shared with this new thread, which reduce the load of the ZK.

Due to the change of the binging from the running executors to the worker, re-routing tuple is also required. To fulfill this prepose, we need to rewrite the following two functions, “transfer-local-fn” and “transfer-fn” (note the rewrite is compulsive because these two functions are immutable in the current implementation). 

Another function needs careful modification is “WorkerTopologyContext.getThisWorkerTasks()”, because the (defn- mk-grouper … :local-or-shuffle) in “excutor.clj” depends on this function to get required context information. Therefore, in the case that an end user calls “WorkerTopologyContext.getThisWorkerTasks()” in the “prepare()”, and stores the results, if the executor has not restarted, using these results may potentially leads to inconsistency.

In summary, we propose this new executor-level rebalance mechanism, which tries to maximize the resource usage and minimize the rebalance cost. This is essential for the whole system, especially important for the the ultimate purpose on elasticity features for Storm."
STORM-587,trident transactional state in zk should be namespaced with topology id,Currently when a trident transaction spout is initialized it creates a node in zk under /transactional with the spout name as the node's name. This is pretty dangerous as any other topology can be submitted with same spout name and  now these 2 spouts will be overwriting each other's states. I believe it is better to namespace this with topologyId just like all other zk entries under /storm.
STORM-580,Tunnel logviewer through nimbus so public IPs aren't required for supervisor nodes,
STORM-579,Trident KafkaSpout throws RunTimeException when trying to re-emit a batch that is no longer in Kafka,"On version 0.9.2 - A message from the future is transmitted once, then a RuntimeException is thrown from the KafkaSpout
On version 0.9.3 it seems that UpdateOffsetException is thrown, but it seems that this will still cause the topology to be killed.

There is some faulty code that is causing a batch to be retransmitted to infinity.
Kafka Spout re-emits the batch, and as intended behaviour, has no limit on how many times it will be re-emitted (which is OK).
At some point in the future, the offset of this batch no longer exists on Kafka.
Then the real action kicks in (code snippets are taken from the v0.9.2 tag) - Kafka Spout is using KafkaUtils.fetchMessages to get the batch from Kafka.
Now let us have a look at the relevant code from fetchMessages

if (error.equals(KafkaError.OFFSET_OUT_OF_RANGE) && config.useStartOffsetTimeIfOffsetOutOfRange && errors == 0) {
  long startOffset = getOffset(consumer, topic, partitionId, config.startOffsetTime);
  LOG.warn(""Got fetch request with offset out of range: ["" + offset + ""]; "" +
  ""retrying with default start offset time from configuration. "" +
  ""configured start offset time: ["" + config.startOffsetTime + ""] offset: ["" + startOffset + ""]"");
  offset = startOffset;
}

So if the offset does not exist of Kafka anymore, we will fetch something with a different offset (not sure why this is a good idea). In practice, this will be much larger offset that originally tried to retrieve.
Now let us go back to the Kafka Spout code, now that it got some messages with a much larger offset than what it originally requested, the behaviour is really interesting.
for (MessageAndOffset msg : msgs) {
  if (offset == nextOffset) {
    break;
  }
if (offset > nextOffset) {
  throw new RuntimeException(""Error when re-emitting batch. overshot the end offset"");
}
  emit(collector, msg.message());
  offset = msg.nextOffset();
}
As you can see, at first, nothing touches the offset, so some random message from a different offset *is emitted*
Then, offset will be updated with nextOffset of current message which is of course is very large, which in the next entry to the loop will cause the ""overshot the end offset"" error."
STORM-576,"Random ""Clojure failed"" message","Tests have been done with IBM JVM, OpenJDK, and Oracle JVM, version 7.
- 2 tests runs done with IBM JVM and 2 tests runs done with OpenJDK shown a ""Clojure failed"" error message at end of tests, with several Exceptions and traces.
- With Oracle JVM, first with jdk1.7.0_67 then with jdk1.7.0_71 has generated, first: no error, second: ""Clojure failed"" error message.

As an example, error message:
  java.lang.RuntimeException: Error when launching multilang subprocess
happens both with IBM JVM (2 times) and with Oracle JVM (only once). But not with OpenJDK.

I gonna provide logs."
STORM-568,"I would like to utilize storm for a code base, unstructured, flowing, and parallel.","I have long considered the problems with parallel computing and to me it is in the structure of the code itself. One process is sitting waiting for another to complete or catch up, wasting resources, time, and energy. Storm gives fault tolerance and a sort of non-rigid flow. I would like to investigate the naturally inherent properties of storm to see if would allow an actual code base to be attached. It would be close to a high-speed code delivery system that could posses incredible possibilities and potential if proven successful. Code Bases considered would be C/C++/Fortran Python Perl OMP MPI OA(OAA) integrated with highly optimized and implemented advanced computational libraries, mixed with some good ol fashioned tinkering to get it right."
STORM-564,Support worker use dynamic port,"Background: When deploy storm mixed with other services, or deploy storm by yarn and mesos or some other scheduling system, worker port conflict is really a big problem.

In order to fix this, we add worker.dynamic.port to indicate whether worker bind dynamic port or not.
When set worker.dynamic.port as true, worker will use port that specified by supervisor.slots.ports;
When set worker.dynamic.port as false, worker will bind 0, and the port that specified by supervisor.slots.ports will be nominally port in storm, which stand for worker really bind port.

And when worker launched, worker will report it's really bind port by heartbeat, and nimbus send worker's bind port by assignment."
STORM-562,"When Isolated topologies are running ""free slots"" can be wrong/misleading","The isolation scheduler will blacklist a node so that other topologies cannot run on it, but this is not reflected anywhere in the UI, and makes it difficult to know how full a cluster is.

I propose that we add in a new concept to the scheduler.  Where it can ""claim"" a slot for a topology without actually running anything on that slot.  We would then reflect the number of claimed slots in thrift interface and in the UI."
STORM-560,ZkHosts in README should use 2181 as port,"Recently I am using kafka spout to consume message from kafka server. While reading documents on https://github.com/apache/storm/tree/master/external/storm-kafka, the instructions on creating ZkHosts is really confusing. It seems like it should point to broker instead of zookeeper because the example uses port 9092. But  after couple of testing, I realized it should point to the zookeeper and use 2181. The document is not clear on that point. It would be great if we can change that. 
"
STORM-559,ZkHosts in README should use 2181 as port,"Recently I am using kafka spout to consume message from kafka server. While reading documents on https://github.com/apache/storm/tree/master/external/storm-kafka, the instructions on creating ZkHosts is really confusing. It seems like it should point to broker instead of zookeeper because the example uses port 9092. But  after couple of testing, I realized it should point to the zookeeper and use 2181. The document is not clearly on that point. It would be great if we can change that. 
"
STORM-556,netty Client  reconnect bug,"if a storm worker die then restarted and the nimbus  reassign the task on the restarted worker with same host:port， the upstream task will not reconnect the host:port and the topology will hang up。

because the connect has broken and netty Client flush timer 
if (null != channel && channel.isWritable()) {
                            flush(channel);
}
always false."
STORM-553,Provide standard Java/Clojure Code Style Formatter profiles,
STORM-551,"Will be better if Use ""storm.conf.dir""  instead of  ""storm.conf.file""",use --config  given the conf dir 
STORM-550,"get an error when use ""--config"" option to override config file","{code:title=Utils.java|borderStyle=solid}
Index: src/jvm/backtype/storm/utils/Utils.java
===================================================================
--- src/jvm/backtype/storm/utils/Utils.java	(revision 4021)
+++ src/jvm/backtype/storm/utils/Utils.java	(working copy)
@@ -19,7 +19,10 @@
 
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.FileNotFoundException;
 import java.io.FileOutputStream;
+import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
@@ -31,6 +34,7 @@
 import java.nio.channels.Channels;
 import java.nio.channels.WritableByteChannel;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -44,6 +48,7 @@
 
 import backtype.storm.serialization.DefaultSerializationDelegate;
 import backtype.storm.serialization.SerializationDelegate;
+
 import org.apache.curator.framework.CuratorFramework;
 import org.apache.curator.framework.CuratorFrameworkFactory;
 import org.apache.commons.lang.StringUtils;
@@ -121,6 +126,27 @@
             throw new RuntimeException(e);
         }
     }
+    
+	@SuppressWarnings(""unchecked"")
+	public static Map<String, Object> loadConfigFile(String file) {
+		Map<String, Object> result = new HashMap<String, Object>();
+		if (file == null) {
+			return result;
+		}
+		Yaml yaml = new Yaml();
+		InputStream in;
+		try {
+			in = new FileInputStream(new File(file));
+			Object obj = yaml.load(in);
+			if (!(obj instanceof Map)) {
+				return Collections.<String, Object> emptyMap();
+			}
+			result = (Map<String, Object>) obj;
+		} catch (FileNotFoundException e) {
+			throw new RuntimeException(e);
+		}
+		return result;
+	}
 
     public static Map findAndReadConfigFile(String name, boolean mustExist) {
         try {
@@ -187,7 +213,7 @@
         if (confFile==null || confFile.equals("""")) {
             storm = findAndReadConfigFile(""storm.yaml"", false);
         } else {
-            storm = findAndReadConfigFile(confFile, true);
+            storm = loadConfigFile(confFile);
         }
         ret.putAll(storm);
         ret.putAll(readCommandLineOpts());
{code}


bin/storm --config ~/deploy/storm-conf/storm.yaml  supervisor 

{code:title=Could not find config file on classpath|borderStyle=solid}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:171)
	at backtype.storm.config$loading__4910__auto__.invoke(config.clj:17)
	at backtype.storm.config__init.load(Unknown Source)
	at backtype.storm.config__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at clojure.lang.RT.loadClassForName(RT.java:2098)
	at clojure.lang.RT.load(RT.java:430)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5417)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:621)
	at clojure.core$use.doInvoke(core.clj:5507)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at backtype.storm.daemon.common$loading__4910__auto__.invoke(common.clj:16)
	at backtype.storm.daemon.common__init.load(Unknown Source)
	at backtype.storm.daemon.common__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at clojure.lang.RT.loadClassForName(RT.java:2098)
	at clojure.lang.RT.load(RT.java:430)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5417)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:621)
	at clojure.core$use.doInvoke(core.clj:5507)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at backtype.storm.daemon.supervisor$loading__4910__auto__.invoke(supervisor.clj:16)
	at backtype.storm.daemon.supervisor__init.load(Unknown Source)
	at backtype.storm.daemon.supervisor__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at clojure.lang.RT.loadClassForName(RT.java:2098)
	at clojure.lang.RT.load(RT.java:430)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.lang.Var.invoke(Var.java:415)
	at backtype.storm.daemon.supervisor.<clinit>(Unknown Source)
Caused by: java.lang.RuntimeException: Could not find config file on classpath /home/caokun/deploy/storm-conf/storm.yaml
	at backtype.storm.utils.Utils.findAndReadConfigFile(Utils.java:129)
	at backtype.storm.utils.Utils.readStormConfig(Utils.java:190)
	at backtype.storm.utils.Utils.<clinit>(Utils.java:71)
	... 57 more
Could not find the main class: backtype.storm.daemon.supervisor.  Program will exit.
{code}"
STORM-548,"Receive Thread Shutdown hook should connect to local hostname but not ""localhost"" ","backtype.storm.messaging.loader#launch-receive-thread!
kill-socket should connect to local hostname but not ""localhost""

See Code Line 72:
https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/messaging/loader.clj#L72


{code:title=loader.clj|borderStyle=solid}
Index: src/clj/backtype/storm/messaging/loader.clj
===================================================================
--- src/clj/backtype/storm/messaging/loader.clj	(revision 4017)
+++ src/clj/backtype/storm/messaging/loader.clj	(working copy)
@@ -65,11 +65,12 @@
    :kill-fn (fn [t] (System/exit 1))
    :priority Thread/NORM_PRIORITY]
   (let [max-buffer-size (int max-buffer-size)
+        local-hostname (memoized-local-hostname)
         socket (.bind ^IContext context storm-id port)
         thread-count (if receiver-thread-count receiver-thread-count 1)
         vthreads (mk-receive-threads context storm-id port transfer-local-fn daemon kill-fn priority socket max-buffer-size thread-count)]
     (fn []
-      (let [kill-socket (.connect ^IContext context storm-id ""localhost"" port)]
+      (let [kill-socket (.connect ^IContext context storm-id local-hostname port)]
         (log-message ""Shutting down receiving-thread: ["" storm-id "", "" port ""]"")
         (.send ^IConnection kill-socket
                   -1 (byte-array []))
 {code}  "
STORM-547,Build Problem(s),"David-Laxers-MacBook-Pro:leiningen davidlaxer$ which lein
/Users/davidlaxer/bin/lein
David-Laxers-MacBook-Pro:leiningen davidlaxer$ lein
Could not find artifact leiningen-core:leiningen-core:jar:2.5.1-SNAPSHOT in clojars (https://clojars.org/repo/)
This could be due to a typo in :dependencies or network issues.
If you are behind a proxy, try setting the 'http_proxy' environment variable.
David-Laxers-MacBook-Pro:leiningen davidlaxer$ cd
David-Laxers-MacBook-Pro:~ davidlaxer$ which lein
/Users/davidlaxer/bin/lein
David-Laxers-MacBook-Pro:~ davidlaxer$ lein
Leiningen is a tool for working with Clojure projects.

Several tasks are available:
bluuugh             Dummy task for tests.
change              Rewrite project.clj by applying a function.
check               Check syntax and warn on reflection.
classpath           Print the classpath of the current project.
clean               Remove all files from project's target-path.
compile             Compile Clojure source into .class files.
deploy              Build and deploy jar to remote repository.
deps                Download all dependencies.
do                  Higher-order task to perform other tasks in succession.
downloads           Calculate download statistics from logs.
echo                Task: 'echo' not found
help                Display a list of tasks or help for a given task.
install             Install the current project to the local repository.
jar                 Package up all the project's files into a jar file.
javac               Compile Java source files.
new                 Generate project scaffolding based on a template.
one-or-two          Dummy task for tests
plugin              DEPRECATED. Please use the :user profile instead.
pom                 Write a pom.xml file to disk for Maven interoperability.
pprint              Task: 'pprint' not found
leiningen.project  Problem loading: java.lang.RuntimeException: Unable to resolve symbol: defproject in this context, compiling:(leiningen/project.clj:4:1)
release             Perform :release-tasks.
repl                Start a repl session either with the current project or standalone.
retest              Run only the test namespaces which failed last time around.
run                 Run a -main function with optional command-line arguments.
search              Search remote maven repositories for matching jars.
show-profiles       List all available profiles or display one if given an argument.
sirius              Task: 'sirius' not found
test                Run the project's tests.
trampoline          Run a task without nesting the project's JVM inside Leiningen's.
uberjar             Package up the project files and dependencies into a jar file.
update-in           Perform arbitrary transformations on your project map.
upgrade             Upgrade Leiningen to specified version or latest stable.
var-args            Dummy task for tests.
vcs                 Interact with the version control system.
version             Print version for Leiningen and the current JVM.
with-profile        Apply the given task with the profile(s) specified.
zero                Dummy task for tests.

Run `lein help $TASK` for details.

Global Options:
  -o             Run a task offline.
  -U             Run a task after forcing update of snapshots.
  -h, --help     Print this help or help for a specific task.
  -v, --version  Print Leiningen's version.

See also: readme, faq, tutorial, news, sample, profiles, deploying, gpg,
mixed-source, templates, and copying.
David-Laxers-MacBook-Pro:~ davidlaxer$ 
"
STORM-546,Local hostname configuration ignored by executor,"The executor reports hostname using `util/memoized-local-hostname`, but doesn't check the configuration to see whether `STORM_LOCAL_HOSTNAME` has been set. Under this change the executor checks the configuration before falling back to guessing the local hostname."
STORM-545,DRPC does not return a cause of failure,"Throwing the {{backtype.storm.topology.FailedException}} is a way  to report an error. Unfortunately the clue of the error is not transmitted to the {{backtype.storm.utils.DRPCClient}}. {{DRPCClient}} catches {{backtype.storm.generated.DRPCExecutionException}} that does not carry any cause. It carries a static message ??""Request failed""??.
The optimal solution would be carrying {{FailedException}} or its subclass, or at least its message.
"
STORM-543,Storm failing all input messages (although successfully processed) post restarting worker on the same supervisor slot,"Storm failing all input messages (although successfully processed) post restarting worker on the same supervisor slot.

Steps to simulate the behaviour,
1. Run topology(spout as single instance and multiple instances of bolts)
on multiple workers.
2. Identify the slot on which the single spout instance is running (from STORM UI) and kill it (using kill -9)
3. See if the supervisor started the worker on the same supervisor port. If not then repeat step 2 untill you get supervisor on the same slot as previous one.
4. Pump in a message into the topology.
5. You will see message being processed successfully but acker failing the tuple after the message times out (defaults to 30 secs).
"
STORM-542,Topology summary executors/workers count labels are mixed up,"The ""Num executors"" and ""Num workers"" labels under the ""Topology summary"" section in the main storm ui page are mixed up, their order should be the other way around."
STORM-536,java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/TBinaryProtocol on windows,"When I run a program which works with Storm and cassandra in windows7 I got the following error:

java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/TBinaryProtocol
	at com.netflix.astyanax.thrift.ThriftSyncConnectionFactoryImpl.createConnection(ThriftSyncConnectionFactoryImpl.java:89) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.SimpleHostConnectionPool.tryOpenAsync(SimpleHostConnectionPool.java:416) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.SimpleHostConnectionPool.borrowConnection(SimpleHostConnectionPool.java:181) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.RoundRobinExecuteWithFailover.borrowConnection(RoundRobinExecuteWithFailover.java:66) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.AbstractExecuteWithFailoverImpl.tryOperation(AbstractExecuteWithFailoverImpl.java:67) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.AbstractHostPartitionConnectionPool.executeWithFailover(AbstractHostPartitionConnectionPool.java:253) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl.executeOperation(ThriftKeyspaceImpl.java:465) ~[Storm-starter.jar:na]
"
STORM-535,setup 'java.library.path' for native-storm code if necessary," JAVA_LIBRARY_PATH=${STORM_HOME}/lib/native/${JAVA_PLATFORM}

eg:
If run storm on amd64 , then will add following into  JAVA_LIBRARY_PATH
${STORM_HOME}/lib/native/Linux-amd64-64/libsigar-amd64-linux.so

If run storm on x86_64 , then will add following into  JAVA_LIBRARY_PATH
${STORM_HOME}/lib/native/Linux-x86_64-64/libsigar-x86-linux.so
"
STORM-534,Store Nimbus Server Information in zookeeper path {storm.zookeeper.root}/nimbus,"1) {nimbus.host} {nimbus.thrift.port} {storm.version} will be stored in  {storm.zookeeper.root}/nimbus like  ""localhost:8826:0.9.3-r1234""

2) Storm Clients only need to configure {storm.zookeeper.root} to get Nimbus Server Information, Configuration like {nimbus.host} {nimbus.thrift.port} {storm.version} will be ignored"
STORM-532,"Supervisor should restart worker immediately, if the worker process does not exist any more ","For now 
if the worker process does not exist any more 
Supervisor will have to wait a few seconds for worker heartbeart timeout and restart worker .

If supervisor knows the worker processid  and check if the process exists in the sync-processes thread ,may need less time to restart worker.

1: record worker process id in the worker local heartbeart 
2: in supervisor  sync-processes ,get process id from worker local heartbeat 
and check if the process exits 
3: if not restart it immediately"
STORM-530,Upgrade version of HttpClient,"Storm 0.9.2 seems to ship with version 4.1.1 of HttpClient in its lib directory.  This version is very out dated (GA in May of 2011) and becomes problematic when integrating tools like Solr with Storm.   Solr 4.7.2 for example depends on a much newer version 4.3.1 (GA in October 2013).  This may be resolved by STORM-447, but wanted to make sure this was part of that fix.

The workaround seems to be replacing the older versions in the lib directory with newer ones, but that shouldn't be required for such a simple integration."
STORM-528,examples/storm-starter/multilang/resources/storm.py diverged,"As we know storm.py is committed three times. One of these files (the one in examples) have diverged from the other two. It also introduces some strange behavior as it calls ""log"" instead of ""error"" when a bolt fails.
I would recommend to override the storm/examples version with the one in storm-core and then have the diff as a new pull request"
STORM-526,Nimbus triggered complete removal of all topologies due to maintenance in 2 out of 3 zookeeper servers,"We use a cluster of 3 zookeepers, all 3 ip addresses are in the storm.yml file. We were restarting one zookeeper, and once it was ready, we restarted the second zookeeper. All this time the third zookeeper was ""green"" (as monitored by Netfix Exhibitor).

At this same time nimbus has ""decided"" to remove all topologies (log entry is ""Corrupt topology my-topology-xxx has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up..."").

I looked at the relevant code and I am not entirely sure the log message describes correctly the code.

Could anyone please read the nimbus.clj#cleanup-corrupt-topologies and explain under what conditions does nimbus act in that way ?
https://github.com/apache/storm/blob/v0.9.2-incubating/storm-core/src/clj/backtype/storm/daemon/nimbus.clj#L854


Log file:
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-1-2-1412151059 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-0-1-1412151059 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-3-4-1412151062 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-2-3-1412151060 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Starting Nimbus server...
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-1-2-1412151059
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-0-1-1412151059
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-3-4-1412151062
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-2-3-1412151060
2014-10-01 10:52:16 b.s.d.nimbus [INFO] Shutting down master


"
STORM-524,Please create a DOAP file for your TLP,"Please can you set up a DOAP for your project and get it added to files.xml?

Please see http://projects.apache.org/create.html

Once you have created the DOAP and committed it to your source code repository, please submit it for inclusion in the Apache projects listing as per:

http://projects.apache.org/create.html#submit

Remember, if you ever move or rename the doap file in future, please
ensure that files.xml is updated to point to the new location.

Thanks!"
STORM-522,NullPointerException in storm.kafka.ZkCoordinator,"We've been seeing these about once a week. The nimbus will kill and reassign the topology (on the same hardware) get caught up to ""now"" in kafka and happily continue running until the next time it encounters an event like this.

{code}
2014-09-30 13:59:40 s.k.ZkCoordinator [INFO] Deleted partition managers: [10.1.2.1:9092:0, 10.1.2.2:9092:1, 10.1.2.3:9092:3]
2014-09-30 13:59:40 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:148) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:77) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:94) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__5573$fn__5588$fn__5617.invoke(executor.clj:563) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55]
Caused by: java.lang.NullPointerException: null
        at storm.kafka.DynamicPartitionConnections.unregister(DynamicPartitionConnections.java:39) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.close(PartitionManager.java:205) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:135) ~[stormjar.jar:na]
        ... 6 common frames omitted
2014-09-30 13:59:40 b.s.d.executor [ERROR]
java.lang.RuntimeException: java.lang.NullPointerException
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:148) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:77) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:94) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__5573$fn__5588$fn__5617.invoke(executor.clj:563) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55]
Caused by: java.lang.NullPointerException: null
        at storm.kafka.DynamicPartitionConnections.unregister(DynamicPartitionConnections.java:39) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.close(PartitionManager.java:205) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:135) ~[stormjar.jar:na]
        ... 6 common frames omitted
2014-09-30 13:59:40 b.s.util [INFO] Halting process: (""Worker died"")
{code}"
STORM-521,UI: No way of telling easily that tuples failed because of a TOPOLOGY_MESSAGE_TIMEOUT_SECS,"In the UI and in general there's no easy way to figure out that tuples have failed because of a timeout becoming fully processed.  

It would really great if one could tell what was the last bolt a tuple in the tuple DAG got processed in before the whole tree failed because of a timeout."
STORM-520,Using storm.local.hostname causes bolts to not communicate with each other,"In our setup we run different supervisors as docker container on top of CoreOS.

If I use docker's -h (host) and give the container the same hostname as the host machine, the bolts are able to register with nimbus correctly, talk to each other as well as transfer data inside the same container between different workers located on the same machine.

Utilizing 'storm.local.hostname' instead of using docker's -h option results in the bolts having a hard time (read: unable to) communicating between workers on the same machine/container (we only run one container per physical machine which contains 4 workers + supervisor) and having sporadic success (50/50) communicating with bolts in different machines."
STORM-518,"subprocess.py not found while executing ""bin/storm nimbus"" python startup script error ","While starting storm via script ""bin/storm nimbus"" in 0.9.2
It kept giving the following python error under Ubuntu 12.

------------------------ERROR---------------------------------------------
 p = sub.Popen(command, stdout=sub.PIPE)

  File ""/usr/lib64/python2.6/subprocess.py"", line 639, in __init__
    errread, errwrite)

  File ""/usr/lib64/python2.6/subprocess.py"", line 1228, in _execute_child
    raise child_exception

OSError: [Errno 2] No such file or directory
-------------------------------------------------------------------------------
by putting additional
""shell=True"" param in each call of  p = sub.Popen(command, stdout=sub.PIPE) solved the issue.

"
STORM-517,"Support for ""-Dservice="" in bin/storm, via JAVA_SERVICE_NAME environment variable","*Reasoning:*

Currently the way that _bin/storm_ starts the various storm processes (nimbus, ui, supervisor) results in a process list entry like the following, for each process:

{code}
ubuntu   21940  0.4  3.9 2294904 151384 ?      Ssl  Oct03   2:47 java -server -Dstorm.options= -Dstorm.home=/home/ubuntu/apache-storm-0.9.2-incubating -Djava.library.path=/usr/local/lib -Dstorm.conf.file= -cp /home/ubuntu/apache-storm-0.9.2-incubating/lib/asm-4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-logging-1.1.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-devel-0.3.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/storm-core-0.9.2-incubating.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/log4j-over-slf4j-1.6.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/tools.cli-0.2.4.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-servlet-0.3.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-codec-1.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/logback-classic-1.0.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clj-time-0.4.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/httpclient-4.3.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/slf4j-api-1.6.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/httpcore-4.3.2.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/carbonite-1.4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clout-1.0.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jetty-6.1.26.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clojure-1.5.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-io-2.4.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-exec-1.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jgrapht-core-0.9.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/curator-client-2.4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/tools.macro-0.1.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/zookeeper-3.4.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jline-2.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/minlog-1.2.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/tools.logging-0.2.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/reflectasm-1.07-shaded.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/guava-13.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-core-1.1.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/logback-core-1.0.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/netty-3.6.3.Final.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-jetty-adapter-0.3.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/core.incubator-0.1.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-lang-2.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/netty-3.2.2.Final.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/curator-framework-2.4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/disruptor-2.10.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/servlet-api-2.5-20081211.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clj-stacktrace-0.2.4.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/chill-java-0.3.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/compojure-1.1.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/kryo-2.21.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jetty-util-6.1.26.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/servlet-api-2.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/json-simple-1.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-fileupload-1.2.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/snakeyaml-1.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/math.numeric-tower-0.0.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/joda-time-2.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/hiccup-0.3.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/objenesis-1.2.jar:/home/ubuntu/apache-storm-0.9.2-incubating/conf -Xmx1024m -Dlogfile.name=nimbus.log -Dlogback.configurationFile=/home/ubuntu/apache-storm-0.9.2-incubating/logback/cluster.xml backtype.storm.daemon.nimbus
{code}

This means that it can be difficult to differentiate between the processes in the process list and monitoring tools that use SNMP to get the process list (such as Zenoss) can't differentiate because the process list arguments gets truncated way before the list of .jars in the text stops being the same.

Ideally we'd be able to set *-Dservice=foo* in _bin/storm_ via an environment variable, so it's easy to set with things like upstart, runit, etc

*Fix:*

I've added support for *-Dservice=* in bin/storm, via a JAVA_SERVICE_NAME environment variable, in _bin/storm_

You can see the changes I made against _bin/storm_ from 0.9.2-incubating in https://github.com/solarce/storm-cookbook/blob/master/templates/default/bin_storm.py.erb and how I use it in an upstart template, https://github.com/solarce/storm-cookbook/blob/master/templates/default/storm-upstart-conf.erb#L19

I'm going to submit the same changes against master on github"
STORM-516,Thrift source files in storm-core.jar remain in package org.apache.thrift instead of thrift7,"All thrift *.java source files in the storm-core package remain in the package org.apache.thirft instead of org.apache.thrift7

The compiled *.class files however get relocated by the shade plugin

steps to reproduce:
1) build storm using mvn package -DskipTests
2) unzip -p storm-core/target/storm-core-0.9.2-incubating.jar org/apache/thrift7/TException.java

results in:
snip
package org.apache.thrift;
snip

The problem seems to have been introduced with this commit: https://github.com/apache/storm/commit/22ddd6e6d5c78e36610366e71ea879b283575e01"
STORM-514,Update storm-starter README now that Storm has graduated from Incubator,The usage instructions in the storm-starter README still refer to the pre-graduation code setup (e.g. git repo urls).
STORM-510,Netty messaging client blocks transfer thread on reconnect,"The latest netty client code will attempt to reestablish the connection on failure as part of the send method call.  It will block until the connection is established or a timeout happens, by default this is about 30 seconds, which is also the default tuple timeout.  

This is exacerbated by the read lock that is held during the send, that prevents the node->socket mapping from changing while we are sending.  This is mostly so that we don't close connections while we are trying to write to them, which would cause an exception.  But this makes it so if there are multiple workers on a node that all get rescheduled we will wait the full 30 seconds to timeout for each worker.

send must be non-blocking in the current design of the worker, or it will prevent other messages from being delivered, and is likely to cause many many messages to timeout on a reschedule."
STORM-498,storm-kafka: make ZK connection timeout configurable in Kafka spout,"Currently the Kafka spout uses a hardcoded ZK connection timeout of 15 seconds in {{external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java}}:

{code}
_curator = CuratorFrameworkFactory.newClient(
    zkStr,
    Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT)),
    15000,
    new RetryNTimes(Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_TIMES)),
        Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL))));
{code}

We should make this setting configurable, similar to the ZK session timeout setting."
STORM-494,Update lein install instructions,"https://storm.incubator.apache.org/documentation/Maven.html is currently quite out of date. It took me a while to figure out that I need to add (specifically)

    [org.apache.storm/storm.core ""0.9.2-incubating""]

versus the old `[storm ""0.9.0.1""]`."
STORM-492,Test timeout should be configurable,Test timeout is hard coded to 5000ms in https://github.com/apache/incubator-storm/blob/master/storm-core/src/clj/backtype/storm/testing.clj. Provide a way to override this value in order to handle longer running tests.
STORM-491,(Security) Security Enhancements Break Storm on Windows,"Some of the enhancements made under the security branch will break functionality of Storm in windows environments.

I'm fine with making support for security on Windows a separate effort/jira, but basic Storm functionality (i.e. with security turned off) should work under windows before we consider merging the security branch to master.

"
STORM-484,Implement topology validation logic in TopologyBuilder,"Currently topology validation occurs only when submitting the topology to nimbus.  Once a topology is built with TopologyBuilder.createTopology() all relevant data to validate a topology is correctly wired already exists.  However, TopologyBuilder.createTopology() incurs the cost of serializing the ComponentObject which isn't needed to validate the topology is wired correctly.  The operation of validating a topology can provided in TopologyBuilder, or a helper class like TopologyValidator, to validate a topology while it is being built by consumers without the cost of serializing ComponentObjects or submitting to nimbus."
STORM-481,A specific trident topology pattern causes batches to never complete,"We've discovered that a very specific pattern of partitioning and state persists (that we happen to use) causes a trident topology to never commit batches.

We've boiled the pattern we use down to this:

{code}
        /->Op->\     /->Op->\
Spout ->-------->->->-------->-x-> Persist
                   \-> Persist
{code}

If there is a {{partitionBy}} at x, then the batches never complete. If the partitioning is not there, then the topology works fine.

This is reproduced by the attached java testcase. The test will hang (FeederBatchSpout hangs on acquiring a semaphore) if {{failTest}} is true"
STORM-470,DisruptorQueue catch blocks do not capture stack trace,"The catch blocks for many of the Exceptions in the DisruptorQueue.java file do not extract the stack trace for debugging with the result being that errors cannot readily be diagnosed. The stack trace output should become part of the subsequent Runtime exception text such that it can be used in the diagnosis of problems. As it is now, all that a person gets is the error message which depends highly on the quality of the error text that the code author wrote for the class that raised the error. In many cases, this can be poor."
STORM-469,Storm UI Last Error Detail Insufficient for debugging,The error text that is captured in the Storm UI is insufficient to debug the issue as it gets cut off and does not link to the actual error to allow a person to get more detail. This means that the debugging is unnecessarily blind. This prevents diagnosis of the error.
STORM-467,storm jar executes non-existant or old jar files and classes,"When issuing the storm jar command, the command will launch with some cached version of the jar contents even if no jar file is now present at the location specified on the command line. This should instead cause an error so that a user is actually running what they think they are.

The second part of this is that some part of storm is caching topology classes so that when debugging errors, old code is executed instead of the new version of a class. I would argue that storm should attempt to destroy cached topology classes if presented with a new version or when an active topology is terminated. Again this is to avoid running versions of code that are not those which have been specified."
STORM-465,Hook API for Trident Operations,"The existing hooks API only makes sense at the level of storm core components.

In

https://github.com/apache/incubator-storm/pull/235

I tried adding them anyway but that didn't fly, but I still want hooks! It's not clear to me how granular a hooks API for Trident. There could be operation-specific hooks (e.g. methods specifically for aggregators, functions, spouts, etc) or perhaps there is a more generic way of inserting hooks. "
STORM-462,Find a way to only have one copy of storm.rb and storm.py,"We currently have two copies of storm.py and storm.rb, I would really like to have only one of each.  "
STORM-459,multilang seems had read from pipe but didn't clear the spout process's STDOUT,"Hi,All
     I'am using the storm's multilang throughing PHP. But it seems have some problem.
    Then my spout is a php script which read contents from a file.And in the beginning of the 4Mb content,which runs correctly.But the php process will blocks in the write(1,xxxx...,when i strace -p the php spout.
     But when i use 'cat' read from the /proc/pid/fd/1 which is the php process STDOUT. All of the content had been handle correctly in the bolt is still there.
     So, i doubt that spout hangs out when write(1,xxx , is because that the process's STDOUT buffer is full of contents. But how can i prevent this issue.
      My environment is
             Linux : 2.6.32_1-9-3-1 kernel.
             Java: 1.6.0_33
             Storm:0.8.2 or 0.8.1 
             PHP: 5.2


                                                                                                    Thx
                                                                                                      Sean"
STORM-452,Update link to Perl multilang implementation,"The current link on the [DSL and Multilang adapters|https://storm.incubator.apache.org/documentation/DSLs-and-multilang-adapters.html] page for IO::Storm is out of date.  I have recently taken over as the maintainer, and now it actually works with the latest version of Storm.  Please link to my fork at https://github.com/dan-blanchard/io-storm

Also, there's currently no link for the Python [streamparse|https://github.com/Parsely/streamparse] module, which provides both a more robust Python Multilang adapter than the included {{storm.py}} and a bunch of handy utilities for creating Python Storm projects."
STORM-440,NimbusClient throws NPE if Config.STORM_THRIFT_TRANSPORT_PLUGIN is not set,"We just upgraded from 0.8.2 to 0.9.2 and noticed that when constructing a NimbusClient if Config.STORM_THRIFT_TRANSPORT_PLUGIN is not specified then AuthUtils[1] throws a NPE.

[1] - https://github.com/bbaugher/incubator-storm/blob/master/storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java#L73-L74"
STORM-438,SimpleACLAuthorizer should allow users with same keytab as supervisor to perform user operations,"Storm security allows user to provider jaas.conf with StormServer and StormClient. If the user who is submitting a topology uses StormClient keytab  than it would throw AuthorizationException. In SimpleACLAuthorizer we check if supervisor_users contains context user if that matches we return true or false if the operation requested is a supervisor operation.
In the above case it would return false as user exists in supervisors and the operation requested would be ""getClusterInfo"". This shouldn't fail since its part of userOperations."
STORM-436,Clean up Clojure namespace declarations,"Some of the Clojure namespace declarations in the storm project are messy and non-idiomatic. https://github.com/apache/incubator-storm/blob/master/storm-core/src/clj/backtype/storm/ui/core.clj#L18-L38 is a good example of this.

There are a few things I'd like to improve:
1. Coalesce multiple use/require/import's into a single use/require/import.
2. Order imports use, require, import.
3. Optionally, replacing use with some mix of
    * [... :refer :all]
    * Referring just the vars that are used
    * Qualifying the namespace imports

Is there a reason why the namespaces were done in the way they have been? What would be the preferred way to do 3?"
STORM-433,Give users visibility to the depth of queues at each bolt,"I envision being able to browse the Storm UI and see where queues of tuples are backing up.

Today if I see latencies increasing at a bolt, it may not be due to anything specific to that bolt, but that it is backed up behind an overwhelmed bolt (which has too low of parallelism or too high of latency).

I would expect this could use sampling like the metrics reported to the UI today, and just retrieve data from netty about the state of the queues. I wouldn't imagine supporting zeromq on the first pass."
STORM-432,Enable version-agnostic serialization of storm's metadata,"For STORM-376, we've enabled compression of storm's metadata it serializes with Utils#serialize and Utils#deserialize. To facilitate simpler upgrades, we should add functionality to handle deserializing data written by a different version than that running."
STORM-431,Topology back pressure using netty transport,"There are several community threads that talk about automatically managing back pressure based on send and receive buffers between the spouts and the bolts. This works perfectly in a single worker scenario. Supposedly, it doesn't work across multiple workers because of zmq deficiency. Now that netty has replaced zmq as a network transport, is it possible to officially support buffer lengths as a flow control mechanism in non-reliable topologies with high throughput needs?"
STORM-430,(Security) Allow netty SASL to support encryption/decryption,"SASL provides more then just authentication, it can also provide integraty guarantees.
as described here http://docs.oracle.com/javase/7/docs/api/javax/security/sasl/Sasl.html#QOP
and http://docs.oracle.com/javase/7/docs/technotes/guides/security/sasl/sasl-refguide.html

In order to provide those guarantees encryption is used, and the wrap/unwrap methods for the SaslClient and Server must be used.  It would be great to support this for storm as well, allowing users to configure the level of security they want."
STORM-429,Document Storm vs. Spark Streaming,"There's been a lot of press comparing storm with Spark streaming. Much of which is inaccurate in terms of Storm's features and performance.

We should provide open (I.e.source) and honest counters to inaccurate claims."
STORM-426,Upgrade to Clojure 1.6,"It would be very nice if it was possible to use Clojure 1.6 with Storm.

I'm currently testing it at the moment, and the storm-core tests fail, but I'm struggling to narrow down exactly what the issue is.

With three runs of {{mvn install}} on storm-core, I get 3 different issues/exceptions.

* A FileNotFoundException (test1.log)
* The tests hang and never complete (test2.log)
* A report that 'There were some failures', but I struggle to find where exactly the failure is amongst all the log files. (test3.log)

I've attached the test logs in question.

For reference I also included master.log to show my results when running tests against master.

To determine the issues, I have run each of the tests individually from the REPL."
STORM-425,Storm Integration with YARN Timeline Server,"Once Storm topologies die or are moved by Nimbus, the information about previous assignments is gone as far as the UI is concerned, which makes it difficult to find log files and debug. The YARN Timeline server offers an API to keep track the history of a topology, and I've attached a proposal document outlining a potential architecture for interfacing with the timeline server.

I also have a proof-of-concept running which is available here:
https://github.com/knusbaum/incubator-storm/tree/timeline-server-POC"
STORM-423,Allow a SpoutConfig to specify to begin at the end/newest entry on the topic,"Based on this exchange 
http://mail-archives.apache.org/mod_mbox/incubator-storm-user/201407.mbox/%3C1406236756.15830.145373285.397958AA%40webmail.messagingengine.com%3E

It would be nice to be able to set some parameter in the SpoutConfig that would allow a user to specify that the spout should begin with the end/newest message on the topic.  In case there was historical information on a topic that we don't care about."
STORM-418,using storm kafka spout doesn't scale when increasing topic partition,"We are running a topology of kafka spout + single bolt on a single supervisor machine.

Bolt functionality is limited to log parsed data from spout only.

When Trying to consume same type of data by different topics using varying topic partitions topology throughput remain same in order of 70000 msg/sec.

However, when multiple spout instances are used in same topology , we were able to increase topology throughput to more than 100,000 msg/sec.

 We have tried to increase kafka topic partition from 8 to 200, however topology throughput remains same for sinlge spout instance."
STORM-416,Allow user defined system streams,"See discussion in [https://mail-archives.apache.org/mod_mbox/incubator-storm-user/201407.mbox/%3C1405958743.31781.144006701.0C306616%40webmail.messagingengine.com%3E]

We currently get an exception when defining a stream starting with ""__"". There are cases where we might want to be able to do that, e.g. treat a stream as a system one so that its stats can be hidden in the UI just like any other system stream. 

As suggested by Harsha in the mail thread above 

bq. we can probably add a exception in nimbus if a stream starts with ""__"" and its in storm.user.system.streams.

"
STORM-413,Investigate Tests using Simulated Time and Mocking,"Several issues with running storm's tests have arisen that are related to timing and race conditions.

Hang: STORM-200 (https://github.com/apache/incubator-storm/commit/573c42a64885dac9a6a0d4c69a754500b607a8f1)

Unexpected Exception: STORM-403

We should investigate tests that use simulated time while mocking out functions to ensure that bindings are properly set where they need to be."
STORM-411,Extend file uploads to support more distributed cache like semantics,"One of the big features that we are asked about for a hosted storm instance is how to distribute and update large shared data sets with topologies.  These could be things like ip to geolocation tables, machine learned models or just about anything else.

Currently with storm you either have to package it as part of your topology jar, install in on the machine already, or access an external service to pull the data down.  Packaging it in the jar does not allow users to update the dataset without restarting their topologies, installing it on the machine will not work for a hosted storm solution, and pulling it form an external service without the supervisors being aware of it would mean it would be downloaded multiple times, and may not be cleaned up properly afterwards.

I propose that instead we setup something similar to the distributed cache on Hadoop, but with a pluggable backend.  The APIs would be for a simple blobstore so they could be backed by local disk on nimbus, HDFS, swift, or even bittorrent.

Adding new ""files"" to the blob store or downloading them would by default go through nimbus, but if an external store is properly configured direct access into the store could be used.

The worker process would access the files through symlinks in the current working directory of the worker.  For posix systems when a new version of the file is made available the symlink would atomically be replaced by a new one pointing to the new version.  Windows does not support atomic replacement of a symlink so we should provide a simple library that will return resolved paths to be used, and can detect when the links have changed, but have some retry logic built in, if the symlink disappears in the middle.

We are in the early stages of implementing this functionality and would like some feedback on the concepts before getting too far along. "
STORM-409,(Security) add groups authorization support to DRPC,"Once STORM-407 goes in DRPC should hopefully not be slowed down by checking for groups.  We should add in support for groups in the ACLs, mostly for those submitting requests, but support on the topology side would be good too."
STORM-394,"Messages has expired, OFFSET_OUT_OF_RANGE, new offset startOffsetTime, no new messages, again and again","Issue created here (https://github.com/wurstmeister/storm-kafka-0.8-plus/issues/55) but closed since the module is maintened under the Storm umbrella now.

I think there might be a case that is not covered :
0) messages in Kafka has expired
1) so offset stored in Zookeeper are no longer valid
2) error OFFSET_OUT_OF_RANGE is thrown
3) getOffset with startOffsetTime
4) retry the fetch with the returned startOffset
5) get an ByteBufferMessageSet but empty

KafkaUtils.fetchMessages seeems to be called again and again with the old offset and we get to step 2 again.
I guess the new startOffset is not commited to Zookeeper since we do not have new messages.

This can happen in the case of a topology restart, so it goes through the TridentKafkaEmitter.reEmitPartitionBatch"
STORM-393,Add topic to KafkaSpout output,"It would be beneficial to have topic as a tuple value emitted from KafkaSpout.
Not only it is useful if STORM-392 is implemented, but also in case when we have more than one KafkaSpout in a system"
STORM-392,Implement WhiteList topics for KafkaSpout,"A ""default"" high-level kafka consumer has the ability to read from multiple topics using a WhiteList notation (which is basically just a regex).

It would be great to have that ability in KafkaSpout too."
STORM-391,KafkaSpout to await for the topic,"When topic does not yet exist and the consumer is asked to consume from it, the default behaviour for Kafka heigh-level consumer is to ""await"" for the topic without a failure.

KafkaSpout currently fails trying to get the partition information about the topic that does not exist.

It may be a good idea to have the same common behaviour in KafkaSpout and it can probably be implemented through the zookeeper watchers: if topic does not exist, then set up a watcher and don't do anything until it yields."
STORM-389,Wrong default wait time when killing a topology from the UI,"When a topology is killed from the Storm UI, the wait time displayed in the confirm box is always the default value, 30 secs. The ""topology.message.timeout.secs"" setting is not used."
STORM-385,Clojure code examples not displaying with any formatting,The Clojure source code examples on the Clojure DSL documentation at http://storm.incubator.apache.org/documentation/Clojure-DSL.html are not displaying with any formatting making them very hard to read.
STORM-383,Test time-outs when running metrics_test/test-builtin-metrics-2,"https://github.com/apache/incubator-storm/pull/38#issuecomment-47627909

In the course of testing a metrics patch, we found that metrics_test/test-builtin-metrics-2 caused test time-outs."
STORM-378,"SleepSpoutWaitStrategy.emptyEmit should use  the variable ""streak""","{code:java}
Index: src/jvm/backtype/storm/spout/SleepSpoutWaitStrategy.java
===================================================================
--- src/jvm/backtype/storm/spout/SleepSpoutWaitStrategy.java	(revision 2868)
+++ src/jvm/backtype/storm/spout/SleepSpoutWaitStrategy.java	(working copy)
@@ -18,6 +18,8 @@
 package backtype.storm.spout;
 
 import backtype.storm.Config;
+import backtype.storm.utils.Utils;
+
 import java.util.Map;
 
 
@@ -27,13 +29,14 @@
     
     @Override
     public void prepare(Map conf) {
-        sleepMillis = ((Number) conf.get(Config.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS)).longValue();
+        sleepMillis = Utils.getLong(
+            conf.get(Config.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS), 500);
     }
 
     @Override
     public void emptyEmit(long streak) {
         try {
-            Thread.sleep(sleepMillis);
+            Thread.sleep(Math.abs(sleepMillis + streak));
         } catch (InterruptedException e) {
             throw new RuntimeException(e);
         }
Index: src/jvm/backtype/storm/utils/Utils.java
===================================================================
--- src/jvm/backtype/storm/utils/Utils.java	(revision 2888)
+++ src/jvm/backtype/storm/utils/Utils.java	(working copy)
@@ -325,6 +325,24 @@
           throw new IllegalArgumentException(""Don't know how to convert "" + o + "" + to int"");
       }
     }
+    
+    public static Long getLong(Object o, long defaultValue) {
+
+      if (o == null) {
+        return defaultValue;
+      }
+
+      if (o instanceof String) {
+        return Long.valueOf(String.valueOf(o));
+      } else if (o instanceof Integer) {
+        Integer value = (Integer) o;
+        return Long.valueOf((Integer) value);
+      } else if (o instanceof Long) {
+        return (Long) o;
+      } else {
+        return defaultValue;
+      }
+    }
 
     public static boolean getBoolean(Object o, boolean defaultValue) {
       if (null == o) {
{code}
"
STORM-377,hs_err_pid.log,"worker died and i found some hs_err_pid.log per minute.
it's fine after i reinstall the zmq
thank you


#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f161193d1c0, pid=11021, tid=139733713794816
#
# JRE version: 6.0_45-b06
# Java VM: Java HotSpot(TM) 64-Bit Server VM (20.45-b01 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libzmq.so.3+0x261c0]  zmq::signaler_t::signaler_t()+0x30
#
# If you would like to submit a bug report, please visit:
#   http://java.sun.com/webapps/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

---------------  T H R E A D  ---------------

Current thread (0x00007f164400a800):  JavaThread ""main"" [_thread_in_native, id=11022, stack(0x00007f164a4fb000,0x00007f164a5fc000)]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x00000000000002f9

Registers:
RAX=0x0000000000000001, RBX=0x0000000000000001, RCX=0x0000000000000004, RDX=0x00007f164a5f9f7c
RSP=0x00007f164a5f9f38, RBP=0x0000000000000011, RSI=0x0000000000000011, RDI=0x0000000000000001
R8 =0x00007f164400b0f0, R9 =0x00000000a328f240, R10=0x00007f164a5f9cf0, R11=0x00007f16119529c0
R12=0x00007f164a5f9f7c, R13=0x0000000000000004, R14=0x00007f164a5fa040, R15=0x00007f164400a800
RIP=0x00007f161193d1c0, EFLAGS=0x0000000000010202, CSGSFS=0xffff000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f164a5f9f38)
0x00007f164a5f9f38:   00007f16119529ed 0000000000000001
0x00007f164a5f9f48:   0000000000000011 00007f164400a9d0
0x00007f164a5f9f58:   0000000000001388 00000000fc7e6a50
0x00007f164a5f9f68:   00007f1611b688f9 0000000000000000
0x00007f164a5f9f78:   00001388fc7e6a50 00000000fc7e6a50
0x00007f164a5f9f88:   00007f164a5fa008 0000000000000000
0x00007f164a5f9f98:   00007f1641010eee 00007f164a5fa088
0x00007f164a5f9fa8:   00007f164a5fa028 00007f16440092b0
0x00007f164a5f9fb8:   00007f1644009688 00007f164400a800
0x00007f164a5f9fc8:   00007f164a5f9fc8 00000000fc7e6a50
0x00007f164a5f9fd8:   00007f164a5fa040 00000000fc7e7170
0x00007f164a5f9fe8:   0000000000000000 00000000fc7e6a50
0x00007f164a5f9ff8:   0000000000000000 00007f164a5fa028
0x00007f164a5fa008:   00007f164a5fa088 00007f1641005a82
0x00007f164a5fa018:   0000000000000000 00007f164100df58
0x00007f164a5fa028:   0000000000001388 0000000000000001
0x00007f164a5fa038:   0000000000000011 00000000a328f240
0x00007f164a5fa048:   00007f164a5fa048 00000000fc7e5109
0x00007f164a5fa058:   00007f164a5fa0a8 00000000fc7e7170
0x00007f164a5fa068:   0000000000000000 00000000fc7e5118
0x00007f164a5fa078:   00007f164a5fa028 00007f164a5fa098
0x00007f164a5fa088:   00007f164a5fa0f0 00007f1641005a82
0x00007f164a5fa098:   0000000000001388 00000000a0e2bf40
0x00007f164a5fa0a8:   00000000a328f240 00007f164a5fa0b0
0x00007f164a5fa0b8:   00000000fc7eccfe 00007f164a5fa118
0x00007f164a5fa0c8:   00000000fc7ed2c8 0000000000000000
0x00007f164a5fa0d8:   00000000fc7ecd40 00007f164a5fa098
0x00007f164a5fa0e8:   00007f164a5fa108 00007f164a5fa170
0x00007f164a5fa0f8:   00007f1641005e03 00000000a328f240
0x00007f164a5fa108:   0000000000000000 0000000000000000
0x00007f164a5fa118:   00000000a2dce6e8 00000000a2dd3548
0x00007f164a5fa128:   00000000a2dfbcd0 00007f164a5fa130 

Instructions: (pc=0x00007f161193d1c0)
0x00007f161193d1a0:   85 c0 75 0f 8b 3b e8 c5 e8 fe ff 8b 7b 04 e8 bd
0x00007f161193d1b0:   e8 fe ff e8 68 54 fe ff 89 43 08 5b c3 90 90 90
0x00007f161193d1c0:   81 bf f8 02 00 00 af ec dd ba 0f 94 c0 c3 66 90
0x00007f161193d1d0:   48 8d 87 00 03 00 00 c3 0f 1f 84 00 00 00 00 00 

Register to memory mapping:

RAX=0x0000000000000001 is an unknown value
RBX=0x0000000000000001 is an unknown value
RCX=0x0000000000000004 is an unknown value
RDX=0x00007f164a5f9f7c is pointing into the stack for thread: 0x00007f164400a800
RSP=0x00007f164a5f9f38 is pointing into the stack for thread: 0x00007f164400a800
RBP=0x0000000000000011 is an unknown value
RSI=0x0000000000000011 is an unknown value
RDI=0x0000000000000001 is an unknown value
R8 =0x00007f164400b0f0 is an unknown value
R9 =
[error occurred during error reporting (printing register info), id 0xb]

Stack: [0x00007f164a4fb000,0x00007f164a5fc000],  sp=0x00007f164a5f9f38,  free space=1019k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libzmq.so.3+0x261c0]  zmq::signaler_t::signaler_t()+0x30

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
j  org.zeromq.ZMQ$Socket.setLongSockopt(IJ)V+0
j  org.zeromq.ZMQ$Socket.setLinger(J)V+17
j  zilch.mq$set_linger.invoke(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+14
j  backtype.storm.messaging.zmq.ZMQContext.connect(Ljava/lang/String;Ljava/lang/String;I)Lbacktype/storm/messaging/IConnection;+91
j  backtype.storm.daemon.worker$mk_refresh_connections$this__5827$iter__5834__5838$fn__5839.invoke()Ljava/lang/Object;+474
J  clojure.lang.RT.seq(Ljava/lang/Object;)Lclojure/lang/ISeq;
J  clojure.core$seq.invoke(Ljava/lang/Object;)Ljava/lang/Object;
j  clojure.core$dorun.invoke(Ljava/lang/Object;)Ljava/lang/Object;+10
j  clojure.core$doall.invoke(Ljava/lang/Object;)Ljava/lang/Object;+10
j  backtype.storm.daemon.worker$mk_refresh_connections$this__5827.invoke(Ljava/lang/Object;)Ljava/lang/Object;+512
j  backtype.storm.daemon.worker$fn__5882$exec_fn__1229__auto____5883.invoke(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+620
j  clojure.lang.AFn.applyToHelper(Lclojure/lang/IFn;Lclojure/lang/ISeq;)Ljava/lang/Object;+416
j  clojure.lang.AFn.applyTo(Lclojure/lang/ISeq;)Ljava/lang/Object;+8
j  clojure.core$apply.invoke(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+26
j  backtype.storm.daemon.worker$fn__5882$mk_worker__5938.doInvoke(Ljava/lang/Object;)Ljava/lang/Object;+16
j  clojure.lang.RestFn.invoke(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+123
j  backtype.storm.daemon.worker$_main.invoke(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+77
j  clojure.lang.AFn.applyToHelper(Lclojure/lang/IFn;Lclojure/lang/ISeq;)Ljava/lang/Object;+261
j  clojure.lang.AFn.applyTo(Lclojure/lang/ISeq;)Ljava/lang/Object;+8
j  backtype.storm.daemon.worker.main([Ljava/lang/String;)V+29
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( => current thread )
  0x00007f1644bf6000 JavaThread ""Thread-4"" daemon [_thread_blocked, id=11049, stack(0x00007f1611513000,0x00007f1611614000)]
  0x00007f1644ddd000 JavaThread ""Thread-3"" daemon [_thread_blocked, id=11048, stack(0x00007f1611614000,0x00007f1611715000)]
  0x00007f1644ddc000 JavaThread ""Thread-2"" daemon [_thread_blocked, id=11047, stack(0x00007f1611715000,0x00007f1611816000)]
  0x00007f164509b800 JavaThread ""Thread-1"" daemon [_thread_blocked, id=11046, stack(0x00007f1611816000,0x00007f1611917000)]
  0x00007f1644cc4800 JavaThread ""Thread-0"" daemon [_thread_blocked, id=11045, stack(0x00007f1630055000,0x00007f1630156000)]
  0x00007f1644aca800 JavaThread ""CuratorFramework-1"" [_thread_blocked, id=11044, stack(0x00007f1611d6b000,0x00007f1611e6c000)]
  0x00007f16450b5800 JavaThread ""ConnectionStateManager-0"" [_thread_blocked, id=11043, stack(0x00007f1611f6d000,0x00007f161206e000)]
  0x00007f1644f71000 JavaThread ""main-EventThread"" daemon [_thread_blocked, id=11042, stack(0x00007f161206e000,0x00007f161216f000)]
  0x00007f164509f800 JavaThread ""main-SendThread(s254:2181)"" daemon [_thread_in_native, id=11041, stack(0x00007f1611e6c000,0x00007f1611f6d000)]
  0x00007f164412d800 JavaThread ""Low Memory Detector"" daemon [_thread_blocked, id=11035, stack(0x00007f164072c000,0x00007f164082d000)]
  0x00007f164412b000 JavaThread ""C2 CompilerThread1"" daemon [_thread_blocked, id=11034, stack(0x00007f164082d000,0x00007f164092e000)]
  0x00007f1644128800 JavaThread ""C2 CompilerThread0"" daemon [_thread_blocked, id=11033, stack(0x00007f164092e000,0x00007f1640a2f000)]
  0x00007f1644126800 JavaThread ""Signal Dispatcher"" daemon [_thread_blocked, id=11032, stack(0x00007f1640a2f000,0x00007f1640b30000)]
  0x00007f1644124800 JavaThread ""Surrogate Locker Thread (Concurrent GC)"" daemon [_thread_blocked, id=11031, stack(0x00007f1640b30000,0x00007f1640c31000)]
  0x00007f16440fc000 JavaThread ""Finalizer"" daemon [_thread_blocked, id=11030, stack(0x00007f1640c31000,0x00007f1640d32000)]
  0x00007f16440fa000 JavaThread ""Reference Handler"" daemon [_thread_blocked, id=11029, stack(0x00007f1648052000,0x00007f1648153000)]
=>0x00007f164400a800 JavaThread ""main"" [_thread_in_native, id=11022, stack(0x00007f164a4fb000,0x00007f164a5fc000)]

Other Threads:
  0x00007f16440f3000 VMThread [stack: 0x00007f1640d32000,0x00007f1640e33000] [id=11028]
  0x00007f1644138800 WatcherThread [stack: 0x00007f164062b000,0x00007f164072c000] [id=11036]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap
 par new generation   total 290304K, used 143655K [0x000000009ae00000, 0x00000000b2800000, 0x00000000b2800000)
  eden space 193536K,  74% used [0x000000009ae00000, 0x00000000a3a49ca0, 0x00000000a6b00000)
  from space 96768K,   0% used [0x00000000a6b00000, 0x00000000a6b00000, 0x00000000ac980000)
  to   space 96768K,   0% used [0x00000000ac980000, 0x00000000ac980000, 0x00000000b2800000)
 concurrent mark-sweep generation total 1185792K, used 0K [0x00000000b2800000, 0x00000000fae00000, 0x00000000fae00000)
 concurrent-mark-sweep perm gen total 27392K, used 27246K [0x00000000fae00000, 0x00000000fc8c0000, 0x0000000100000000)

Code Cache  [0x00007f1641000000, 0x00007f1641270000, 0x00007f1644000000)
 total_blobs=586 nmethods=291 adapters=249 free_code_cache=48638080 largest_free_block=12800

Dynamic libraries:
40000000-40009000 r-xp 00000000 08:03 15860968                           /usr/java/jdk1.6.0_45/bin/java
40108000-4010a000 rwxp 00008000 08:03 15860968                           /usr/java/jdk1.6.0_45/bin/java
40bf1000-40c12000 rwxp 00000000 00:00 0                                  [heap]
9ae00000-fc8c0000 rwxp 00000000 00:00 0 
fc8c0000-100000000 rwxp 00000000 00:00 0 
31f4000000-31f4020000 r-xp 00000000 08:03 13631490                       /lib64/ld-2.12.so
31f421f000-31f4220000 r-xp 0001f000 08:03 13631490                       /lib64/ld-2.12.so
31f4220000-31f4221000 rwxp 00020000 08:03 13631490                       /lib64/ld-2.12.so
31f4221000-31f4222000 rwxp 00000000 00:00 0 
31f4400000-31f4402000 r-xp 00000000 08:03 13631510                       /lib64/libdl-2.12.so
31f4402000-31f4602000 ---p 00002000 08:03 13631510                       /lib64/libdl-2.12.so
31f4602000-31f4603000 r-xp 00002000 08:03 13631510                       /lib64/libdl-2.12.so
31f4603000-31f4604000 rwxp 00003000 08:03 13631510                       /lib64/libdl-2.12.so
31f4800000-31f498b000 r-xp 00000000 08:03 13631495                       /lib64/libc-2.12.so
31f498b000-31f4b8a000 ---p 0018b000 08:03 13631495                       /lib64/libc-2.12.so
31f4b8a000-31f4b8e000 r-xp 0018a000 08:03 13631495                       /lib64/libc-2.12.so
31f4b8e000-31f4b8f000 rwxp 0018e000 08:03 13631495                       /lib64/libc-2.12.so
31f4b8f000-31f4b94000 rwxp 00000000 00:00 0 
31f4c00000-31f4c17000 r-xp 00000000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4c17000-31f4e17000 ---p 00017000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4e17000-31f4e18000 r-xp 00017000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4e18000-31f4e19000 rwxp 00018000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4e19000-31f4e1d000 rwxp 00000000 00:00 0 
31f5000000-31f5083000 r-xp 00000000 08:03 13631502                       /lib64/libm-2.12.so
31f5083000-31f5282000 ---p 00083000 08:03 13631502                       /lib64/libm-2.12.so
31f5282000-31f5283000 r-xp 00082000 08:03 13631502                       /lib64/libm-2.12.so
31f5283000-31f5284000 rwxp 00083000 08:03 13631502                       /lib64/libm-2.12.so
31f5400000-31f5407000 r-xp 00000000 08:03 13631919                       /lib64/librt-2.12.so
31f5407000-31f5606000 ---p 00007000 08:03 13631919                       /lib64/librt-2.12.so
31f5606000-31f5607000 r-xp 00006000 08:03 13631919                       /lib64/librt-2.12.so
31f5607000-31f5608000 rwxp 00007000 08:03 13631919                       /lib64/librt-2.12.so
3201800000-3201816000 r-xp 00000000 08:03 13631922                       /lib64/libgcc_s-4.4.7-20120601.so.1
3201816000-3201a15000 ---p 00016000 08:03 13631922                       /lib64/libgcc_s-4.4.7-20120601.so.1
3201a15000-3201a16000 rwxp 00015000 08:03 13631922                       /lib64/libgcc_s-4.4.7-20120601.so.1
3202400000-32024e8000 r-xp 00000000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32024e8000-32026e8000 ---p 000e8000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32026e8000-32026ef000 r-xp 000e8000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32026ef000-32026f1000 rwxp 000ef000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32026f1000-3202706000 rwxp 00000000 00:00 0 
3204c00000-3204c16000 r-xp 00000000 08:03 13631635                       /lib64/libnsl-2.12.so
3204c16000-3204e15000 ---p 00016000 08:03 13631635                       /lib64/libnsl-2.12.so
3204e15000-3204e16000 r-xp 00015000 08:03 13631635                       /lib64/libnsl-2.12.so
3204e16000-3204e17000 rwxp 00016000 08:03 13631635                       /lib64/libnsl-2.12.so
3204e17000-3204e19000 rwxp 00000000 00:00 0 
7f15d4000000-7f15d4021000 rwxp 00000000 00:00 0 
7f15d4021000-7f15d8000000 ---p 00000000 00:00 0 
7f15dc000000-7f15dc021000 rwxp 00000000 00:00 0 
7f15dc021000-7f15e0000000 ---p 00000000 00:00 0 
7f15e0000000-7f15e0021000 rwxp 00000000 00:00 0 
7f15e0021000-7f15e4000000 ---p 00000000 00:00 0 
7f15e4000000-7f15e4021000 rwxp 00000000 00:00 0 
7f15e4021000-7f15e8000000 ---p 00000000 00:00 0 
7f15e8000000-7f15e8021000 rwxp 00000000 00:00 0 
7f15e8021000-7f15ec000000 ---p 00000000 00:00 0 
7f15ec000000-7f15ec021000 rwxp 00000000 00:00 0 
7f15ec021000-7f15f0000000 ---p 00000000 00:00 0 
7f15f0000000-7f15f0021000 rwxp 00000000 00:00 0 
7f15f0021000-7f15f4000000 ---p 00000000 00:00 0 
7f15f4000000-7f15f4021000 rwxp 00000000 00:00 0 
7f15f4021000-7f15f8000000 ---p 00000000 00:00 0 
7f15f8000000-7f15f8021000 rwxp 00000000 00:00 0 
7f15f8021000-7f15fc000000 ---p 00000000 00:00 0 
7f15fc000000-7f15fc021000 rwxp 00000000 00:00 0 
7f15fc021000-7f1600000000 ---p 00000000 00:00 0 
7f1600000000-7f1600021000 rwxp 00000000 00:00 0 
7f1600021000-7f1604000000 ---p 00000000 00:00 0 
7f1604000000-7f1605508000 rwxp 00000000 00:00 0 
7f1605508000-7f1608000000 ---p 00000000 00:00 0 
7f1608000000-7f16094a0000 rwxp 00000000 00:00 0 
7f16094a0000-7f160c000000 ---p 00000000 00:00 0 
7f160c000000-7f160c021000 rwxp 00000000 00:00 0 
7f160c021000-7f1610000000 ---p 00000000 00:00 0 
7f1611513000-7f1611516000 ---p 00000000 00:00 0 
7f1611516000-7f1611614000 rwxp 00000000 00:00 0 
7f1611614000-7f1611617000 ---p 00000000 00:00 0 
7f1611617000-7f1611715000 rwxp 00000000 00:00 0 
7f1611715000-7f1611718000 ---p 00000000 00:00 0 
7f1611718000-7f1611816000 rwxp 00000000 00:00 0 
7f1611816000-7f1611819000 ---p 00000000 00:00 0 
7f1611819000-7f1611917000 rwxp 00000000 00:00 0 
7f1611917000-7f1611962000 r-xp 00000000 08:03 15744848                   /usr/local/lib/libzmq.so.3.1.0
7f1611962000-7f1611b61000 ---p 0004b000 08:03 15744848                   /usr/local/lib/libzmq.so.3.1.0
7f1611b61000-7f1611b65000 rwxp 0004a000 08:03 15744848                   /usr/local/lib/libzmq.so.3.1.0
7f1611b65000-7f1611b6b000 r-xp 00000000 08:03 15744808                   /usr/local/lib/libjzmq.so.0.0.0
7f1611b6b000-7f1611d6a000 ---p 00006000 08:03 15744808                   /usr/local/lib/libjzmq.so.0.0.0
7f1611d6a000-7f1611d6b000 rwxp 00005000 08:03 15744808                   /usr/local/lib/libjzmq.so.0.0.0
7f1611d6b000-7f1611d6e000 ---p 00000000 00:00 0 
7f1611d6e000-7f1611e6c000 rwxp 00000000 00:00 0 
7f1611e6c000-7f1611e6f000 ---p 00000000 00:00 0 
7f1611e6f000-7f1611f6d000 rwxp 00000000 00:00 0 
7f1611f6d000-7f1611f70000 ---p 00000000 00:00 0 
7f1611f70000-7f161206e000 rwxp 00000000 00:00 0 
7f161206e000-7f1612071000 ---p 00000000 00:00 0 
7f1612071000-7f161216f000 rwxp 00000000 00:00 0 
7f161216f000-7f1618000000 r-xp 00000000 08:03 15738011                   /usr/lib/locale/locale-archive
7f1618000000-7f1618021000 rwxp 00000000 00:00 0 
7f1618021000-7f161c000000 ---p 00000000 00:00 0 
7f161c000000-7f161c021000 rwxp 00000000 00:00 0 
7f161c021000-7f1620000000 ---p 00000000 00:00 0 
7f1620000000-7f1620021000 rwxp 00000000 00:00 0 
7f1620021000-7f1624000000 ---p 00000000 00:00 0 
7f1624000000-7f1624021000 rwxp 00000000 00:00 0 
7f1624021000-7f1628000000 ---p 00000000 00:00 0 
7f1628000000-7f1628021000 rwxp 00000000 00:00 0 
7f1628021000-7f162c000000 ---p 00000000 00:00 0 
7f162c000000-7f162c021000 rwxp 00000000 00:00 0 
7f162c021000-7f1630000000 ---p 00000000 00:00 0 
7f1630055000-7f1630058000 ---p 00000000 00:00 0 
7f1630058000-7f1630156000 rwxp 00000000 00:00 0 
7f1630156000-7f163015c000 r-xp 00000000 08:03 16256555                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libmanagement.so
7f163015c000-7f163025b000 ---p 00006000 08:03 16256555                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libmanagement.so
7f163025b000-7f163025d000 rwxp 00005000 08:03 16256555                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libmanagement.so
7f163025d000-7f1630b85000 rwxp 00000000 00:00 0 
7f1630b85000-7f1630b98000 r-xp 00000000 08:03 16256559                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnet.so
7f1630b98000-7f1630c99000 ---p 00013000 08:03 16256559                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnet.so
7f1630c99000-7f1630c9c000 rwxp 00014000 08:03 16256559                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnet.so
7f1630c9c000-7f1630c9f000 r-xs 000cc000 08:03 16525449                   /usr/java/jdk1.6.0_45/jre/lib/ext/localedata.jar
7f1630c9f000-7f1634000000 rwxp 00000000 00:00 0 
7f1634000000-7f1634021000 rwxp 00000000 00:00 0 
7f1634021000-7f1638000000 ---p 00000000 00:00 0 
7f1638000000-7f1638021000 rwxp 00000000 00:00 0 
7f1638021000-7f163c000000 ---p 00000000 00:00 0 
7f163c000000-7f163c021000 rwxp 00000000 00:00 0 
7f163c021000-7f1640000000 ---p 00000000 00:00 0 
7f164003e000-7f1640045000 r-xp 00000000 08:03 16256560                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnio.so
7f1640045000-7f1640144000 ---p 00007000 08:03 16256560                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnio.so
7f1640144000-7f1640146000 rwxp 00006000 08:03 16256560                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnio.so
7f1640146000-7f164014b000 r-xs 00029000 08:03 14032062                   /app/storm-supervisor/data/supervisor/stormdist/oracle-13-1403837712/stormjar.jar
7f164014b000-7f1640152000 r-xs 0004e000 08:03 15204423                   /app/storm-supervisor/lib-ext/mongo-java-driver-2.8.0.jar
7f1640152000-7f1640158000 r-xs 00031000 08:03 15204404                   /app/storm-supervisor/lib-ext/jaxen-1.1.3.jar
7f1640158000-7f164015c000 r-xs 00021000 08:03 15204408                   /app/storm-supervisor/lib-ext/jersey-json-1.9.jar
7f164015c000-7f164015e000 r-xs 00006000 08:03 15204393                   /app/storm-supervisor/lib-ext/htrace-core-2.01.jar
7f164015e000-7f1640162000 r-xs 00014000 08:03 15204430                   /app/storm-supervisor/lib-ext/xz-1.0.jar
7f1640162000-7f1640164000 r-xs 00000000 08:03 15204355                   /app/storm-supervisor/lib-ext/aopalliance-1.0.jar
7f1640164000-7f1640165000 r-xs 00007000 08:03 15204397                   /app/storm-supervisor/lib-ext/jackson-xc-1.8.3.jar
7f1640165000-7f1640177000 r-xs 0012c000 08:03 15204389                   /app/storm-supervisor/lib-ext/hadoop-yarn-common-2.2.0.jar
7f1640177000-7f16401f5000 r-xs 007f5000 08:03 15204426                   /app/storm-supervisor/lib-ext/scala-library-2.9.2.jar
7f16401f5000-7f1640228000 r-xs 0024a000 08:03 15204417                   /app/storm-supervisor/lib-ext/kafka_2.9.2-0.8.0.jar
7f1640228000-7f164022d000 r-xs 0002a000 08:03 15204361                   /app/storm-supervisor/lib-ext/commons-beanutils-1.7.0.jar
7f164022d000-7f164022e000 r-xs 00003000 08:03 15204431                   /app/storm-supervisor/lib-ext/xmlenc-0.52.jar
7f164022e000-7f1640231000 r-xs 00018000 08:03 15204357                   /app/storm-supervisor/lib-ext/bson-2.9.3.jar
7f1640231000-7f1640243000 r-xs 0009d000 08:03 15204409                   /app/storm-supervisor/lib-ext/jersey-server-1.9.jar
7f1640243000-7f164024f000 r-xs 000be000 08:03 15204390                   /app/storm-supervisor/lib-ext/hbase-client-0.96.1-hadoop2.jar
7f164024f000-7f1640253000 r-xs 00020000 08:03 15204368                   /app/storm-supervisor/lib-ext/commons-digester-1.8.jar
7f1640253000-7f164025a000 r-xs 00039000 08:03 15204364                   /app/storm-supervisor/lib-ext/commons-codec-1.7.jar
7f164025a000-7f164025f000 r-xs 0004a000 08:03 15204410                   /app/storm-supervisor/lib-ext/jets3t-0.6.1.jar
7f164025f000-7f164026f000 r-xs 00081000 08:03 15204419                   /app/storm-supervisor/lib-ext/mchange-commons-java-0.2.5.jar
7f164026f000-7f1640276000 r-xs 00068000 08:03 15204358                   /app/storm-supervisor/lib-ext/c3p0-0.9.5-pre2.jar
7f1640276000-7f164027a000 r-xs 0002f000 08:03 15204362                   /app/storm-supervisor/lib-ext/commons-beanutils-core-1.8.0.jar
7f164027a000-7f1640280000 r-xs 00044000 08:03 15204405                   /app/storm-supervisor/lib-ext/jedis-2.4.2.jar
7f1640280000-7f1640281000 r-xs 00004000 08:03 15204395                   /app/storm-supervisor/lib-ext/jackson-jaxrs-1.8.3.jar
7f1640281000-7f1640286000 r-xs 0005f000 08:03 15204398                   /app/storm-supervisor/lib-ext/jasper-compiler-5.5.23.jar
7f1640286000-7f1640298000 r-xs 001d6000 08:03 15204403                   /app/storm-supervisor/lib-ext/jdbc-11.2.0.1.0.jar
7f1640298000-7f164029a000 r-xs 00006000 08:03 15204422                   /app/storm-supervisor/lib-ext/paranamer-2.3.jar
7f164029a000-7f164029c000 r-xs 0000f000 08:03 15204411                   /app/storm-supervisor/lib-ext/jettison-1.1.jar
7f164029c000-7f16402a3000 r-xs 00044000 08:03 15204356                   /app/storm-supervisor/lib-ext/avro-1.7.4.jar
7f16402a3000-7f16402a8000 r-xs 00036000 08:03 15204366                   /app/storm-supervisor/lib-ext/commons-compress-1.4.1.jar
7f16402a8000-7f16402ab000 r-xs 0002b000 08:03 15204413                   /app/storm-supervisor/lib-ext/jsch-0.1.42.jar
7f16402ab000-7f1640343000 r-xs 007ae000 08:03 15204425                   /app/storm-supervisor/lib-ext/scala-compiler-2.8.0.jar
7f1640343000-7f1640346000 r-xs 00016000 08:03 15204414                   /app/storm-supervisor/lib-ext/jsp-api-2.1.jar
7f1640346000-7f1640348000 r-xs 000f2000 08:03 15204427                   /app/storm-supervisor/lib-ext/snappy-java-1.0.4.1.jar
7f1640348000-7f164034e000 r-xs 00054000 08:03 15204391                   /app/storm-supervisor/lib-ext/hbase-common-0.96.1-hadoop2.jar
7f164034e000-7f1640351000 r-xs 00019000 08:03 15204369                   /app/storm-supervisor/lib-ext/commons-el-1.0.jar
7f1640351000-7f1640353000 r-xs 00009000 08:03 15204363                   /app/storm-supervisor/lib-ext/commons-cli-1.2.jar
7f1640353000-7f1640356000 r-xs 00012000 08:03 15204421                   /app/storm-supervisor/lib-ext/metrics-core-2.2.0.jar
7f1640356000-7f1640358000 r-xs 0000e000 08:03 15204432                   /app/storm-supervisor/lib-ext/zkclient-0.3.jar
7f1640358000-7f164035e000 r-xs 00045000 08:03 15204370                   /app/storm-supervisor/lib-ext/commons-httpclient-3.1.jar
7f164035e000-7f1640360000 r-xs 0000b000 08:03 15204385                   /app/storm-supervisor/lib-ext/hadoop-auth-2.2.0.jar
7f1640360000-7f164036a000 r-xs 0008a000 08:03 15204433                   /app/storm-supervisor/lib-ext/zookeeper-3.3.4.jar
7f164036a000-7f1640382000 r-xs 0014c000 08:03 15204387                   /app/storm-supervisor/lib-ext/hadoop-mapreduce-client-core-2.2.0.jar
7f1640382000-7f1640391000 r-xs 0009f000 08:03 15204382                   /app/storm-supervisor/lib-ext/guice-3.0.jar
7f1640391000-7f1640393000 r-xs 00005000 08:03 15204428                   /app/storm-supervisor/lib-ext/stax-api-1.0.1.jar
7f1640393000-7f1640395000 r-xs 00002000 08:03 15204379                   /app/storm-supervisor/lib-ext/findbugs-annotations-1.3.9-1.jar
7f1640395000-7f164039e000 r-xs 0006f000 08:03 15204418                   /app/storm-supervisor/lib-ext/log4j-1.2.17.jar
7f164039e000-7f16403a1000 r-xs 00018000 08:03 15204371                   /app/storm-supervisor/lib-ext/commons-io-1.4.jar
7f16403a1000-7f16403a9000 r-xs 0007b000 08:03 15204424                   /app/storm-supervisor/lib-ext/protobuf-java-2.5.0.jar
7f16403a9000-7f16403ab000 r-xs 00000000 08:03 15204420                   /app/storm-supervisor/lib-ext/metrics-annotation-2.2.0.jar
7f16403ab000-7f16403ad000 r-xs 00009000 08:03 15204415                   /app/storm-supervisor/lib-ext/energy-commons-mini-1.0.16.jar
7f16403ad000-7f16403bc000 r-xs 000b0000 08:03 15204396                   /app/storm-supervisor/lib-ext/jackson-mapper-asl-1.9.9.jar
7f16403bc000-7f16403bf000 r-xs 00017000 08:03 15204401                   /app/storm-supervisor/lib-ext/jaxb-api-2.2.2.jar
7f16403bf000-7f16403d2000 r-xs 000c7000 08:03 15204402                   /app/storm-supervisor/lib-ext/jaxb-impl-2.2.3-1.jar
7f16403d2000-7f16403dc000 r-xs 00066000 08:03 15204406                   /app/storm-supervisor/lib-ext/jersey-core-1.9.jar
7f16403dc000-7f16403dd000 r-xs 00003000 08:03 15204407                   /app/storm-supervisor/lib-ext/jersey-guice-1.9.jar
7f16403dd000-7f16403e1000 r-xs 00035000 08:03 15204394                   /app/storm-supervisor/lib-ext/jackson-core-asl-1.9.9.jar
7f16403e1000-7f16403ef000 r-xs 0007f000 08:03 15204365                   /app/storm-supervisor/lib-ext/commons-collections-3.2.1.jar
7f16403ef000-7f16403f0000 r-xs 00000000 08:03 15204400                   /app/storm-supervisor/lib-ext/javax.inject-1.jar
7f16403f0000-7f16403f3000 r-xs 0000e000 08:03 15204429                   /app/storm-supervisor/lib-ext/storm-kafka-plus-1.0.0-SNAPSHOT.jar
7f16403f3000-7f1640404000 r-xs 0010a000 08:03 15204388                   /app/storm-supervisor/lib-ext/hadoop-yarn-api-2.2.0.jar
7f1640404000-7f1640406000 r-xs 00011000 08:03 15204399                   /app/storm-supervisor/lib-ext/jasper-runtime-5.5.23.jar
7f1640406000-7f1640430000 r-xs 002d4000 08:03 15204392                   /app/storm-supervisor/lib-ext/hbase-protocol-0.96.1-hadoop2.jar
7f1640430000-7f1640436000 r-xs 00043000 08:03 15204367                   /app/storm-supervisor/lib-ext/commons-configuration-1.6.jar
7f1640436000-7f164043b000 r-xs 00048000 08:03 15204377                   /app/storm-supervisor/lib-ext/dom4j-1.6.1.jar
7f164043b000-7f164043d000 r-xs 00004000 08:03 15204378                   /app/storm-supervisor/lib-ext/kafka-client-1.0.0-SNAPSHOT.jar
7f164043d000-7f164044e000 r-xs 000bb000 08:03 15204374                   /app/storm-supervisor/lib-ext/commons-math-2.1.jar
7f164044e000-7f1640453000 r-xs 0003e000 08:03 15204375                   /app/storm-supervisor/lib-ext/commons-net-3.1.jar
7f1640453000-7f164047e000 r-xs 00271000 08:03 15204386                   /app/storm-supervisor/lib-ext/hadoop-common-2.2.0.jar
7f164047e000-7f1640480000 r-xs 0000e000 08:03 15204354                   /app/storm-supervisor/lib-ext/activation-1.1.jar
7f1640480000-7f1640485000 r-xs 00041000 08:03 15204372                   /app/storm-supervisor/lib-ext/commons-lang-2.6.jar
7f1640485000-7f1640487000 r-xs 0000b000 08:03 15204412                   /app/storm-supervisor/lib-ext/jopt-simple-3.2.jar
7f1640487000-7f1640489000 r-xs 0000e000 08:03 15204383                   /app/storm-supervisor/lib-ext/guice-servlet-3.0.jar
7f1640489000-7f164048b000 r-xs 00003000 08:03 15204384                   /app/storm-supervisor/lib-ext/hadoop-annotations-2.2.0.jar
7f164048b000-7f164048e000 r-xs 00018000 08:03 15204376                   /app/storm-supervisor/lib-ext/commons-pool2-2.0.jar
7f164048e000-7f1640496000 r-xs 0004e000 08:03 14024880                   /app/storm-supervisor/lib/httpclient-4.1.1.jar
7f1640496000-7f1640498000 r-xs 00014000 08:03 14024885                   /app/storm-supervisor/lib/jline-0.9.94.jar
7f1640498000-7f1640499000 r-xs 00003000 08:03 14024887                   /app/storm-supervisor/lib/json-simple-1.1.jar
7f1640499000-7f164049b000 r-xs 00018000 08:03 14024906                   /app/storm-supervisor/lib/servlet-api-2.5.jar
7f164049b000-7f164049c000 r-xs 00002000 08:03 14024863                   /app/storm-supervisor/lib/clj-time-0.4.1.jar
7f164049c000-7f164049e000 r-xs 0000b000 08:03 14024877                   /app/storm-supervisor/lib/disruptor-2.10.1.jar
7f164049e000-7f16404a0000 r-xs 0000d000 08:03 14024867                   /app/storm-supervisor/lib/commons-codec-1.4.jar
7f16404a0000-7f16404a7000 r-xs 00035000 08:03 14024884                   /app/storm-supervisor/lib/jgrapht-0.8.3.jar
7f16404a7000-7f16404b0000 r-xs 0004d000 08:03 14024894                   /app/storm-supervisor/lib/logback-core-1.0.6.jar
7f16404b0000-7f16404b4000 r-xs 00024000 08:03 14024890                   /app/storm-supervisor/lib/kryo-2.17.jar
7f16404b4000-7f16404b7000 r-xs 0001b000 08:03 14024888                   /app/storm-supervisor/lib/junit-3.8.1.jar
7f16404b7000-7f16404bd000 r-xs 00038000 08:03 14024893                   /app/storm-supervisor/lib/logback-classic-1.0.6.jar
7f16404bd000-7f16404e1000 r-xs 001aa000 08:03 14024878                   /app/storm-supervisor/lib/guava-13.0.jar
7f16404e1000-7f16404e3000 r-xs 00004000 08:03 14024901                   /app/storm-supervisor/lib/ring-core-1.1.5.jar
7f16404e3000-7f16404e7000 r-xs 00028000 08:03 14024883                   /app/storm-supervisor/lib/jetty-util-6.1.26.jar
7f16404e7000-7f16404e8000 r-xs 00000000 08:03 14024865                   /app/storm-supervisor/lib/clojure-complete-0.2.3.jar
7f16404e8000-7f16404ef000 r-xs 0003c000 08:03 14024908                   /app/storm-supervisor/lib/snakeyaml-1.11.jar
7f16404ef000-7f16404f0000 r-xs 00001000 08:03 14024902                   /app/storm-supervisor/lib/ring-devel-0.3.11.jar
7f16404f0000-7f16404f2000 r-xs 0000a000 08:03 14024847                   /app/storm-supervisor/lib/asm-4.0.jar
7f16404f2000-7f16404f7000 r-xs 00040000 08:03 14024871                   /app/storm-supervisor/lib/commons-lang-2.5.jar
7f16404f7000-7f16404f8000 r-xs 00001000 08:03 14024910                   /app/storm-supervisor/lib/tools.logging-0.2.3.jar
7f16404f8000-7f16404f9000 r-xs 00000000 08:03 14024866                   /app/storm-supervisor/lib/clout-1.0.1.jar
7f16404f9000-7f1640511000 r-xs 0010e000 08:03 14024898                   /app/storm-supervisor/lib/netty-3.6.3.Final.jar
7f1640511000-7f1640512000 r-xs 00000000 08:03 14024874                   /app/storm-supervisor/lib/core.incubator-0.1.0.jar
7f1640512000-7f1640513000 r-xs 00001000 08:03 14024911                   /app/storm-supervisor/lib/tools.macro-0.1.0.jar
7f1640513000-7f1640514000 r-xs 00000000 08:03 14024904                   /app/storm-supervisor/lib/ring-servlet-0.3.11.jar
7f1640514000-7f1640516000 r-xs 0000d000 08:03 14024848                   /app/storm-supervisor/lib/carbonite-1.5.0.jar
7f1640516000-7f1640518000 r-xs 00005000 08:03 14024907                   /app/storm-supervisor/lib/slf4j-api-1.6.5.jar
7f1640518000-7f1640527000 r-xs 0007c000 08:03 14024886                   /app/storm-supervisor/lib/joda-time-2.0.jar
7f1640527000-7f1640529000 r-xs 0000d000 08:03 14024872                   /app/storm-supervisor/lib/commons-logging-1.1.1.jar
7f1640529000-7f164052b000 r-xs 00004000 08:03 14024892                   /app/storm-supervisor/lib/log4j-over-slf4j-1.6.6.jar
7f164052b000-7f164052c000 r-xs 00001000 08:03 14024862                   /app/storm-supervisor/lib/clj-stacktrace-0.2.2.jar
7f164052c000-7f164052d000 r-xs 00000000 08:03 14024903                   /app/storm-supervisor/lib/ring-jetty-adapter-0.3.11.jar
7f164052d000-7f164052f000 r-xs 00005000 08:03 14024875                   /app/storm-supervisor/lib/curator-client-1.0.1.jar
7f164052f000-7f1640532000 r-xs 00018000 08:03 14024870                   /app/storm-supervisor/lib/commons-io-1.4.jar
7f1640532000-7f1640536000 r-xs 00015000 08:03 14024876                   /app/storm-supervisor/lib/curator-framework-1.0.1.jar
7f1640536000-7f164053b000 r-xs 00028000 08:03 14024881                   /app/storm-supervisor/lib/httpcore-4.1.jar
7f164053b000-7f164055c000 r-xs 00162000 08:03 14024897                   /app/storm-supervisor/lib/mockito-all-1.9.5.jar
7f164055c000-7f164055e000 r-xs 0000f000 08:03 14024900                   /app/storm-supervisor/lib/reflectasm-1.07-shaded.jar
7f164055e000-7f1640560000 r-xs 00008000 08:03 14024912                   /app/storm-supervisor/lib/tools.nrepl-0.2.3.jar
7f1640560000-7f1640561000 r-xs 00001000 08:03 14024873                   /app/storm-supervisor/lib/compojure-1.1.3.jar
7f1640561000-7f164059d000 r-xs 00308000 08:03 14024864                   /app/storm-supervisor/lib/clojure-1.4.0.jar
7f164059d000-7f16405a0000 r-xs 0000c000 08:03 14024869                   /app/storm-supervisor/lib/commons-fileupload-1.2.1.jar
7f16405a0000-7f16405a7000 r-xs 0007d000 08:03 14024882                   /app/storm-supervisor/lib/jetty-6.1.26.jar
7f16405a7000-7f16405a9000 r-xs 0001f000 08:03 14024905                   /app/storm-supervisor/lib/servlet-api-2.5-20081211.jar
7f16405a9000-7f16405ab000 r-xs 00000000 08:03 14024895                   /app/storm-supervisor/lib/math.numeric-tower-0.0.1.jar
7f16405ab000-7f16405b1000 r-xs 00044000 08:03 14024891                   /app/storm-supervisor/lib/libthrift7-0.7.0-2.jar
7f16405b1000-7f16405b3000 r-xs 0000b000 08:03 14024868                   /app/storm-supervisor/lib/commons-exec-1.1.jar
7f16405b3000-7f16405b4000 r-xs 00000000 08:03 14024909                   /app/storm-supervisor/lib/tools.cli-0.2.2.jar
7f16405b4000-7f16405b5000 r-xs 00001000 08:03 14024879                   /app/storm-supervisor/lib/hiccup-0.3.6.jar
7f16405b5000-7f16405b7000 r-xs 00000000 08:03 14024896                   /app/storm-supervisor/lib/minlog-1.2.jar
7f16405b7000-7f16405c0000 r-xs 0008a000 08:03 14024913                   /app/storm-supervisor/lib/zookeeper-3.3.3.jar
7f16405c0000-7f16405c2000 r-xs 00007000 08:03 14024899                   /app/storm-supervisor/lib/objenesis-1.2.jar
7f16405c2000-7f16405c3000 r-xs 00003000 08:03 14024889                   /app/storm-supervisor/lib/jzmq-2.1.0.jar
7f16405c3000-7f16405c5000 r-xs 00009000 08:03 14024936                   /app/storm-supervisor/storm-netty-0.9.0.1.jar
7f16405c5000-7f16405c6000 r-xs 00000000 08:03 14024934                   /app/storm-supervisor/storm-console-logging-0.9.0.1.jar
7f16405c6000-7f164062b000 r-xs 004b8000 08:03 14024935                   /app/storm-supervisor/storm-core-0.9.0.1.jar
7f164062b000-7f164062c000 ---p 00000000 00:00 0 
7f164062c000-7f164072c000 rwxp 00000000 00:00 0 
7f164072c000-7f164072f000 ---p 00000000 00:00 0 
7f164072f000-7f164082d000 rwxp 00000000 00:00 0 
7f164082d000-7f1640830000 ---p 00000000 00:00 0 
7f1640830000-7f164092e000 rwxp 00000000 00:00 0 
7f164092e000-7f1640931000 ---p 00000000 00:00 0 
7f1640931000-7f1640a2f000 rwxp 00000000 00:00 0 
7f1640a2f000-7f1640a32000 ---p 00000000 00:00 0 
7f1640a32000-7f1640b30000 rwxp 00000000 00:00 0 
7f1640b30000-7f1640b33000 ---p 00000000 00:00 0 
7f1640b33000-7f1640c31000 rwxp 00000000 00:00 0 
7f1640c31000-7f1640c34000 ---p 00000000 00:00 0 
7f1640c34000-7f1640d32000 rwxp 00000000 00:00 0 
7f1640d32000-7f1640d33000 ---p 00000000 00:00 0 
7f1640d33000-7f1640e67000 rwxp 00000000 00:00 0 
7f1640e67000-7f1641000000 r-xs 03087000 08:03 16126619                   /usr/java/jdk1.6.0_45/jre/lib/rt.jar
7f1641000000-7f1641270000 rwxp 00000000 00:00 0 
7f1641270000-7f164520e000 rwxp 00000000 00:00 0 
7f164520e000-7f1648000000 ---p 00000000 00:00 0 
7f1648052000-7f1648055000 ---p 00000000 00:00 0 
7f1648055000-7f16483d9000 rwxp 00000000 00:00 0 
7f16483d9000-7f16483da000 ---p 00000000 00:00 0 
7f16483da000-7f164913a000 rwxp 00000000 00:00 0 
7f164913a000-7f1649156000 rwxp 00000000 00:00 0 
7f1649156000-7f1649aac000 rwxp 00000000 00:00 0 
7f1649aac000-7f1649ac7000 rwxp 00000000 00:00 0 
7f1649ac7000-7f1649ac8000 rwxp 00000000 00:00 0 
7f1649ac8000-7f1649ac9000 ---p 00000000 00:00 0 
7f1649ac9000-7f1649bc9000 rwxp 00000000 00:00 0 
7f1649bc9000-7f1649bca000 ---p 00000000 00:00 0 
7f1649bca000-7f1649cca000 rwxp 00000000 00:00 0 
7f1649cca000-7f1649ccb000 ---p 00000000 00:00 0 
7f1649ccb000-7f1649dcb000 rwxp 00000000 00:00 0 
7f1649dcb000-7f1649dcc000 ---p 00000000 00:00 0 
7f1649dcc000-7f1649ed6000 rwxp 00000000 00:00 0 
7f1649ed6000-7f1649f8c000 rwxp 00000000 00:00 0 
7f1649f8c000-7f1649f9a000 r-xp 00000000 08:03 16256568                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libzip.so
7f1649f9a000-7f164a09c000 ---p 0000e000 08:03 16256568                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libzip.so
7f164a09c000-7f164a09f000 rwxp 00010000 08:03 16256568                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libzip.so
7f164a09f000-7f164a0a0000 rwxp 00000000 00:00 0 
7f164a0a0000-7f164a0ac000 r-xp 00000000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a0ac000-7f164a2ac000 ---p 0000c000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a2ac000-7f164a2ad000 r-xp 0000c000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a2ad000-7f164a2ae000 rwxp 0000d000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a2ae000-7f164a2bd000 r-xs 00667000 08:03 16126602                   /usr/java/jdk1.6.0_45/jre/lib/charsets.jar
7f164a2bd000-7f164a2e6000 r-xp 00000000 08:03 16256546                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libjava.so
7f164a2e6000-7f164a3e5000 ---p 00029000 08:03 16256546                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libjava.so
7f164a3e5000-7f164a3ec000 rwxp 00028000 08:03 16256546                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libjava.so
7f164a3ec000-7f164a3f9000 r-xp 00000000 08:03 16256567                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libverify.so
7f164a3f9000-7f164a4f8000 ---p 0000d000 08:03 16256567                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libverify.so
7f164a4f8000-7f164a4fb000 rwxp 0000c000 08:03 16256567                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libverify.so
7f164a4fb000-7f164a4fe000 ---p 00000000 00:00 0 
7f164a4fe000-7f164a5fc000 rwxp 00000000 00:00 0 
7f164a5fc000-7f164af1a000 r-xp 00000000 08:03 16388640                   /usr/java/jdk1.6.0_45/jre/lib/amd64/server/libjvm.so
7f164af1a000-7f164b01c000 ---p 0091e000 08:03 16388640                   /usr/java/jdk1.6.0_45/jre/lib/amd64/server/libjvm.so
7f164b01c000-7f164b1d2000 rwxp 00920000 08:03 16388640                   /usr/java/jdk1.6.0_45/jre/lib/amd64/server/libjvm.so
7f164b1d2000-7f164b20f000 rwxp 00000000 00:00 0 
7f164b20f000-7f164b216000 r-xp 00000000 08:03 16256297                   /usr/java/jdk1.6.0_45/jre/lib/amd64/jli/libjli.so
7f164b216000-7f164b317000 ---p 00007000 08:03 16256297                   /usr/java/jdk1.6.0_45/jre/lib/amd64/jli/libjli.so
7f164b317000-7f164b319000 rwxp 00008000 08:03 16256297                   /usr/java/jdk1.6.0_45/jre/lib/amd64/jli/libjli.so
7f164b319000-7f164b31a000 rwxp 00000000 00:00 0 
7f164b31f000-7f164b327000 rwxs 00000000 08:03 2883591                    /tmp/hsperfdata_root/11021
7f164b327000-7f164b328000 rwxp 00000000 00:00 0 
7f164b328000-7f164b329000 r-xp 00000000 00:00 0 
7f164b329000-7f164b32a000 rwxp 00000000 00:00 0 
7fff411d7000-7fff411ee000 rwxp 00000000 00:00 0                          [stack]
7fff411ff000-7fff41200000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Xms1536m -Xmx1536m -Xmn378m -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=65 -Xloggc:/app/storm-supervisor/logs/worker-6701-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+HeapDumpOnOutOfMemoryError -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dlogfile.name=worker-6701.log -Dstorm.home=/app/storm-supervisor -Dlogback.configurationFile=/app/storm-supervisor/logback/cluster.xml -Dstorm.id=oracle-13-1403837712 -Dworker.id=ae1a2f48-91eb-4b8c-96ac-f0340a78d441 -Dworker.port=6701 
java_command: backtype.storm.daemon.worker oracle-13-1403837712 57bf1040-b334-4bb0-a21c-90d8176c98c7 6701 ae1a2f48-91eb-4b8c-96ac-f0340a78d441
Launcher Type: SUN_STANDARD

Environment Variables:
JAVA_HOME=/usr/java/jdk1.6.0_45
CLASSPATH=/usr/java/jdk1.6.0_45/lib/dt.jar:/usr/java/jdk1.6.0_45/lib/tools.jar:/app/zookeeper/lib
PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/usr/java/jdk1.6.0_45/bin:/app/zookeeper/bin:/root/bin
LD_LIBRARY_PATH=/usr/java/jdk1.6.0_45/jre/lib/amd64/server:/usr/java/jdk1.6.0_45/jre/lib/amd64:/usr/java/jdk1.6.0_45/jre/../lib/amd64:/usr/local/lib:/opt/local/lib:/usr/lib
SHELL=/bin/bash

Signal Handlers:
SIGSEGV: [libjvm.so+0x862a30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGBUS: [libjvm.so+0x862a30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGFPE: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGPIPE: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGXFSZ: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGILL: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGUSR1: SIG_DFL, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGUSR2: [libjvm.so+0x713520], sa_mask[0]=0x00000004, sa_flags=0x10000004
SIGHUP: SIG_IGN, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGINT: [libjvm.so+0x713120], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGTERM: [libjvm.so+0x713120], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGQUIT: [libjvm.so+0x713120], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004


---------------  S Y S T E M  ---------------

OS:Red Hat Enterprise Linux Server release 6.4 (Santiago)

uname:Linux 2.6.32-358.el6.x86_64 #1 SMP Tue Jan 29 11:47:41 EST 2013 x86_64
libc:glibc 2.12 NPTL 2.12 
rlimit: STACK 10240k, CORE 0k, NPROC 61813, NOFILE 200000, AS infinity
load average:20.05 19.75 21.56

/proc/meminfo:
MemTotal:        7932088 kB
MemFree:         2699168 kB
Buffers:          242928 kB
Cached:          1789068 kB
SwapCached:         4288 kB
Active:          2858560 kB
Inactive:        2014480 kB
Active(anon):    2076152 kB
Inactive(anon):   768512 kB
Active(file):     782408 kB
Inactive(file):  1245968 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:       8388600 kB
SwapFree:        8369768 kB
Dirty:               784 kB
Writeback:             0 kB
AnonPages:       2778976 kB
Mapped:            25216 kB
Shmem:              3608 kB
Slab:             253560 kB
SReclaimable:     221768 kB
SUnreclaim:        31792 kB
KernelStack:        3040 kB
PageTables:        18696 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    12354644 kB
Committed_AS:    5425796 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      368384 kB
VmallocChunk:   34359365100 kB
HardwareCorrupted:     0 kB
AnonHugePages:   2635776 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:        8192 kB
DirectMap2M:     8263680 kB


CPU:total 4 (2 cores per cpu, 2 threads per core) family 6 model 58 stepping 9, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, ht

/proc/cpuinfo:
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 1600.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 0
cpu cores	: 2
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 3300.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 1
cpu cores	: 2
apicid		: 2
initial apicid	: 2
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 2
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 1600.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 0
cpu cores	: 2
apicid		: 1
initial apicid	: 1
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 3
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 1600.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 1
cpu cores	: 2
apicid		: 3
initial apicid	: 3
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:



Memory: 4k page, physical 7932088k(2699168k free), swap 8388600k(8369768k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (20.45-b01) for linux-amd64 JRE (1.6.0_45-b06), built on Mar 26 2013 14:07:02 by ""java_re"" with gcc 3.2.2 (SuSE Linux)

time: Fri Jun 27 11:10:30 2014
elapsed time: 2 seconds

"
STORM-374,storm0.9.1&kafka0.8.1 with storm-kafka-0.8-plus-0.5.0 can not run,"ERROR backtype.storm.util - Async loop died!
java.lang.NoSuchMethodError: com.netflix.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lcom/netflix/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at storm.kafka.ZkState.writeBytes(ZkState.java:59) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.ZkState.writeJSON(ZkState.java:53) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.PartitionManager.commit(PartitionManager.java:188) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.commit(KafkaSpout.java:169) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:134) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at backtype.storm.daemon.executor$eval5100$fn__5101$fn__5116$fn__5145.invoke(executor.clj:562) ~[na:na]
        at backtype.storm.util$async_loop$fn__390.invoke(util.clj:433) ~[na:na]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_45]
12949 [Thread-27-words] ERROR backtype.storm.daemon.executor - 
java.lang.NoSuchMethodError: com.netflix.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lcom/netflix/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at storm.kafka.ZkState.writeBytes(ZkState.java:59) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.ZkState.writeJSON(ZkState.java:53) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.PartitionManager.commit(PartitionManager.java:188) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.commit(KafkaSpout.java:169) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:134) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at backtype.storm.daemon.executor$eval5100$fn__5101$fn__5116$fn__5145.invoke(executor.clj:562) ~[na:na]
        at backtype.storm.util$async_loop$fn__390.invoke(util.clj:433) ~[na:na]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_45]
12971 [Thread-27-words] INFO  backtype.storm.util - Halting process: (""Worker died"")"
STORM-371,Windows support to run workers as user,Storm security provides support for launching workers as user by using linux native code worker-launcher. We need something similar for windows.
STORM-368,Trident groupBy().aggregate() produces no results if Spout.nextTuple always emits,"I was debugging an issue with Pig-Squeal and noticed that my Combine Aggregators weren't working with the RandomSentenceSpout.  After some investigation, I discovered that no values are emitted from the aggregate if the underlying Spout always emits tuples.

I will attach a test case to demonstrate shortly."
STORM-362,Update version requirement for clojure/tools.cli to 0.3.1,"Currently, there are issues using storm together with the latest tools.cli, and for some reason leiningen is not able to force storm to use the more recent version of this library (which should work, according to the docs, since the cli function is preserved, in favor of parse-opts for the new functionality). "
STORM-353,Add configuration per kafka-bolt ,"Currently kafka bolt configuration is passed through storm configuration - which limits possibility to use more than one storm bolt/topic per topology.

I can suggest something like this:
diff --git a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
index b9ea948..2a78f84 100644
--- a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
+++ b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
@@ -55,14 +55,20 @@ public class KafkaBolt<K, V> extends BaseRichBolt {
     private OutputCollector collector;
     private String topic;
 
+    private Map boltConfig;
+
+    public KafkaBolt(Map boltConfig) {
+        this.boltConfig = boltConfig;
+    }
+
     @Override
     public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
-        Map configMap = (Map) stormConf.get(KAFKA_BROKER_PROPERTIES);
+        Map configMap = (Map) boltConfig.get(KAFKA_BROKER_PROPERTIES);
         Properties properties = new Properties();
         properties.putAll(configMap);
         ProducerConfig config = new ProducerConfig(properties);
         producer = new Producer<K, V>(config);
-        this.topic = (String) stormConf.get(TOPIC);
+        this.topic = (String) boltConfig.get(TOPIC);
         this.collector = collector;
     }
 
After which you can initialize each bolt with own topic and even own zk servers.

Use case: 
I'm using several streams for output and want each of stream to be published in own topic. "
STORM-339,Severe memory leak to OOM when ackers disabled,"Without any ackers enabled, fast component  will continuously leak memory and causing OOM problems when target component is slow. The OOM problem can be reproduced by running this fast-slow-topology:

https://github.com/Gvain/storm-perf-test/tree/fast-slow-topology

with command:

{code}
$ storm jar storm_perf_test-1.0.0-SNAPSHOT-jar-with-dependencies.jar com.yahoo.storm.perftest.Main --spout 1 --bolt 1 --workers 2 --testTime 600 --messageSize 6400
{code}

And the worker childopts with {{-Xms2g -Xmx2g -Xmn512m ...}}.

At the same time, the executed count of target component is far behind from the emitted count of source component.  I guess it could be that netty client is buffering too much messages in its message_queue as target component sends back OK/Failure Response too slowly. "
STORM-334,Unable to compile storm code,"I have trying to compile the storm code using mvn clean install -X but it keeps failing with the following error,
{code:borderStyle=solid}
[DEBUG] Command line: [java, -Dclojure.compile.path=/Users/gkhare/git/incubator-storm/storm-core/target/classes, -jar, /var/folders/x9/gsh9kn3x5wjch7jfnwh_mf4dn5z616/T/clojuremavenplugin9203516485011075775jar, backtype.storm.command.deactivate, backtype.storm.tuple, backtype.storm.command.shell-submission, backtype.storm.bootstrap, backtype.storm.daemon.acker, backtype.storm.log, backtype.storm.ui.helpers, backtype.storm.testing4j, backtype.storm.config, backtype.storm.zookeeper, backtype.storm.daemon.nimbus, backtype.storm.daemon.builtin-metrics, backtype.storm.ui.core, backtype.storm.event, backtype.storm.command.rebalance, backtype.storm.metric.testing, backtype.storm.command.dev-zookeeper, backtype.storm.daemon.common, backtype.storm.daemon.executor, backtype.storm.daemon.worker, backtype.storm.command.config-value, backtype.storm.process-simulator, backtype.storm.testing, backtype.storm.daemon.supervisor, backtype.storm.disruptor, backtype.storm.thrift, backtype.storm.util, backtype.storm.daemon.logviewer, backtype.storm.messaging.loader, backtype.storm.scheduler.IsolationScheduler, backtype.storm.clojure, backtype.storm.stats, backtype.storm.messaging.local, backtype.storm.timer, backtype.storm.LocalDRPC, backtype.storm.scheduler.DefaultScheduler, backtype.storm.command.list, backtype.storm.cluster, backtype.storm.scheduler.EvenScheduler, backtype.storm.daemon.task, backtype.storm.daemon.drpc, storm.trident.testing, backtype.storm.command.activate, backtype.storm.command.kill-topology, backtype.storm.LocalCluster]
Compiling backtype.storm.command.deactivate to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.tuple to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.command.shell-submission to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.bootstrap to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.daemon.acker to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.log to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.ui.helpers to /Users/gkhare/git/incubator-storm/storm-core/target/classes
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 18.097s
[INFO] Finished at: Thu May 29 16:00:32 IST 2014
[INFO] Final Memory: 25M/278M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.20:compile (compile-clojure) on project storm-core: Clojure failed. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.20:compile (compile-clojure) on project storm-core: Clojure failed.
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)
Caused by: org.apache.maven.plugin.MojoExecutionException: Clojure failed.
	at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:463)
	at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:379)
	at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:356)
	at com.theoryinpractise.clojure.ClojureCompilerMojo.execute(ClojureCompilerMojo.java:40)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)
	... 19 more
{code}

This is possibly failing for following namespace
_backtype.storm.ui.helpers_
"
STORM-333,Kryo Serializers not threadsafe,"I am troubleshooting a customer problem, so please forgive me if I don't know an important implementation detail, but from the looks of it 
backtype.storm.serialization.KryoValuesSerializer
and relatives are not threadsafe.

SerializableSerializer is, and can be used by many threads at the same time. 

The others are not, because they share buffers as internal state and are not synchronized.

I can put together a PR on github if you could confirm that this is a bug and not intended."
STORM-327,command-line interface  for Storm and provide some lkind of Script  to assemble and install topology ,"{code}
> set topology.name=test_topology;
> set storm.jar=./storm-example-0.9.0.jar;
> add jar storm-example-0.9.0.jar;
> REGISTER spout=SPOUT(""storm.starter.spout.RandomSentenceSpout"", 5);
> REGISTER split=BOLT(""storm.starter.WordCountTopology$SplitSentence"", 8).SHUFFLE(""spout"");
> REGISTER count=BOLT(""storm.starter.WordCountTopology$WordCount"", 12).FIELDS(""split"", ""word"");
> submit;

{code}
"
STORM-324,Require notes on internal of storm,"Require information about the internals of __tick and __metrics_tick.
The only information about __tick tuples was from 0.8 release notes.

This is more from a troubleshooting perspective since these tuples are generated by the system so frequently (can be seen when topology.debug is true)

The kind of information that would help:
1. The mechanism of collecting metrics from each executor, spout or bolt
2. Looking at the tuple from debug logs, how to figure out which bolt these are intended to?
2. How does a bolt/spout/executor respond to it?
etc.
"
STORM-319,"mvn build storm failed for that ""../../multilang/rb/storm.rb"" have syntax error.","I download latest storm code from master branch,use ""mvn clean install"" to build.It reports that ""../../multilang/rb/storm.rb"" in resources/storm.rb have syntax error.I am not ruby expert.Who can help me ? thanks.

The error information are below:
145772 [Thread-402] INFO  org.apache.curator.framework.imps.CuratorFrameworkImpl - Starting
145804 [Thread-402-EventThread] INFO  org.apache.curator.framework.state.ConnectionStateManager - State change: CONNECTED
145805 [ConnectionStateManager-0] WARN  org.apache.curator.framework.state.ConnectionStateManager - There are no ConnectionStateListeners registered.
145819 [Thread-402-EventThread] INFO  backtype.storm.zookeeper - Zookeeper state update: :connected:none
146000 [Thread-413-1] ERROR backtype.storm.util - Async loop died!
java.lang.RuntimeException: Pipe to subprocess seems to be broken! No output read.
Shell Process Exception:
/usr/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require': /tmp/7233be77-e821-4ad1-8421-cbbc14c43281/supervisor/stormdist/test-1-1400079903/resources/storm.rb:1: syntax error, unexpected .. (SyntaxError)
../../multilang/rb/storm.rb
  ^
/tmp/7233be77-e821-4ad1-8421-cbbc14c43281/supervisor/stormdist/test-1-1400079903/resources/storm.rb:1: unknown regexp options - ltlag
        from /usr/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
        from tester_spout.rb:19:in `<main>'


        at backtype.storm.utils.ShellProcess.readString(ShellProcess.java:135) ~[classes/:na]
        at backtype.storm.utils.ShellProcess.readMessage(ShellProcess.java:81) ~[classes/:na]
        at backtype.storm.utils.ShellProcess.launch(ShellProcess.java:62) ~[classes/:na]
        at backtype.storm.spout.ShellSpout.open(ShellSpout.java:53) ~[classes/:na]
        at backtype.storm.daemon.executor$fn__3570$fn__3585.invoke(executor.clj:519) ~[classes/:na]
        at backtype.storm.util$async_loop$fn__442.invoke(util.clj:432) ~[classes/:na]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_05]
146002 [Thread-413-1] ERROR backtype.storm.daemon.executor - 
java.lang.RuntimeException: Pipe to subprocess seems to be broken! No output read.
Shell Process Exception:
/usr/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require': /tmp/7233be77-e821-4ad1-8421-cbbc14c43281/supervisor/stormdist/test-1-1400079903/resources/storm.rb:1: syntax error, unexpected .. (SyntaxError)
../../multilang/rb/storm.rb
  ^
/tmp/7233be77-e821-4ad1-8421-cbbc14c43281/supervisor/stormdist/test-1-1400079903/resources/storm.rb:1: unknown regexp options - ltlag
        from /usr/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'
        from tester_spout.rb:19:in `<main>'


        at backtype.storm.utils.ShellProcess.readString(ShellProcess.java:135) ~[classes/:na]
        at backtype.storm.utils.ShellProcess.readMessage(ShellProcess.java:81) ~[classes/:na]
        at backtype.storm.utils.ShellProcess.launch(ShellProcess.java:62) ~[classes/:na]
        at backtype.storm.spout.ShellSpout.open(ShellSpout.java:53) ~[classes/:na]
        at backtype.storm.daemon.executor$fn__3570$fn__3585.invoke(executor.clj:519) ~[classes/:na]
        at backtype.storm.util$async_loop$fn__442.invoke(util.clj:432) ~[classes/:na]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_05]
146052 [Thread-413-1] INFO  backtype.storm.util - Halting process: (""Worker died"")
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm ............................................. SUCCESS [  9.727 s]
[INFO] maven-shade-clojure-transformer ................... SUCCESS [ 11.872 s]
[INFO] Storm Core ........................................ FAILURE [04:07 min]
[INFO] storm-starter ..................................... SKIPPED
[INFO] storm-kafka ....................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:32 min
[INFO] Finished at: 2014-05-14T08:05:05-08:00
[INFO] Final Memory: 26M/148M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.18:test-with-junit (test-clojure) on project storm-core: Clojure failed. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :storm-core"
STORM-314,Storm breaks tools.cli upgrades on Clojure projects that depend on Storm,"We're working on new a Python + Storm interop library called streamparse (https://github.com/Parsely/streamparse/). To submit topologies to Storm and run local clusters, it leverages lein and the Clojure DSL. In the project we create for Storm, our lein project.clj configuration includes these dependencies:

  :dependencies [
                 [storm ""0.9.0.1""]
                 [org.clojure/clojure ""1.5.1""]
                 [org.clojure/data.json ""0.2.4""]
                 [org.clojure/tools.cli ""0.3.1""]
                 ]

The last dependency, org.clojure/tools.cli, is problematic. This is because Storm apparently bundles org.clojure/tools.cli 0.2.x, and due to the way Storm is compiled, it masks over the 0.3.1 dependency, which changes the API dramatically. I discussed this with technomancy (lein's creator) on IRC, and he said this was probably due to ""AOT"" -- ahead-of-time compilation -- causing incorrect classpath resolution to Storm's bundled version

To work around the issue right now, I need to add 

    :exclusions [org.clojure/tools.cli]

to my project.clj. However, I am filing this bug because as the lein author says in the project's FAQ:

""You may also want to report a bug with the dependency that uses hard version ranges as they cause all kinds of problems and exhibit unintuitive behaviour."""
STORM-311,Add command line tool 'monitor' to monitor given topology's given component's throughput performance interactively,"This cmd line 'monitor' will use Nimbus.Client to get throughput information from zookeeper.
1)One can specify topology's name, component's name or type of component such as spout/bolt, without knowing any components' name.
2) It will statistics 'emit' and/or 'transferred' throughput in a given time window and print it in a given time frequency

The implementation will be much like yahoo's storm-perf-test http://yahooeng.tumblr.com/post/64758709722/making-storm-fly-with-netty"
STORM-300,Add need_task_ids field to the multilang protocol description,"The Storm ShellBolt is aware of a need_task_ids field in messages received from non-JVM bolts (https://github.com/apache/incubator-storm/blob/master/storm-core/src/jvm/backtype/storm/task/ShellBolt.java :line 233).

This flag determines whether a ShellBolt will in fact respond with task IDs to a  request from a non-JVM bolt. No mention of this flag is made in the Storm multilang protocol.

I believe this flag should be added the the multilang protocol description or removed from its implementation.n Personally, I'm not a great fan of how this functionality has been implemented (on a per message basis)."
STORM-293,Realtime Topologies Time interval ,"We are running realtime trident topologies that would listen to the Database for new / updated records at every 5 minutes and then send to bolts for some operations. We use Utils.Sleep(300000) of Storm to keep the spout sleep for 5 minutes. 

TridentTopology topology = new TridentTopology();
topology.newStream(""Migration"",new MigrationAuditor()).name(""MigrationAuditorSpout"").parallelismHint(1).shuffle()            
             .name(""MigratorBolt"").each(new Fields(""vcRelationsVO"",""vcendTimeStamp"",""vcskip""),new VCMigrator(),new Fields(""vcendtime"",""vccnt"")).parallelismHint(2).shuffle()
             .name(""LastRunBoltVC"").each(new Fields(""vcendtime"",""vccnt""),new LastRunCalculatorForVC(),new Fields(""vcmlpaData"")).parallelismHint(1).shuffle();

The issue that we face are given below:

1. The spout is getting down randomly at sometime and not runs continuously for every 5 minutes.
2. The bolts (Trident Function) that receives the data from the Spout goes down randomly and hence missing the continuity / data.

For example, when the spout picked up 5 events and goes to sleep, the VCMigrator in this example process 2 events and goes down, there are no traces in log. Sometimes none of the events are processed by the bolt and no traces are found in log. We could not see any log related to this bolt after it goes down. This happens for all the realtime topologies that we run and not only with one.

This runs perfect in the version 0.8.2 and is creating problem in 0.9.0.1 and 0.9.1.

"
STORM-284,Priority in streams,"When a single bolt has multiple input streams, they both flow into the same input queue for the bolt. If one of the streams if much faster, the other will ""starve"".
This is a big issue when trying to use separate control/data streams in Storm, where naturally data arrives much faster than control, but control messages are more important.
A simple way to solve this issue is to declare an optional (class of) priority when latching a bolt onto a stream. Then keep separate queues in the platform, one for each priority level, and check them for messages in order."
STORM-278,Storm on Windows: Modifications required for storm.cmd," When I started working on storm.cmd on windows platform, identified that logs for nimbus/supervisor were not seen. Also, storm classpath doesn't seem to have storm jars and jars in lib.

The childopts from storm.yaml are not read by the bat files. Could see some hardcoded values for these childopts in storm.cmd. 

storm.cmd file doesn't provide all features as in storm.py"
STORM-272,Make worker receiver thread number configurable,"In my profiling, I found the receiver thread of worker can be a performance bottle-neck. 

Now each worker has single receiver thread, and it is responsbile to transfer information generated from multiple netty client to tens of executor disruptor queue. It is too much for busy topology.

Without this fix, we have to increase the number of workers, which will create more intra-worker traffic that we don't want. 
 
I suggest that we can add a config called ""worker.receiver.thread.count"" to control the parallism of the receiver thread, and make it default to 1."
STORM-271,Naming storm threads more meaningful names.,"In my test topology, each worker has 108 threads. Some  threads are generated from a thread pool with a vague name.

For example, the netty threads, the worker receiver thread, the worker transfer threads, the disruptor thread are not good named.

May be we should name them with a more meaningful name so that it is more easy for us to track the performance issues.

Here is the naming I used:

name            description
-------------------------------------
worker-transfer-thread: the single transfer thread for each worker
worker-receiver-thread: the receiver thread for each worker
netty-server-boss-host-port-seqenceId: the netty server boss thread.
netty-server-worker-host-port-sequenceId:the netty server worker thread
executor-disruptor-transfer-thread-[executorId]: executor transfer thread for disrupotor queue.
netty-client-boss-targetHost-port-sequenceId: the netty client bos thread
netty-client-worker-targetHost-port-sequenceId: the netty client worker thread."
STORM-261,Workers should commit suicide if not scheduled any more.,"I know this is a bit far fetched.

If for some reason a supervisor dies and does not come back up again, dead HDD for example, but the workers remain up, and the scheduler decides to move the worker to a new host, a rebalance for instance, the old workers will never go away.  Ideally the worker should know that it is not running in the correct place any more and die instead of waiting for the supervisor to kill it."
STORM-257,Need to harvest doc from old GitHub issues,"STORM-5 led to every GitHub issue being closed out and migrated to this JIRA instance. I think the assumption was the discussion would still be available back on GitHub, however. If the doc still lives there (and is just 404'ing upon retrieval due to Issues being disabled on that repo), then we need to re-enable Issues and archive that discussion before destroying it. There are several issues that had a large amount of conversation that would be very helpful to recover."
STORM-256,storm rebalance bug caused supervisor miss topology’s assignment,"in our 300+ nodes cluster，when do rebalance low probability occurred supervisor miss topology‘s assignmet
Process as Follows：
 nimbus rebalance：
1 . receive relanace command
2.  nimbus chanage job status in zookeeper to ""KILLED""
3.  compute new assignment and write assignment to zookeeper
4. chanage job status to “ACTIVE”

supervisor rebalance (supervisor watch topology assinment node in zookeeper ):
1. when topology's status change to “KILLED”  ,supervisor receive chanage call  mk-synchronize-supervisor function
2.  in  mk-synchronize-supervisor function try to read  assignment from zookeeper ，For simplicity we name assigment-A，but before read out topology‘s status has change to “ACTIVE”，job’s assignment changed to assignment-B ， mk-synchronize-supervisor only read out assignment-B miss assignment-A
3. assignment-A missed, rebanace not become effective in this supervisor ， the whole topology not woring


"
STORM-255,supervisor shutdown worker bug Cause woker port Conflict,"in our storm 300+ node storm cluster ,Encounter the problems：
1. cluster kill topology
2. supervisor try to kill worker，call ensure-process-killed! function ，the function call kill -9 to kill worker，but sometime the process not kill sucess and not throw exception. this function not do a ps to make sure worker was killed.
3. supervisor remove worker-pid-path , 
4. supervisor never know worker's pid and never killed it, the port Occupated by this worker never release
5.if  other topology‘s worker assign worker to this port ，port Conflict occurred
 "
STORM-253,Allow worker to choose available port instead of pre-configed,"To avoid port conflict, we should allow supervisor to choose available port for worker, instead of pre-configuring it in storm.yaml.

Currently, the config for port is:
supervisor.slots.ports: [6700,6701,6702]

We need to obsolete it, and add a new config ""supervisor.slots“. For example:
supervisor.slots: 3

There was a pull request from Andy Feng. We can refer to it and pull in the changes of worker port first. (https://github.com/nathanmarz/storm/pull/622/commits)"
STORM-250,Storm UI: Too many open files,"Occasionally, the ui daemon runs out of available file handles and can't open a socket to the nimbus:

# grep -A1 ERROR ui.log 
2014-02-28 07:44:14 o.a.t.t.TSocket [ERROR] Could not configure socket.
java.net.SocketException: Too many open files
--
2014-03-01 16:44:10 o.a.t.t.TSocket [ERROR] Could not configure socket.
java.net.SocketException: Too many open files
--
2014-03-03 01:29:04 o.a.t.t.TSocket [ERROR] Could not configure socket.
java.net.SocketException: Too many open files
--
2014-03-03 01:34:04 o.a.t.t.TSocket [ERROR] Could not configure socket.
java.net.SocketException: Too many open files
--
2014-03-04 09:39:01 o.a.t.t.TSocket [ERROR] Could not configure socket.
java.net.SocketException: Too many open files
--
2014-03-04 09:45:33 o.a.t.t.TSocket [ERROR] Could not configure socket.
java.net.SocketException: Too many open files"
STORM-249,Activate not called in trident.spout.RichSpoutBatchExecutor,"Activate not called in trident.spout.RichSpoutBatchExecutor, looks like it ought to be called after the open method."
STORM-246,Make the storm ui css/js resource path to be relative so that it can be rendered in YARN page proxy,
STORM-240,Make the Juju Charm for Storm work great,"Update the Juju Charm for Storm in order for users to:
do a one line install (juju quickstart bundle...)
have a one line upgrade (juju upgrade-charm)
instantly scale (juju add-unit)
instantly integrate with other charms like Cassandra, HBase, Tomcat, Presto, etc. (juju add-relation)

More info on Juju Charms at http://juju.ubuntu.com/docs. "
STORM-230,Validate Topology Size,Zookeeper by default has a maximum size of 1MB that can be stored in a znode.  For very large topologies they end up compiling down into something larger then ZK can deal with and nimbus gets stuck in an infinite loop trying to upload the data to ZK.  Also as a sanity check we should provide checks when the topology is submitted to see that it does not go over some cluster defined limitations.
STORM-215,Document How to Replace Netty Transport with 0mq Transport,"As of 0.9.1 Storm uses the Netty transport by default. We should document the process of building, installing, and configuring the 0mq-based transport for those who want to use it."
STORM-212,Add storm-jms as a module,Repo location: https://github.com/ptgoetz/storm-jms
STORM-209,Add storm-cassandra as a module,"Current repo: https://github.com/hmsonline/storm-cassandra

We would likely need to go through an IP clearance process for this since the repo is owned by an organization (HMS - http://www.healthmarketscience.com), but they have expressed some interest in donating/transferring the project."
STORM-206,"Incorporate ""Contrib"" Modules into Apache Build and Release Process","It would be nice to have some sort of process for including optional Storm components (Kafka Spout, etc.) in the Storm build and release process.

This would make it easier to keep such components version-aligned with the main Storm build and potentially reduce fragmentation (like that which occurred with the Kafka Spout).

We will need to figure out how such modules will be incorporated into the project/build, as well as a process for adding and maintaining modules.

Some existing modules might need to go through the IP clearance process before being included, so we'll need some process around that as well."
STORM-204,Local topologies that throw exceptions can crash the REPL,"We've been testing some Storm topologies using ""lein repl"". Based on the storm-starter project, our local testing environment offers a Clojure function (run-local!), which builts up the topology using the Clojure DSL and runs it. The issue is that there are conditions that can cause topology to fail; the exception thrown then crashes the Storm worker, which, in turn, crashes the lein repl itself. This is frustrating since part of the purpose of running a local cluster is to test it from something like a REPL.

I think I've narrowed down what's going wrong. You can see an example session with stacktrace here:

https://gist.github.com/amontalenti/8677464#file-example_storm_crashing_lein-txt-L142-L173

The way I created this error is by renaming my Python module before running the topology, so that the Python file could not be found.

I think what's going on is that the ShellBolt is throwing a RuntimeException, which is uncaught by whatever is running the ShellBolt. This, in turn, crashes the worker: ""Error when launching multilang subprocess"". The executor notices that the worker dies, but is a bit zealous and decides to call (halt-process!) on it. Under the hood, halt-process! uses Runtime.getRuntime#halt, which is a forcible kill of the running JVM. Since the JVM, in this case, is ""lein repl"", I think this is what ultimately kills the REPL.

http://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.html#halt(int)

I suppose there are two possible fixes here. One is to make the Storm worker a little more resilient to a misconfigured ShellBolt. The other is to make the halt-process! call not run when a REPL environment is detected.

I am glad to work on either of these fixes once the issue is confirmed and a path forward is suggested."
STORM-202,Worker can possibly hang when the stdout/stderr buffer is full,"In util.clj

(defnk launch-process [command :environment {}]
  (let [command (->> (seq (.split command "" ""))
                     (filter (complement empty?)))
        builder (ProcessBuilder. command)
        process-env (.environment builder)]
    (doseq [[k v] environment]
      (.put process-env k v))
    (.start builder)
    ))

ProcessBuilder stdout and stderr is not properly handled. When there are too much messsage for stdout and stderror, the process will freeze.

"
STORM-201,JAVA Home should be read from current JVM path instead of getting it from PATH,
STORM-199,Move off of Custom Thrift for Code Generation,"Storm currently depends on a patched version of Apache Thrift (7) for code generation. (N.B. -- this only applies to code generation, not the thrift client library)

This has not been a big problem since the storm thrift API is relatively stable, but something that should be kept in mind moving forward. It would be great if storm could move to a newer, stock version of thrift."
STORM-197,registering metrics in an IRichSpout used in Trident Topology throws an Exception,"Because of how the RichSpoutBatchExecutor initializes and calls open() of the IRichSpout, any metric registrations cause an exception to be thrown.  See stacktrace below.

java.lang.RuntimeException: java.lang.RuntimeException: TopologyContext.registerMetric can only be called from within overridden IBolt::prepare() or ISpout::open() method.
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:90)
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:61)
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62)
    at backtype.storm.daemon.executor$fn__3498$fn__3510$fn__3557.invoke(executor.clj:730)
    at backtype.storm.util$async_loop$fn__444.invoke(util.clj:403)
    at clojure.lang.AFn.run(AFn.java:24)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: TopologyContext.registerMetric can only be called from within overridden IBolt::prepare() or ISpout::open() method.
    at backtype.storm.task.TopologyContext.registerMetric(TopologyContext.java:213)
    at com.XXXXXXX.spouts.amqp.AMQPSpout.open(AMQPSpout.java:232)
    at storm.trident.spout.$RichSpoutEmitter.emitBatch(RichSpoutBatchExecutor.java:88)
    at storm.trident.spout.TridentSpoutExecutor.execute(TridentSpoutExecutor.java:65)
    at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:352)
    at backtype.storm.daemon.executor$fn__3498$tuple_action_fn__3500.invoke(executor.clj:615)
    at backtype.storm.daemon.executor$mk_task_receiver$fn__3421.invoke(executor.clj:383)
    at backtype.storm.disruptor$clojure_handler$reify__2962.onEvent(disruptor.clj:43)
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:87)
    ... 6 more"
STORM-195,dependency-reduced-pom.xml should be in .gitignore,"The new Maven-based build system generates a file for the shaded libthrift dependency: storm-deps/libthrift/dependency-reduced-pom.xml. This file doesn't belong in source control, but it's not in .gitignore, meaning that it clutters up git status output, and risks accidental inclusion from a rogue `git add .`"
STORM-193,I think it would be more user friendly if [storm kill ...] was synchronous,"Currently the [storm kill topology-name] command is not synchronous, even if [-w 0] is used. After killing a topology using the kill command then one must wait an unspecified amount of time (a second or so) before running [storm jar] with the same topology name, otherwise storm will complain that the topology already exists.

This is frustrating me during development when I am repeatedly killing and deploying new versions of my topology.

I would like if there was an option to have [storm kill] be synchronous, or even if it was the default behavior."
STORM-192,"NETTY DRPC cluster: When the number of worker if higher than 1 with more than one supervisor, the topology fail to execute ","On a cluster with one supervisor it work.
Even it have more than one worker.
But when I had more than one supervisor it don't work when I have more than one worker.

I have port set for the worker and they are not used.

Worker log look like this:
2014-01-07 16:09:08 STDIO [ERROR] boss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.conn
2014-01-07 16:09:08 STDIO [ERROR] ect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.reconnect(Client.java:96)
	at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:118)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:124)
	at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:69)
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
	at org.jboss.netty.channel.Channels.connect(Channels.java:634)
	at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
	at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
	at backtype.storm.messaging.netty.Client.<init>(Client.java:84)
	at backtype.storm.messaging.netty.Context.connect(Context.java:53)
	at backtype.storm.daemon.worker$mk_refresh_connections$this__5822$iter__5829__5833$fn__5834.invoke(worker.clj:265)
	at clojure.lang.LazySeq.sval(LazySeq.java:42)
	at clojure.lang.LazySeq.seq(LazySeq.java:60)
	at clojure.lang.Cons.next(Cons.java:39)
	at clojure.lang.RT.next(RT.java:587)
	at clojure.core$next.invoke(core.clj:64)
	at clojure.core$dorun.invoke(core.clj:2726)
	at clojure.core$doall.invoke(core.clj:2741)
	at backtype.storm.daemon.worker$mk_refresh_connections$this__5822.invoke(worker.clj:259)
	at backtype.storm.daemon.worker$fn__5877$exec_fn__1242__auto____5878.invoke(worker.clj:372)
	at clojure.lang.AFn.applyToHelper(AFn.java:185)
	at clojure.lang.AFn.applyTo(AFn.java:151)
	at clojure.core$apply.invoke(core.clj:601)
	at backtype.storm.daemon.worker$fn__5877$mk_worker__5933.doInvoke(worker.clj:344)
	at clojure.lang.RestFn.invoke(RestFn.java:512)
	at backtype.storm.daemon.worker$_main.invoke(worker.clj:454)
	at clojure.lang.AFn.applyToHelper(AFn.java:172)
	at clojure.lang.AFn.applyTo(AFn.java:151)
	at backtype.storm.daemon.worker.main(Unknown Source)"
STORM-190,"the allocation of workers, executors and tasks is amazing in our storm-0.9.0.1 cluster","we are now testing storm-0.9.0.1 cluster for migrating. When running the topologies in storm-starter,  such as WordCountTopology, ReachTopology, we found the number of workers, executors, and tasks is unexplainable (according to https://github.com/nathanmarz/storm/wiki/Understanding-the-parallelism-of-a-Storm-topology ):

1) WordCountTopology:
      Name              Id            Status   Uptime   Num workers Num executors Num tasks
   wc-test3   wc-test3-3-1388986658   ACTIVE 29m 5s     25          50            50
   wc-test2   wc-test2-2-1388986111   ACTIVE 38m 12s    4           29            29
   wc-test    wc-test-1-1388981297    ACTIVE 1h 58m 26s 3           28            28

This topology's configuration is as follows:
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout(""spout"", new RandomSentenceSpout(), 5);
    builder.setBolt(""split"", new SplitSentence(), 8).shuffleGrouping(""spout"");
    builder.setBolt(""count"", new WordCount(), 12).fieldsGrouping(""split"", new Fields(""word""));
    ......
    conf.setNumWorkers(3); #4, 25

2) ReachTopology (which is a drpc topology):
      Name              Id            Status  Uptime   Num workers Num executors Num tasks
   reach-test reach-test-4-1388986889 ACTIVE 32m 20s   6           35            35

This topology's configuration is as follows:
    LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder(""reach"");
    builder.addBolt(new GetTweeters(), 4);
    builder.addBolt(new GetFollowers(), 12).shuffleGrouping();
    builder.addBolt(new PartialUniquer(), 6).fieldsGrouping(new Fields(""id"", ""follower""));
    builder.addBolt(new CountAggregator(), 3).fieldsGrouping(new Fields(""id""));
    ......
    conf.setNumWorkers(6);
"
STORM-189,Allow user to specifiy full configuration path when running storm command,"Currently, storm will only look up configuration path in java classpath. We should also allow user to specify full configuration path. This is very important for a shared cluster environment, like YARN. Multiple storm cluster may runs with different configuration, but share same binary folder.

"
STORM-185,solution for sending notification to spout for the completion of task of particular batch,"Hi Everyone,

In my use case i need to send a notification to spout as soon as the processing of a batch is completed by the final bolt.After the completion of this batch only new batch has to be emitted.How do i implement this scenario as static variables cant be used in multi node storm cluster.I am using trident with state management implementation.I am struggling for this since last 15 days.I tried implementing ITridentSpout but still not able to send notification back to spout.

Please suggest some solution.
"
STORM-184,workers should use user defined logback configurationFile,"nohup storm -c logback.configurationFile=/tmp/cluster.xml  supervisor  & 
but the workers will always use '$STORM_HOME/logback/cluster.xml'

https://github.com/apache/incubator-storm/blob/master/storm-core/src/clj/backtype/storm/daemon/supervisor.clj#L458
"
STORM-182,Add Tool Tip Help to UI,"Very often the information presented on storm's UI leads to questions that can be answered via the documentation.  Many times it is unclear to new users where the correct documentation is, and it is easier to ask others via the mail list or otherwise.  Also, maintaining documentation separate from code leads to version mismatch headaches, and users can be uncertain that the documentation they read is valid for the version of storm they are using.

We could add short documentation directly to the UI in the form of tool tips.  This way, useful information is handy and can be expected to match the functionality of the storm version being used."
STORM-179,integration testing of custom serializer code without running a multi-node cluster ,"
I need to integration test  various customer serializers we are writing for our current  project.
As I understand it, only if a spout or bolt is sending a tuple to a tuple in another JVM does kryo serialization occur.  That's a good performance optimziation, but it would be nice to be able to disable it for testing purposes.
With such an option one could  force that custom  serialization code to be invoked  even when  running everything in
memory  in one JVM   (via LocalCluster).  That makes for more convenient debugging and faster turn-around.

thanks
 - chris
"
STORM-178,sending metrics to Ganglia,"provide something like ./conf/storm-metrics.properties 
and GangliaContext.java 

Collect and sent metrics to Ganglia
"
STORM-177,Add cpu metrics,
STORM-176,UI will lead to wrong url when running storm on yarn ,"
For Example:
On Pure Storm Cluster , the url is  in this pattern, and that's right :
http://nimbus-host:port/topology/topology-id 


But On Yarn the link should be :
http://app-master-host:port/proxy/app-id/topology/topology-id 

Current Storm core.clj will mislead to http://app-master-host:port/topology/topology-id "
STORM-175, Wokers should extend supervisor’s config and merge with local config ,"
For example:
config 'storm.zookeeper.root ' .its default value is '/storm'
but we can define a new value use '-c' command when  start up services: 
storm -c storm.zookeeper.root=/storm/cluster01 nimbus 
storm -c storm.zookeeper.root=/storm/cluster01  supervisor

workers’ configuration will be {storm.zookeeper.root=/storm}
but they’re should be {storm.zookeeper.root=/storm/cluster01}"
STORM-172,Add hooks for creating shared resources among tasks,"https://github.com/nathanmarz/storm/issues/240

Building off of #155, the worker hook API should have a way to define shared resources to be used by all tasks. These resources can then be cleaned up in the worker hook's cleanup method.

----------
xumingming: @nathanmarz is setTaskData implemented for this?

----------
danehammer: I remember seeing several TODOs around this when I was working on the worker lifecycle code."
STORM-170,serialize all storm zk state as EDN,"https://github.com/nathanmarz/storm/issues/525

this will make it possible to add new entries without breaking backwards compatibility.

I implemented push away from java serialization in issue #419 and pr #497. It seems straightforward to change the serialization in this pr to EDN. Is this what everyone would like me to do?

----------
hausdorff: I want to make sure I know specifically what you want, since I'm a new contributor to the project.

Currently the pr mentioned above transitions zk serialization clojure s-exps, represented as UTF-8 strings.

This issue proposes changing the zk serialization to EDN so we can ""add new entries without breaking backwards compatibility."" I don't know EDN that well, but the spec says it's a superset of Clojure, so will what I've written already work? It's unclear to me whether the answer is yes, because I'm not actually sure what you mean by adding ""new entries.""

----------
jasonjckn: clojure data is sufficient to meet ""add new entries without breaking backwards compatibility.""
I suggest you wait for Nathan's feedback before doing anymore work, he's busy doing a start-up though."
STORM-169,Storm depends on outdated versions of hiccup and ring,"https://github.com/nathanmarz/storm/issues/707

Storm core depends on a very old versions of

hiccup 0.3.6 => 1.0.4
ring-* 0.3.11 => 1.2.0

Especially the hiccup version conflicts with modern libraries like liberator (see also weavejester/hiccup#86

IllegalStateException escape-html already refers to: #'hiccup.core/escape-html in namespace: hiccup.page

----------
revans2: I agree that storm depends on old versions of libraries that I would love to see updated, but simply updating the versions can cause a lot of pain too. Having worked through this on Hadoop the only real solution is to get to true dependency isolation. There are just too many libraries that are not backwards and forwards compatible in very subtle ways, even for minor releases. If you upgrade one dependency to fix a specific incompatibility it will break someone else with a new incompatibility.

I am not saying that we should not do this. People should be willing to pay off tech debt and upgrade, but until we can isolate a user's dependencies from storm's dependencies we need to consider major dependency upgrades as incompatible changes.

----------
jocrau: Yes, updating versions can cause a lot of pain. But deferring every update to the time when the user's dependency is isolated might also cause a lot of pain and slow down the adoption of Storm in some domains (like RESTful services). I would love to see a more differentiated approach. Hiccup for example is only used in the Storm UI; a place where I expect little impact.

----------
revans2: I totally agree. I just wanted to be sure that the storm version number was bumped when we did this, so that we are indicating to downstream users that something incompatible may have happened and they need to retest.

Now that storm is a multi-module project splitting out the UI and logviewer to be their own jars with separate dependencies really seems like a good start for isolation. More then that gets a little bit harry but is still doable.

If you think about it the worker only needs to pull in a few clojure APIs, disruptor, curator, and jzmq or netty depending on how it is configured.
"
STORM-168,Serialization of spouts/bolts,"https://github.com/nathanmarz/storm/issues/367

Once annoying thing about Java serialization is that it requires all serializable objects to be explicitly marked with Serializable interface. When using 3rd party libraries, sometimes people forget to do this. An idea here is to use Kryo serialization instead and have it default to Java serialization. The serializer should be prepped with the same serializers as is used for the Kryo serializer for tuples in the topology.

----------
RomezzBorisov: It should be possible to choose the s11n approach via configuration

----------
danehammer: @nathanmarz Can you explain further? Would this be implementing a spout serializer? A bolt serializer?


----------
RomezzBorisov: It should be possible to provide a custom s11n for Storm objects in topology (bolts and spouts both, I don't think it's necessary to distinguish them) via implementing smth like StormSystemSerializer interface.
The instance of serializer should be passed to topology builder.

There should be several default implementations like JavaSystemSerializer, KryoSystemSerializer e.t.c.
"
STORM-167,proposal for storm topology online update,"https://github.com/nathanmarz/storm/issues/540

Now update topology code can only be done by kill it and re-submit a new one. During the kill and re-submit process some request may delay or fail. It is not so good for online service. So we consider to add topology online update recently.

Mission

update running topology code gracefully one worker after another without service total interrupted. Just update topology code, not update topology DAG structure including component, stream and task number.

Proposal

* client use ""storm update topology-name new-jar-file"" to submit new-jar-file update request
* nimbus update stormdist dir, link topology-dir to new one
* nimbus update topology version on zk
* the supervisors that running this topology update it
** check topology version on zk, if it is not the same as local version, a topology update begin
** each supervisor schedule the topology's worker update at a rand(expect-max-update-time) time point
** sync-supervisor download the latest code from nimbus
** sync-process check local worker heartbeat version(to be added), if it is not the same with sync-supervisor downloaded version, kill the worker
** sync-process restart killed worker
** new worker heartbeat to zk with version(to be added), it can be displayed on web ui to check update progress.

This feature is deployed in our production clusters. It's really useful for topologys handling online request waiting for response. Topology jar can be updated without entire service offline.

We hope that this feature is useful for others too.
"
STORM-165,Adding more supervisor causes topology (LinearDRPC) to fail,"https://github.com/nathanmarz/storm/issues/292

Following my previous email, i just want to add this to the issue log so everyone is aware. One new thing I found is that it's not associated with a ""magic number"" of supervisor. I have it running at 3 but not at 4. I think it has to do with this specific error:

2012-08-14 11:29:33 worker [WARN] Received invalid messages for unknown tasks. Dropping... 
2012-08-14 11:29:33 worker [WARN] Received invalid messages for unknown tasks. Dropping... 
2012-08-14 11:29:33 worker [WARN] Received invalid messages for unknown tasks. Dropping...
Seems the fourth one gets messages for tasks it does not know about- possibly out of sync.

Steps to reproduce:

1. Bring one supervisor up 2 mvn -f m2-pom.xml compile exec:java -Dexec.classpathScope=compile -Dexec.mainClass=storm.starter.DRPCTestClient -Dexec.args=""2000""

DRPRC CLIENT
SIZE 2000
Result = []
db time 3268 ms
[INFO] ------------

2. Repeat adding supervisor and rerun the client.

#DRPRC CLIENT #
SIZE 2000
[WARNING] 
java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
..


----------
amuraru: @nathanmarz What kind of key is used to uniquely identify the supervisor in nimbus? I saw similar errors when running multiple supervisors on different machines but with the same hostname (e.g. localhost.localdomain) listed in storm ui. COuld be this an issue?

----------
nathanmarz: Supervisors generate a unique id for themselves and store it in their local dir. It's fine to run many supervisors on the same host as long as they don't share a local dir.

----------
willfcareer: Hi, there can't be more supervisor in trident drpc ?"
STORM-164,Split Storm into multiple modules,"https://github.com/nathanmarz/storm/issues/548

The preliminary work of making Storm a multimodule setup has been done (currently pretty much everything is in storm-core). The new setup is a drop-in replacement for the old setup, so the same maven artifact names still work and have the same semantics.

We have the flexibility to split Storm into smaller modules that can be included / excluded as desired by users. This makes it easier to control dependencies. Here's my current list of modules to create:

storm-ui
storm-linear-drpc
storm-drpc-server
storm-transactional-topology
storm-trident
storm-zmq-messaging
storm-client (bin/storm and related java/clojure files)

Please comment with additional modules you think Storm should be broken up into.

----------
mrflip: When you do this surgery, is it reasonable to do away with the backtype. prefix, and normalize everything onto storm. and trident. (or storm.core and storm.trident) namespaces?

Watching other folks learn the codebase (self included), it's a hurdle to keep track of where in {clj,jvm}/{backtype/storm,storm/trident} everything is.

----------
nathanmarz: That would be backwards incompatible. I'd be for that change, but as part of a future major release and independent from this issue.

----------
mrflip: sounds good, and prudent. Thanks for considering it."
STORM-163,Simulated Cluster Time doesn't work well for me.,"https://github.com/nathanmarz/storm/issues/512

{code}
(deftest test-builtin-metrics-2
  (with-simulated-time-local-cluster
    [cluster :daemon-conf {TOPOLOGY-ENABLE-MESSAGE-TIMEOUTS true                         
                           TOPOLOGY-MESSAGE-TIMEOUT-SECS 5
                           }]
    (let [feeder (feeder-spout [""field1""])
          tracker (AckFailMapTracker.)
          _ (.setAckFailDelegate feeder tracker)
          topology (thrift/mk-topology
                    {""myspout"" (thrift/mk-spout-spec feeder)}
                    {""mybolt"" (thrift/mk-bolt-spec {""myspout"" :shuffle} ack-every-other)})]      
      (submit-local-topology (:nimbus cluster)
                             ""metrics-tester""
                             {TOPOLOGY-ENABLE-MESSAGE-TIMEOUTS true                           
                              TOPOLOGY-MESSAGE-TIMEOUT-SECS 5}
                             topology)
      (.feed feeder [""a""] 1)              
      (.feed feeder [""b""] 2)
      (advance-cluster-time cluster 10)
      (assert-failed tracker 2)
      )))
{code}

The above unit test just hangs. 
This isn't just a one off unit test, there's a whole class of these when advance-cluster time is near message-timeout-secs (but greater than).

I noticed that when I added system executors in order to get heap size metrics, that an existing metrics unit test started to fail at assert-failed where previously it succeeded. So it seems like the amount that advance-cluster-time has to exceed message timeout by is not constant . This might explain the why zookeeper 3.4.5 upgrade caused unit tests to hang (where mk-in-process zookeeper has slower performance and start-up time).

And it'll pass if I run it with lein2 test selector , but fail if I run all unit tests together

I spent 6 hours trying to fix the unit tests, but haven't figured it out yet.
"
STORM-161,Proposal: move the Kryo config code out to chill-java,"https://github.com/nathanmarz/storm/issues/617

twitter/chill#81

It would be really great to remove the duplication around building Kryo objects (chill-hadoop does it, chill-akka (not yet split out of scala-fish) does it, chill-scala has another one (KryoRegistrationHelper) and storm has one).

It would be really awesome if we (@sritchie, @nathanmarz @jasonjckn @johnynek) could hammer out a plan to have a common dependency.

---------
nathanmarz: I don't view this as very important. It looks like your chill/Storm integration code is about 10 lines, so this doesn't appear to be causing very much pain. Extracting a common dependency sounds good in theory, but as soon as we want to add a new serialization feature to Storm we'll be forking the dependency anyway.

---------
sritchie: The pain is in the diamond dependency on Kryo. Users process the same data in Hadoop and Storm (via Summingbird, for one, but even when writing separate jobs) and serialization versioning mismatches are one of the biggest pains we face.

It's unclear why adding a new serialization feature to storm would need a new fork of Carbonite. That sounds like a design issue."
STORM-158,Live tracing of tuples through the flow,"https://github.com/nathanmarz/storm/issues/531

Storm should let you bless a record as a ""tracer bullet"", to be specially reported on as it progresses through the flow. It's important that this be completely transparent -- that is, I can unintrusively switch tracing on in the flow, and that tracer bullets are real, live records (not specially-crafted packets). The intent is that a small fraction of records be tracer bullets. If you harm performance by passing too many through, that's your fault for passing too many through.

@maphysics is working on code to implement this --- we're finding it very useful for debugging flows -- and so we'd like to see if this is functionality you'd pull into the mainline or storm-contrib. (If this already exists, please advise.)

Pragmatically, what we're working on is a TracerHook that...

* In 'prepare', captures some helpful information about the flow.
* The hook points (emit, boltExecute, both Ack/Fails) look for a field called '_trace'; if absent or null, they return immediately. Otherwise, the field is a HashMap indicating that the tuple should be traced. (We're using a HashMap to allow whoever designated the tuple for tracing to inject extra metadata for the trace report)
* If it's a tracer bullet each hook point simply writes a verbose, helpfully-formatted biography of the record to the log (the execute hook is more verbose than the others).
the component_id, sources & targets, etc of the bolt/spout
the hook point it hit ('emit', 'ack', etc)
list of output tuple's values in regular order
* ...and records minimal provenance:
The execute hook saves off (into a private variable on the hook) the _trace field of the input tuple (a question about this is below)
calls to the emit hook take the saved _trace info and duplicate it into the output tuple

Some Questions

Should these be metrics or hooks? Right now, we're using the hook functionality, not the metrics, because...

* It wasn't clear how to inspect the tuple from the metric
* The lifecycle of the metrics matches the bolt's, not the record's -- we'd prefer as-rapid-as-reasonable reporting, tied to the tuple's progress
* Along with, basically, using metrics would require more spelunking to figure out... We'll follow up to the mailing list with questions on this. So it seems like a hook is the right thing, although cycling a trace trail back to nimbus has a lot of appeal.

Are the hook points dependably executed in-order? That is, if the execute hook point for bolt A on tuple Q is invoked, can we depend that (until execute is called again), the calls to emit and then to ack/fail are direct consequence of processing tuple Q? (The code seems to say yes, but can we treat that as part of the contract?)

How do we transparently carry the traceinfo all the way through the topology_ -- yet not annotate every single bolt/spout with trace_info as a field? @maphysics is following up with a separate issue on this. We need to decorate each tuple generated from processing a tracer bullet with a _trace field of its own -- but without modifying the topology or its bolts.

/cc @maphysics @kornypoet

As mentioned, we'd eventually like to dispatch tracings to nimbus (or somewhere central). Instead of metrics, another approach would send them to an implicit 'tracings' stream, similar to the 'failure stream' mentioned in #13. Has there been any progress on implicit failure streams?

see also #146"
STORM-157,ack/fail should not require keeping input values in scope.,"https://github.com/nathanmarz/storm/issues/752

ack/fail takes a Tuple, but it appears the values are not needed to ack. If we aggregate many things locally before we commit, we keep refs to many Tuples. We think this could be keeping more in memory than we need and pushing some topologies to the breaking point.

We are not 100% that this is the issue, but it would be good to have something like, getToken on Tuple. And ack/fail might take that Token which is an opaque object that includes the minimal refs to ack.

----------
nathanmarz: +1

This should be pretty easy to implement. Tuples already have a MessageID which is the primary object used for acking. There's also the ""ack val"" (the xors of tuples anchored onto this tuple) which should be moved into the MessageID. Then we can use the MessageID as the token, and update the APIs to accept the MessageID for acking and MessageID for anchoring as part of execute. IOutputCollector should be changed to only accept MessageID for acking/failing/anchoring, and then OutputCollector can add the convenience methods for accepting Tuple acking/anchoring/failing.

----------
jmlogan: We had this similar issue when we had very large tuples...at time time I ""worked around it"" by using Reflection to get ahold of the internal list storing the values, and clearing it.

I ended up mitigating this problem for good by having smaller tuples, and passing the large payloads through Redis."
STORM-156,Compiling Storm 0.9.0 results in java.lang.ClassNotFoundException: backtype.storm.LocalDRPC,"https://github.com/nathanmarz/storm/issues/495

In reference to pull request #460 I was able to reproduce the error
and gather log files. I am using the bin/build_release.sh to compile
Storm. Storm compiles on Mac OS X 10.8.2 using Lein 2.0.0 and Java JDK
1.7. On Ubuntu 12.04 using Lein 2.0.0 and Java JDK 1.7 it fails with
the following error:

Compiling storm.trident.testing Exception in thread ""main""
java.lang.ClassNotFoundException: backtype.storm.LocalDRPC,
compiling:(testing.clj:1) at
clojure.lang.Compiler$InvokeExpr.eval(Compiler.java:3387) at
clojure.lang.Compiler.compile1(Compiler.java:7035) at
clojure.lang.Compiler.compile1(Compiler.java:7025) at
clojure.lang.Compiler.compile(Compiler.java:7097) at
clojure.lang.RT.compile(RT.java:387) at
clojure.lang.RT.load(RT.java:427) at clojure.lang.RT.load(RT.java:400)
at clojure.core$load$fn__4890.invoke(core.clj:5415) at
clojure.core$load.doInvoke(core.clj:5414) at
clojure.lang.RestFn.invoke(RestFn.java:408) at
clojure.core$load_one.invoke(core.clj:5227) at
clojure.core$compile$fn__4895.invoke(core.clj:5426) at
clojure.core$compile.invoke(core.clj:5425) at
user$eval7.invoke(NO_SOURCE_FILE:1) at
clojure.lang.Compiler.eval(Compiler.java:6511) at
clojure.lang.Compiler.eval(Compiler.java:6501) at
clojure.lang.Compiler.eval(Compiler.java:6477) at
clojure.core$eval.invoke(core.clj:2797) at
clojure.main$eval_opt.invoke(main.clj:297) at
clojure.main$initialize.invoke(main.clj:316) at
clojure.main$null_opt.invoke(main.clj:349) at
clojure.main$main.doInvoke(main.clj:427) at
clojure.lang.RestFn.invoke(RestFn.java:421) at
clojure.lang.Var.invoke(Var.java:419) at
clojure.lang.AFn.applyToHelper(AFn.java:163) at
clojure.lang.Var.applyTo(Var.java:532) at
clojure.main.main(main.java:37)

I have contacted leiningen on IRC and they suggested that it might be
Storm. I am attaching the logs that I generated with: export
DEBUG=true lein with-profile release jar > system_jar.txt 2>&1

Mac OS X Log: http://pastebin.com/YvBRisSH Ubuntu 12.04 Log:
http://pastebin.com/6KMKF63e

---------- d2r: I had to add :require LocalDRPC before the :import.

The cause might have to do with build order...

--------- alexcpsec: I had the same issue and fixed it the same
way. Is there a pull request for this yet

---------- alchemist0: I am running into this error too. I am not very
strong with the clojure syntax. Could someone either give me the code
which needs to be inserted verbatim, or share their working
testing4j.clj file with me?

---------- d2r: I have the following:

  1 (ns backtype.storm.testing4j 
  2 (:import [java.util Map List Collection ArrayList]) 
  3 (:require [backtype.storm LocalCluster]) 
  4 (:import [backtype.storm Config ILocalCluster LocalCluster])

---------- ypf412: In my environment, I have the following: 1 (ns
backtype.storm.testing4j 2 (:import [java.util Map List Collection
ArrayList]) 3 (:require [backtype.storm.LocalCluster :as
LocalCluster]) and 1 (ns storm.trident.testing 2 (:require
[backtype.storm.LocalDRPC :as LocalDRPC])"
STORM-154,"Provide more information to spout ""fail"" method","https://github.com/nathanmarz/storm/issues/39


It might be helpful to distinguish between unexpected errors (when they can be caught) and timeouts.

----------
conflagrator: +1 on this. I wrote a class extending OutputCollector with the following wrapper functions:

public class VerboseOutputCollector extends OutputCollector {
    public void fail(Tuple tuple) {}
    public void fail(Tuple tuple, String message) {}
    public void fail(Tuple tuple, Exception e) {}
    public void fail(Tuple tuple, Exception e, String message) {}
}
Each function generates an output containing the class and the line number of the ""fail"" call and the message or Exception, if provided. It's very handy for log analytic.

----------
dmoore247: +1

With 0.8.1 on a local cluster I've spent many hours tracking down failures, going through executor.clj code, turning on full logging, adding TaskHooks, playing with time out parameters, adding exception handling etc. 
As an aside, the SpoutFail....latencyMs value was always a null in my tests on the LocalCluster.

Still, all I know is that the message failed, but not why (Timeout?). 
Based on playing with the timeout parameters, I deduce that the failures were caused by timeouts.

Where in Storm does it determine, hey, we've exceeded a timeout, let's fail this Tuple? At least we/I could add debug message to Storm.

Many thanks.

----------
ruleb: +1

Had the same situation, searched a whole day to conclude that a trident topology regularly dropped complete batches of tuples because of timeout reached when they are queued up at a busy bolt. 
Having a small ""tuple timeout reached"" in the logs @ info level will save many developer days.

Many thanks.

----------
thecoop: This would be very helpful to determine why tuples are failing, rather than just an arbitrary number in the UI - just putting something in the logs as an info or warn saying a tuple failed and some information on why it failed.

----------
brianantonelli: +1

Would be great to get more information about what caused the spout to fail. I'm also seeing that the latency is always null too.

----------
revans2: It is fairly simple to extend spout to indicate if a tuple failed because of a timeout or if it failed because of something else, but it is much harder to determine what that something else was. The fail API on all output collectors does not have anything that could be used to map it to a reason. We would have to extend the API and decide what the failure reason should look like. Perhaps a free form string, but that is really horrible if you want to aggregate the failures in metrics. Also we would want to limit the size of the string so an to not overwhelm the acker bolts."
STORM-153,Scaling storm beyond 1.2k workers,"https://github.com/nathanmarz/storm/issues/620

Our storm & zookeeper cluster has reached a scaling limit, on the
zookeeper we retain storm state, kafka commit offsets and trident
transaction state. Our hardware is no joke (RAIDed 6 disks with 512mb
cache). The reason I say it's reached scaling limit is because a few
users have reported trying to deploy a topology, only to see the
workers continually die and restart with executor not alive in Nimbus
logs. Additionally, recently when a user deployed a topology with 140
workers, not only did his topology enter into continuous workers death
& restart, but at the same moment, half of production also went into
this state and never recovered until I manually deactivated a bunch of
production. So this person's topology caused previously stable workers
to get executor not alive exceptions.

We have a dedicated zookeeper SRE who says zkget, and zk list
children, and the heartbeats of zookeeper ephemeral nodes are
relatively very cheap operations, but zk create and set are expensive
since they require quorum, and he's never seen a zookeeper cluster
with so many quorum operations at any company he's worked at in the
past. The rate of quorum ops tends to be at around 1.5k/s sustained
rate. If you do the math, starting with the fact we have 1.2k workers,
and that our workers are configured for 1 heartbeat a second, that
implies 1.2k/s zksets per second which is 80% of the total number of
quorum operations we're measuring via zookeeper metrics. Comparing our
empirical throughput to this benchmark
http://zookeeper.apache.org/doc/r3.1.2/zookeeperOver.html our real
world results are not quite as good as what's claimed here, but in the
same order of magnitude, storm also uses much larger zknode payload
approaching 1mb in some cases. We need a way to scale to AT LEAST 10k
workers, preferably 100k workers. So what are the solutions here?

SOLUTION #1

Ephemeral nodes are an obvious solution here. There's a few hidden
complexities:

#1 We're not actually reducing the peak quorum operation rate, because
 creating an ephemeral node requires a quorum operation. Clearly it's
 less load overall though, because you don't have sustained zksets
 every second. However if hitting a peak quorum operation of 1.5k/s
 were to cause zk sessions to die such as what we've observed on the
 current cluster, then ephemeral don't solve this problem, because
 they still require quorum to create. I think the only way to
 determine if this will occur is to build it and extensive testing.

I'm not sure what netflix curator does here, but IF the ephemeral
nodes are immediately recreated when the session is re-established
then you easily enter into cascading failure because that means higher
reconnect rates are positively correlated with higher quorum ops, so
this easily could enter into tailspin. A simple solution here is to
NOT recreate the ephemeral nodes during a reconnect, and instead wait
for the heartbeat to occur.

#2 Ephemeral nodes are tied to zk session, so if you kill -9 a worker,
 from my understanding there will be some latency in when the
 ephemeral node is actually cleaned up. If the worker is restarted,
 and tries to recreate the node during start-up the node will already
 exist, so it should delete existing nodes on start-up. Re:
 http://developers.blog.box.com/2012/04/10/a-gotcha-when-using-zookeeper-ephemeral-nodes/
 (URL courtesy of Siddharth)

#3 During a leader elections, I see all the zk sessions dying, and
 curator code entering into a reconnecting phase. (This is true for
 zookeeper 3.4 and 3.3, maybe not 3.5). Since ephemeral nodes have a
 lifetime tied to zk sessions, all the ephemeral nodes will dissapear
 during zk election. We don't want the whole storm cluster to die
 during zk elections, so the nimbus algorithm must do more than just
 kill workers when the ephemeral nodes dissapear. Instead you want a
 different algorithm, something that waits for ephemeral nodes to have
 dissapeared for a certain length of time.

#5 Strictly speaking this solution is not horizontally scalable, it
 scales with the number of nodes in the zookeeper cluster, but the
 quorum op rate decreases when you scale up a zookeeper cluster. Not
 sure what the upper bound is here and if this actually gets us to
 100k or just 10k.

#6 What code does the ping to keep the zookeeper session alive? Is
 that zookeeper client code? because it should be ran in a high
 priority thread. I've seen storm workers die from executor not alive
 just because they have CPU load spikes and the heartbeat doesn't get
 in, once I raised the priority of this thread it solved the
 problem. So the ping thread should do the same.

Anyone have experience with using ephemeral nodes as a health status
metric, not merely for service discovery?

Overall all these complexities seem manageable.

SOLUTION #2

You actually don't need state persistence, or quorum to do heartbeats,
which is what zookeper is adding in causing complexity. Heartbeats
could just be sent to a series of heartbeat-daemons that have no
persistent storage using random distribution. Nimbus would then query
the heartbeat-daemons to understand the state of the cluster. This is
horizontally scalable, could scale to 100k. We would use zookeeper to
do discovery of the heartbeat-daemons. This would substantially reduce
the load on zookeeper by several orders of magnitude.

This is similar to how mesos does heartbeats. Mesos actually has the
'master' receive heartbeats from all the slaves. They've scaled this
to a couple thousand slaves, and are looking at partioning the
heartbeats across the non-elected masters as well as the master to
scale this up further. So that's another idea, to use all the nimbus
masters to handle this. Which is nice because you don't have to add
yet another daemon to the storm ecosystem.

SOLUTION #3

You can also decrease the heartbeat frequency from 1 second, to 10
seconds. Presumbly this will let you scale 10x then. Any draw backs to
this? I know the nimbus UI stats will update less frequently, it'll
take slightly longer to find dead workers, is that it? It doesn't get
us to 100x scale, which potentially the first 2 solutions could.

---------- rgs1: wrt to:

"" Ephemeral nodes are tied to zk session, so if you kill -9 a worker,
from my understanding there will be some latency in when the ephemeral
node is actually cleaned up. If the worker is restarted, and tries to
recreate the node during start-up the node will already exist, so it
should delete existing nodes on start-up.""

this is trivially cheap on most cases, where the session will be gone
for good, since you would just call exists() before the delete() which
is not a quorom operation (but served from memory on whatever host you
are connected to).

---------- d2r: +1

storm also uses much larger zknode payload approaching 1mb in some
cases.  #604 : I think this might happen depending on the number of
workers & parallelism of the topology itself, not the number of
workers in the cluster.

---------- revans2: We ran into similar scaling issues too. We have
not done anything permanent yet, except reconfigure the nodes that ZK
is running on to be able to handle more IOPS, which seems to be the
primary limiting factor for us. Although we are not at the same scale
that you are yet so the number of network connections to ZK may start
to be more of an issue soon.

I did some research into the size of the data sent to ZK by our
instance of storm. I found that the data is rather compressible and
with a simple GzipOutputStream I could reduce the size to 1/5th the
original at the cost of some CPU. The 1MB limit that ZK has is also
configurable -Djute.maxbuffer=<NUM_BYTES>, although it has to be
configured for all the ZK nodes, the supervisors and all the workers
too because it is a limit when reading jute encoded data (what ZK uses
for serialization).

Solution 3 is how Hadoop, Mesos, and many other large systems
work. But it requires a new solution to persist state and will need a
lot of work to be resistant to multiple failures, which are things
that ZK gives you for free. Most of these large systems that heartbeat
to a central server either have no state recovery on failure, or if
they do it was added in very recently. Because the zk layer is
abstracted away from the code that reads/persists the state we could
make it pluggable and play around with other key/value stores. We
could still leave ZK for things that need it like leader election.

Another thing we thought about doing was creating a RAM disk and
configuring ZK to write edit logs to the RAM disk, along with very
aggressive rolling and cleanup of those logs. This would only work if
IOPS is really the bottleneck for you and not something else in ZK
like lock contention or CPU.

---------- rgs1: One detail of implementing heartbeats by ephemeral
znodes (so the actual heart-beating is delegate to ZooKeeper pings
which are local to each cluster node - not a quorum operation - hence
very cheap) is how to deal with the transient nodes of ephemeral
znodes. Because of network hiccups (and clients issues) ephemeral
nodes can come and go. So this means workers could be killed/started
when flaps happen. We could mitigate this (pretty much entirely) by
not reacting immediately to ephemeral nodes appearing/disappearing but
keeping a local tab of the available nodes and only remove them when
they've been gone for a while (and potentially only consider them
alive when they've been around for a while). This shouldn't be
expensive to implement and in terms of Zk it means polling via
continous getChildren() calls which are cheap and local (to a cluster
node).

---------- d2r: Regarding the discussion on storm-user, we'd like to
secure heartbeats such that workers of one topology cannot access
those of another topology. So what follows is the requirements we have
and how we're approaching it. Maybe this could be valuable to keep in
mind as we're discussing the new design, so that the design will not
make it unnecessarily difficult to add confidentiality guarantees to
heartbeats later on.

I think it is sufficient to guarantee the following:

storm cluster can do anything it wants topology workers can only read
heartbeats of workers in that same topology topology workers can only
write to or delete heartbeats of workers in that same topology.  We're
trying out ZooKeeper ACLs with MD5-digest authentication in the
near-term. The only tricky thing really is getting around the fact
that child znodes do not inherit from parents, and so they must be
explicitly set on each znode.

This complication arises in two cases with the current implementation:

when nimbus cleans up worker heartbeats in ZK and must be authorized
to remove those nodes and any children the worker may no longer clean
up its own heartbeats, as allowing DELETE on the parent node (created
by nimbus) would also allow any other worker to delete the workers
heartbeats We have the worker set an ACL with two entries, one to
allow the worker itself full access, and one that uses the digest of
the cluster's credentials to allow full access to the cluster's user.
"
STORM-152,Permissions of resources are not preserved,"https://github.com/nathanmarz/storm/issues/229

When the resources are copied from the resources directory to something like /tmp/09c4e529-7e64-496e-8afa-4990b7aba206/supervisor/stormdist/word-count-1-1337616827/resources the permissions of the files are not preserved, but rather all set to 644. This causes a problem because then we cannot execute binary files as bolts and spouts.

----------
tobigue: I can second this issue. The workaround I ended up with for now is creating a shell script like this in the same folder:

chmod u+x ./relative/path/to/mybinary
./relative/path/to/mybinary
and calling

super(""sh"", ""myscript.sh"")
in the ShellSpout/ShellBolt instead of calling the binary directly.

----------
analogue: Tobigue,

I don't see how your workaround actually works because myscript.sh will not be executable after after being unzipped and will fail to run.

My workaround is as follows:

For a given spout or bolt:

    public static class MySpout extends ShellSpout implements IRichSpout {

        public MySpout() {
            super(""bash"", ""-c"", ""chmod +x ./pystorm;./pystorm my_spout.py arg1 arg2 argN"");
        }
And a shell script multilang/resources/pystorm:

#!/bin/sh
PYTHONPATH=`pwd` python $*
This also handles not being able to set environment variables :-)

I'm thinking that ShellSpout should have an additional constructor which takes a ProcessBuilder to provide maximum flexibility. There are downsides to this though. The author of the spout or bolt will then have access to the ProcessBuilder and could do something to screw things up like inadvertently reading/writing to the InputStream or OutputStream in an effort to debug things.

----------
tobigue: Hi!
I think for me it worked, because I had the permission to execute sh.

So this works:

sh myscript.sh
while this does not

./myscript.sh
as there are no execute permissions on the file itself.

----------
analogue: Ok, I see my problem. bash -c ./myscript.sh fails if the script is not executable but bash ./myscript.sh doesn't.

----------
alien2150: I am having the same problem with a C++ programm. The original permissions are:

ls -l target/classes/resources/xy
-rwxr-xr-x 1 tech tech 1080184 Jul 19 17:26 target/classes/resources/xy

But later it is (ls -l /tmp/448ce6ba-ccd2-4335-a119-903381aee09c/supervisor/stormdist/foo-1374248239/resources):
-rw-rw-rw- 1 tech tech 1080184 Jul 19 17:37 xy

Is there any solution for this?

----------
revans2: jar files are really just zip files and do not store any kind of permissions. There are some extensions to zip that do store permissions but jar does not use those. Because these resources are pushed to the nodes through a jar the permissions are being lost. Hadoop gets around this by doing the really ugly thing of changing all of the permissions to be user executable after unzipping the file. We could try something like this in the supervisor when it unzips the files but I would rather provide a different way of distributing files beyond a single jar file once the bit-torrent code is in place.

----------
patricklucas: We can reproduce this consistently by launching a topology to a ""fresh"" cluster. It appears that if the storm-local directory has to be created, unpacked files will not have the right permissions. If it is a subsequent run, and the storm-local directory already exists, permissions of unpacked files are correct.

Will this be fixed in 0.9.0?
"
STORM-151,Support multilang in Trident,"https://github.com/nathanmarz/storm/issues/353

Since it's impractical to have a separate subprocess for every operation packed into a bolt, I think the best tradeoff is to allow multilang operations that will just be their own bolt. This is a low level exposure of the API and the multilang program will have to make sure to emit the batch id as the first field. Multilang operations would be treated similar to aggregators in that the tuples they produce are brand new with brand new fields.

----------
colinsurprenant: I am trying to wrap my head around this issue. Would it make sense to expose classes like ShellFunction, ShellAggregator, ShellFilter, etc?

A topology definition could be something like:

topology.newDRPCStream(""words"")
      .each(new Fields(""args""), new ShellFunction(""ruby"", ""split.rb""), new Fields(""word""))
      .groupBy(new Fields(""word""))
      .stateQuery(wordCounts, new Fields(""word""), new MapGet(), new Fields(""count""))
      .each(new Fields(""count""), new ShellFilter(""ruby"", ""somefilter.rb""))
      .aggregate(new Fields(""count""), new ShellCombinerAggregator(""ruby"", ""someaggregator.rb""), new Fields(""sum""));

Then the builder would assign a new shell bolt for each of these shell operations.

----------
nathanmarz: That would be one way to go about it – though that will have a lot of overhead with serializing back and forth between Java-land and multilang-land. This is especially true if you have many operations running in the same bolt (like 5 eaches in a row). Another way to go about it would be to expose a lower level facility, where the shell process is a regular bolt and has full control over the input and output tuples. Whatever fields it emits will be the fields available in the output tuples. Additionally, one of Trident's constraints is that the first field of output tuples contain the ""batch id"".

----------
colinsurprenant:Right. My proposition is basically what you described as ""impractical to have a separate subprocess for every operation"" in the issue description.

So what you are suggesting is to just expose a ShellBolt, which would basically respect the same semantic as the general Aggregator operation in that it can emit any number of tuples with any number of fields in regard to to input batch.

----------
joshbronson: I'm probably missing something. But would exposing an interface for an aggregator and using it carefully to avoid spawning too many processes accomplish something similar, without having to worry about batch ids?

We've run up against this problem recently and come up with a couple of ways to get around the many-processes issue. We've considered spawning a server that would listen on a standard TCP port range or Unix sockets. Is there a place to hook in, other than the prepare method, to launch such a beast? Other functions would presumably use their prepare method to discover and connect to the server, and it would be nice to avoid races or locks.

Also, we're working on an adapter for ""BaseFunction"" at the moment, though it's not quite ready for public use, and I wonder if anything will go wrong if a function continues to emit tuples to its collector after its execute method has returned. We're operating under the assumption that something will, so we've required the shell command the tell me when it's done responding to the input we just gave it. Currently we do that by requiring it to emit a JSON array. If we go with the server approach, we may also require the script to handle a tab-delimited ID telling the server to delegate to the appropriate handler for a function. Presumably this would require a thin library in languages used to script storm. There seems to be an appetite here for open-sourcing the components required to do this in Ruby; we definitely want to make this easy for folks.

----------
quintona: Hi Guys, I am needing the multilang support. From what I gather, the idea would be to hook the shell bolt into the trident topology, and have it ignore and handle the batch id field?

If that understanding is correct, then it raises the interesting question around support for any bolt within a Trident topology.

----------
nathanmarz: Well, the right way to go is to essentially expose the Aggregator interface via multilang. This is general enough to encompass functions, filters, and aggregation. We could also allow users to mark multilang aggregators as committers so that they can achieve exactly-once semantics when interacting with external state.

----------
quintona: I wasn't intending to allow the updating of external state via a multilang function. Surely that is a multilang state discussion, which is fundamentally different? Or am I missing something?
"
STORM-150,Replace jar distribution strategy with bittorent,"https://github.com/nathanmarz/storm/issues/435

Consider using http://turn.github.com/ttorrent/

----------
ptgoetz: I've been looking into implementing this, but have a design question that boils down to this:

Should the client (storm jar command) be the initial seeder, or should we wait and let nimbus do the seeding?

The benefit of doing the seeding client-side is that we would only have to transfer a small .torrent through the nimbus thrift API. But I can imagine situations where the network environment would prevent BitTorrent clients from connecting back to the machine that's submitting the topology. This would create an indefinitely ""stalled submission"" since none of the cluster nodes would be able to connect to the seeder.

The alternative would be to use the current technique of uploading the jar to nimbus, and have nimbus generate and distribute the .torrent file, and provide the initial seed. If the cluster is properly configured, we're pretty much guaranteed connectivity between nimbus and supervisor nodes.

I'm leaning toward the latter approach, but would be interested in others' opinions.

----------
nathanmarz: @ptgoetz I think Nimbus should do the seeding. That ensures that when the client finishes submitting, it can disconnect/go away without having to worry about making the topology unlaunchable.


----------
jasonjckn: @nathanmarz How does this solve the nimbuses dependency on reliable local disk state (as you talked about in person)?

What happens when zookeeper is offline for 1 hour? All the workers will die, and nimbus will be continually restarting. The onus is still on nimbus to store topology jars on local disk, so that when the workers and supervisors reboots it can seed all this again.

You -can- solve the local disk persistence problem with replicated state to the non-elected nimbuses, but that's orthogonal to a distribution strategy. Yes there is some replication going on in bittorrent, but it's not really a protocol that delivers reliable persistence of state.

I think it's still a good feature if it gives us performant topology submit times even with 500 workers, which take 3 minutes for us.

Particularly with the worker heartbeat start-up timeout of 120s, you want to be able to start 500 workers within 120s, or even 1500 workers within 120s, the current distribution strategy is not scalable in that way.

----------
nathanmarz: @jasonjckn On top of the bittorrent stuff we can ensure that a topology is considered submitted only when the artifacts exist on at least N nodes. Nimbus would only be the initial seed for topology files. Also, it wouldn't have to only be Nimbus that acts as a seed, that work could be shared by the supervisors. That's less relevant in the storm-mesos world, but you could still fairly easily run multiple Nimbus's to get replication.

----------
jasonjckn: This PR might be aided by ""topology packages"" #557, as it bundles all the state that needs to be replicated."
STORM-149,"""storm jar"" doesn't work on Windows","https://github.com/nathanmarz/storm/issues/19

Looks like there's problems with the STORM_JAR environment variable and various issues with paths.

Relevant thread:
http://groups.google.com/group/storm-user/browse_thread/thread/da5eeb6f348e2a5b

0.6.1 uses a java property to pass in the storm jar rather than an env variable

----------
ChitturiPadma: Hi Nathan,
I want to setup storm cluster on windows. Are there any pre-requisites to run this on windows ? are the same zip files that we run on Linux machines could be used to run on Windows?

----------
davidlao2k: Hi Chitturi
You can run storm on Windows. At the moment it requires merging pull request 722 into 0.9.0 to create usable jar. In addition to java and python, you will need the Windows version of zmq binaries as well as unzip.exe. Cygwin is not required. I'll be posting detailed build and setup instructions at https://github.com/davidlao2k/storm-windows in the next couple of days.

----------
jmlogan: You could try switching to Netty to avoid the Windows-zmq dependency?

----------
ChitturiPadma: Hi David,

Any update on how the storm stuff works on Windows ? I have build 0mq, zmq dependencies on windows. Even the zookeeper is running on master node. My major concern is storm 0.8.2 version contains .sh files which actually work on linux environment. How to make these work on windows environment. What modifications are required?

Hi David,
I downloaded the zip file (storm-windows-master.zip) from https://github.com/davidlao2k/storm-windows. how to proceed with running .cmd files in bin directory. I have modified the storm.yaml file il conf. Are any changes required for .cmd file to run on windows machine. When i execute storm.cmd and storm-config.cmd, its throwing error that STORM_HOME variable is not set to the absolute path of the directory that contains sand storm distribution. Can u please suggest me how to proceed on this."
STORM-148,Track Nimbus actions in UI,"https://github.com/nathanmarz/storm/issues/77

1. Worker reassignment history
2. Task timeout history

----------
danehammer: I feel like the logical next step would be to click on a supervisor from the main page, and get details about that supervisor node's going-ons. Workers running, their uptime, and the history you mention. Could even go one step further in, click on a worker, and see the executors/tasks running on that worker.

----------
cnardi: It would be really nice. Sometimes a worker is not behaving as expected (memory or cpu problems) and its important to know what it's being executed there. The only way so far is to go through all the bolts/spouts and see where it's being executed.

----------
danehammer: I've started familiarizing myself with what would be required to implement this. It feels like the part I'm thinking about, having the workers for every supervisor known, would require changes to the thrift API. I currently have no way of identifying an individual worker. I can get a supervisor, it can tell me the number of workers it has and how many are used, and executors know their host and port, but it feels like there should be a worker object between these two. A supervisor has a set of workers, and an executor lives on a worker. The worker has an uptime, port, host, id, as well as an understanding of its executors.

Sound right?

----------
nathanmarz: A worker is identified by its [supervisor id, port]. The uptime for a worker is the same as all its executors.

It would be useful to have a new Thrift method that gets the list of all workers in the cluster, including information such as:

Supervisor id and port
Host it's running on
Executors running in the worker
Once you have that method, you can easily implement supervisor pages. I think you should leave uptime out for now as that would require fetching the executor heartbeats, which is a very large amount of Zookeeper calls.

----------
danehammer: I would love to see from the UI if a worker's uptime is abnormal. Today if I hit the storm UI and a supervisor has recently gone down, it stands out immediately - its uptime is way lower than the other supervisors. I would imagine the same sort of ""one of these does not belong"" would be easily recognizable on a supervisor page.

The uptime for a worker is the same as all its executors
Would looking up one of these executor's heartbeats be a valid test of the worker's uptime? I take it this means the executor's heartbeats are what tell the supervisor the worker is up, and that the worker does not have its own heartbeat.

----------
nathanmarz: Well, all the executor heartbeats are kept in worker heartbeats. Fetching all the worker heartbeats for every topology is just going to be too expensive.

We can solve the heartbeat problem in the future by having the supervisor keep the uptime stats (from its perspective) in the supervisor heartbeat.
"
STORM-147,UI should use metrics framework,"https://github.com/nathanmarz/storm/issues/612

If I understand correctly, the stats framework is deprecated in favor of the metrics framework. However, the UI currently relies on the older stats framework, and so there are duplicated calls to the stats code all through the critical loops, and several interesting numbers gathered only by the metrics framework (heap info, etc) absent from the UI.

A CompileAndZookeepMetrics consumer could listen on the metrics stream, assemble data objects that look the same as what the stats framework produces, and serialize them into zookeeper. That lets us remove the stats tracking calls from the executor and makes it easier to add new metrics to the UI, yet doesn't require changes to the underlying UI code. Also, anyone else could use zookeeper or thrift to retrieve that synthesized view of the cluster metrics.

My thought is to have one metrics compiler per executor and one per worker. Each compiler would maintain a composite object and update it as new metrics roll in. As a new value for say the emitted count is received, it updates that field in-place, leaving all other last-known values. The compiler would clear out its data object on cleanup().

In the current implementation, the workerbeat has information about the worker and all stats rolled up into a single object. We can have the response of the current get*Info() thrift calls stay the same, but there would be an increase in number of zookeeper calls to build it.

If this is a welcome feature, I believe @arrawatia is excited to implement it.

Data objects stored in ZK: one per worker and one per executor? one per worker? or one per compiled metric?
What tempo should the compiler push its compiled view to Zookeeper: on each metrics update, or on a heartbeat?
(This may be a relative of #527)

----------
nathanmarz: Yes, I would like to see this work done. I think the best would be:

One Zookeeper metrics consumer per worker
All metrics stats get routed to local Zookeeper metrics consumer (should make an explicit localGrouping for this that errors if that executor is not there)
That metrics consumer updates a single node in ZK representing stats for that worker, the same way it works now.
It should update ZK after it receives N updates, where N is the number of executors in that worker. That will keep the tempo at approximately the same rate as metrics are emitted.

----------
mrflip: possible:

make a MetricsZkSummarizer that populates a thrift-generated object with metrics and serializes them back to zookeeper
make a subclass SystemZkSummarizer for the specific purpose here
it sends updates on a tempo of one report per producer
make the UI work with new object
beautiful:

Add fields for other interesting numbers to the worker and executor summaries, such as GC and disruptor queues
display those interesting numbers on UI
fast:

make a localGrouping, just like the localOrShuffleGrouping, but which errors rather than doing a shuffle
In--- system-topology! (common.clj), add an add-system-zk-summarizer! method to attach the SystemZkSummarizer
currently, the metrics consumers are attached always with the :shuffle grouping (metrics-consumer-bolt-specs). Modify this to get the grouping from the MetricsConsumer instead.
btw -- would the default grouping of a MetricsConsumer be better of as :local-or-shuffle, not :shuffle? There doesn't seem to be a reason to deport metrics if a consumer bolt is handy.

----------
nathanmarz: Since metrics are written per worker, Storm should actually guarantee that there's one ZK metrics executor per worker. And the reason it should be :local instead of :local-or-shuffle is because of that guarantee – if that executor isn't local then there's a serious problem and there should be an error. The ZK metrics executor should be spawned like the SystemBolt in order to get the one per worker guarantee, and ensure that if the number of workers changes the number of ZKMetrics executors change appropriately.

----------
mrflip: Understood -- my question at the end regarded other MetricsConsumers, not this special one: right now JoeBobMetricConsumer gets shuffle grouping, but I was wondering if it should get local-or-shuffle instead. 
The SystemZkSummarizer must be local-or-die, and created specially at the same lifecycle as the system bolt.

----------
nathanmarz: Ah, well we should make the type of grouping configurable. fieldsGrouping on executor id is probably the most logical default.

----------
mrflip: (Addresses #27 )
"
STORM-145,worker and supervisor heartbeat to nimbus using socket instead of write zookeeper node,"https://github.com/nathanmarz/storm/issues/732

when a storm cluster manager over thousands excutors， zookeeper Under great pressure and Very slow to respond write and read, nimbus Judge excutor timeover and reassigment excutors ， all cluster into a dead loop. 
if worker and supervisor heartbeat to nimbus using socket will resove this problem

----------
d2r: Yes, ZK has trouble keeping up sometimes. See #620.

Hopefully #706 would help. We have well over 1k workers on a number of storm clusters with this patch, and we no longer see this mass-reassignment happening.

----------
revans2: We also had to tune the FileSystem ZK uses to not be as safe in power failures as it otherwise would be. On ext4 we turned off the barrier my remounting the disk with -o nobarrier. You also want to make sure that your disk's cache is enabled. Unless you have a high end RAID controller with a battery backed cache most admins disable the disk cache on DB boxes to be sure that data is not lost in the case of a power outage. For us the added performance is worth the risk of data loss.

----------
xiaokang: Storm daemons(nimbus, supervisor, worker, etc.) are designed to be stateless so all state including heartbeats are stored in ZK. Workers will continue work on nimbus failure. If supervisor and workers heartbeat directly to nimbus, it may be hard to keep this nice feature.

----------
viceyang: @xiaokang socket heartbeat not break stateless feature。if worker （supervisor）heartbeart to nimbus failure,worker catch the exception and going on work.when nimbus restart heartbeart will success。

ps： nimbus ha is aonther feature, I am also working on it now.

@d2r seems #706 work well，but using zk do heartbeart seems not nessary, heartbeart and stats infomation only a Snapshot not need to store， socket heartbeart and stats information store in nimbus memory meet needs。 on aonther way our cluster has exceed 100，000 executors，5 node zk can‘t work well.

ps: zk load too high key reason is excutors stats information too large when executors too much. store heartbeat and stats information in nimbus' memory is a good way.

----------
d2r: @d2r seems #706 work well，but using zk do heartbeart seems not nessary,
@vinceyang Yes, I agree: There is discussion about this already --> #620.
"
STORM-144,Provide visibiilty into buffers between components,"https://github.com/nathanmarz/storm/issues/222

It would be nice to see how many tuples are in the input and output buffers of Storm components to understand where things are getting bottled up. 0mq doesn't currently provide this visibility so it's not clear how to implement this.

----------
nahap: maybe now that you have internal message buffers in storm 0.8 you could use these as an indicator. it is not perfect but better than nothing

----------
dkincaid: Based on my understanding of how messages get moved between bolts I think there are two places that they can get ""stuck"" in a queue. The first is in the inbound and outbound worker message queues. Prior to 0.8.0 those were unbounded LinkedBlockingQueue's. Version 0.8.0 changed to use LMAX Disruptor queues which are bounded. At this point then we should focus on the Disruptor queues.

The second place messages can get ""stuck"" is in the ZeroMQ sockets that are used to send messages between machines.

It seems to me that the first thing to do here would be to provide visibility into the size of the Disruptor queues in some manner.

Next, we should look for a way to provide some visibility into the queuing of messages within ZeroMQ. I'm far from an expert on ZeroMQ, but from looking at the documentation for the zmq_getsockopt call it looks promising:

ZMQ_BACKLOG: Retrieve maximum length of the queue of outstanding connections
The ZMQ_BACKLOG option shall retrieve the maximum length of the queue of outstanding peer connections for the specified socket; this only applies to connection-oriented transports. For details refer to your operating system documentation for the listen function.
Maybe that won't show the actual number of messages waiting in the queue, but should still be an indication of a backup.

Since the rest of the stats for workers, bolts, etc are sent to Zookeeper does it make sense to send a snapshot count of these queues at the same time? Personally I'd like to be able to see average size over the time period as well as max and min, but then we'd be starting to throw more data into Zookeeper which Nathan has been trying to prune.

-----------
sustrik: ZMQ_BACKLOG is listen function's 'backlog' parameter and has nothing to do with queued messages.

Btw, even without queueing on ZeroMQ layer there's still queueing going on on the lower layers (TCP) which is kind of hard to assess. The only reasonable solution, AFAICS, is to hard-limit the buffers (on all layers) and consider the max buffered amount of messages to be the error of the queue depth measurement.

Say, if you are using raw TCP and it's possible to buffer 100 messages in TCP's tx and rx buffers, you can measure the number of outstanding messages buffered in the application and report (N, N+100) interval as the queue depth.

The problem gets more complex when there are many TCP connections involved. If there are 1000 connections the (N, N+100) interval expands to (N, N+100,000).

----------
mrflip: Fixed by #633 ?"
STORM-142,Errors during deserialization of spouts/bolts should go to the Storm UI,"https://github.com/nathanmarz/storm/issues/358

Can just do this in mk-task-data by wrapping the call to get-task-object and calling (:report-error-and-die executor-data) with the error"
STORM-141,Provide support for custom stats in Storm UI,"https://github.com/nathanmarz/storm/issues/27

Something along the lines of Hadoop counters

----------
gsilk: Would this be implemented as an implicit stream? It would be great to have global and per-task aggregations in addition to having per-bolt counters.

----------
nathanmarz: gsilk: Well, besides the aggregation mechanism we need to figure out how the stats are sent back to nimbus, stored, etc. Not sure the current approach of putting the stats in ZK is a good one. Using implicit streams and an aggregation bolt to sum things together, and then having the aggregation bolt connect to nimbus and send updates might be a better design.

----------
ehudl: how about using emit to a specific stream (not default) as a counter, do you think this is a good approach ? 
could this approach cause a memory leak? if no one is listening to this stream or in case you use a large amount of streams?"
STORM-140,Provide unique working directory to each task,"https://github.com/nathanmarz/storm/issues/111

The directory should be ephemeral in that it's automatically cleaned up when the worker dies or is killed. The directory should be provided via the TopologyContext. This feature request originated from the desire to use an http listener library which writes its logs to a file.

This could eventually be used to hold the state coming from a state spout as well."
STORM-137,"add new feature: ""topology package""","https://github.com/nathanmarz/storm/issues/557

Submitting a topology to storm requires executing code which constructs bolts, spouts, a topology and then calling StormSubmitter.submitTopology and uploads the jar, config, serialized objects to Nimbus.

If you want to have a topology binary store, so that you can retain all versions of production topologies, there's this really precarious property of stormSubmitter where the serialized object state needs to be recomputed, and the assumption is the code used to create serialized object state is a pure function, and has no external dependencies. If either of those properties are not true, for example maybe your API key is queried at this point in time then merely storing the jar is not sufficient to redeploy an older version of a topology (i'm making this up, but one user was trying to access ZK here, don't know why, it's just very precarious to recompute state each time.)

So my proposal is adding to StormSubmitter and storm command line tool this ability to create ""topology package"" which contain inside the jar, object serialized state, and topology config (preferably in yaml). And then another StormSubmitter API for accepting topology packages.

---------
jasonjckn: @nathanmarz could you comment on this asap?

---------
nathanmarz: It would be interesting to have syntax like:

storm package {name of output file} {jar} {class} {args}

which changes the behavior of StormSubmitter#submitTopology to serialize the topology and package it with the jar into a ""package"" file. It would also be cool if storm deploy would automatically detect these package files and do the right thing with them.

---------
jasonjckn: So will user code still call StormSubmitter.submitTopology? or should they call .createPackage?

Right now people can call StormSubmitter.submitTopology as many times as they want in the main function and submit multiple topologies.

storm package {name of output file} {jar} {class} {args}
What happens if they call submitTopology twice, is the same outfile filename is used twice? That's why i'm recommending we do this:

package mytopologpkg;
class MyTopology {
public static main(String[] args) {
Topology topology = TopologyBuilder.setSpout(...).buildTopology();
TopologyCommandLine.processAction(topology, args);
}
}
Then the user would execute commands like this:


java -cp topology.jar mytopologpkg.MyTopology make_package <filename>
java -cp topology.jar mytopologpkg.MyTopology submit -c nimbus.host=xyz  <name>

java -cp topology.jar mytopologpkg.MyTopology kill <name>
java -cp topology.jar mytopologpkg.MyTopology submit_package -c nimbus.host=xyz <filename>
This is also valid:

java -cp topology.jar backtype.storm.CommandLine kill <name>
java -cp topology.jar backtype.storm.CommandLine submit_package -c nimbus.host=xyz <filename>
Users you have this complexity of needing to ensure the storm zip release you downloaded matches the library version you compiled the code with. Everything you need is actually in the storm library jar.

My last idea has sit in my head for a while, It has a couple problems and not a good idea.

Something like this would work:

package mytopologpkg;
class MyTopology implements storm.ITopologyPackage {
     @override
     Map getTopologyConf(String[] commandLineArgs) {
           ......
     }
     @override
     StormTopology getTopology(String[] commandLineArgs) {
           .....
     }
}
$ storm make-package-file mytopology.MyTopology package-filename.zip [commandLineArg]
$ storm submit package-filename.zip -c nimbus.host=xyz -c topology.name=my-topology-name

OR

$ storm submit mytopology.MyTopology -c nimbus.host=xyz -c topology.name=my-topology-name  [commandLineArgs]
NOTE: ""-c"" means override the conf returned from getTopologyConf.

NOTE: there's no main function anymore, however if someone does write a main function, they can run it with ""storm jar"" semantically i think of this as ""exec-jar-main with the ability to pass topology conf overrides"" and semantically says nothing about how many topologies are submitted or if StormSubmitter.makePackage(filename, topology, stormConf) is called.

static void main(String[] args) {
      t1 = buildTopologyT1();
      .... StormSubmitter.submitTopology(t1)
      t2 = buildTopologyT2();
      .... StormSubmitter.submitTopology(t2)
} 


I passed this issue to another engineer who was working on package versioning and they didn't want to implement this, so it never got done.

I just realized there is a tradeoff going on here, if you persist the jar AND serialized object state in a version binary store, then if the class serializationUID ever changes, that state is useless. However if you merely store the jar in the version binary store, and rerun the main method with $ storm jar path.to.mainClass, even if the class serializationUID changed, it doesn't matter. So you get more affordances on making backwards compatible changes if you don't store serialized object state in a topology package as proposed above.


An API for downloading a topology package would be useful for writing automated tools that move topologies between multiple storm clusters
"
STORM-136,[Trident] Bug when working with two spouts with stateful operator,"https://github.com/nathanmarz/storm/issues/711

Hi,

I'm working with Trident and basically what i am trying to do is a windowed join across batches using partitionPersist and stateQuery on two different stream that come from TWO DIFFERENT SPOUTS.

Both the spouts implement the IBatchSpout interface.

The error i get is a NPE on StateQueryProcessor or on PartitionPersistProcessor depending on which one of the two spouts starts early.

I try to debug this and what i have understand is that Trident use the same BatchID(txid) for the two different spouts and this take to a wrong initialization of state in the core processing nodes.

If i use the same throughput for the two spout and i make one spout starts with a delay the problem doesn't occur (we don't have an overlap between the BachID(txid) of the different spouts).

----------
liesrock: Sorry for the delay.
I send to you the source code.

https://dl.dropboxusercontent.com/u/49470846/TridentBug.tar.gz

In this example i am using two spout:
1) the first one starts emitting tuples at a certain rate.
2) the second one is delayed by 5 seconds and emits at an higher rate than the first one.

I print on the sterr some debug info, the most important is the Batch ID (txid).
I print on the stdout info about the state window.

You can see that this is exactly the case that i was explain in my first message.

Thank you for your attention, i can't figure out on this, so i left Trident and i start working with Storm(i didn't have problem in implementing this on Storm).

Let me know what is your opinion about it.

Luca Muratore

----------
xumingming: @liesrock try to minimize your test case so it can reproduce the issue but contains the least source files(preferably only one source file), the least lines of source code --- that makes us more easier to see whether the issue is in your app code or in storm core.

----------
liesrock: As you requested, in the following link you can find the test case in one source file and with the minimum lines of code:

https://dl.dropboxusercontent.com/u/49470846/SimpleTridentBug.zip

Thanks for your patience.

----------
xumingming: paste the case source code here to make it easier for others to follow:

package storm.starter.trident;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import storm.trident.Stream;
import storm.trident.TridentState;
import storm.trident.TridentTopology;
import storm.trident.operation.TridentCollector;
import storm.trident.spout.IBatchSpout;
import storm.trident.state.BaseQueryFunction;
import storm.trident.state.BaseStateUpdater;
import storm.trident.state.State;
import storm.trident.state.StateFactory;
import storm.trident.tuple.TridentTuple;
import backtype.storm.Config;
import backtype.storm.LocalCluster;
import backtype.storm.task.IMetricsContext;
import backtype.storm.task.TopologyContext;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Values;


public class BugTest {

    //Spout that simulates an ATM
    @SuppressWarnings(""serial"")
    public static class ATMSpout implements IBatchSpout {   
        private int batchSize;
        private int initialSleepMilliTime;
        private int rate;
        private String name;
        private List<String> currentCCIDList;
        private long withdrawalID = 0;
        private final static String[] LOCATIONS = { ""Madrid"", ""Barcelona"", ""Siviglia"", ""Granada"", ""Toledo"", ""Ibiza"", ""Valladolid"", ""Valencia"" };


        public ATMSpout(int batchSize, int initialSleepMilliTime, int rate, String name) throws IOException {
            this.batchSize = batchSize;
            this.initialSleepMilliTime = initialSleepMilliTime;
            this.rate = rate;
            this.name = name;
        }

        //generate a list of ""size"" CCID
        private List<String> generateCCID(int size){
            //check the input parameter
            if(size < 0){
                System.err.println(""Negative CCID list size"");
                return null;
            }
            //initialize some variables
            List<String> aux = new ArrayList<String>();
            Integer randDigit = 0;
            String currentCCID = """";
            //create a random CCID
            for(int i = 0; i < size; i++){
                for(int j = 0; j < 16; j++){
                    randDigit = (int)(Math.random() * 10);
                    currentCCID +=  randDigit;
                }
                aux.add(currentCCID);
                currentCCID = """";
            }
            return aux;
        }


        @SuppressWarnings(""rawtypes"")
        @Override
        public void open(Map conf, TopologyContext context) {
            System.err.println(""Open Spout instance"");
            this.currentCCIDList = generateCCID(10000);
            //initial delay
            try {
                Thread.sleep(initialSleepMilliTime);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }

        }

        @Override
        public void emitBatch(long batchId, TridentCollector collector) {
            System.err.println(name + "" ---> Starting emitting Batch number : "" + batchId);
            for(int i = 0; i < batchSize; i++) {
                try {
                    Thread.sleep(rate);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                collector.emit(getNextWithdrawal());
            }
        }

        @Override
        public void ack(long batchId) {
        }

        @Override
        public void close() {
        }

        @SuppressWarnings(""rawtypes"")
        @Override
        public Map getComponentConfiguration() {
            return new Config();
        }

        @Override
        public Fields getOutputFields() {
            return new Fields(""withdrawalID"", ""ccID"", ""location"", ""amount"", ""timestamp"");
        }

        private Values getNextWithdrawal(){
            int randIndexCCID = (int) ((Math.random()) * this.currentCCIDList.size());
            int randAmmount = (int) ((Math.random()) * 1000);
            int randIndexLocation = (int) ((Math.random()) * LOCATIONS.length);
            return new Values(++withdrawalID, 
                              this.currentCCIDList.get(randIndexCCID), 
                              LOCATIONS[randIndexLocation],
                              randAmmount,
                              System.currentTimeMillis());
        }

    }


    //state updater
    @SuppressWarnings(""serial"")
    private static class Updater extends BaseStateUpdater<DB>  {
        private String streamName;

        public Updater(String streamName) {
            this.streamName = streamName;
        }

        @Override
        public void updateState(DB state, List<TridentTuple> tuples, TridentCollector collector) {
            System.err.println(streamName + "" Update State"");
            }
    }

    //query
    @SuppressWarnings(""serial"")
    public static class Query extends BaseQueryFunction<DB, TridentTuple> {
        private String streamName;

        public Query(String streamName) {
            this.streamName = streamName;
        }

        @Override
        public List<TridentTuple> batchRetrieve(DB state, List<TridentTuple> inputs) {
            System.err.println(streamName + "" Query"");

            List<TridentTuple> retList = new ArrayList<TridentTuple>(); //return list
            for(int i = 0; i < 5; i++){
                retList.add(null);
            }
            return retList;
        }

        @Override
        public void execute(TridentTuple tuple, TridentTuple result, TridentCollector collector) {

        }

    }

    //state
    public static class DB implements State {

        @Override
        public void beginCommit(Long txid) {

        }

        @Override
        public void commit(Long txid) {

        }

    }

    //factory
    @SuppressWarnings(""serial"")
    public static class Factory implements StateFactory {

        @SuppressWarnings(""rawtypes"")
        @Override
        public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
            return new DB();
        }       
    }

    public static void main(String[] args) throws Exception {
        Config conf = new Config();
        LocalCluster cluster = new LocalCluster();

        final int BATCH_SIZE = 5;
        final int DELAY_1 = 100;
        final int RATE_1 = 300;
        final int DELAY_2 = 5000;
        final int RATE_2 = 100;

        ATMSpout spout1 = new ATMSpout(BATCH_SIZE, DELAY_1, RATE_1, ""SpoutLowRate"");
        ATMSpout spout2 = new ATMSpout(BATCH_SIZE, DELAY_2, RATE_2, ""SpoutHighRate"");   
        TridentTopology topology = new TridentTopology();

        Stream s1 = topology.newStream(""stream1"", spout1);
        Stream s2 = topology.newStream(""stream2"", spout2);

        TridentState leftState = s1.partitionPersist(new Factory(), 
                                                     s1.getOutputFields(),
                                                     new Updater(""left""));
        s2.stateQuery(leftState,
                      s1.getOutputFields(),
                      new Query(""right""),
                      new Fields(""out""));

        cluster.submitTopology(""BugTest"", conf, topology.build());

        Thread.sleep(20000);

        cluster.shutdown();
    }
}

----------
xumingming: stacktrace:

5320 [Thread-27-b-0] ERROR backtype.storm.util - Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:90) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:61) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.daemon.executor$fn__3496$fn__3508$fn__3555.invoke(executor.clj:730) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.util$async_loop$fn__442.invoke(util.clj:403) ~[storm-core-0.9.0-rc3.jar:na]
    at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
    at java.lang.Thread.run(Thread.java:680) [na:1.6.0_29]
Caused by: java.lang.NullPointerException: null
    at storm.trident.planner.processor.StateQueryProcessor.execute(StateQueryProcessor.java:69) ~[storm-core-0.9.0-rc3.jar:na]
    at storm.trident.planner.SubtopologyBolt$InitialReceiver.receive(SubtopologyBolt.java:194) ~[storm-core-0.9.0-rc3.jar:na]
    at storm.trident.planner.SubtopologyBolt.execute(SubtopologyBolt.java:130) ~[storm-core-0.9.0-rc3.jar:na]
    at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:355) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.daemon.executor$fn__3496$tuple_action_fn__3498.invoke(executor.clj:615) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.daemon.executor$mk_task_receiver$fn__3419.invoke(executor.clj:383) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.disruptor$clojure_handler$reify__2960.onEvent(disruptor.clj:43) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:87) ~[storm-core-0.9.0-rc3.jar:na]
    ... 6 common frames omitted
5320 [Thread-27-b-0] ERROR backtype.storm.daemon.executor -
java.lang.RuntimeException: java.lang.NullPointerException
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:90) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:61) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.daemon.executor$fn__3496$fn__3508$fn__3555.invoke(executor.clj:730) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.util$async_loop$fn__442.invoke(util.clj:403) ~[storm-core-0.9.0-rc3.jar:na]
    at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
    at java.lang.Thread.run(Thread.java:680) [na:1.6.0_29]
Caused by: java.lang.NullPointerException: null
    at storm.trident.planner.processor.StateQueryProcessor.execute(StateQueryProcessor.java:69) ~[storm-core-0.9.0-rc3.jar:na]
    at storm.trident.planner.SubtopologyBolt$InitialReceiver.receive(SubtopologyBolt.java:194) ~[storm-core-0.9.0-rc3.jar:na]
    at storm.trident.planner.SubtopologyBolt.execute(SubtopologyBolt.java:130) ~[storm-core-0.9.0-rc3.jar:na]
    at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:355) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.daemon.executor$fn__3496$tuple_action_fn__3498.invoke(executor.clj:615) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.daemon.executor$mk_task_receiver$fn__3419.invoke(executor.clj:383) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.disruptor$clojure_handler$reify__2960.onEvent(disruptor.clj:43) ~[storm-core-0.9.0-rc3.jar:na]
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:87) ~[storm-core-0.9.0-rc3.jar:na]
    ... 6 common frames omitted
5334 [Thread-27-b-0] INFO  backtype.storm.util - Halting process: (""Worker died"")

The NullPointerException is thrown because state is null:

https://github.com/nathanmarz/storm/blob/master/storm-core/src/jvm/storm/trident/planner/processor/StateQueryProcessor.java?source=cc#L57-L61

    @Override
    public void execute(ProcessorContext processorContext, String streamId, TridentTuple tuple) {
        BatchState state = (BatchState) processorContext.state[_context.getStateIndex()];
        state.tuples.add(tuple);
        state.args.add(_projection.create(tuple));
    }
Why state is null? state is initialized in startBatch method:
https://github.com/nathanmarz/storm/blob/master/storm-core/src/jvm/storm/trident/planner/processor/StateQueryProcessor.java?source=cc#L52-L54

    public void startBatch(ProcessorContext processorContext) {
        processorContext.state[_context.getStateIndex()] =  new BatchState();
    }
In this case, startBatch is not called before execute is called.

Why startBatch is not called? startBatch is called by initBatchState, initBatchState should be called by TridentBoltExecutor.execute:

        if(tracked==null) {
            tracked = new TrackedBatch(new BatchInfo(batchGroup, id, _bolt.initBatchState(batchGroup, id)), _coordConditions.get(batchGroup), id.getAttemptId());
            _batches.put(id.getId(), tracked);
        }
In the code above, initBatchState is called with a batchGroup arg, when there are multiple batchGroups in one batch, the TrackedBatch will only be created once, then the initBatchState will only be called for the first batchGroup, then startBatch method of processors in the rest batchGroups will not be called, then the NullPointerException, @nathanmarz, is this a bug or just not using Trident in the correct way?

----------
nathanmarz: Looks like a bug."
STORM-135,Tracer bullets,"https://github.com/nathanmarz/storm/issues/146

Debugging the flow of tuples through a Storm topology can be pretty tedious. One might have to do lots of logging and watch many log files, or do other kinds of instrumentation. It would be great to include a system to select certain tuples for tracing, and track the progress of those tuples through the topology.

Here is a use case:

Suppose one were to do stats aggregation using Storm. Some things I might want to ensure are:
Is the aggregation and flush happening in a timely way?
Are there hotspots?
Are there unexpected latencies? Are some bolts taking a long time?
To answer the above questions, I might select a random sample of tuples, or maybe a random sample of a specific subset of tuples. The tuples to be traced could be tagged with a special attribute.

I would want to track the following events:

Spout emit - send (task id, spout name, timestamp)
For each bolt:
When a traced tuple arrives and execute() is called: (task id, bolt name, timestamp)
When a tuple is emitted that is anchored on the tuple that arrived: (task id, bolt name, timestamp)
Here is what I can do with the data from above (assuming one can correlate tuples emitted with incoming tuples, based on the anchor):

For the aggregation bolt, I can look at the distribution of (emit timestamp - incoming timestamp) and see if it makes sense.
I can graph the life of one tuple, look at spout/bolt vs timestamp graph, and visually see how much time is being spent in each bolt, as well as how much time is spent in the Storm infrastructure / ZMQ.
This data can be overlayed for multiple tuples to get a sense of the timing distribution for the topology.
Using the task ID information, one can do a cool overlay graph that traces the distribution of a number of tuples over a topology. One can use that to see if field groupings are working, are unevenly distributed, etc.
For now I may start implementing this idea in scala-storm DSL.

----------
tdunning: I actually think that, if possible, unanchored tuples should also be traced.

One simple implementation would be to add some information to each tuple to indicate the tracing status of the tuple.

As each tuple arrives, the tracing status would be inspected. If set, a tracing wrapper for the collector would be used in place of the actual collector for that tuple. This would make tracing of all resulting tuples possible, not just the anchored tuples.

It would also be very useful to have a traceMessage() method on the collector that could be used by the bolt to record a trace message if tracing is on.

It would also make sense to have a method that turns tracing on or off for a collector. This might need to return a new tracing collector in order to allow collectors to be immutable.

The tracing information that I see would be useful includes:

a) possibly a trace level similar to the logging level used by log4j and other logging packages

b) a trace id so that multiple traces can be simultaneously active. This could be generated when tracing is turned on. It would be nice to have a provision to provide an external id that could be correlated to outside entities like a user-id.

----------
velvia: +1 for adding tracing level to the tuple metadata.

Nathan or others:

I think this ticket should be split up into a couple parts:
1) A generic callback or hook mechanism for when tuples are emitted and when they arrive via execute() in bolts.

2) A specific callback for filtering and implementing tracer bullets
3) Additional metadata in the Tuple class to track tracing, and changes to allow it to be serialized

Should this be split up into multiple issues?

Also pointers to where in the code the three could be implemented would be awesome.

Thanks!
Evan

----------
tdunning: With JIRA, sub-tasks would be a great idea. With Github's very basic issue tracker, probably not so much.

----------
nathanmarz: FYI, I've added hooks into Storm for 0.7.1

----------
tdunning: Can you provide a pointer or three to where the hooks are?

----------
nathanmarz: I explained it here: #153 (comment)

I'll have a wiki page about hooks once the feature is released.

----------
mrflip: @thedatachef has implemented this. We'd like guidance on the implementation choices made; you'll see the pull request shortly.

We targeted Trident, not Storm. It's our primary use case, and we want to see values at each operation boundary (not each bolter); meanwhile hooks seem to give good-enough support for Storm.
Trident Tuples have methods to set, unset and test if the tuple is traceable.
They become labeled as traceable with an assembly, which you can put in anywhere in the topology. We have have one such that makes every nth tuple traceable.
All descendants of a traceable tuple are traceable. The framework doesn't ever unlabel things, even if a tuple is prolific -- it's easy enough to thin the herd with an assembly.
When the collector emits a tuple, if the tuple is traceable it
anoints the new tuple as traceable
records the current step in the trace history -- a tracer bullet carries the history of every stage it's passed through.
writes an illustration of the trace history to the progress log. Since only a fraction of tuples are expected to be traceable, we feel efficiency is less important that this be structured, verbose and readable.
We don't do anything to preserve traceability across an aggregation, mostly because we don't know what to uniformly do in that case."
STORM-134,FileNotFoundException during supervisor cleanup due to non-existant file,"https://github.com/nathanmarz/storm/issues/356

During development runs with local clusters I've sometimes managed to encounter FileNotFoundException thrown from FileUtils.forceDelete during cleanup.

Without knowing the internals of Storm it feels a bit strange that clean up...
a) ...fails when trying to delete a non existing file/folder
b) ...tries to delete a non-existant file

Would suppressing FileNotFoundException from FileUtils.forceDelete or switching to FileUtils.deleteQuietly be reasonable?

Sample stack trace:

ERROR [2012-10-09 22:08:51,255] org.apache.zookeeper.server.NIOServerCnxn: Thread Thread[main,5,main] died
! java.io.FileNotFoundException: File does not exist: /var/folders/30/4f0z00f56sqfh512yr1l61ym0000gn/T/5844d75c-3801-4757-b7ea-f20ca76e4e52/workers/08a06e4f-4cbc-4e10-b013-ff4fcb4cbf51/heartbeats/1349813330619.version
! at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:1386)
! at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1044)
! at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:977)
! at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:1381)
! at backtype.storm.util$rmr.invoke(util.clj:413)
! at backtype.storm.daemon.supervisor$try_cleanup_worker.invoke(supervisor.clj:146)
! at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:165)

----------
MichaelBlume: Getting the same error while trying to test with complete-topology

Note: I'm using storm 0.8.1. Have reproduced on my mac and my ubuntu netbook. Can try with development version if desired.

Reproduced with development version. It's a really weird bug, because the code is actually checking for the existence of the path before attempting to delete it.

OK, I think I partially understand. I put some logging into rmr to log out a listing of the tree it's attempting to remove. I get this:

[clojure-test] 3501 [Thread-10] INFO backtype.storm.util - [""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990601844"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990601844.version"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990602801"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990602801.version""]
[clojure-test] 3518 [main] INFO backtype.storm.util - [""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats""]

Looks like backtype.storm.util.rmr's being called on the same dir from two threads. The second call happens when the directory's just about empty, but not quite gone, but by the time rmr calls forceDelete, the directory's gone entirely and an exception gets raised. I've thrown a band-aid on this on my fork, but the proper solution would be to figure out why two threads are deleting the same directory.

----------
nathanmarz: Please attach a reproducible test case that I can run so I can look into what's going on. I run local cluster's all the time (Storm's unit tests bring clusters up and down dozens of times) and have never seen this.

----------
chids: I've not been able to create a reliable test case but the condition does occur ""regularly"". Right now I'm running the testBasicTopology code and got bitten by this a couple of times in a row. Then it stops happening.

@MichaelBlume's have you tried to identify what Thread-10 is?

----------
travis: yea, I'm seeing this regularly as well, not quite sure what the pattern is yet

I got stack traces of the two threads trying to rmr:

11073 [main] ERROR backtype.storm.util  - Rmr path /var/folders/ll/0csh8smd025d9mgh2nf842400000gn/T//88915881-f92a-44be-93c1-434d9be6ac28/workers/bf2411b5-830a-4549-85e8-76637dcf352b/heartbeats
java.lang.Exception
    at backtype.storm.util$rmr$fn__4117.invoke(util.clj:411)
    at backtype.storm.util$rmr.invoke(util.clj:411)
    at backtype.storm.daemon.supervisor$try_cleanup_worker.invoke(supervisor.clj:146)
    at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:165)
    at backtype.storm.daemon.supervisor$fn__4733$exec_fn__1207__auto__$reify__4742.shutdown_all_workers(supervisor.clj:368)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:298)
    at backtype.storm.testing$kill_local_storm_cluster.invoke(testing.clj:154)
... (application specific stacktrack - let me know if it would be useful)
11073 [Thread-19] ERROR backtype.storm.util  - Rmr path /var/folders/ll/0csh8smd025d9mgh2nf842400000gn/T//88915881-f92a-44be-93c1-434d9be6ac28/workers/bf2411b5-830a-4549-85e8-76637dcf352b/heartbeats
java.lang.Exception
    at backtype.storm.util$rmr$fn__4117.invoke(util.clj:411)
    at backtype.storm.util$rmr.invoke(util.clj:411)
    at backtype.storm.daemon.supervisor$try_cleanup_worker.invoke(supervisor.clj:146)
    at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:165)
    at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:223)
    at clojure.lang.AFn.applyToHelper(AFn.java:161)
    at clojure.lang.AFn.applyTo(AFn.java:151)
    at clojure.core$apply.invoke(core.clj:603)
    at clojure.core$partial$fn__4070.doInvoke(core.clj:2343)
    at clojure.lang.RestFn.invoke(RestFn.java:397)
    at backtype.storm.event$event_manager$fn__2484.invoke(event.clj:24)
    at clojure.lang.AFn.run(AFn.java:24)
        at java.lang.Thread.run(Thread.java:680)
Still not quite sure what this all means, but getting closer. This happens pretty much every time I run tests on my mac and on our CI box.

Still no test case, but I did find a workaround. I was running the topology like:

(with-local-cluster [cluster]
    (complete-topology cluster
                       (my-topology drpc)
                       :mock-sources {""actions"" (map vector actions)
                                      ""drpc-requests"" requests}))
Sleeping for 5 seconds before shutting down the cluster seems to fix it (shorter periods just made the error sporadic rather than constant:

(with-local-cluster [cluster]
  (let [results
        (complete-topology cluster
                           (my-topology drpc)
                           :mock-sources {""actions"" (map vector actions)
                                          ""drpc-requests"" requests})]
    (Thread/sleep 5000)
    results))
"
STORM-133,FileNotFoundException during supervisor cleanup due to non-existant file,"https://github.com/nathanmarz/storm/issues/356

During development runs with local clusters I've sometimes managed to encounter FileNotFoundException thrown from FileUtils.forceDelete during cleanup.

Without knowing the internals of Storm it feels a bit strange that clean up...
a) ...fails when trying to delete a non existing file/folder
b) ...tries to delete a non-existant file

Would suppressing FileNotFoundException from FileUtils.forceDelete or switching to FileUtils.deleteQuietly be reasonable?

Sample stack trace:

ERROR [2012-10-09 22:08:51,255] org.apache.zookeeper.server.NIOServerCnxn: Thread Thread[main,5,main] died
! java.io.FileNotFoundException: File does not exist: /var/folders/30/4f0z00f56sqfh512yr1l61ym0000gn/T/5844d75c-3801-4757-b7ea-f20ca76e4e52/workers/08a06e4f-4cbc-4e10-b013-ff4fcb4cbf51/heartbeats/1349813330619.version
! at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:1386)
! at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1044)
! at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:977)
! at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:1381)
! at backtype.storm.util$rmr.invoke(util.clj:413)
! at backtype.storm.daemon.supervisor$try_cleanup_worker.invoke(supervisor.clj:146)
! at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:165)

----------
MichaelBlume: Getting the same error while trying to test with complete-topology

Note: I'm using storm 0.8.1. Have reproduced on my mac and my ubuntu netbook. Can try with development version if desired.

Reproduced with development version. It's a really weird bug, because the code is actually checking for the existence of the path before attempting to delete it.

OK, I think I partially understand. I put some logging into rmr to log out a listing of the tree it's attempting to remove. I get this:

[clojure-test] 3501 [Thread-10] INFO backtype.storm.util - [""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990601844"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990601844.version"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990602801"" ""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats/1349990602801.version""]
[clojure-test] 3518 [main] INFO backtype.storm.util - [""/private/var/folders/qz/qs47vb4938bbk3djt74vrxwr0000gn/T/dadf34c4-ad02-4728-9d91-8b3300bcc926/workers/94008c8d-593e-463d-8665-d58fbb682f0e/heartbeats""]

Looks like backtype.storm.util.rmr's being called on the same dir from two threads. The second call happens when the directory's just about empty, but not quite gone, but by the time rmr calls forceDelete, the directory's gone entirely and an exception gets raised. I've thrown a band-aid on this on my fork, but the proper solution would be to figure out why two threads are deleting the same directory.

----------
nathanmarz: Please attach a reproducible test case that I can run so I can look into what's going on. I run local cluster's all the time (Storm's unit tests bring clusters up and down dozens of times) and have never seen this.

----------
chids: I've not been able to create a reliable test case but the condition does occur ""regularly"". Right now I'm running the testBasicTopology code and got bitten by this a couple of times in a row. Then it stops happening.

@MichaelBlume's have you tried to identify what Thread-10 is?

----------
travis: yea, I'm seeing this regularly as well, not quite sure what the pattern is yet

I got stack traces of the two threads trying to rmr:

11073 [main] ERROR backtype.storm.util  - Rmr path /var/folders/ll/0csh8smd025d9mgh2nf842400000gn/T//88915881-f92a-44be-93c1-434d9be6ac28/workers/bf2411b5-830a-4549-85e8-76637dcf352b/heartbeats
java.lang.Exception
    at backtype.storm.util$rmr$fn__4117.invoke(util.clj:411)
    at backtype.storm.util$rmr.invoke(util.clj:411)
    at backtype.storm.daemon.supervisor$try_cleanup_worker.invoke(supervisor.clj:146)
    at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:165)
    at backtype.storm.daemon.supervisor$fn__4733$exec_fn__1207__auto__$reify__4742.shutdown_all_workers(supervisor.clj:368)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:298)
    at backtype.storm.testing$kill_local_storm_cluster.invoke(testing.clj:154)
... (application specific stacktrack - let me know if it would be useful)
11073 [Thread-19] ERROR backtype.storm.util  - Rmr path /var/folders/ll/0csh8smd025d9mgh2nf842400000gn/T//88915881-f92a-44be-93c1-434d9be6ac28/workers/bf2411b5-830a-4549-85e8-76637dcf352b/heartbeats
java.lang.Exception
    at backtype.storm.util$rmr$fn__4117.invoke(util.clj:411)
    at backtype.storm.util$rmr.invoke(util.clj:411)
    at backtype.storm.daemon.supervisor$try_cleanup_worker.invoke(supervisor.clj:146)
    at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:165)
    at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:223)
    at clojure.lang.AFn.applyToHelper(AFn.java:161)
    at clojure.lang.AFn.applyTo(AFn.java:151)
    at clojure.core$apply.invoke(core.clj:603)
    at clojure.core$partial$fn__4070.doInvoke(core.clj:2343)
    at clojure.lang.RestFn.invoke(RestFn.java:397)
    at backtype.storm.event$event_manager$fn__2484.invoke(event.clj:24)
    at clojure.lang.AFn.run(AFn.java:24)
        at java.lang.Thread.run(Thread.java:680)
Still not quite sure what this all means, but getting closer. This happens pretty much every time I run tests on my mac and on our CI box.

Still no test case, but I did find a workaround. I was running the topology like:

(with-local-cluster [cluster]
    (complete-topology cluster
                       (my-topology drpc)
                       :mock-sources {""actions"" (map vector actions)
                                      ""drpc-requests"" requests}))
Sleeping for 5 seconds before shutting down the cluster seems to fix it (shorter periods just made the error sporadic rather than constant:

(with-local-cluster [cluster]
  (let [results
        (complete-topology cluster
                           (my-topology drpc)
                           :mock-sources {""actions"" (map vector actions)
                                          ""drpc-requests"" requests})]
    (Thread/sleep 5000)
    results))
"
STORM-131,Intermittent Zookeper errors when shutting down local Topology,"https://github.com/nathanmarz/storm/issues/259

We have a great deal of Storm integration tests in our project (Storm version 0.7.3) using local topology. We have only one topology operational at any moment in time. As tests run they are organized in groups. Each group works within the boundaries of a topology. When the tests finish executing they shutdown their local cluster, then the new group of tests launches its own cluster.

We see with some remarkable regularity failures related to, what looks like, incorrect Zookeeper shutdown, which leads to a JVM exit (which is a disaster as no test information is recorded at the end). Here is what we see in the main error log (log level: WARN and higher):

{code}
2012-07-07 00:22:58,420 WARN [ConnectionStateManager-0|]@jenkins com.netflix.curator.framework.state.ConnectionStateManager
=> There are no ConnectionStateListeners registered.

2012-07-07 00:22:58,534 WARN [Thread-23-EventThread|]@jenkins backtype.storm.cluster
=> Received event :disconnected::none: with disconnected Zookeeper.

2012-07-07 00:23:00,013 WARN [Thread-23-SendThread(localhost:2000)|]@jenkins org.apache.zookeeper.ClientCnxn
=> Session 0x1385ece8f1b0017 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.$$YJP$$checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.checkConnect(SocketChannelImpl.java)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1119)
2012-07-07 00:23:01,527 WARN [Thread-23-SendThread(localhost:2000)|]@jenkins org.apache.zookeeper.ClientCnxn
=> Session 0x1385ece8f1b0017 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.$$YJP$$checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.checkConnect(SocketChannelImpl.java)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1119)
2012-07-07 00:23:03,510 WARN [Thread-23-SendThread(localhost:2000)|]@jenkins org.apache.zookeeper.ClientCnxn
=> Session 0x1385ece8f1b0017 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.$$YJP$$checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.checkConnect(SocketChannelImpl.java)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1119)
2012-07-07 00:23:04,687 WARN [Thread-23-SendThread(localhost:2000)|]@jenkins org.apache.zookeeper.ClientCnxn
=> Session 0x1385ece8f1b0017 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.$$YJP$$checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.checkConnect(SocketChannelImpl.java)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1119)
2012-07-07 00:23:05,961 WARN [Thread-23-SendThread(localhost:2000)|]@jenkins org.apache.zookeeper.ClientCnxn
=> Session 0x1385ece8f1b0017 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.$$YJP$$checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.checkConnect(SocketChannelImpl.java)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1119)
2012-07-07 00:23:07,588 WARN [Thread-23-SendThread(localhost:2000)|]@jenkins org.apache.zookeeper.ClientCnxn
=> Session 0x1385ece8f1b0017 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.$$YJP$$checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.checkConnect(SocketChannelImpl.java)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1119)
2012-07-07 00:23:07,691 ERROR [Thread-23-EventThread|]@jenkins com.netflix.curator.framework.imps.CuratorFrameworkImpl
=> Background operation retry gave up

org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
at com.netflix.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:380)
at com.netflix.curator.framework.imps.BackgroundSyncImpl$1.processResult(BackgroundSyncImpl.java:49)
at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:617)
at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
2012-07-07 00:23:07,697 WARN [ConnectionStateManager-0|]@jenkins com.netflix.curator.framework.state.ConnectionStateManager
=> There are no ConnectionStateListeners registered.

2012-07-07 00:23:07,699 ERROR [Thread-23-EventThread|]@jenkins backtype.storm.zookeeper
=> Unrecoverable Zookeeper error Background operation retry gave up

org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
at com.netflix.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:380)
at com.netflix.curator.framework.imps.BackgroundSyncImpl$1.processResult(BackgroundSyncImpl.java:49)
at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:617)

at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
And here is what we see in Storm dedicated log file (log level: DEBUG):

2012-07-07 00:22:58,306 INFO [main|]@jenkins backtype.storm.daemon.task
=> Shut down task TLTopology-1-1341620393:31

2012-07-07 00:22:58,306 INFO [main|]@jenkins backtype.storm.messaging.loader
=> Shutting down receiving-thread: [TLTopology-1-1341620393, 5]

2012-07-07 00:22:58,307 INFO [main|]@jenkins backtype.storm.messaging.loader
=> Waiting for receiving-thread:[TLTopology-1-1341620393, 5] to die

2012-07-07 00:22:58,307 INFO [Thread-319|]@jenkins backtype.storm.messaging.loader
=> Receiving-thread:[TLTopology-1-1341620393, 5] received shutdown notice

2012-07-07 00:22:58,307 INFO [main|]@jenkins backtype.storm.messaging.loader
=> Shutdown receiving-thread: [TLTopology-1-1341620393, 5]

2012-07-07 00:22:58,307 INFO [main|]@jenkins backtype.storm.daemon.worker
=> Terminating zmq context

2012-07-07 00:22:58,307 INFO [main|]@jenkins backtype.storm.daemon.worker
=> Waiting for threads to die

2012-07-07 00:22:58,307 INFO [Thread-318|]@jenkins backtype.storm.util
=> Async loop interrupted!

2012-07-07 00:22:58,309 INFO [main|]@jenkins backtype.storm.daemon.worker
=> Disconnecting from storm cluster state context

2012-07-07 00:22:58,311 INFO [main|]@jenkins backtype.storm.daemon.worker
=> Shut down worker TLTopology-1-1341620393 96e12303-4c22-4821-9f3b-3bce2230bf08 5

2012-07-07 00:22:58,311 DEBUG [main|]@jenkins backtype.storm.util
=> Rmr path /tmp/f308eb0e-2e72-4221-9620-43e15a9c1bdc/workers/16966f32-d0d4-4ee1-a0fe-1d85fc4a478e/heartbeats

2012-07-07 00:22:58,313 DEBUG [main|]@jenkins backtype.storm.util
=> Removing path /tmp/f308eb0e-2e72-4221-9620-43e15a9c1bdc/workers/16966f32-d0d4-4ee1-a0fe-1d85fc4a478e/pids

2012-07-07 00:22:58,313 DEBUG [main|]@jenkins backtype.storm.util
=> Removing path /tmp/f308eb0e-2e72-4221-9620-43e15a9c1bdc/workers/16966f32-d0d4-4ee1-a0fe-1d85fc4a478e

2012-07-07 00:22:58,313 INFO [main|]@jenkins backtype.storm.daemon.supervisor
=> Shut down 96e12303-4c22-4821-9f3b-3bce2230bf08:16966f32-d0d4-4ee1-a0fe-1d85fc4a478e

2012-07-07 00:22:58,314 INFO [main|]@jenkins backtype.storm.daemon.supervisor
=> Shutting down supervisor 96e12303-4c22-4821-9f3b-3bce2230bf08

2012-07-07 00:22:58,314 INFO [Thread-25|]@jenkins backtype.storm.event
=> Event manager interrupted

2012-07-07 00:22:58,315 INFO [Thread-26|]@jenkins backtype.storm.event
=> Event manager interrupted

2012-07-07 00:22:58,318 INFO [main|]@jenkins backtype.storm.testing
=> Shutting down in process zookeeper

2012-07-07 00:22:58,321 INFO [main|]@jenkins backtype.storm.testing
=> Done shutting down in process zookeeper

2012-07-07 00:22:58,321 INFO [main|]@jenkins backtype.storm.testing
=> Deleting temporary path /tmp/0202cf11-6ad7-4dda-94d6-622a63c9f6b6

2012-07-07 00:22:58,321 DEBUG [main|]@jenkins backtype.storm.util
=> Rmr path /tmp/0202cf11-6ad7-4dda-94d6-622a63c9f6b6

2012-07-07 00:22:58,322 INFO [main|]@jenkins backtype.storm.testing
=> Deleting temporary path /tmp/ee47e3e3-752f-40a8-b6a9-a197a9dda3de

2012-07-07 00:22:58,323 DEBUG [main|]@jenkins backtype.storm.util
=> Rmr path /tmp/ee47e3e3-752f-40a8-b6a9-a197a9dda3de

2012-07-07 00:22:58,323 INFO [main|]@jenkins backtype.storm.testing
=> Deleting temporary path /tmp/ece72b84-357e-4183-aeb5-e0d2dc5d6eca

2012-07-07 00:22:58,323 DEBUG [main|]@jenkins backtype.storm.util
=> Rmr path /tmp/ece72b84-357e-4183-aeb5-e0d2dc5d6eca

2012-07-07 00:22:58,326 INFO [main|]@jenkins backtype.storm.testing
=> Deleting temporary path /tmp/f308eb0e-2e72-4221-9620-43e15a9c1bdc

2012-07-07 00:22:58,326 DEBUG [main|]@jenkins backtype.storm.util
=> Rmr path /tmp/f308eb0e-2e72-4221-9620-43e15a9c1bdc

2012-07-07 00:22:58,534 WARN [Thread-23-EventThread|]@jenkins backtype.storm.cluster
=> Received event :disconnected::none: with disconnected Zookeeper.

2012-07-07 00:23:07,699 ERROR [Thread-23-EventThread|]@jenkins backtype.storm.zookeeper
=> Unrecoverable Zookeeper error Background operation retry gave up

org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
at com.netflix.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:380)
at com.netflix.curator.framework.imps.BackgroundSyncImpl$1.processResult(BackgroundSyncImpl.java:49)
at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:617)
at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
2012-07-07 00:23:07,702 INFO [Thread-23-EventThread|]@jenkins backtype.storm.util

=> Halting process: (""Unrecoverable Zookeeper error"")

{code}
It seems like a threading issue to me personally. I wonder if there is some form of workaround. I also understand that since this is a ""local"" topology issue, this might not receive due attention... However, fundamentally this is what new users would start with when they begin to play with Storm, and, I think, it is important to make this experience positive.

Nathan, thank you very much for everything that you're doing.

-Kyrill

----------
dkincaid: Looking through the shutdown code for local clusters I noticed a comment in the code about a possible race condition. I'm wondering if we could be running into this on our Jenkins server (which we know runs pretty slowly). Is a worker getting restarted before the supervisor can be shutdown?

Here is the function with the comment:

{code}
(defn kill-local-storm-cluster [cluster-map]
  (.shutdown (:nimbus cluster-map))
  (.close (:state cluster-map))
  (.disconnect (:storm-cluster-state cluster-map))
  (doseq [s @(:supervisors cluster-map)]
    (.shutdown-all-workers s)
    ;; race condition here? will it launch the workers again?
    (supervisor/kill-supervisor s))
  (psim/kill-all-processes)
  (log-message ""Shutting down in process zookeeper"")
  (zk/shutdown-inprocess-zookeeper (:zookeeper cluster-map))
  (log-message ""Done shutting down in process zookeeper"")
  (doseq [t @(:tmp-dirs cluster-map)]
    (log-message ""Deleting temporary path "" t)
    (rmr t)
    ))
{code}

--------
kyrill007: Fantastic catch, Dave!!! This exactly what is happening: supervisor begins launching new workers when the other ones are still being shut down. Here is the proof from the logs:

{code}
Shut down process is initiated at 04:37:05,136.

2012-07-11 04:37:05,136 INFO [main|]@jenkins backtype.storm.daemon.nimbus
  => Shutting down master

2012-07-11 04:37:05,145 INFO [main|]@jenkins backtype.storm.daemon.nimbus
  => Shut down master

2012-07-11 04:37:05,151 INFO [main|]@jenkins backtype.storm.daemon.supervisor
  => Shutting down 5c48d4fc-769f-41ef-abd6-f92df60fa543:12eba15d-fb17-4a3c-8e25-1c0266eed04d

2012-07-11 04:37:05,152 INFO [main|]@jenkins backtype.storm.process-simulator
  => Killing process ea132b37-dc6a-447c-b1de-ac6727c82cef

2012-07-11 04:37:05,152 INFO [main|]@jenkins backtype.storm.daemon.worker
  => Shutting down worker TLTopology-1-1341981237 5c48d4fc-769f-41ef-abd6-f92df60fa543 1

2012-07-11 04:37:05,152 INFO [main|]@jenkins backtype.storm.daemon.task
  => Shutting down task TLTopology-1-1341981237:64

2012-07-11 04:37:05,153 INFO [Thread-129|]@jenkins backtype.storm.util
  => Async loop interrupted!

2012-07-11 04:37:05,180 INFO [main|]@jenkins backtype.storm.daemon.task
  => Shut down task TLTopology-1-1341981237:64

2012-07-11 04:37:05,180 INFO [main|]@jenkins backtype.storm.daemon.task
  => Shutting down task TLTopology-1-1341981237:34
It continues for a while (we have a lot of workers). Then at 04:37:05,665 we start seeing this:

012-07-11 04:37:05,665 DEBUG [Thread-19|]@jenkins backtype.storm.daemon.supervisor
  => Assigned tasks: {2 #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""TLTopology-1-1341981237"", :task-ids (96 66 36 6 102 72 42 12 108 78 48 18 114 84 54 24 120 90 60 30 126)}, 1 #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""TLTopology-1-1341981237"", :task-ids (64 34 4 100 70 40 10 106 76 46 16 112 82 52 22 118 88 58 28 124 94)}, 3 #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""TLTopology-1-1341981237"", :task-ids (32 2 98 68 38 8 104 74 44 14 110 80 50 20 116 86 56 26 122 92 62)}}

2012-07-11 04:37:05,665 DEBUG [Thread-19|]@jenkins backtype.storm.daemon.supervisor
  => Allocated: {""a724dc19-84ec-46dc-9768-afb73df94237"" [:valid #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1341981425, :storm-id ""TLTopology-1-1341981237"", :task-ids #{96 66 36 6 102 72 42 12 108 78 48 18 114 84 54 24 120 90 60 30 126}, :port 2}]}

2012-07-11 04:37:05,665 DEBUG [Thread-19|]@jenkins backtype.storm.util
  => Making dirs at /tmp/4884ffb5-c6c7-43a9-ac72-e0a5426eea3c/workers/a7f81ea0-a5f6-47de-9a89-47998b1e1639/pids

2012-07-11 04:37:05,666 DEBUG [Thread-19|]@jenkins backtype.storm.util
  => Making dirs at /tmp/4884ffb5-c6c7-43a9-ac72-e0a5426eea3c/workers/1b5c4c87-4e05-4cab-a580-ae1dabb3fd2e/pids

2012-07-11 04:37:05,666 INFO [main|]@jenkins backtype.storm.daemon.worker
  => Shut down worker TLTopology-1-1341981237 5c48d4fc-769f-41ef-abd6-f92df60fa543 2

2012-07-11 04:37:05,667 DEBUG [main|]@jenkins backtype.storm.util
  => Rmr path /tmp/4884ffb5-c6c7-43a9-ac72-e0a5426eea3c/workers/a724dc19-84ec-46dc-9768-afb73df94237/heartbeats

2012-07-11 04:37:05,669 DEBUG [main|]@jenkins backtype.storm.util
  => Removing path /tmp/4884ffb5-c6c7-43a9-ac72-e0a5426eea3c/workers/a724dc19-84ec-46dc-9768-afb73df94237/pids

2012-07-11 04:37:05,669 DEBUG [main|]@jenkins backtype.storm.util
  => Removing path /tmp/4884ffb5-c6c7-43a9-ac72-e0a5426eea3c/workers/a724dc19-84ec-46dc-9768-afb73df94237

2012-07-11 04:37:05,669 INFO [main|]@jenkins backtype.storm.daemon.supervisor
  => Shut down 5c48d4fc-769f-41ef-abd6-f92df60fa543:a724dc19-84ec-46dc-9768-afb73df94237

2012-07-11 04:37:05,669 INFO [main|]@jenkins backtype.storm.daemon.supervisor
  => Shutting down supervisor 5c48d4fc-769f-41ef-abd6-f92df60fa543

2012-07-11 04:37:05,670 INFO [Thread-18|]@jenkins backtype.storm.event
  => Event manager interrupted

2012-07-11 04:37:05,670 INFO [Thread-19|]@jenkins backtype.storm.daemon.supervisor
  => Launching worker with assignment #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""TLTopology-1-1341981237"", :task-ids (64 34 4 100 70 40 10 106 76 46 16 112 82 52 22 118 88 58 28 124 94)} for this supervisor 5c48d4fc-769f-41ef-abd6-f92df60fa543 on port 1 with id a7f81ea0-a5f6-47de-9a89-47998b1e1639

2012-07-11 04:37:05,672 INFO [Thread-19|]@jenkins backtype.storm.daemon.worker
  => Launching worker for TLTopology-1-1341981237 on 5c48d4fc-769f-41ef-abd6-f92df60fa543:1 with id a7f81ea0-a5f6-47de-9a89-47998b1e1639 and conf {""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.fall.back.on.java.serialization"" true, ""zmq.linger.millis"" 0, ""topology.skip.missing.kryo.registrations"" true, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""nimbus.reassign"" true, ""nimbus.monitor.freq.secs"" 10, ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""storm.local.dir"" ""/tmp/4884ffb5-c6c7-43a9-ac72-e0a5426eea3c"", ""supervisor.worker.start.timeout.secs"" 120, ""nimbus.cleanup.inbox.freq.secs"" 600, ""nimbus.inbox.jar.expiration.secs"" 3600, ""nimbus.host"" ""localhost"", ""storm.zookeeper.port"" 2000, ""transactional.zookeeper.port"" nil, ""transactional.zookeeper.servers"" nil, ""storm.zookeeper.root"" ""/storm"", ""supervisor.enable"" true, ""storm.zookeeper.servers"" [""localhost""], ""transactional.zookeeper.root"" ""/transactional"", ""topology.worker.childopts"" nil, ""worker.childopts"" ""-Xmx768m"", ""supervisor.heartbeat.frequency.secs"" 5, ""drpc.port"" 3772, ""supervisor.monitor.frequency.secs"" 3, ""task.heartbeat.frequency.secs"" 3, ""topology.max.spout.pending"" nil, ""storm.zookeeper.retry.interval"" 1000, ""supervisor.slots.ports"" (1 2 3), ""topology.debug"" false, ""nimbus.task.launch.secs"" 120, ""nimbus.supervisor.timeout.secs"" 60, ""topology.message.timeout.secs"" 30, ""task.refresh.poll.secs"" 10, ""topology.workers"" 1, ""supervisor.childopts"" ""-Xmx1024m"", ""nimbus.thrift.port"" 6627, ""topology.stats.sample.rate"" 0.05, ""worker.heartbeat.frequency.secs"" 1, ""nimbus.task.timeout.secs"" 30, ""drpc.invocations.port"" 3773, ""zmq.threads"" 1, ""storm.zookeeper.retry.times"" 5, ""topology.state.synchronization.timeout.secs"" 60, ""supervisor.worker.timeout.secs"" 30, ""nimbus.file.copy.expiration.secs"" 600, ""drpc.request.timeout.secs"" 600, ""storm.local.mode.zmq"" false, ""ui.port"" 8080, ""nimbus.childopts"" ""-Xmx1024m"", ""topology.ackers"" 1, ""storm.cluster.mode"" ""local"", ""topology.optimize"" true, ""topology.max.task.parallelism"" nil}

2012-07-11 04:37:05,675 INFO [Thread-19|]@jenkins backtype.storm.event
  => Event manager interrupted

2012-07-11 04:37:05,677 INFO [Thread-19-EventThread|]@jenkins backtype.storm.zookeeper
  => Zookeeper state update: :connected:none
which at the end result in this:

2012-07-11 04:37:06,175 INFO [Thread-19-EventThread|]@jenkins backtype.storm.zookeeper
  => Zookeeper state update: :disconnected:none

2012-07-11 04:37:06,175 WARN [Thread-22-EventThread|]@jenkins backtype.storm.cluster
  => Received event :disconnected::none: with disconnected Zookeeper.

2012-07-11 04:37:15,923 ERROR [Thread-22-EventThread|]@jenkins backtype.storm.zookeeper
  => Unrecoverable Zookeeper error Background operation retry gave up

org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
    at com.netflix.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:380)
    at com.netflix.curator.framework.imps.BackgroundSyncImpl$1.processResult(BackgroundSyncImpl.java:49)
    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:613)
    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
2012-07-11 04:37:15,926 INFO [Thread-22-EventThread|]@jenkins backtype.storm.util
  => Halting process: (""Unrecoverable Zookeeper error"")
{code}

Dear Nathan,

If this race condition could somehow be fixed (presumably it is not that hard since we know what the problem is), it would so much appreciated!!!
"
STORM-129,Keep Storm dependencies and user dependencies independent,"https://github.com/nathanmarz/storm/issues/115

For instance, user classes should be able to use a different version of a library than Storm uses. Users have run into issues with Hibernate since Hibernate requires asm 1.4 while Storm requires asm 3.3 (through Kryo).

This is tricky since the Storm code and user code run together. e.g., when a user does an emit call, that calls into Storm code which calls into Kryo, 0mq, etc. It's not clear how to keep the dependencies separate.

Possible technologies to look at include OSGi and JPF. However, there is a serious concern about the complexity and overhead of these approaches.

Hadoop has the exact same problem, so it's worth looking to see if it does anything to address this. AFAIK, it doesn't."
STORM-128,Topology fails to start if a configured DRPC server is down,"https://github.com/nathanmarz/storm/issues/696

In our environment we have 3 DRPC servers running. This was done mainly for availability and capacity. However, we noticed that when even one of these servers is down, topologies fail to start with the following exception:

java.lang.RuntimeException: org.apache.thrift7.transport.TTransportException: java.net.NoRouteToHostException: No route to host
at backtype.storm.drpc.DRPCInvocationsClient.(DRPCInvocationsClient.java:23)
at backtype.storm.drpc.DRPCSpout.open(DRPCSpout.java:65)
at storm.trident.spout.RichSpoutBatchTriggerer.open(RichSpoutBatchTriggerer.java:41)
at backtype.storm.daemon.executor$fn__3985$fn__3997.invoke(executor.clj:460)
at backtype.storm.util$async_loop$fn__465.invoke(util.clj:375)
at clojure.lang.AFn.run(AFn.java:24)
at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.thrift7.transport.TTransportException: java.net.NoRouteToHostException: No route to host
at org.apache.thrift7.transport.TSocket.open(TSocket.java:183)
at org.apache.thrift7.transport.TFramedTransport.open(TFramedTransport.java:81)
at backtype.storm.drpc.DRPCInvocationsClient.connect(DRPCInvocationsClient.java:30)
at backtype.storm.drpc.DRPCInvocationsClient.(DRPCInvocationsClient.java:21)
... 6 more
Caused by: java.net.NoRouteToHostException: No route to host
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)
at java.net.Socket.connect(Socket.java:579)
at org.apache.thrift7.transport.TSocket.open(TSocket.java:178)
... 9 more

I was wondering if it makes sense to make Storm handle this gracefully instead of failing fast. Otherwise, the DRPC servers become a SPOF.

If the topologies are already running the topology usually just logs an error message and continues.

----------
dkador: +1 on figuring out how to make the DRPC stuff not a SOP. I'd be happy to look into it myself but not sure where to start. Any guidance?

----------
rijuk: For reference, the stack trace I see when a DRPC server goes down while a topology is running is the following. In this case, the topology continues to function normally.

[backtype.storm.drpc.DRPCSpout Thread-65]: Failed to fetch DRPC result from DRPC server
org.apache.thrift7.transport.TTransportException: java.net.ConnectException: Connection refused
at org.apache.thrift7.transport.TSocket.open(TSocket.java:183)
at org.apache.thrift7.transport.TFramedTransport.open(TFramedTransport.java:81)
at backtype.storm.drpc.DRPCInvocationsClient.connect(DRPCInvocationsClient.java:30)
at backtype.storm.drpc.DRPCInvocationsClient.fetchRequest(DRPCInvocationsClient.java:53)
at backtype.storm.drpc.DRPCSpout.nextTuple(DRPCSpout.java:89)
at storm.trident.spout.RichSpoutBatchTriggerer.nextTuple(RichSpoutBatchTriggerer.java:68)
at backtype.storm.daemon.executor$fn__3985$fn__3997$fn__4026.invoke(executor.clj:502)
at backtype.storm.util$async_loop$fn__465.invoke(util.clj:377)
at clojure.lang.AFn.run(AFn.java:24)
at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.ConnectException: Connection refused
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)
at java.net.Socket.connect(Socket.java:579)
at org.apache.thrift7.transport.TSocket.open(TSocket.java:178)
... 9 more

In this case I'd the host up, but the DRPC server process was down. Hence the ConnectException. But, the behavior is the same even when the host is unreachable, except for the Exception type.

@dkador, I'm not sure what the right solution is. One naive solution I can think of is to make DRPCInvocationsClient constructor rethrow TException instead of throwing a RuntimeException. Obviously, you'll have to make sure that all callers of this higher up in the stack handle this exception properly.

Actually, on second thoughts that's not a good idea. You probably still want the DRPCInvocationsClient object to be constructed. So, maybe you can log an error and just eat that exception. All other methods in that class call ""connect"" if necessary anyway."
STORM-125,Multilang doesn't work on Ubuntu 10.04,"https://github.com/nathanmarz/storm/issues/55

java.io.IOException: Cannot run program ""python""

See here for more details: http://groups.google.com/group/storm-user/browse_thread/thread/c061b826a5acae48"
STORM-124,"Tuple Lost in Trident Topology on Remote Cluster, while the same topology works correctly on local cluster","https://github.com/nathanmarz/storm/issues/535

I have created a trident topology, while it runs correctly on local cluster , it gets random result when deployed on remote cluster due to tuple loss in trident

Code example is as followed, In ExtractFunction it will parse kafka message and create 4 tuples from the content(5 fields), then CustomReduceAggregator will aggregate based on first 4 fields.
I generated 800 kafka messages during the test, ExtractFunction is expected to get 800 tuples and generate 800*4=3200 tuples for CustomReduceAggregator.
This works correctly in local cluster.
In remoted cluster, while ExtractFunction generated 3200 tuples, CustomReduceAggregator will always get only a few hundred tuples, therefore the result is not correct

Is this a bug in storm/trident or there is some configuration needed for my trident topology?

    TridentKafkaConfig tridentKafkaConfig = new TridentKafkaConfig(new ZkHosts(zkServer+"":""+zkPort, zkBrokerPath), kafkaTopic);
    tridentKafkaConfig.forceStartOffsetTime(-2);

    TridentTopology topology=new TridentTopology();
    TridentState tridentState = topology.newStream(""kafkaSpout""+System.currentTimeMillis(), new TransactionalTridentKafkaSpout(tridentKafkaConfig))
        .parallelismHint(1)
        .each(new Fields(""bytes""), new ExtractFunction(), new Fields(""a"", ""b"", ""c"", ""b"", ""e""))
        .parallelismHint(1)
        .groupBy(new Fields(""a"", ""b"", ""c"", ""f""))
        .persistentAggregate(new CassandraMapState.Factory(StateType.TRANSACTIONAL, ""cf_1""),  
                new Fields(""a"", ""b"", ""c"", ""b"", ""e""), 
                new CustomReduceAggregator(), 
                new Fields(""value""))
        .parallelismHint(1);

    topology.newDRPCStream(""profile"", localDRPC)
        .each(new Fields(""args""), new DrpcParamSplitFunction(), new Fields(""a"", ""b"", ""c"", ""b""))
        .groupBy(new Fields(""a"", ""b"", ""c"", ""b""))
        .stateQuery(tridentState, new Fields(""a"", ""b"", ""c"", ""b""), new MapGet(), new Fields(""value""));

    return topology.build();"
STORM-123,NImbus BCP,"https://github.com/nathanmarz/storm/issues/737

Hi,
We are building a system where we need to have a BCP for nimbus box. Topologies will already be deployed on nimbus1 box while nimbus2 is our BCP. If nimbus1 goes down due to hardware failure how do get nimbus2 in so that we can control the start/kill of the topologies which were already deployed on nimbus1. I understand that topologies deployed earlier will continue to run since the jars have already been distributed to the supervisors."
STORM-122,log4j-over-slf4j in 0.9.0-rc2 breaks existing topologies,"https://github.com/nathanmarz/storm/issues/745

Apologies if this ticket is off-base, I'm not an expert in this area. We've been testing an existing topology we ran on Storm 0.8.1 on a deployment of Storm 0.9.0-rc2. The only roadblock we've encountered so far is the presence of the log4j-over-slf4j jar in the newer version. It turns out that log4j-over-slf4j is incompatible with slf4j-log4j12.

Basically our topology uses slf4j as the the logging facade and log4j as the backend, which I've heard is a common pattern. It seems that in order to migrate to the new Storm version we must either move our code off of log4j or delete the log4j-over-slf4j jar from the storm classpath.

Are we doing something crazy here? I'm open to alternate solutions, but it seems like an issue to break legacy topologies over logging. My understanding based on #144 was that Storm was migrating completely to slf4j, is this no longer true? Any insight, advice, or fixes would be helpful.

--------
jmlogan: You're going to have to exclude log4j and log4j-over-slf4j. Storm calls mostly seem to be slf4j, but it is not logger-agonistic. In 0.9.0, it will force logback onto your classpath at runtime, and will crash if it has multiple adapters.

If you see a job failing to launch, right after updating, it's probably this issue. We ended up just excluding log4j from the offending dependencies.


---------
bakks: Hey, thanks for your response. The issue is that log4j-over-slf4j is now a Storm dependency, so its loaded into the topology classpath by default. This wasn't true in earlier Storm versions, so perhaps this is intentional, but our topology which used to work now doesn't. It seems like there is now no way to use log4j as an slf4j backend. If that is the new state of the world, its fine, but it seems like an issue and I'm hoping for a workaround."
STORM-121,A hook should be able to modify a tuple,"https://github.com/nathanmarz/storm/issues/534

High level:

I should be able to add a field like “_trace” using a hook and have it be passed through the topology without having to change the rest of the topology. In this way a record can be followed through the topology and information about it’s process can be extracted.

Want:

https://github.com/maphysics/storm-starter/blob/trace_injector/src/jvm/storm/starter/ReverseStringTopology.java 
In the example topology linked above (which is on my fork of storm starter), the hook is looking for a field called _trace containing a hashmap. The idea is that somewhere in the topology that field is turned on and a hashmap is put in containing some basic starting information. In my example that was done in the spout and all the bolts emit an _trace field but really we want to be able to touch one bolt and add the field when a particular requirement is met. Then that field will continue with the record even when the OutputFieldsDeclarer does not have _trace as a field so that one does not have to edit each of the subsequential bolts.

(This might be actually a how-to and not a bug in which case let me know and I’ll send it to the mailing list.)

/cc @mrflip"
STORM-120,util/acquire-random-range-id is not thread-safe,"https://github.com/nathanmarz/storm/issues/724

Concurrent calls to util/acquire-random-range-id with the same parameters can result in an IndexOutOfBoundsException, as an increment in one thread may occur after the bounds check in another. The resulting curr value can be >= the size of the List state.

https://github.com/nathanmarz/storm/blob/fc5fbb8b352cf91050cdde4a9f9e77e673ab7f48/storm-core/src/clj/backtype/storm/util.clj#L606"
STORM-119,Nimbus and Supervisor process crash with error.,"Hi,
We are getting the below exception while running storm and zookeper on the same box.

2013-11-26 16:42:48 CuratorFrameworkImpl [ERROR] Background operation retry gave up
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
at com.netflix.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:380)
at com.netflix.curator.framework.imps.BackgroundSyncImpl$1.processResult(BackgroundSyncImpl.java:49)
at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:617)
at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
2013-11-26 16:42:52 nimbus [ERROR] Error when processing event
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /storms
at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1363)
at com.netflix.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:184)
at com.netflix.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:173)
at com.netflix.curator.RetryLoop.callWithRetry(RetryLoop.java:85)
at com.netflix.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:169)
at com.netflix.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:161)
at com.netflix.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:36)
at backtype.storm.zookeeper$get_children.invoke(zookeeper.clj:111)
at backtype.storm.cluster$mk_distributed_cluster_state$reify__1996.get_children(cluster.clj:86)
at backtype.storm.cluster$mk_storm_cluster_state$reify__2415.active_storms(cluster.clj:237)
at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:298)
at backtype.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:634)
at clojure.lang.RestFn.invoke(RestFn.java:410)
at backtype.storm.daemon.nimbus$fn__3592$exec_fn__1228__auto____3593$fn__3598$fn__3599.invoke(nimbus.clj:872)
at backtype.storm.daemon.nimbus$fn__3592$exec_fn__1228__auto____3593$fn__3598.invoke(nimbus.clj:871)
at backtype.storm.timer$schedule_recurring$this__1776.invoke(timer.clj:69)
at backtype.storm.timer$mk_timer$fn__1759$fn__1760.invoke(timer.clj:33)
at backtype.storm.timer$mk_timer$fn__1759.invoke(timer.clj:26)
at clojure.lang.AFn.run(AFn.java:24)
at java.lang.Thread.run(Thread.java:724)

The exception is stopping nimbus and supervisor processes. We are using storm-0.8.2 and zookeeper-3.4.5."
STORM-117,Storm should provide client library,"Developers that would like to use distributed services such as Storm should have client libraries that allow users to deploy applications (Topologies) and send requests to these application without requiring dependencies only required by the services. 

Definitions:
Service Environment - Storm Nimbus, DRPC and, workers
Client Environment - Standalone JVM, Web Application etc.

One example of this is hadoop-client. Before it was created user would use hadoop-core and they would be force to include or manually exclude servlet artifacts and other such artifacts that may cause classpath issues in the client's environment. This will cut down on confusion to new storm users that are unfamiliar with the details of storm and logging frameworks or user integrate storm into existing applications.

One such example is storm now includes Logback-classic, when user adds this to a project that uses slf4j-API it causes their logging system to break. This happen to our project until we manually excluded Logback. In our environment like others uses log4j for logging.  Yes Logback is an improvement over log4j but you can't expect others to change their standard logging framework because of one external dependency.  Also there are newer alternatives like log4j 2 and jboss logging that will compete to be the upgrade path for those that would like to update.

To start off with I do not expect storm to allow users to change the logging framework used for client code while executing in the context of storm workers just the code run from the requesting application. Note allowing system administrators to change the logging framework would be easy since you are using slf4j."
STORM-116,[UI] Sort bolt/spout in order that wrote in toplogy,"https://github.com/nathanmarz/storm/issues/129

I think bolt/spout in storm UI should be sorted in the order that I wrote in topology.

For example, I wrote

builder.setBolt(""SplitSentence"", new SplitSentenceBolt(), 3)
        .shuffleGrouping(""SentenceSpout"");
builder.setBolt(""Exclamation"", new ExclamationBolt(), 2)
        .shuffleGrouping(""SplitSentence"");


In UI, they are sorted by their ID like this:
Exclamation    2 .....
SplitSentence 3 .....

To fix this issue, I think it is good to add left column that shows its raw number like this:

# / id / parallelism ....
1 / splitsentence / 3 ...
2 / exclamation / 2 ....

----------
xumingming: things will get complicated because the topology could be a graph. maybe it could be implemented in another project to have a more 'visualized' view of topology. e.g. the topology is shown as a connected node of bolt, and you can click on each node to check the details of each bolt, task etc.

----------
mrflip: Barring DRPC links, is it the case that a topology is always a DAG? If so, a ""topological sort"" is always possible (with a tiebreaker to make it unique), and a depth-first (1, 1.a, 1.a.i, 1.b, 2) or breadth-first (1, 2, 1.a, 1.b, 1.a.1) enumeration of the graph would give stable sortable labels."
STORM-115,Allow bin/storm to take a config as a parameter,"https://github.com/nathanmarz/storm/issues/243

We're running a cluster on EC2 and have opened the thrift port to our office so we can deploy easily. But every now and then i need to deploy from other networks. Hence i need to set up a tunnel.
This causes problems since i need to change my deploy configuration back and forth. I know i can specify -c nimbus.host=hostname but there's a bug so i can't specify a different port this way (since it's expecting an int).
It's a big annoyance having to change ~/.storm/storm.yaml for this.

A good solution in my eyes would be to specify a path to a yaml-file, so i can have different yaml-files to tunnel/office deploys. 
I.e.

storm jar allmycode.jar -config conf/tunnel.yaml org.me.MyTopology arg1 arg2 arg3

----------
tomdz: As implemented, this requires that the config file is in the classpath. Unfortunately this is a major hassle as it forces config files to be in either ~/.storm or the conf directory of the storm installation. Ideally I can specify a file path here so I can for instance bundle config files with my topologies or structure them in some other way. Reasons for this are:

Storm only reads the config file from the classpath, it does not try to load it as a file.
Most commands only have ~/.storm and the the conf folder in the storm distribution in the classpath and there is no commandline option to add additional entries.
The storm script uses -cp but does not check whether CLASSPATH is already set. Since -cp makes java ignore CLASSPATH this means that the environment variable can't be used to specify additional classpath entries.
Ideally either Utils.findAndReadConfigFile or Utils.findResources test if the path can be loaded as a file after loading as a classpath resource failed."
STORM-114,Runtime ClassCastException while attempting to log problems with file deletion while cleaning inbox.,"I noticed this while starting nimbus with a user that didn't have privileges to cleanup the inbox (also see this https://groups.google.com/forum/#!msg/storm-user/I8gTBg8-qy8/U7Q41AiJYkUJ)

nimbus clean-inbox calls log-error incorrectly (first argument should be a Throwable) generating a runtime exception.

        (log-message ""Cleaning inbox ... deleted: "" (.getName f))
        ;; This should never happen
        (log-error ""Cleaning inbox ... error deleting: "" (.getName f))
        ))))
I guess a ""this should never happen"" should be instead be replaced with a RuntimeException.

2013-06-26 04:45:59,999 nimbus [Thread-1] [ERROR] Error when processing event
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Throwable
    at clojure.tools.logging$eval1$fn__7.invoke(NO_SOURCE_FILE:0)
    at clojure.tools.logging.impl$fn__56$G__49__67.invoke(impl.clj:16)
    at clojure.tools.logging$log_STAR_.invoke(logging.clj:59)
    at backtype.storm.daemon.nimbus$clean_inbox.invoke(nimbus.clj:832)
    at backtype.storm.daemon.nimbus$fn__3596$exec_fn__1231__auto____3597$fn__3607.invoke(nimbus.clj:880)
    at backtype.storm.timer$schedule_recurring$this__1779.invoke(timer.clj:77)
    at backtype.storm.timer$mk_timer$fn__1762$fn__1763.invoke(timer.clj:33)
    at backtype.storm.timer$mk_timer$fn__1762.invoke(timer.clj:26)
    at clojure.lang.AFn.run(AFn.java:24)
    at java.lang.Thread.run(Thread.java:662)
2013-06-26 04:46:00,022 util [Thread-1] [INFO] Halting process: (""Error when processing an event"")

both #611 and #756 are trying to solve this issue"
STORM-112,Race condition between Topology Kill and Worker Timeout can crash supervisor,"Recently during testing on a single node cluster we saw a supervisor crash when a topology was killed. The supervisor came back up and recovered, so it was not that big of a deal, but when we dug into it, it appears that there is a race.
https://github.com/nathanmarz/storm/issues/656

When a topology is killed the local assignments are reset, and then stormconf.ser is deleted right away. But at the same time sync-process may already be running with old state indicating that a worker timed out and needs to be relaunched. launch-worker then tries to read in the topology conf which was deleted and crashes.

The following is a sanitized version of the supervisor log that shows this happening.
https://gist.github.com/revans2/6282830"
STORM-111,Corrupt worker heartbeat on server reboot,"https://github.com/nathanmarz/storm/issues/23

Reported on the user group: http://groups.google.com/group/storm-user/browse_thread/thread/dd3fe363db21f4f5
FileOutputStream.close() might not be sufficient to sync the file to disk. Might need to do a manual fsync.

---------
z1lv1n4s: Is this still an issue? It was reported on storm-0.5.* :)

--------
doreflend: This happened to me today. I cleared zookeeper and the contents of my storm.local.dir.

I think zookeeper was overkill, but the storm local dir did it."
STORM-110,Add metrics for disruptor queue population,"https://github.com/nathanmarz/storm/issues/613

@arrawatia has a first stab at a metrics producer for the disruptor queues: read head position, write head position and size, allowing a consumer to in turn calculate %full and throughput. Currently it's an IMetric that reports the numbers directly whenever getValueAndReset() is called.

Would you like to add this to the builtin metrics in core? It's useful for determining if the buffer capacities needs tuning -- and it's otherwise really difficult to get figures on it without flooding the logs.

* Should this be done with the stats framework, the metrics framework, or both?
* One metric per disruptor queue, or one combined hash per worker? I prefer one-per-queue, but we had trouble finding the right place to instantiate the metric: the metric needs a handle on the disruptor queue and needs to know what executor it will belong to. So right now the spike implementation lives with the system metrics.
* Is direct response to getValueAndReset() the right approach, or should this be a MeanReduced metric?

----------
nathanmarz: This will be an awesome feature addition and should be done with the metrics framework. There should be one metric per queue.

I'm not entirely sure on the right way to implement this, but it's critical to a) not introduce any race conditions, and b) have minimal impact on performance. I look forward to seeing a pull request for this and let me know if you have any questions regarding semantics."
STORM-109,Deploying topology with 540 workers caused nimbus to crash.,"https://github.com/nathanmarz/storm/issues/604

When deploying a topology to a storm cluster and requesting 540 workers, nimbus entered into a continuous exception, die, restart loop printing this in the logs:

2013-06-21 02:14:23,551 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181] - Exception causing close of session 0x13d7f3c28867c9f due to java.io.IOException: Len error 1277489
When the topology was killed by nuking the local disk state for that topology, nimbus recovered itself. When the topology was redeployed with less workers, it did not cause nimbus to fail.

Probably what's happening is nimbus is trying to create a zknode that is larger than 1MB which is the default max size for a zk node.

One solution is to increase this threshold in zookeeper to a larger value than 1MB.


---------
d2r: This is exactly what we had to do. We did it by setting jute.maxbuffer property to some higher power of 2.

I think it happened to us because we launched > 12k workers and all of the assignments were serialized at once and written to a zk node at once, and this constantly exceeded the 1MB buffer. I think we ended up using jute.maxbuffer=4097150.

EDIT: Should also note that workers/supervisor read this zk node after it is written, and the same buffer issue applies. So the childopts for supervisors and workers need to be set in the same manner in addition to nimbus."
STORM-108,Add commit support to Trident Transactional Spouts,"https://github.com/nathanmarz/storm/issues/559

There is no notice from Trident back to the Spout when a batch is successfully completed (for a specific transaction id). When building a Transactional Spout it would be useful to have a success method on the Coordinator to know the batch was completed.

Looking at code: 
On completion of a batch, PartitionedTridentSpoutExecutor's success method is called on the Coordinator, but the Coordinator doesn't do anything. And the ITridentSpout.BatchCoordinator interface doesn't even define a 'success' method.

It looks like what I need to do to complete this code is to:

change the IPartitionedTridentSpout.Coordinator to have a success(long txid) method
change PartitionedTridentSpoutExecutor's success to call the coordinator's success method
Within my own IPartitionedTridentSpout-derived Spout:

have a common state object in the Spout accessible by both my Emitter and the Coordinator
implement the success() method on the Coordinator
when an batch is emitted via emitPartitionBatchNew write information about which messages were included in that batch to the shared state object with the transaction id
when the Coordinator success() method is called, find the transaction and then 'acknowledge' the messages in that batch back to the source.
to handle failures, have the emitPartitionBatch method check a counter in the shared state for the transaction id and fail after 'x' retries. By 'fail' I mean execute my own logic, such as writing to a dead.letter queue, then not output any tuples, thus allowing Trident to advance to the next transactions.
I understand that some messages in the batch may have succeeded when I give up, but I have no way of knowing which ones, so we'll have to handle that in our recovery logic outside of Trident.
Am I missing anything?

Is there something in the TridentSpout lifecycle I haven't figured out by looking at the code? I see a 'success' method on the Coordinator but should there be a complementary 'failed' method as well? I didn't see any retry logic on the calls to emitPartitionBatch either so I'm not sure my failure handling above is correct."
STORM-107,Add better ways to construct topologies,"https://github.com/nathanmarz/storm/issues/649

AFAIK the only way to construct a topology is to manually wire them together, e.g.

{code}
  (topology
   {""firehose"" (spout-spec firehose-spout)}
   {""our-bolt-1"" (bolt-spec {""firehose"" :shuffle}
                            some-bolt
                            :p 5)
    ""our-bolt-2"" (bolt-spec {""our-bolt-1"" [""word""]}
                             some-other-bolt
                             :p 6)})
{code}

This sort of manual specification of edges seems a bit too 1990's for me. I would like a modular way to express topologies, so that you can compose sub-topologies together. Another benefit of an alternative to this graph setup is that ensuring that the topology is correct does not mean tracing every edge in the graph to make sure the graph is right.

I am thinking maybe some sort of LINQ-style query that simply desugars to the arguments we pass into topology.

For example, the following could desugar into the two map arguments we're passing to topology:

{code}
(def firehose (mk-spout ""firehose"" firehose-spout))
(def bolt1 (mk-bolt ""our-bolt-1"" some-bolt :p 5))
(def bolt2 (mk-bolt ""our-bolt-1"" some-other-bolt :p 6))

(from-in thing (compose firehose
                        bolt1
                        bolt2)
  (select thing))
{code}

Here from-in is pulling thing out of the result of compose'ing the firehose and the bolts, forming the topology we saw before. mk-spout would register a named spout spec, and the from macro would return the two dictionaries passed into topology.

The specification needs a lot of work, but I'm willing to write the patch myself once it's nailed down. The question is, do you want me to write it and send it off to you, or am I going to have to build a storm-tools repot to distribute it?


----------
mrflip:We have an internal tool for describing topologies at a high level, and though it hasn't reached production we have found:
1. it definitely makes sense to have one set of objects that describe topologies, and a different set of objects that express them. 
2. it probably makes sense to have those classes generate a static manifest: a lifeless JSON representation of a topology.

To the first point, initially we did it like storm: the FooEacher class would know how to wire itself into a topology(), and also know how to Foo each record that it received. We later refactored to separate topology construction from data handling: there is an EacherStage that represents anything that obeys the Each contract, so you'd say flow do source(:kafka_trident_spout) > eacher(:foo_eacher) > so_on() > and_so_forth(). The code became simpler and more powerful.
() Actually in storm stages are wired into the topology, but the issue is that they're around at run-time in both cases, requiring serialization and so forth.

More importantly, it's worth considering a static manifest.

The virtue of a manifest is that it is universal and static. If it's a JSON file, anything can generate it and anything can consume it; that would meet the needs of external programs which want to orchestrate Storm/Trident, as well as the repeated requests to visualize a topology in the UI. Also since it's static, the worker logic can simplify as it will know the whole graph in advance. From my experience, apart from the transactional code, the topology instantiation logic is the most complicated in the joint. That feels justifiable for the transaction logic but not for the topology instantiation.

The danger of a manifest is also that it is static -- you could find yourself on the primrose path to maven-style XML hell, where you wake up one day and find you've attached layers of ponderous machinery to make a static config file Turing-complete. I think the problem comes when you try to make the file human-editable. The manifest should expressly be the porcelain result of a DSL, with all decisions baked in -- it must not be a DSL.

In general, we find that absolute separation of orchestration (what things should be wired together) and action (actually doing things) seems painful at design time but ends up making things simpler and more powerful."
STORM-106,storm stop emitting tick tuples unreasonably,"https://github.com/nathanmarz/storm/issues/442

Hi, I use tick tuples to do some data storage job every 60 secs, but topology stopped emitting tick tuples unreasonably after running for about 10 hours and i didn't find any error logs.
do you have any idea about how could this happen?"
STORM-105,storm.py should turn off sysout to avoid python code interfering with data exchange,"https://github.com/nathanmarz/storm/issues/351

I ran across this issue because I was trying to wrap an existing python script as a bolt and it had prints in it. This will mess up the tuple emits from the python bolt. I recommend that you add some code like this to the storm.py script to avoid the problem...

    #turn off stdout to avoid messing with JSON payload sent back to JAVA layer
    original_stdout = sys.stdout  # keep a reference to STDOUT
    sys.stdout = NullDevice()

    try:
        //callout to bolt process method
    finally:
        sys.stdout = original_stdout; # turn back on to allow emit to work"
STORM-104,Debugging exceptions where the workers die instantly.,"https://github.com/nathanmarz/storm/issues/410

I was debugging an issue with storm library version incompatibility, and the storm UI just showed a spout being reassigned repeatedly with no exceptions list. With storm-mesos it's extremely hard to find the logs directory when it's been reassigned. How can we make this easier to debug?

Either full history of all the logs directories used in storm-mesos between reassignments, or storm UI augmented with all exception data? Not just reportError."
STORM-103,Notify spout nodes from within the topology,"https://github.com/nathanmarz/storm/issues/401

We got an usecase where it will useful and convenient to send ticks periodically to spout nodes in storm. This would be similar to current ticker nodes but extending them to send the ticks not only to bolt nodes but to spouts as well."
STORM-102,Nimbus service throws unchecked Runtime Exceptions for faulty file uploads and downloads.,"https://github.com/nathanmarz/storm/issues/482

Nimbus' File IO methods defined in the Thrift file:
beginFileUpload
uploadChunk
finishedUpload
beginFileDownload
downloadChunk

Instead of throwing a RuntimeException for faulty requests, throw instead a checked Exception like:
IOException,
FileNotFoundException,
WrongIDException,
etc.

These would need to be defined in the Thrift file also. It would also be nice to have a fileExists(1:string file) method as well. This will allow the use of Nimbus as a central location for resources in dynamically created Topologies without the need for a separate proprietary daemon."
STORM-101,Merging two streams with the same fields in a different order results in the wrong values being returned when getting tuple values by index,"https://github.com/nathanmarz/storm/issues/688

Given the following topology (Constant is a function that simply injects a constant into the tuple):

    TridentTopology topology = new TridentTopology();

    List<Object> tuple = Arrays.<Object>asList(10);

    Stream stream = topology.newStream(""spout"", new FixedBatchSpout(new Fields(""trigger""), 1, tuple));

    Stream s1 = stream
            .each(new Constant(""StringVal""), new Fields(""string""))
            .each(new Constant(20), new Fields(""number""));

    Stream s2 = stream
            .each(new Constant(20), new Fields(""number""))
            .each(new Constant(""StringVal""), new Fields(""string""));

    topology.merge(s1, s2).each(new Fields(""string""), new Debug());
What I expect to be output:

DEBUG: [StringVal]
DEBUG: [StringVal]

What is actually output:

DEBUG: [StringVal]
DEBUG: [20]

I expect the field selector for the input to Debug to pick the first field for the tuple from s1, and the second field for the tuple from s2."
STORM-100,MultiReducerProcessor should pass the context,"https://github.com/nathanmarz/storm/issues/747

this prepare call should add argument to pass the _context object to _reducer, so _reducer can get something from this _context, such as the taskid of this _reducer

i don't want to modify the interface of MultiReducer, so pass the taskid to reducer like this:

image"
STORM-98,.stateQuery twice halts tuple execution?,"https://github.com/nathanmarz/storm/issues/310

Having the following example, it will never execute the .aggregate()


FixedBatchSpout spout = new FixedBatchSpout(new Fields(""sentence""), 3,
        new Values(""cow""),
        new Values(""candy""),
        new Values(""year""));

spout.setCycle(true);

TridentTopology topology = new TridentTopology();

TridentState urlToTweeters =
        topology.newStaticState(
                new TridentReach.StaticSingleKeyMapState.Factory(TridentReach.TWEETERS_DB));

Stream wordStream = topology.newStream(""spout1"", spout)
        .each(new Fields(""sentence""), new Split(), new Fields(""word""))
        .stateQuery(urlToTweeters, new Fields(""word""), new MapGet(), new Fields(""output1""))
        .groupBy(new Fields(""word""))
        .stateQuery(urlToTweeters, new Fields(""word""), new MapGet(), new Fields(""output2""))
        .aggregate(new Fields(""word""), new PrintAggregator(), new Fields(""count""));

PrintAggregator:

public static class PrintAggregator extends BaseAggregator<PrintAggregator.State> {

    static class State {
        int counter = 0;
    }

    @Override
    public State init(Object o, TridentCollector collector) {
        return new State();
    }

    @Override
    public void aggregate(State state, TridentTuple tuple, TridentCollector collector) {
        state.counter++;
        System.out.println(tuple.getString(0) + "" is on: "" + state.counter);
    }

    @Override
    public void complete(State state, TridentCollector collector) {
        collector.emit(new Values(state.counter));
    }

}

----------
nathanmarz: Trident currently doesn't support recursive topologies. In this case you have the output of a state feeding back into a query on the same state. You can workaround this by making two separate static state instances for urlToTweeters."
STORM-97,Setting custom log levels per topology,"https://github.com/nathanmarz/storm/issues/149

There is a log4j.xml that packaged in MyTopology.jar , when I startup the supervisor node, the task's logger use the config under the storm/log4j/storm.log.properties instead of my log4j.xml in jar .

Otherware, I want to dynamic adjust the logger level on the fly. for example, I run a normal java application, the first thing is load logger config. for example:

String log4jFile = System.getProperty(""log4j.configuration"",""log4j.xml"");
if (!(new File(log4jFile).isFile())) return;

String refreshInterval = System.getProperty(""log4j.refreshInterval"");
long interval = StringUtils.isNotBlank(refreshInterval) ? Long.valueOf(refreshInterval) : 2000;
try {
    Log4jConfigurer.initLogging(log4jFile, interval);
} catch (FileNotFoundException e) {
    throw new RuntimeException(""no find the log4j file. file = "" + log4jFile, e);
}
Use the above code, I can change the logger level and don't redeploy the application."
STORM-96,_msgIdToBatchId in RichSpoutBatchTriggerer.java did not give the right value,"https://github.com/nathanmarz/storm/issues/734

_msgIdToBatchId did not give the right value, and the fail/ack in the underline Spout will be never called, and the _finishConditions will never released.

The fix will be like follows in RichSpoutBatchTriggerer.java

    public List<Integer> emit(String ignore, List<Object> values, Object msgId) {
        long batchIdVal = _rand.nextLong();
        Object batchId = new RichSpoutBatchId(batchIdVal);
        FinishCondition finish = new FinishCondition();
        finish.msgId = msgId;
        List<Integer> tasks = _collector.emit(_stream, new ConsList(batchId, values));
        Set<Integer> outTasksSet = new HashSet<Integer>(tasks);
        for(Integer t: _outputTasks) {
            int count = 0;
            if(outTasksSet.contains(t)) {
                count = 1;
            }
            long r = _rand.nextLong();
            _collector.emitDirect(t, _coordStream, new Values(batchId, count), r);
            finish.vals.add(r);

            //Ming Li: Adding the follow line to init the _msgIdToBatchId with correct value
            _msgIdToBatchId.put(r, batchIdVal);
        }
        _finishConditions.put(batchIdVal, finish);
        return tasks;
    }"
STORM-95,Topology hangs with worker processor threads TIMED_WAITING. Edit,"https://github.com/nathanmarz/storm/issues/763

Hi Nathan,

We are this issue very frequently now while using Storm 0.8.2. There are no errors in any worker logs/ supervisor.log/nimbus.log . However the topology stops processing the tuples.
On collesting the thread dump of the worker processor we can see all the threads are going into TIMED_WAITING states and toplogy hangs.

The following is the brief on our toplogy.

We are using BaseIRich Spout and bolts.
We have one file reader spout and three processing bolts.(24, 48 and 24 executors)
Each tuple will contain 100 messages of size 10kb each totaling 1mb.
We aim to process 30 mil such records within 6 hrs.
We are running it on SUSE Linux 11 entreprise server.
We are using all the recomended versions (Storm 0.8.2,Java 1.7, Zookeeper 3.4.5, ZeroMQ - 2.1.7, JZMQ-)
Below are the list of variuos combination of the storm configuration we tried.

Conf -3

worker.childopts: ""-Xmx3072m""
topology.acker.executors: 20
topology.max.spout.pending: 50
topology.message.timeout.secs: 300
topology.executor.receive.buffer.size: 16384 #batched
topology.executor.send.buffer.size: 16384

Conf-2

worker.childopts: ""-Xmx3072m""
topology.acker.executors: 20
topology.max.spout.pending: 300
topology.message.timeout.secs: 300
topology.executor.receive.buffer.size: 16384 #batched
topology.executor.send.buffer.size: 16384

Conf-1

worker.childopts: ""-Xmx3072m""
topology.acker.executors: 20
topology.max.spout.pending: 1000
topology.message.timeout.secs: 300

Also attaching the thread dumps for your reference.

We desperately need your help to resolve this issue as we are looking to go live soon."
STORM-94,Extend QueryFunction API to support paging based queries,"https://github.com/nathanmarz/storm/issues/553

Suppose some state holder supports paging or some sort of bulk iteration of its query results. In the current solution the QueryFunction must get all of the results for the input tuples before pushing the results further in the topology. It would be preferable to use some sort of PagedQueryFunction API which will push the pages further into the topology as they are fetched, page by page."
STORM-93,Storm UI breaks when proxied,"The Storm UI doesn't render properly when placed behind a proxy. The UI uses absolute paths to refer to it's stylesheets and JavaScript. As a result, unless you proxy it from the root URL it will fail to load properly. Ideally, the Storm UI resources would refer to relative locations.

For example: In our case, we proxy the UI under /storm. Unfortunately the Storm UI refers to /css/style.css instead of /storm/css/style.css.

Possible solution:
In the https://github.com/nathanmarz/storm/blob/24106aa13ded590c0d9ab747d8bf1a149414244a/src/clj/backtype/storm/ui/core.clj#L40 an additional parameter for ""depth"" could passed that provides the proper number of ../ in front of the resource path."
STORM-92,"storm should permit storing configuration, logs outside of the release dir","https://github.com/nathanmarz/storm/issues/432

the bin/storm script enforces a requirement that logs and conf be in the release directory itself.

It's quite common for sysadmins to require that logs be sent to different file systems, that configuration be managed separately from software installs, and that one software install be usable with multiple configurations, either on the same host, or when provided by a network file system.

One can hack around this by supplying symlinks for logs and conf, but allowing users to specify the location of these directories seems a tad more elegant.

I can write this up if a PR is likely to be accepted."
STORM-91,Registering already registered serializations causes strange runtime errors,"https://github.com/nathanmarz/storm/issues/279

e.g. if registering String:

java.lang.RuntimeException: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: char[]
Note: To register this class use: kryo.register(char[].class);
Serialization trace:
value (java.lang.String)
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:82)
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:55)
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:56)
    at backtype.storm.disruptor$consume_loop_STAR_$fn__1596.invoke(disruptor.clj:67)
    at backtype.storm.util$async_loop$fn__465.invoke(util.clj:377)
    at clojure.lang.AFn.run(AFn.java:24)
    at java.lang.Thread.run(Thread.java:662)
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: char[]
Note: To register this class use: kryo.register(char[].class);
Serialization trace:
value (java.lang.String)
    at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:495)
    at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
    at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:554)
    at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:77)
    at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
    at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:472)
    at backtype.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:27)
    at backtype.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:27)
    at backtype.storm.daemon.worker$mk_transfer_fn$fn__4120$fn__4124.invoke(worker.clj:99)
    at backtype.storm.util$fast_list_map.invoke(util.clj:770)
    at backtype.storm.daemon.worker$mk_transfer_fn$fn__4120.invoke(worker.clj:99)
    at backtype.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__3898.invoke(executor.clj:205)
    at backtype.storm.disruptor$clojure_handler$reify__1584.onEvent(disruptor.clj:43)
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:79)
    ... 6 more
Caused by: java.lang.IllegalArgumentException: Class is not registered: char[]
Note: To register this class use: kryo.register(char[].class);
    at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:426)
    at com.esotericsoftware.kryo.Kryo.getSerializer(Kryo.java:446)
    at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:477)
    ... 19 more
This error catching should happen at registration time (ideally in Kryo itself). Or, even better, Storm should allow duplicate registrations and skip any duplicates when initializing Kryo. This is consistent with how serialization registration works w.r.t. component configs."
STORM-90, Make it possible to use the ShellBolt without having the need to have resources,"https://github.com/nathanmarz/storm/issues/678

Hi,

there seems to be a problem in the ShellBolt that I would like to figure out. If you use Storm multilanguage features and you try to communicate with a C++ using something like StormCpp it will possibly fail as long as you embed the resource direclty (Also see nathanmarz/storm#229). But if you just have a simple running executable that is placed in a PATH folder (like /usr/local/bin on Linux) it will fail with ""Caused by: java.io.IOException: Cannot run program ""xy"""". The problem seems to be the ShellProcess - class where it does something like setting the directory to the codeBaseDir which is set to the resource - folder. As long as you have not assigned a resource - folder in your maven file it will always fail. Something like:

<build>
<resources>
<resource>
<directory>${basedir}/SomeDirectory</directory>
</resource>
</resources>
</build>

will fix it but it's only a workaround. I can also provide you with a simple code-example if needed. A possible solution would be that the ShellProcess should check if the codeBaseDir exists so that I cannot fail."
STORM-89,Can't use more complex types in the topology configuration,"https://github.com/nathanmarz/storm/issues/441

Storm's use of json-simple means that you can't use any type more complex than Map/Array for values in the topology configuration. It would be nice to switch to something like Jackson for json writing/parsing (and coincidentally, it also supports Yaml) of the configuration."
STORM-88,ICommitterTridentSpout.commit can't fail a batch,"https://github.com/nathanmarz/storm/issues/643

Currently, ICommitterTridentSpout.commit can't fail a batch, as the logic calling that method doesn't catch FailedException. Without being able to fail a batch, there's little difference between the commit and success methods."
STORM-85,IsolationScheduler fails due to NoSuchElementException,"https://github.com/nathanmarz/storm/issues/738

Repro step:
1. update Nimbus config, add following lines, and re-start Nimbus

storm.scheduler: backtype.storm.scheduler.IsolationScheduler

isolation.scheduler.machines: 
    ""WordCountIso"": 5
Nimbus failed to re-start and ""NullPointerException"" found in log files:
2013-10-28 00:46:32 TNonblockingServer [ERROR] Unexpected exception while invoking!
java.util.NoSuchElementException
at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
at java.util.HashMap$KeyIterator.next(Unknown Source)
at backtype.storm.scheduler.IsolationScheduler$remove_elem_from_set_BANG_.invoke(IsolationScheduler.clj:140)
at backtype.storm.scheduler.IsolationScheduler$schedule.invoke(IsolationScheduler.clj:190)
at backtype.storm.scheduler.IsolationScheduler.schedule(Unknown Source)
at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
at backtype.storm.daemon.nimbus$computenew_topology__GT_executor__GT_node_PLUS_port.invoke(nimbus.clj:587)
at backtype.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:674)
at clojure.lang.RestFn.invoke(RestFn.java:410)
at backtype.storm.daemon.nimbus$fn__3812$exec_fn__1239__auto__$reify__3827.submitTopologyWithOpts(nimbus.clj:965)
at backtype.storm.daemon.nimbus$fn__3812$exec_fn__1239__auto__$reify__3827.submitTopology(nimbus.clj:973)
at backtype.storm.generated.Nimbus$Processor$submitTopology.getResult(Nimbus.java:1223)
at backtype.storm.generated.Nimbus$Processor$submitTopology.getResult(Nimbus.java:1211)
at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:32)
at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:34)
at org.apache.thrift7.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:632)
at org.apache.thrift7.server.THsHaServer$Invocation.run(THsHaServer.java:201)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)

Possible root cause:
missing null checking in IsolationScheduler.remove-elem-from-set!() and IsolationScheduler.-schedule()

fixing:
/src/clj/backtype/storm/scheduler/IsolationScheduler.clj

line 140-142:

   (if (-> aset .iterator .hasNext)                         // add hasNext() checking here
    (let [elem (-> aset .iterator .next)]
      (.remove aset elem)
      elem
      )))

line 191:

          (if (not-nil? executors-set)                // add null checking here. if missing this line, there will be NullPointerException when call cluster.assign()
            (.assign cluster slot top-id executors-set)))"
STORM-84,Don't use hashCode for fields grouping,"https://github.com/nathanmarz/storm/issues/244

The contract of hashCode does not guarantee that it will be consistent across JVM's, so it's just a coincidence that it happens to work. We need a new way to do hashcodes. Hopefully there's an open source library that already solves this problem, otherwise we'll have to build a solution for this to get hashcodes for arbitrary objects.

A tempting way to do it would be to hash the serialized form of an object, but this has two issues:

1. Equal objects are not guaranteed to have same serialized form (e.g., it represents a set and elements are serialized out of order
2. We don't want to incur the serialization cost when sending between tasks colocated in the same worker"
STORM-83,zeromq send can cause divide by 0 in multithreaded cod,"https://github.com/nathanmarz/storm/issues/474

When running the unit tests I got a SIGFPE (FPE_INTDIV) in zmq::lb_t::send(zmq_msg_t*, int)+0x176 I traced this down to line 137 of lb.cpp in version 2.1.7 of zeromq. The error is a divide by 0 error. The denominator in this code is an active count, and about 7 lines above active passed a check for 0. zmq::lb_t is very much not thread safe, but it looks like we are somehow calling a single instance of it from multiple threads."
STORM-82,Please support settings for changing the worker launching command,"https://github.com/nathanmarz/storm/issues/124

The `collect' tool in Oracle Solaris Studio can be used to profile Java applications, but it requires the command line to transform from

java ....

to

collect -j on java ....

Currently, the only simple, non-compilation way to use collect' with Storm is to write a wrapper aroundjava' to invoke `collect' as needed. Please support settings for changing the command to launch a worker, so that there is no need of such a wrapper."
STORM-81,"BUG: In local topology, ShellSpout attempts to execute in a non-existent directory","https://github.com/nathanmarz/storm/issues/569

Attempting to execute code in a ShellSpout under a local topology causes a Java runtime exception with the text ""Error when launching multilang subprocess.""

The inner exception has text in the form of ""Cannot run program ""jruby"" (in directory ""C:\Users\jake\AppData\Local\Temp\c2581165-37ba-402a-ad3c-38d3544d259c\supervisor\stormdist\mauth-firehose-1-1368550578\resources""): CreateProcess error=267, The directory name is invalid."" The path in question does not exist beyond C:\Users\Jake.

This has been reproduced under Windows 7 using JRE 1.7 with JRuby 1.7.3 and Ruby 1.9.3."
STORM-80,NPE caused by TridentBoltExecutor reusing TrackedBatches between batch groups,"https://github.com/nathanmarz/storm/issues/421

I'm seeing intermittent errors caused by SubtopologyBolt.execute being called with a BatchInfo whose ProcessorContext is set up for a different Batch Group. In particular I'm seeing null pointer exceptions from PartitionPersistProcessor because its state fields were never set up correctly.

The best I can tell the id key (IBatchID) being used for the _batches map in TridentBoltExecutor is not unique between batch groups. As a result the tracked batch will have been initialized for a different Batch Group and set of processors.

I hoped to be able to track down the source of this issue but can't determine where the BatchIDs are being added to the tuples.

If it matters, my topology has two streams each reading from their own OpaqueTransactionalKafka spout w/different topics.

Backtrace:

65108 [Thread-25] ERROR backtype.storm.daemon.executor - 
java.lang.RuntimeException: java.lang.NullPointerException
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:87) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:58) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.daemon.executor$fn__3551$fn__3563$fn__3610.invoke(executor.clj:712) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.util$async_loop$fn__436.invoke(util.clj:377) ~[storm-0.9.0-wip4.jar:na]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:722) [na:1.7.0_09]
Caused by: java.lang.NullPointerException: null
        at storm.trident.planner.processor.PartitionPersistProcessor.execute(PartitionPersistProcessor.java:59) ~[storm-0.9.0-wip4.jar:na]
        at storm.trident.planner.SubtopologyBolt$InitialReceiver.receive(SubtopologyBolt.java:189) ~[storm-0.9.0-wip4.jar:na]
        at storm.trident.planner.SubtopologyBolt.execute(SubtopologyBolt.java:129) ~[storm-0.9.0-wip4.jar:na]
        at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:352) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.daemon.executor$fn__3551$tuple_action_fn__3553.invoke(executor.clj:607) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__3474.invoke(executor.clj:379) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.disruptor$clojure_handler$reify__3011.onEvent(disruptor.clj:43) ~[storm-0.9.0-wip4.jar:na]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84) ~[storm-0.9.0-wip4.jar:na]
        ... 6 common frames omitted

Also, I'm only seeing this in LocalCluster mode, not in production."
STORM-79,Don't use Java serialization for storing state on disk or in Zookeeper,"https://github.com/nathanmarz/storm/issues/419

Java serialization causes problems when things are upgraded and we want to make changes to the stored format. Even if the change is backwards compatible semantically, Java will complain about a version mismatch. instead we should use JSON or clojure serialization."
STORM-78,Storm UI Needs HTTP Auth,"https://github.com/nathanmarz/storm/issues/452

When we start storm ui, any one can access it by ip / port. It would be good if there is a HTTP Auth for basic security.

----------
Jagaran: We are also implementing Storm for one of our Client.
We also need to secure HTTP based Storm UI ? Any updates or future road map for implementing this Security ??

----------
Jargaran: I can also contribute to this part in Storm ? let me know the steps

----------
lockwobr: I would also really find the feature useful. when you are in cloud you can't just let this kind of thing hanging in the wind.

----------
tvpavan: @Jagaran Do you have a fork which has this fix ?"
STORM-77,Nimbus should ignore topologies for which it's missing code or configs rather than kill them,"https://github.com/nathanmarz/storm/issues/411

It should also log errors. Let's have a new list of topologies in the UI called ""Corrupt topologies"" that displays only if there are corrupt topologies


---------------
jasonjckn: This is the case where storm-local directory is inconsistent with zookeeper state (e.g. missing jars for a topology in active state). Atm Nimbus reaches a consistent state by deleting zookeeper state, the suggest here is to instead to reach consistency by marking that particular zookeeper state as corrupt, and have the UI display this fact."
STORM-75,Dead lock between ShellBolt and ShellProcess,"https://github.com/nathanmarz/storm/issues/423

The ShellBolt creates shell process and read data from output stream and error stream. The current implementation only read error stream when the output stream is closed. So messages in error stream will be put into the buffer of error stream. When the buffer is fully filled, the output in shell process would be blocked waiting for the error stream buffer to become available. While in ShellBolt it will also block there wait for the output in output stream from shell process. So it's a dead lock.

This behavior seems dangerous as the issue can be hidden, it can hardly be seen in normal tests. And normally the error output won't be too big to fill up the error stream buffer, but after the system have been running for a while on production, the error stream can be accumulated to full, and then dead lock would happen. There's no any error in log, hard to debug.

Here in Yahoo we are using many native libraries which is built long time ago, which sometimes writes to error stream when there's some error. It's impossible for us to inspect all the direct and indirect native library dependencies and rebuild all to remove all the error stream writing.

Now we used a workaround to redirect error stream into /dev/null at the beginning of our shell process. But I think in long term it should be fixed in ShellBolt and ShellProcess."
STORM-74,LocalDRPC.execute never returns after timeout,"https://github.com/nathanmarz/storm/issues/574

LocalDRPC.execute should ideally raise an Exception if the DRPC call times out for whatever reason. Current observed behaviour is to not return.

Spent far too long trying to identify a problem with a local topology whereby DRPC calls were simply never returning. It ended up being a simple issue of not setting the message timeout value high enough (via setMessageTimeoutSecs), as the topology was taking over a minute to complete. But - frustratingly - there was no indication that this was the cause.

(I've observed this behaviour on both 0.8.2 and 0.9.0-wip17)"
STORM-73,TridentTopology with a RichSpout and tuples that fail,"https://github.com/nathanmarz/storm/issues/323

Please see the following gist for a testcase: https://gist.github.com/3679478

Tested this with 0.8.1

What this Spout does is keep a counter for the tuples it emits and splits them into sequences. The the ack() and fail() methods expect that they receive exactly the number of ack or fail calls you would expect as emited for each sequence.

But watching the console, after around 13 sequences:

Acking sequence: 0
Acking sequence: 1
Acking sequence: 2
Acking sequence: 3
Acking sequence: 4
Acking sequence: 5
Acking sequence: 6
Acking sequence: 7
Acking sequence: 8
Acking sequence: 9
Acking sequence: 10
Acking sequence: 11
Acking sequence: 12
Acking sequence: 13
Failing sequence: 15
Some sequence didn't fail completly!! {14=396}
Failing sequence: 16
Some sequence didn't fail completly!! {14=396}
Failing sequence: 17

etc... etc.. Keeps failing from now on

-----------
barrywhart: This sounds very similar to a problem we were having last week on 0.8.0 on a conventional topology (non-Trident) written in Python. The spout would hang forever after about 100 messages.

In our case we were able to downgrade to 0.7.4, and it has been working fine since."
STORM-68,Need multiple output streams for a bolt in Trident,"https://github.com/nathanmarz/storm/issues/638

Nathan suggested to open an issue for this:

Transactional Topologies gave us the flexibility to have multiple output streams from a bolt from which different downstreamer bolts can subscribe per bolt per stream; while in Trident, though the bolt can take multiple input streams, it is not possible (as per our understanding) to emit different streams. All downstreamer bolts have to subscribe to the same output stream from the bolt.
The only way to solve this in the current system is to tunnel multiple streams in the outgoing stream, which all downstreamers receive and then demultiplex it in each of those bolts and select whatever stream the bolt is interested in. This will have perf implications for our usecases which deals with good amount of traffic.

The discussion:
https://groups.google.com/forum/#!topic/storm-user/G8POD1Hb89I"
STORM-64,Add Handshake when establishing new connection to worker.,"https://github.com/nathanmarz/storm/issues/503

Right now it's possible for a worker to connect to a worker of a different topology during a reassignment. Generally this happens very rarely in practice, and when it does causes ClassNotFoundExceptions.

Add in a handshake when worker connection is first established, and ensure they agree on topology id. If they don't agree they shouldn't transfer any data, but neither should halt their process."
STORM-62,Update Jetty to latest version,"Currently Storm uses Jetty 6, I'd suggest to upgrade this to a more current version, e.g. Jetty 9 or even 8.

Actually I noticed that the dependency comes from Ring, perhaps it's an idea to see if that can't be updated?"
STORM-61,Compatibility matrix,"Would be very helpful to include a compatibility matrix as one of the wiki pages. It should include such fields as platform (linux, windows), java version, storm version, zookeeper version, zeromq version."
STORM-60,Document Thrift APIs,https://github.com/nathanmarz/storm/issues/61
STORM-59,Support status messages on Storm UI,"https://github.com/nathanmarz/storm/issues/342

Set the status message as a string through the config.

Status message should display by itself if Nimbus is down, otherwise display on the top of the cluster UI

-----------------------------
jasonjckn: One common issue is zookeeper goes down, it'd be nice if the status message reported ZK online/offline.

Also, the most common instant message I get are ""when will it be up?"" and ""if I'm working on the problem"" can the status message be dynamically set?

-----------------------------
nathanmarz: We can just update the config file and have the UI reload the config each time it receives a request.


-----------------------------
jasonjckn: In the situation where ZK is offline. I don't want to have to manually set the status message about ZK online/offline. I want to be the consumer of this information. It'd be great, if the error page was server rendered, and we did a quick ZK connect, to see if it exists. The code could be easily added to src/clj/backtype/storm/ui/core.clj

We could still have a status message in the config, which would be read by error page in ui/core.clj.

-----------------------------
nathanmarz: Good idea. This would be in addition to a status message. We can call this ""health checks.""
"
STORM-58,Provide worker pids via Thrift API,"https://github.com/nathanmarz/storm/issues/418

Workers are writing this to a pid file, the supervisor just needs to report it in its heartbeat.


--------------------
 NJtwentyone: Let me get accustom to clojure first, I'll take a crack at it."
STORM-57,Make Storm handle ByteBuffers as fields values natively,"https://github.com/nathanmarz/storm/issues/199

Just need to make a ByteBuffer serializer and register it with Kryo as default.

-------------------------
brianmartin: I've written most of this but have run into a problem.

ByteBuffer's are often HeapByteBuffer's, which is a private to java.nio. Therefore, it seems like even though ByteBuffer is registered, Kryo goes to the default serializer anyway because HeapByteBuffer is not registered.

I'm not sure what a reasonable workaround for this would be.

patch draft w/ test: https://gist.github.com/3094071


--------------------------
nathanmarz: Is there any sort of reflection trick you could use to get ahold of the HeapByteBuffer class and register it?

--------------------------
brianmartin: Ah, thanks for the tip.

I've submitted a pull request for this. Apologies for not being able to attach the request to this issue (if that's possible).

Let me know if any changes or contributor agreement are needed."
STORM-56,"Provide support for handling ""bad data""","https://github.com/nathanmarz/storm/issues/13

Examples:

1. Scheme can't deserialize the tuple
2. An object that serializes but can't be deserialized. From Sam Stokes: ""I've seen JSON libraries that incorrectly serialised strings containing multi-byte characters, and then unsurprisingly weren't able to parse the resulting byte soup. ""

This could be as simple as providing an exception type for deserialization problems (InvalidTupleException) and a Storm config for skipping bad data. Perhaps there can also be an implicit stream where those bad tuples are sent as binary data. With the implicit stream, applications can do something with the bad data like record it somewhere.

-------------------
malur: This would be very useful. Does it make sense to have an error handler bolt at different levels like spout and topology?

------------------
nathanmarz: Yes, it does. There's already a planned feature called ""failure streams"" for spouts: an implicit stream where all failed spout tuples are sent to. Bad data could be sent to another kind of failure stream."
STORM-55,Replace Storm's scheduler with a constraint logic programming engine,"https://github.com/nathanmarz/storm/issues/383

CLP seems to be a great fit for Storm's resource scheduling. We want to be able to declaratively specify constraints, such as:

1. Topology A's slots should be <= 10 and as close to 10 as possible (minimize the delta between assigned slots and 10)
2. All topologies should use less than 200 CPU's and less than 600 GB of memory
3. Topology B should run at most 2 workers on each host
4. Each worker for topology C should run at most one task for component X and one task for component Y
5. Should minimize the amount of reassignment to running topologies in order to satisfy constraints
6. Should only be allowed to reassign workers for an individual topology whose individual constraints are satisfied once every 10 minutes

And then the logic engine should do an efficient search and optimize the constraints.

What's going to make this especially powerful is exposing the logic engine to users (so people can plug in functions that return new constraints), making it really easy for people to add sophisticated scheduling logic with minimal effort.

Clojure's core.logic may be a great fit for this. According to this thread it doesn't do minimization goals ( https://groups.google.com/group/clojure/browse_thread/thread/a050a4e6b390514a ), but it may in the future.

-------------------------------------------
@nathanmarz: There also appears to be a fair amount of research in this area that we should look at: http://scholar.google.com/scholar?q=resource+scheduling+constraint+logic+programming&hl=en&as_sdt=0&as_vis=1&oi=scholart&sa=X&ei=X1uIUK6gA4rBigL5yoCgBg&ved=0CD0QgQMwAA


-------------------------------------------
bgedik: It would be good if the default scheduler understands placement constraints. Would be very helpful if the placement constraints can be attached to the topology, relieving the developer from having to write a scheduler to get the placement she wants.

The placement constraints can come in various forms. Here are some examples:

Isolation constraint: The host/spout task cannot share its host with any other task
Co-location constraint: A group of host/spout tasks has to be placed on the same host
Ex-location constraint: A group of host/spout tasks cannot be placed on the same host
Pool placement: A group of host/spout tasks are sprayed over a pool of hosts
Explicit placement: A host/spout task is placed on a user-specified host
Pools can be defined using host properties. For instance, configuration files can be used to assign properties to hosts. As a specific example, some hosts can have GPUs installed. As an application developer, I can request 4 such hosts (with gpu=true property) to create a pool in my topology construction code (without hardcoding the identities of the hosts). Then I can spray some of my bolt instances across the hosts in my GPU pool."
STORM-53,Add interface and StateUpdater for appending to a queueing system,"This makes sense to be in Storm core since it's simple and there will be many external implementations (e.g. Kestrel, Kafka, etc.)


----------------
koshelev: I will implement such a functionality for a project i'm currently working on and would like to contribute it, along with implementations for JMS, AMQP (RabbitMQ) and Redis Pub/Sub. I would suggest following interfaces for the queueing system: https://github.com/koshelev/storm/tree/master/src/jvm/backtype/storm/queueing."
STORM-52,Implement DRPC futures,"Need ""executeAsync"", ""fetch"", and possibly other methods to support the Java Future API."
STORM-51,DRPC client pool,"DRPC clients should really be working via a pool to manage connection resources and load against the DRPC server. Opening this ticket to track this PR -https://github.com/nathanmarz/storm/pull/584

There's a couple of other issues that could be helped along with a pool, for example, https://github.com/nathanmarz/storm/pull/430, or async requests as per https://github.com/nathanmarz/storm/pull/62"
STORM-50,Refactor DRPC logic,"https://github.com/nathanmarz/storm/issues/392

The idea here is to refactor the DRPC logic from backtype.storm.daemon.drpc so that logic can be plugged into specific server implementations. So you might have the standard Thrift implementation, and then separate projects can create other implementations for things like Finagle."
STORM-49,Make State implement Closeable,"https://github.com/nathanmarz/storm/issues/644

It would be useful to be able to 'close' a State object when the topology is being shut down, so that it can cleanly close any connections & resources it is using."
STORM-48,Storm may not work with IPv6,"See this thread and the resolution:

http://groups.google.com/group/storm-user/browse_thread/thread/6eea0a572db23a5a"
STORM-47,add TopologyContext#getNimbusClient,"https://github.com/nathanmarz/storm/issues/445

Remote mode would return a Nimbus client over the network, local mode would return the local Nimbus instance

One use case for this is topologies that shut themselves down when some condition is met. This would let you use the same code in local mode as in cluster mode."
STORM-46,Allow scheduler to configure memory per worker,"https://github.com/nathanmarz/storm/issues/272

Useful if different workers have different memory needs."
STORM-45,Stateful bolts,"https://github.com/nathanmarz/storm/issues/204

This is an idea to build abstractions for bolts with fault-tolerant state (so if a task dies and gets reassigned to another machine it still has its state). The idea is to use a similar strategy as HBase uses: Append changes to local state into a file on a DFS, and occasionally compact the file on the DFS. This would be easier to use and more efficient than using an external database like Cassandra.

This abstraction is actually independent to Storm, it should be done as a separate project or in storm-contrib. It does require a DFS with append functionality, which more recent versions of HDFS might provide (or MapR might provide it). The interface for a map-like state can look something like this:

PersistentMap(String dfsDir) {
    Object get(Object key)
    void put(Object key, Object value);
}

Using transactional topologies, it would probably look more like:

PersistentMap(String dfsDir) {
    Object get(Object key)
    void putAll(Long txid, List pairs);
}

If you colocate your DFS with your Storm cluster, you should get data locality when writing. Having a pluggable scheduler can help with this as well.

The first version should just keep all the state in memory, using the DFS just for reliability.

-----------------------------------------------------------------------------------------------------
SirWellington: This is a good idea. 
What is it that currently occurs when a bolt task fails? Does the source spout re-emit the tuple?

Some questions for implementation: 
What happens while you are re-initiating the bolt with the previous one's state? Will you pause all threads in the topology or will you temporarily increase the timeout for processing a tuple? I am not sure how long it would take to re-start a bolt with state, but it could trigger a fail(), causing a replay.

From the API side:
Does this mean that the prepare() method will not be called?
And finally, will this be optional or seamlessly integrated?

Thank You

-----------------------------------------------------------------------------------------------------
nathanmarz: The tuple trees that are made incomplete due to the bolt task failure will time-out and the spout will be able to replay the source tuple for that tree. Tuples that have already successfully completed will not be replayed. So generally you keep any persistent state in a database, oftentimes doing something like waiting to ack tuples until you've done a batch update to the database. Stateful bolts will just be a much more efficient way of keeping a large amount of state at hand in a bolt.

Good point regarding reinitialization. The first implementation will target amounts of state that can fit into memory, so reinitialization time won't be a concern. Once we look at doing much larger amounts of state we'll revisit this question.

This is orthogonal to the prepare method being called. The prepare method is called whenever a task starts up in a worker, regardless if it existed before in another worker.

This will most definitely be optional. Actually, I think this will end up being a storm-contrib submodule.

-----------------------------------------------------------------------------------------------------
pereferrera: Is this issue outdated as of: https://github.com/stormprocessor/storm-state ? Should it be closed then?

Has anybody tested storm-state in a big setup? Is there a corresponding backing State for Trident?"
STORM-44,Replication,"https://github.com/nathanmarz/storm/issues/132

This is an idea to replicate a computation across many tasks on different machines. The ""replication"" part is already possible since you can implement your own grouping which sends the tuple to multiple tasks. What is needed is help from Nimbus to make sure those tasks run on different machines.

Replicated computation would be useful for doing things like highly available DRPC. Essentially you do the same DRPC multiple times and at the end pick the first one that finishes for the result.

-----------------------------------------------------------------------------------------------------
LiSu: I am trying to implement this replication feature. My idea is to have replica tasks for each primary task. The replica and the primary are receiving the same input and doing the same execution. When the primary is running, the output of the replica will be blocked. After the primary failed, the output of the replica will be used instead. What I am doing now is that I set a switch in the component common(or topology context for each task) to control the blocking. After I listened to the heartbeat of primary stopped from the nimbus, the state of the switch will be changed. But my problem now is that change of the ComponentCommon on nimbus will not reflect in the workers. Do you guys have any ideas that the state of the switch will be announced to the tasks in time ?

-----------------------------------------------------------------------------------------------------
nathanmarz: Trying to coordinate the replicas with each other like this is a dead-end. The point of this feature is that if a replica suddenly dies, there's no loss of availability because the computation is happening anyway on another task. Obviously, replicated tasks require there to be no side effects.
"
STORM-43,Detect and kill blocked tasks,"https://github.com/nathanmarz/storm/issues/230

If a task blocks indefinitely, it would be good if Storm detected this automatically and timed out that worker (like what Hadoop does). Need to be careful to avoid false positives."
STORM-42,Support sending arbitrary data types back and forth through DRPC,"https://github.com/nathanmarz/storm/issues/41

Right now, you can only send Strings back and forth due to the limitations of Thrift. It would be nice if DRPC were structured in a way so that you could put any interface you want on it. So the Thrift interface would be one implementation, and another implementation could just send Java objects back and forth (perhaps by using Kryo).

-----------------------------------------------------------------------------------------------------
thatsnotright: Why would you define the DRPCRequest to have function args as a string? What is the limitation I am not seeing of Thrift? There already exists kryo and java serialization support, why not define func_args to be binary and serialize in to that?

-----------------------------------------------------------------------------------------------------
dmk23: So are there any specific plans to make this feature happen? Thrift obviously can be used to define objects of arbitrary complexity and this would also allow for cross-language DRPC calls.

We currently have a set of custom Thrift services written in Java that are called from Python clients. We'd be interested in converting our Java backend into Storm topologies but RPC data type limitation is a significant barrier, especially given that this was to work across languages.

-----------------------------------------------------------------------------------------------------
nathanmarz: Yea, the right solution to this is to just do binary inputs and outputs through DRPC. Actually it should be something like:

list<list<binary>> execute(string function, binary args)

Then you can do the serialization/deserialization around that. The return type there is because Trident returns a list of tuples for the output of drpc, where each tuple is a list of values. Each value is in serialized form here.


-----------------------------------------------------------------------------------------------------
dmk23: I understand anything can be manually serialized / deserialized and pushed through the existing ""execute"" call.

But this looks quite clumsy / redundant since these DRPC service definitions could be easily expressed with Thrift, then converted into easy-to-use generated code in any language and imported into Storm through Java. Given that Thrift is already used within Storm this seems like an obvious solution.

Would be nice to have support for having Storm import arbitrary Thrfit service definition and have DRPC services natively expose interfaces / data structures as defined in those IDLs...

-----------------------------------------------------------------------------------------------------
suchetchachra:A temporary workaround for the string size limitation (64K) could be to save the object to be returned to a distributed cache and return the key (String). On the client side, the key can be used to retrieve the object from the cache.

-----------------------------------------------------------------------------------------------------
dimazaur: Hello, Nathan.
Is there any plans about drpc binary support?
We are thinking about move to Storm system on our servers.
And this String type limitation is blocking us.
Thanks a lot."
STORM-41,show worker's working directory in storm UI,"https://github.com/nathanmarz/storm/issues/345

Show worker's working directory in storm UI. This helps you locate log files quickly."
STORM-40,Turn worker GC logging and heapdump on by default,"https://github.com/nathanmarz/storm/issues/492

This would be an update to the default worker.childopts. The logs should be configured to have a ceiling on the amount of space they use.

-----------------------------------------------------------------------------------------------------
hausdorff:
What sounds like a reasonable amount of default space for logging to you?

-----------------------------------------------------------------------------------------------------
mrflip:
This is what we've been using; it will use no more than 10 files of 1MB each. Most gc logging statements are on except PrintGCApplicationStoppedTime.

worker.childopts: >-
      -Xloggc:/var/log/storm/gc-worker-%ID%.log -verbose:gc                                                                                                   
      -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1m
      -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps -XX:+PrintClassHistogram                                                                                         
      -XX:+PrintTenuringDistribution -XX:-PrintGCApplicationStoppedTime 
                                                                                                                                                                                                        
The challenge here is -Xloggc:/var/log/storm/gc-worker-%ID%.log -- how would you like the directory to be specified? I can add a new config variable, or add a new % interpolant, or default it to /tmp/gc-worker-%ID%.log.

"
STORM-39,Expose number of attempts for a batch within Trident API,"https://github.com/nathanmarz/storm/issues/352

This would let someone implement policies like ""retry a batch up to 4 times, then skip it"".

-----------------------------------------------------------------------------------------------------
nathanmarz: 
I think the best strategy is to just expose this in the coordinator. isReady can be changed to take in a TransactionAttempt and return an enum:

enum BatchAction {
   PROCESS,
   WAIT,
   SKIP
}

Perhaps isReady can be renamed to ""getBatchAction"".

-----------------------------------------------------------------------------------------------------
nathanmarz: 
Actually the semantics aren't that clear. What does skip mean for an opaque spout, for example? In an opaque spout, not every batch is the same.

Additionally, multiple spouts can be participating in the same batch. You'd want SKIP to cause only that one particular spout to skip the batch, while the other one emits.

-----------------------------------------------------------------------------------------------------
mrclip:
It might make sense pull the failed records into a separate stream -- one that could be routed into a separate kafka topic for instance, or counted and then dropped. If nothing is listening on that stream, the records are effectively skipped. The spout just emits transparently, and the skipped records can still be transactionally handled on the Lethe stream.
"
STORM-38,Should be able to do state updates through DRPC,"https://github.com/nathanmarz/storm/issues/307

The batch size of 1 might make it inefficient, but it should still be possible. In addition, there should be a ""batch DRPC"" in which a single batch contains multiple DRPC request tuples. Each tuple would be [request id, args], and the result should be `[[request id, ...] ...]""

-----------------------------------------------------------------------------------------------------
childs: @nathanmarz can you given any hint as to wether this is nice to have or we need to do this from your point of view?

-----------------------------------------------------------------------------------------------------
nathanmarz: This is something we most definitely want to implement.

-----------------------------------------------------------------------------------------------------
danielpizarro: I've just hit the wall with this today. @nathanmarz, do you have any hints of a workaround to get same functionality?"
STORM-37,Auto-deactivate topologies that are continuously erroring,"There exists a bad interaction between the isolation scheduler, how Mesos does resource offers (in storm-mesos), and continuously erroring topologies. The effect is that no non-isolated topologies can run because the isolation scheduler needs to kill non-isolated topologies to free up resources for isolated topologies in the next scheduling iteration, and continuously does so because the isolated topology always errors.

A nice fix for this would be for Nimbus to automatically deactivate topologies that are continuously erroring. It should measure the number of X worker failures in the last Y minutes and put the topology into ""DEACTIVATED_ERRORED"" state if there's too many errors.

This would also be good for non-Mesos clusters in order to avoid the cost of continuous JVM startups from erroring topologies."
STORM-36,partitionPersist that can take as input multiple streams with differing schemas,"https://github.com/nathanmarz/storm/issues/369

Each stream may do different actions to the State object.

-----------------------------------------------------------------------------------------------------
quintona: I think this would solve an issue I am having, let me explain the use case. I essentially have a single spout emitting links. The content of the link is then downloaded and analyzed. Various parallel logical streams are then derived, some relatively static compared to the others. What I mean by that is some state is log living, well beyond the batch, like the number of links. This is state increments with each tuple within and across batches. At the same time the actual main stream is deriving measures at the batch level, and these 2 figures need to be combined into a single expression later. The approach I was using was to persist the long running count using the persistentAggregate, then I was intending to use a statequery to derive a stream off that state and merge that into main stream. I could therefore have a single function receiving a batch level count, and a running total across time which it needs in order to do its calculation.

The only other approach I can think of to achieve this (given that I can't merge the streams suggested above, and I have no means of joining), is to have multiple streams effect a single state as you suggest here, or a periodic DRPC approach.

If this the sort of thing you were trying to solve here?

Here is potentially one approach? https://gist.github.com/quintona/5558787"
STORM-34,"Check that same version of Storm used during compilation, submission, and execution","https://github.com/nathanmarz/storm/issues/220

* Compilation jars are ""dev dependency"" Storm jars on the classpath when topology jar is built
* Submission jars are the one used by ""storm"" client
* Execution jars are the ones on the Storm cluster itself

A helpful error message should be thrown if these are ever mismatched."
STORM-33,Log warning if number of un-acked tuples in a bolt gets too large,"https://github.com/nathanmarz/storm/issues/162

This is a common application bug, so it would be nice if Storm helped out in tracking these down.

-----------------------------------------------------------------------------------------------------
ghost : is there a definition of too large? Heap consumption? Retrieval/Insert cost?

Obviously configuration tracked would be preferred, but I'm trying to get a feel for the nature of the problem. And its causes? It seems that we are mostly talking about compute costs 'downstream' since most other anomalies would fall into the category of (unforeseen or anomalous) capacity planning.

Is this issue the start of a dynamic/adaptive scheduling fix for exceptional compute costs? Something like RTRebal?

-----------------------------------------------------------------------------------------------------
ghost: Ok. I made it through several other issues. fwiw, ignore the question about RTRebal :-) Its a recurring documented issue in several places; not sure issue is the right term ."
STORM-32,Turn Nimbus JMX port on by default,"https://github.com/nathanmarz/storm/issues/493

This will be a change to nimbus.childopts. This makes it easy to capture JVM diagnostic info."
STORM-31,Auto-tune max spout pending,"https://github.com/nathanmarz/storm/issues/385

It should be relatively easy to have spouts automatically tune max spout pending so that topologies can handle high throughputs with minimal tuning. A spout should look at the average complete latency over the last 10 minutes and compare that to the message timeout. If it's significantly lower, it should increase pending. If it's close, it should decrease.

--------------------------------------------------------------------------------------------------
@jasonjckn: So as you know there's no way to count # of replays in storm, or more generally, there's no storm spout concept of consumption progress. There is # of acked tuples, but this doesn't differentiate between replays. While I think the design choice is actually appropriate when you think how consumption means different things in kafka, the variety of different spouts that can exist, etc. This does lead to problems with any auto tune max spout pending algorithm I could possibly devise based on the current executor stats. For example with kafka a particular offset references a block of messages compressed together, not individual messages, and when a particular tuple fails this will lead to that entire block of tuples being replayed, and possibly other blocks that came afterwords. Now imagine a kafka block of 1000 tuples, and there's a 1% chance of a failed tuple, so this can lead to zero progress, because every single kafka block has at least 1 tuple that failed. However this also has a nice ratio of # acks / # emits, because all of the acks are replayes. Contrasting this where the user logic does some kind of batching, and database batch updates, this tends to have the property where entire batches succeed or fail (This is how trident works), so if 4 out of 5 batches were to succeed, and a batch includes 1 kafka block from each partition, then you're making a lot of progress, but your # acks / # emits ratio is 4/5. So lower emit ratio, but a lot of progress is being made.

So here's an idea to solve this:

Expose setMaxSpoutPending to spout implementations so they can call it whenever they want. Then the user would enable auto tune max spout pending as a KafkaSpout option or KestrelSpout Option (instead of topology config).

Then you would write a generic max spout pending algorithm which would be a thermostat on 'consumption throughput'. 'amount consumed' is an abstract concept, and it's an input into this algorithm. 'amount consumed' is assumed to be a monotonically increasing number and to indicate durable consumption occurring. So in the kafka implementation you would plugin this algorithm, and 'amount consumed' would be defined as the oldest offset to a kafka block that's been acked.

I think moving this logic into the spout implementation a lot of sense when consumption means different things depending on the whatever is backing the spout. I thought about introducing tuple replays as a concept in storm, but there's no easy way to track this at the tuple level in kafka, just the kafka block level. Also in trident this makes a lot of sense because the # of pending tuples is actually the number of parallel batches. So if you set max spout pending to 5, then that's 5 parallel batches. By allowing the trident master coordinator spout to set it's own max spout pending it could do something smart for optimizing the number of parallel batches.

As a side note, a kafka block may contain any number of messages, and the kafka offsets jump in steps of block size (block size is not fixed either), it's basically just an offset on disk, so this would lead to the auto tune max spout pending maximizing not on message throughput, but rather compressed data throughput. I don't think this matters one way or the other, you could always do message throughput by counting the number of messages in each block, but that's a more complex implementation, i'd just do the simpler one first (the oldest unacked kafka offset)."
STORM-30,Modularized Storm artifacts for using Storm as a library,"https://github.com/nathanmarz/storm/issues/313

When using Storm as a library (for example, the DRPC client), storm-lib brings in a lot of dependencies you probably don't need, such as all the UI-related libraries. It would be nice if the build were more sophisticated, so that rather than have one giant storm-lib, you'd have dependencies for:

Just the Thrift code + helpful wrapper like DRPCClient
Storm itself
The UI stuff

Leiningen 2 may be able to do this via submodules"
STORM-29,Redirect Clojure's *out* and *err* to log4j,"@nathanmarz: I played around with resetting the root value of those vars to redirect the streams. I could get it to work in local mode, but it would cause strange errors on an actual cluster.

istream: hi nathan, is this feature ready to use now? We work around this using bash stdout/stderr redirection to worker-*.out files. It works but is a little urgly."
STORM-28,DRPCClient should support connection pooling/failover,"https://github.com/nathanmarz/storm/issues/38

Finagle would be useful for this"
STORM-27,Expose add-supervisor via Java testing API,"https://github.com/nathanmarz/storm/issues/486

This allows you to configure each supervisor with a different configuration and/or number of ports."
STORM-26,Bound serialization buffers,"https://github.com/nathanmarz/storm/issues/468

Storm's serialization buffers currently expand to the size of the largest record seen. We've seen cases where unexpected or corrupt records can cause the serialization buffer to grow extremely large (like to size of heap), even though all valid records never come close to this size. This causes strange perf issues that are hard to debug. Storm should bound the size of the serialization buffer, and handle records larger than that with a special, temporary buffer."
STORM-25,Trident should track number of tuples in a batch,"https://github.com/nathanmarz/storm/issues/440

This is a useful metric for understanding performance of Trident. It can be used to normalize other statistics (like average latencies) if batch size is changing over time.

This can be written to the metrics system in 0.9.0"
STORM-24,Refactor internal routing to more efficiently send the same values to multiple tasks,"https://github.com/nathanmarz/storm/issues/408

Storm should be more efficient when sending the same payload to multiple tasks. Rather than create many tuples for each target task, the internal routing should send to the target worker [list of task ids, payload] as one message, and then the recipient will turn that into a tuple for each task in the worker.

This issue is a prerequisite for having a ""stats"" stream (for use in dynamically adjusting tasks), as the stats payload is fairly large.

This issue comprises the following pieces:

Internal routing changed from being [task id, tuple] to [list of task ids, tuple values, list of message ids]
Transfer thread turns [list of task ids, tuple values, list of message ids] into as few messages as possible
Routing thread needs similar modifications as transfer thread (should probably share code)
Reciever transforms [list of task ids, tuple values, list of message ids] into a tuple for every task
Serialization code needs to be refactored to understand this new format
Tuples aren't created outright, but are created later once it reaches the destination worker (because message ids and tuple payload need to be kept separate)
Another emitDirect that takes in a list of task ids"
STORM-23,Spouts should track nextTuple latency,"https://github.com/nathanmarz/storm/issues/391

Should work similar to execute latency for bolts"
STORM-22,Expose stream metadata via TridentOperationContexts,"https://github.com/nathanmarz/storm/issues/386

You should be able to query for the declared output fields of the operation, the fields of the incoming tuples, and so on."
STORM-21,Provide metrics on the size of serialized tuples,"https://github.com/nathanmarz/storm/issues/381

Should track by output stream"
STORM-20,Add API for controlling the parallelism of returning results in DRPC,"https://github.com/nathanmarz/storm/issues/357

Right now the parallelism is always 1. This can just be a simple configuration passed to newDRPCStream that takes effect in TridentTopology#completeDRPC"
STORM-19,Trident operations should be able to register serializations,https://github.com/nathanmarz/storm/issues/336
STORM-18,Trident sections connected by a merge should not be required to enforce equal parallelisms,"https://github.com/nathanmarz/storm/issues/322

Currently, Trident enforces that partitionings are consistent, so that when Trident is forced to split up two pieces into separate bolts, an identity partitioning is used and the parallelism of the two parts must be identical. Enforcing this isn't necessary in a merge and in fact just makes it harder to have separate parallelisms for the different pieces.

A current workaround is to manually set the partitioning to separate the pieces (like with a shuffle partitioning)."
STORM-17,Spouts should be their own ackers,"This would create the following improvements:

https://github.com/nathanmarz/storm/issues/282

1) Operational: no more need to specify the number of ackers
2) More accurate tuple tree tracking: When this is done, spouts can generate the tuple ids and guarantee that they're unique by checking if it already exists already. With the current acker approach, the birthday problem takes effect, and occasionally two different spout tuples will have the same id, causing them both to timeout and fail. See this thread for more details: http://groups.google.com/group/storm-user/browse_frm/thread/f3267a4a605757e9
3) Better performance: No need for the ack init message from spout -> acker, and no need for ack/fail message from acker -> spout"
STORM-16,Better logging when worker slots are constrained,"https://github.com/nathanmarz/storm/issues/170

When Storm packs a topology into just a few worker slots, or there are no worker slots available for the topology, it should log messages to that effect.

Relevant thread: https://groups.google.com/forum/?fromgroups#!searchin/storm-user/timeouts$20when$20deploying$20my$20second$20topology/storm-user/2ZaOue5QxtI/gXfX0WmG0WgJ"
STORM-15,"Storm UI should measure distribution of latencies, not just averages","https://github.com/nathanmarz/storm/issues/53

Distributions are much more useful. You would be able to diagnose things like variance in IO causing tuple timeouts."
STORM-13,Change license on README.md,"The license on the README still refers to the EPL.
It should mention the ASL v2 instead."
STORM-11,Want the ability to deactivate specific spouts,"https://github.com/nathanmarz/storm/issues/619

I imagine this being a new method on the thrift service. I would like to be able to turn off segments of a topology without having to stop and restart the topology."
STORM-10,Exception when tick tuples are enabled on a trident topology,"https://github.com/nathanmarz/storm/issues/637

When tick tuples are enabled for a trident topology, the following exception is produced:

java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Long cannot be cast to    storm.trident.topology.TransactionAttempt
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:87)
at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:58)
at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62)
at backtype.storm.daemon.executor$fn__4050$fn__4059$fn__4106.invoke(executor.clj:658)
at backtype.storm.util$async_loop$fn__465.invoke(util.clj:377)
at clojure.lang.AFn.run(AFn.java:24)
at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to storm.trident.topology.TransactionAttempt
at storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:46)
at backtype.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:32)
at backtype.storm.daemon.executor$fn__4050$tuple_action_fn__4052.invoke(executor.clj:566)
at backtype.storm.daemon.executor$mk_task_receiver$fn__3976.invoke(executor.clj:348)
at backtype.storm.disruptor$clojure_handler$reify__1606.onEvent(disruptor.clj:43)
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:84)
... 6 more


Ideally there would be an exception thrown at deploy time saying tick tuples aren't supported for trident topologies."
STORM-9,Add test utilities for Trident topology,"For raw storm topology, we have backtype.storm.Testing, so can mock a spout like this:

MockedSources mockedSources = new MockedSources();
mockedSources.addMockData(""1"", new Values(""nathan""),
     new Values(""bob""), new Values(""joey""), new Values(""nathan""));

the first argument of addMockData is the id of the spout to be mocked, while in trident, the id of spout is not specified by developer, it is auto-generated by trident, so we need utilities to mock spout for trident."
STORM-8,deactivate command is not work with IPartitionedTridentSpout style spout,"Reported by @frady 

when i use deactivate a topology,it does't work, the spout still emit tuples"
STORM-7,storm.trident.operation.Aggregator: include group information in init() method,"Reported by @lorenzfischer

To be able to share resources between different groups in a grouped aggregator, it would be helpful to have information about the group available in the init() method of the aggregator interface.

The concrete use case is the following:

For our project we need to count the number of unique values in a field of a grouped stream. We have hundreds of millions of unique values and millions of grouped values. For this reason, we're currently deploying the HyperLogLog class that has generously been made available by the people at Clearspring >(https://github.com/clearspring/stream-lib). Naturally, we end up with millions of counter objects.

The DSI-Utils library (http://dsiutils.di.unimi.it) offers a class that allows one to reduce the overhead incurred by this many HLL objects through its HyperLogLogCounterArray class. We're struggling with the implementation in Trident though, as the init(Object batchId, TridentCollector collector) method of the aggregator interface does not provide any information about the current ""group"" the aggregator should be initialized for.

(This was initially posted on Google Groups: https://groups.google.com/forum/#!topic/storm-user/dthUfkMRNhU)"
STORM-6,Upgrade jgrapht to 0.9.x,"Storm currently uses version 0.8.3 of jgrapht, which is LGPL licensed.

Version 0.9.0 of jgrapht is dual-licensned (EPL/LGPL) and should be compatible with Apache's licensing requirements."
STORM-5,Migrate Github Issues to JIRA,Migrate issues from https://github.com/nathanmarz/storm to Apache JIRA.
STORM-2,Move all dependencies off of storm-specific builds,"Move all dependencies off of storm-specific builds (groupId=storm/backtype) and on to specific releases. (Nathan can probably best answer to which ones involve changes, and which ones are just point-in-time forks)."
STORM-1,Remove JZMQ as a build dependency / requirement.,"
Since we now have netty, it would be great if developers were able to build Storm w/o compiling/installing zmq.  This has been a pain point for a lot of people and is a significant hurdle people need to overcome before they can start contributing."
