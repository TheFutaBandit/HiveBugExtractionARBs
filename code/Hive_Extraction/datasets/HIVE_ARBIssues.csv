Bug_ID,Bug_Summary,Bug_Description
HIVE-28076,Selecting data from a bucketed table with decimal column type throwing NPE.,"selecting data from a bucketed table with decimal bucket column type throwing NPE.

Steps to reproduce:
{noformat}
create table bucket_table(id decimal(38,0), name string) clustered by(id) into 3 buckets;
insert into bucket_table values(5000000000000999640711, 'Cloud');

select * from bucket_table bt where id = 5000000000000999640711;{noformat}
HS2 log contains NPE:
{noformat}
ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
   at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hashCodeMurmur(ObjectInspectorUtils.java:889)
   at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getBucketHashCode(ObjectInspectorUtils.java:805)
   at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getBucketNumber(ObjectInspectorUtils.java:638)
   at org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer$BucketBitsetGenerator.generatePredicate(FixedBucketPruningOptimizer.java:225)
   at org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory$FilterPruner.process(PrunerOperatorFactory.java:87)
   at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
   at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
   at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
   at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:158)
   at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
   at org.apache.hadoop.hive.ql.optimizer.PrunerUtils.walkOperatorTree(PrunerUtils.java:84)
   at org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.transform(FixedBucketPruningOptimizer.java:331)
   at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:249)
   at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12995)
   at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:443)
   at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:303)
   at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:220)
   at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105)
   at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:194)
   at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:621)
   at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:567)
   at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:561)
   at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:127)
   at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:231)
   at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:260)
   at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:204)
   at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:130)
   at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:429)
   at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:360)
   at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:857)
   at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:827)
   at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:191)
   at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:104)
   at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
   at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
   at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
   at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
   at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:92)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
   at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
   at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
   at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
   at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
   at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
   at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
   at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
   at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
   at org.junit.runners.Suite.runChild(Suite.java:128)
   at org.junit.runners.Suite.runChild(Suite.java:27)
   at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
   at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
   at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
   at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
   at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
   at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:73)
   at org.junit.rules.RunRules.evaluate(RunRules.java:20)
   at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
   at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
   at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
   at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
   at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
   at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
   at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
   at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
   at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
   at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451){noformat}"
HIVE-27943,NPE in VectorMapJoinCommonOperator.setUpHashTable when running query with join on date,"The error can be reproduced on [current master|https://github.com/apache/hive/commit/fd6ced288dbf9ce7f3c3a2ca948d78f3b88f170f] using the following steps.
{code:sql}
set hive.auto.convert.join=true;

CREATE TABLE person (fname string, birthDate date);
INSERT INTO person VALUES ('Victor', '2023-11-27'), ('Alexandre', '2023-11-28');

SELECT * FROM person p1 INNER JOIN person p2 ON p1.birthDate=p2.birthDate;
{code}
The stacktrace is shown below.
{noformat}
2023-12-07T07:06:39,744 ERROR [TezTR-592758_1_2_1_0_0] tez.TezProcessor: Failed initializeAndRunProcessor
java.lang.RuntimeException: Map operator initialization failed
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:351) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:292) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:276) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:381) ~[tez-runtime-internals-0.10.2.jar:0.10.2]
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:82) ~[tez-runtime-internals-0.10.2.jar:0.10.2]
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:69) ~[tez-runtime-internals-0.10.2.jar:0.10.2]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_261]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_261]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899) ~[hadoop-common-3.3.6.jar:?]
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:69) ~[tez-runtime-internals-0.10.2.jar:0.10.2]
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:39) ~[tez-runtime-internals-0.10.2.jar:0.10.2]
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.10.2.jar:0.10.2]
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118) ~[hive-llap-server-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_261]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_261]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_261]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_261]
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.setUpHashTable(VectorMapJoinCommonOperator.java:673) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.completeInitializationOp(VectorMapJoinCommonOperator.java:634) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.completeInitialization(Operator.java:450) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:382) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:549) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:503) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:369) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:332) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        ... 16 more
{noformat}

This is probably caused by HIVE-23852 which added the DATE type as a possible key for the HashTable."
HIVE-27916,Increase tez.am.resource.memory.mb for TestIcebergCliDrver to 512MB,this is HIVE-27695 for another tez drivers
HIVE-27867,Incremental materialized view throws NPE whew Iceberg source table is empty,"Repro
https://github.com/apache/hive/blob/master/iceberg/iceberg-handler/src/test/queries/positive/mv_iceberg_orc.q

in hive.log
{code}
2023-11-09T05:17:05,625  WARN [e35c7637-b0ba-4e30-8448-5bdc0d0e4779 main] rebuild.AlterMaterializedViewRebuildAnalyzer: Exception loading materialized views
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.Hive.getValidMaterializedViews(Hive.java:2321) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Hive.getMaterializedViewForRebuild(Hive.java:2227) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer$MVRebuildCalcitePlannerAction.applyMaterializedViewRewriting(AlterMaterializedViewRebuildAnaly
zer.java:215) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1700) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1569) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.tools.Frameworks.lambda$withPlanner$0(Frameworks.java:131) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:914) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:180) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:126) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.logicalPlan(CalcitePlanner.java:1321) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:570) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:13113) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:465) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.ddl.view.materialized.alter.rebuild.AlterMaterializedViewRebuildAnalyzer.analyzeInternal(AlterMaterializedViewRebuildAnalyzer.java:135) ~[hive-exec-4.0.0-beta-2-SNAPSH
OT.jar:4.0.0-beta-2-SNAPSHOT]
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:180) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:224) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:107) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:519) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:471) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:436) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:430) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:121) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:227) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:257) ~[hive-cli-4.0.0-beta-2-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:201) ~[hive-cli-4.0.0-beta-2-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:127) ~[hive-cli-4.0.0-beta-2-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:425) ~[hive-cli-4.0.0-beta-2-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:356) ~[hive-cli-4.0.0-beta-2-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:733) ~[hive-it-util-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:703) ~[hive-it-util-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:115) ~[hive-it-util-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157) ~[hive-it-util-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.cli.TestIcebergLlapLocalCliDriver.testCliDriver(TestIcebergLlapLocalCliDriver.java:60) ~[test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_301]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_301]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_301]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_301]
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) ~[junit-4.13.2.jar:4.13.2]
        at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135) ~[hive-it-util-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.Suite.runChild(Suite.java:128) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.Suite.runChild(Suite.java:27) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
        at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95) ~[hive-it-util-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.junit.rules.RunRules.evaluate(RunRules.java:20) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) ~[surefire-junit4-3.0.0-M4.jar:3.0.0-M4]
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
        at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451) ~[surefire-booter-3.0.0-M4.jar:3.0.0-M4]
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveAugmentSnapshotMaterializationRule.onMatch(HiveAugmentSnapshotMaterializationRule.java:128) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewUtils.applyRule(HiveMaterializedViewUtils.java:295) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewUtils.augmentMaterializationWithTimeInformation(HiveMaterializedViewUtils.java:243) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Hive.getValidMaterializedViews(Hive.java:2313) ~[hive-exec-4.0.0-beta-2-SNAPSHOT.jar:4.0.0-beta-2-SNAPSHOT]
        ... 73 more
{code}"
HIVE-27565,Fix NPE when dropping table in HiveQueryLifeTimeHook::checkAndRollbackCTAS,"If dropping a iceberg table which is used by a materialized view, HiveQueryLifeTimeHook::checkAndRollbackCTAS will throw NPE.

 

Step to repro:
 * create a iceberg table:

create table test_ice1 (id int) stored by iceberg;
 * create a materialized view:

create materialized view ice_mat1 as select * from test_ice1;
 * drop the iceberg table:

drop table test_ice1;

 
{code:java}
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_291]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:4462) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at com.sun.proxy.$Proxy47.dropTable(Unknown Source) ~[?:?]
        at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1500) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        ... 26 more
ERROR : FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:Cannot drop table as it is used in the following materialized views [testdbpr.ice_mat1]
)
WARN  : Failed when invoking query after execution hook
java.lang.RuntimeException: Not able to check whether the CTAS table directory exists due to:
        at org.apache.hadoop.hive.ql.HiveQueryLifeTimeHook.checkAndRollbackCTAS(HiveQueryLifeTimeHook.java:84) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.HiveQueryLifeTimeHook.afterExecution(HiveQueryLifeTimeHook.java:65) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.HookRunner.runAfterExecutionHook(HookRunner.java:185) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Executor.cleanUp(Executor.java:525) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:118) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:367) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:205) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:154) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:149) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:185) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236) ~[hive-service-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:90) ~[hive-service-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) ~[hive-service-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_291]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_291]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) ~[hadoop-common-3.3.1.jar:?]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:356) ~[hive-service-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_291]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_291]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_291]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_291]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_291]
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.HiveQueryLifeTimeHook.checkAndRollbackCTAS(HiveQueryLifeTimeHook.java:79) ~[hive-exec-4.0.0-beta-1-SNAPSHOT.jar:4.0.0-beta-1-SNAPSHOT]
        ... 21 more
INFO  : Completed executing command(queryId=hive_20230804145734_08837e22-5ff0-4b56-a0cf-69b0414171dd); Time taken: 0.073 seconds
Error: Error while compiling statement: FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.ddl.DDLTask. MetaException(message:Cannot drop table as it is used in the following materialized views [testdbpr.ice_mat1]
) (state=08S01,code=40000)
 {code}
 

 "
HIVE-27487,NPE in Hive JDBC storage handler,"A simple query against a Hive JDBC table: ""select * from sample_nightly"" would fail due to:
{noformat}
 Caused by: java.lang.NullPointerException
    at org.apache.hive.storage.jdbc.JdbcSerDe.deserialize(JdbcSerDe.java:168)
    at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:623)
    ... 21 more{noformat}"
HIVE-27344,ORC RecordReaderImpl throws NPE when close() is called from the constructor,"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl throws NullPointerException when the constructor of its super class org.apache.orc.impl.RecordReaderImpl calls this.close().

close() mutates the field 'batch', which is initialized after super class constructor is done. Therefore, we need a null check of field 'batch' in close(). "
HIVE-27294,Remove redundant qt_database_all.q for memory consumption reasons,"Currently, while running qt_database_all.q the qtest environment starts and runs all the RDMBS docker containers at the same time in beforeTest, which might end up in extreme memory consumption. This is suboptimal, and considering that the test cases are all covered by single, separate qtests, we can simply remove qt_database_all.q.

{code}
./ql/src/test/queries/clientpositive/qt_database_postgres.q
./ql/src/test/queries/clientpositive/qt_database_oracle.q
./ql/src/test/queries/clientpositive/qt_database_mssql.q
./ql/src/test/queries/clientpositive/qt_database_mariadb.q
./ql/src/test/queries/clientpositive/qt_database_mysql.q
{code}"
HIVE-27240,NPE on Hive Hook Proto Log Writer,"Post deployment of Hive 4.0.0-alpha-1 observed NPE error blocking proto logger to serialize json on HiveHookEventProtoPartialBuilder
{code:java}
023-04-10T17:43:44,226 ERROR [Hive Hook Proto Log Writer 0]: hooks.HiveHookEventProtoPartialBuilder (:()) - Unexpected exception while serializing json.
java.lang.NullPointerException: null
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:986) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:908) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:1263) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputStagePlans(ExplainTask.java:1408) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONPlan(ExplainTask.java:367) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONPlan(ExplainTask.java:268) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.hooks.HiveHookEventProtoPartialBuilder.getExplainJSON(HiveHookEventProtoPartialBuilder.java:84) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.hooks.HiveHookEventProtoPartialBuilder.addQueryObj(HiveHookEventProtoPartialBuilder.java:75) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.hooks.HiveHookEventProtoPartialBuilder.build(HiveHookEventProtoPartialBuilder.java:55) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook$EventLogger.writeEvent(HiveProtoLoggingHook.java:312) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook$EventLogger.lambda$handle$1(HiveProtoLoggingHook.java:274) ~[hive-exec-3.1.4.3.2.2.0-1.jar:3.1.4.3.2.2.0-1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_362]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_362]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_362]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_362]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_362]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362] {code}

ExplainTask isn't getting initialised as earlier leading to querystate as null value, attaching earlier init code from HiveProtoLoggingHook
{code:java}
explain.initialize(hookContext.getQueryState(), plan, null, null); {code}"
HIVE-27223,Show Compactions failing with NPE,"{noformat}
java.lang.NullPointerException: null
	at java.io.DataOutputStream.writeBytes(DataOutputStream.java:274) ~[?:?]
	at org.apache.hadoop.hive.ql.ddl.process.show.compactions.ShowCompactionsOperation.writeRow(ShowCompactionsOperation.java:135) 
	at org.apache.hadoop.hive.ql.ddl.process.show.compactions.ShowCompactionsOperation.execute(ShowCompactionsOperation.java:57) 
	at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:84)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213) 
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105) 
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:360) 
{noformat}
"
HIVE-27201,Inconsistency between session Hive and thread-local Hive may cause HS2 deadlock,"The HiveServer2’s server handler can switch to process the operation from other session, in such case, the Hive cached in ThreadLocal is not the same as the Hive in SessionState, and can be referenced by another session. 

If the two handlers swap their sessions to process the DatabaseMetaData request, and the HiveMetastoreClientFactory obtains the Hive via Hive.get(), then there is a chance that the deadlock can happen."
HIVE-27179,HS2 WebUI throws NPE when JspFactory loaded from jetty-runner,"In HIVE-17088{*},{*} we resolved a NPE thrown from HS2 WebUI by introducing 

javax.servlet.jsp-api. It works as expected when the javax.servlet.jsp-api jar prevails jetty-runner jar, but things can be different in some environments, it still throws NPE when opening the HS2 web:
{noformat}
java.lang.NullPointerException 
at org.apache.hive.generated.hiveserver2.hiveserver2_jsp._jspService(hiveserver2_jsp.java:286) 
at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:71) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) 
at org.eclipse.jetty.servlet.ServletHolder$NotAsync.service(ServletHolder.java:1443) 
at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:791) 
at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
...{noformat}
The jetty-runner JspFactory.getDefaultFactory() just returns null."
HIVE-27138,MapJoinOperator throws NPE when computing OuterJoin with filter expressions on small table,"Hive throws NPE when running mapjoin_filter_on_outerjoin.q using Tez engine. (I used TestMiniLlapCliDriver.)
The NPE is thrown by CommonJoinOperator.getFilterTag(), which just retreives the last object from the given list.
To the best of my knowledge, if Hive selects MapJoin to perform Join operation, filterTag should be computed and appended to a row before the row is passed to MapJoinOperator.
In the case of MapReduce engine, this is done by HashTableSinkOperator.
However, I cannot find any logic pareparing filterTag for small tables when Hive uses Tez engine.

I think there are 2 available options:
1. Don't use MapJoinOperator if a small table has filter expression.
2. Add a new logic that computes and passes filterTag to MapJoinOperator.

I am working on the second option and ready to discuss about it.
It would be grateful if you could give any opinion about this issue."
HIVE-27093,Fix NPE in initialize() of Partition class,"We will get a *NullPointerException* when get_partitions from HMS if the partition SD does not exists.
!image-2023-02-19-02-00-43-537.png!"
HIVE-26903,Compactor threads should gracefully shutdown,"Currently the compactor threads are daemon threads, which means the JVM will not wait for these threads to finish. (see: [https://github.com/apache/hive/blob/431e7d9e5431a808106d8db81e11aea74f040da5/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java#L81)] As a result during system shutdown, JVM may close all daemon threads abruptly (JVM won't wait for a thread to sleep/wait, and no InterruptedException is thrown), so the threads don't have any chance to shutdown gracefully. This can lead to inconsistent/corrupted state in the Metastore or on the File system.

Make the compactor threads user threads, and handle shutdown accordingly. Make sure interrupts and InterruptedException is handled accordingly."
HIVE-26885,Iceberg: Parquet Vectorized V2 reads fails with NPE,"In case the Iceberg Parquet table lands up having an empty batch, in that case while fetching the row number, used for filtering leads to NPE.

The row number to block mapping is only done if the parquetSplit isn't null, so in that case, here:
{code:java}
if (parquetInputSplit != null) {
  initialize(parquetInputSplit, conf);
} {code}
row numbers aren't initialised, so we should skip fetching the row numbers later"
HIVE-26828,Fix OOM for hybridgrace_hashjoin_2.q,"_hybridgrace_hashjoin_2.q_ test was disabled because it was failing with OOM transiently (from [flaky_test output|http://ci.hive.apache.org/blue/organizations/jenkins/hive-flaky-check/detail/hive-flaky-check/597/tests/], in case it disappears):
{quote}< Status: Failed
< Vertex failed, vertexName=Map 2, vertexId=vertex_#ID#, diagnostics=[Vertex vertex_#ID# [Map 2] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: z1 initializer failed, vertex=vertex_#ID# [Map 2], java.lang.RuntimeException: Failed to load plan: hdfs://localhost:45033/home/jenkins/agent/workspace/hive-flaky-check/itests/qtest/target/tmp/scratchdir/jenkins/88f705a8-2d67-4d0a-92fd-d9617faf4e46/hive_2022-12-08_02-25-15_569_4666093830564098399-1/jenkins/_tez_scratch_dir/5b786380-b362-45e0-ac10-0f835ef1d8d7/map.xml
< #### A masked pattern was here ####
< Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.OutOfMemoryError: GC overhead limit exceeded
< Serialization trace:
< childOperators (org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator)
< childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)
< aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork)
< #### A masked pattern was here ####
< Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
< #### A masked pattern was here ####
< ]
< [Masked Vertex killed due to OTHER_VERTEX_FAILURE]
< [Masked Vertex killed due to OTHER_VERTEX_FAILURE]
< [Masked Vertex killed due to OTHER_VERTEX_FAILURE]
< [Masked Vertex killed due to OTHER_VERTEX_FAILURE]
< [Masked Vertex killed due to OTHER_VERTEX_FAILURE]
< DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:5
< FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 2, vertexId=vertex_#ID#, diagnostics=[Vertex vertex_#ID# [Map 2] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: z1 initializer failed, vertex=vertex_#ID# [Map 2], java.lang.RuntimeException: Failed to load plan: hdfs://localhost:45033/home/jenkins/agent/workspace/hive-flaky-check/itests/qtest/target/tmp/scratchdir/jenkins/88f705a8-2d67-4d0a-92fd-d9617faf4e46/hive_2022-12-08_02-25-15_569_4666093830564098399-1/jenkins/_tez_scratch_dir/5b786380-b362-45e0-ac10-0f835ef1d8d7/map.xml
< #### A masked pattern was here ####
< Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.OutOfMemoryError: GC overhead limit exceeded
< Serialization trace:
< childOperators (org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator)
< childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator)
< aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork)
< #### A masked pattern was here ####
< Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
< #### A masked pattern was here ####
< ][Masked Vertex killed due to OTHER_VERTEX_FAILURE][Masked Vertex killed due to OTHER_VERTEX_FAILURE][Masked Vertex killed due to OTHER_VERTEX_FAILURE][Masked Vertex killed due to OTHER_VERTEX_FAILURE][Masked Vertex killed due to OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:5
< PREHOOK: query: SELECT COUNT( * )
< FROM src1 x
< JOIN srcpart z1 ON (x.key = z1.key)
< JOIN src y1 ON (x.key = y1.key)
< JOIN srcpart z2 ON (x.value = z2.value)
< JOIN src y2 ON (x.value = y2.value)
< WHERE z1.key < 'zzzzzzzz' AND z2.key < 'zzzzzzzzzz'
< AND y1.value < 'zzzzzzzz' AND y2.value < 'zzzzzzzzzz'
< PREHOOK: type: QUERY
< PREHOOK: Input: default@src
< PREHOOK: Input: default@src1
< PREHOOK: Input: default@srcpart
< PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
< PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
< PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
< PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
< PREHOOK: Output: hdfs://### HDFS PATH ###
{quote}
The aim of this ticket is to investigate the issue, fix it and re-enable the test.

The problem seems to lie in the deserialization of the computed tez dag plan."
HIVE-26820,Disable hybridgrace_hashjoin_2.q flaky test,"Had this test failing many times in the last months, let's disable it for the moment:

[http://ci.hive.apache.org/blue/organizations/jenkins/hive-flaky-check/detail/hive-flaky-check/597/tests]"
HIVE-26758,Allow use scratchdir for staging final job,"The query results are staged in stagingdir that is relative to the destination path <destination_dir>/<staging_dir>/

during blobstorage optimzation HIVE-17620 final job is set to use stagingdir.

HIVE-15215 mentioned the possibility of using scratch for staging when write to S3 but it was long time ago and no activity.

 

This is to allow final job to use hive.exec.scratchdir as the interim jobs, with a configuration 

hive.use.scratchdir.for.staging

This is useful for cross Filesystem, user can use local source filesystem instead of remote filesystem for the staging."
HIVE-26724,Mask UDF failing with NPE,"The mask UDF fails with NPE in prod, due to unavailability of the session conf.

Trace:
{noformat}
	... 20 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.udf.generic.MaskHashTransformer.transform(GenericUDFMaskHash.java:50)
	at org.apache.hadoop.hive.ql.udf.generic.StringTransformerAdapter.getTransformedWritable(BaseMaskUDF.java:459)
	at org.apache.hadoop.hive.ql.udf.generic.BaseMaskUDF.evaluate(BaseMaskUDF.java:84)
	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:235)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	... 24 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1667823513257_0010_2_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:280)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374){noformat}"
HIVE-26541,WebHCatServer start fails with NPE,"The TestWebHCatE2e  test fails due to the NPE shown below.

{noformat}
templeton: Server failed to start: null
[main] ERROR org.apache.hive.hcatalog.templeton.Main - Server failed to start: 
java.lang.NullPointerException
at org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:174)
at org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44)
at org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:220)
at org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:143)
at org.apache.hive.hcatalog.templeton.Main.createChannelConnector(Main.java:295)
at org.apache.hive.hcatalog.templeton.Main.runServer(Main.java:252)
at org.apache.hive.hcatalog.templeton.Main.run(Main.java:147)
at org.apache.hive.hcatalog.templeton.TestWebHCatE2e.startHebHcatInMem(TestWebHCatE2e.java:94)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
{noformat}
"
HIVE-26488,Fix NPE in DDLSemanticAnalyzerFactory during compilation,"*Exception Trace:*

{noformat}
java.lang.ExceptionInInitializerError
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.getInternal(SemanticAnalyzerFactory.java:62)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:41)
	at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:209)
	at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:106)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:507)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:459)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:424)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:418)
{noformat}

*Cause:*

{noformat}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.<clinit>(DDLSemanticAnalyzerFactory.java:84)
	... 40 more
{noformat}"
HIVE-26404,HMS memory leak when compaction cleaner fails to remove obsolete files,"While investigating an issue where HMS becomes unresponsive we noticed a lot of failed attempts from the compaction Cleaner thread to remove obsolete directories with exceptions similar to the one below.
{noformat}
2022-06-16 05:48:24,819 ERROR org.apache.hadoop.hive.ql.txn.compactor.Cleaner: [Cleaner-executor-thread-0]: Caught exception when cleaning, unable to complete cleaning of id:4410976,dbname:my_database,tableName:my_table,partName:day=20220502,state:,type:MAJOR,enqueueTime:0,start:0,properties:null,runAs:some_user,tooManyAborts:false,hasOldAbort:false,highestWriteId:187502,errorMessage:null java.io.IOException: Not enough history available for (187502,x).  Oldest available base: hdfs://nameservice1/warehouse/tablespace/managed/hive/my_database.db/my_table/day=20220502/base_0188687_v4297872
	at org.apache.hadoop.hive.ql.io.AcidUtils.getAcidState(AcidUtils.java:1432)
	at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.removeFiles(Cleaner.java:261)
	at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.access$000(Cleaner.java:71)
	at org.apache.hadoop.hive.ql.txn.compactor.Cleaner$1.run(Cleaner.java:203)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
	at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.clean(Cleaner.java:200)
	at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.lambda$run$0(Cleaner.java:105)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorUtil$ThrowingRunnable.lambda$unchecked$0(CompactorUtil.java:54)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
In addition the logs contained a large number of long JVM pauses as shown below and the HMS (RSZ) memory kept increasing at rate of 90MB per hour.
{noformat}
2022-06-16 16:17:17,805 WARN  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 34346ms
2022-06-16 16:17:21,497 INFO  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 1690ms
2022-06-16 16:17:57,696 WARN  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 34697ms
2022-06-16 16:18:01,326 INFO  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 1628ms
2022-06-16 16:18:37,280 WARN  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 34453ms
2022-06-16 16:18:40,927 INFO  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 1646ms
2022-06-16 16:19:16,929 WARN  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 33997ms
2022-06-16 16:19:20,572 INFO  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 1637ms
2022-06-16 16:20:01,643 WARN  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 39329ms
2022-06-16 16:20:05,572 INFO  org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor: [org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor$Monitor@5b022296]: Detected pause in JVM or host machine (eg GC): pause of approximately 1927ms
{noformat}
We took a heapdump of the HMS around the time that it becomes unresponsive and we have seen many Configuration objects (~40K) occupying more than 90% of the current heap (~9GB).
{noformat}
Class Name                                   |     Objects |  Shallow Heap |    Retained Heap
----------------------------------------------------------------------------------------------
org.apache.hadoop.conf.Configuration         |      39,452 |     1,893,696 | >= 8,560,573,960
java.util.concurrent.ConcurrentHashMap       |     155,863 |     9,975,232 | >= 4,696,003,968
java.util.concurrent.ConcurrentHashMap$Node[]|     139,348 | 1,312,967,944 | >= 4,686,230,296
java.util.Properties                         |      87,119 |     4,181,712 | >= 4,193,638,904
java.util.Hashtable$Entry[]                  |      87,840 |   987,968,472 | >= 4,189,518,928
java.util.concurrent.ConcurrentHashMap$Node  |  99,097,078 | 3,171,106,496 | >= 3,375,319,552
java.util.Hashtable$Entry                    | 100,047,081 | 3,201,506,592 | >= 3,201,551,936
org.postgresql.jdbc.PgConnection             |       6,488 |       830,464 |   >= 551,442,952
----------------------------------------------------------------------------------------------

{noformat}
It turns out that these Configuration objects are all referenced by CACHE entries in org.apache.hadoop.fs.FileSystem$Cache.
{noformat}
Class Name                                                                             | Shallow Heap | Retained Heap
----------------------------------------------------------------------------------------------------------------------
org.apache.hadoop.fs.FileSystem$Cache @ 0x45403fe70                                    |           32 |   108,671,824
|- <class> class org.apache.hadoop.fs.FileSystem$Cache @ 0x45410c3e0                   |            8 |           544
'- map java.util.HashMap @ 0x453ffb598                                                 |           48 |    92,777,232
   |- <class> class java.util.HashMap @ 0x4520382c8 System Class                       |           40 |           168
   |- entrySet java.util.HashMap$EntrySet @ 0x454077848                                |           16 |            16
   '- table java.util.HashMap$Node[32768] @ 0x463585b68                                |      131,088 |    92,777,168
      |- class java.util.HashMap$Node[] @ 0x4520b7790                                  |            0 |             0
      '- [1786] java.util.HashMap$Node @ 0x451998ce0                                   |           32 |         9,968
         |- <class> class java.util.HashMap$Node @ 0x4520b7728 System Class            |            8 |            32
         '- value org.apache.hadoop.hdfs.DistributedFileSystem @ 0x452990178           |           56 |         4,976
            |- <class> class org.apache.hadoop.hdfs.DistributedFileSystem @ 0x45402e290|            8 |         4,664
            |- uri java.net.URI @ 0x451a05cd0  hdfs://nameservice1                     |           80 |           432
            |- dfs org.apache.hadoop.hdfs.DFSClient @ 0x451f5d9b8                      |          128 |         3,824
            '- conf org.apache.hadoop.hive.conf.HiveConf @ 0x453a34b38                 |           80 |       250,160
----------------------------------------------------------------------------------------------------------------------
{noformat}
As long as they are in the CACHE they cannot be garbage collected so this leads to a memory leak.

The memory leak seems to come from the fact the compaction Cleaner attempts to [remove|https://github.com/apache/hive/blob/69e6a5a4151100849d2b03b6b14b1605c3abc3f1/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java#L266] the obsolete files and fails. The exception does not allow the [filesystem cleanup|https://github.com/apache/hive/blob/69e6a5a4151100849d2b03b6b14b1605c3abc3f1/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java#L270] to take place so we are leaving filesystem entries in the CACHE and subsequently configuration objects.

Although, the HMS unresponsiveness in this use-case may not be due to lack of memory the leak needs to be addressed to avoid hitting OOM."
HIVE-26380,Fix NPE when reading a struct field with null value from iceberg table,"When reading a map that contains a struct of null an NPE is raised
{code:java}
Caused by: java.lang.NullPointerException at org.apache.iceberg.mr.hive.serde.objectinspector.IcebergRecordObjectInspector.getStructFieldData(IcebergRecordObjectInspector.java:75)at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator._evaluate(ExprNodeFieldEvaluator.java:94) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct.evaluate(GenericUDFStruct.java:70) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:197) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator._evaluate(ExprNodeFieldEvaluator.java:79) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68) at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88) {code}
 

 "
HIVE-26288,NPE in CompactionTxnHandler.markFailed(),"Unhandled exceptions in IMetaStoreClient.findNextCompact(FindNextCompactRequest) handled incorrectly in worker. I these cases the CompcationInfo remains null, but the catch block passes it to CompactionTxnHandler.markFailed() which causes an NPE."
HIVE-26277,NPEs and rounding issues in ColumnStatsAggregator classes,"Fix NPEs and rounding errors in _ColumnStatsAggregator_ classes, add unit-tests for all the involved classes."
HIVE-26147,OrcRawRecordMerger throws NPE when hive.acid.key.index is missing for an acid file,"When _hive.acid.key.index_ is missing for an acid ORC file _OrcRawRecordMerger_ throws as follows:

{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.discoverKeyBounds(OrcRawRecordMerger.java:795) ~[hive-exec-4.0.0-alpha-2-SNAPS
HOT.jar:4.0.0-alpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:1053) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.
0.0-alpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:2096) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-a
lpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1991) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4
.0.0-alpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:769) ~[hive-exec-4.0.0-alpha
-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:335) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-
alpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:560) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha
-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:529) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-
SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:150) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.getFetchingTableResults(Driver.java:719) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNA
PSHOT]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:671) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha-2-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:233) ~[hive-exec-4.0.0-alpha-2-SNAPSHOT.jar:4.0.0-alpha
-2-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:489) ~[hive-service-4.0.0-alpha-2-SNAPSHOT.jar:
4.0.0-alpha-2-SNAPSHOT]
        ... 24 more
{noformat}

For this situation to happen, the ORC file must have more than one stripe, and the offset of the element to seek should either locate it beyond the first stripe (but before the last one), or in the first one if not the last one, as the code shows:

{code:java}
    if (firstStripe != 0) {
      minKey = keyIndex[firstStripe - 1];
    }
    if (!isTail) {
      maxKey = keyIndex[firstStripe + stripeCount - 1];
    }
{code}

However, in the context of the detection of the original issue, the NPE was triggered even by a simple ""select *"" over a table with ORC files missing the _hive.acid.key.index_ metadata information, but it was never failing for ORC files with a single stripe. The file was generated after a major compaction of acid and non-acid data.

If the ""select *"" is not triggering the NPE, either pick the values of the row obtained with ""select * from $table limit 1"", or try to select based on different values trying to get into the sought situation with a filter like this:

{code:sql}
select * from $table where c = $value
{code}

_OrcRawRecordMerger_ should simply leave as ""null"" the min and max keys when the _hive.acid.key.index_ metadata is missing."
HIVE-26036,NPE caused by getMTable() in ObjectStore,"*Issue*

Some api in ObjectStore invoke getMTable() but not check that if the returned value is null, which caused the NPE, like addPartitions(), addPartition(), alterPartition() et.

*Reason*

Such api described above will check that whether the table exists in HMSHandler (first check), but if the table is dropped by other threads after the first check, the NPE will happen.

The simple idea is that we can check the table each time we get from getMTable().

*Stack example*

!NPE_From_getMTable.png!"
HIVE-25912,Drop external table at root of s3 bucket throws NPE,"*new update:* 

I test the master branch, have the same problem.

----------

ENV:

Hive 3.1.2

HDFS:3.3.1

enable OpenLDAP and Ranger .

 

I create the external hive table using this command:

 
{code:java}
CREATE EXTERNAL TABLE `fcbai`(
`inv_item_sk` int,
`inv_warehouse_sk` int,
`inv_quantity_on_hand` int)
PARTITIONED BY (
`inv_date_sk` int) STORED AS ORC
LOCATION
'hdfs://emr-master-1:8020/';
{code}
 

The table was created successfully, but  when I drop the table throw the NPE:

 
{code:java}
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.NullPointerException) (state=08S01,code=1){code}
 

The same bug can reproduction on the other object storage file system, such as S3 or TOS:
{code:java}
CREATE EXTERNAL TABLE `fcbai`(
`inv_item_sk` int,
`inv_warehouse_sk` int,
`inv_quantity_on_hand` int)
PARTITIONED BY (
`inv_date_sk` int) STORED AS ORC
LOCATION
's3a://bucketname/'; // 'tos://bucketname/'{code}
 

I see the source code found:

 common/src/java/org/apache/hadoop/hive/common/FileUtils.java
{code:java}
// check if sticky bit is set on the parent dir
FileStatus parStatus = fs.getFileStatus(path.getParent());
if (!shims.hasStickyBit(parStatus.getPermission())) {
  // no sticky bit, so write permission on parent dir is sufficient
  // no further checks needed
  return;
}{code}
 

because I set the table location to HDFS root path (hdfs://emr-master-1:8020/), so the  path.getParent() function will be return null cause the NPE.

I think have four solutions to fix the bug:
 # modify the create table function, if the location is root dir return create table fail.
 # modify the  FileUtils.checkDeletePermission function, check the path.getParent(), if it is null, the function return, drop successfully.
 # modify the RangerHiveAuthorizer.checkPrivileges function of the hive ranger plugin(in ranger rep), if the location is root dir return create table fail.
 # modify the HDFS Path object, if the URI is root dir, path.getParent() return not null.

I recommend the first or second method, any suggestion for me? thx.

 

 "
HIVE-25830,Hive::loadPartitionInternal occur connection leak,"when Hive::loadPartitionInternal is invoked and there's no metastore connection in Hive.ThreadLocalHive, will create a metastore connection  for Hive.ThreadLocalHive, if external part hold the Hive.ThreadLocalHive for cache, and Hive.ThreadLocalHive update in some special scene, this will cause external part hold the different Hive object to Hive.ThreadLocalHive.
when external part want to create a metastore connection by invoking Hive::loadPartitionInternal, Hive will create a metastore connection for Hive.ThreadLocalHive, not for external part's holding.so these can cause a connection leaks easliy.

I found it will cause a metastore connection leak in spark-beeline when invoke Hive::loadPartitionInternal.

there's a commit HIVE-25075 relate before, I think it should remove the Hive.get() in Hive::loadPartitionInternal at the same time.
[~rbalamohan]
"
HIVE-25806,"Possible leak in LlapCacheAwareFs - Parquet, LLAP IO","there is an inputstream there which is never closed:
https://github.com/apache/hive/blob/9f9844dbc881e2a9267c259b8c04e7787f7fadc4/ql/src/java/org/apache/hadoop/hive/llap/LlapCacheAwareFs.java#L243

my understanding is that in an InputStream chain, every InputStream is responsible for closing its enclosed InputStream, here the chain is like:
DelegatingSeekableInputStream -> io.DataInputStream -> LlapCacheAwareFs$CacheAwareInputStream -> io.DataInputStream -> crypto.CryptoInputStream -> hdfs.DFSInputStream

{code}
	at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:106)
	at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:60)
	at java.nio.channels.SocketChannel.open(SocketChannel.java:145)
	at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62)
	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2933)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:821)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:746)
	at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:379)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:644)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:575)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:757)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:836)
	at org.apache.hadoop.crypto.CryptoInputStream.read(CryptoInputStream.java:183)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at org.apache.hadoop.hive.llap.LlapCacheAwareFs$CacheAwareInputStream.read(LlapCacheAwareFs.java:264)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.parquet.io.DelegatingSeekableInputStream.readFully(DelegatingSeekableInputStream.java:102)
	at org.apache.parquet.io.DelegatingSeekableInputStream.readFullyHeapBuffer(DelegatingSeekableInputStream.java:127)
	at org.apache.parquet.io.DelegatingSeekableInputStream.readFully(DelegatingSeekableInputStream.java:91)
	at org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:1174)
	at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:805)
	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:429)
	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:407)
	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:359)
	at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:93)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:361)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:117)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

usually, streams have the enclosed stream as a field in order to have a reference for the time they want to close it, but here LlapCacheAwareFs$CacheAwareInputStream.read opens a DataInputStream which is never closed, only used in the read method, which is suspicious to me"
HIVE-25794,CombineHiveRecordReader: log statements in a loop leads to memory pressure,"Similar to HIVE-16150, a huge string will be built in a loop, even the log level is INFO. That leads to memory pressure when processing a big number of split files. 

From [CombineHiveRecordReader.java|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveRecordReader.java#L116], the following needs to be fixed.



LOG.debug(""Found spec for "" + path + "" "" + otherPart + "" from "" + pathToPartInfo);


{code}
""TezChild"" #26 daemon prio=5 os_prio=0 tid=0x00007f5fd1716000 nid=0x2118a runnable [0x00007f5f8c411000]
   java.lang.Thread.State: RUNNABLE
	at java.lang.String.valueOf(String.java:2994)
	at java.lang.StringBuilder.append(StringBuilder.java:131)
	at java.util.AbstractMap.toString(AbstractMap.java:557)
	at java.lang.String.valueOf(String.java:2994)
	at java.lang.StringBuilder.append(StringBuilder.java:131)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.extractSinglePartSpec(CombineHiveRecordReader.java:119)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:88)
	at sun.reflect.GeneratedConstructorAccessor22.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:257)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:144)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.run(MergeFileRecordProcessor.java:153)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileTezProcessor.run(MergeFileTezProcessor.java:42)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:69)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

{code}"
HIVE-25749,Check if RelMetadataQuery.collations() returns null to avoid NPE,"Accoring to ""RelMetadataQuery.collations()"" [javadoc|https://github.com/apache/calcite/blob/calcite-1.25.0/core/src/main/java/org/apache/calcite/rel/metadata/RelMetadataQuery.java#L537], the method can return ""null"" if collactions information are not available.

Hive invokes the method in two places ([RelFieldTrimmer|https://github.com/apache/hive/blob/1046f41ea36ab3c8b036481128ba9b76dda2882a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/RelFieldTrimmer.java#L192] and [HiveJoin|https://github.com/apache/hive/blob/1046f41ea36ab3c8b036481128ba9b76dda2882a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java#L206]), but it does not check for ""null"" return values, which can cause NPE.

For RelFieldTrimmer, the same bug has been fixed in Calcite (where the code has been taken from) here: https://github.com/apache/calcite/commit/47871235177a3a0d398b1d890d1d2e947028e052"
HIVE-25618,Stack trace is difficult to find when qtest fails during setup/teardown,"When a qtest fails while executing one of the setup/teardown methods of a CLI driver ([CliAdapter|https://github.com/apache/hive/blob/3e37ba473545a691f5f32c08fc4b62b49257cab4/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/CliAdapter.java#L36] and its subclasses):

{code:java}
  public abstract void beforeClass() throws Exception;
  public abstract void setUp();
  public abstract void tearDown();
  public abstract void shutdown() throws Exception;
{code}

the original stack trace leading to the failure cannot be found easily. 

Maven console shows a stack trace which doesn't correspond to the actual exception causing the problem but another one which in most cases does not contain the original cause. 

The original stack trace is not displayed in the maven console and it is not in the {{target/tmp/logs/hive.log}} either. At the moment it goes to {{target/surefire-reports/...-output.txt}}. 

The developer needs to search in 2-3 places and navigate back and forth to the code in order to find what went wrong.

Ideally the stack trace from the original exception should be printed directly in maven console. "
HIVE-25518,CompactionTxHandler NPE if no CompactionInfo,"If no {{CompactionInfo}} is provided to the {{CompactionTxHandler#markFailed()}} then an NPE happens at the beginning of the method. No information inside the COMPLETED_COMPACTION info.
Stacktrace:
{noformat}
[TThreadPoolServer WorkerProcess-%d] ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler - java.lang.NullPointerException
	at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:1116)
	at org.apache.hadoop.hive.metastore.HMSHandler.mark_failed(HMSHandler.java:8716)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy13.mark_failed(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$mark_failed.getResult(ThriftHiveMetastore.java:23846)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$mark_failed.getResult(ThriftHiveMetastore.java:23825)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:248)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}"
HIVE-25482,Add option to enable connectionLeak detection for Hikari datasource,"There are corner cases where we observed connection leaks to DB.

 

It will be good to add an option to provide connection leak timeout parameter in HikariCPDataSourceProvider.

[https://github.com/apache/hive/blob/master/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/datasource/HikariCPDataSourceProvider.java#L69]

e.g following should help Hikari to warn about connection leak, when a connection is not returned to the pool for 1 hour.
{noformat}
config.setLeakDetectionThreshold(3600*1000); {noformat}
 "
HIVE-25416,Hive metastore memory leak because datanucleus-api-jdo bug,"I encountered a memory leak case. The MAT info :

!leak.jpg!

Full error message is :
{code:java}
Cannot get Long result for param = 8 for column ""`FUNCS`.`FUNC_ID`"" : Operation not allowed after ResultSet closed{code}
This is because there is a bug in the JDOPersistenceManager.retrieveAll code.
{code:java}
// code placeholder
JDOPersistenceManager{
public void retrieveAll(Collection pcs, boolean useFetchPlan) {
    this.assertIsOpen();
    ArrayList failures = new ArrayList();
    Iterator i = pcs.iterator();

    while(i.hasNext()) {
        try {
            this.jdoRetrieve(i.next(), useFetchPlan);
        } catch (RuntimeException var6) {
            failures.add(var6);
        }
    }

    if (!failures.isEmpty()) {
        throw new JDOUserException(Localiser.msg(""010038""), (Exception[])((Exception[])failures.toArray(new Exception[failures.size()])));
    }
}
}
{code}
In some extreme cases   the function of next() does not work . This will result in a very large failures ArrayList like as shown above.

 

The bug detail can see this : [https://github.com/datanucleus/datanucleus-api-jdo/issues/106]

This problem is fixed in datanucleus-api-jdo version 5.2.6. So we should upgrade it .

 

 

 "
HIVE-25364,NPE while estimating row count in external JDBC tables,"Running the query below for external tables produces NPE because of missing APIs to handle JDBC Converter and JdbcHiveTableScan in [RelMdDistinctRowCount.java|https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/rel/metadata/RelMdDistinctRowCount.java] (calcite). The catch-all method [getDistinctRowCount|https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/rel/metadata/RelMdDistinctRowCount.java#L76] returns null for HiveJdbcConverter and JdbcHiveTableScan, which ultimately results in a null value for *computeInnerJoinSelectivity(j, mq, predicate)* method at [HiveRelMdSelectivity.java|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java#L78]
{code:java}
double innerJoinSelectivity = computeInnerJoinSelectivity(j, mq, predicate);
{code}
Query:
{code:java}
explain cbo
with t1 as (select fkey, ikey, sum(dkey) as dk_sum, sum(dkey2) as dk2_sum
            from ext_simple_derby_table1 left join ext_simple_derby_table3
            on ikey = ikey2
            where fkey2 is null
            group by fkey, ikey),
t2 as (select datekey, fkey, ikey, sum(dkey) as dk_sum2, sum(dkey2) as dk2_sum2
       from ext_simple_derby_table2 left join ext_simple_derby_table4
       on ikey = ikey2
       where fkey2 is null
       group by datekey, fkey, ikey)
select t1.fkey, t2.ikey, sum(t1.ikey)
from t1 left join t2
on t1.ikey = t2.ikey AND t1.fkey = t2.fkey
where t2.fkey is null
group by t2.datekey, t1.fkey, t2.ikey
{code}
The stacktrace:
{code:java}
 java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.getSelectivity(HiveRelMdSelectivity.java:78)
	at GeneratedMetadataHandler_Selectivity.getSelectivity_$(Unknown Source)
	at GeneratedMetadataHandler_Selectivity.getSelectivity(Unknown Source)
	at GeneratedMetadataHandler_Selectivity.getSelectivity_$(Unknown Source)
	at GeneratedMetadataHandler_Selectivity.getSelectivity(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getSelectivity(RelMetadataQuery.java:426)
	at org.apache.calcite.rel.metadata.RelMdUtil.estimateFilteredRows(RelMdUtil.java:765)
	at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:131)
	at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.getRowCount(HiveRelMdRowCount.java:175)
	at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRuntimeRowCount.getRowCount(HiveRelMdRuntimeRowCount.java:53)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:205)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:140)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.calcite.rel.metadata.RelMdUtil.getJoinRowCount(RelMdUtil.java:723)
	at org.apache.calcite.rel.core.Join.estimateRowCount(Join.java:205)
	at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.getRowCount(HiveRelMdRowCount.java:113)
	at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source)
	at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source)
	at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:212)
	at org.apache.hadoop.hive.ql.optimizer.calcite.stats.FilterSelectivityEstimator.<init>(FilterSelectivityEstimator.java:62)
	at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSortPredicates$RexSortPredicatesShuttle.<init>(HiveFilterSortPredicates.java:126)
	at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSortPredicates$RexSortPredicatesShuttle.<init>(HiveFilterSortPredicates.java:120)
	at org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSortPredicates.onMatch(HiveFilterSortPredicates.java:91)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.executeProgram(CalcitePlanner.java:2440)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.executeProgram(CalcitePlanner.java:2406)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.applyPostJoinOrderingTransform(CalcitePlanner.java:2326)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1735)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1588)
	at org.apache.calcite.tools.Frameworks.lambda$withPlanner$0(Frameworks.java:131)
	at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:914)
	at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:180)
	at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:126)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.logicalPlan(CalcitePlanner.java:1340)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:559)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12513)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:452)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:175)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:317)
	at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:223)
	at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:105)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:500)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:453)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:417)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:411)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:125)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:229)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:256)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:201)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:127)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:353)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:744)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:714)
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:170)
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157)
	at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)

{code}"
HIVE-25287,NPE on insertions into Iceberg-backed tables,"NPE is thrown upon inserting values into an Iceberg table, as statistics generation triggers an alter table operation with undefined (alter)operation type:
{code:java}
INFO  : Executing stats task
INFO  : [Warning] could not update stats.Failed with exception null
java.lang.NullPointerException
        at org.apache.iceberg.mr.hive.HiveIcebergMetaHook.commitAlterTable(HiveIcebergMetaHook.java:283)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:572)
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(SessionHiveMetaStoreClient.java:500)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
        at com.sun.proxy.$Proxy124.alter_table(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:854)
        at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:806)
        at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:786)
        at org.apache.hadoop.hive.ql.stats.BasicStatsTask.aggregateStats(BasicStatsTask.java:279)
        at org.apache.hadoop.hive.ql.stats.BasicStatsTask.process(BasicStatsTask.java:100)
        at org.apache.hadoop.hive.ql.exec.StatsTask.execute(StatsTask.java:107)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:348)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:204)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:153)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:148)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:164)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:234)
        at org.apache.hive.service.cli.operation.SQLOperation.access$500(SQLOperation.java:89)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:337)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:357)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

 {code}"
HIVE-25051,Callers can access uninitialized MessageBuilder instance causing NPE,"The creation of the singleton MessageBuilder instance is unsafe, threads can access the uninitialized instance.

https://github.com/apache/hive/blob/326abf9685de39cf4f1b3222d84fe9cbc465710a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/messaging/MessageBuilder.java#L154"
HIVE-25009,Compaction worker and initiator version check can cause NPE if the COMPACTION_QUEUE is empty,
HIVE-24862,Fix race condition causing NPE during dynamic partition loading,"Following properties default to 15 threads.
{noformat}
hive.load.dynamic.partitions.thread
hive.mv.files.thread  
{noformat}
During loadDynamicPartitions, it ends ups initializing {{newFiles}} without synchronization (HIVE-20661, HIVE-24738). 
 [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2871]

This causes race condition when dynamic partition thread internally makes use of {{hive.mv.files.threads}} in copyFiles/replaceFiles. 
 This causes ""NPE"" during retrieval in {{addInsertFileInformation()}}.

 

e.g stacktrace
{noformat}
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.fs.FileSystem.fixRelativePart(FileSystem.java:2734)
  at org.apache.hadoop.hdfs.DistributedFileSystem.fixRelativePart(DistributedFileSystem.java:3396)
  at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1740)
  at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1740)
  at org.apache.hadoop.hive.ql.metadata.Hive.addInsertFileInformation(Hive.java:3566)
  at org.apache.hadoop.hive.ql.metadata.Hive.fireInsertEvent(Hive.java:3540)
  at org.apache.hadoop.hive.ql.metadata.Hive.loadPartitionInternal(Hive.java:2414)
  at org.apache.hadoop.hive.ql.metadata.Hive.lambda$loadDynamicPartitions$4(Hive.java:2909)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
{noformat}"
HIVE-24858,UDFClassLoader leak in Configuration.CACHE_CLASSES,"If a UDF jar has been registered in a session and a temporary function created from it, when the session is closed its UDFClassLoader is not GC'd as it has been leaked to the session's HiveConf object's cache. Since the ClassLoader is not GC'd, the UDF jar's classes aren't GC'd from Metaspace. This can potentially lead to Metaspace OOM.
 Path to GC root is:
{code:java}
Class Name                                                                                                                          | Shallow Heap | Retained Heap
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
contextClassLoader org.apache.hive.service.server.ThreadWithGarbageCleanup @ 0x7164deb50  HiveServer2-Handler-Pool: Thread-72 Thread|          128 |        79,072
referent java.util.WeakHashMap$Entry @ 0x7164e67d0                                                                                  |           40 |           824
'- [6] java.util.WeakHashMap$Entry[16] @ 0x71581aac0                                                                                |           80 |         5,056
   '- table java.util.WeakHashMap @ 0x71580f510                                                                                     |           48 |         6,920
      '- CACHE_CLASSES class org.apache.hadoop.conf.Configuration @ 0x71580f3d8                                                     |           64 |        74,528
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
{code}"
HIVE-24853,HMS leaks queries in case of timeout,The queries aren't closed in case of timeout.
HIVE-24851,resources leak on exception in AvroGenericRecordReader constructor,"AvroGenericRecordReader constructor creates an instance of FileReader but lacks proper exception handling, and reader is not closed on the failure path.

This results in leaking of underlying resources (e.g. S3 connections).

 "
HIVE-24841,Parallel edge fixer may run into NPE when RS is missing a duplicate column from the output schema,This may mean that the RS has an incorrect schema - but that will be investigated separately
HIVE-24792,Potential thread leak in Operation,"The _scheduledExecutorService_  in _Operation_ does not shut down after scheduling delay operationlog cleanup, which may result to thread leak in hiveserver2..."
HIVE-24683,Hadoop23Shims getFileId prone to NPE for non-existing paths,"HIVE-23840 introduced the feature of reading delete deltas from LLAP cache if it's available. This refactor opens an opportunity for NPE to happen:
{code:java}
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.shims.Hadoop23Shims.getFileId(Hadoop23Shims.java:1410)
at org.apache.hadoop.hive.ql.io.HdfsUtils.getFileId(HdfsUtils.java:55)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.determineFileId(OrcEncodedDataReader.java:509)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.getOrcTailForPath(OrcEncodedDataReader.java:579)
at org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.getOrcTailFromCache(LlapIoImpl.java:322)
at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.getOrcTail(VectorizedOrcAcidRowBatchReader.java:683)
at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.access$500(VectorizedOrcAcidRowBatchReader.java:82)
at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry.<init>(VectorizedOrcAcidRowBatchReader.java:1581){code}
ColumnizedDeleteEventRegistry infers the file name of a delete delta bucket by looking at the bucket number (from the corresponding split) but this file may not exist if no deletion happen from that particular bucket.

Earlier this was handled by always trying to open an ORC reader on the path and catching FileNotFoundException. However in the refactor we first try to look into the cache, and for that try to retrieve a file ID first. This entails a getFileStatus call on HDFS which returns null for non-existing paths, causing the NPE eventually.

This was later fixed by HIVE-23956, nevertheless Hadoop23Shims.getFileId should be refactored in a way that it's not error prone anymore."
HIVE-24653,Race condition between compactor marker generation and get splits,"In a rear scenario it's possible that the compactor moved the files in the final location before creating the compactor marker, so it can be fetched by get splits before the marker is created.

2020-09-14 04:55:25,978 [ERROR] ORC_GET_SPLITS #4 |io.AcidUtils|: Failed to read hdfs://host/warehouse/tablespace/managed/hive/database.db/table/partition=x/base_0011535/_metadata_acid: No content to map to Object due to end of input
java.io.EOFException: No content to map to Object due to end of input
"
HIVE-24636,Memory leak due to stacking UDFClassLoader in Apache Commons LogFactory,"Much the same as [HIVE-7563|https://issues.apache.org/jira/browse/HIVE-7563], after ClassLoader is closed in JavaUtils, it should be released by Apache Commons LogFactory, or the ClassLoader can't be Garbage Collected, which leads to memory leak, exactly our PROD met."
HIVE-24590,Operation Logging still leaks the log4j Appenders,"I'm using Hive 3.1.2 with options below.
 * hive.server2.logging.operation.enabled=true
 * hive.server2.logging.operation.level=VERBOSE
 * hive.async.log.enabled=false

I already know the ticket, https://issues.apache.org/jira/browse/HIVE-17128 but HS2 still leaks log4j RandomAccessFileManager.

!Screen Shot 2021-01-06 at 18.42.05.png|width=756,height=197!

I checked the operation log file which is not closed/deleted properly.

!Screen Shot 2021-01-06 at 18.42.24.png|width=603,height=272!

Then there's the log,
{code:java}
client.TezClient: Shutting down Tez Session, sessionName= ....{code}
!Screen Shot 2021-01-06 at 18.42.55.png|width=1372,height=26!"
HIVE-24585,NPE in VectorizedOrcAcidRowBatchReader if LLAP is used with IO disabled,"NPE is thrown if LLAP mode is turned on and LLAP daemon executes a query on an ACID table if LLAP IO is disabled. Although this doesn't seem to be a very useful LLAP environment setup, we'll need to cover this edge case too.
{code:java}
Caused by: java.lang.RuntimeException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:156)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:82)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:543)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:189)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)
	... 15 more
Caused by: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:431)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)
	... 26 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.getOrcTail(VectorizedOrcAcidRowBatchReader.java:680)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.findMinMaxKeys(VectorizedOrcAcidRowBatchReader.java:426)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:273)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:159)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:154)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2074)
	at org.apache.hadoop.hive.ql.io.RecordReaderWrapper.create(RecordReaderWrapper.java:72)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:428)
	... 27 more {code}"
HIVE-24569,LLAP daemon leaks file descriptors/log4j appenders,"With HIVE-9756 query logs in LLAP are directed to different files (file per query) using a Log4j2 routing appender. Without a purge policy in place, appenders are created dynamically by the routing appender, one for each query, and remain in memory forever. The dynamic appenders write to files so each appender holds to a file descriptor. 

Further work HIVE-14224 has mitigated the issue by introducing a custom purging policy (LlapRoutingAppenderPurgePolicy) which deletes the dynamic appenders (and closes the respective files) when the query is completed (org.apache.hadoop.hive.llap.daemon.impl.QueryTracker#handleLogOnQueryCompletion). 

However, in the presence of multiple threads appending to the logs there are race conditions. In an internal Hive cluster the number of file descriptors started going up approx one descriptor leaking per query. After some debugging it turns out that one thread (running the QueryTracker#handleLogOnQueryCompletion) signals that the query has finished and thus the purge policy should get rid of the respective appender (and close the file) while another (Task-Executor-0) attempts to append another log message for the same query. The initial appender is closed after the request from the query tracker but a new one is created to accomodate the message from the task executor and the latter is never removed thus creating a leak. 

Similar leaks have been identified and fixed for HS2 with the most similar one being that described [here|https://issues.apache.org/jira/browse/HIVE-22753?focusedCommentId=17021041&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17021041]. 

The problem relies on the timing of threads so it may not manifestate in all versions between 2.2.0 and 4.0.0. Usually the leak can be seen either via lsof (or other similar command) with the following output:

{noformat}
# 1494391 is the PID of the LLAP daemon process
ls -ltr /proc/1494391/fd
...
lrwx------ 1 hive hadoop 64 Dec 24 12:08 978 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121724_66ce273d-54a9-4dcd-a9fb-20cb5691cef7-dag_1608659125567_0008_194.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 977 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121804_ce53eeb5-c73f-4999-b7a4-b4dd04d4e4de-dag_1608659125567_0008_197.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 974 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224122002_1693bd7d-2f0e-4673-a8d1-b7cb14a02204-dag_1608659125567_0008_204.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 989 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121909_6a56218f-06c7-4906-9907-4b6dd824b100-dag_1608659125567_0008_201.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 984 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121754_78ef49a0-bc23-478f-9a16-87fa25e7a287-dag_1608659125567_0008_196.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 983 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121855_e65b9ebf-b2ec-4159-9570-1904442b7048-dag_1608659125567_0008_200.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 981 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121818_e9051ae3-1316-46af-aabb-22c53ed2fda7-dag_1608659125567_0008_198.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 980 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121744_fcf37921-4351-4368-95ee-b5be2592d89a-dag_1608659125567_0008_195.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 979 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121837_e80c0024-f6bc-4b3c-85ed-5c0c85c55787-dag_1608659125567_0008_199.log
{noformat}

or in the heap dump with many appenders (in my case {{LlapWrappedAppender}}) holding indirectly open file descriptors:
!llap-appender-gc-roots.png! 

"
HIVE-24552,Possible HMS connections leak or accumulation in loadDynamicPartitions,"When loadDynamicPartitions (Hive.java) is called, it generates several threads to handle FileMove. These threads may generate HiveMetaStore connections. These connections may not be closed in time and cause many accumulated connections. Following is the log got from running insert overwrites many times, you can see these threads created new HMS connections, and the total number of open connections is large. And the finalizer closes the connections and sometimes had errors:
{noformat}
<14>1 2020-12-15T17:06:15.894Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-14""] Opened a connection to metastore, current connections: 44021
<14>1 2020-12-15T17:06:15.894Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-14""] Connected to metastore.
<14>1 2020-12-15T17:06:15.894Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.RetryingMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-14""] RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive/dwx-env-mdrkp9@HALXG.CLOUDERA.COM (auth:KERBEROS) retries=24 delay=5 lifetime=0
<14>1 2020-12-15T17:06:15.895Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-5""] Opened a connection to metastore, current connections: 44022
<14>1 2020-12-15T17:06:15.895Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-5""] Connected to metastore.
<14>1 2020-12-15T17:06:15.895Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.RetryingMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-5""] RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive/dwx-env-mdrkp9@HALXG.CLOUDERA.COM (auth:KERBEROS) retries=24 delay=5 lifetime=0
<14>1 2020-12-15T17:06:15.895Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-6""] Opened a connection to metastore, current connections: 44023
<14>1 2020-12-15T17:06:15.895Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-6""] Connected to metastore.
<14>1 2020-12-15T17:06:15.895Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.RetryingMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-6""] RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive/dwx-env-mdrkp9@HALXG.CLOUDERA.COM (auth:KERBEROS) retries=24 delay=5 lifetime=0
<14>1 2020-12-15T17:06:15.895Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""load-dynamic-partitionsToAdd-3""] Opened a connection to metastore, current connections: 44024
....

<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43904
<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43903
<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43902
<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43901
<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43900
<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43899
<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43898
<14>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""metastore.HiveMetaStoreClient"" level=""INFO"" thread=""Finalizer""] Closed a connection to metastore, current connections: 43897
<12>1 2020-12-15T17:05:38.485Z hiveserver2-0 hiveserver2 1 a3671b96-74fb-4ee9-b186-aeff0de0bbec [mdc@18060 class=""transport.TIOStreamTransport"" level=""WARN"" thread=""Finalizer""] Error closing output stream.
java.net.SocketException: Socket closed
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
	at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)
	at org.apache.thrift.transport.TSocket.close(TSocket.java:235)
	at org.apache.thrift.transport.TSaslTransport.close(TSaslTransport.java:400)
	at org.apache.thrift.transport.TSaslClientTransport.close(TSaslClientTransport.java:37)
	at org.apache.hadoop.hive.metastore.security.TFilterTransport.close(TFilterTransport.java:52)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close(HiveMetaStoreClient.java:729)
	at sun.reflect.GeneratedMethodAccessor160.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212)
	at com.sun.proxy.$Proxy59.close(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor160.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:3411)
	at com.sun.proxy.$Proxy59.close(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.close(Hive.java:540)
	at org.apache.hadoop.hive.ql.metadata.Hive.finalize(Hive.java:512)
	at java.lang.System$2.invokeFinalize(System.java:1273)
	at java.lang.ref.Finalizer.runFinalizer(Finalizer.java:102)
	at java.lang.ref.Finalizer.access$100(Finalizer.java:34)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:217)
{noformat}"
HIVE-24548,CompactionHeartbeater leaks metastore connections,"Every Heartbeater thread creates a new metastore client, that is never closed"
HIVE-24530,Potential NPE in FileSinkOperator.closeRecordwriters method,"During testing a NPE occurred in the FileSinkOperator.closeRecordwriters method.
After investigating, turned out there was an underlaying IOException during executing the FileSinkOperator.process method. It got caught by the following code part:
{noformat}
    } catch (IOException e) {
      closeWriters(true);
      throw new HiveException(e);
    } catch (SerDeException e) {
      closeWriters(true);
      throw new HiveException(e);
    }
{noformat}
First the closeWriters method was called:
{noformat}
  private void closeWriters(boolean abort) throws HiveException {
    fpaths.closeWriters(true);
    closeRecordwriters(true);
  }

  private void closeRecordwriters(boolean abort) {
    for (RecordWriter writer : rowOutWriters) {
      try {
        LOG.info(""Closing {} on exception"", writer);
        writer.close(abort);
      } catch (IOException e) {
        LOG.error(""Error closing rowOutWriter"" + writer, e);
      }
    }
{noformat}
If the writers had got closed successfully, a HiveException would have been thrown with the original IOException.
But when the IOException occurred the writers in the rowOutWriters were not yet initialised, so a NPE occurred. This was very misleading as the NPE was not the real issue, but the original IOException was hidden.
"
HIVE-24520,Fix stackoverflow error in HiveMetaStore::get_partitions_by_names,"Need to fix the recursive call of the same method.

 

(May have been introduced as a part of https://issues.apache.org/jira/browse/HIVE-22017) 

 
{code:java}
  @Override
    @Deprecated
    public List<Partition> get_partitions_by_names(final String dbName, final String tblName,
                                                   final List<String> partNames)
            throws TException {
      return get_partitions_by_names(dbName, tblName, partNames);
    }

    
        

 {code}"
HIVE-24411,Make ThreadPoolExecutorWithOomHook more awareness of OutOfMemoryError,"Now the ThreadPoolExecutorWithOomHook invokes some oom hooks and stops the HiveServer2 in case of OutOfMemoryError when executing the tasks. The exception is obtained by calling method _future.get()_, however the exception should not be an instance of OutOfMemoryError,  as the exception is wrapped in ExecutionException,  refer to the method _report_ in FutureTask."
HIVE-24297,LLAP buffer collision causes NPE,"HIVE-23741 introduced an optimization so that CacheTags are not stored on buffer level, but rather on file level, as one cache tag can only relate to one file. With this change a buffer->filecache reference was introduced so that the buffer's tag can be calculated with an extra indirection i.e. buffer.filecache.tag.

However during buffer collision in putFileData method, we don't set the filecache reference of the collided (new) buffer: [https://github.com/apache/hive/commit/2e18a7408a8dd49beecad8d66bfe054b7dc474da#diff-d2ccd7cf3042845a0812a5e118f82db49253d82fc86449ffa408903bf434fb6dR309-R311]

Later this cases NPE when the new (instantly decRef'ed) buffer is evicted:
{code:java}
Caused by: java.lang.NullPointerException
        at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:778)
        at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546)
        at org.apache.hadoop.hive.llap.cache.CacheContentsTracker.getTagState(CacheContentsTracker.java:129)
        at org.apache.hadoop.hive.llap.cache.CacheContentsTracker.getTagState(CacheContentsTracker.java:125)
        at org.apache.hadoop.hive.llap.cache.CacheContentsTracker.reportRemoved(CacheContentsTracker.java:109)
        at org.apache.hadoop.hive.llap.cache.CacheContentsTracker.notifyEvicted(CacheContentsTracker.java:238)
        at org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.evictSomeBlocks(LowLevelLrfuCachePolicy.java:276)
        at org.apache.hadoop.hive.llap.cache.CacheContentsTracker.evictSomeBlocks(CacheContentsTracker.java:177)
        at org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.reserveMemory(LowLevelCacheMemoryManager.java:98)
        at org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.reserveMemory(LowLevelCacheMemoryManager.java:65)
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:323)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.allocateMultiple(EncodedReaderImpl.java:1302)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedStream(EncodedReaderImpl.java:930)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:506)
        ... 16 more {code}"
HIVE-24293,Integer overflow in llap collision mask,"If multiple threads put the same buffer to the cache, only one succeeds. The other one detects this, and replaces its own buffer. This is marked by a bit mask encoded in a long, where the collided buffers are marked with a 1."
HIVE-24266,Committed rows in hflush'd ACID files may be missing from query result,"in HDFS environment if a writer is using hflush to write ORC ACID files during a transaction commit, the results might be seen as missing when reading the table before this file is completely persisted to disk (thus synced)

This is due to hflush not persisting the new buffers to disk, it rather just ensures that new readers can see the new content. This causes the block information to be incomplete, on which BISplitStrategy relies on. Although the side file (_flush_length) tracks the proper end of the file that is being written, this information is neglected in the favour of block information, and we may end up generating a very short split instead of the larger, available length.
When ETLSplitStrategy is used there is not even a try to rely on ACID side file when calculating file length, so that needs to fixed too.

Moreover we might see the newly committed rows not to appear due to OrcTail caching in ETLSplitStrategy. For now I'm just going to recommend turning that cache off to anyone that wants real time row updates to be read in:
{code:java}
set hive.orc.cache.stripe.details.mem.size=0;  {code}
..as tweaking with that code would probably open a can of worms.."
HIVE-24236,Connection leak in TxnHandler,"We see failures in QE tests with cannot allocate connections errors. The exception stack like following:
{noformat}
2020-09-29T18:44:26,563 INFO  [Heartbeater-0]: txn.TxnHandler (TxnHandler.java:checkRetryable(3733)) - Non-retryable error in heartbeat(HeartbeatRequest(lockid:0, txnid:11908)) : Cannot get a connection, general error (SQLState=null, ErrorCode=0)
2020-09-29T18:44:26,564 ERROR [Heartbeater-0]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(201)) - MetaException(message:Unable to select from transaction database org.apache.commons.dbcp.SQLNestedException: Cannot get a connection, general error
        at org.apache.commons.dbcp.PoolingDataSource.getConnection(PoolingDataSource.java:118)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:3605)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:3598)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeat(TxnHandler.java:2739)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.heartbeat(HiveMetaStore.java:8452)
        at sun.reflect.GeneratedMethodAccessor415.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy63.heartbeat(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.heartbeat(HiveMetaStoreClient.java:3247)
        at sun.reflect.GeneratedMethodAccessor414.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:213)
        at com.sun.proxy.$Proxy64.heartbeat(Unknown Source)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat(DbTxnManager.java:671)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.lambda$run$0(DbTxnManager.java:1102)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.run(DbTxnManager.java:1101)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at org.apache.commons.pool.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:1112)
        at org.apache.commons.dbcp.PoolingDataSource.getConnection(PoolingDataSource.java:106)
        ... 29 more
)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeat(TxnHandler.java:2747)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.heartbeat(HiveMetaStore.java:8452)
        at sun.reflect.GeneratedMethodAccessor415.invoke(Unknown Source)
{noformat}

and
{noformat}
Caused by: java.util.NoSuchElementException: Timeout waiting for idle object
        at org.apache.commons.pool.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:1134)
        at org.apache.commons.dbcp.PoolingDataSource.getConnection(PoolingDataSource.java:106)
        ... 53 more
)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.cleanupRecords(TxnHandler.java:3375)
        at org.apache.hadoop.hive.metastore.AcidEventListener.onDropTable(AcidEventListener.java:65)
        at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier$19.notify(MetaStoreListenerNotifier.java:103)
        at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:285)
        at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:347)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:2986)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:3240)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table(HiveMetaStore.java:3227)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_database_core(HiveMetaStore.java:1879)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_database(HiveMetaStore.java:1978)
        at sun.reflect.GeneratedMethodAccessor369.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy63.drop_database(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabaseCascadePerDb(HiveMetaStoreClient.java:1393)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1324)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:1277)
        at sun.reflect.GeneratedMethodAccessor368.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:213)
        at com.sun.proxy.$Proxy64.dropDatabase(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:618)
        at org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.execute(DropDatabaseOperation.java:50)
        at org.apache.hadoop.hive.ql.ddl.DDLTask.execute(DDLTask.java:80)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
        at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:357)
        at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:330)
        at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:246)
        at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:109)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:740)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:495)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:489)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:225)
{noformat}

It is not easy to reproduce, so there are possible connection leaks under some error conditions. This jira is to fix code that may cause connection leaks in TxnHandler."
HIVE-24179,Memory leak in HS2 DbTxnManager when compiling SHOW LOCKS statement,"The problem can be reproduced by executing repeatedly a SHOW LOCK statement and monitoring the heap memory of HS2. For a small heap (e.g., 2g) it only takes a few minutes before the server crashes with OutOfMemory error such as the one shown below.

{noformat}
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.util.Arrays.copyOf(Arrays.java:3332)
        at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
        at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)
        at java.lang.StringBuilder.append(StringBuilder.java:136)
        at org.apache.maven.surefire.booter.ForkedChannelEncoder.encodeMessage(ForkedChannelEncoder.j
        at org.apache.maven.surefire.booter.ForkedChannelEncoder.setOutErr(ForkedChannelEncoder.java:
        at org.apache.maven.surefire.booter.ForkedChannelEncoder.stdErr(ForkedChannelEncoder.java:166
        at org.apache.maven.surefire.booter.ForkingRunListener.writeTestOutput(ForkingRunListener.jav
        at org.apache.maven.surefire.report.ConsoleOutputCapture$ForwardingPrintStream.write(ConsoleO
        at org.apache.logging.log4j.core.util.CloseShieldOutputStream.write(CloseShieldOutputStream.j
        at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStream
        at org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager
        at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:
        at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(Abst
        at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutp
        at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputS
        at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:
        at org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:12
        at org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(Appender
        at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:84)
        at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:543)
        at org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:502)
        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:485)
        at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:460)
        at org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletio
        at org.apache.logging.log4j.core.Logger.log(Logger.java:162)
        at org.apache.logging.log4j.spi.AbstractLogger.tryLogMessage(AbstractLogger.java:2190)
        at org.apache.logging.log4j.spi.AbstractLogger.logMessageTrackRecursion(AbstractLogger.java:2
        at org.apache.logging.log4j.spi.AbstractLogger.logMessageSafely(AbstractLogger.java:2127)
        at org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:2008)
        at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1867)
        at org.apache.logging.slf4j.Log4jLogger.info(Log4jLogger.java:179)
{noformat}

The heap dump shows (summary.png) that most of the memory is consumed by {{Hashtable$Entry}} and {{ConcurrentHashMap$Node}} objects coming from Hive configurations referenced by {{DbTxnManager}}. 

The latter are not eligible for garbage collection since at [construction|https://github.com/apache/hive/blob/975c832b6d069559c5b406a4aa8def3180fe4e75/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java#L212] time they are passed implicitly in a callback  stored inside ShutdownHookManager.  

When the {{DbTxnManager}} is closed properly the leak is not present since the callback is [removed|https://github.com/apache/hive/blob/975c832b6d069559c5b406a4aa8def3180fe4e75/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java#L882] from ShutdownHookManager. 

{{SHOW LOCKS}} statements create ([ShowDbLocksAnalyzer|https://github.com/apache/hive/blob/975c832b6d069559c5b406a4aa8def3180fe4e75/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowDbLocksAnalyzer.java#L52], [ShowLocksAnalyzer|https://github.com/apache/hive/blob/975c832b6d069559c5b406a4aa8def3180fe4e75/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowLocksAnalyzer.java#L72]) a new {{TxnManager}} and they never close it leading to the memory leak.
"
HIVE-24113,NPE in GenericUDFToUnixTimeStamp,"Following query will trigger the getPartitionsByExpr call at HMS, HMS will try to evaluate the filter based on the PartitionExpressionForMetastore proxy, this proxy uses the QL packages to evaluate the filter and call GenericUDFToUnixTimeStamp.

select * from table_name where hour between from_unixtime(unix_timestamp('2020090120', 'yyyyMMddHH') - 1*60*60, 'yyyyMMddHH') and from_unixtime(unix_timestamp('2020090122', 'yyyyMMddHH') + 2*60*60, 'yyyyMMddHH');

I think SessionState in the code path will always be NULL thats why it hit the NPE.


{code:java}
java.lang.NullPointerException: null
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.initializeInput(GenericUDFToUnixTimeStamp.java:126) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.initialize(GenericUDFToUnixTimeStamp.java:75) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:148) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:146) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:140) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:140) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:140) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:140) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.prepareExpr(PartExprEvalUtils.java:119) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.prunePartitionNames(PartitionPruner.java:551) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.filterPartitionsByExpr(PartitionExpressionForMetastore.java:82) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionNamesPrunedByExprNoTxn(ObjectStore.java:3527) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.metastore.ObjectStore.access$1400(ObjectStore.java:252) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.metastore.ObjectStore$10.getJdoResult(ObjectStore.java:3493) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.metastore.ObjectStore$10.getJdoResult(ObjectStore.java:3464) ~[hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:3764) [hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:3499) [hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:3452) [hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) [hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at com.sun.proxy.$Proxy28.getPartitionsByExpr(Unknown Source) [?:?]
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:6637) [hive-exec-3.1.0.3.1.5.65-1.jar:3.1.0.3.1.5.65-1]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.ja


{code}
"
HIVE-24104,NPE due to null key columns in ReduceSink after deduplication,"In some cases the {{ReduceSinkDeDuplication}} optimization creates ReduceSink operators where the key columns are null. This can lead to NPE in various places in the code. 

The following stracktraces show some places where a NPE appears. Note that the stacktraces do not correspond to the same query.

+NPE  during planning+
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.plan.ExprNodeDesc$ExprNodeDescEqualityWrapper.equals(ExprNodeDesc.java:141)
	at java.util.AbstractList.equals(AbstractList.java:523)
	at org.apache.hadoop.hive.ql.optimizer.SetReducerParallelism.process(SetReducerParallelism.java:101)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
	at org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:74)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:492)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:226)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:161)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12643)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:443)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:301)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:171)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:301)
	at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:220)
	at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:104)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:173)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:414)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:363)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:357)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:129)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:231)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:355)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:740)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:710)
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:170)
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157)
	at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
{noformat}

+NPE at runtime+
{noformat}
org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1598975134540_0001_2_00, diagnostics=[Task failed, taskId=task_1598975134540_0001_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1598975134540_0001_2_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)
	... 15 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(ReduceSinkOperator.java:242)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:359)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeMapOperator(MapOperator.java:506)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:314)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(ReduceSinkOperator.java:160)
	... 37 more
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1598975134540_0001_2_00_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:118)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:351)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)
	... 15 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(ReduceSinkOperator.java:242)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:359)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:548)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:368)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeMapOperator(MapOperator.java:506)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:314)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(ReduceSinkOperator.java:160)
	... 37 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1598975134540_0001_2_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1598975134540_0001_2_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:1, Vertex vertex_1598975134540_0001_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:244)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:213)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:105)
	at org.apache.hadoop.hive.ql.Executor.launchTask(Executor.java:361)
	at org.apache.hadoop.hive.ql.Executor.launchTasks(Executor.java:334)
	at org.apache.hadoop.hive.ql.Executor.runTasks(Executor.java:245)
	at org.apache.hadoop.hive.ql.Executor.execute(Executor.java:108)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:498)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:307)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:302)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:166)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:232)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:203)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:129)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:355)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:740)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:710)
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:170)
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157)
	at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
{noformat}

"
HIVE-24097,correct NPE exception in HiveMetastoreAuthorizer,"In some testing, we found it's possible to have NPE if the preEventType does not fall within the several the HMS currently checks. This makes the AuthzContext a null pointer. "
HIVE-24070,ObjectStore.cleanWriteNotificationEvents OutOfMemory on large number of pending events,"If there are large number of events that haven't been cleaned up for some reason, then ObjectStore.cleanWriteNotificationEvents() can run out of memory while it loads all the events to be deleted.
 It should fetch events in batches.

Similar to https://issues.apache.org/jira/browse/HIVE-19430"
HIVE-23935,Fetching primaryKey through beeline fails with NPE,"Fetching PrimaryKey of a table through Beeline !primarykey fails with NPE
{noformat}
0: jdbc:hive2://localhost:10000> !primarykeys Persons
Error: MetaException(message:java.lang.NullPointerException) (state=,code=0)
org.apache.hive.service.cli.HiveSQLException: MetaException(message:java.lang.NullPointerException)
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:360)
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:351)
	at org.apache.hive.jdbc.HiveDatabaseMetaData.getPrimaryKeys(HiveDatabaseMetaData.java:573)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hive.beeline.Reflector.invoke(Reflector.java:89)
	at org.apache.hive.beeline.Commands.metadata(Commands.java:125)
	at org.apache.hive.beeline.Commands.primarykeys(Commands.java:231)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hive.beeline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:57)
	at org.apache.hive.beeline.BeeLine.execCommandWithPrefix(BeeLine.java:1465)
	at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1504)
	at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:1364)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1134)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1082)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:546)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:528)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236){noformat}"
HIVE-23873,Querying Hive JDBCStorageHandler table fails with NPE when CBO is off,"Scenario is Hive table having same schema as table in Oracle, however when we query the table with data it fails with NPE, below is the trace.

{code}
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:617) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:524) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2739) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:473) ~[hive-service-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        ... 34 more
Caused by: java.lang.NullPointerException
        at org.apache.hive.storage.jdbc.JdbcSerDe.deserialize(JdbcSerDe.java:164) ~[hive-jdbc-handler-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:598) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:524) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2739) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:473) ~[hive-service-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        ... 34 more
{code}

Problem appears when column names in Oracle are in Upper case and since in Hive, table and column names are forced to store in lowercase during creation. User runs into NPE error while fetching data.

While deserializing data, input consists of column names in lower case which fails to get the value

https://github.com/apache/hive/blob/rel/release-3.1.2/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcSerDe.java#L136
{code}
rowVal = ((ObjectWritable)value).get();
{code}

Log Snio:
=============
{code}
2020-07-17T16:49:09,598 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: dao.GenericJdbcDatabaseAccessor (:()) - Query to execute is [select * from TESTHIVEJDBCSTORAGE]
2020-07-17T16:49:10,642 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: jdbc.JdbcSerDe (:()) - *** ColumnKey = ID
2020-07-17T16:49:10,642 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: jdbc.JdbcSerDe (:()) - *** Blob value = {fname=OW[class=class java.lang.String,value=Name1], id=OW[class=class java.lang.Integer,value=1]}
{code}

Simple Reproducer for this case.
=============
1. Create table in Oracle
{code}
create table TESTHIVEJDBCSTORAGE(ID INT, FNAME VARCHAR(20));
{code}

2. Insert dummy data.
{code}
Insert into TESTHIVEJDBCSTORAGE values (1, 'Name1');
{code}

3. Create JDBCStorageHandler table in Hive.
{code}
CREATE EXTERNAL TABLE default.TESTHIVEJDBCSTORAGE_HIVE_TBL (ID INT, FNAME VARCHAR(20)) 
STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' 
TBLPROPERTIES ( 
""hive.sql.database.type"" = ""ORACLE"", 
""hive.sql.jdbc.driver"" = ""oracle.jdbc.OracleDriver"", 
""hive.sql.jdbc.url"" = ""jdbc:oracle:thin:@orachehostname/XE"", 
""hive.sql.dbcp.username"" = ""chiran"", 
""hive.sql.dbcp.password"" = ""supersecurepassword"", 
""hive.sql.table"" = ""TESTHIVEJDBCSTORAGE"", 
""hive.sql.dbcp.maxActive"" = ""1"" 
);
{code}

4. Query Hive table, fails with NPE.
{code}
> select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL;
INFO  : Compiling command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc): select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:testhivejdbcstorage_hive_tbl.id, type:int, comment:null), FieldSchema(name:testhivejdbcstorage_hive_tbl.fname, type:varchar(20), comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc); Time taken: 9.914 seconds
INFO  : Executing command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc): select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL
INFO  : Completed executing command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc); Time taken: 0.019 seconds
INFO  : OK
Error: java.io.IOException: java.lang.NullPointerException (state=,code=0)
{code}

Assuming that there are no repercussions, can we convert the column names to lowercase fetched from Database/Query pointing to table in JDBCStorageHandler?
Attaching the patch for the case."
HIVE-23800,Add hooks when HiveServer2 stops due to OutOfMemoryError,"Make oom hook an interface of HiveServer2,  so user can implement the hook to do something before HS2 stops, such as dumping the heap or altering the devops."
HIVE-23764,Remove unnecessary getLastFlushLength when checking delete delta files,"VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry calls OrcAcidUtils.getLastFlushLength for every delete delta file.
Even the comment says:
{code}
              // NOTE: Calling last flush length below is more for future-proofing when we have
              // streaming deletes. But currently we don't support streaming deletes, and this can
              // be removed if this becomes a performance issue.
{code}

If we have a table with 5 updates (1 base + 5 delta + 5 delete_delta), then for every base + delta dir we will check all of the delete_delta directories, and check the getLastFlushLength method which will result in 6*5=30 unnecessary NN/S3 calls.

We should remove the check as already proposed in the comment."
HIVE-23615,Do not deference null pointers in Beeline Commands Class,"[This pull request|https://github.com/apache/hive/pull/62] is focused on resolving occurrences of Sonar rule squid:S2259

Beeline: Null pointers should not be dereferenced"
HIVE-23580,"deleteOnExit set is not cleaned up, causing memory pressure",removeScratchDir doesn't always calls cancelDeleteOnExit() on context::clear
HIVE-23534,NPE in RetryingMetaStoreClient#invoke when catching MetaException with no message,"RetryingMetaStoreClient#invoke method catches MetaException and attempts to classify it by checking the message. However there are cases (e.g., various places in [ObjectStore|https://github.com/apache/hive/blob/716f1f9a945a9a11e6702754667660d27e0a5cf4/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L3916]) where the message of the MetaException is null and this leads to NPE."
HIVE-23498,Disable HTTP Trace method on ThriftHttpCliService,
HIVE-23436,Staging directory is not removed for stats gathering tasks,"When running a query which generates stats, then the staging directory is not removed when the query is finished"
HIVE-23388,CTAS queries should use target's location for staging.,"In cloud based storage systems, renaming files across different root level buckets seem to be disallowed. The S3AFileSystem throws the following exception. This appears to be bug in S3FS impl.

Failed with exception Wrong FS s3a://hive-managed/clusters/env-xxxxx/warehouse-xxxx-xxxx/warehouse/tablespace/managed/hive/tpch.db/customer/delta_0000001_0000001_0000 -expected s3a://hive-external
2020-04-27T19:34:27,573 INFO  [Thread-6] jdbc.TestDriver: java.lang.IllegalArgumentException: Wrong FS s3a://hive-managed//clusters/env-xxxx/warehouse-xxxx-xxxx/warehouse/tablespace/managed/hive/tpch.db/customer/delta_0000001_0000001_0000 -expected s3a://hive-external

But we should fix our query plans to use the target table's directory for staging as well. That should resolve this issue and it is the right thing to do as well (in case there are different encryption zones/keys for these buckets).

Fix in HIVE-22995 probably changed this behavior."
HIVE-23305,NullPointerException in LlapTaskSchedulerService addNode due to race condition,"{code:java}
java.lang.NullPointerException at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.addNode(LlapTaskSchedulerService.java:1575)    at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.registerAndAddNode(LlapTaskSchedulerService.java:1566) at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.access$1800(LlapTaskSchedulerService.java:128) at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService$NodeStateChangeListener.onCreate(LlapTaskSchedulerService.java:831)    at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService$NodeStateChangeListener.onCreate(LlapTaskSchedulerService.java:823)    at org.apache.hadoop.hive.registry.impl.ZkRegistryBase$InstanceStateChangeListener.childEvent(ZkRegistryBase.java:612)   at  {code}
The above exception happens when a node registers too fast, before the active activeInstances field was initialized.

 

The registry is started and the listener is registered before initializing activeInstances.
{code:java}
registry.start();
registry.registerStateChangeListener(new NodeStateChangeListener());
activeInstances = registry.getInstances(); {code}
 "
HIVE-23295,Possible NPE when on getting predicate literal list when dynamic values are not available,"getLiteralList() in SearchArgumentImpl$PredicateLeafImpl returns null if dynamic values are not available.
{code:java}
@Override
public List<Object> getLiteralList() {
  if (literalList != null && literalList.size() > 0 && literalList.get(0) instanceof LiteralDelegate) {
    List<Object> newLiteraList = new ArrayList<Object>();
    try {
      for (Object litertalObj : literalList) {
        Object literal = ((LiteralDelegate) litertalObj).getLiteral();
        if (literal != null) {
          newLiteraList.add(literal);
        }
      }
    } catch (NoDynamicValuesException err) {
      LOG.debug(""Error while retrieving literalList, returning null"", err);
      return null;
    }
    return newLiteraList;
  }
  return literalList;
} {code}
 

There are multiple call sites where the return value is used without a null check. E.g:  leaf.getLiteralList().stream(). 

 

The return null was added as part of HIVE-18827 to avoid having an unimportant warning message when dynamic values have not been delivered yet.

 

[~sershe], [~jdere], I propose return an empty list instead of null in a case like this. What do you think?"
HIVE-23246,Reduce MiniDruidCluster memory requeirements,"this is needed to enable the druid and kafka related cli tests to be run reliably with at most 8G of memory

* reduce processing buffer size to 10M  (from 200M)
* set Xmx512m for spawned peon processes"
HIVE-23212,SemanticAnalyzer::getStagingDirectoryPathname should check for encryption zone only when needed,"[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java#L2572]

 

When cluster does not have encryption zones configured, this ends up making 2 calls to NN unnecessarily. It would be good to guard it with config or check for the KMS config from HDFS and invoke it on need basis."
HIVE-23079,Remove Calls to printStackTrace in Module hive-serde,
HIVE-23077,Remove Calls to printStackTrace in Module hive-jdbc,"Only one ""tricky"" change.  Throw an Exception instead of {{printStackTrace}} in the static Driver loader as suggested from the reference here:

https://github.com/mariadb-corporation/mariadb-connector-j/blob/3bc66153b51aca188afc50ff35a0123f16c099ed/src/main/java/org/mariadb/jdbc/Driver.java#L72"
HIVE-23064,Remove Calls to printStackTrace in Module hive-exec,
HIVE-23057,ColumnStatsMergerFactory NPE Possible,"{code:java|title=ColumnStatsMergerFactory.java}
    // make sure that they have the same type
    typeNew = typeNew == typeOld ? typeNew : null;
    switch (typeNew) {
    case BOOLEAN_STATS:
{code}

This will throw a NPE if 'typeNew' does not equal 'typeOld'.  Better to throw an Exception with a helpful error message here.

Also add some cleanup and more null-safety checks in the code to fail-fast."
HIVE-22898,CharsetDecoder race condition in OrcRecordUpdater ,"Instances of CharsetDecoder are not thread safe, causing race condition in OrcRecordUpdater"
HIVE-22840,Race condition in formatters of TimestampColumnVector and DateColumnVector ,"HIVE-22405 added support for proleptic calendar. It uses java's SimpleDateFormat/Calendar APIs which are not thread-safe and cause race in some scenarios. 

As a result of those race conditions, we see some exceptions like
{code:java}
1) java.lang.NumberFormatException: For input string: """" 
OR 
java.lang.NumberFormatException: For input string: "".821582E.821582E44""

OR

2) Caused by: java.lang.ArrayIndexOutOfBoundsException: -5325980
    	at sun.util.calendar.BaseCalendar.getCalendarDateFromFixedDate(BaseCalendar.java:453)
    	at java.util.GregorianCalendar.computeFields(GregorianCalendar.java:2397)
{code}


This issue is to address those thread-safety issues/race conditions.

cc [~jcamachorodriguez] [~abstractdog] [~omalley]"
HIVE-22829,Decimal64: NVL in vectorization miss NPE with CBO on,"{code}
select  
sum(NVL(ss_sales_price, 1.0BD))
from store_sales where ss_sold_date_sk % 1111 = 1;
{code}

{code}
|                 notVectorizedReason: exception: java.lang.NullPointerException stack trace: org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.fixDecimalDataTypePhysicalVariations(Vectorizer.java:4754), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.fixDecimalDataTypePhysicalVariations(Vectorizer.java:4687), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.vectorizeSelectOperator(Vectorizer.java:4669), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateAndVectorizeOperator(Vectorizer.java:5269), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.doProcessChild(Vectorizer.java:977), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.doProcessChildren(Vectorizer.java:864), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateAndVectorizeOperatorTree(Vectorizer.java:834), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.access$2500(Vectorizer.java:245), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateAndVectorizeMapOperators(Vectorizer.java:2103), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateAndVectorizeMapOperators(Vectorizer.java:2055), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateAndVectorizeMapWork(Vectorizer.java:2030), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:1185), org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:1017), org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111), org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180), ... |
{code}"
HIVE-22788,Query cause NPE due to implicit cast on ROW__ID,"*Repro*
{code:sql}
CREATE TABLE table_16 (
timestamp_col_19    timestamp,
timestamp_col_29    timestamp,
int_col_27          int,
int_col_39          int,
boolean_col_18      boolean,
varchar0045_col_23  varchar(45)
);


CREATE TABLE table_7 (
int_col_10      int,
bigint_col_3    bigint
);

CREATE TABLE table_10 (
boolean_col_8       boolean,
boolean_col_16      boolean,
timestamp_col_5     timestamp,
timestamp_col_15    timestamp,
timestamp_col_30    timestamp,
decimal3825_col_26  decimal(38, 25),
smallint_col_9      smallint,
int_col_18          int
);

explain cbo 
SELECT
    DISTINCT COALESCE(a4.timestamp_col_15, IF(a4.boolean_col_16, a4.timestamp_col_30, a4.timestamp_col_5)) AS timestamp_col
FROM table_7 a3
RIGHT JOIN table_10 a4 
WHERE (a3.bigint_col_3) >= (a4.int_col_18)
INTERSECT ALL
SELECT
    COALESCE(LEAST(
        COALESCE(a1.timestamp_col_19, CAST('2010-03-29 00:00:00' AS TIMESTAMP)),
        COALESCE(a1.timestamp_col_29, CAST('2014-08-16 00:00:00' AS TIMESTAMP))
        ),
        GREATEST(COALESCE(a1.timestamp_col_19, CAST('2013-07-01 00:00:00' AS TIMESTAMP)),
        COALESCE(a1.timestamp_col_29, CAST('2028-06-18 00:00:00' AS TIMESTAMP)))
    ) AS timestamp_col
FROM table_16 a1
    GROUP BY COALESCE(LEAST(
        COALESCE(a1.timestamp_col_19, CAST('2010-03-29 00:00:00' AS TIMESTAMP)),
        COALESCE(a1.timestamp_col_29, CAST('2014-08-16 00:00:00' AS TIMESTAMP))
    ),
    GREATEST(
        COALESCE(a1.timestamp_col_19, CAST('2013-07-01 00:00:00' AS TIMESTAMP)),
        COALESCE(a1.timestamp_col_29, CAST('2028-06-18 00:00:00' AS TIMESTAMP)))
    );
{code}"
HIVE-22753,Fix gradual mem leak: Operationlog related appenders should be cleared up on errors ,"In case of exception in SQLOperation, operational log does not get cleared up. This causes gradual build up of HushableRandomAccessFileAppender causing HS2 to OOM after some time.

!image-2020-01-21-11-14-37-911.png|width=936,height=580!

 

Allocation tree

!image-2020-01-21-11-18-37-294.png|width=929,height=389!

 

Prod instance mem

!image-2020-01-21-11-17-59-279.png|width=671,height=201!

 

Each HushableRandomAccessFileAppender holds internal ref to RandomAccessFileAppender which holds a 256 KB bytebuffer, causing the mem leak.

Related ticket: HIVE-18820"
HIVE-22744,TezTask for the vertex with more than one outedge should have proportional sort memory,TezTask for the vertex with more than one outedge should have proportional sort memory
HIVE-22700,Compactions may leak memory when unauthorized,"Initiator class determines compaction type periodically. Initiator either runs as hive user or impersonates the owner of the table. When impersonation is used, Initiator#checkForCompaction may leak memory. If impersonation (ugi.doAs()) call fails, FileSystem.closeAllForUGI does not run, therefore does not clean the file system cache."
HIVE-22588,Flush the remaining rows for the rest of the grouping sets when switching the vector groupby mode,Flush the remaining rows for the rest of the grouping sets when switching the vector groupby mode
HIVE-22514,HiveProtoLoggingHook might consume lots of memory,"HiveProtoLoggingHook uses a ScheduledThreadPoolExecutor to submit writer tasks and to periodically handle rollover. The builtin ScheduledThreadPoolExecutor uses a unbounded queue which cannot be replaced from the outside. If log events are generated at a very fast rate this queue can grow large.

!Screen Shot 2019-11-18 at 2.19.24 PM.png|width=650,height=101!"
HIVE-22484,Remove Calls to printStackTrace,"In many cases, the call to {{printStackTrace}} bypasses the logging framework, in other cases, the error stack trace is printed and the exception is re-thrown (log-and-throw is a bad pattern), and then there are some other edge cases.

Remove this call and replace with calls to the logging framework or remove altogether if exception is wrapped and re-thrown."
HIVE-22461,NPE Metastore Transformer,"The stack looks as following:
{noformat}
2019-10-08 18:09:12,198 INFO  org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer: [pool-6-thread-328]: Starting translation for processor Hiveserver2#3.1.2000.7.0.2.0-59@vc0732.halxg.cloudera.com on list 1
2019-10-08 18:09:12,198 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-6-thread-328]: java.lang.NullPointerException
	at org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.transform(MetastoreDefaultTransformer.java:99)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getTableInternal(HiveMetaStore.java:3391)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_req(HiveMetaStore.java:3352)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy28.get_table_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_req.getResult(ThriftHiveMetastore.java:16633)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_req.getResult(ThriftHiveMetastore.java:16617)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:636)
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:631)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:631)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-08 18:09:12,199 ERROR org.apache.thrift.server.TThreadPoolServer: [pool-6-thread-328]: Error occurred during processing of message.
java.lang.NullPointerException: null
	at org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.transform(MetastoreDefaultTransformer.java:99) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getTableInternal(HiveMetaStore.java:3391) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_req(HiveMetaStore.java:3352) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_141]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_141]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at com.sun.proxy.$Proxy28.get_table_req(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_req.getResult(ThriftHiveMetastore.java:16633) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_req.getResult(ThriftHiveMetastore.java:16617) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:636) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:631) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_141]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_141]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) ~[hadoop-common-3.1.1.7.0.2.0-59.jar:?]
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:631) ~[hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.2000.7.0.2.0-59.jar:3.1.2000.7.0.2.0-59]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_141]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_141]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_141]

{noformat}
"
HIVE-22437,LLAP Metadata cache NPE on locking metadata.,"{code}
java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.io.metadata.MetadataCache.unlockSingleBuffer(MetadataCache.java:464)
	at org.apache.hadoop.hive.llap.io.metadata.MetadataCache.lockBuffer(MetadataCache.java:409)
	at org.apache.hadoop.hive.llap.io.metadata.MetadataCache.lockOldVal(MetadataCache.java:314)
	at org.apache.hadoop.hive.llap.io.metadata.MetadataCache.putInternal(MetadataCache.java:287)
	at org.apache.hadoop.hive.llap.io.metadata.MetadataCache.putFileMetadata(MetadataCache.java:199)
{code}"
HIVE-22414,Make LLAP CacheTags more memory efficient,"MultiPartitionCacheTag relies on LinkedLists.

A LinkedList object that holds e.g. 2 nodes consumes 112 bytes roughly in this composition:
 * 16 bytes for LinkedList object header
 * 8 bytes for referring head
 * 8 bytes for referring tail
 * 80 = 2 x (16 bytes for LinkedList$Node header, 3 x 8 bytes for referring prev, next, item)

This is a lot, so I propose to replace LinkedList in MultiPartitionCacheTag with a simple String array. (For a similar scenario an array would take 16 + 8 + 2 x 8 = 40 bytes, as per header, count, and 2 references for our actual objects)."
HIVE-22412,StatsUtils throw NPE when explain,"The demo like this:
{code:java}
drop table if exists explain_npe_map;
drop table if exists explain_npe_array;
drop table if exists explain_npe_struct;

create table explain_npe_map    ( c1 map<string, string> );
create table explain_npe_array  ( c1 array<string> );
create table explain_npe_struct ( c1 struct<name:string, age:int> );

-- error
set hive.cbo.enable=false;
explain select c1 from explain_npe_map where c1 is null;
explain select c1 from explain_npe_array where c1 is null;
explain select c1 from explain_npe_struct where c1 is null;

-- correct
set hive.cbo.enable=true;
explain select c1 from explain_npe_map where c1 is null;
explain select c1 from explain_npe_array where c1 is null;
explain select c1 from explain_npe_struct where c1 is null;{code}
 

if the conf 'hive.cbo.enable' set false , NPE will be thrown ; otherwise will not.
{code:java}
hive> drop table if exists explain_npe_map;
OK
Time taken: 0.063 seconds
hive> drop table if exists explain_npe_array;
OK
Time taken: 0.035 seconds
hive> drop table if exists explain_npe_struct;
OK
Time taken: 0.015 seconds
hive>
    > create table explain_npe_map    ( c1 map<string, string> );
OK
Time taken: 0.584 seconds
hive> create table explain_npe_array  ( c1 array<string> );
OK
Time taken: 0.216 seconds
hive> create table explain_npe_struct ( c1 struct<name:string, age:int> );
OK
Time taken: 0.17 seconds
hive>
    > set hive.cbo.enable=false;
hive> explain select c1 from explain_npe_map where c1 is null;
FAILED: NullPointerException null
hive> explain select c1 from explain_npe_array where c1 is null;
FAILED: NullPointerException null
hive> explain select c1 from explain_npe_struct where c1 is null;
FAILED: RuntimeException Error invoking signature method
hive>
    > set hive.cbo.enable=true;
hive> explain select c1 from explain_npe_map where c1 is null;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stageSTAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: explain_npe_map
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Filter Operator
            predicate: false (type: boolean)
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: c1 (type: map<string,string>)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              ListSinkTime taken: 1.593 seconds, Fetched: 20 row(s)
hive> explain select c1 from explain_npe_array where c1 is null;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stageSTAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: explain_npe_array
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Filter Operator
            predicate: false (type: boolean)
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: c1 (type: array<string>)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              ListSinkTime taken: 1.969 seconds, Fetched: 20 row(s)
hive> explain select c1 from explain_npe_struct where c1 is null;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stageSTAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: explain_npe_struct
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Filter Operator
            predicate: false (type: boolean)
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: c1 (type: struct<name:string,age:int>)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              ListSinkTime taken: 2.932 seconds, Fetched: 20 row(s)
hive>
{code}
ms error like:

for map:
{code:java}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfMap(StatsUtils.java:1045)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(StatsUtils.java:931)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOfVariableLengthTypes(StatsUtils.java:869)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(StatsUtils.java:526)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:223)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:136)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:124)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:111)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:56)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:192)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10205)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:210)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:425)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:309)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1153)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1206)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1082)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1072)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136){code}
 

for array:
{code:java}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(StatsUtils.java:1168)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOf(StatsUtils.java:1132)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(StatsUtils.java:686)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(StatsUtils.java:664)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:254)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:162)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:150)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:142)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:250)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12481)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11824)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:166)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1854)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1801)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1796)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:226)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:141)
{code}
for struct:

Maybe correct in branch of master,  but i think it is necessary to initialize the value of StandardConstantStructObjectInspector
{code:java}
//代码占位符
2020-06-10T16:40:56,971 ERROR [52839d08-57a7-475f-b87f-8f1410978b8a main] ql.Driver: FAILED: RuntimeException Error invoking signature method
java.lang.RuntimeException: Error invoking signature method
        at org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils$SignatureMapper.write(SignatureUtils.java:76)
        at org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils.write(SignatureUtils.java:40)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpSignature.<init>(OpSignature.java:53)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpSignature.of(OpSignature.java:57)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignature.<init>(OpTreeSignature.java:50)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignature.of(OpTreeSignature.java:63)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignatureFactory$CachedFactory.lambda$getSignature$0(OpTreeSignatureFactory.java:62)
        at java.util.Map.computeIfAbsent(Map.java:957)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignatureFactory$CachedFactory.getSignature(OpTreeSignatureFactory.java:62)
        at org.apache.hadoop.hive.ql.plan.mapper.PlanMapper.getSignatureOf(PlanMapper.java:265)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.applyRuntimeStats(StatsRulesProcFactory.java:2666)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.access$000(StatsRulesProcFactory.java:116)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$SelectStatsRule.process(StatsRulesProcFactory.java:211)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:250)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12481)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11824)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:166)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1854)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1801)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1796)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:226)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:141)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils$SignatureMapper.write(SignatureUtils.java:73)
        ... 42 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.getExprString(ExprNodeConstantDesc.java:158)
        at org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getExprString(ExprNodeDesc.java:90)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.addExprToStringBuffer(PlanUtils.java:1104)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.getExprListString(PlanUtils.java:1092)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.getExprListString(PlanUtils.java:1075)
        at org.apache.hadoop.hive.ql.plan.SelectDesc.getColListString(SelectDesc.java:79)
        ... 47 more
{code}
 

We can fix it by initializing value for StandardConstantMapObjectInspector, StandardConstantListObjectInspector and StandardConstantStructObjectInspector.

 "
HIVE-22393,"HiveStreamingConnection: Exception in beginTransaction causes AbstractRecordWriter to throw NPE, covering up real exception","Getting an error in my code that does some basic stuff with {{HiveStreamingConnection}}:

{code}
19/10/23 15:12:44 ERROR yarn_logger.Main: Thread worker-0 uncaught exception
java.lang.RuntimeException: org.apache.hive.streaming.StreamingException: Unable to close
	at com.datto.yarn_logger.WorkerThread.run(WorkerThread.java:51)
Caused by: org.apache.hive.streaming.StreamingException: Unable to close
	at org.apache.hive.streaming.HiveStreamingConnection$TransactionBatch.close(HiveStreamingConnection.java:973)
	at org.apache.hive.streaming.HiveStreamingConnection$TransactionBatch.markDead(HiveStreamingConnection.java:833)
	at org.apache.hive.streaming.HiveStreamingConnection$TransactionBatch.<init>(HiveStreamingConnection.java:677)
	at org.apache.hive.streaming.HiveStreamingConnection$TransactionBatch.<init>(HiveStreamingConnection.java:596)
	at org.apache.hive.streaming.HiveStreamingConnection.createNewTransactionBatch(HiveStreamingConnection.java:485)
	at org.apache.hive.streaming.HiveStreamingConnection.beginNextTransaction(HiveStreamingConnection.java:466)
	at org.apache.hive.streaming.HiveStreamingConnection.beginTransaction(HiveStreamingConnection.java:507)
	at com.datto.yarn_logger.WorkerThread.run(WorkerThread.java:49)
Caused by: java.lang.NullPointerException
	at org.apache.hive.streaming.AbstractRecordWriter.logStats(AbstractRecordWriter.java:547)
	at org.apache.hive.streaming.AbstractRecordWriter.close(AbstractRecordWriter.java:352)
	at org.apache.hive.streaming.HiveStreamingConnection$TransactionBatch.closeImpl(HiveStreamingConnection.java:979)
	at org.apache.hive.streaming.HiveStreamingConnection$TransactionBatch.close(HiveStreamingConnection.java:970)
	... 7 more
{code}

Digging through the stack trace... {{TransactionBatch}} will try to catch exception in its constructor, and calls the close method if an exception is thrown (which is definitely happening; that's line 677 in {{HiveStreamingConnection.java}}). This eventually calls {{AbstractRecordWriter::close}} which, calls {{logStats}}, which tries to use {{heapMemoryMonitor}}, which is null because presumably {{init}} had never been called on the writer yet.

Easy fix would to just check in {{logStats}} if {{heapMemoryMonitor}} is null, and do the same thing if the method it calls returns null in it."
HIVE-22391,NPE while checking Hive query results cache,"NPE when results cache was enabled:
{noformat}
2019-10-21T14:51:55,718 ERROR [b7d7bea8-eef0-4ea4-ae12-951cb5dc96e3 HiveServer2-Handler-Pool: Thread-210]: ql.Driver (:()) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkResultsCache(SemanticAnalyzer.java:15061)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12320)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:360)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:289)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1869)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1816)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1811)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:262)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:575)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:561)
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:315)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:566)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:647)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}"
HIVE-22375,ObjectStore.lockNotificationSequenceForUpdate is leaking query in case of error,"In the ObjectStore.lockNotificationSequenceForUpdate method, the query doesn't get closed if an error occur:
{noformat}
  private void lockNotificationSequenceForUpdate() throws MetaException {
    if (sqlGenerator.getDbProduct() == DatabaseProduct.DERBY && directSql != null) {
      // Derby doesn't allow FOR UPDATE to lock the row being selected (See https://db.apache
      // .org/derby/docs/10.1/ref/rrefsqlj31783.html) . So lock the whole table. Since there's
      // only one row in the table, this shouldn't cause any performance degradation.
      new RetryingExecutor(conf, () -> {
        directSql.lockDbTable(""NOTIFICATION_SEQUENCE"");
      }).run();
    } else {
      String selectQuery = ""select \""NEXT_EVENT_ID\"" from \""NOTIFICATION_SEQUENCE\"""";
      String lockingQuery = sqlGenerator.addForUpdateClause(selectQuery);
      new RetryingExecutor(conf, () -> {
        prepareQuotes();
        Query query = pm.newQuery(""javax.jdo.query.SQL"", lockingQuery);
        query.setUnique(true);
        // only need to execute it to get db Lock
        query.execute();
        query.closeAll();
      }).run();
    }
  }
{noformat}"
HIVE-22305,Add the kudu-handler to the packaging module,The hive-kudu-handler needs to be added to the packaging module to ensure the jars are packaged into the tar distribution.
HIVE-22289,Regenerate test output for tests broken due to commit race,HIVE-22042 got committed which changed the plans of a few tests (by enabling nonstrict partitioning mode by default) then HIVE-22269 got committed which fixes a bug with stats not being correctly calculated on some operators. Each patch got green runs individually but together causes test output differences.
HIVE-22232,NPE when hive.order.columnalignment is set to false,"When {{hive.order.columnalignment}} is disabled and the plan contains an Aggregate operator, we hit a NPE.

{code}
 java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:163)
	at org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.convert(ASTConverter.java:111)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:1555)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:483)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12630)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:357)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:175)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:522)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1385)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1332)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1327)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:124)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:217)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:242)
...
{code}"
HIVE-22209,Creating a materialized view with no tables should be handled more gracefully,"Currently, materialized views without a table reference are not supported. However, instead of printing a clear message about it, when a materialized view is created without a table reference, we fail with an unclear message.

{code}
> create materialized view mv_test1 as select 5;
(...)
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Add request failed :
INSERT INTO MV_TABLES_USED (MV_CREATION_METADATA_ID,TBL_ID) VALUES (?,?) )
INFO : Completed executing command(queryId=hive_20190916203511_b609cccf-f5e3-45dd-abfd-6e869d94e39a); Time taken: 10.469 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaExcep
tion(message:Add request failed : INSERT INTO MV_TABLES_USED (MV_CREATION_METADATA_ID,TBL_ID) VALUES (?,?) ) (state=08S01,code=1)
{code}"
HIVE-22193,Graceful Shutdown HiveServer2,"We have a lot of HiveSever2 servers deployed on production environment (about 10 nodes). 

However, if we want to change configuration or add patches, we would have to restart all of them one by one. So all the Hive Sql job running on the server will be defeated, and there may be some mistakes come up on the jdbc client occasionally.

In the proposed changes,  planning to add Graceful Shutdown HiveSever2 method to avoid affecting the production environment jobs"
HIVE-22151,Turn off hybrid grace hash join by default,
HIVE-22126,hive-exec packaging should shade guava,"The ql/pom.xml includes complete guava library into hive-exec.jar https://github.com/apache/hive/blob/master/ql/pom.xml#L990 This causes a problems for downstream clients of hive which have hive-exec.jar in their classpath since they are pinned to the same guava version as that of hive. 

We should shade guava classes so that other components which depend on hive-exec can independently use a different version of guava as needed."
HIVE-21992,REPL DUMP throws NPE when dumping Create Function event.,"REPL DUMP throws NPE while dumping Create Function event.It seems, null check is missing for function.getResourceUris().

{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.repl.dump.io.FunctionSerializer.writeTo(FunctionSerializer.java:54)
        at org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateFunctionHandler.handle(CreateFunctionHandler.java:48)
        at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.dumpEvent(ReplDumpTask.java:304)
        at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.incrementalDump(ReplDumpTask.java:231)
        at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.execute(ReplDumpTask.java:121)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:103)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2727)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2394)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2066)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1764)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1758)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:324)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:342)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask. java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.repl.dump.io.FunctionSerializer.writeTo(FunctionSerializer.java:54)
        at org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateFunctionHandler.handle(CreateFunctionHandler.java:48)
        at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.dumpEvent(ReplDumpTask.java:304)
        at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.incrementalDump(ReplDumpTask.java:231)
        at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.execute(ReplDumpTask.java:121)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:103)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2727)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2394)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2066)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1764)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1758)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:324)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:342)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}"
HIVE-21971,HS2 leaks classloader due to `ReflectionUtils::CONSTRUCTOR_CACHE` with temporary functions + GenericUDF,"https://issues.apache.org/jira/browse/HIVE-10329 helped in moving away from hadoop's ReflectionUtils constructor cache issue (https://issues.apache.org/jira/browse/HADOOP-10513).

However, there are corner cases where hadoop's {{ReflectionUtils}} is in use and this causes gradual build up of memory in HS2.

I have observed this in Hive 2.3. But the codepath in master for this has not changed much.

Easiest way to repro would be to add a temp function which extends {{GenericUDF}}. In {{FunctionRegistry::cloneGenericUDF,}} this would 
end up using {{org.apache.hadoop.util.ReflectionUtils.newInstance}} which in turn lands up in COSNTRUCTOR_CACHE of ReflectionUtils. 


{noformat}

CREATE TEMPORARY FUNCTION dummy AS 'com.hive.test.DummyGenericUDF' USING JAR 'file:///home/test/udf/dummy.jar';

select dummy();

	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:107)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.cloneGenericUDF(FunctionRegistry.java:1353)
	at org.apache.hadoop.hive.ql.exec.FunctionInfo.getGenericUDF(FunctionInfo.java:122)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:983)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1359)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
	at org.apache.hadoop.hive.ql.lib.ExpressionWalker.walk(ExpressionWalker.java:76)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
{noformat}

Note: Reflection based invocation of hadoop's {{ReflectionUtils::clear}} was removed in 2.x. "
HIVE-21844,HMS schema Upgrade Script is failing with NPE,"schema upgrade tool is failing with NPE while executing ""SELECT 'Upgrading MetaStore schema from 1.2.0 to 2.0.0' AS ' '"". The header row (metadata) is coming with rows having value null. This is causing null pointer access in function TableOutputFormat::getOutputString when row.values[i] is accessed. Instead of "" AS ' ' "", if some other value  like ""AS dummy"" is given, it's working fine. The issue is coming with mysql version 5.7 and above.

 

 

 "
HIVE-21811,Load data into partitioned table throws NPE if DB is enabled for replication.,"When load data into a partitioned table with hive.autogather.stats=true and DB is enabled for replication (""repl.source.for"" property is set in DB), throws NPE. Here is the call stack.
{code}
0: jdbc:hive2://ctr-e139-1542663976389-126983> LOAD DATA INPATH '/tmp/traffic_data/traffic_data-QUEENS.csv' INTO TABLE traffic_data partition (county='QUEENS');
INFO  : Loading data to table traffic_database.traffic_data partition (county=QUEENS) from hdfs://ctr-e139-1542663976389-126983-01-000003.hwx.site:8020/tmp/traffic_data/traffic_data-QUEENS.csv
INFO  : Partition traffic_database.traffic_data{county=QUEENS} stats: [numFiles=1, numRows=0, totalSize=64398392, rawDataSize=0]
INFO  : [Warning] could not update stats.Failed with exception Unable to alter partition. java.lang.NullPointerException
org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter partition. java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.Hive.alterPartitions(Hive.java:678)
	at org.apache.hadoop.hive.ql.exec.StatsTask.aggregateStats(StatsTask.java:261)
	at org.apache.hadoop.hive.ql.exec.StatsTask.execute(StatsTask.java:122)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:177)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:96)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1777)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1511)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1308)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1175)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)
	at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:273)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6161)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:3908)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy20.alter_partitions_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partitions(HiveMetaStoreClient.java:1485)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:178)
	at com.sun.proxy.$Proxy21.alter_partitions(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterPartitions(Hive.java:676)
	... 23 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.metastore.HiveAlterHandler.blockPartitionLocationChangesOnReplSource(HiveAlterHandler.java:729)
	at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:657)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:3870)
	... 38 more

No rows affected (2.086 seconds)
{code}"
HIVE-21752,Thread Safety and Memory Leaks in HCatRecordObjectInspectorFactory,"h3. Summary

There are a couple of issues in HCatRecordObjectInspectorFactory[1] because it uses a static Java HashMap to cache objects:
 # Java HashMap is not thread safe. This can lead to data corruptions and race conditions in multithreaded servers when two threads update the ObjectInspector.
 # There is no eviction policy and as a result, this can result in memory leaks. If user reads a lot of different schemas, Hive server will start seeing memory pressure, once it start going to have a lot of cached record and object inspectors.

This patch propose to replace the cache using a Guava cache which enables cache evictions and thread safety. Guava cache is already used in Hive ObjectInspectorFactory [2], so this change is consistent with the rest of Hive.

Attached is a patch that fixes this issue.
h3. References:
 # [https://github.com/apache/hive/blob/b58d50cb73a1f79a5d079e0a2c5ac33d2efc33a0/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordObjectInspectorFactory.java#L44-L47]
 # [https://github.com/apache/hive/blob/b58d50cb73a1f79a5d079e0a2c5ac33d2efc33a0/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorFactory.java#L68-L87]

 
h4. Review Board Link:
 *  [https://reviews.apache.org/r/70674/]"
HIVE-21669,HS2 throws NPE when HiveStatement.getQueryId is invoked and query is closed concurrently.,HS2 throws NullPointerException if HiveStatement.getQueryId invoked without executing any query or query is closed. It should instead return null so that caller would check it.
HIVE-21507,Hive swallows NPE if no delegation token found,"In case if there is no delegation token put into token file, this [line|https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java#L777] will cause a NullPointerException which is not handled and the user is not notified in any way.

To cause NPE the use case is to have an Oozie Sqoop import to Hive in a kerberized cluster. Oozie puts the delegation token into the token file with id: *HIVE_DELEGATION_TOKEN_hiveserver2ClientToken*. So with id *hive* it is not working. However, fallback code uses the key which Oozie provides [this|https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java#L784] way.

I suggest to have warning message to user that key with id *hive* cannot be used and falling back to get delegation token from the session.

I am creating the patch."
HIVE-21496,Automatic sizing of unordered buffer can overflow,"HIVE-21329 added automatic sizing of tez unordered partitioned KV buffer based on group by statistics. However, some corner cases for group by statistics sets Long.MAX for data size. This ends up setting Integer.MAX for unordered KV buffer size. This buffer size is expected to be in MB. Converting Integer.MAX value from MB to bytes will overflow and following exception is thrown.
{code:java}
2019-03-23T01:35:17,760 INFO [Dispatcher thread {Central}] HistoryEventHandler.criticalEvents: [HISTORY][DAG:dag_1553330105749_0001_1][Event:TASK_ATTEMPT_FINISHED]: vertexName=Map 1, taskAttemptId=attempt_1553330105749_0001_1_00_000000_0, creationTime=1553330117468, allocationTime=1553330117524, startTime=1553330117562, finishTime=1553330117755, timeTaken=193, status=FAILED, taskFailureType=NON_FATAL, errorEnum=FRAMEWORK_ERROR, diagnostics=Error: Error while running task ( failure ) : attempt_1553330105749_0001_1_00_000000_0:java.lang.IllegalArgumentException
at com.google.common.base.Preconditions.checkArgument(Preconditions.java:108)
at org.apache.tez.runtime.common.resources.MemoryDistributor.registerRequest(MemoryDistributor.java:177)
at org.apache.tez.runtime.common.resources.MemoryDistributor.requestMemory(MemoryDistributor.java:110)
at org.apache.tez.runtime.api.impl.TezTaskContextImpl.requestInitialMemory(TezTaskContextImpl.java:214)
at org.apache.tez.runtime.library.output.UnorderedPartitionedKVOutput.initialize(UnorderedPartitionedKVOutput.java:76)
at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask$InitializeOutputCallable._callInternal(LogicalIOProcessorRuntimeTask.java:537)
at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask$InitializeOutputCallable.callInternal(LogicalIOProcessorRuntimeTask.java:520)
at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask$InitializeOutputCallable.callInternal(LogicalIOProcessorRuntimeTask.java:505)
at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745){code}
 

Stats for GBY operator is getting Long.MAX_VALUE as seen below
{code:java}
2019-03-23T01:35:16,466 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [0] STATS-TS[0] (logs): numRows: 1795 dataSize: 4443078 basicStatsState: PARTIAL colStatsState: NONE colStats: {severity= colName: severity colType: string countDistincts: 359 numNulls: 89 avgColLen: 100.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true}
2019-03-23T01:35:16,466 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: Estimating row count for GenericUDFOPEqual(Column[severity], Const string ERROR) Original num rows: 1795 New num rows: 5
2019-03-23T01:35:16,467 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [1] STATS-FIL[8]: numRows: 5 dataSize: 12376 basicStatsState: PARTIAL colStatsState: NONE colStats: {severity= colName: severity colType: string countDistincts: 359 numNulls: 89 avgColLen: 100.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true}
2019-03-23T01:35:16,467 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] exec.FilterOperator: Setting stats (Num rows: 5 Data size: 12376 Basic stats: PARTIAL Column stats: NONE) on: FIL[8]
2019-03-23T01:35:16,468 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] exec.SelectOperator: Setting stats (Num rows: 5 Data size: 12376 Basic stats: PARTIAL Column stats: NONE) on: SEL[2]
2019-03-23T01:35:16,468 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [1] STATS-SEL[2]: numRows: 5 dataSize: 12376 basicStatsState: PARTIAL colStatsState: NONE colStats: {severity= colName: severity colType: string countDistincts: 359 numNulls: 89 avgColLen: 100.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true}
2019-03-23T01:35:16,471 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: STATS-GBY[3]: inputSize: 4443078 maxSplitSize: 256000000 parallelism: 1 containsGroupingSet: false sizeOfGroupingSet: 1
2019-03-23T01:35:16,471 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [Case 1] STATS-GBY[3]: cardinality: 5
2019-03-23T01:35:16,472 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] exec.GroupByOperator: Setting stats (Num rows: 1 Data size: 9223372036854775807 Basic stats: PARTIAL Column stats: NONE) on: GBY[3]
2019-03-23T01:35:16,472 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [0] STATS-GBY[3]: numRows: 1 dataSize: 9223372036854775807 basicStatsState: PARTIAL colStatsState: NONE colStats: {severity= colName: severity colType: string countDistincts: 1 numNulls: 18 avgColLen: 100.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true, _col0= colName: _col0 colType: bigint countDistincts: 1 numNulls: 0 avgColLen: 8.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: false}
2019-03-23T01:35:16,473 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] exec.ReduceSinkOperator: Setting stats (Num rows: 1 Data size: 9223372036854775807 Basic stats: PARTIAL Column stats: NONE) on: RS[4]
2019-03-23T01:35:16,474 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [0] STATS-RS[4]: numRows: 1 dataSize: 9223372036854775807 basicStatsState: PARTIAL colStatsState: NONE colStats: {severity= colName: severity colType: string countDistincts: 1 numNulls: 18 avgColLen: 100.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true, _col0= colName: _col0 colType: bigint countDistincts: 1 numNulls: 0 avgColLen: 8.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: false}
2019-03-23T01:35:16,474 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: STATS-GBY[5]: inputSize: 1 maxSplitSize: 256000000 parallelism: 1 containsGroupingSet: false sizeOfGroupingSet: 1
2019-03-23T01:35:16,474 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [Case 7] STATS-GBY[5]: cardinality: 0
2019-03-23T01:35:16,474 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] stats.StatsUtils: STATS-GBY[5]: Equals 0 in number of rows. 0 rows will be set to 1
2019-03-23T01:35:16,474 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] exec.GroupByOperator: Setting stats (Num rows: 1 Data size: 9223372036854775807 Basic stats: PARTIAL Column stats: NONE) on: GBY[5]
2019-03-23T01:35:16,474 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [0] STATS-GBY[5]: numRows: 1 dataSize: 9223372036854775807 basicStatsState: PARTIAL colStatsState: NONE colStats: {severity= colName: severity colType: string countDistincts: 1 numNulls: 18 avgColLen: 100.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true, _col0= colName: _col0 colType: bigint countDistincts: 1 numNulls: 0 avgColLen: 8.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: false}
2019-03-23T01:35:16,474 DEBUG [c779e956-b3b9-451a-8248-6ae7c669854f main] annotation.StatsRulesProcFactory: [0] STATS-FS[7]: numRows: 1 dataSize: 9223372036854775807 basicStatsState: PARTIAL colStatsState: NONE colStats: {severity= colName: severity colType: string countDistincts: 1 numNulls: 36 avgColLen: 100.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true, _col0= colName: _col0 colType: bigint countDistincts: 1 numNulls: 0 avgColLen: 8.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: false}{code}"
HIVE-21479,NPE during metastore cache update,"Saw the following stack during a long periodical update:
{code}
2019-03-12T10:01:43,015 ERROR [CachedStore-CacheUpdateService: Thread-36] cache.CachedStore: Update failure:java.lang.NullPointerException
	at org.apache.hadoop.hive.metastore.cache.CachedStore$CacheUpdateMasterWork.updateTableColStats(CachedStore.java:508)
	at org.apache.hadoop.hive.metastore.cache.CachedStore$CacheUpdateMasterWork.update(CachedStore.java:461)
	at org.apache.hadoop.hive.metastore.cache.CachedStore$CacheUpdateMasterWork.run(CachedStore.java:396)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

The reason is we get the table list at very early stage and then refresh table one by one. It is likely table is removed during the interim. We need to deal with this case during cache update."
HIVE-21471,Replicating conversion of managed to external table leaks HDFS files at target.,"While replicating the ALTER event to convert managed table to external table, the data location for the table is changed under input base directory for external tables replication. But, the old location remains there and would be leaked for ever.

ALTER TABLE T1 SET TBLPROPERTIES('EXTERNAL'='true');"
HIVE-21440,Fix test_teradatabinaryfile to not run into stackoverflows,"this test seems to be failing in recent runs; taking a closer look shows that it might be some kryo related stackoverflow

{code}
Caused by: java.lang.IllegalArgumentException: Unable to create serializer ""org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer"" for class: org.apache.hadoop.hive.ql.io.TeradataBinaryFileOutput
Format
        at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67)
        at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45)
        at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380)
        at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364)
        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74)
        at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:490)
        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:97)
        at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:517)
        at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.write(DefaultSerializers.java:321)
        at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.write(DefaultSerializers.java:314)
        at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606)
        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:87)
        ... 104 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:54)
        ... 115 more
Caused by: java.lang.StackOverflowError
        at java.util.HashMap.hash(HashMap.java:338)
        at java.util.HashMap.get(HashMap.java:556)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:61)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
        at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)
{code}"
HIVE-21421,HiveStatement.getQueryId throws NPE when query is not running. ,HiveStatement.getQueryId throws NullPointerException if it invoked without executing any query or query is closed. It should instead return null so that caller would check it.
HIVE-21371,Make NonSyncByteArrayOutputStream Overflow Conscious ,"{code:java|title=NonSyncByteArrayOutputStream}
  private int enLargeBuffer(int increment) {
    int temp = count + increment;
    int newLen = temp;
    if (temp > buf.length) {
      if ((buf.length << 1) > temp) {
        newLen = buf.length << 1;
      }
      byte newbuf[] = new byte[newLen];
      System.arraycopy(buf, 0, newbuf, 0, count);
      buf = newbuf;
    }
    return newLen;
  }
{code}

This will fail if the array is 2GB or larger because it will double the size every time without consideration for the 4GB limit on arrays."
HIVE-21342,Analyze compute stats for column leave behind staging dir on hdfs,"staging dir cleanup does not happen for the ""analyze table .. compute statistics for columns"", this leads to stale directory on hdfs.
the problem seems to be with ColumnStatsSemanticAnalyzer which don't have hdfscleanup set for the context.
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java#L310"
HIVE-21280,Null pointer exception on running compaction against a MM table.,"On running compaction on MM table, got a null pointer exception while getting HDFS session path. The error seemed to me that the session state was not started for these queries. Even after making it start it further fails in running a Teztask for insert overwrite on temp table with the contents of the original table. The cause for this is Tezsession state is not able to initialize due to Illegal Argument exception being thrown at the time of setting up caller context in Tez task due to caller id which uses queryid being an empty string. 
I do think session state needs to be started and each of the queries running for compaction (I'm also doubtful for stats updater thread's queries) should have a query id. Some details are as follows:


Steps to reproduce:
1) Using beeline with HS2 and HMS
2) create an MM table
3) Insert a few values in the table
4) alter table mm_table compact 'major'; 

Stack trace on HMS:
{code:java}
compactor.Worker: Caught exception while trying to compact id:8,dbname:default,tableName:acid_mm_orc,partName:null,state:^@,type:MAJOR,properties:null,runAs:null,tooManyAborts:false,highestWriteId:0. Marking failed to avoid repeated failures, java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to run create temporary table default.tmp_compactor_acid_mm_orc_1550222367257(`a` int, `b` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'WITH SERDEPROPERTIES (
'serialization.format'='1')STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' LOCATION 'hdfs://localhost:9000/user/hive/warehouse/acid_mm_orc/_tmp_2d8a096c-2db5-4ed8-921c-b3f6d31e079e/_base' TBLPROPERTIES ('transactional'='false')
at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.runMmCompaction(CompactorMR.java:373)
at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:241)
at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:174)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to run create temporary table default.tmp_compactor_acid_mm_orc_1550222367257(`a` int, `b` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'WITH SERDEPROPERTIES (
'serialization.format'='1')STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' LOCATION 'hdfs://localhost:9000/user/hive/warehouse/acid_mm_orc/_tmp_2d8a096c-2db5-4ed8-921c-b3f6d31e079e/_base' TBLPROPERTIES ('transactional'='false')
at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.runOnDriver(CompactorMR.java:525)
at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.runMmCompaction(CompactorMR.java:365)
... 2 more
Caused by: java.lang.NullPointerException: Non-local session path expected to be non-null
at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:228)
at org.apache.hadoop.hive.ql.session.SessionState.getHDFSSessionPath(SessionState.java:815)
at org.apache.hadoop.hive.ql.Context.<init>(Context.java:309)
at org.apache.hadoop.hive.ql.Context.<init>(Context.java:295)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:591)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1684)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1807)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1567)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1556)
at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.runOnDriver(CompactorMR.java:522)
... 3 more
{code}
cc: [~ekoifman] [~vgumashta] [~sershe]"
HIVE-21186,External tables replication throws NPE if hive.repl.replica.external.table.base.dir is not fully qualified HDFS path.,"REPL DUMP is fine. Load seems to be throwing exception:
{code}
2019-01-29 09:25:12,671 ERROR HiveServer2-Background-Pool: Thread-4864: ql.Driver (SessionState.java:printError(1129)) - FAILED: Execution Error, return code 40000 from org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask. java.lang.NullPointerException
2019-01-29 09:25:12,671 INFO HiveServer2-Background-Pool: Thread-4864: ql.Driver (Driver.java:execute(1661)) - task failed with
org.apache.hadoop.hive.ql.parse.SemanticException: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.tasks(LoadTable.java:154)
at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.executeBootStrapLoad(ReplLoadTask.java:141)
at org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.execute(ReplLoadTask.java:82)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:177)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:93)
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1777)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1511)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1308)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1175)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)
at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)
at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)
at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:273)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.PathUtils.getExternalTmpPath(PathUtils.java:35)
at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.loadTableTask(LoadTable.java:245)
at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.newTableTasks(LoadTable.java:189)
at org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.tasks(LoadTable.java:136)
... 23 more
{code}
REPL Load statement: 
{code}
REPL LOAD `testdb1_tgt` FROM 'hdfs://ctr-e139-1542663976389-56533-01-000011.hwx.site:8020/apps/hive/repl/c9476207-8179-4db7-b947-ba67c950a340' WITH ('hive.query.id'='testHive1_3dd5e281-89ef-4054-850e-8a34386fc2c8','hive.exec.parallel'='true','hive.repl.replica.external.table.base.dir'='/tmp/someNewloc/','hive.repl.include.external.tables'='true','mapreduce.map.java.opts'='-Xmx640m','hive.distcp.privileged.doAs'='beacon','distcp.options.pugpb'='')
{code}

This is an issue with Hive unable to handle path without schema/authority input for ""hive.repl.replica.external.table.base.dir"".
Here the input was 'hive.repl.replica.external.table.base.dir'='/tmp/someNewloc/','.
If we set a fully qualified HDFS path (such as hdfs://<host>:<port:/tmp/someNewloc/), then it works fine.
Need to fix it in Hive to accept path without schema/authority and obtain it from local cluster."
HIVE-21099,Do Not Print StackTraces to STDERR in ConditionalResolverMergeFiles,"[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java]#L193

 
{code:java}
} catch (IOException e){ 
  e.printStackTrace(); 
}
{code}
This is inline with the HIVE-20159 to print the error  using the SLF4J. in Method getTasks

 "
HIVE-21041,"NPE, ParseException in getting schema from logical plan","HIVE-20552 makes getting schema from logical plan faster. But it throws ParseException when it has column alias, and NullPointerException when it has subqueries."
HIVE-21035,Race condition in SparkUtilities#getSparkSession,"It can happen, that when in one given session, multiple queries are executed, that due to a race condition, multiple spark application master gets kicked off.
In this case, the one that started earlier, will not be killed, when the hive session closes, consuming resources."
HIVE-21028,get_table_meta should use a fetch plan to avoid race conditions ending up in NucleusObjectNotFoundException,"The {{getTableMeta}} call retrieves the tables, loops through the tables and during this loop it retrieves the database object to get the containing database name. DataNuclues does a lazy retrieval and so, when the first call to get all the tables is done, it does not retrieve the database objects.

When this query is executed
{code}query = pm.newQuery(MTable.class, filterBuilder.toString());
{code}

it loads all the tables, and when you do
{code}
table.getDatabase().getName()
{code}
it then goes and retrieves the database object.

*However*, there could be another thread which actually has deleted the database!! If this happens, we end up with exceptions such as
{code}
2018-12-04 22:25:06,525 INFO  DataNucleus.Datastore.Retrieve: [pool-7-thread-191]: Object with id ""6930391[OID]org.apache.hadoop.hive.metastore.model.MTable"" not found !
2018-12-04 22:25:06,527 WARN  DataNucleus.Persistence: [pool-7-thread-191]: Exception thrown by StateManager.isLoaded
No such database row
org.datanucleus.exceptions.NucleusObjectNotFoundException: No such database row
{code}

We see this happen especially with calls which retrieve all the tables in all the databases (basically a call to get_table_meta with dbNames=""\*"" and tableNames=""\*"").

To avoid this, we can define a custom fetch plan and activate it only for the get_table_meta query. This fetch plan would fetch the database object along with the MTable object.

We would first create a fetch plan on the pmf
{code}
pmf.getFetchGroup(MTable.class, ""mtable_db_fetch_group"").addMember(""database"");
{code}

Then we use it just before calling the query
{code}
pm.getFetchPlan().addGroup(""mtable_db_fetch_group"");
query = pm.newQuery(MTable.class, filterBuilder.toString());
Collection<MTable> tables = (Collection<MTable>) query.executeWithArray(...);
...
{code}

Before the API call ends, we can remove the fetch plan by
{code}
pm.getFetchPlan().removeGroup(""mtable_db_fetch_group"");
{code}"
HIVE-21005,LLAP: Reading more stripes per-split leaks ZlibCodecs,"OrcEncodedDataReader - calls ensureDataReader in a loop, overwriting itself

{code}
    for (int stripeIxMod = 0; stripeIxMod < stripeRgs.length; ++stripeIxMod) {
....
        // 6.2. Ensure we have stripe metadata. We might have read it before for RG filtering.
        if (stripeMetadatas != null) {
          stripeMetadata = stripeMetadatas.get(stripeIxMod);
        } else {
...
          ensureDataReader();
...
        }
{code}

{code}
  private void ensureDataReader() throws IOException {
...
    stripeReader = orcReader.encodedReader(
        fileKey, dw, dw, useObjectPools ? POOL_FACTORY : null, trace, useCodecPool, cacheTag);
{code}

creates new encodedReader without closing previous stripe's encoded reader."
HIVE-20981,streaming/AbstractRecordWriter leaks HeapMemoryMonitor,Each record writer registers a memory monitor with the MemoryMXBean but they aren't removed. So the listener objects/lambdas accumulate over time in the bean. 
HIVE-20979,Fix memory leak in hive streaming,"{{1) HiveStreamingConnection.Builder#init() adds a shutdown hook handler via }}{{ShutdownHookManager.addShutdownHook but it is never removed which causes all the handlers to accumulate and hence a memory leak.}}

2) AbstractRecordWriter creates an instance of FileSystem but does not close it which in turn causes a leak due to accumulation in FileSystem$Cache#map

 "
HIVE-20969,HoS sessionId generation can cause race conditions when uploading files to HDFS,"The observed exception is:
{code}
Caused by: java.io.FileNotFoundException: File does not exist: /tmp/hive/_spark_session_dir/0/hive-exec-2.1.1-SNAPSHOT.jar (inode 21140) [Lease.  Holder: DFSClient_NONMAPREDUCE_304217459_39, pending creates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2781)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2660)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:872)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:550)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:815)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2675)
{code}"
HIVE-20905,querying streaming table fails with out of memory exception,"Streaming app was ran for 24hrs post which it went down due authentication issue . The table was accessible for 12hrs into the run, however currently querying the table fails with OOM exception."
HIVE-20886,Fix NPE: GenericUDFLower,"{noformat}
create table if not exists test1(uuid array<string>);
select lower(uuid) from test1;

Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)
{noformat}"
HIVE-20829,JdbcStorageHandler range split throws NPE,"{code}
2018-10-29T06:37:14,982 ERROR [HiveServer2-Background-Pool: Thread-44466]: operation.Operation (:()) - Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:228) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:318) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_161]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_161]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.1.1.3.0.3.0-150.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:338) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:240) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:210) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2707) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2378) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2054) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1752) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1746) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	... 13 more
{code}"
HIVE-20711,Race Condition when Multi-Threading in SessionState.createRootHDFSDir,java.util.concurrent.ExecutionException: java.lang.RuntimeException: The root scratch dir: /home/hiveptest/hive-ptest-cloudera-slaves-17e5-13.gce.cloudera.com-hiveptest-0/cdh-source/itests/hive-unit/target/tmp/scratchdir on HDFS should be writable. Current permissions are: rwxr-xr-x at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:714) at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:637) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:567) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:532) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:512) at
HIVE-20698,Better error instead of NPE when timestamp is null for any row when ingesting to druid,"Currently when ingesting data to druid we get a wierd NPE when timestamp is null for any row. 
We should provide an error with a better message which helps user to know what is actually wrong. 

{code} 
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hive.druid.serde.DruidSerDe.serialize(DruidSerDe.java:364)
  at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:957)
  at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:111)
  at org.apache.hadoop.hive.ql.exec.Operator.vectorForward(Operator.java:965)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:938)
  at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:158)
  at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:480)
{code}"
HIVE-20656,Sensible defaults: Map aggregation memory configs are too aggressive,"The defaults for the following configs seems to be too aggressive. In java this can easily lead to several full GC pauses whose memory cannot be reclaimed.
{code:java}
HIVEMAPAGGRHASHMEMORY(""hive.map.aggr.hash.percentmemory"", (float) 0.99,
    ""Portion of total memory to be used by map-side group aggregation hash table""),
HIVEMAPAGGRMEMORYTHRESHOLD(""hive.map.aggr.hash.force.flush.memory.threshold"", (float) 0.9,
    ""The max memory to be used by map-side group aggregation hash table.\n"" +
    ""If the memory usage is higher than this number, force to flush data""),{code}
 

We can be little bit conservative for these configs to avoid getting into GC pause. "
HIVE-20649,LLAP aware memory manager for Orc writers,ORC writer has its own memory manager that assumes memory usage or memory available based on JVM heap (MemoryMX bean). This works on tez container mode execution model but not in LLAP where container sizes (and Xmx) are typically high and there are multiple executors per LLAP daemon. This custom memory manager should be aware of memory bounds per executor. 
HIVE-20648,LLAP: Vector group by operator should use memory per executor,HIVE-15503 treatment has to be applied for vector group by operator as well. Vector group by currently uses MemoryMX bean to get heap usage and heap max memory which will not work for LLAP. Instead it should use memory per executor as upper bound to make flush decision.  
HIVE-20627,Concurrent async queries intermittently fails with LockException and cause memory leak.,"When multiple async queries are executed from same session, it leads to multiple async query execution DAGs share the same Hive object which is set by caller for all threads. In case of loading dynamic partitions, it creates MoveTask which re-creates the Hive object and closes the shared Hive object which causes metastore connection issues for other async execution thread who still access it. This is also seen if ReplDumpTask and ReplLoadTask are part of the DAG.

*Call Stack:*
{code:java}
2018-09-16T04:38:04,280 ERROR [load-dynamic-partitions-7]: metadata.Hive (Hive.java:call(2436)) - Exception when loading partition with parameters partPath=hdfs://mycluster/warehouse/tablespace/managed/hive/tbl_3bcvvdubni/.hive-staging_hive_2018-09-16_04-35-50_708_7776079613819042057-1147/-ext-10000/age=55, table=tbl_3bcvvdubni, partSpec={age=55}, loadFileType=KEEP_EXISTING, listBucketingLevel=0, isAcid=true, hasFollowingStatsTask=true
org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore
at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidWriteIds(DbTxnManager.java:714) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.io.AcidUtils.getTableValidWriteIdListWithTxnList(AcidUtils.java:1791) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.io.AcidUtils.getTableSnapshot(AcidUtils.java:1756) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.io.AcidUtils.getTableSnapshot(AcidUtils.java:1714) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1976) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.metadata.Hive$5.call(Hive.java:2415) [hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.metadata.Hive$5.call(Hive.java:2406) [hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
Caused by: org.apache.thrift.protocol.TProtocolException: Required field 'validTxnList' is unset! Struct:GetValidWriteIdsRequest(fullTableNames:[default.tbl_3bcvvdubni], validTxnList:null)
at org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.validate(GetValidWriteIdsRequest.java:396) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args.validate(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args$get_valid_write_ids_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args$get_valid_write_ids_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args.write(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:71) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_valid_write_ids(ThriftHiveMetastore.java:5443) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_valid_write_ids(ThriftHiveMetastore.java:5435) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidWriteIds(HiveMetaStoreClient.java:2589) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) ~[?:?]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at com.sun.proxy.$Proxy57.getValidWriteIds(Unknown Source) ~[?:?]
at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) ~[?:?]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2934) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at com.sun.proxy.$Proxy57.getValidWriteIds(Unknown Source) ~[?:?]
at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidWriteIds(DbTxnManager.java:712) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
... 10 more{code}
*Root cause:*
 For Async query execution from SQLOperation.runInternal, we set the Thread local Hive object for all the child threads as parentHive (parentSession.getSessionHive())
{code:java}
@Override
 public void run() {
 PrivilegedExceptionAction<Object> doAsAction = new PrivilegedExceptionAction<Object>() {
 @Override
 public Object run() throws HiveSQLException {
 Hive.set(parentHive); // Setting parentHive for all async operations.
 // TODO: can this result in cross-thread reuse of session state?
 SessionState.setCurrentSessionState(parentSessionState);
 PerfLogger.setPerfLogger(parentPerfLogger);
 LogUtils.registerLoggingContext(queryState.getConf());
 try {
 if (asyncPrepare) {
 prepare(queryState);
 }
 runQuery();
 } catch (HiveSQLException e) {
 // TODO: why do we invent our own error path op top of the one from Future.get?
 setOperationException(e);
 LOG.error(""Error running hive query: "", e);
 } finally {
 LogUtils.unregisterLoggingContext();
 }
 return null;
 }
 };
{code}
Now, when async execution in progress and if one of the thread re-creates the Hive object, it closes the parentHive object first which impacts other threads using it and hence conf object it refers too gets cleaned up and hence we get null for VALID_TXNS_KEY value.
{code:java}
private static Hive create(HiveConf c, boolean needsRefresh, Hive db, boolean doRegisterAllFns)
 throws HiveException {
 if (db != null) {
 LOG.debug(""Creating new db. db = "" + db + "", needsRefresh = "" + needsRefresh +
 "", db.isCurrentUserOwner = "" + db.isCurrentUserOwner());
 db.close();
 }
 closeCurrent();
 if (c == null) {
 c = createHiveConf();
 }
 c.set(""fs.scheme.class"", ""dfs"");
 Hive newdb = new Hive(c, doRegisterAllFns);
 hiveDB.set(newdb);
 return newdb;
 }
{code}
*Fix:*
 We shouldn't clean the old Hive object if it is shared by multiple threads. Shall use a flag to know this.

*Memory leak issue:*
 Memory leak is found if one of the threads from Hive.loadDynamicPartitions throw exception. rawStoreMap is used to store rawStore objects which has to be cleaned. In this case, it is populated only in success flow but if there are exceptions, it is not and hence there is a leak.
{code:java}
futures.add(pool.submit(new Callable<Void>() {
 @Override
 public Void call() throws Exception {
 try {
 // move file would require session details (needCopy() invokes SessionState.get)
 SessionState.setCurrentSessionState(parentSession);
 LOG.info(""New loading path = "" + partPath + "" with partSpec "" + fullPartSpec);

// load the partition
 Partition newPartition = loadPartition(partPath, tbl, fullPartSpec, loadFileType,
 true, false, numLB > 0, false, isAcid, hasFollowingStatsTask, writeId, stmtId,
 isInsertOverwrite);
 partitionsMap.put(fullPartSpec, newPartition);

if (inPlaceEligible) {
 synchronized (ps) {
 InPlaceUpdate.rePositionCursor(ps);
 partitionsLoaded.incrementAndGet();
 InPlaceUpdate.reprintLine(ps, ""Loaded : "" + partitionsLoaded.get() + ""/""
 + partsToLoad + "" partitions."");
 }
 }
 // Add embedded rawstore, so we can cleanup later to avoid memory leak
 if (getMSC().isLocalMetaStore()) {
 if (!rawStoreMap.containsKey(Thread.currentThread().getId())) {
 rawStoreMap.put(Thread.currentThread().getId(), HiveMetaStore.HMSHandler.getRawStore());
 }
 }
 return null;
 } catch (Exception t) {
 }
{code}"
HIVE-20582,Make hflush in hive proto logging configurable,Hive proto logging does hflush to avoid small files issue in hdfs. This may not be ideal for blobstorage where hflush gets applied only on closing of the file. Make hflush configurable so that blobstorage can do close instead of hflush. 
HIVE-20560,Set hive.llap.memory.oversubscription.max.executors.per.query to be 1/3rd of num of executors per node,"Unless hive.llap.memory.oversubscription.max.executors.per.query is not specified by the user,
it should be calculated like this:
{noformat}
hive.llap.memory.oversubscription.max.executors.per.query = min(max(1, int(hive.llap.daemon.num.executors / 3)),8)
{noformat}

In container mode it remains 3 (unless specified otherwise)."
HIVE-20517,Creation of staging directory and Move operation is taking time in S3,"Operations like insert and add partition creates a staging directory to generate the files and then move the files created to actual location. In replication flow, the files are first copied to the staging directory and then moved (rename) to the actual table location. In case of S3, move is not an atomic operation. It internally does a copy and delete. So it can not guarantee the consistency required. So it is better to copy the files directly to the actual location. This will help in avoiding the staging directory creation (which takes 1-2 seconds in s3) and move (which takes time proportional to file size)."
HIVE-20512,Improve record and memory usage logging in SparkRecordHandler,"We currently log memory usage and # of records processed in Spark tasks, but we should improve the methodology for how frequently we log this info. Currently we use the following code:

{code:java}
private long getNextLogThreshold(long currentThreshold) {
    // A very simple counter to keep track of number of rows processed by the
    // reducer. It dumps
    // every 1 million times, and quickly before that
    if (currentThreshold >= 1000000) {
      return currentThreshold + 1000000;
    }
    return 10 * currentThreshold;
  }
{code}

The issue is that after a while, the increase by 10x factor means that you have to process a huge # of records before this gets triggered.

A better approach would be to log this info at a given interval. This would help in debugging tasks that are seemingly hung."
HIVE-20511,REPL DUMP is leaking metastore connections,"With remote metastore, REPL DUMP  leaking connections. Each repl dump task is leaking one connection due to the usage of stale hive object. 

{code}
18/09/04 16:01:46 INFO ReplState: REPL::EVENT_DUMP: {""dbName"":""*"",""eventId"":""566"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""1/0"",""dumpTime"":1536076906}
18/09/04 16:01:46 INFO events.AbstractEventHandler: Processing#567 OPEN_TXN message : {""txnIds"":null,""timestamp"":1536076905,""fromTxnId"":269,""toTxnId"":269,""server"":""thrift://metastore-service.warehouse-1536062326-s74h.svc.cluster.local:9083"",""servicePrincipal"":""""}
18/09/04 16:01:46 INFO ReplState: REPL::EVENT_DUMP: {""dbName"":""*"",""eventId"":""567"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""2/0"",""dumpTime"":1536076906}
18/09/04 16:01:46 INFO metastore.HiveMetaStoreClient: Trying to connect to metastore with URI thrift://metastore-service.warehouse-1536062326-s74h.svc.cluster.local:9083
18/09/04 16:01:46 INFO metastore.HiveMetaStoreClient: Opened a connection to metastore, current connections: 471
18/09/04 16:01:46 INFO metastore.HiveMetaStoreClient: Connected to metastore.
18/09/04 16:01:46 INFO metastore.RetryingMetaStoreClient: RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive (auth:SIMPLE) retries=24 delay=5 lifetime=0
18/09/04 16:01:46 INFO ReplState: REPL::END: {""dbName"":""*"",""dumpType"":""INCREMENTAL"",""actualNumEvents"":2,""dumpEndTime"":1536076906,""dumpDir"":""/user/hive/repl/e45bde27-74dc-45cd-9823-400a8fc1aea3"",""lastReplId"":""567""}
18/09/04 16:01:46 INFO repl.ReplDumpTask: Done dumping events, preparing to return /user/hive/repl/e45bde27-74dc-45cd-9823-400a8fc1aea3,567
18/09/04 16:01:46 INFO ql.Driver: Completed executing command(queryId=hive_20180904160145_30f9570a-44e0-4f3b-b961-1906d3972fc4); Time taken: 0.585 seconds
OK
18/09/04 16:01:46 INFO ql.Driver: OK
18/09/04 16:01:46 INFO lockmgr.DbTxnManager: Stopped heartbeat for query: hive_20180904160145_30f9570a-44e0-4f3b-b961-1906d3972fc4
18/09/04 16:01:46 INFO metastore.HiveMetaStoreClient: Trying to connect to metastore with URI thrift://metastore-service.warehouse-1536062326-s74h.svc.cluster.local:9083
18/09/04 16:01:46 INFO metastore.HiveMetaStoreClient: Opened a connection to metastore, current connections: 472
18/09/04 16:01:46 INFO metastore.HiveMetaStoreClient: Connected to metastore.
{code}"
HIVE-20509,Plan: fix wasted memory in plans with large partition counts,"{code}
  public void addPathToAlias(Path path, String newAlias){
    ArrayList<String> aliases = pathToAliases.get(path);
    if (aliases == null) {
      aliases = new ArrayList<>();
      StringInternUtils.internUriStringsInPath(path);
      pathToAliases.put(path, aliases);
    }
    aliases.add(newAlias.intern());
  }
{code}

ArrayList::DEFAULT_CAPACITY is 10, so this wastes 500 bytes of memory due to the {{new ArrayList<>();}}."
HIVE-20502,Fix NPE while running skewjoin_mapjoin10.q when column stats is used.,"Enabling {{hive.stats.fetch.column.stats}} makes this test fail during:

{code}
EXPLAIN
SELECT a.*, b.* FROM T1_n151 a RIGHT OUTER JOIN T2_n88 b ON a.key = b.key
{code}

Seems like joinKeys is null at [this point|https://github.com/apache/hive/blob/48f92c31dee3983f573f2e66baaa213a0196f1ba/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java#L2169]

Exception:

{code}
2018-09-04T23:47:02,398 DEBUG [fef236ce-e62e-4c20-b0c0-3b15d2b336f7 main] annotation.StatsRulesProcFactory: STATS-JOIN[15]: detects none/multiple PK parents.
2018-09-04T23:47:02,409 ERROR [fef236ce-e62e-4c20-b0c0-3b15d2b336f7 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.isJoinKey(StatsRulesProcFactory.java:2169)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.updateNumNulls(StatsRulesProcFactory.java:2210)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.updateColStats(StatsRulesProcFactory.java:2276)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.process(StatsRulesProcFactory.java:1785)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
{code}

"
HIVE-20441,NPE in GenericUDF  when hive.allow.udf.load.on.demand is set to true,"When hive.allow.udf.load.on.demand is set to true and hiveserver2 has been started, the new created function from other clients or hiveserver2 will be loaded from the metastore at the first time. 

When the udf is used in where clause, we got a NPE like:

{code:java}
Error executing statement:
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException null
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:206) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:290) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:320) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:530) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAP
SHOT]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:517) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHO
T]
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:310) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:542) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1437) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNA
PSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1422) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNA
PSHOT]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:57) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:236) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1104) ~[hive-exec-2.
3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1359) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.
3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.ExpressionWalker.walk(ExpressionWalker.java:76) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:229) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:176) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:11613) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:11568) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:11536) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(SemanticAnalyzer.java:3303) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(SemanticAnalyzer.java:3283) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:9592) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:10549) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:10427) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:11125) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11138) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10807) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1295) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:204) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
{code}
 
The code to get udf from metastore is:
{code:java}
private FunctionInfo getFunctionInfoFromMetastoreNoLock(String functionName, HiveConf conf) {
    try {
      String[] parts = FunctionUtils.getQualifiedFunctionNameParts(functionName);
      Function func = Hive.get(conf).getFunction(parts[0].toLowerCase(), parts[1]);
      if (func == null) {
        return null;
      }
      // Found UDF in metastore - now add it to the function registry.
      FunctionInfo fi = registerPermanentFunction(functionName, func.getClassName(), true,
          FunctionTask.toFunctionResource(func.getResourceUris()));
      if (fi == null) {
        LOG.error(func.getClassName() + "" is not a valid UDF class and was not registered"");
        return null;
      }
      return fi;
    } catch (Throwable e) {
      LOG.info(""Unable to look up "" + functionName + "" in metastore"", e);
    }
    return null;
  }
{code}
 
After getting the function, the function is registered to permanent function list through method 'registerPermanentFunction'.


{code:java}
public FunctionInfo registerPermanentFunction(String functionName,
      String className, boolean registerToSession, FunctionResource... resources) {
    FunctionInfo function = new FunctionInfo(functionName, className, resources);
    // register to session first for backward compatibility
    if (registerToSession) {
      String qualifiedName = FunctionUtils.qualifyFunctionName(
          functionName, SessionState.get().getCurrentDatabase().toLowerCase());
      if (registerToSessionRegistry(qualifiedName, function) != null) {
        addFunction(functionName, function);
        return function;
      }
    } else {
        addFunction(functionName, function);
    }
    return null;
  }
{code}

And the variable registerToSession is true, so  the object 'function' will be returned. But the genericUDF field of the returned function is null which cause the error. 

We should return the result of the method registerToSessionRegistry returned.

"
HIVE-20439,Use the inflated memory limit during join selection for llap,
HIVE-20412,NPE in HiveMetaHook,"{noformat}
java.lang.NullPointerException: null
        at org.apache.hadoop.hive.metastore.HiveMetaHook.preAlterTable(HiveMetaHook.java:113) ~[hive-exec-3.1.0.3.0.1.0-104.jar:3.1.0.3.0.1.0-104]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:427) ~[hive-exec-3.1.0.3.0.1.0-104.jar:3.1.0.3.0.1.0-104]
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(SessionHiveMetaStoreClient.java:415) ~[hive-exec-3.1.0.3.0.1.0-104.jar:3.1.0.3.0.1.0-104]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.0.3.0.1.0-104.jar:3.1.0.3.0.1.0-104]
        at com.sun.proxy.$Proxy37.alter_table(Unknown Source) ~[?:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2933) ~[hive-exec-3.1.0.3.0.1.0-104.jar:3.1.0.3.0.1.0-104]
        at com.sun.proxy.$Proxy37.alter_table(Unknown Source) ~[?:?]
        at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:708) ~[hive-exec-3.1.0.3.0.1.0-104.jar:3.1.0.3.0.1.0-104]
        at org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration$HiveUpdater.updateTableProperties(HiveStrictManagedMigration.java:954) ~[hive-exec-3.1.0.3.0.1.0-104.jar:3.1.0.3.0.1.0-104]
{noformat}"
HIVE-20409,Hive ACID: Update/delete/merge does not clean hdfs staging directory,"UpdateDeleteSemanticAnalyzer creates query context while rewriting the context which doesn't set hdfscleanup, As a result, Driver doesn't clear the staging dir."
HIVE-20389,NPE in SessionStateUserAuthenticator when authenticator=SessionStateUserAuthenticator,"Introduced in HIVE-20118, get the following stack in schematool:
{code}
Caused by: java.lang.IllegalArgumentException: Null user
        at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1221) ~[hadoop-common-3.1.0.jar:?]
        at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1208) ~[hadoop-common-3.1.0.jar:?]
        at org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.getGroupNames(SessionStateUserAuthenticator.java:44) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.session.SessionState.getGroupsFromAuthenticator(SessionState.java:1288) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFCurrentGroups.initialize(GenericUDFCurrentGroups.java:53) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:148) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:260) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1215) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1516) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.ExpressionWalker.walk(ExpressionWalker.java:76) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:241) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:187) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:12752) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:12707) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:12675) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(SemanticAnalyzer.java:3469) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(SemanticAnalyzer.java:3449) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10549) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11526) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11396) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:12160) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:628) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12250) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:356) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:284) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:663) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1865) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1812) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1807) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
{code}"
HIVE-20321,Vectorization: Cut down memory size of 1 col VectorHashKeyWrapper to <1 CacheLine,"With a full sized LLAP instance, the memory size of the VectorHashKeyWrapper is bigger than the low Xmx JVMs.

{code}
***** 64-bit VM: **********************************************************
org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper object internals:
 OFFSET  SIZE                                                                     TYPE DESCRIPTION                                  VALUE
      0    16                                                                          (object header)                              N/A
     16     4                                                                      int VectorHashKeyWrapper.hashcode                N/A
     20     4                                                                          (alignment/padding gap)                     
     24     8                                                                   long[] VectorHashKeyWrapper.longValues              N/A
     32     8                                                                 double[] VectorHashKeyWrapper.doubleValues            N/A
     40     8                                                                 byte[][] VectorHashKeyWrapper.byteValues              N/A
     48     8                                                                    int[] VectorHashKeyWrapper.byteStarts              N/A
     56     8                                                                    int[] VectorHashKeyWrapper.byteLengths             N/A
     64     8                   org.apache.hadoop.hive.serde2.io.HiveDecimalWritable[] VectorHashKeyWrapper.decimalValues           N/A
     72     8                                                     java.sql.Timestamp[] VectorHashKeyWrapper.timestampValues         N/A
     80     8                 org.apache.hadoop.hive.common.type.HiveIntervalDayTime[] VectorHashKeyWrapper.intervalDayTimeValues   N/A
     88     8                                                                boolean[] VectorHashKeyWrapper.isNull                  N/A
     96     8   org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.HashContext VectorHashKeyWrapper.hashCtx                 N/A
Instance size: 104 bytes
Space losses: 4 bytes internal + 0 bytes external = 4 bytes total
{code}

Pulling this up to a parent class allows for this to be cut down to 32 bytes for the single column case."
HIVE-20299,potential race in LLAP signer unit test,
HIVE-20274,HiveServer2 ObjectInspectorFactory leaks for Struct and List object inspectors,"Fix in HIVE-19860 needs to be applied to 

{code}
  static ConcurrentHashMap<ObjectInspector, StandardListObjectInspector>
      cachedStandardListObjectInspector = new ConcurrentHashMap<ObjectInspector, StandardListObjectInspector>();
...
  static ConcurrentHashMap<ArrayList<List<?>>, StandardStructObjectInspector> cachedStandardStructObjectInspector =
      new ConcurrentHashMap<ArrayList<List<?>>, StandardStructObjectInspector>();
...
  static ConcurrentHashMap<ArrayList<Object>, ColumnarStructObjectInspector> cachedColumnarStructObjectInspector =
      new ConcurrentHashMap<ArrayList<Object>, ColumnarStructObjectInspector>();
{code}

And possibly for 

{code}
  static ConcurrentHashMap<Type, ObjectInspector> objectInspectorCache = new ConcurrentHashMap<Type, ObjectInspector>();
...
{code}"
HIVE-20249,LLAP IO: NPE during refCount decrement,"NPE on deallocating buffers
{code:java}
Ignoring exception when closing input calls(cleanup). Exception class=java.lang.NullPointerException

java.lang.NullPointerException: null
at org.apache.hadoop.hive.llap.cache.BuddyAllocator$Arena.deallocate(BuddyAllocator.java:1355) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.llap.cache.BuddyAllocator.deallocate(BuddyAllocator.java:685) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.releaseInitialRefcounts(EncodedReaderImpl.java:676) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:543) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:404) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:263) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:260) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_112]
at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_112]
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[hadoop-common-3.0.0.3.0.0.0-SNAPSHOT.jar:?]
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:260) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:109) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.9.2-SNAPSHOT.jar:0.9.2-SNAPSHOT]
at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) ~[hive-llap-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]{code}"
HIVE-20239,Do Not Print StackTraces to STDERR in MapJoinProcessor,"{code:java|title=MapJoinProcessor.java}
    } catch (Exception e) {
      e.printStackTrace();
      throw new SemanticException(""Failed to generate new mapJoin operator "" +
          ""by exception : "" + e.getMessage());
    }
{code}

Please change to... something like...

{code}
    } catch (Exception e) {
      throw new SemanticException(""Failed to generate new mapJoin operator"", e);
    }
{code}"
HIVE-20237,Do Not Print StackTraces to STDERR in HiveMetaStore,"{code:java|title=HiveMetaStore.java}
    } catch (Throwable x) {
      x.printStackTrace();
      HMSHandler.LOG.error(StringUtils.stringifyException(x));
      throw x;
    }
{code}

Bad design here of ""log and throw"".  Don't do it.  Just throw the exception and let it be handled, and logged, in one place.  At the very least, we don't need the error message to go into the STDERR logs with {{printStackTrace}}, please remove.  And remove the {{stringifyException}} code.  Just use the normal logging faciltiies at the 'debug' level logging to hide the stack trace during normal operations.

{code}
HMSHandler.LOG.debug(""Error"", e);
{code}"
HIVE-20236,Do Not Print StackTraces to STDERR in DDLTask,"{code:java|title=DDLTask.java}
        try {
          ret = ToolRunner.run(fss, args.toArray(new String[0]));
        } catch (Exception e) {
          e.printStackTrace();
          throw new HiveException(e);
        }
{code}

Don't print the stacktrace to STDERR, deal with handling the error up the call stack by using the HiveException."
HIVE-20203,Arrow SerDe leaks a DirectByteBuffer,"ArrowColumnarBatchSerDe allocates an arrow NullableMapVector for each task that uses the serde.

The vector is a DirectByteBuffer allocated from Arrow's off-heap buffer pool.

This buffer is never closed and leaks about 1K of physical memory for each task.

This patch does three things:
 # Ensure the buffer is closed when the RecordWriter for the task is closed. 
 # Adds per-task memory accounting by assigning a ChildAllocator to each task from the RootAllocator.
 # Enforces that the ChildAllocator for a task has released all memory assigned to it, when the task is completed. 

The patch assumes that close() is always called on the RecordWriter when a task is finished (even if there is a failure during task execution). "
HIVE-20192,HS2 with embedded metastore is leaking JDOPersistenceManager objects.,"Hiveserver2 instances where crashing every 3-4 days and observed HS2 in on unresponsive state. Also, observed that the FGC collection happening regularly

From JXray report it is seen that pmCache(List of JDOPersistenceManager objects) is occupying 84% of the heap and there are around 16,000 references of UDFClassLoader.
{code:java}
10,759,230K (84.7%) Object tree for GC root(s) Java Static org.apache.hadoop.hive.metastore.ObjectStore.pmf
- org.datanucleus.api.jdo.JDOPersistenceManagerFactory.pmCache ↘ 10,744,419K (84.6%), 1 reference(s)
  - j.u.Collections$SetFromMap.m ↘ 10,744,419K (84.6%), 1 reference(s)
    - {java.util.concurrent.ConcurrentHashMap}.keys ↘ 10,743,764K (84.5%), 16,872 reference(s)
      - org.datanucleus.api.jdo.JDOPersistenceManager.ec ↘ 10,738,831K (84.5%), 16,872 reference(s)
        ... 3 more references together retaining 4,933K (< 0.1%)
    - java.util.concurrent.ConcurrentHashMap self 655K (< 0.1%), 1 object(s)
      ... 2 more references together retaining 48b (< 0.1%)
- org.datanucleus.api.jdo.JDOPersistenceManagerFactory.nucleusContext ↘ 14,810K (0.1%), 1 reference(s)
... 3 more references together retaining 96b (< 0.1%){code}
When the RawStore object is re-created, it is not allowed to be updated into the ThreadWithGarbageCleanup.threadRawStoreMap which leads to the new RawStore never gets cleaned-up when the thread exit.

 "
HIVE-20162,Do Not Print StackTraces to STDERR in AbstractJoinTaskDispatcher,"https://github.com/apache/hive/blob/6d890faf22fd1ede3658a5eed097476eab3c67e9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/AbstractJoinTaskDispatcher.java

{code}
    } catch (Exception e) {
      e.printStackTrace();
      throw new SemanticException(""Generate Map Join Task Error: "" + e.getMessage());
    }
{code}

Remove the call to {{printStackTrace}} and just throw the error.  If the stack trace really is needed (doubtful), then pass it to the {{SemanticException}} constructor."
HIVE-20161,Do Not Print StackTraces to STDERR in ParseDriver,"https://github.com/apache/hive/blob/6d890faf22fd1ede3658a5eed097476eab3c67e9/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java

{code}
// Do not print stack trace to STDERR - remove this, just throw the HiveException
    } catch (Exception e) {
      e.printStackTrace();
      throw new HiveException(e);
    }
...
// Do not log and throw.  log *or* throw.  In this case, just throw. Remove logging.
// Remove explicit 'return' call. No need for it.
      try {
        skewJoinKeyContext.endGroup();
      } catch (IOException e) {
        LOG.error(e.getMessage(), e);
        throw new HiveException(e);
      }
      return;
{code}
"
HIVE-20160,Do Not Print StackTraces to STDERR in OperatorFactory,"https://github.com/apache/hive/blob/ac6b2a3fb195916e22b2e5f465add2ffbcdc7430/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorFactory.java#L158

{code}
    } catch (Exception e) {
      e.printStackTrace();
      throw new HiveException(...
{code}

Do not print the stack trace.  The error is being wrapped in a HiveException.  Allow the code catching this exception to print the error to a logger instead of dumping it here to STDERR.  There are several instances of this in the class."
HIVE-20159,Do Not Print StackTraces to STDERR in ConditionalResolverSkewJoin,"https://github.com/apache/hive/blob/6d890faf22fd1ede3658a5eed097476eab3c67e9/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverSkewJoin.java#L121

{code}
    } catch (IOException e) {
      e.printStackTrace();
    }
{code}

Introduce an SLF4J logger to this class and print a WARN level log message if the {{IOException}} from {{Utilities.listStatusIfExists}} is generated.  I suggest WARN because the entire operation doesn't fail if this error happens.  It continues on its way with the data that it was able to collect.  I'm not sure if this is the intended behavior, but for now, a helpful warning message in the logging would be better."
HIVE-20158,Do Not Print StackTraces to STDERR in Base64TextOutputFormat,"https://github.com/apache/hive/blob/6d890faf22fd1ede3658a5eed097476eab3c67e9/contrib/src/java/org/apache/hadoop/hive/contrib/fileformat/base64/Base64TextOutputFormat.java

{code}
      try {
        String signatureString = job.get(""base64.text.output.format.signature"");
        if (signatureString != null) {
          signature = signatureString.getBytes(""UTF-8"");
        } else {
          signature = new byte[0];
        }
      } catch (UnsupportedEncodingException e) {
        e.printStackTrace();
      }
{code}

The {{UnsupportedEncodingException}} is coming from the {{getBytes}} method call.  Instead, use the {{CharSet}} version of the method and it doesn't throw this explicit exception so the 'try' block can simply be removed.  Every JVM will support UTF-8.

https://docs.oracle.com/javase/7/docs/api/java/lang/String.html#getBytes(java.nio.charset.Charset)
https://docs.oracle.com/javase/7/docs/api/java/nio/charset/StandardCharsets.html#UTF_8"
HIVE-20156,Printing Stacktrace to STDERR,"Class {{org.apache.hadoop.hive.ql.exec.JoinOperator}} has the following code:

{code}
    } catch (Exception e) {
      e.printStackTrace();
      throw new HiveException(e);
    }
{code}

Do not print the stack trace to STDERR with a call to {{printStackTrace()}}.  Please remove that line and let the code catching the {{HiveException}} worry about printing any messages through a logger."
HIVE-20153,Count and Sum UDF consume more memory in Hive 2+,"While playing with Hive2, we noticed that queries with a lot of count() and sum() aggregations run out of memory on Hadoop side where they worked before in Hive1. 

In many queries, we have to double the Mapper Memory settings (in our particular case mapreduce.map.java.opts from -Xmx2000M to -Xmx4000M), it makes it not so easy to upgrade to Hive 2.

Taking heap dump, we see one of the main culprit is the field 'uniqueObjects' in GeneraicUDAFSum and GenericUDAFCount, which was added to support Window functions."
HIVE-20098,Statistics: NPE when getting Date column partition statistics,"The issue reproduces only for a date column for a partitioned table. It reproduces only if the date column has all the values set to null, and if the partition is not empty.

Here is a quick reproducer:

 

 
{code:java}
CREATE TABLE dummy_table (
c_date DATE,
c_bigint BIGINT
)
PARTITIONED BY (ds STRING);

INSERT OVERWRITE TABLE dummy_table PARTITION (ds='2018-01-01') SELECT CAST(null AS DATE), CAST(null AS BIGINT) FROM <any non empty table>;

ANALYZE TABLE dummy_table COMPUTE STATISTICS FOR COLUMNS;

DESCRIBE FORMATTED dummy_table.c_bigint PARTITION (ds='2018-01-01');
DESCRIBE FORMATTED dummy_table.c_date PARTITION (ds='2018-01-01');
{code}
 

 

The first `DESCRIBE FORMATTED` statement succeeds, when the second fails with an `NPE`

 

It happens because the null check is missing when converting Object from the ObjectStore to the Thrift object. The null check is missing only in the date statistics conversion for the partitioned table. 

Missing: [https://github.com/apache/hive/blob/master/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java#L469]

Present: https://github.com/apache/hive/blob/master/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java#L298

 "
HIVE-20038,Update queries on non-bucketed + partitioned tables throws NPE,"With HIVE-19890 delete deltas of non-bucketed tables are computed from ROW__ID. This can create holes in output paths (and final paths) in FSOp.commit() resulting in NPE. 

Following is the exception
{code:java}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commitOneOutPath(FileSinkOperator.java:246)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:235)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$400(FileSinkOperator.java:168)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:1325)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:733)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:757)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:383){code}"
HIVE-19970,Replication dump has a NPE when table is empty,if table directory or partition directory is missing ..dump is throwing NPE instead of file missing exception.
HIVE-19884,Invalidation cache may throw NPE when there is no data in table used by materialized view,
HIVE-19860,HiveServer2 ObjectInspectorFactory memory leak with cachedUnionStructObjectInspector,"hiveserver2 is start seeing the memory pressure once the cachedUnionStructObjectInspector start going 

[https://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorFactory.java#L345]

I did not see any eviction policy for cachedUnionStructObjectInspector, so we should implement some size or time-based eviction policy. 

  !Screen Shot 2018-06-11 at 2.01.00 PM.png!"
HIVE-19777,NPE in TezSessionState,"Encountered while running ""insert into table values (..)""

Looks like it is due to the fact that TezSessionState.close() sets console to null at the start of the method, and then calls getSession() which attempts to log to console.

{noformat}
java.lang.NullPointerException: null
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getSession(TezSessionState.java:711) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.close(TezSessionState.java:646) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.closeIfNotDefault(TezSessionPoolManager.java:353) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.getSession(TezSessionPoolManager.java:467) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.getUnmanagedSession(WorkloadManagerFederation.java:66) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.getSession(WorkloadManagerFederation.java:38) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:184) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2497) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2149) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1826) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1569) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1563) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:218) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) ~[hive-cli-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188) ~[hive-cli-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402) ~[hive-cli-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) ~[hive-cli-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) ~[hive-cli-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683) ~[hive-cli-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]
        at org.apache.hadoop.util.RunJar.run(RunJar.java:308) ~[hadoop-common-3.0.0.3.0.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.util.RunJar.main(RunJar.java:222) ~[hadoop-common-3.0.0.3.0.0.0-SNAPSHOT.jar:?]
{noformat}"
HIVE-19731,Change staging tmp directory used by TestHCatLoaderComplexSchema,"Another one that is set to default and hence is flaky.

https://builds.apache.org/job/PreCommit-HIVE-Build/11321/testReport/org.apache.hive.hcatalog.pig/TestHCatLoaderComplexSchema/testSyntheticComplexSchema_3_/

{noformat}
org.apache.hadoop.util.Shell$ExitCodeException: chmod: cannot access ‘/tmp/hadoop/mapred/staging/hiveptest985275899/.staging/job_local985275899_0088’: No such file or directory

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:902) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1321) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1303) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:840) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:508) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:489) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:511) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:727) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.mkdirs(JobResourceUploader.java:658) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:172) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:133) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_102]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_102]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.submit(ControlledJob.java:336) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at org.apache.pig.backend.hadoop23.PigJobControl.submit(PigJobControl.java:128) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:194) [pig-0.16.0-h2.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_102]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276) [pig-0.16.0-h2.jar:?]
{noformat}"
HIVE-19654,Change tmp staging mapred directory for TestBlobstoreCliDriver,Similar to HIVE-19626.
HIVE-19628,possible NPE in LLAP testSigning,
HIVE-19626,Change tmp staging mapred directory for CliDriver,"We do not see many failures anymore, but one of the intermittent ones is this:
https://builds.apache.org/job/PreCommit-HIVE-Build/11101/testReport/junit/org.apache.hadoop.hive.cli/TestCliDriver/testCliDriver_localtimezone_/

As with other tests, we can change the staging directory property value."
HIVE-19430,ObjectStore.cleanNotificationEvents OutOfMemory on large number of pending events,"If there are large number of events that haven't been cleaned up for some reason, then ObjectStore.cleanNotificationEvents() can run out of memory while it loads all the events to be deleted.
It should fetch events in batches.
"
HIVE-19424,NPE In MetaDataFormatters,"h2. Overview

According to the Hive Schema definition, a table's {{INPUT_FORMAT}} class can be set to NULL.  However, there are places in the code where we do not account for this NULL value, in particular the {{MetaDataFormatters}} classes {{TextMetaDataFormatter}} and {{JsonMetaDataFormatter}}.  In addition, there is no debug level logging in the {{MetaDataFormatters}} classes to tell me which table in particular is causing the problem.

{code:sql|title=hive-schema-2.2.0.mysql.sql}
CREATE TABLE IF NOT EXISTS `SDS` (
  `SD_ID` bigint(20) NOT NULL,
  `CD_ID` bigint(20) DEFAULT NULL,
  `INPUT_FORMAT` varchar(4000) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,
  `IS_COMPRESSED` bit(1) NOT NULL,
...
{code}

{code:java|title=TextMetaDataFormatter.java}
// Not checking for a null return from getInputFormatClass
inputFormattCls = par.getInputFormatClass().getName();
outputFormattCls = par.getOutputFormatClass().getName();
{code}

h2. Reproduction

{code:sql}
-- MySQL Backend
update SDS SET INPUT_FORMAT=NULL WHERE SD_ID=XXX;
{code}

{code}
// Hive
SHOW TABLE EXTENDED FROM default LIKE '*';

// HS2 Logs
[HiveServer2-Background-Pool: Thread-464]: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Exception while processing show table status
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:238)
	at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:89)
	at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:301)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
	at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:314)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Exception while processing show table status
	at org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:3025)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:405)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:99)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2052)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1748)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1501)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1285)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1280)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236)
	... 11 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.showTableStatus(TextMetaDataFormatter.java:202)
	at org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:3020)
	... 20 more
{code}"
HIVE-19423,REPL LOAD creates staging directory in source dump directory instead of table data location,REPL LOAD creates staging directory in source dump directory instead of table data location. In case of replication from on-perm to cloud it can create problem. 
HIVE-19331,"Repl load config in ""with"" clause not pass to Context.getStagingDir","Another failure similar to HIVE-18626, causing exception when s3 credentials are in ""REPL LOAD"" with clause.

{code}
Caused by: java.lang.IllegalStateException: Error getting FileSystem for s3a://nat-yc-r7-nmys-beacon-cloud-s3-2/hive_incremental_testing.db/hive_incremental_testing_new_tabl...: org.apache.hadoop.fs.s3a.AWSClientIOException: doesBucketExist on nat-yc-r7-nmys-beacon-cloud-s3-2: com.amazonaws.AmazonClientException: No AWS Credentials provided by BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider SharedInstanceProfileCredentialsProvider : com.amazonaws.AmazonClientException: Unable to load credentials from Amazon EC2 metadata service: No AWS Credentials provided by BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider SharedInstanceProfileCredentialsProvider : com.amazonaws.AmazonClientException: Unable to load credentials from Amazon EC2 metadata service
        at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:359)
        at org.apache.hadoop.hive.ql.Context.getExternalScratchDir(Context.java:487)
        at org.apache.hadoop.hive.ql.Context.getExternalTmpPath(Context.java:565)
        at org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.loadTable(ImportSemanticAnalyzer.java:370)
        at org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.createReplImportTasks(ImportSemanticAnalyzer.java:926)
        at org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.prepareImport(ImportSemanticAnalyzer.java:329)
        at org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.handle(TableHandler.java:43)
        ... 24 more
{code}"
HIVE-19265,Potential NPE and hiding actual exception in Hive#copyFiles,"{{In Hive#copyFiles}} we have such code
{code:java}
if (src.isDirectory()) {
        try {
          files = srcFs.listStatus(src.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);
        } catch (IOException e) {
          pool.shutdownNow();
          throw new HiveException(e);
        }
      }
{code}
If pool is null we will get NPE and actual cause will be lost.

Initializing of pool
{code:java}
    final ExecutorService pool = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25) > 0 ?
        Executors.newFixedThreadPool(conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25),
        new ThreadFactoryBuilder().setDaemon(true).setNameFormat(""Move-Thread-%d"").build()) : null;
{code}
So in the case when the pool is not created we can get potential NPE and swallow an actual exception"
HIVE-19222,"TestNegativeCliDriver tests are failing due to ""java.lang.OutOfMemoryError: GC overhead limit exceeded""",TestNegativeCliDriver tests are failing with OOM recently. Not sure why. I will try to increase the memory to test out.  
HIVE-19206,Automatic memory management for open streaming writers,"Problem:
 When there are 100s of record updaters open, the amount of memory required by orc writers keeps growing because of ORC's internal buffers. This can lead to potential high GC or OOM during streaming ingest.

Solution:
 The high level idea is for the streaming connection to remember all the open record updaters and flush the record updater periodically (at some interval). Records written to each record updater can be used as a metric to determine the candidate record updaters for flushing. 
 If stripe size of orc file is 64MB, the default memory management check happens only after every 5000 rows which may which may be too late when there are too many concurrent writers in a process. Example case would be 100 writers open and each of them have almost full stripe of 64MB buffered data, this would take 100*64MB ~=6GB of memory. When all of the record writers flush, the memory usage drops down to 100*~2MB which is just ~200MB memory usage."
HIVE-19172,NPE due to null EnvironmentContext in DDLTask,"Stack Trace -
{code}
2018-04-11T02:52:51,386 ERROR [5f2e24bf-ac93-4977-84fe-aa2c5f674ea4 main] exec.DDLTask: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3539)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:392)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1987)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1667)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1414)
{code}"
HIVE-19158,Fix NPE in the HiveMetastore add partition tests,"The TestAddPartitions and TestAddPartitionsFromPartSpec tests revealed that NPE is thrown in some cases. These NPEs could be prevented with a simple null check and a MetaException with a proper error message should be thrown instead.
 Example: NPE is thrown in the following test cases
 * TestAddPartitions
 ** testAddPartitionNullPartition
 ** testAddPartitionNullValue
 ** testAddPartitionsNullList
 * TestAddPartitionsFromPartSpec
 ** testAddPartitionSpecNullSpec
 ** testAddPartitionSpecNullPartList
 ** testAddPartitionSpecNoDB
 ** testAddPartitionSpecNoTable
 ** testAddPartitionSpecNoDBAndTableInPartition
 ** testAddPartitionSpecNullPart
 ** testAddPartitionSpecChangeRootPathToNull
 ** testAddPartitionSpecWithSharedSDNullSd
 ** testAddPartitionSpecWithSharedSDNoValue
 ** testAddPartitionSpecNullValue"
HIVE-19130,NPE is thrown when REPL LOAD applied drop partition event.,"During incremental replication, if we split the events batch as follows, then the REPL LOAD on second batch throws NPE.

Batch-1: CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION (t1.p1)

Batch-2: DROP_TABLE(t1) ->  CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION (t1.p1)



{code}
2018-04-05 16:20:36,531 ERROR [HiveServer2-Background-Pool: Thread-107044]: metadata.Hive (Hive.java:getTable(1219)) - Table catalog_sales_new not found: new5_tpcds_real_bin_partitioned_orc_1000.catalog_sales_new table not found
2018-04-05 16:20:36,538 ERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException
        at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4016)
        at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3983)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:341)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:162)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1765)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1506)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1303)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1165)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)
        at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)
        at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
        at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)
        at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)
        ... 23 more
{code}"
HIVE-19126,CachedStore: Use memory estimation to limit cache size during prewarm,"We can rely on https://github.com/apache/hive/blob/master/llap-server/src/java/org/apache/hadoop/hive/llap/IncrementalObjectSizeEstimator.java to estimate memory of SharedCache. This jira addresses the size estimation during prewarm, so that we can stop when we hit the memory limit. In a follow-up jira, we will work on estimation/eviction after prewarm is complete, so that we can keep the frequently used tables and their partitions in cache."
HIVE-19076,Fix NPE and TApplicationException in function related HiveMetastore methods,"The TestFunctions tests revealed that NPE is thrown in some cases. These NPEs could be prevented with a simple null check and a MetaException with a proper error message should be thrown instead.
 Example: NPE is thrown in the following test cases
 * testCreateFunctionNullFunctionName
 * testCreateFunctionNullDatabaseName
 * testCreateFunctionNullOwnerType
 * testCreateFunctionNullFunctionType
 * testGetFunctionNullDatabase
 * testDropFunctionNullDatabase
 * testDropFunctionNullFunctionName
 * testAlterFunctionNullDatabase
 * testAlterFunctionNullFunctionName
 * testAlterFunctionNullFunction
 * testAlterFunctionNullFunctionNameInNew
 * testAlterFunctionNullDatabaseNameInNew
 * testAlterFunctionNullOwnerTypeInNew
 * testAlterFunctionNullFunctionTypeInNew

Also there are some alter function tests where InvalidObjectException is thrown with Embedded MetaStore, but TApplicationException it thrown with Remote MetaStore. The reason is that the InvalidObjectException is not defined for the alter_function method in the thrift interface, so we got the TApplicationException when the InvalidObjectException was thrown. In these cases the InvalidObjectException could be handled on the server side and re-throw it as a MetaException"
HIVE-19075,Fix NPE when trying to drop or get DB with null name,"The TestDatabases tests revealed that NPE is thrown if the get_database_core and drop_database_core methods are called with null DB name. These NPEs could be prevented with a simple null check and a MetaException with a proper error message should be thrown instead.
Example: NPE is thrown in the following test cases
 * TestDatabases.testGetDatabaseNullName
 * TestDatabases.testDropDatabaseNullName"
HIVE-18986,Table rename will run java.lang.StackOverflowError in dataNucleus if the table contains large number of columns,"If the table contains a lot of columns e.g, 5k, simple table rename would fail with the following stack trace. The issue is datanucleus can't handle the query with lots of colName='c1' && colName='c2' && ... .

 

2018-03-13 17:19:52,770 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit: [pool-5-thread-200]: ugi=anonymous ip=10.17.100.135 cmd=source:10.17.100.135 alter_table: db=default tbl=fgv_full_var_pivoted02 newtbl=fgv_full_var_pivoted 2018-03-13 17:20:00,495 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-5-thread-200]: java.lang.StackOverflowError at org.datanucleus.store.rdbms.sql.SQLText.toSQL(SQLText.java:330) at org.datanucleus.store.rdbms.sql.SQLText.toSQL(SQLText.java:339) at org.datanucleus.store.rdbms.sql.SQLText.toSQL(SQLText.java:339) at org.datanucleus.store.rdbms.sql.SQLText.toSQL(SQLText.java:339) at org.datanucleus.store.rdbms.sql.SQLText.toSQL(SQLText.java:339)

 "
HIVE-18975,NPE when inserting NULL value in structure and array with HBase table,"STR (Structure)

*STEP 1. Create tables*

{code}
CREATE TABLE IF NOT EXISTS t1 (id INT);
INSERT INTO TABLE t1 VALUES (1),(2),(3),(4),(5);
CREATE TABLE IF NOT EXISTS `htable`(
  `id` INT, 
  `map_column` STRUCT<s_int:INT,s_string:STRING,s_date:DATE>) ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe'  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  WITH SERDEPROPERTIES (   'hbase.columns.mapping'=':key,id:id',    'serialization.format'='1') TBLPROPERTIES ( 'hbase.table.name'='tmp/h');
{code}

*STEP 2. Insert into table stored in HBase the struct with NULL value in it*

{code}
INSERT INTO `htable` SELECT 2,NAMED_STRUCT(""s_int"",CAST(NULL AS INT),""s_string"",""s1"",""s_date"",CAST('2018-03-12' AS DATE)) FROM t1 LIMIT 1;
{code}

*ACTUAL RESULT*

The query fails with NPE.

{code}
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":2,""_col1"":{""s_int"":null,""s_string"":""s1"",""s_date"":""2018-03-12""}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:257)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1631)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":2,""_col1"":{""s_int"":null,""s_string"":""s1"",""s_date"":""2018-03-12""}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:245)
	... 7 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:787)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.LimitOperator.process(LimitOperator.java:63)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:236)
	... 7 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:301)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:714)
	... 12 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)
	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:239)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:295)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:297)
	... 13 more
{code}

*EXPECTED RESULT*

The query finished successfully.

STR (arrays)

*STEP 1. Create tables*

{code}
CREATE TABLE hbase_list(id INT, list_column array<string>) ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe' STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,id:id', 'serialization.format'='1') TBLPROPERTIES ( 'hbase.table.name'='tmp/htest');
{code}

*STEP 2. Insert into table stored in HBase the array with NULL value in it*

{code}
insert into hbase_list SELECT 2, array(""a"", CAST (NULL AS STRING),  ""b"") FROM t1 LIMIT 1;
{code}

*ACTUAL RESULT*

The query fails with NPE.

{code}
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":2,""_col1"":[""a"",null,""b""]}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:257)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1631)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":2,""_col1"":[""a"",null,""b""]}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:245)
	... 7 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:787)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.LimitOperator.process(LimitOperator.java:63)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:236)
	... 7 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:301)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:714)
	... 12 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:260)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:251)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:297)
	... 13 more
{code}


*EXPECTED RESULT*

The query finished successfully."
HIVE-18946,Fix columnstats merge NPE,"after analyzing an empty table may lead to an NPE when inserting into it...

{code}
2018-03-13T06:54:22,503 ERROR [df3fb505-e0bc-4595-a874-b735dab8dff6 main] metastore.RetryingHMSHandler: java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.api.Decimal.compareTo(Decimal.java:318)
        at org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger.merge(DecimalColumnStatsMerger.java:35)
        at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.mergeColStats(MetaStoreUtils.java:778)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.set_aggr_stats_for(HiveMetaStore.java:6934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy55.set_aggr_stats_for(Unknown Source)
[...]
{code}

reproduce

{code}
set hive.stats.autogather=true;
set hive.explain.user=true;

drop table if exists testdeci2;

create table testdeci2(
id int,
amount decimal(10,3),
sales_tax decimal(10,3),
item string)
stored as orc location '/tmp/testdeci2'
TBLPROPERTIES (""transactional""=""false"")
;


analyze table testdeci2 compute statistics for columns;

insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2');
{code}"
HIVE-18929,The method humanReadableInt in HiveStringUtils.java has a race condition.,"I found that the {{humanReadableInt(long number)}} method in the hive/common/src/java/org/apache/hive/common/util/HiveStringUtils.java file contains code which has a race condition as shown in Hadoop (issue tracking ID HADOOP-9252: https://issues.apache.org/jira/browse/HADOOP-9252). The fix can also be seen in the Hadoop code base.

I couldn't find a call to the method anywhere else in the code. But it might be worth to fix."
HIVE-18928,HS2: Perflogger has a race condition,"{code}

Caused by: java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437) ~[?:1.8.0_112]
        at java.util.HashMap$EntryIterator.next(HashMap.java:1471) ~[?:1.8.0_112]
        at java.util.HashMap$EntryIterator.next(HashMap.java:1469) ~[?:1.8.0_112]
        at java.util.AbstractCollection.toArray(AbstractCollection.java:196) ~[?:1.8.0_112]
        at com.google.common.collect.Iterables.toArray(Iterables.java:316) ~[guava-19.0.jar:?]
        at com.google.common.collect.ImmutableMap.copyOf(ImmutableMap.java:342) ~[guava-19.0.jar:?]
        at com.google.common.collect.ImmutableMap.copyOf(ImmutableMap.java:327) ~[guava-19.0.jar:?]
        at org.apache.hadoop.hive.ql.log.PerfLogger.getEndTimes(PerfLogger.java:218) ~[hive-common-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1561) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1498) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:198) ~[hive-service-3.0.0.3.0.0.2-132.jar:3.0.0.3.0.0.2-132]
{code}
"
HIVE-18898,Fix NPEs in HiveMetastore.dropPartition method,"The TestDropPartitions tests revealed that NPE is thrown if the dropPartition(String db_name, String tbl_name, List<String> part_vals, PartitionDropOptions options) method is called with null options and with a part_vals list which contains null elements.

Example: NPE is thrown in the following test cases
 * testDropPartitionNullPartDropOptions
 * testDropPartitionNullVal"
HIVE-18892,Fix NPEs in HiveMetastore.exchange_partitions method,"The TestExchangePartitions tests revealed that NPE is thrown if the exchange_partitions method is called with null, empty or non-existing DB and table names. These NPEs could be prevented with a simple null check and a MetaException with a proper error message should be thrown instead.
Example: NPE is thrown in the following test cases
 * testExchangePartitionsNonExistingSourceTable
 * testExchangePartitionsNonExistingSourceDB
 * testExchangePartitionsNonExistingDestTable
 * testExchangePartitionsNonExistingDestDB
 * testExchangePartitionsEmptySourceTable
 * testExchangePartitionsEmptySourceDB
 * testExchangePartitionsEmptyDestTable
 * testExchangePartitionsEmptyDestDB
 * testExchangePartitionsNullSourceTable
 * testExchangePartitionsNullSourceDB
 * testExchangePartitionsNullDestTable
 * testExchangePartitionsNullDestDB
 * testExchangePartitionsNullPartSpec"
HIVE-18886,ACID: NPE on unexplained mysql exceptions ,"At 200+ sessions on a single HS2, the DbLock impl fails to propagate mysql exceptions

{code}
2018-03-06T22:55:16,197 ERROR [HiveServer2-Background-Pool: Thread-12867]: ql.Driver (:()) - FAILED: Error in acquiring locks: null
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.DatabaseProduct.isDeadlock(DatabaseProduct.java:56)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkRetryable(TxnHandler.java:2459)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:499)
{code}

{code}
    return e instanceof SQLTransactionRollbackException
        || ((dbProduct == MYSQL || dbProduct == POSTGRES || dbProduct == SQLSERVER)
            && e.getSQLState().equals(""40001""))
        || (dbProduct == POSTGRES && e.getSQLState().equals(""40P01""))
        || (dbProduct == ORACLE && (e.getMessage().contains(""deadlock detected"")
            || e.getMessage().contains(""can't serialize access for this transaction"")));
{code}"
HIVE-18885,DbNotificationListener has a deadlock between Java and DB locks (2.x line),"You can see the problem from looking at the code, but it actually created severe problems for real life Hive user.

When {{alter table}} has {{cascade}} option it does the following:
{code:java}
         msdb.openTransaction()
          ...
          List<Partition> parts = msdb.getPartitions(dbname, name, -1);
          for (Partition part : parts) {
            List<FieldSchema> oldCols = part.getSd().getCols();
            part.getSd().setCols(newt.getSd().getCols());
            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());
            updatePartColumnStatsForAlterColumns(msdb, part, oldPartName, part.getValues(), oldCols, part);
            msdb.alterPartition(dbname, name, part.getValues(), part);
          }
 {code}

So it walks all partitions (and this may be huge list) and does some non-trivial operations in one single uber-transaction.

When DbNotificationListener is enabled, it adds an event for each partition, all while
holding a row lock on NOTIFICATION_SEQUENCE table. As a result, while this is happening no other write DDL can proceed. This can sometimes cause DB lock timeouts which cause HMS level operation retries which make things even worse.

In one particular case this pretty much made HMS unusable."
HIVE-18786,NPE in Hive windowing functions,"When I run a Hive query with windowing functions, if there's enough data I get an NPE.

For example something like this query might break:

select id, created_date, max(created_date) over (partition by id) latest_created_any from ...

The only workaround I've found is to remove the windowing functions entirely.

The stacktrace looks suspiciously similar to {+}HIVE-15278{+}, but I'm in hive-2.3.2 which appears to have the bugfix applied.

 

 Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) <some row data here>
       at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:297)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:317)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
       ... 14 more

 Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) <some row data here>
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:365)
       at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:287)
        ... 16 more

Caused by: java.lang.NullPointerException
         at org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.first(PTFRowContainer.java:115)
         at org.apache.hadoop.hive.ql.exec.PTFPartition.iterator(PTFPartition.java:114)
         at org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator.getPartitionAgg(BasePartitionEvaluator.java:200)
         at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.evaluateFunctionOnPartition(WindowingTableFunction.java:155)
         at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.iterator(WindowingTableFunction.java:538)
         at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.finishPartition(PTFOperator.java:349)
         at org.apache.hadoop.hive.ql.exec.PTFOperator.process(PTFOperator.java:123)
         at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
         at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
         at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:356)"
HIVE-18776,MaterializationsInvalidationCache loading causes race condition in the metastore,"I am seeing occasional failures running metastore tests where operations are failing saying that there is no open transaction.  I have traced this to a race condition in loading the materialized view invalidation cache.  When it is initialized (either in HiveMetaStoreClient in embedded mode or in HiveMetaStore in remote mode) it grabs a copy of the current RawStore instance and then loads the cache in a separate thread.  But ObjectStore keeps state regarding JDO transactions with the underlying RDBMS.  So with the loader thread and the initial thread both doing operations against the RawStore they sometimes mess up each others transaction stack.  In a quick test I used HMSHandler.newRawStoreForConf() to fix this, which seemed to work.

A reference to the TxnHandler is also called.  I suspect this will run into a similar issue."
HIVE-18766,"Race condition during shutdown of RemoteDriver, error messages aren't always sent","When we send an error during shutdown of the {{RemoteDriver}}, we don't wait for the error message to be sent. We just send it and then close the RPC channel. For a graceful shutdown, that doesn't seem ideal. We should at least wait a bit for the RPC message to be sent before shutting things down. I ran some tests locally and its pretty easy to hit a situation where the error message doesn't even get sent to the {{SparkClientImpl}}"
HIVE-18611,Avoid memory allocation of aggregation buffer during stats computation ,Bloom filter aggregation buffer may result in allocation of upto ~594MB array which is unnecessary.
HIVE-18606,CTAS on empty table throws NPE from org.apache.hadoop.hive.ql.exec.MoveTask,"{noformat}
@Test
public void testCtasEmpty() throws Exception {
  MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);
  runStatementOnDriver(""create table myctas stored as ORC as"" +
      "" select a, b from "" + Table.NONACIDORCTBL);
  List<String> rs = runStatementOnDriver(""select ROW__ID, a, b, INPUT__FILE__NAME"" +
      "" from myctas order by ROW__ID"");
}
{noformat}
{noformat}
2018-02-01T19:08:52,813 INFO  [HiveServer2-Background-Pool: Thread-463]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 114: Done cleaning up thread local RawStore
2018-02-01T19:08:52,813 INFO  [HiveServer2-Background-Pool: Thread-463]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(305)) - ugi=hive ip=unknown-ip-addr      cmd=Done cleaning up thread local RawStore
2018-02-01T19:08:52,815 ERROR [HiveServer2-Background-Pool: Thread-463]: exec.Task (SessionState.java:printError(1228)) - Failed with exception null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.Hive.moveAcidFiles(Hive.java:3816)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:298)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2267)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1919)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1651)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1395)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1388)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:253)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:92)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:345)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:358)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

2018-02-01T19:08:52,815 ERROR [HiveServer2-Background-Pool: Thread-463]: ql.Driver (SessionState.java:printError(1228)) - FAILED: Execution Error, return code 1 from {noformat}
 "
HIVE-18551,Vectorization: VectorMapOperator tries to write too many vector columns for Hybrid Grace,Code incorrectly uses projectedColumns.length instead of singleRow.length
HIVE-18459,hive-exec.jar leaks contents fb303.jar into classpath,"thrift classes are now in the hive classpath in the hive-exec.jar (HIVE-11553). This makes it hard to test with other versions of this library. This library is already a declared dependency and is not required to be included in the hive-exec.jar.

I am proposing that we not include these classes like we have done in the past releases."
HIVE-18426,Memory leak in RoutingAppender for every hive operation,"Each new operation creates new entry in the ConcurrentMap in RoutingAppender but when the operation ends, AppenderControl stored in the map is retrieved and stopped but the entry in ConcurrentMap is never cleaned up.
"
HIVE-18421,Vectorized execution handles overflows in a different manner than non-vectorized execution,"In vectorized execution arithmetic operations which cause integer overflows can give wrong results. Issue is reproducible in both Orc and parquet.

Simple test case to reproduce this issue

{noformat}
set hive.vectorized.execution.enabled=true;
create table parquettable (t1 tinyint, t2 tinyint) stored as parquet;
insert into parquettable values (-104, 25), (-112, 24), (54, 9);
select t1, t2, (t1-t2) as diff from parquettable where (t1-t2) < 50 order by diff desc;
+-------+-----+-------+
|  t1   | t2  | diff  |
+-------+-----+-------+
| -104  | 25  | 127   |
| -112  | 24  | 120   |
| 54    | 9   | 45    |
+-------+-----+-------+
{noformat}

When vectorization is turned off the same query produces only one row."
HIVE-18360,NPE in TezSessionState,"{noformat}
2018-01-03T01:09:23,822 ERROR [HiveServer2-Background-Pool: Thread-409]: exec.Task (:()) - Failed to execute tez graph.
java.lang.NullPointerException: null
  at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.ensureLocalResources(TezSessionState.java:600) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:264) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.openInternal(TezSessionPoolSession.java:127) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:223) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.exec.tez.TezTask.ensureSessionHasResources(TezTask.java:352) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:188) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2257) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1909) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1640) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1385) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1378) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
{noformat}"
HIVE-18314,qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q	,"{code}
mvn install -q -am -pl itests/qtest -DskipSparkTests -Dtest=TestMiniLlapLocalCliDriver -Dqfile=semijoin_hint.q,hybridgrace_hashjoin_2.q
{code}"
HIVE-18284,NPE when inserting data with 'distribute by' clause with dynpart sort optimization,"A Null Pointer Exception occurs when inserting data with 'distribute by' clause. The following snippet query reproduces this issue:
*(non-vectorized , non-llap mode)*

{code:java}
create table table1 (col1 string, datekey int);
insert into table1 values ('ROW1', 1), ('ROW2', 2), ('ROW3', 1);
create table table2 (col1 string) partitioned by (datekey int);

set hive.vectorized.execution.enabled=false;
set hive.optimize.sort.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
insert into table table2
PARTITION(datekey)
select col1,
datekey
from table1
distribute by datekey ;
{code}

I could run the insert query without the error if I remove Distribute By  or use Cluster By clause.
It seems that the issue happens because Distribute By does not guarantee clustering or sorting properties on the distributed keys.

FileSinkOperator removes the previous fsp. FileSinkOperator will remove the previous fsp which might be re-used when we use Distribute By.
https://github.com/apache/hive/blob/branch-2.3/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L972

The following stack trace is logged.

{code:java}
Vertex failed, vertexName=Reducer 2, vertexId=vertex_1513111717879_0056_1_01, diagnostics=[Task failed, taskId=task_1513111717879_0056_1_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1513111717879_0056_1_01_000000_0:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":""ROW3"",""_col1"":1}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":""ROW3"",""_col1"":1}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:365)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:250)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:317)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:762)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:356)
	... 17 more
{code}

"
HIVE-18232,Packaging: add dfs-init script in package target,As discussed with Ashutosh Chauhan this change is to include init-hive-dfs.sh in the hive package.
HIVE-18148,NPE in SparkDynamicPartitionPruningResolver,"The stack trace is:
{noformat}
2017-11-27T10:32:38,752 ERROR [e6c8aab5-ddd2-461d-b185-a7597c3e7519 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.physical.SparkDynamicPartitionPruningResolver$SparkDynamicPartitionPruningDispatcher.dispatch(SparkDynamicPartitionPruningResolver.java:100)
        at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
        at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)
        at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)
        at org.apache.hadoop.hive.ql.optimizer.physical.SparkDynamicPartitionPruningResolver.resolve(SparkDynamicPartitionPruningResolver.java:74)
        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.optimizeTaskPlan(SparkCompiler.java:568)
{noformat}
At this stage, there shouldn't be a DPP sink whose target map work is null. The root cause seems to be a malformed operator tree generated by SplitOpTreeForDPP."
HIVE-18088,Add WM event traces at query level for debugging,"For debugging and testing purpose, expose workload manager events via /jmx endpoint and print summary at the scope of query."
HIVE-18006,Optimize memory footprint of HLLDenseRegister,"{code}
private double[] invPow2Register;
{code}
seems to add up memory when caching column stats (#table * #partition * #cols). This register can be pre-computed and stored as constant. "
HIVE-17962,org.apache.hadoop.hive.metastore.security.MemoryTokenStore - Parameterize Logging,"* Parameterize logging
* Small simplification"
HIVE-17961,NPE during initialization of VectorizedParquetRecordReader when input split is null,HIVE-16465 introduces the regression which causes a NPE during initialize of the vectorized reader when input split is null. This was already fixed in HIVE-15718 but got exposed again we refactored for HIVE-16465. We should also add a test case to catch such regressions in the future.
HIVE-17918,NPE during semijoin reduction optimization when LLAP caching disabled,"DynamicValue (used by semijoin reduction optimization) relies on the ObjectCache. If LLAP cache is disabled then the DynamicValue is broken in LLAP:

{noformat}
org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:283)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:237)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:101)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:76)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:419)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:254)
        ... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:928)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:92)
        ... 18 more
Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_25_household_demographics_hd_demo_sk_min
        at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:130)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColumnBetweenDynamicValue.evaluate(FilterLongColumnBetweenDynamicValue.java:80)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.evaluate(FilterExprAndExpr.java:39)
        at org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.evaluate(FilterExprAndExpr.java:41)
        at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:112)
        at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:959)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:907)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:137)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:828)
        ... 19 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:61)
        at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:50)
        at org.apache.hadoop.hive.ql.exec.ObjectCacheWrapper.retrieve(ObjectCacheWrapper.java:40)
        at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:123)
        ... 27 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:59)
        ... 30 more
{noformat}"
HIVE-17835,HS2 Logs print unnecessary stack trace when HoS query is cancelled,"Example:

{code}
2017-10-05 17:47:11,881 ERROR org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor: [HiveServer2-Background-Pool: Thread-131]: Failed to monitor Job[ 2] with exception 'java.lang.InterruptedException(sleep interrupted)'
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:124)
	at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:111)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:99)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2052)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1748)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1501)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1285)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1280)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236)
	at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:89)
	at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:301)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
	at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:314)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2017-10-05 17:47:11,881 WARN  org.apache.hadoop.hive.ql.Driver: [HiveServer2-Handler-Pool: Thread-105]: Shutting down task : Stage-2:MAPRED
2017-10-05 17:47:11,882 ERROR org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor: [HiveServer2-Background-Pool: Thread-131]: Failed to monitor Job[ 2] with exception 'java.lang.InterruptedException(sleep interrupted)'
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:124)
	at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:111)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:99)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2052)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1748)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1501)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1285)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1280)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:236)
	at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:89)
	at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:301)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
	at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:314)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}"
HIVE-17735,ObjectStore.addNotificationEvent is leaking queries,"In ObjectStore.addNotificationEvent():

{code}
      Query objectQuery = pm.newQuery(MNotificationNextId.class);
      Collection<MNotificationNextId> ids = (Collection) objectQuery.execute();
{code}

The query is never closed."
HIVE-17684,HoS memory issues with MapJoinMemoryExhaustionHandler,"We have seen a number of memory issues due the {{HashSinkOperator}} use of the {{MapJoinMemoryExhaustionHandler}}. This handler is meant to detect scenarios where the small table is taking too much space in memory, in which case a {{MapJoinMemoryExhaustionError}} is thrown.

The configs to control this logic are:

{{hive.mapjoin.localtask.max.memory.usage}} (default 0.90)
{{hive.mapjoin.followby.gby.localtask.max.memory.usage}} (default 0.55)

The handler works by using the {{MemoryMXBean}} and uses the following logic to estimate how much memory the {{HashMap}} is consuming: {{MemoryMXBean#getHeapMemoryUsage().getUsed() / MemoryMXBean#getHeapMemoryUsage().getMax()}}

The issue is that {{MemoryMXBean#getHeapMemoryUsage().getUsed()}} can be inaccurate. The value returned by this method returns all reachable and unreachable memory on the heap, so there may be a bunch of garbage data, and the JVM just hasn't taken the time to reclaim it all. This can lead to intermittent failures of this check even though a simple GC would have reclaimed enough space for the process to continue working.

We should re-think the usage of {{MapJoinMemoryExhaustionHandler}} for HoS. In Hive-on-MR this probably made sense to use because every Hive task was run in a dedicated container, so a Hive Task could assume it created most of the data on the heap. However, in Hive-on-Spark there can be multiple Hive Tasks running in a single executor, each doing different things."
HIVE-17535,Select 1 EXCEPT Select 1 fails with NPE,Since Hive CBO isn't able to handle queries with no table e.g. {{select 1}} queries with SET operators fail (intersect requires CBO).
HIVE-17496,Bootstrap repl is not cleaning up staging dirs,This will put more pressure on the HDFS file limit.
HIVE-17479,Staging directories do not get cleaned up for update/delete queries,"When these queries are internally rewritten, a new context is created with a new execution id. This id is used to create the scratch directories. However, only the original context is cleared, and thus the directories created with the original execution id.
The solution is to pass the execution id to the new context when the queries are internally rewritten."
HIVE-17344,LocalCache element memory usage is not calculated properly.,"Orc footer cache has a calculation of memory usage:
{code:java}
public int getMemoryUsage() {
  return bb.remaining() + 100; // 100 is for 2 longs, BB and java overheads (semi-arbitrary).
}
{code}

ByteBuffer.remaining returns the remaining space in the bytebuffer, thus allowing this cache have elements MAXWEIGHT/100 of arbitrary size. I think the correct solution would be bb.capacity."
HIVE-17311,Numeric overflow in the HiveConf,"multiplierFor() method contains a typo, which causes wrong parsing of the rare suffixes ('tb' & 'pb').
"
HIVE-17272,"when hive.vectorized.execution.enabled is true, query on empty partitioned table fails with NPE","{noformat}
set hive.vectorized.execution.enabled=true;
CREATE TABLE `tab`(`x` int) PARTITIONED BY ( `y` int) stored as parquet;
select * from tab t1 join tab t2 where t1.x=t2.x;
{noformat}

The query fails with the following exception.
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.createAndInitPartitionContext(VectorMapOperator.java:386) ~[hive-exec-2.3.0.jar:2.3.0]
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.internalSetChildren(VectorMapOperator.java:559) ~[hive-exec-2.3.0.jar:2.3.0]
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.setChildren(VectorMapOperator.java:474) ~[hive-exec-2.3.0.jar:2.3.0]
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:106) ~[hive-exec-2.3.0.jar:2.3.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_101]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_101]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_101]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_101]
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_101]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_101]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_101]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_101]
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:413) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:268) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_101]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_101]
        at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_101]
{noformat}"
HIVE-17237,HMS wastes 26.4% of memory due to dup strings in metastore.api.Partition.parameters,"I've analyzed a heap dump from a production Hive installation using jxray (www.jxray.com) It turns out that there are a lot of duplicate strings in memory, that waste 26.4% of the heap. Most of them come from HashMaps referenced by org.apache.hadoop.hive.metastore.api.Partition.parameters. Below is the relevant section of the jxray report.

Looking at Partition.java, I see that in the past somebody has already added code to intern keys and values in the parameters table when it's first set up. However, when more key-value pairs are added, they are not interned, and that probably explains the reason for all these duplicate strings. Also when a Partition instance is deserialized, no interning of parameters is currently done.

{code}
6. DUPLICATE STRINGS

Total strings: 3,273,557  Unique strings: 460,390  Duplicate values: 110,232  Overhead: 3,220,458K (26.4%)
....

===================================================

7. REFERENCE CHAINS FOR DUPLICATE STRINGS

  2,326,150K (19.1%), 597058 dup strings (36386 unique), 597058 dup backing arrays:
39949 of ""-1"", 39088 of ""true"", 28959 of ""8"", 20987 of ""1"", 18437 of ""10"", 9583 of ""9"", 5908 of ""269664"", 5691 of ""174528"", 4598 of ""133980"", 4598 of ""BgUGBQgFCAYFCgYIBgUEBgQHBgUGCwYGBwYHBgkKBwYGBggIBwUHBgYGCgUJCQUG ...[length 3560]""
... and 419200 more strings, of which 36376 are unique
Also contains one-char strings: 217 of ""6"", 147 of ""7"", 91 of ""4"", 28 of ""5"", 28 of ""2"", 21 of ""0""
     <--  {j.u.HashMap}.values <-- org.apache.hadoop.hive.metastore.api.Partition.parameters <--  {j.u.ArrayList} <-- org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.success <-- Java Local (org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result) [@6e33618d8,@6eedb9a80,@6eedbad68,@6eedbc788] ... and 3 more GC roots
  463,060K (3.8%), 119644 dup strings (34075 unique), 119644 dup backing arrays:
7914 of ""true"", 7912 of ""-1"", 6578 of ""8"", 5606 of ""1"", 2302 of ""10"", 1626 of ""174528"", 1223 of ""9"", 970 of ""171680"", 837 of ""269664"", 657 of ""133980""
... and 84009 more strings, of which 34065 are unique
Also contains one-char strings: 42 of ""7"", 31 of ""6"", 20 of ""4"", 8 of ""5"", 5 of ""2"", 3 of ""0""
     <--  {j.u.HashMap}.values <-- org.apache.hadoop.hive.metastore.api.Partition.parameters <--  {j.u.TreeMap}.values <-- Java Local (j.u.TreeMap) [@6f084afa0,@73aac9e68]
  233,384K (1.9%), 64601 dup strings (27295 unique), 64601 dup backing arrays:
4472 of ""true"", 4173 of ""-1"", 3798 of ""1"", 3591 of ""8"", 813 of ""174528"", 684 of ""10"" ... and 44568 more strings, of which 27285 are unique
Also contains one-char strings: 305 of ""7"", 301 of ""0"", 277 of ""4"", 146 of ""6"", 29 of ""2"", 23 of ""5"", 19 of ""9"", 2 of ""3""
     <--  {j.u.HashMap}.values <-- org.apache.hadoop.hive.metastore.api.Partition.parameters <--  {j.u.ArrayList} <-- Java Local (j.u.ArrayList) [@4f4cfbd10,@536122408,@726616778]
...
  52,916K (0.4%), 597058 dup strings (16 unique), 597058 dup backing arrays:
     <--  {j.u.HashMap}.keys <-- org.apache.hadoop.hive.metastore.api.Partition.parameters <--  {j.u.ArrayList} <-- org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.success <-- Java Local (org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result) [@6e33618d8,@6eedb9a80,@6eedbad68,@6eedbc788] ... and 3 more GC roots
{code}"
HIVE-17222,Llap: Iotrace throws  java.lang.UnsupportedOperationException with IncompleteCb,"branch: hive master 
Running Q76 at 1 TB generates the following exception.

{noformat}

Caused by: java.io.IOException: java.lang.UnsupportedOperationException
        at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.rethrowErrorIfAny(LlapRecordReader.java:349)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.nextCvb(LlapRecordReader.java:304)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.next(LlapRecordReader.java:244)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.next(LlapRecordReader.java:67)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:360)
        ... 23 more
Caused by: java.lang.UnsupportedOperationException
        at org.apache.hadoop.hive.common.io.DiskRange.getData(DiskRange.java:86)
        at org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace.logRange(IoTrace.java:304)
        at org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace.logRanges(IoTrace.java:291)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:328)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:426)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:250)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:247)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:247)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:96)
        ... 6 more        
{noformat}

When {{IncompleteCb}} is encountered, it ends up throwing this error."
HIVE-17188,ObjectStore runs out of memory for large batches of addPartitions().,"For large batches (e.g. hundreds) of {{addPartitions()}}, the {{ObjectStore}} runs out of memory. Flushing the {{PersistenceManager}} alleviates the problem.

Note: The problem being addressed here isn't so much with the size of the hundreds of Partition objects, but the cruft that builds with the PersistenceManager, in the JDO layer, as confirmed through memory-profiling.

(Raising this on behalf of [~cdrome] and [~thiruvel].)"
HIVE-17128,Operation Logging leaks file descriptors as the log4j Appender is never closed,"[HIVE-16061] and [HIVE-16400] changed operation logging to use the Log4j2 RoutingAppender to automatically output the log for each query into each individual operation log file. As log4j does not know when a query is finished it keeps the OutputStream in the Appender open even when the query completes. The stream holds a file descriptor and so we leak file descriptors. Note that we are already careful to close any streams reading from the operation log file.

h2. Fix

To fix this we use a technique described in the comments of [LOG4J2-510] which uses reflection to close the appender. The test in TestOperationLoggingLayout will be extended to check that the Appender is closed."
HIVE-17098,Race condition in Hbase tables,"These steps simulate our customer production env.

*STEP 1. Create test tables*

{code}
CREATE TABLE for_loading(
  key int, 
  value string,
  age int,
  salary decimal (10,2)
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
{code}

Table {{test_1}} belongs to user {{testuser1}}.

{code}
CREATE TABLE test_1(
  key int, 
  value string,
  age int,
  salary decimal (10,2)
)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key, cf1:value, cf1:age, cf1:salary', 
  'serialization.format'='1')
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='{\""BASIC_STATS\"":\""true\""}', 
  'hbase.table.name'='test_1', 
  'numFiles'='0', 
  'numRows'='0', 
  'rawDataSize'='0', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1495769316');
{code}

Table {{test_2}} belongs to user {{testuser2}}.

{code}
CREATE TABLE test_2(
  key int, 
  value string,
  age int,
  salary decimal (10,2)
)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key, cf1:value, cf1:age, cf1:salary', 
  'serialization.format'='1')
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='{\""BASIC_STATS\"":\""true\""}', 
  'hbase.table.name'='test_2', 
  'numFiles'='0', 
  'numRows'='0', 
  'rawDataSize'='0', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1495769316');
{code}


*STEP 2. Create test data*

{code}
import java.io.IOException;
import java.math.BigDecimal;
import java.nio.charset.Charset;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Random;

import static java.lang.String.format;

public class Generator {
    private static List<String> lines = new ArrayList<>();
    private static List<String> name = Arrays.asList(""Brian"", ""John"", ""Rodger"", ""Max"", ""Freddie"", ""Albert"", ""Fedor"", ""Lev"", ""Niccolo"");
    private static List<BigDecimal> salary = new ArrayList<>();

    public static void main(String[] args) {
        generateData(Integer.parseInt(args[0]), args[1]);
    }

    public static void generateData(int rowNumber, String file) {

        double maxValue = 20000.55;
        double minValue = 1000.03;

        Random random = new Random();
        for (int i = 1; i <= rowNumber; i++) {
            lines.add(
                i + "","" +
                    name.get(random.nextInt(name.size())) + "","" +
                    (random.nextInt(62) + 18) + "","" +
                    format(""%.2f"", (minValue + (maxValue - minValue) * random.nextDouble())));
        }

        Path path = Paths.get(file);

        try {
            Files.write(path, lines, Charset.forName(""UTF-8""), StandardOpenOption.APPEND);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
{code}

{code}
javac Generator.java
java Generator 3000000 dataset.csv
hadoop fs -put dataset.csv /
{code}


*STEP 3. Upload test data*

{code}
load data local inpath '/home/myuser/dataset.csv' into table for_loading;
{code}

{code}
from for_loading
insert into table test_1
select key,value,age,salary;
{code}

{code}
from for_loading
insert into table test_2
select key,value,age,salary;
{code}

*STEP 4. Run test queries*

Run in 5 parallel terminals for table {{test_1}}

{code}
for i in {1..500}; do beeline -u ""jdbc:hive2://localhost:10000/default testuser1"" -e ""select * from test_1 limit 10;"" 1>/dev/null; done
{code}


Run in 5 parallel terminals for table {{test_2}}

{code}
for i in {1..500}; do beeline -u ""jdbc:hive2://localhost:10000/default testuser2"" -e ""select * from test_2 limit 10;"" 1>/dev/null; done
{code}

*EXPECTED RESULT:*

All queris are OK.




*ACTUAL RESULT*


{code}
org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.IllegalStateException: The input format instance has not been properly ini
tialized. Ensure you call initializeTable either in your constructor or initialize method
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:484)
        at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:308)
        at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:847)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1595)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
        at com.sun.proxy.$Proxy25.fetchResults(Unknown Source)
        at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:504)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:698)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1717)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1702)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:521)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:428)
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146)
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2099)
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:479)
        ... 24 more
Caused by: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method
        at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getRegionLocator(TableInputFormatBase.java:579)
        at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getStartEndKeys(TableInputFormatBase.java:225)
        at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:261)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplitsInternal(HiveHBaseTableInputFormat.java:525)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:452)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:372)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:304)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:459)
        ... 28 more
{code}
"
HIVE-17010,Fix the overflow problem of Long type in SetSparkReducerParallelism,"We use [numberOfBytes|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java#L129] to collect the numberOfBytes of sibling of specified RS. We use Long type and it happens overflow when the data is too big. After happening this situation, the parallelism is decided by [sparkMemoryAndCores.getSecond()|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java#L184] if spark.dynamic.allocation.enabled is true, sparkMemoryAndCores.getSecond is a dymamic value which is decided by spark runtime. For example, the value of sparkMemoryAndCores.getSecond is 5 or 15 randomly. There is possibility that the value may be 1. The may problem here is the overflow of addition of Long type.  You can reproduce the overflow problem by following code
{code}
    public static void main(String[] args) {
      long a1= 9223372036854775807L;
      long a2=1022672;

      long res = a1+a2;
      System.out.println(res);  //-9223372036853753137

      BigInteger b1= BigInteger.valueOf(a1);
      BigInteger b2 = BigInteger.valueOf(a2);

      BigInteger bigRes = b1.add(b2);

      System.out.println(bigRes); //9223372036855798479

    }
{code}"
HIVE-17007,NPE introduced by HIVE-16871,"Stack:
{code}
2017-06-30T02:39:43,739 ERROR [HiveServer2-Background-Pool: Thread-2873]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(200)) - MetaException(message:java.lang.NullPointerException)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6066)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3993)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3944)
        at sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy32.alter_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:397)
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:325)
        at sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
        at com.sun.proxy.$Proxy33.alter_table_with_environmentContext(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2306)
        at com.sun.proxy.$Proxy33.alter_table_with_environmentContext(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:624)
        at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3490)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:383)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1905)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1607)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1354)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1123)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242)
        at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:348)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.cache.SharedCache.getCachedTableColStats(SharedCache.java:140)
        at org.apache.hadoop.hive.metastore.cache.CachedStore.getTableColumnStatistics(CachedStore.java:1409)
        at sun.reflect.GeneratedMethodAccessor165.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
        at com.sun.proxy.$Proxy28.getTableColumnStatistics(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:800)
        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:257)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3973)
        ... 43 more
{code}"
HIVE-16961,Hive on Spark leaks spark application in case user cancels query and closes session,"It's found that a Spark application is leaked when user cancels query and closes the session while Hive is waiting for remote driver to connect back. This is found for asynchronous query execution, but seemingly equally applicable for synchronous submission when session is abruptly closed. The leaked Spark application that runs Spark driver connects back to Hive successfully and run for ever (until HS2 restarts), but receives no job submission because the session is already closed. Ideally, Hive should rejects the connection from the driver so the driver will exist."
HIVE-16949,Leak of threads from Get-Input-Paths and Get-Input-Summary thread pool,"The commit [20210de|https://github.com/apache/hive/commit/20210dec94148c9b529132b1545df3dd7be083c3] which was part of HIVE-15546 [introduced a thread pool|https://github.com/apache/hive/blob/824b9c80b443dc4e2b9ad35214a23ac756e75234/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L3109] which is not shutdown upon completion of its threads. This leads to a leak of threads for each query which uses more than 1 partition. They are not removed automatically. When queries spanning multiple partitions are made the number of threads increases and is never reduced. On my machine hiveserver2 starts to get slower and slower once 10k threads are reached.

Thread pools only shutdown automatically in special circumstances (see [documentation section _Finalization_|https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html]). This is not currently the case for the Get-Input-Paths thread pool. I would add a _pool.shutdown()_ in a finally block just before returning the result to make sure the threads are really shutdown.

My current workaround is to set {{hive.exec.input.listing.max.threads = 1}}. This prevents the the thread pool from being spawned [\[1\]|https://github.com/apache/hive/blob/824b9c80b443dc4e2b9ad35214a23ac756e75234/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L2118] [\[2\]|https://github.com/apache/hive/blob/824b9c80b443dc4e2b9ad35214a23ac756e75234/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L3107].

The same issue probably also applies to the [Get-Input-Summary thread pool|https://github.com/apache/hive/blob/824b9c80b443dc4e2b9ad35214a23ac756e75234/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L2193]."
HIVE-16933,ORA-00060: deadlock detected while waiting on commit,"When running transactional workload (esp streaming ingest api) with Oracle backed Hive metastore it's possible to see Deadlock exceptions from the DB.

This due to lack of indexes on Foreign Key columns of Acid related metastore tables.
For example, TXN_COMPONENTS references TXNS.  It should have

CREATE INDEX TC_TXNID_INDEX ON TXN_COMPONENTS (TC_TXNID);

{noformat}
2017-06-20 13:42:00,687 ERROR [pool-3-thread-182]: txn.TxnHandler (TxnHandler.java:checkRetryable(1952)) - Too many repeated deadlocks in commitTxn(CommitTxnRequest(txnid:293)), giving up.
2017-06-20 13:42:00,696 ERROR [pool-3-thread-182]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(195)) - MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-00060: deadlock detected while waiting for resource

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)
        at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1822)
        at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1787)
        at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:280)
        at org.apache.commons.dbcp.DelegatingStatement.executeUpdate(DelegatingStatement.java:228)
        at org.apache.commons.dbcp.DelegatingStatement.executeUpdate(DelegatingStatement.java:228)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn(TxnHandler.java:756)
{noformat}"
HIVE-16877,NPE when issue query like alter table ... cascade onto non-partitioned table ,"After HIVE-8839 in 1.1.0 support ""alter table ... cascade"" to cascade table changes to partitions as well.  But NPE thrown when issue query like ""alter table ... cascade"" onto non-partitioned table 

Sample Query:
{code}
create table test_cascade_npe (id int);
alter table test_cascade_npe add columns (name string ) cascade;
{code}

Exception stack:
{code}
2017-06-09T22:16:05,913 ERROR [main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
    at org.apache.hadoop.hive.metastore.Warehouse.makePartName(Warehouse.java:547)
    at org.apache.hadoop.hive.metastore.Warehouse.makePartName(Warehouse.java:489)
    at org.apache.hadoop.hive.ql.metadata.Partition.getName(Partition.java:198)
    at org.apache.hadoop.hive.ql.hooks.Entity.computeName(Entity.java:339)
    at org.apache.hadoop.hive.ql.hooks.Entity.<init>(Entity.java:208)
    at org.apache.hadoop.hive.ql.hooks.WriteEntity.<init>(WriteEntity.java:104)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1496)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1473)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableModifyCols(DDLSemanticAnalyzer.java:2685)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:284)
    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:474)
    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1245)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1387)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1174)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1164)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)

{code}
"
HIVE-16848,NPE during CachedStore refresh,"CachedStore refresh only happen once due to NPE during refresh of column statistics. ScheduledExecutorService canceled subsequent refreshes:

{code}
java.lang.NullPointerException
    at org.apache.hadoop.hive.metastore.cache.CachedStore$CacheUpdateMasterWork.updateTableColStats(CachedStore.java:458)
    at org.apache.hadoop.hive.metastore.cache.CachedStore$CacheUpdateMasterWork.run(CachedStore.java:348)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:748)
{code}"
HIVE-16845,INSERT OVERWRITE a table with dynamic partitions on S3 fails with NPE,"*How to reproduce*
- Create a partitioned table on S3:
{noformat}
CREATE EXTERNAL TABLE s3table(user_id string COMMENT '', event_name string COMMENT '') PARTITIONED BY (reported_date string, product_id int) LOCATION 's3a://<bucket name>'; 
{noformat}
- Create a temp table:
{noformat}
create table tmp_table (id string, name string, date string, pid int) row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;
{noformat}
- Load the following rows to the tmp table:
{noformat}
u1	value1	2017-04-10	10000
u2	value2	2017-04-10	10000
u3	value3	2017-04-10	10001
{noformat}
- Set the following parameters:
-- hive.exec.dynamic.partition.mode=nonstrict
-- mapreduce.input.fileinputformat.split.maxsize=10
-- hive.blobstore.optimizations.enabled=true
-- hive.blobstore.use.blobstore.as.scratchdir=false
-- hive.merge.mapfiles=true
- Insert the rows from the temp table into the s3 table:
{noformat}
INSERT OVERWRITE TABLE s3table
PARTITION (reported_date, product_id)
SELECT
  t.id as user_id,
  t.name as event_name,
  t.date as reported_date,
  t.pid as product_id
FROM tmp_table t;
{noformat}

A NPE will occur with the following stacktrace:
{noformat}
2017-05-08 21:32:50,607 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-184028]: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null
at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)
at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)
at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)
at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)
at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)
at org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1201)
at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)
... 11 more 
{noformat}"
HIVE-16844,Fix Connection leak in ObjectStore when new Conf object is used,"The code path in ObjectStore.java currently leaks BoneCP (or Hikari) connection pools when a new configuration object is passed in. The code needs to ensure that the persistence-factory is closed before it is nullified.

The relevant code is [here|https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L290]. Note that pmf is set to null, but the underlying connection pool is not closed."
HIVE-16831,Add unit tests for NPE fixes in HIVE-12054,"HIVE-12054 fixed NPE issues related to ObjectInspector which get triggered when an empty ORC table/partition is read.

This work adds tests that trigger that path."
HIVE-16788,ODBC call SQLForeignKeys leads to NPE if you use PK arguments rather than FK arguments,"This ODBC call is meant to allow you to determine FK relationships either from the PK side or from the FK side.

Hive only allows you to traverse from the FK side, trying it from the PK side leads to an NPE.

Example using the table ""customer"" from TPC-H with FKs defined in Hive:

{code}
=== Foreign Keys ===
Using table as foreign source
(u'HIVE', u'tpch_bin_flat_orc_2', u'nation', u'n_nationkey', u'HIVE', u'tpch_bin_flat_orc_2', u'customer', u'c_nationkey', 1, 0, 0, u'custome
r_c2', u'nation_c1', 0)
Not using table as foreign source
Got an error from the server for customer!
{code}

Compare: Postgres
{code}
=== Foreign Keys ===
Using table as foreign source
(u'vagrant', u'public', u'nation', u'n_nationkey', u'vagrant', u'public', u'customer', u'c_nationkey', 1, 3, 3, u'customer_c_nationkey_fkey', u'nation_pkey', 7)
Not using table as foreign source
(u'vagrant', u'public', u'customer', u'c_custkey', u'vagrant', u'public', u'orders', u'o_custkey', 1, 3, 3, u'orders_o_custkey_fkey', u'customer_pkey', 7)
{code}

Note that Postgres allows traversal from either way. The traceback you get in the HS2 logs is this:

{code}
2016-12-04T21:08:55,398 ERROR [8998ca98-9940-49f8-8833-7c6ebd8c96a2 HiveServer2-Handler-Pool: Thread-53] metastore.RetryingHMSHandler: MetaEx
ception(message:java.lang.NullPointerException)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5785)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_foreign_keys(HiveMetaStore.java:6474)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
        at com.sun.proxy.$Proxy25.get_foreign_keys(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getForeignKeys(HiveMetaStoreClient.java:1596)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
        at com.sun.proxy.$Proxy26.getForeignKeys(Unknown Source)
        at org.apache.hive.service.cli.operation.GetCrossReferenceOperation.runInternal(GetCrossReferenceOperation.java:128)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:324)
        at org.apache.hive.service.cli.session.HiveSessionImpl.getCrossReference(HiveSessionImpl.java:933)
        at org.apache.hive.service.cli.CLIService.getCrossReference(CLIService.java:411)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetCrossReference(ThriftCLIService.java:738)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetCrossReference.getResult(TCLIService.java:1617)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetCrossReference.getResult(TCLIService.java:1602)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.apache.hive.common.util.HiveStringUtils.normalizeIdentifier(HiveStringUtils.java:919)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.<init>(ObjectStore.java:2722)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetListHelper.<init>(ObjectStore.java:2863)
        at org.apache.hadoop.hive.metastore.ObjectStore$11.<init>(ObjectStore.java:8455)
        at org.apache.hadoop.hive.metastore.ObjectStore.getForeignKeysInternal(ObjectStore.java:8455)
        at org.apache.hadoop.hive.metastore.ObjectStore.getForeignKeys(ObjectStore.java:8445)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
        at com.sun.proxy.$Proxy24.getForeignKeys(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_foreign_keys(HiveMetaStore.java:6465)
        ... 28 more
{code}"
HIVE-16779,CachedStore leak PersistenceManager resources,See OOM when running CachedStore. We didn't shutdown rawstore in refresh thread.
HIVE-16765,ParquetFileReader should be closed to avoid resource leak,ParquetFileReader should be closed to avoid resource leak
HIVE-16737,LLAP: Shuffle handler TCP listen queue overflows,"{code}
$ netstat -s | grep ""listen queue of a socket""
localhost:     297070 times the listen queue of a socket overflowed
{code}

{code}
$ ss -tl
LISTEN     0      50         *:15551                    *:*
{code}"
HIVE-16692,LLAP: Keep alive connection in shuffle handler should not be closed until entire data is flushed out,"In corner cases with keep-alive enabled, it is possible that the headers are written out in the response and downstream was able to read the headers.  

But possible that the mapOutput construction took a lot longer time (due to disk or any other issue) in server side. In the mean time, keep alive timeout can kick in and close the connection from server side. In such cases, there is a possibility that downstream can get ""connection reset"". Ideally keep alive should kick in only after flushing entire response downstream.

e.g error msg in client side
{noformat}
java.net.SocketException: Connection reset
        at java.net.SocketInputStream.read(SocketInputStream.java:209) ~[?:1.8.0_112]
        at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[?:1.8.0_112]
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[?:1.8.0_112]
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[?:1.8.0_112]
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_112]
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[?:1.8.0_112]
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[?:1.8.0_112]
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:675) ~[?:1.8.0_112]
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569) ~[?:1.8.0_112]
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474) ~[?:1.8.0_112]
        at org.apache.tez.http.HttpConnection.getInputStream(HttpConnection.java:260) ~[tez-runtime-library-0.8.4.2.6.1.0-11.jar:0.8.4.2.6.1.0-11]
        at org.apache.tez.runtime.library.common.shuffle.Fetcher.setupConnection(Fetcher.java:460) ~[tez-runtime-library-0.8.4.2.6.1.0-11.jar:0.8.4.2.6.1.0-11]
        at org.apache.tez.runtime.library.common.shuffle.Fetcher.doHttpFetch(Fetcher.java:492) ~[tez-runtime-library-0.8.4.2.6.1.0-11.jar:0.8.4.2.6.1.0-11]
        at org.apache.tez.runtime.library.common.shuffle.Fetcher.doHttpFetch(Fetcher.java:417) ~[tez-runtime-library-0.8.4.2.6.1.0-11.jar:0.8.4.2.6.1.0-11]
        at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:215) ~[tez-runtime-library-0.8.4.2.6.1.0-11.jar:0.8.4.2.6.1.0-11]
        at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:73) ~[tez-runtime-library-0.8.4.2.6.1.0-11.jar:0.8.4.2.6.1.0-11]
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.8.4.2.6.1.0-11.jar:0.8.4.2.6.1.0-11]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
{noformat}

This corner case handling was not pulled in earlier from MR handler fixes."
HIVE-16665,Race condition in Utilities.GetInputPathsCallable --> createDummyFileForEmptyPartition,Looks like there is a race condition in the {{GetInputPathsCallable}} thread when modifying the input {{MapWork}} object.
HIVE-16651,LlapProtocolClientProxy stack trace when using llap input format,"Seeing this after LlapBaseRecordReader.close():

{noformat}
16/06/28 22:05:32 WARN LlapProtocolClientProxy: RequestManager shutdown with error
java.util.concurrent.CancellationException
	at java.util.concurrent.FutureTask.report(FutureTask.java:121)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.util.concurrent.Futures$4.run(Futures.java:1170)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
	at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
	at com.google.common.util.concurrent.ListenableFutureTask.done(ListenableFutureTask.java:91)
	at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:384)
	at java.util.concurrent.FutureTask.cancel(FutureTask.java:180)
	at org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.serviceStop(LlapProtocolClientProxy.java:131)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	at org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.serviceStop(LlapTaskUmbilicalExternalClient.java:135)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	at org.apache.hadoop.service.AbstractService.close(AbstractService.java:250)
	at org.apache.hadoop.hive.llap.LlapBaseRecordReader.close(LlapBaseRecordReader.java:84)
	at org.apache.hadoop.hive.llap.LlapRowRecordReader.close(LlapRowRecordReader.java:80)
{noformat}"
HIVE-16599,NPE in runtime filtering cost when handling SMB Joins,A test with SMB joins failed with NPE in runtime filtering costing logic.
HIVE-16588,Resource leak by druid http client,"Current implementation of druid storage handler does leak some resources if the creation of the http client fails due to too many files exception.
The reason this is leaking is the fact the cleaning hook is registered after the client starts.
In order to fix this will extract the creation of the HTTP client to become static and reusable instead of per query creation.
 "
HIVE-16587,NPE when inserting complex types with nested null values,"
{noformat}
CREATE TABLE complex1 (c0 int, c1 array<int>, c2 map<int, string>, c3 struct<f1:int, f2:string, f3:array<int>>, c4 array<struct<f1:int, f2:string, f3:array<int>>>)

insert into complex1
 select 3, array(1, 2, null), map(1, 'one', 2, null), named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1,2,null)), array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2,3,4)))
{noformat}

Gives the following error:
{noformat}
Caused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException null
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207)
	at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:291)
	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:531)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:517)
	at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
	at com.sun.proxy.$Proxy126.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:310)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:530)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1437)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1422)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException: null
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getWritableSize(StatsUtils.java:1144)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfMap(StatsUtils.java:1106)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(StatsUtils.java:978)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOf(StatsUtils.java:916)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExpression(StatsUtils.java:1371)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExprMap(StatsUtils.java:1194)
	at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$SelectStatsRule.process(StatsRulesProcFactory.java:187)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
	at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
	at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
	at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:343)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:102)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11382)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:293)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:551)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1371)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1345)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:205)
	... 26 more
{noformat}"
HIVE-16546,LLAP: Fail map join tasks if hash table memory exceeds threshold,"When map join task is running in llap, it can potentially use lot more memory than its limit which could be memory per executor or no conditional task size. If it uses more memory, it can adversely affect other query performance or it can even bring down the daemon. In such cases, it is better to fail the query than to bring down the daemon. "
HIVE-16503,LLAP: Oversubscribe memory for noconditional task size,"When running map joins in llap, it can potentially use more memory for hash table loading (assuming other executors in the daemons have some memory to spare). This map join conversion decision has to be made during compilation that can provide some more room for LLAP. "
HIVE-16497,"FileUtils. isActionPermittedForFileHierarchy, isOwnerOfFileHierarchy file system operations should be impersonated","FileUtils.isActionPermittedForFileHierarchy checks if user has permissions for given action. The checks are made by impersonating the user.
However, the listing of child dirs are done as the hiveserver2 user. If the hive user doesn't have permissions on the filesystem, it gives incorrect error that the user doesn't have permissions to perform the action.
Impersonating the end user for all file operations in that function is also logically correct thing to do.
"
HIVE-16487,Serious Zookeeper exception is logged when a race condition happens,"A customer started to see this in the logs, but happily everything was working as intended:
{code}
2017-03-30 12:01:59,446 ERROR ZooKeeperHiveLockManager: [HiveServer2-Background-Pool: Thread-620]: Serious Zookeeper exception: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hive_zookeeper_namespace/<TABLE_NAME>/LOCK-SHARED-
{code}
This was happening, because a race condition between the lock releasing, and lock acquiring. The thread releasing the lock removes the parent ZK node just after the thread acquiring the lock made sure, that the parent node exists.

Since this can happen without any real problem, I plan to add NODEEXISTS, and NONODE as a transient ZooKeeper exception, so the users are not confused.

Also, the original author of ZooKeeperHiveLockManager maybe planned to handle different ZooKeeperExceptions differently, and the code is hard to understand. See the {{continue}} and the {{break}}. The {{break}} only breaks the switch, and not the loop which IMHO is not intuitive:
{code}
    do {
      try {
[..]
        ret = lockPrimitive(key, mode, keepAlive, parentCreated, 
      } catch (Exception e1) {
        if (e1 instanceof KeeperException) {
          KeeperException e = (KeeperException) e1;
          switch (e.code()) {
          case CONNECTIONLOSS:
          case OPERATIONTIMEOUT:
            LOG.debug(""Possibly transient ZooKeeper exception: "", e);
            continue;
          default:
            LOG.error(""Serious Zookeeper exception: "", e);
            break;
          }
        }
[..]
      }
    } while (tryNum < numRetriesForLock);
{code}

If we do not want to try again in case of a ""Serious Zookeeper exception:"", then we should add a label to the do loop, and break it in the switch.

If we do want to try regardless of the type of the ZK exception, then we should just change the {{continue;}} to {{break;}} and move the lines part of the code which did not run in case of {{continue}} to the {{default}} switch, so it is easier to understand the code.

Any suggestions or ideas [~ctang.ma] or [~szehon]?"
HIVE-16462,Vectorization: Enabling hybrid grace disables specialization of all reduce side joins,"Observed by [~gopalv].

Having grace hash join enabled prevents the specialized vector hash joins during the vectorizer stage of query planning. However hive.llap.enable.grace.join.in.llap will later disable grace hash join (LlapDecider runs after Vectorizer). If we can disable the grace hash join before vectorization kicks in then we can still benefit from the specialized vector hash joins.

This can be special cased for the llap.execution.mode=only case."
HIVE-16451,Race condition between HiveStatement.getQueryLog and HiveStatement.runAsyncOnServer,"During the BeeLineDriver testing I have met the following race condition:
- Run the query asynchronously through BeeLine
- Querying the logs in the BeeLine

In the following code:
{code:title=HiveStatement.runAsyncOnServer}
  private void runAsyncOnServer(String sql) throws SQLException {
    checkConnection(""execute"");

    closeClientOperation();
    initFlags();
[..]
  }
{code}

{code:title=HiveStatement.getQueryLog}
  public List<String> getQueryLog(boolean incremental, int fetchSize)
      throws SQLException, ClosedOrCancelledStatementException {
[..]
    try {
      if (stmtHandle != null) {
[..]
      } else {
        if (isQueryClosed) {
          throw new ClosedOrCancelledStatementException(""Method getQueryLog() failed. The "" +
              ""statement has been closed or cancelled."");
        } else {
          return logs;
        }
      }
    } catch (SQLException e) {
[..]
    }
[..]
  }
{code}

The runAsyncOnServer {{closeClientOperation}} sets {{isQueryClosed}} flag to true:
{code:title=HiveStatement.closeClientOperation}
  void closeClientOperation() throws SQLException {
[..]
    isQueryClosed = true;
    isExecuteStatementFailed = false;
    stmtHandle = null;
  }
{code}

The {{initFlags}} sets it to false:
{code}
  private void initFlags() {
    isCancelled = false;
    isQueryClosed = false;
    isLogBeingGenerated = true;
    isExecuteStatementFailed = false;
    isOperationComplete = false;
  }
{code}

If the {{getQueryLog}} is called after the {{closeClientOperation}}, but before the {{initFlags}}, then we will have a following warning if verbose mode is set to true in BeeLine:
{code}
Warning: org.apache.hive.jdbc.ClosedOrCancelledStatementException: Method getQueryLog() failed. The statement has been closed or cancelled. (state=,code=0)
{code}

This caused this fail:
https://builds.apache.org/job/PreCommit-HIVE-Build/4691/testReport/org.apache.hadoop.hive.cli/TestBeeLineDriver/testCliDriver_smb_mapjoin_11_/
{code}
Error Message

Client result comparison failed with error code = 1 while executing fname=smb_mapjoin_11
16a17
> Warning: org.apache.hive.jdbc.ClosedOrCancelledStatementException: Method getQueryLog() failed. The statement has been closed or cancelled. (state=,code=0)
{code}"
HIVE-16433,"Not nullify variable ""rj"" to avoid NPE due to race condition in ExecDriver.","Not nullify variable {{rj}} to avoid NPE due to race condition in ExecDriver. currently  {{rj}} is set to null in ExecDriver.shutdown which is called from other thread for query cancellation. It can happen at any time. There is a potential race condition,  the {{rj}} is still accessed after shutdown is called. For example: if the following code is executed right after ExecDriver.shutdown is called.
{code}
      this.jobID = rj.getJobID();
      updateStatusInQueryDisplay();
      returnVal = jobExecHelper.progress(rj, jc, ctx);
{code}
Currently the purpose of nullifying  {{rj}} is mainly to make sure {{rj.killJob()}} is only called once.
I will add a flag {{jobKilled}} to make sure {{rj.killJob()}} will be only called once."
HIVE-16343,LLAP: Publish YARN's ProcFs based memory usage to metrics for monitoring,Publish MemInfo from ProcfsBasedProcessTree to llap metrics. This will useful for monitoring and also setting up triggers via JMC. 
HIVE-16329,TopN: use local executor info for LLAP memory checks,"{code}
      // TODO: For LLAP, assumption is off-heap cache.
      final long memoryUsedPerExecutor = (memoryMXBean.getHeapMemoryUsage().getUsed() / numExecutors);
      // this is total free memory available per executor in case of LLAP
      totalFreeMemory = conf.getMaxMemoryAvailable() - memoryUsedPerExecutor;
{code}

{code}
exec.TopNHash: isTez parameters -615768144 = 5312782848 - 71142611912 / 12
{code}

This turns off the TopNHash entirely causing something trivial like 

{code}
select c_custkey, count(1) from customer group by c_custkey limit 10;
{code}

To shuffle 30M rows instead of 10."
HIVE-16323,HS2 JDOPersistenceManagerFactory.pmCache leaks after HIVE-14204,"Hive.loadDynamicPartitions creates threads with new embedded rawstore, but never close them, thus we leak PersistenceManager one per such thread."
HIVE-16321,Possible deadlock in metastore with Acid enabled,"TxnStore.MutexAPI is a mechanism how different Metastore instances can coordinate their operations.  It uses a JDBCConnection to achieve it.

In some cases this may lead to deadlock.  TxnHandler uses a connection pool of fixed size.  Suppose you have X simultaneous calls to  TxnHandler.lock(), where X is >= size of the pool.  This take all connections form the pool, so when
{noformat}
handle = getMutexAPI().acquireLock(MUTEX_KEY.CheckLock.name());
{noformat} 
is executed in _TxnHandler.checkLock(Connection dbConn, long extLockId)_ the pool is empty and the system is deadlocked.

MutexAPI can't use the same connection as the operation it's protecting.  (TxnHandler.checkLock(Connection dbConn, long extLockId) is an example).

We could make MutexAPI use a separate connection pool (size > 'primary' conn pool).

Or we could make TxnHandler.lock(LockRequest rqst) return immediately after enqueueing the lock with the expectation that the caller will always follow up with a call to checkLock(CheckLockRequest rqst).

cc [~f1sherox]

"
HIVE-16307,add IO memory usage report to LLAP UI,
HIVE-16305,Additional Datanucleus ClassLoaderResolverImpl leaks causing HS2 OOM,This is a followup for HIVE-16160. We see additional ClassLoaderResolverImpl leaks even with the patch.
HIVE-16233,llap: Query failed with AllocatorOutOfMemoryException,"{code}
TaskAttempt 5 failed, info=[Error: Error while running task ( failure ) : attempt_1488231257387_2288_25_05_000056_5:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 262144; at 0 out of 1
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 262144; at 0 out of 1
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:74)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:419)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
        ... 15 more
Caused by: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 262144; at 0 out of 1
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:365)
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62)
        ... 17 more
Caused by: java.io.IOException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 262144; at 0 out of 1
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:425)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:235)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:232)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:232)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:94)
        ... 6 more
Caused by: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 262144; at 0 out of 1
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:275)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedStream(EncodedReaderImpl.java:675)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:412)
        ... 14 more
{code}
Also reported by [~selinazh] while using older bits.


This JIRA will be used to track a long-term fix. 
We were initially planning to write a different allocator with compaction, but now we are stuck with buddyallocator.
There are two approaches - compaction on top of buddy allocator (should be fairly simple actually, aside from indirection... luckily for the latter, we can take the buffer out in most places and we can reuse the existing locking-to-prevent-eviction mechanism to also prevent reallocation); or writing a better allocator. Probably the former is preferred, need to think about the latter.
"
HIVE-16225,Memory leak in webhcat service (FileSystem CACHE entries),"This is a known beast. here are details

The problem seems to be similar to the one discussed in HIVE-13749. If we submit very large number of jobs like 1000 to 2000 then we can see increase in Configuration objects count."
HIVE-16213,ObjectStore can leak Queries when rollbackTransaction throws an exception,"In ObjectStore.java there are a few places with the code similar to:

{code}
    Query query = null;
    try {
      openTransaction();
      query = pm.newQuery(Something.class);
      ...
      commited = commitTransaction();
    } finally {
      if (!commited) {
        rollbackTransaction();
      }
      if (query != null) {
        query.closeAll();
      }
    }
{code}

The problem is that rollbackTransaction() may throw an exception in which case query.closeAll() wouldn't be executed. 

The fix would be to wrap rollbackTransaction in its own try-catch block."
HIVE-16116,Beeline throws NPE when beeline.hiveconfvariables={} in beeline.properties,"Env: hive master
Steps to reproduce:

1. clear previous beeline.properties (rm -rf ~/.beeline/beeline.properties)

2. Launch beeline, ""!save"" and exit. This would create new ""~/.beeline/beeline.properties"", which would have ""beeline.hiveconfvariables={}""

3. Launch ""beeline --hiveconf hive.tmp.dir=/tmp"". This would throw NPE

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hive.beeline.BeeLine.setHiveConfVar(BeeLine.java:885)
        at org.apache.hive.beeline.BeeLine.connectUsingArgs(BeeLine.java:832)
        at org.apache.hive.beeline.BeeLine.initArgs(BeeLine.java:775)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1009)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:519)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:501)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{noformat}"
HIVE-16079,HS2: high memory pressure due to duplicate Properties objects,"I've created a Hive table with 2000 partitions, each backed by two files, with one row in each file. When I execute some number of concurrent queries against this table, e.g. as follows

{code}
for i in `seq 1 50`; do beeline -u jdbc:hive2://localhost:10000 -n admin -p admin -e ""select count(i_f_1) from misha_table;"" & done
{code}

it results in a big memory spike. With 20 queries I caused an OOM in a HS2 server with -Xmx200m and with 50 queries - in the one with -Xmx500m.

I am attaching the results of jxray (www.jxray.com) analysis of a heap dump that was generated in the 50queries/500m heap scenario. It suggests that there are several opportunities to reduce memory pressure with not very invasive changes to the code. One (duplicate strings) has been addressed in https://issues.apache.org/jira/browse/HIVE-15882 In this ticket, I am going to address the fact that almost 20% of memory is used by instances of java.util.Properties. These objects are highly duplicate, since for each partition each concurrently running query creates its own copy of Partion, PartitionDesc and Properties. Thus we have nearly 100,000 (50 queries * 2,000 partitions) Properties in memory. By interning/deduplicating these objects we may be able to save perhaps 15% of memory.

Note, however, that if there are queries that mutate partitions, the corresponding Properties would be mutated as well. Thus we cannot simply use a single ""canonicalized"" Properties object at all times for all Partition objects representing the same DB partition. Instead, I am going to introduce a special CopyOnFirstWriteProperties class. Such an object initially internally references a canonicalized Properties object, and keeps doing so while only read methods are called. However, once any mutating method is called, the given CopyOnFirstWriteProperties copies the data into its own table from the canonicalized table, and uses it ever after."
HIVE-15956,StackOverflowError when drop lots of partitions,"Repro steps:
1. Create partitioned table and add 10000 partitions
{code}
create table test_partition(id int) partitioned by (dt int);

alter table test_partition add partition(dt=1);
alter table test_partition add partition(dt=3);
alter table test_partition add partition(dt=4);
...
alter table test_partition add partition(dt=10000);
{code}

2. Drop 9000 partitions:
{code}
alter table test_partition drop partition(dt<9000);
{code}

Step 2 will fail with StackOverflowError:
{code}
Exception in thread ""pool-7-thread-161"" java.lang.StackOverflowError
    at org.datanucleus.query.expression.ExpressionCompiler.isOperator(ExpressionCompiler.java:819)
    at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:190)
    at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)
    at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192)
    at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)
    at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192)
    at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)
{code}

{code}
Exception in thread ""pool-7-thread-198"" java.lang.StackOverflowError
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:83)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
{code}"
HIVE-15904,select query throwing Null Pointer Exception from org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.generateSemiJoinOperatorPlan,"Following query failing with Null Pointer Exception from org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.generateSemiJoinOperatorPlan

Attaching create table statements for table_1 and table_18

Query:
SELECT
COALESCE(498, LEAD(COALESCE(-973, -684, 515)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50), FLOOR(t1.double_col_16) DESC), 524) AS int_col,
(t2.int_col_10) + (t1.smallint_col_50) AS int_col_1,
FLOOR(t1.double_col_16) AS float_col,
COALESCE(SUM(COALESCE(62, -380, -435)) OVER (PARTITION BY (t2.int_col_10 + t1.smallint_col_50) ORDER BY (t2.int_col_10 + t1.smallint_col_50) DESC, FLOOR(t1.double_col_16) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 48 FOLLOWING), 704) AS int_col_2
FROM table_1 t1
INNER JOIN table_18 t2 ON (((t2.tinyint_col_15) = (t1.bigint_col_7)) AND
((t2.decimal2709_col_9) = (t1.decimal2016_col_26))) AND
((t2.tinyint_col_20) = (t1.tinyint_col_3))
WHERE (t2.smallint_col_19) IN (SELECT
COALESCE(-92, -994) AS int_col
FROM table_1 tt1
INNER JOIN table_18 tt2 ON (tt2.decimal1911_col_16) = (tt1.decimal2612_col_77)
WHERE (t1.timestamp_col_9) = (tt2.timestamp_col_18));

Error Stack:

org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException null
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:387) 
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:193) 
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:276) 
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:324) 
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:507) 
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:495) 
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:308) 
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:506) 
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1437) 
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1422) 
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) 
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) 
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:599) 
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.generateSemiJoinOperatorPlan(DynamicPartitionPruningOptimization.java:402) 
        at org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.process(DynamicPartitionPruningOptimization.java:226) 
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) 
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) 
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) 
        at org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:74) 
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) 
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runDynamicPartitionPruning(TezCompiler.java:358) 
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:90) 
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:134) 
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11126) 
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:288) 
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:257) 
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:447) 
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:329) 
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1189) 
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1176) 
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:191) 
        ... 15 more"
HIVE-15829,LLAP text cache: disable memory tracking on the writer,See ORC-141 and HIVE-15672 for context
HIVE-15669,LLAP: Improve aging in shortest job first scheduler,"Under high concurrency, some jobs can gets starved for longer time when hive.llap.task.scheduler.locality.delay is set to -1 (infinitely wait for locality).

"
HIVE-15649,LLAP IO may NPE on all-column read,"It seems like very few paths use READ_ALL_COLUMNS config, but some do. LLAP IO doesn't account for that."
HIVE-15647,Combination of a boolean condition and null-safe comparison leads to NPE,"Here's a simple example with the foodmart database:

{code}
hive> explain select count(*) from
    > sales_fact_1997 join store on sales_fact_1997.store_id = store.store_id
    > where ((store.salad_bar)) and ((store_number) <=> (customer_id));
FAILED: NullPointerException null
{code}

This happens on trunk and on HDP 2.5.3 / Hive 2. If you use = the NPE doesn't happen. If you remove the boolean condition the NPE doesn't happen.

{code}
FAILED: NullPointerException null
2016-12-13T18:23:33,604 ERROR [c4b7242e-1252-4709-8adf-22f631af75e8 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateFilterProc.process(ConstantPropagateProcFactory.java:1047)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate$ConstantPropagateWalker.walk(ConstantPropagate.java:151)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.transform(ConstantPropagate.java:120)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:242)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10913)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:246)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:75)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:435)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:326)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1169)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{code}"
HIVE-15613,Include druid-handler sources in src packaging,We forgot to do this.
HIVE-15565,LLAP: GroupByOperator flushes hash table too frequently,"{{GroupByOperator::isTez}} would be true in LLAP mode. Current memory computations can go wrong with {{isTez}} checks in {{GroupByOperator}}. For e.g, in a LLAP instance with Xmx128G and 12 executors, it would start flushing hash table for every record once it reaches around 42GB (hive.tez.container.size=7100, hive.map.aggr.hash.percentmemory=0.5).

{noformat}
2017-01-08T23:40:21,339 INFO  [TezTaskRunner (1480722417364_1922_7_03_000004_1)] org.apache.hadoop.hive.ql.exec.GroupByOperator: Hash Table flushed: new size = 0
2017-01-08T23:40:21,339 INFO  [TezTaskRunner (1480722417364_1922_7_03_000012_1)] org.apache.hadoop.hive.ql.exec.GroupByOperator: Hash Table flushed: new size = 0
2017-01-08T23:40:21,339 INFO  [TezTaskRunner (1480722417364_1922_7_03_000004_1)] org.apache.hadoop.hive.ql.exec.GroupByOperator: Hash Tbl flush: #hash table = 1
2017-01-08T23:40:21,339 INFO  [TezTaskRunner (1480722417364_1922_7_03_000012_1)] org.apache.hadoop.hive.ql.exec.GroupByOperator: Hash Tbl flush: #hash table = 1
{noformat}"
HIVE-15562,LLAP TaskExecutorService race can lead to some fragments being permanently lost,"A fragment can be evicted before it's submission completed. A race can cause a situation As a result, we end up trying to unregister the fragment for stateChangeNotifications before it has been registered - leading to an exception, which skips the actual fragment kill."
HIVE-15551,memory leak in directsql for mysql+bonecp specific initialization,"We observed HMS memory leak when directsql is enabled for MySQL metastore DB.  

The affected code is in the method MetaStoreDirecdtSql.executeNoResult():

((Connection)jdoConn.getNativeConnection()).createStatement().execute(queryText);

The statement object (from createStatement()) is unfortunately referenced in the Connection object.  Although close() is called on the Connection object  in finally block, the BoneCP just moves it to a freeConnection list. Hence, statement object never get chances to be closed.

The leaked statement object is not huge (~1KB as observed in memory analyzer). However long running Hive Metastore Server is very likely ended up with bad performance doing frequent garbage collection."
HIVE-15542,NPE in StatsUtils::getColStatistics when all values in DATE column are NULL,"Observed the following stacktrace, when all the values are NULL in date column.
 
{noformat}
2017-01-04T19:10:37,779 ERROR [46f293ab-1516-429d-aaab-4d5818ef8b82 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:792)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:206)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11071)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10644)
        at org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.analyze(ColumnStatsSemanticAnalyzer.java:412)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:510)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1302)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1442)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1222)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}"
HIVE-15503,LLAP: Fix use of Runtime.getRuntime.maxMemory in Hive operators,"{code}
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java:    maxHashTblMemory = (long) (memoryPercentage * Runtime.getRuntime().maxMemory());
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java:    // Total Free Memory = maxMemory() - Used Memory;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java:    long totalFreeMemory = Runtime.getRuntime().maxMemory() -
{code}

This will not work very well with LLAP because of the memory sharing by executors. "
HIVE-15347,LLAP: Executor memory and Xmx should have some headroom for other services,"If executor memory + cache memory is configured close or equal to Xmx, the task attempts that is causing OOM can take down the LLAP daemon. Provide some leeway for other services during memory crunch. "
HIVE-15309,RemoteException(java.io.FileNotFoundException): File does not exist... _flush_length,"OrcAcidUtils.getLastFlushLength() should check for file existence first.  Currently causes unnecessary/confusing logging:
{noformat}
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/r\
rslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1831)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1744)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolSe\
rverSideTranslatorPB.java:373)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientName\
nodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)

        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)
        at org.apache.hadoop.ipc.Client.call(Client.java:1496)
        at org.apache.hadoop.ipc.Client.call(Client.java:1396)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
        at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB\
.java:270)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)
        at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1211)
        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)
        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)
        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1536)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:330)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:326)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:782)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:460)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1525)
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:631)
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:610)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
{noformat}

Also,
{noformat}
2016-08-02 01:05:01,107 INFO  [org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService-0]: txn.TxnHandler (TxnHandler.java:timeOutLocks(2836)) - Deleted 9 ext locks from HIVE_LOCKS due to timeout (vs. 1 found. List: [738]) maxHeartbeatTime=1470099601000
{noformat}

Note that the msg says ""Deleted 9 ext locks...""  It actually delete 1 ext which has 9 internal components.  Need to follow up on this.

Also,
TxnHandler has
{noformat}
        LOG.info(quoteString(key) + "" locked by "" + quoteString(TxnHandler.hostname));
{noformat}
and a corresponding ""unlock"" msg which flood the metastore log.
"
HIVE-15278,PTF+MergeJoin = NPE,"Manifests as
{noformat}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.first(PTFRowContainer.java:115)
	at org.apache.hadoop.hive.ql.exec.PTFPartition.iterator(PTFPartition.java:114)
	at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.finishPartition(PTFOperator.java:340)
	at org.apache.hadoop.hive.ql.exec.PTFOperator.process(PTFOperator.java:114)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:838)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343)
	... 29 more
{noformat}

It's actually a somewhat subtle ordering problem in sortmerge - as it stands, it calls different branches of the tree in closeOp after they themselves have already been closed. Other operators that clean stuff up in close may result in different errors. The common pattern is
{noformat}
   1125         at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352)
   1126         at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:274)
   1127         at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.fetchOneRow(CommonMergeJoinOperator.java:404)
...
   1131         at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:428)
   1132         at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:388)
   1133         at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:617)
...
   1139         at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:294)
{noformat}"
HIVE-15162,NPE in ATSHook,"{noformat}
2016-11-08T14:21:15,025 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:run(156)) - Failed to submit plan to ATS: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.hooks.ATSHook$2.run(ATSHook.java:141)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}"
HIVE-14924,MSCK REPAIR table with single threaded is throwing null pointer exception,"MSCK REPAIR TABLE is throwing Null Pointer Exception while running on single threaded mode (hive.mv.files.thread=0)

Error:
2016-10-10T22:27:13,564 ERROR [e9ce04a8-2a84-426d-8e79-a2d15b8cee09 main([])]: exec.DDLTask (DDLTask.java:failed(581)) - java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkPartitionDirs(HiveMetaStoreChecker.java:423)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.findUnknownPartitions(HiveMetaStoreChecker.java:315)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:291)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:236)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkMetastore(HiveMetaStoreChecker.java:113)
	at org.apache.hadoop.hive.ql.exec.DDLTask.msck(DDLTask.java:1834)

In order to reproduce:

set hive.mv.files.thread=0 and run MSCK REPAIR TABLE command"
HIVE-14889,Beeline leaks sensitive environment variables of HiveServer2 when you type set;,"When you type set; beeline prints all the environment variables including passwords which could be major security risk. Eg: HADOOP_CREDENTIAL_PASSWORD below is leaked.

{noformat}
| env:HADOOP_CREDSTORE_PASSWORD=password             |
| env:HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS  |
| env:HADOOP_HOME_WARN_SUPPRESS=true                 |
| env:HADOOP_IDENT_STRING=vihang                     |
| env:HADOOP_PID_DIR=                                |
{noformat}"
HIVE-14887,"Reduce the memory used by MiniMr, MiniTez, MiniLlap tests","The clusters that we spin up end up requiring 16GB at times. Also the maven arguments seem a little heavy weight.
Reducing this will allow for additional ptest drones per box, which should bring down the runtime."
HIVE-14814,metastoreClient is used directly in Hive cause NPE,Changes introduced by HIVE-13622 uses metastoreClient directly in Hive.java which may be null causing NPE. Instead it should use getMSC() which will initialize metastoreClient variable when null.
HIVE-14773,NPE aggregating column statistics for date column in partitioned table,"Hive runs into a NPE when the query has a filter on a date column and the partitioned column 
eg: 

{code}
create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc;
set hive.exec.dynamic.partition.mode=nonstrict;
insert into date_dim partition(d_date_sk=2416945) values('1905-04-09');
insert into date_dim partition(d_date_sk=2416946) values('1905-04-10');
insert into date_dim partition(d_date_sk=2416947) values('1905-04-11');
analyze table date_dim partition(d_date_sk) compute statistics for columns;

explain select count(*) from date_dim where d_date > date ""1900-01-02"" and d_date_sk= 2416945;
{code}

Here d_date_sk is a partition column and d_date is of type date.

{code}
2016-09-16T08:27:06,510 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.AggregateStatsCache: No aggregate stats cached for database:default, table:date_dim, column:d_date
2016-09-16T08:27:06,512 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: Direct SQL query in 1.302231ms + 0.00653ms, the query is [select ""COLUMN_NAME"", ""COLUMN_TYPE"", min(""LONG_LOW_VALUE""), max(""LONG_HIGH_VALUE""), min(""DOUBLE_LOW_VALUE""), max(""DOUBLE_HIGH_VALUE""), min(cast(""BIG_DECIMAL_LOW_VALUE"" as decimal)), max(cast(""BIG_DECIMAL_HIGH_VALUE"" as decimal)), sum(""NUM_NULLS""), max(""NUM_DISTINCTS""), max(""AVG_COL_LEN""), max(""MAX_COL_LEN""), sum(""NUM_TRUES""), sum(""NUM_FALSES""), avg((""LONG_HIGH_VALUE""-""LONG_LOW_VALUE"")/cast(""NUM_DISTINCTS"" as decimal)),avg((""DOUBLE_HIGH_VALUE""-""DOUBLE_LOW_VALUE"")/""NUM_DISTINCTS""),avg((cast(""BIG_DECIMAL_HIGH_VALUE"" as decimal)-cast(""BIG_DECIMAL_LOW_VALUE"" as decimal))/""NUM_DISTINCTS""),sum(""NUM_DISTINCTS"") from ""PART_COL_STATS"" where ""DB_NAME"" = ? and ""TABLE_NAME"" = ?  and ""COLUMN_NAME"" in (?) and ""PARTITION_NAME"" in (?) group by ""COLUMN_NAME"", ""COLUMN_TYPE""]
2016-09-16T08:27:06,526  INFO [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: useDensityFunctionForNDVEstimation = false
partsFound = 1
ColumnStatisticsObj = [ColumnStatisticsObj(colName:d_date, colType:date, statsData:<ColumnStatisticsData >)]
2016-09-16T08:27:06,526 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
        org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2827)
2016-09-16T08:27:06,531 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: null retrieved using SQL in 43.425925ms
2016-09-16T08:27:06,545 ERROR [90d4780f-77e4-4704-9907-4860ce11a206 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:451)
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDateStats(ColumnStatisticsData.java:574)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:304)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:126)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10928)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:255)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:251)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:467)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1235)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1355)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{code}"
HIVE-14739,Replace runnables directly added to runtime shutdown hooks to avoid deadlock,"[~deepesh] reported that a deadlock can occur when running queries through hive cli. [~cnauroth] analyzed it and reported that hive adds shutdown hooks directly to java Runtime which may execute in non-deterministic order causing deadlocks with hadoop's shutdown hooks. In one case, hadoop shutdown locked FileSystem#Cache and FileSystem.close whereas hive shutdown hook locked FileSystem.close and FileSystem#Cache order causing a deadlock. 
Hive and Hadoop has ShutdownHookManager that runs the shutdown hooks in deterministic order based on priority. We should use that to avoid deadlock throughout the code."
HIVE-14621,LLAP: memory.mode = none has NPE,"When IO elevator is enabled, but cache and allocator are both disabled, NPEs happen. It's not really a recommended mode, but it's the only way to disable cache, so we probably need to fix it. I am also going to nuke the intermediate mode (allocator w/no cache) meanwhile cause it's pointless and just creates a zoo of configurations.
{noformat}
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.llap.cache.LlapDataBuffer.getByteBufferDup(LlapDataBuffer.java:59)
at org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.createDiskRangeInfo(StreamUtils.java:63)
at org.apache.hadoop.hive.ql.io.orc.encoded.StreamUtils.createSettableUncompressedStream(StreamUtils.java:48)
at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory$LongStreamReader$StreamReaderBuilder.build(EncodedTreeReaderFactory.java:514)
at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.createEncodedTreeReader(EncodedTreeReaderFactory.java:1737)
at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.decodeBatch(OrcEncodedDataConsumer.java:162)
at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.decodeBatch(OrcEncodedDataConsumer.java:55)
at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:76)
at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:30)
at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:408)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:424)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:227)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:224)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:224)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:93)
... 6 more
{noformat}"
HIVE-14446,Add switch to control BloomFilter in Hybrid grace hash join and make the FPP adjustable,"When row count exceeds certain limit, it doesn't make sense to generate a bloom filter, since its size will be a few hundred MB or even a few GB."
HIVE-14428,HadoopMetrics2Reporter leaks memory if the metrics sink is not configured correctly,"About 80MB held after 7 hours of running. Metrics2Collector aggregates only when it's invoked by the Hadoop sink.

Options - the first one is better IMO.
1. Fix Metrics2Collector to aggregate more often, and fix the dependency in Hive accordingly
2. Don't setup the metrics sub-system if a sink is not configured."
HIVE-14402,Vectorization: Fix Mapjoin overflow deserialization ,"This is in a codepath currently disabled in master, however enabling it triggers OOB.

{code}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setRef(BytesColumnVector.java:92)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeRowColumn(VectorDeserializeRow.java:415)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorDeserializeRow.java:674)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultLargeMultiValue(VectorMapJoinGenerateResultOperator.java:307)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultMultiValue(VectorMapJoinGenerateResultOperator.java:226)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultRepeatedAll(VectorMapJoinGenerateResultOperator.java:391)
{code}"
HIVE-14298,NPE could be thrown in HMS when an ExpressionTree could not be made from a filter,In many cases where an ExpressionTree could not be made from a filter (e.g. parser fails to parse a filter etc.) and its value is null. But this null is passed around and used by a couple of HMS methods which can cause NullPointerException.
HIVE-14173,NPE was thrown after enabling directsql in the middle of session,"hive.metastore.try.direct.sql is initially set to false in HMS hive-site.xml, then changed to true using set metaconf command in the middle of a session, running a query will be thrown NPE with error message is as following:
{code}
2016-07-06T17:44:41,489 ERROR [pool-5-thread-2]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(192)) - MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5741)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rethrowException(HiveMetaStore.java:4771)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:4754)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
	at com.sun.proxy.$Proxy18.get_partitions_by_expr(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:12048)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:12032)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.<init>(ObjectStore.java:2667)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetListHelper.<init>(ObjectStore.java:2825)
	at org.apache.hadoop.hive.metastore.ObjectStore$4.<init>(ObjectStore.java:2410)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:2410)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:2400)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
	at com.sun.proxy.$Proxy17.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:4749)
	... 20 more
{code}"
HIVE-14172,LLAP: force evict blocks by size to handle memory fragmentation,"In the long run, we should replace buddy allocator with a better scheme. For now do a workaround for fragmentation that cannot be easily resolved. It's still not perfect but works for practical  ORC cases, where we have the default size and smaller blocks, rather than large allocations having trouble."
HIVE-14139,NPE dropping permanent function,"To reproduce:
1. Start a CLI session and create a permanent function.
2. Exit current CLI session.
3. Start a new CLI session and drop the function.

Stack trace:
{noformat}
FAILED: error during drop function: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.Registry.removePersistentFunctionUnderLock(Registry.java:513)
	at org.apache.hadoop.hive.ql.exec.Registry.unregisterFunction(Registry.java:501)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.unregisterPermanentFunction(FunctionRegistry.java:1532)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.dropPermanentFunction(FunctionTask.java:228)
	at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:95)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1860)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1564)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1316)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1085)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1073)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
{noformat}"
HIVE-14090,JDOExceptions thrown by the Metastore have their full stack trace returned to clients,"When user try to create any database or table with a name longer than 128 characters:

{code}
create database test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongNametableFAIL;
{code}

It dumps the full exception stack-trace in a non-user-friendly message. The lends to relatively negative user-experience for Beeline users who hit this exception, they are generally not interested in the full stack-trace.

The formatted stack-trace is below:

{code}
Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:javax.jdo.JDOFatalUserException: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data!
at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:528)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:569)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy10.createDatabase(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:923)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:962)
at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
at com.sun.proxy.$Proxy12.create_database(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8863)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8847)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:707)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:702)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:702)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745) NestedThrowablesStackTrace: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data! org.datanucleus.exceptions.NucleusUserException: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data!
at org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping.setString(CharRDBMSMapping.java:263)
at org.datanucleus.store.rdbms.mapping.java.SingleFieldMapping.setString(SingleFieldMapping.java:201)
at org.datanucleus.store.rdbms.fieldmanager.ParameterSetter.storeStringField(ParameterSetter.java:159)
at org.datanucleus.state.JDOStateManager.providedStringField(JDOStateManager.java:1256)
at org.apache.hadoop.hive.metastore.model.MDatabase.jdoProvideField(MDatabase.java)
at org.apache.hadoop.hive.metastore.model.MDatabase.jdoProvideFields(MDatabase.java)
at org.datanucleus.state.JDOStateManager.provideFields(JDOStateManager.java:1346)
at org.datanucleus.store.rdbms.request.InsertRequest.execute(InsertRequest.java:289)
at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertTable(RDBMSPersistenceHandler.java:167)
at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertObject(RDBMSPersistenceHandler.java:143)
at org.datanucleus.state.JDOStateManager.internalMakePersistent(JDOStateManager.java:3784)
at org.datanucleus.state.JDOStateManager.makePersistent(JDOStateManager.java:3760)
at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2219)
at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:569)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy10.createDatabase(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:923)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:962)
at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
at com.sun.proxy.$Proxy12.create_database(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8863)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8847)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:707)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:702)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:702)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745) )
{code}

The ideal situation would be to just return the following message to Beeline users:

{code}
Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:javax.jdo.JDOFatalUserException: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data!)
{code}

And have the full stack trace should up in the HiveServer2 logs."
HIVE-13955,Include service-rpc and llap-ext-client in packaging files,"Include info in packaging/pom.xml, packaging/src/main/assembly/src.xml, and packaging/src/main/assembly/bin.xml"
HIVE-13934,Configure Tez to make nocondiional task size memory available for the Processor,"Currently, noconditionaltasksize is not validated against the container size, the reservations made in the container by Tez for Inputs / Outputs etc.

Check this at compile time to see if enough memory is available, or set up the vertex to reserve additional memory for the Processor."
HIVE-13932,Hive SMB Map Join with small set of LIMIT failed with NPE,"1) prepare sample data:
a=1
while [[ $a -lt 100 ]]; do echo $a ; let a=$a+1; done > data

2) prepare source hive table:
CREATE TABLE `s`(`c` string);
load data local inpath 'data' into table s;

3) prepare the bucketed table:
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
CREATE TABLE `t`(`c` string) CLUSTERED BY (c) SORTED BY (c) INTO 5 BUCKETS;
insert into t select * from s;

4) reproduce this issue:
SET hive.auto.convert.sortmerge.join = true;
SET hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;
SET hive.auto.convert.sortmerge.join.noconditionaltask = true;
SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
select * from t join t t1 on t.c=t1.c limit 1;"
HIVE-13852,NPE in TaskLocationHints during LLAP GetSplits request,"{noformat}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.process(GenericUDTFGetSplits.java:194)
at org.apache.hadoop.hive.ql.exec.UDTFOperator.process(UDTFOperator.java:116)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:434) 
at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:426) 
at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:144)
... 15 more
Caused by: java.io.IOException: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.getSplits(GenericUDTFGetSplits.java:366)
at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.process(GenericUDTFGetSplits.java:185)
... 23 more
Caused by: java.lang.NullPointerException: null
at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.getSplits(GenericUDTFGetSplits.java:344)
... 24 more
{noformat}"
HIVE-13809,Hybrid Grace Hash Join memory usage estimation didn't take into account the bloom filter size,"Memory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there's enough memory but it turns out not the case, we will run into OOM problem.

Currently hybrid grace hash join memory usage estimation didn't take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.

The solution is to count in the bloom filter size into memory estimation.

Another issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join."
HIVE-13786,Fix the unit test failure org.apache.hive.service.cli.session.TestHiveSessionImpl.testLeakOperationHandle,
HIVE-13754,Fix resource leak in HiveClientCache,"Found that the {{users}} reference count can go into negative values, which prevents {{tearDownIfUnused}} from closing the client connection when called.
This leads to a build up of clients which have been evicted from the cache, are no longer in use, but have not been shutdown.
GC will eventually call {{finalize}}, which forcibly closes the connection and cleans up the client, but I have seen as many as several hundred open client connections as a result.

The main resource for this is caused by RetryingMetaStoreClient, which will call {{reconnect}} on acquire, which calls {{close}}. This will decrement {{users}} to -1 on the reconnect, then acquire will increase this to 0 while using it, and back to -1 when it releases it."
HIVE-13749,Memory leak in Hive Metastore,"Looking a heap dump of 10GB, a large number of Configuration objects(> 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.

I will attach an exported snapshot from the eclipse MAT."
HIVE-13730,Avoid double spilling the same partition when memory threshold is set very low,I am seeing hybridgrace_hashjoin_1.q getting stuck on master.
HIVE-13729,FileSystem$Cache leaks in FileUtils.checkFileAccessWithImpersonation,"Didn't invoke FileSystem.closeAllForUGI after checkFileAccess. This results leak in FileSystem$Cache and eventually OOM for HS2.

Workaround without fix - 
fs.hdfs.impl.disable.cache=true
fs.file.impl.disable.cache=true
"
HIVE-13656,need to set direct memory limit higher in LlapServiceDriver for certain edge case configurations,
HIVE-13621,compute stats in certain cases fails with NPE,"{code}
FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:693)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:739)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:728)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:183)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:136)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:124){code}"
HIVE-13588,NPE is thrown from MapredLocalTask.executeInChildVM,"NPE was thrown out from MapredLocalTask.executeInChildVM in running some queries with CLI, see error below:
{code}
  java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeInChildVM(MapredLocalTask.java:321) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.execute(MapredLocalTask.java:148) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:172) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1868) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1595) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1346) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1117) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1105) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:236) [hive-cli-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187) [hive-cli-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) [hive-cli-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782) [hive-cli-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:721) [hive-cli-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:648) [hive-cli-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_45]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_45]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_45]
{code}
It is because the operationLog is only applicable to HS2 but CLI, therefore it might not be set (null)
It is related to HIVE-13183"
HIVE-13561,HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used,"I can repo this on branch-1.2 and branch-2.0.

It looks to be the same issues as: HIVE-11408

The patch from HIVE-11408 looks to fix the issue as well.

I've updated the patch from HIVE-11408 to be aligned with branch-1.2 and master

"
HIVE-13527,Using deprecated APIs in HBase client causes zookeeper connection leaks.,"When running queries against hbase-backed hive tables, the following log messages are seen in the HS2 log.
{code}
2016-04-11 07:25:23,657 WARN org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: You are using an HTable instance that relies on an HBase-managed Connection. This is usually due to directly creating an HTable, which is deprecated. Instead, you should create a Connection object and then request a Table instance from it. If you don't need the Table instance for your own use, you should instead use the TableInputFormatBase.initalizeTable method directly.
2016-04-11 07:25:23,658 INFO org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: Creating an additional unmanaged connection because user provided one can't be used for administrative actions. We'll close it when we close out the table.
{code}

In a HS2 log file, there are 1366 zookeeper connections established but only a small fraction of them were closed. So lsof would show 1300+ open TCP connections to Zookeeper.
grep ""org.apache.zookeeper.ClientCnxn: Session establishment complete on server"" * |wc -l
1366
grep ""INFO org.apache.zookeeper.ZooKeeper: Session:"" * |grep closed |wc -l
54

According to the comments in TableInputFormatBase, the recommended means for subclasses like HiveHBaseTableInputFormat is to call initializeTable() instead of setHTable() that it currently uses.
""
Subclasses MUST ensure initializeTable(Connection, TableName) is called for an instance to function properly. Each of the entry points to this class used by the MapReduce framework, {@link #createRecordReader(InputSplit, TaskAttemptContext)} and {@link #getSplits(JobContext)}, will call {@link #initialize(JobContext)} as a convenient centralized location to handle retrieving the necessary configuration information. If your subclass overrides either of these methods, either call the parent version or call initialize yourself.
""

Currently setHTable() also creates an additional Admin connection, even though it is not needed.

So the use of deprecated APIs are to be replaced."
HIVE-13523,Fix connection leak in ORC RecordReader and refactor for unit testing,"In RecordReaderImpl, a MetadataReaderImpl object was being created (opening a file), but never closed, causing a leak. This change closes the Metadata object in RecordReaderImpl, and does substantial refactoring to make RecordReaderImpl testable:
 * Created DataReaderFactory and MetadataReaderFactory (plus default implementations) so that the create() methods can be mocked to verify that the objects are actually closed in RecordReaderImpl.close()
 * Created MetadataReaderProperties and DataReaderProperties to clean up argument lists, making code more readable
 * Created a builder() for RecordReaderImpl to make the code more readable
 * DataReader and MetadataReader now extend closeable (there was no reason for them not to in the first place) so I can use the guava Closer interface: http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/io/Closer.html
 * Use the Closer interface to guarantee that regardless of if either close() call fails, both will be attempted (preventing further potential leaks)
 * Create builders for MetadataReaderProperties, DataReaderProperties, and RecordReaderImpl to help with code readability"
HIVE-13500,Launching big queries fails with Out of Memory Exception,"There is a code snipped in Driver class in compile method:

{noformat}
      if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT) ||
           conf.isWebUiQueryInfoCacheEnabled()) {
        String explainOutput = getExplainOutput(sem, plan, tree);
        if (explainOutput != null) {
          if (conf.getBoolVar(ConfVars.HIVE_LOG_EXPLAIN_OUTPUT)) {
            LOG.info(""EXPLAIN output for queryid "" + queryId + "" : ""
              + explainOutput);
          }
          if (conf.isWebUiQueryInfoCacheEnabled()) {
            queryDisplay.setExplainPlan(explainOutput);
          }
        }
      }
{noformat}

This is the stack trace

{noformat}

org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.OutOfMemoryError
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:178) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:216) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:325) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:456) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:433) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_72]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_72]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_72]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_72]
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_72]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_72]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) ~[hadoop-common-2.6.0.2.2.4.2-2.jar:?]
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at com.sun.proxy.$Proxy33.executeStatementAsync(Unknown Source) ~[?:?]
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:272) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:554) [hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317) [hive-service-rpc-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302) [hive-service-rpc-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) [hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_72]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_72]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_72]
Caused by: java.lang.OutOfMemoryError
        at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123) ~[?:1.8.0_72]
        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117) ~[?:1.8.0_72]
        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) ~[?:1.8.0_72]
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) ~[?:1.8.0_72]
        at java.io.PrintStream.write(PrintStream.java:480) ~[?:1.8.0_72]
        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221) ~[?:1.8.0_72]
        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291) ~[?:1.8.0_72]
        at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104) ~[?:1.8.0_72]
        at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185) ~[?:1.8.0_72]
        at java.io.PrintStream.write(PrintStream.java:527) ~[?:1.8.0_72]
        at java.io.PrintStream.print(PrintStream.java:683) ~[?:1.8.0_72]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputMap(ExplainTask.java:420) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:737) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:581) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:765) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:581) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputMap(ExplainTask.java:508) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:737) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:581) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:765) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:581) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:826) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.outputStagePlans(ExplainTask.java:970) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONPlan(ExplainTask.java:223) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.getExplainOutput(Driver.java:597) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:525) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:318) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1192) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1179) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:144) ~[hive-service-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        ... 27 more
        
{noformat}

Now, in plain English, in the compile phase, hive is trying to generate explain output and set that output in the query display. Generating the explain output fails with OOM. 

I propose that setting of the explain output in query display be made optional, so that queries can still launch. "
HIVE-13423,Handle the overflow case for decimal datatype for sum(),"When a column col1 defined as decimal and if the sum of the column overflows, we will try to increase the decimal precision by 10. But if it's reaching 38 (the max precision), the overflow still could happen. Right now, if such case happens, the following exception will throw since hive is writing incorrect data.

Follow the following steps to repro. 
{noformat}
CREATE TABLE DECIMAL_PRECISION(dec decimal(38,18));
INSERT INTO DECIMAL_PRECISION VALUES(98765432109876543210.12345), (98765432109876543210.12345);
SELECT SUM(dec) FROM DECIMAL_PRECISION;
{noformat}

{noformat}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.readVInt(LazyBinaryUtils.java:314) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.checkObjectByteInfo(LazyBinaryUtils.java:219) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.parse(LazyBinaryStruct.java:142) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
{noformat}"
HIVE-13405,Fix Connection Leak in OrcRawRecordMerger,"In OrcRawRecordMerger.getLastFlushLength, if the opened stream throws an IOException on .available() or on .readLong(), the function will exit without closing the stream.

This patch adds a try-with-resources to fix this."
HIVE-13343,Need to disable hybrid grace hash join in llap mode except for dynamically partitioned hash join,"Due to performance reasons, we should disable use of hybrid grace hash join in llap when dynamic partition hash join is not used. With dynamic partition hash join, we need hybrid grace hash join due to the possibility of skews."
HIVE-13311,MetaDataFormatUtils throws NPE when HiveDecimal.create is null,"The {{MetadataFormatUtils.convertToString}} functions have guards to validate for when valid is null, however the {{HiveDecimal.create}} can return null and will throw exceptions when {{.toString()}} is called.

{code}
  private static String convertToString(Decimal val) {
    if (val == null) {
      return """";
    }

    // HERE: Will throw NPE when HiveDecimal.create returns null.
    return HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale()).toString();
  }
{code}"
HIVE-13294,AvroSerde leaks the connection in a case when reading schema from a url,"AvroSerde leaks the connection in a case when reading schema from url:
In 
public static Schema determineSchemaOrThrowException {
...
    return AvroSerdeUtils.getSchemaFor(new URL(schemaString).openStream());
...
}
The opened inputStream is never closed.
"
HIVE-13237,Select parquet struct field with upper case throws NPE,"Query ""select msg.fieldone from test"" throws NPE if msg's fieldone is actually fieldOne:

{noformat}
2016-03-08 17:30:57,772 ERROR [main]: exec.FetchTask (FetchTask.java:initialize(86)) - java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:61)
        at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)
        at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:980)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:63)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
{noformat}
"
HIVE-13163,"ORC MemoryManager thread checks are fatal, should WARN ","The MemoryManager is tied to a WriterOptions on create, which can occur in a different thread from the writer calls.

This is unexpected, but safe and needs a warning not a fatal.

{code}
  /**
   * Light weight thread-safety check for multi-threaded access patterns
   */
  private void checkOwner() {
    Preconditions.checkArgument(ownerLock.isHeldByCurrentThread(),
        ""Owner thread expected %s, got %s"",
        ownerLock.getOwner(),
        Thread.currentThread());
  }
{code}

"
HIVE-13144,HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node,"When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL."
HIVE-13129,CliService leaks HMS connection,"HIVE-12790 fixes the HMS connection leaking. But seems there is one more connection from CLIService.

The init() function in CLIService will get info from DB but we never close the HMS connection for this service main thread.  

{noformat}
    // creates connection to HMS and thus *must* occur after kerberos login above
    try {
      applyAuthorizationConfigPolicy(hiveConf);
    } catch (Exception e) {
      throw new RuntimeException(""Error applying authorization policy on hive configuration: ""
          + e.getMessage(), e);
{noformat}"
HIVE-13099,Non-SQLOperations lead to Web UI NPE,"To support display of live operations in the WebUI, we record SQLOperations (in {{liveSqlOperations}}). 

However, to support historic operations, we save all operations in {{historicSqlOperations}}, including non-SQLOperations which do not have display entries in liveSqlOperations.

This leads to a race condition depending on whether sessions use non-sql operations. Reproduce-able by issuing a 'set' operation.
{code}
java.lang.NullPointerException
        at org.apache.hive.generated.hiveserver2.hiveserver2_jsp._jspService(hiveserver2_jsp.java:131)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)
{code}

We should save only SQLOperations in historicSqlOperations."
HIVE-13090,Hive metastore crashes on NPE with ZooKeeperTokenStore,"Observed that hive metastore shutdown with NPE from ZookeeperTokenStore.

{code}
INFO  [pool-5-thread-192]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(714)) - 191: Metastore shutdown complete.
 INFO  [pool-5-thread-192]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(340)) - ugi=cvdpqap	ip=/19.1.2.129	cmd=Metastore shutdown complete.	
 ERROR [Thread[Thread-6,5,main]]: thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(331)) - ExpiredTokenRemover thread received unexpected exception. org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token
org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token
	at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)
	at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)
	at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)
	... 3 more
 INFO  [Thread-3]: metastore.HiveMetaStore (HiveMetaStore.java:run(5639)) - Shutting down hive metastore.
{code}"
HIVE-13065,Hive throws NPE when writing map type data to a HBase backed table,"Hive throws NPE when writing data to a HBase backed table with below conditions:

# There is a map type column
# The map type column has NULL in its values

Below are the reproduce steps:

*1) Create a HBase backed Hive table*
{code:sql}
create table hbase_test (id bigint, data map<string, string>)
stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
with serdeproperties (""hbase.columns.mapping"" = "":key,cf:map_col"")
tblproperties (""hbase.table.name"" = ""hive_test"");
{code}

*2) insert data into above table*
{code:sql}
insert overwrite table hbase_test select 1 as id, map('abcd', null) as data from src limit 1;
{code}

The mapreduce job for insert query fails. Error messages are as below:
{noformat}
2016-02-15 02:26:33,225 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":1,""_col1"":{""abcd"":null}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":1,""_col1"":{""abcd"":null}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)
	... 7 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)
	... 7 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)
	... 15 more
{noformat}"
HIVE-13008,WebHcat DDL commands in secure mode NPE when default FileSystem doesn't support delegation tokens,"{noformat}
ERROR | 11 Jan 2016 20:19:02,781 | org.apache.hive.hcatalog.templeton.CatchallExceptionMapper |
java.lang.NullPointerException
        at org.apache.hive.hcatalog.templeton.SecureProxySupport$2.run(SecureProxySupport.java:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.hcatalog.templeton.SecureProxySupport.writeProxyDelegationTokens(SecureProxySupport.java:168)
        at org.apache.hive.hcatalog.templeton.SecureProxySupport.open(SecureProxySupport.java:95)
        at org.apache.hive.hcatalog.templeton.HcatDelegator.run(HcatDelegator.java:63)
        at org.apache.hive.hcatalog.templeton.Server.ddl(Server.java:217)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1480)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1411)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1360)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1350)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:538)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:716)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1360)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:615)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:574)
        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:88)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1331)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:477)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
{noformat}"
HIVE-12990,LLAP: ORC cache NPE without FileID support,"{code}
   OrcBatchKey stripeKey = hasFileId ? new OrcBatchKey(fileId, -1, 0) : null;
   ...
          if (hasFileId && metadataCache != null) {
            stripeKey.stripeIx = stripeIx;
            stripeMetadata = metadataCache.getStripeMetadata(stripeKey);
          }
...
  public void setStripeMetadata(OrcStripeMetadata m) {
    assert stripes != null;
    stripes[m.getStripeIx()] = m;
  }
{code}

{code}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.getStripeIx(OrcStripeMetadata.java:106)
        at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.setStripeMetadata(OrcEncodedDataConsumer.java:70)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.readStripesMetadata(OrcEncodedDataReader.java:685)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:283)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:215)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:212)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:212)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:93)
        ... 5 more
{code}"
HIVE-12904,LLAP: deadlock in task scheduling,"{noformat}
Thread 34107: (state = BLOCKED)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.isInWaitQueue() @bci=0, line=690 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.finishableStateUpdated(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=8, line=485 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1500(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService, org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=3, line=78 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.finishableStateUpdated(boolean) @bci=27, line=733 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.sourceStateUpdated(java.lang.String) @bci=76, line=210 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.sourceStateUpdated(java.lang.String) @bci=5, line=164 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.registerSourceStateChange(java.lang.String, java.lang.String, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateProto) @bci=34, line=228 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=47, line=255 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=328 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.sourceStateUpdated(com.google.protobuf.RpcController, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=105 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.RpcController, com.google.protobuf.Message) @bci=80, line=13067 (Compiled frame)
 - org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(org.apache.hadoop.ipc.RPC$Server, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=246, line=616 (Compiled frame)
 - org.apache.hadoop.ipc.RPC$Server.call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=9, line=969 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=38, line=2151 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=1, line=2147 (Compiled frame)
 - java.security.AccessController.doPrivileged(java.security.PrivilegedExceptionAction, java.security.AccessControlContext) @bci=0 (Compiled frame)
 - javax.security.auth.Subject.doAs(javax.security.auth.Subject, java.security.PrivilegedExceptionAction) @bci=42, line=422 (Compiled frame)
 - org.apache.hadoop.security.UserGroupInformation.doAs(java.security.PrivilegedExceptionAction) @bci=14, line=1657 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler.run() @bci=315, line=2145 (Interpreted frame)


and 


Thread 34500: (state = BLOCKED)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.unregisterForUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=0, line=195 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.unregisterFinishableStateUpdate(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=160 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.unregisterForFinishableStateUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=143 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.maybeUnregisterForFinishedStateNotifications() @bci=20, line=681 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(org.apache.tez.runtime.task.TaskRunner2Result) @bci=32, line=548 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(java.lang.Object) @bci=5, line=535 (Compiled frame)
 - com.google.common.util.concurrent.Futures$4.run() @bci=55, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)

""IPC Server handler 0 on 15001"":
  waiting to lock Monitor@0x00007f5d322ecb08 (Object@0x00007f67032cd2c0, a org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService$TaskWrapper),
  which is held by ""ExecutionCompletionThread #0""
""ExecutionCompletionThread #0"":
  waiting to lock Monitor@0x00007f6066b9e8c8 (Object@0x00007f66b6570200, a org/apache/hadoop/hive/llap/daemon/impl/QueryInfo$FinishableStateTracker),
  which is held by ""IPC Server handler 0 on 15001""

Found a total of 1 deadlock.

{noformat}

Looks like it's caused by synchronized blocks:
{noformat}
TaskWrapper:
public synchronized void maybeUnregisterForFinishedStateNotifications
{noformat}
Eventually calls 
{noformat}
FinishableStateTracker
synchronized void unregisterForUpdates(FinishableStateUpdateHandler handler) {
{noformat}

and 
{noformat}
FST
 synchronized void sourceStateUpdated(String sourceName) {
   {noformat}
eventually calls
{noformat}
 public synchronized boolean isInWaitQueue() {
{noformat}

The latter returns the boolean, so it definitely doesn't need synchronized, however I don't know if there are other similar issues and what is necessary inside sync blocks, perhaps there's a better fix.

Overall I'd say synch methods on objects that call any other non-trivial objects should not be used. Perhaps for now it would be good to replace all sync methods by sync blocks that cover entire method, as well as remove the unnecessary ones like the isWait... one. Then the scope of the blocks can be adjusted based on logic in future.
"
HIVE-12864,StackOverflowError parsing queries with very large predicates,"We have seen that queries with very large predicates might fail with the following stacktrace:

{noformat}
016-01-12 05:47:36,516|beaver.machine|INFO|552|5072|Thread-22|Exception in thread ""main"" java.lang.StackOverflowError

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:145)

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,634|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:37,582|main|INFO|552|4568|MainThread|TEST ""test_WideQuery"" FAILED in 10.95 seconds
{noformat}

The problem could be solved by reimplementing some of the parsing methods so they are iterative instead of recursive."
HIVE-12837,Better memory estimation/allocation for hybrid grace hash join during hash table loading,"This is to avoid an edge case when the memory available is very little (less than a single write buffer size), and we start loading the hash table. Since the write buffer is lazily allocated, we will easily run out of memory before even checking if we should spill any hash partition.

e.g.
Total memory available: 210 MB
Size of ref array of BytesBytesMultiHashMap for each hash partition: ~16 MB
Size of write buffer: 8 MB (lazy allocation)
Number of hash partitions: 16
Number of hash partitions created in memory: 13
Number of hash partitions created on disk: 3
Available memory left after HybridHashTableContainer initialization: 210-16*13=2MB

Now let's say a row is to be loaded into a hash partition in memory, it will try to allocate an 8MB write buffer for it, but we only have 2MB, thus OOM.

Solution is to perform the check for possible spilling earlier so we can spill partitions if memory is about to be full, to avoid OOM."
HIVE-12815,column stats NPE for a query w/o a table,"I was running something like create table as select 1;

First it logs why it cannot get stats:
{noformat}
2016-01-08T21:46:31,876 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: stats.StatsUtils (StatsUtils.java:getTableColumnStats(756)) - Failed to retrieve table statistics:
org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Specified database/table does not exist : _dummy_database._dummy_table)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTableColumnStatistics(Hive.java:3195) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:752) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:198) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
{noformat}

and returns null, then it fails with NPE:
{noformat}
2016-01-08T21:46:31,885 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: ql.Driver (SessionState.java:printError(1010)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getDataSizeFromColumnStats(StatsUtils.java:1450)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:199)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
{noformat}

Only ""NullPointerException null"" is logged to CLI... :("
HIVE-12798,CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver.vector* queries failures due to NPE in Vectorizer.onExpressionHasNullSafes(),"As of 01/04/2016, the following tests fail in the MiniTezCliDriver mode when the cbo return path is enabled. We need to fix them :
{code}
 vector_leftsemi_mapjoin
 vector_join_filters
 vector_interval_mapjoin
 vector_left_outer_join
 vectorized_mapjoin
 vector_inner_join
 vectorized_context
 tez_vector_dynpart_hashjoin_1
 count
 auto_sortmerge_join_6
 skewjoin
 vector_auto_smb_mapjoin_14
 auto_join_filters
 vector_outer_join0
 vector_outer_join1
 vector_outer_join2
 vector_outer_join3
 vector_outer_join4
 vector_outer_join5
 hybridgrace_hashjoin_1
 vector_mapjoin_reduce
 vectorized_nested_mapjoin
 vector_left_outer_join2
 vector_char_mapjoin1
 vector_decimal_mapjoin
 vectorized_dynamic_partition_pruning
 vector_varchar_mapjoin1
{code}

This jira is intended to cover the vectorization issues related to the MiniTezCliDriver failures caused by NPE via nullSafes array as shown below :
{code}
private boolean onExpressionHasNullSafes(MapJoinDesc desc) {
     boolean[] nullSafes = desc.getNullSafes();
     for (boolean nullSafe : nullSafes) {
{code}"
HIVE-12790,Metastore connection leaks in HiveServer2,"HiveServer2 keeps opening new connections to HMS each time it launches a task. These connections do not appear to be closed when the task completes thus causing a HMS connection leak. ""lsof"" for the HS2 process shows connections to port 9083.

{code}
2015-12-03 04:20:56,352 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 11 out of 41
2015-12-03 04:20:56,354 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083
2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14824
2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
....
2015-12-03 04:21:06,355 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 12 out of 41
2015-12-03 04:21:06,357 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083
2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14825
2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
...
2015-12-03 04:21:08,357 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 13 out of 41
2015-12-03 04:21:08,360 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083
2015-12-03 04:21:08,364 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14826
2015-12-03 04:21:08,365 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
... 
{code}

The TaskRunner thread starts a new SessionState each time, which creates a new connection to the HMS (via Hive.get(conf).getMSC()) that is never closed.

Even SessionState.close(), currently not being called by the TaskRunner thread, does not close this connection.

Attaching a anonymized log snippet where the number of HMS connections reaches north of 25000+ connections."
HIVE-12787,Trace improvement - Inconsistent logging upon shutdown-start of the Hive metastore process,"The log at: https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L793 logged at the start of the shutdown of the Hive metastore process can be improved to match the finish of the shutdown log at: https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L793
by rephrasing from: ""Shutting down the object store..."" to: ""Metastore shutdown started..."". This will match the shutdown completion log: ""Metastore shutdown complete.""."
HIVE-12761,Add stack trace servlet to HS2 web ui,"To confirm the state of HS2, I add the servlet which prints stack trace. "
HIVE-12740,NPE with HS2 when using null input format,"When we have a query that returns empty rows and when using tez with hs2, we hit NPE:

{code}
java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)
	... 4 more
15/12/17 18:59:06 INFO log.PerfLogger: </PERFLOG method=getSplits start=1450378746335 end=1450378746433 duration=98 from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>
15/12/17 18:59:06 ERROR exec.Task: Failed to execute tez graph.
org.apache.tez.dag.api.TezUncheckedException: Failed to generate InputSplits
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:124)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:502)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)
	... 23 more
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)
	... 27 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)
	... 4 more
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
15/12/17 18:59:06 ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
15/12/17 18:59:06 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1450378746093 end=1450378746434 duration=341 from=org.apache.hadoop.hive.ql.Driver>
15/12/17 18:59:06 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/12/17 18:59:06 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1450378746434 end=1450378746434 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/12/17 18:59:06 ERROR operation.Operation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:367)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:183)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}"
HIVE-12684,NPE in stats annotation when all values in decimal column are NULLs,"When all column values are null for a decimal column and when column stats exists. AnnotateWithStatistics optimization can throw NPE. Following is the exception trace

{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:712)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:764)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:750)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:197)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:143)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:131)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:228)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10156)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:225)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237)

{code}"
HIVE-12673,Orcfiledump throws NPE when no files are available,"{noformat}


Exception in thread ""main"" java.lang.NullPointerException
	at org.codehaus.jettison.json.JSONTokener.more(JSONTokener.java:106)
	at org.codehaus.jettison.json.JSONTokener.next(JSONTokener.java:116)
	at org.codehaus.jettison.json.JSONTokener.nextClean(JSONTokener.java:170)
	at org.codehaus.jettison.json.JSONObject.<init>(JSONObject.java:185)
	at org.codehaus.jettison.json.JSONObject.<init>(JSONObject.java:293)
	at org.apache.hadoop.hive.ql.io.orc.JsonFileDump.printJsonMetaData(JsonFileDump.java:197)
	at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:107)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}

hive --orcfiledump -j -p /tmp/orc/inventory/inv_date_sk=2452654"
HIVE-12662,StackOverflowError in HiveSortJoinReduceRule when limit=0,"L96 of HiveSortJoinReduceRule, you will see 

{noformat}
    // Finally, if we do not reduce the input size, we bail out
    if (RexLiteral.intValue(sortLimit.fetch)
            >= RelMetadataQuery.getRowCount(reducedInput)) {
      return false;
    }
{noformat}

It is using “ RelMetadataQuery.getRowCount” which is always at least 1. This is the problem that we resolved in CALCITE-987.

To confirm this, I just run the q file :

{noformat}
set hive.mapred.mode=nonstrict;
set hive.optimize.limitjointranspose=true;
set hive.optimize.limitjointranspose.reductionpercentage=1f;
set hive.optimize.limitjointranspose.reductiontuples=0;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 0;
{noformat}

  And I got

{noformat}
2015-12-11T10:21:04,435 ERROR [c1efb099-f900-46dc-9f74-97af0944a99d main[]]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(301)) - CBO failed, skipping CBO.
java.lang.RuntimeException: java.lang.StackOverflowError
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.rethrowCalciteException(CalcitePlanner.java:749) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:645) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:264) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10076) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:223) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:456) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1138) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1187) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1063) [hive-exec-2.1.0-SNAPSHOT.jar:?]
{noformat}

via [~pxiong]"
HIVE-12660,HS2 memory leak with .hiverc file use,"The Operation objects created to process .hiverc file in HS2 are not closed.
In HiveSessionImpl, GlobalHivercFileProcessor calls executeStatementInternal but ignores the OperationHandle it returns.
"
HIVE-12610,"Hybrid Grace Hash Join should fail task faster if processing first batch fails, instead of continuing processing the rest","During processing the spilled partitions, if there's any fatal error, such as Kryo exception, then we should exit early, instead of moving on to process the rest of spilled partitions."
HIVE-12585,fix TxnHandler connection leak,checkLock(CheckLockRequest rqst) is leaking connection
HIVE-12577,NPE in LlapTaskCommunicator when unregistering containers,"{code}
2015-12-02 13:29:00,160 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator$EntityTracker.unregisterContainer(LlapTaskCommunicator.java:586)
        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.registerContainerEnd(LlapTaskCommunicator.java:188)
        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:389)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
        at java.lang.Thread.run(Thread.java:745)
2015-12-02 13:29:00,167 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:386)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
        at java.lang.Thread.run(Thread.java:745)
{code}"
HIVE-12557,NPE while removing entry in LRFU cache,"{code}
Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 6, vertexId=vertex_1448429572030_1851_5_00, diagnostics=[Task failed, taskId=task_1448429572030_1851_5_00_000006, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task: attempt_1448429572030_1851_5_00_000006_0:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:195)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:160)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:348)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:74)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:352)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)
        ... 14 more
Caused by: java.io.IOException: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)
        at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62)
        ... 16 more
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:283)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.nextCvb(LlapInputFormat.java:239)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:167)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:103)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
        ... 22 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.removeFromListUnderLock(LowLevelLrfuCachePolicy.java:351)
        at org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.removeFromListAndUnlock(LowLevelLrfuCachePolicy.java:336)
        at org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.notifyUnlock(LowLevelLrfuCachePolicy.java:133)
        at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.unlockBuffer(LowLevelCacheImpl.java:354)
        at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.releaseBuffer(LowLevelCacheImpl.java:338)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$DataWrapperForOrc.releaseBuffer(OrcEncodedDataReader.java:922)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.releaseInitialRefcounts(EncodedReaderImpl.java:453)
        at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:416)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
        ... 5 more
{code}"
HIVE-12532,LLAP Cache: Uncompressed data cache has NPE,"{code}
2015-11-26 08:28:45,232 [TezTaskRunner_attempt_1448429572030_0255_2_02_000019_2(attempt_1448429572030_0255_2_02_000019_2)] WARN org.apache.tez.runtime.LogicalIOProcessorRuntimeTask: Ignoring exception when closing input a(cleanup). Exception class=java.io.IOException, message=java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:283)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.close(LlapInputFormat.java:275)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doClose(HiveRecordReader.java:50)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.close(HiveContextAwareRecordReader.java:104)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.close(TezGroupedSplitsInputFormat.java:177)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.close(MRReaderMapred.java:96)
	at org.apache.tez.mapreduce.input.MRInput.close(MRInput.java:559)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.cleanup(LogicalIOProcessorRuntimeTask.java:872)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:104)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.preReadUncompressedStream(EncodedReaderImpl.java:795)
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:320)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
	... 5 more
{code}

Not clear if current.next can set it to null before the continue; 

{code}
      assert partOffset <= current.getOffset();
      if (partOffset == current.getOffset() && current instanceof CacheChunk) {
        // We assume cache chunks would always match the way we read, so check and skip it.
        assert current.getOffset() == partOffset && current.getEnd() == partEnd;
        lastUncompressed = (CacheChunk)current;
        current = current.next;
        continue;
      }
{code}"
HIVE-12517,Beeline's use of failed connection(s) causes failures and leaks.,"Beeline adds a bad connection(s) to the connection list and makes it the current connection, so any subsequent queries will attempt to use this bad connection and will fail. Even a ""!close"" would not work.
1) all queries fail unless !go is used.
2) !closeall cannot close the active connections either.
3) !exit will exit while attempting to establish these inactive connections without closing the active connections. So this could hold up server side resources.

{code}
beeline> !connect jdbc:hive2://localhost:10000 hive1 hive1
scan complete in 8ms
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 2.0.0-SNAPSHOT)
Driver: Hive JDBC (version 1.1.0-cdh5.7.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> !connect jdbc:hive2://localhost:10000 hive1 hive1
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 2.0.0-SNAPSHOT)
Driver: Hive JDBC (version 1.1.0-cdh5.7.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
1: jdbc:hive2://localhost:10000> !connect jdbc:hive2://localhost:10000 hive1 hive1
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 2.0.0-SNAPSHOT)
Driver: Hive JDBC (version 1.1.0-cdh5.7.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
2: jdbc:hive2://localhost:10000> !tables
+------------+--------------+---------------------+-------------+----------+--+
| TABLE_CAT  | TABLE_SCHEM  |     TABLE_NAME      | TABLE_TYPE  | REMARKS  |
+------------+--------------+---------------------+-------------+----------+--+
|            | default      | char_nested_1       | TABLE       | NULL     |
|            | default      | src                 | TABLE       | NULL     |
|            | default      | char_nested_struct  | TABLE       | NULL     |
|            | default      | src_thrift          | TABLE       | NULL     |
|            | default      | x                   | TABLE       | NULL     |
+------------+--------------+---------------------+-------------+----------+--+
2: jdbc:hive2://localhost:10000> !list
3 active connections:
 #0  open     jdbc:hive2://localhost:10000
 #1  open     jdbc:hive2://localhost:10000
 #2  open     jdbc:hive2://localhost:10000
2: jdbc:hive2://localhost:10000> !connect jdbc:hive2://localhost:11000 hive1 hive1
Connecting to jdbc:hive2://localhost:11000
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)
3: jdbc:hive2://localhost:11000 (closed)> !tables
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)
3: jdbc:hive2://localhost:11000 (closed)> !list
4 active connections:
 #0  open     jdbc:hive2://localhost:10000
 #1  open     jdbc:hive2://localhost:10000
 #2  open     jdbc:hive2://localhost:10000
 #3  closed   jdbc:hive2://localhost:11000
3: jdbc:hive2://localhost:11000 (closed)> !close
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)
3: jdbc:hive2://localhost:11000 (closed)> !closeall
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)
4: jdbc:hive2://localhost:11000 (closed)> !exit
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:11000: java.net.ConnectException: Connection refused (state=08S01,code=0)
{code}

The workaround is to use !go to set the current connection to a ""good"" connection.
"
HIVE-12476,Metastore NPE on Oracle with Direct SQL,"Stack trace looks very similar to HIVE-8485. I believe the metastore's Direct SQL mode requires additional fixes similar to HIVE-8485, around the Partition/StorageDescriptorSerDe parameters.

{noformat}
2015-11-19 18:08:33,841 ERROR [pool-5-thread-2]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)
        at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)
        at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)
        at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)
        at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)
        at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)
        at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)
        at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)
        at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)
        at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}"
HIVE-12445,Tracking of completed dags is a slow memory leak,"LLAP daemons track completed DAGs, but never clean up these structures. This is primarily to disallow out of order executions. Evaluate whether that can be avoided - otherwise this structure needs to be cleaned up with a delay."
HIVE-12419,hive.log.trace.id needs to be whitelisted,HIVE-12249 introduces hive.log.trace.id as part of improving logging for hive queries. The property needs to be added to SQL Std Auth whitelisted properties list to be usable with HiveServer2.
HIVE-12418,HiveHBaseTableInputFormat.getRecordReader() causes Zookeeper connection leak.,"  @Override
  public RecordReader<ImmutableBytesWritable, ResultWritable> getRecordReader(
...
...
 setHTable(HiveHBaseInputFormatUtil.getTable(jobConf));
...

The HiveHBaseInputFormatUtil.getTable() creates new ZooKeeper connections(when HTable instance is created) which are never closed."
HIVE-12349,NPE in ORC SARG for IS NULL queries on Timestamp and Date columns,"IS NULL queries can trigger an NPE for timestamp and date columns. All column values per row group or stripe should be NULL to trigger this case. Following is the exception stack trace
{code}
Caused by: java.lang.NullPointerException 
at org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl$TimestampStatisticsImpl.getMinimum(ColumnStatisticsImpl.java:795) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getMin(RecordReaderImpl.java:2343) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicate(RecordReaderImpl.java:2366) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:2564) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:2627) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:3060) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:3102) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:288) 
at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:534) 
at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183) 
at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226) 
at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437) 
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1141) 
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1039) 
at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:246) 
... 26 more
{code}"
HIVE-12318,qtest failing due to NPE in logStats,"{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.Operator.logStats(Operator.java:899) ~
{noformat}"
HIVE-12310,Update memory estimation login in TopNHash,"HIVE-12084 changes TopNHash to use Runtime.getRuntime().freeMemory() for finding available memory.
However, it does not give the all the memory it could use, it ignores unallocated memory. This is because the heap size of jvm grows up to max heap size (-Xmx) as per it needs. totalMemory() gives total heap space it has allocated, and freeMemory() is the free memory within that.
See http://i.stack.imgur.com/GjuwM.png and http://stackoverflow.com/questions/3571203/what-is-the-exact-meaning-of-runtime-getruntime-totalmemory-and-freememory .
So instead of using Runtime.getRuntime().freeMemory() , I think it should use maxMemory() - totalMemory() + freeMemory()"
HIVE-12268,Context leaks deleteOnExit paths,Long running HS2 saves lots of paths in the FileSystem's deleteOnExit map. We should remove those paths already removed.
HIVE-12257,Enhance ORC FileDump utility to handle flush_length files and recovery,ORC file dump utility currently does not handle delta directories that contain *_flush_length files. These files contains offsets to footer in the corresponding delta file.
HIVE-12250,Zookeeper connection leaks in Hive's HBaseHandler.,"HiveServer2 performance regresses severely due to what appears to be a leak in the ZooKeeper connections. lsof output on the HS2 process shows about 8000 TCP connections to the ZK ensemble nodes.

grep TCP lsof-hive-node11 | grep node11 | grep -E ""node03|node04|node05"" | wc -l
    7866 
grep TCP lsof-hive-node11 | grep node11 | grep -E ""node03"" | wc -l
    2615
grep TCP lsof-hive-node11 | grep node11 | grep -E ""node04"" | wc -l
    2622
grep TCP lsof-hive-node11 | grep node11 | grep -E ""node05"" | wc -l
    2629


node11 - HMS node
node03, node04 and node05 are the hosts for zookeeper ensemble."
HIVE-12208,Vectorized JOIN NPE on dynamically partitioned hash-join + map-join,"TPC-DS Q82 with reducer vectorized join optimizations

{code}
  Reducer 5 <- Map 1 (CUSTOM_SIMPLE_EDGE), Map 2 (CUSTOM_SIMPLE_EDGE), Map 3 (BROADCAST_EDGE), Map 4 (CUSTOM_SIMPLE_EDGE)
{code}

{code}
set hive.optimize.dynamic.partition.hashjoin=true;
set hive.vectorized.execution.reduce.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;

select  i_item_id
       ,i_item_desc
       ,i_current_price
 from item, inventory, date_dim, store_sales
 where i_current_price between 30 and 30+30
 and inv_item_sk = i_item_sk
 and d_date_sk=inv_date_sk
 and d_date between '2002-05-30' and '2002-07-30'
 and i_manufact_id in (437,129,727,663)
 and inv_quantity_on_hand between 100 and 500
 and ss_item_sk = i_item_sk
 group by i_item_id,i_item_desc,i_current_price
 order by i_item_id
 limit 100
{code}

possibly a trivial plan setup issue, since the NPE is pretty much immediate.

{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.forwardBigTableBatch(VectorMapJoinGenerateResultOperator.java:603)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:362)
	... 19 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.commonSetup(VectorMapJoinInnerGenerateResultOperator.java:112)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:96)
	... 22 more
{code}"
HIVE-12202,NPE thrown when reading legacy ACID delta files,"When reading legacy ACID deltas of the form {{delta_$startTxnId_$endTxnId}} a {{NullPointerException}} is thrown on:

{code:title=org.apache.hadoop.hive.ql.io.AcidUtils.deserializeDeltas#371}
if(dmd.getStmtIds().isEmpty()) {
{code}

The older ACID data format (pre-Hive 1.3.0) which does not include the statement ID, and code written for that format should still be supported. Therefore the above condition should also include a null check or alternatively {{AcidInputFormat.DeltaMetaData}} should never return null, and return an empty list in this specific scenario."
HIVE-12196,NPE when converting bad timestamp value,"When I convert a timestamp value that is slightly wrong, the result is a NPE. Other queries correctly reject the timestamp:

{code}
hive> select from_utc_timestamp('2015-04-11-12:24:34.535', 'UTC');
FAILED: NullPointerException null
hive> select TIMESTAMP '2015-04-11-12:24:34.535';
FAILED: SemanticException Unable to convert time literal '2015-04-11-12:24:34.535' to time value.
{code}"
HIVE-12178,LLAP: NPE in LRFU policy,"{noformat}
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.removeFromListUnderLock(LowLevelLrfuCachePolicy.java:346)
at org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.removeFromListAndUnlock(LowLevelLrfuCachePolicy.java:335)
at org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.notifyUnlock(LowLevelLrfuCachePolicy.java:133)
at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.unlockBuffer(LowLevelCacheImpl.java:354)
at org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.releaseBuffers(LowLevelCacheImpl.java:344)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.returnData(OrcEncodedDataReader.java:662)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.returnData(OrcEncodedDataReader.java:74)
at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.returnSourceData(EncodedDataConsumer.java:131)
at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:122)
at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:36)
at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:405)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1655)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37)
... 4 more
{noformat}"
HIVE-12143,LLAP: Adding llap client and server to packaging pom[old task],JIRA for branch commit(s) missing a JIRA: 4b2a13a5e9bccadf1ca430c2a110cbe5d68a66b
HIVE-12141,LLAP: Fix split generation NPE[old task],JIRA for branch commit(s) missing a JIRA: bbaca8b33ed31cb67862f7c815ea7b6bbe5a2b4
HIVE-12125,LLAP: NPE when calling abort on the TezProcessor. (Siddharth Seth)[old task],JIRA for branch commit(s) missing a JIRA: d0881e04e1aa9dd10dde8425427f29a53bee97a
HIVE-12084,Hive queries with ORDER BY and large LIMIT fails with OutOfMemoryError Java heap space,"STEPS TO REPRODUCE:
{code}
CREATE TABLE `sample_07` ( `code` string , `description` string , `total_emp` int , `salary` int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TextFile;
load data local inpath 'sample_07.csv'  into table sample_07;
set hive.limit.pushdown.memory.usage=0.9999;
select * from sample_07 order by salary LIMIT 999999999;
{code}

This will result in 
{code}
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.hive.ql.exec.TopNHash.initialize(TopNHash.java:113)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(ReduceSinkOperator.java:234)
	at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.initializeOp(VectorReduceSinkOperator.java:68)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
{code}

The basic issue lies with top n optimization. We need a limit for the top n optimization. Ideally we would detect that the allocated bytes will be bigger than the ""limit.pushdown.memory.usage"" without trying to alloc it.
"
HIVE-11978,LLAP: NPE in Expr toString,
HIVE-11960,braces in join conditions are not supported,These should be supported; they are ANSI
HIVE-11940,"""INSERT OVERWRITE"" query is very slow because it creates one ""distcp"" per file to copy data from staging directory to target directory","When hive.exec.stagingdir is set to "".hive-staging"", which will be placed under the target directory when running ""INSERT OVERWRITE"" query, Hive will grab all files under the staging directory and copy them ONE BY ONE to target directory.

When hive exec.stagingdir is set to ""/tmp/hive"", Hive will simply do a RENAME operation which will be instant.

This happens with files that are not encrypted. "
HIVE-11935,Race condition in  HiveMetaStoreClient: isCompatibleWith and close,"We saw intermittent failure of the following stack:
{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.isCompatibleWith(HiveMetaStoreClient.java:287)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
        at com.sun.proxy.$Proxy9.isCompatibleWith(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:206)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.createHiveDB(BaseSemanticAnalyzer.java:205)
        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.<init>(DDLSemanticAnalyzer.java:223)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:259)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:409)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1116)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:110)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:181)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:388)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:375)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
        at com.sun.proxy.$Proxy20.executeStatementAsync(Unknown Source)
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:274)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:486)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.thrift.server.TServlet.doPost(TServlet.java:83)
        at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:171)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)
        at org.eclipse.jetty.server.Server.handle(Server.java:349)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}
HiveMetaStoreClient.isCompatibleWith does a null check of currentMetaVars in the beginning, but it is possible HiveMetaStoreClient.close is invoked before it gets used, thus we will see the above stack. Access of currentMetaVars should be synchronized."
HIVE-11849,NPE in HiveHBaseTableShapshotInputFormat in query with just count(*),"Adding the following example as a qfile test in hbase-handler fails. Looks like this may have been introduced by HIVE-5277.

{noformat}
SET hive.hbase.snapshot.name=src_hbase_snapshot;
SET hive.hbase.snapshot.restoredir=/tmp;

select count(*) from src_hbase;
{noformat}"
HIVE-11824,Insert to local directory causes staging directory to be copied,"While running tez_insert_overwrite_local_directory_1.q test under tez and llap, the results were flaky. The reason being insert into local directory copies .staging directory to destination directory. This causes dfs -cat dest-dir/* to fail, failing the tests. Non local insert directory works fine as MoveTask performs move operation with hidden files removal filter before moving whereas local directory insert uses DFS.copyToLocalFile without hidden files filter."
HIVE-11768,java.io.DeleteOnExitHook leaks memory on long running Hive Server2 Instances,"  More than 490,000 paths was added to java.io.DeleteOnExitHook on one of our long running HiveServer2 instances,taken up more than 100MB on heap.
  Most of the paths contains a suffix of "".pipeout"".
"
HIVE-11714,Turn off hybrid grace hash join for cross product join,"Current partitioning calculation is solely based on hash value of the key. For cross product join where keys are empty, all the rows will be put into partition 0. This falls back to the regular mapjoin behavior where we only have one hashtable."
HIVE-11636,NPE in stats conversion with HBase metastore,"NO PRECOMMIT TESTS

{noformat}
2015-08-24T20:37:22,285 ERROR [main]: ql.Driver (SessionState.java:printError(963)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:740)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:731)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:186)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:139)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:127)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:110)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:56)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:249)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:123)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10219)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:212)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:240)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:434)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)
{noformat}

Fails after importing some databases from regular metastore and running TPCDS Q27.
Simple select-where-limit query (not FetchTask) appears to run fine.

With standalone Hbase metastore (might be the same issue):
{noformat}
2015-08-25 14:41:04,793 ERROR [pool-6-thread-53] server.TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)
        at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:393)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1655)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

I think I've reported this in the past for regular metastore and it was fixed somewhere"
HIVE-11596,"nvl(x, y) throws NPE if type x and type y doesn't match, rather than throwing the meaningful error","{noformat}
create table test(key string);
select nvl(key, true) from test;
{noformat}

The query above will throw NPE rather than the meaningful error ""The first and seconds arguments of function NLV should have the same type"".
"
HIVE-11587,Fix memory estimates for mapjoin hashtable,"Due to the legacy in in-memory mapjoin and conservative planning, the memory estimation code for mapjoin hashtable is currently not very good. It allocates the probe erring on the side of more memory, not taking data into account because unlike the probe, it's free to resize, so it's better for perf to allocate big probe and hope for the best with regard to future data size. It is not true for hybrid case.
There's code to cap the initial allocation based on memory available (memUsage argument), but due to some code rot, the memory estimates from planning are not even passed to hashtable anymore (there used to be two config settings, hashjoin size fraction by itself, or hashjoin size fraction for group by case), so it never caps the memory anymore below 1 Gb. 
Initial capacity is estimated from input key count, and in hybrid join cache can exceed Java memory due to number of segments.

There needs to be a review and fix of all this code.
Suggested improvements:
1) Make sure ""initialCapacity"" argument from Hybrid case is correct given the number of segments. See how it's calculated from keys for regular case; it needs to be adjusted accordingly for hybrid case if not done already.
1.5) Note that, knowing the number of rows, the maximum capacity one will ever need for probe size (in longs) is row count (assuming key per row, i.e. maximum possible number of keys) divided by load factor, plus some very small number to round up. That is for flat case. For hybrid case it may be more complex due to skew, but that is still a good upper bound for the total probe capacity of all segments.
2) Rename memUsage to maxProbeSize, or something, make sure it's passed correctly based on estimates that take into account both probe and data size, esp. in hybrid case.
3) Make sure that memory estimation for hybrid case also doesn't come up with numbers that are too small, like 1-byte hashtable. I am not very familiar with that code but it has happened in the past.

Other issues we have seen:
4) Cap single write buffer size to 8-16Mb. The whole point of WBs is that you should not allocate large array in advance. Even if some estimate passes 500Mb or 40Mb or whatever, it doesn't make sense to allocate that.
5) For hybrid, don't pre-allocate WBs - only allocate on write.
6) Change everywhere rounding up to power of two is used to rounding down, at least for hybrid case (?)

I wanted to put all of these items in single JIRA so we could keep track of fixing all of them.
I think there are JIRAs for some of these already, feel free to link them to this one."
HIVE-11580,ThriftUnionObjectInspector#toString throws NPE,"ThriftUnionObjectInspector uses toString from StructObjectInspector, which accesses uninitialized member variable fields."
HIVE-11567,Some trace logs seeped through with new log4j2 changes,Observed hive.log file size difference when running with new log4j2 changes (HIVE-11304). Looks like the default threshold was DEBUG in log4j1.x (as log4j.threshold was misspelt). In log4j2 the default threshold was set to ALL which emitted some trace logs.
HIVE-11499,Datanucleus leaks classloaders when used using embedded metastore with HiveServer2 with UDFs,"When UDFs are used, we create a new classloader to add the UDF jar. Similar to what hadoop's reflection utils does(HIVE-11408), datanucleus caches the classloaders (https://github.com/datanucleus/datanucleus-core/blob/3.2/src/java/org/datanucleus/NucleusContext.java#L161). JDOPersistanceManager factory (1 per JVM) holds on to a NucleusContext reference (https://github.com/datanucleus/datanucleus-api-jdo/blob/3.2/src/java/org/datanucleus/api/jdo/JDOPersistenceManagerFactory.java#L115). Until we call  NucleusContext#close, the classloader cache is not cleared. In case of UDFs this can lead to permgen leak, as shown in the attached screenshot, where NucleusContext holds on to several URLClassloader objects."
HIVE-11470,NPE in DynamicPartFileRecordWriterContainer on null part-keys.,"When partitioning data using {{HCatStorer}}, one sees the following NPE, if the dyn-part-key is of null-value:

{noformat}
2015-07-30 23:59:59,627 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.lang.NullPointerException
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.NullPointerException
at org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)
at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)
at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)
at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)
at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)
at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)
... 11 more
{noformat}

The reason is that the {{DynamicPartitionFileRecordWriterContainer}} makes an unfortunate assumption when fetching a local file-writer instance:

{code:title=DynamicPartitionFileRecordWriterContainer.java}
  @Override
  protected LocalFileWriter getLocalFileWriter(HCatRecord value) 
    throws IOException, HCatException {
    
    OutputJobInfo localJobInfo = null;
    // Calculate which writer to use from the remaining values - this needs to
    // be done before we delete cols.
    List<String> dynamicPartValues = new ArrayList<String>();
    for (Integer colToAppend : dynamicPartCols) {
      dynamicPartValues.add(value.get(colToAppend).toString()); // <-- YIKES!
    }
    ...
  }
{code}

Must check for null, and substitute with {{""\_\_HIVE_DEFAULT_PARTITION\_\_""}}, or equivalent."
HIVE-11449,"""Capacity must be a power of two"" error when HybridHashTableContainer memory threshold is too low","Currently it only logs a warning message:

{code}
  public static int calcNumPartitions(long memoryThreshold, long dataSize, int minNumParts,
      int minWbSize, HybridHashTableConf nwayConf) throws IOException {
    int numPartitions = minNumParts;

    if (memoryThreshold < minNumParts * minWbSize) {
      LOG.warn(""Available memory is not enough to create a HybridHashTableContainer!"");
    }
{code}

Because we only log a warning, processing continues and hits a hard-to-diagnose error (log below also includes extra logging I added to help track this down). We should probably just fail the query a useful logging message instead.
{noformat}
2015-07-30 18:49:29,696 [pool-1269-thread-8()] WARN org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer: Available memory is not enough to create HybridHashTableContainers consistently!
2015-07-30 18:49:29,696 [pool-1269-thread-8()] ERROR org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap: *** initialCapacity 1: 100000
2015-07-30 18:49:29,696 [pool-1269-thread-8()] ERROR org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap: *** initialCapacity 2: 131072
2015-07-30 18:49:29,696 [pool-1269-thread-8()] ERROR org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap: *** maxCapacity: 0
2015-07-30 18:49:29,696 [pool-1269-thread-8()] ERROR org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap: *** initialCapacity 3: 0
2015-07-30 18:49:29,699 [TezTaskRunner_attempt_1437197396589_0685_1_49_000000_2(attempt_1437197396589_0685_1_49_000000_2)] ERROR org.apache.hadoop.hive.ql.exec.tez.TezProcessor: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:258)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:168)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:157)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:349)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Async initialization failed
	at org.apache.hadoop.hive.ql.exec.Operator.completeInitialization(Operator.java:419)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:389)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:514)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:467)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:379)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:243)
	... 15 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: Capacity must be a power of two
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:188)
	at org.apache.hadoop.hive.ql.exec.Operator.completeInitialization(Operator.java:409)
	... 20 more
Caused by: java.lang.AssertionError: Capacity must be a power of two
	at org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.validateCapacity(BytesBytesMultiHashMap.java:573)
	at org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.<init>(BytesBytesMultiHashMap.java:178)
	at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer$HashPartition.<init>(HybridHashTableContainer.java:120)
	at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.<init>(HybridHashTableContainer.java:296)
	at org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.<init>(HybridHashTableContainer.java:222)
	at org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(HashTableLoader.java:188)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:295)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:178)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator$1.call(MapJoinOperator.java:174)
	at org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.retrieve(LlapObjectCache.java:105)
	at org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache$1.call(LlapObjectCache.java:132)
	... 4 more
{noformat}"
HIVE-11433,NPE for a multiple inner join query,"NullPointException is thrown for query that has multiple (greater than 3) inner joins. Stacktrace for 1.1.0
{code}
NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.ParseUtils.getIndex(ParseUtils.java:149)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.checkJoinFilterRefersOneAlias(ParseUtils.java:166)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.checkJoinFilterRefersOneAlias(ParseUtils.java:185)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.checkJoinFilterRefersOneAlias(ParseUtils.java:185)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.mergeJoins(SemanticAnalyzer.java:8257)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.mergeJoinTree(SemanticAnalyzer.java:8422)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9805)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9714)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10150)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10161)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10078)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1104)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:386)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:373)
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:486)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}.
However, the problem can also be reproduced in latest master branch. Further investigation shows that the following code (in ParseUtils.java) is problematic:
{code}
  static int getIndex(String[] list, String elem) {
    for(int i=0; i < list.length; i++) {
      if (list[i].toLowerCase().equals(elem)) {
        return i;
      }
    }
    return -1;
  }
{code}
The code assumes that every element in the list is not null, which isn't true because of the following code in SemanticAnalyzer.java (method genJoinTree()):
{code}
    if ((right.getToken().getType() == HiveParser.TOK_TABREF)
        || (right.getToken().getType() == HiveParser.TOK_SUBQUERY)
        || (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
      String tableName = getUnescapedUnqualifiedTableName((ASTNode) right.getChild(0))
          .toLowerCase();
      String alias = extractJoinAlias(right, tableName);
      String[] rightAliases = new String[1];
      rightAliases[0] = alias;
      joinTree.setRightAliases(rightAliases);
      String[] children = joinTree.getBaseSrc();
      if (children == null) {
        children = new String[2];
      }
      children[1] = alias;
      joinTree.setBaseSrc(children);
      joinTree.setId(qb.getId());
      joinTree.getAliasToOpInfo().put(
          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));
      // remember rhs table for semijoin
      if (joinTree.getNoSemiJoin() == false) {
        joinTree.addRHSSemijoin(alias);
      }
    } else {
{code}.
Specifically, this code can result a null element as base source:
{code}
      if (children == null) {
        children = new String[2];
      }
      children[1] = alias;
{code}
This appears to be a regression from earlier release (0.14.1). However, it's unclear which commit caused this."
HIVE-11380,NPE when FileSinkOperator is not initialized,"When FileSinkOperator's initializeOp is not called (which may happen when an operator before FileSinkOperator initializeOp failed), FileSinkOperator will throw NPE at close time. The stacktrace:
{noformat}
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException

at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:523)

at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:952)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:199)

at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)

at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)

at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)

at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)

at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)

at java.util.concurrent.FutureTask.run(FutureTask.java:262)

at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.NullPointerException

at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:519)

... 18 more
{noformat}
This Exception is misleading and often distracts users from finding real issues. "
HIVE-11371,Null pointer exception for nested table query when using ORC versus text,"Following query will fail if the file format is ORC 

select tj1rnum, tj2rnum, tjoin3.rnum as rnumt3 from   (select tjoin1.rnum tj1rnum, tjoin2.rnum tj2rnum, tjoin2.c1 tj2c1  from tjoin1 left outer join tjoin2 on tjoin1.c1 = tjoin2.c1 ) tj  left outer join tjoin3 on tj2c1 = tjoin3.c1 


aused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow$LongCopyRow.copy(VectorCopyRow.java:60)
	at org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow.copyByReference(VectorCopyRow.java:260)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultMultiValue(VectorMapJoinGenerateResultOperator.java:238)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.finishOuter(VectorMapJoinOuterGenerateResultOperator.java:495)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.process(VectorMapJoinOuterLongOperator.java:430)
	... 22 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1437788144883_0004_2_02 [Map 1] killed/failed due to:null]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
SQLState:  08S01
ErrorCode: 2

getDatabaseProductName	Apache Hive
getDatabaseProductVersion	1.2.1.2.3.0.0-2557
getDriverName	Hive JDBC
getDriverVersion	1.2.1.2.3.0.0-2557
getDriverMajorVersion	1
getDriverMinorVersion	2



create table  if not exists TJOIN1 (RNUM int , C1 int, C2 int)
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc 					;


create table  if not exists TJOIN2 (RNUM int , C1 int, C2 char(2))
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc ;


create table  if not exists TJOIN3 (RNUM int , C1 int, C2 char(2))
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc ;


create table  if not exists TJOIN4 (RNUM int , C1 int, C2 char(2))
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc ;


"
HIVE-11260,LLAP: NPE in AMReporter,"{noformat}
2015-07-14 15:14:36,583 [ExecutionCompletionThread #0()] ERROR org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable: TezTaskRunner execution failed for : AppId=application_1435700346116_1882, containerId=container_222212222_1882_01_002033, Dag=sershe_20150714151421_0d6c548d-077e-407c-a5ef-d86b6a830a73:14, Vertex=Map 1, FragmentNum=66, Attempt=2
java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.daemon.impl.AMReporter.unregisterTask(AMReporter.java:207)
        at org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.callInternal(TaskRunnerCallable.java:152)
        at org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.callInternal(TaskRunnerCallable.java:74)
        at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}"
HIVE-11222,LLAP: occasional NPE in parallel queries in ORC reader,"{noformat}
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:275)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.nextCvb(LlapInputFormat.java:227)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:155)
        at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:101)
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
        ... 22 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$SargApplier.pickRowGroups(RecordReaderImpl.java:709)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.determineRgsToRead(OrcEncodedDataReader.java:618)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:195)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:59)
        at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37)
        ... 4 more
{noformat}"
HIVE-11221,"In Tez mode, alter table concatenate orc files can intermittently fail with NPE","We are not waiting for input ready events which can trigger occasional NPE if input is not actually ready.

Stacktrace:
{code}
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileTezProcessor.run(MergeFileTezProcessor.java:42)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:265)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:478)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:471)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:648)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:146)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.<init>(MRReaderMapred.java:73)
	at org.apache.tez.mapreduce.input.MRInput.initializeInternal(MRInput.java:483)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:108)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.getMRInput(MergeFileRecordProcessor.java:220)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.init(MergeFileRecordProcessor.java:72)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)
	... 13 more
{code}"
HIVE-11216,UDF GenericUDFMapKeys throws NPE when a null map value is passed in,"We can reproduce the problem as below:
{noformat}
hive> show create table map_txt;
OK
CREATE  TABLE `map_txt`(
  `id` int,
  `content` map<int,string>)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
...
Time taken: 0.233 seconds, Fetched: 18 row(s)
hive> select * from map_txt;
OK
1       NULL
Time taken: 0.679 seconds, Fetched: 1 row(s)
hive> select id, map_keys(content) from map_txt;
....
Error during job, obtaining debugging information...
Examining task ID: task_1435534231122_0025_m_000000 (and more) from job job_1435534231122_0025

Task with the most failures(4):
-----
Task ID:
  task_1435534231122_0025_m_000000

URL:
  http://host-10-17-80-40.coe.cloudera.com:8088/taskdetails.jsp?jobid=job_1435534231122_0025&tipid=task_1435534231122_0025_m_000000
-----
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""id"":1,""content"":null}
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:198)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""id"":1,""content"":null}
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:559)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:180)
        ... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating map_keys(content)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
        ... 9 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.evaluate(GenericUDFMapKeys.java:64)
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:166)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:79)
        ... 13 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL
hive>
{noformat}

The error is as below (in mappers):
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.evaluate(GenericUDFMapKeys.java:64)
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:166)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
        at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.getNewKey(KeyWrapperFactory.java:113)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:778)
        ... 17 more
{noformat}

Looking at the source code:
{code}
  public Object evaluate(DeferredObject[] arguments) throws HiveException {
    retArray.clear();
    Object mapObj = arguments[0].get();
    retArray.addAll(mapOI.getMap(mapObj).keySet());
    return retArray;
  }
{code}
It is obvious that we will have a NPE when a NULL map value is passed in"
HIVE-11215,Vectorized grace hash-join throws FileUtil warnings,"TPC-DS query13 warnings about a null-file deletion.

{code}
2015-07-09 03:14:18,880 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows restored from KeyValueContainer: 31184
2015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Deserializing spilled hash partition...
2015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows in hashmap: 31184
2015-07-09 03:14:18,897 INFO [TezChild] exec.MapJoinOperator: spilled: true abort: false. Clearing spilled partitions.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
{code}"
HIVE-11200,LLAP: Cache BuddyAllocator throws NPE,"Built off da1e0cf21aeff0a9501c5e220a6f66ba61f6da94 merge point

{code}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator$Arena.allocateWithSplit(BuddyAllocator.java:331)
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator$Arena.allocateWithExpand(BuddyAllocator.java:399)
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator$Arena.access$300(BuddyAllocator.java:228)
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:156)
        at org.apache.hadoop.hive.ql.io.orc.InStream.readEncodedStream(InStream.java:761)
        at org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:462)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:342)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:59)
        at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37)
        ... 4 more
2015-07-08 01:17:42,798 [TezTaskRunner_attempt_1435700346116_1212_4_05_000080_0(attempt_1435700346116_1212_4_05_000080_0)] ERROR org.apache.hadoop.hive.ql.exec.tez.TezProcessor: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException
{code}"
HIVE-11062,Remove Exception stacktrace from Log.info when ACL is not supported.,"When logging set to info, Extended ACL Enabled and the file system does not support ACL, there are a lot of Exception stack trace in the log file. Although it is benign, it can easily make users frustrated. We should set the level to show the Exception in debug. 
Current, the Exception in the log looks like:
{noformat}
2015-06-19 05:09:59,376 INFO org.apache.hadoop.hive.shims.HadoopShimsSecure: Skipping ACL inheritance: File system for path s3a://yibing/hive does not support ACLs but dfs.namenode.acls.enabled is set to true: java.lang.UnsupportedOperationException: S3AFileSystem doesn't support getAclStatus
java.lang.UnsupportedOperationException: S3AFileSystem doesn't support getAclStatus
	at org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)
	at org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:729)
	at org.apache.hadoop.hive.ql.metadata.Hive.inheritFromTable(Hive.java:2786)
	at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:2694)
	at org.apache.hadoop.hive.ql.metadata.Table.replaceFiles(Table.java:640)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1587)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:297)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1638)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1397)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1181)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1042)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:145)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:209)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}
"
HIVE-11015,"LLAP: MiniTez tez_smb_main, tez_bmj_schema_evolution fail with NPE","Didn't spend a lot of time investigating
{noformat}
2015-06-15 17:00:49,334 ERROR [main]: SessionState (SessionState.java:printError(984)) - Vertex failed, vertexName=Map 2, vertexId=vertex_1434412732572_0002_5_01, diagnostics=[Task failed, taskId=task_1434412732572_0002_5_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task: attempt_1434412732572_0002_5_01_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:181)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:146)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:349)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:255)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:157)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapOperator.getNominalPath(MapOperator.java:399)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeContexts(MapOperator.java:644)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getKeyValueReader(MapRecordProcessor.java:293)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.initializeMapRecordSources(MapRecordProcessor.java:269)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:223)
	... 15 more
{noformat}"
HIVE-11013,MiniTez tez_join_hash test on the branch fails with NPE (initializeOp not called?),"Line numbers are shifted due to logging; the NPE is at 
{noformat}
        hashMapRowGetters = new ReusableGetAdaptor[mapJoinTables.length];
{noformat}
So looks like mapJoinTables is null.
I added logging to see if they could be set to null from cache, but that doesn't seem to be the case.
Looks like initializeOp is not called. 



{noformat}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception from MapJoinOperator : null
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:428)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:872)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:87)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:872)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(CommonJoinOperator.java:643)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:656)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:659)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:755)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:315)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:278)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:271)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.process(CommonMergeJoinOperator.java:257)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:361)
	... 17 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:339)
	... 29 more
{noformat}"
HIVE-11011,LLAP: test auto_sortmerge_join_5 on MiniTez fails with NPE,"Original issue here was fixed by TEZ-2568.
The new issue is:
{noformat}
2015-07-01 15:53:44,374 ERROR [main]: SessionState (SessionState.java:printError(987)) - Vertex failed, vertexName=Map 2, vertexId=vertex_1435791127343_0002_2_00, diagnostics=[Task failed, taskId=task_1435791127343_0002_2_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task: attempt_1435791127343_0002_2_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:181)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:146)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:349)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:71)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:60)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:60)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:255)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:157)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeLocalWork(CommonMergeJoinOperator.java:631)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeLocalWork(Operator.java:439)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeLocalWork(Operator.java:439)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeLocalWork(Operator.java:439)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:221)
	... 15 more
{noformat}"
HIVE-10963,Hive throws NPE rather than meaningful error message when window is missing,"{{select sum(salary) over w1 from emp;}} throws NPE rather than meaningful error message like ""missing window"".

And also give the right window name rather than the classname in the error message after NPE issue is fixed.
{noformat}
org.apache.hadoop.hive.ql.parse.SemanticException: Window Spec org.apache.hadoop.hive.ql.parse.WindowingSpec$WindowSpec@7954e1de refers to an unknown source
{noformat}"
HIVE-10961,LLAP: ShuffleHandler + Submit work init race condition,"When flexing in a new node, it accepts DAG requests before the shuffle handler is setup, causing fatals

{code}
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:2
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1433459966952_0729_1_00, diagnostics=[Task failed, taskId=task_1t
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.get(ShuffleHandler.java:353)
        at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.submitWork(ContainerRunnerImpl.java:192)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.submitWork(LlapDaemon.java:301)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.submitWork(LlapDaemonProtocolServerImpl.java:75)
        at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(LlapDaemonProtocolProtos.java:12094)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:972)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2085)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1654)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2081)
], TaskAttempt 1 failed, info=[org.apache.hadoop.ipc.RemoteException(java.lang.IllegalStateException): ShuffleHandler must be started before invoking get
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.get(ShuffleHandler.java:353)
        at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.submitWork(ContainerRunnerImpl.java:192)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.submitWork(LlapDaemon.java:301)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.submitWork(LlapDaemonProtocolServerImpl.java:75)
        at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(LlapDaemonProtocolProtos.java:12094)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:972)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2085)
{code}"
HIVE-10956,HS2 leaks HMS connections,"HS2 uses threadlocal to cache HMS client in class Hive. When the thread is dead, the HMS client is not closed. So the connection to the HMS is leaked."
HIVE-10906,Value based UDAF function without orderby expression throws NPE,"The following query throws NPE.
{noformat}
select key, value, min(value) over (partition by key range between unbounded preceding and current row) from small;
FAILED: NullPointerException null


2015-06-03 13:48:09,268 ERROR [main]: ql.Driver (SessionState.java:printError(957)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.WindowingSpec.validateValueBoundary(WindowingSpec.java:293)
        at org.apache.hadoop.hive.ql.parse.WindowingSpec.validateWindowFrame(WindowingSpec.java:281)
        at org.apache.hadoop.hive.ql.parse.WindowingSpec.validateAndMakeEffective(WindowingSpec.java:155)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genWindowingPlan(SemanticAnalyzer.java:11965)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8910)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8868)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9713)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9606)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10079)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:327)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10090)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:208)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1124)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1172)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1061)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1051)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}
"
HIVE-10895,"ObjectStore does not close Query objects in some calls, causing a potential leak in some metastore db resources","During testing, we've noticed Oracle db running out of cursors. Might be related to this."
HIVE-10889,LLAP: HIVE-10778 has NPE,
HIVE-10816,NPE in ExecDriver::handleSampling when submitted via child JVM,"When {{hive.exec.submitviachild = true}}, parallel order by fails with NPE and falls back to single-reducer mode. Stack trace:
{noformat}
2015-05-25 08:41:04,446 ERROR [main]: mr.ExecDriver (ExecDriver.java:execute(386)) - Sampling error
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}"
HIVE-10793,Hybrid Hybrid Grace Hash Join : Don't allocate all hash table memory upfront,"HybridHashTableContainer will allocate memory based on estimate, which means if the actual is less than the estimate the allocated memory won't be used.

Number of partitions is calculated based on estimated data size
{code}
numPartitions = calcNumPartitions(memoryThreshold, estimatedTableSize, minNumParts, minWbSize,
          nwayConf);
{code}

Then based on number of partitions writeBufferSize is set

{code}
writeBufferSize = (int)(estimatedTableSize / numPartitions);
{code}

Each hash partition will allocate 1 WriteBuffer, with no further allocation if the estimate data size is correct.

Suggested solution is to reduce writeBufferSize by a factor such that only X% of the memory is preallocated.

"
HIVE-10781,HadoopJobExecHelper Leaks RunningJobs,"On one of our busy hadoop cluster, hiveServer2 holds more than 4000 org.apache.hadoop.mapred.JobClient$NetworkedJob instances,while only has less than 3 backgroud handler thread at the same time.
All these instances are hold in one LinkedList from org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper's  runningJobs property,which is static."
HIVE-10765,LLAP: NPE when calling abort on the TezProcessor,"{code}
2015-05-19 19:48:42,827 [Wait-Queue-Scheduler-0(null)] ERROR org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: Wait queue scheduler worker exited with failure!
java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.abort(TezProcessor.java:177)
  at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.abortTask(LogicalIOProcessorRuntimeTask.java:698)
  at org.apache.tez.runtime.task.TaskRunner2Callable.interruptTask(TaskRunner2Callable.java:118)
  at org.apache.tez.runtime.task.TezTaskRunner2.killTask(TezTaskRunner2.java:261)
  at org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.killTask(TaskRunnerCallable.java:240)
  at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.trySchedule(TaskExecutorService.java:262)
  at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$700(TaskExecutorService.java:64)
  at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$WaitQueueWorker.run(TaskExecutorService.java:162)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
  at java.util.concurrent.FutureTask.run(FutureTask.java:262)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
{code}

rrProc should be volatile. There likely need to be some checks around it to ensure it's setup."
HIVE-10759,LLAP: Add aging to wait queue tasks,Wait queue priority does not increase the priority (aging) of pre-empted tasks making them starve in concurrent tests.
HIVE-10721,SparkSessionManagerImpl leaks SparkSessions [Spark Branch],"In #getSession(), we create a SparkSession and save it in a set. If the session is failed to open, it will stay in the set till shutdown."
HIVE-10595,Dropping a table can cause NPEs in the compactor,"Reproduction:
# start metastore with compactor off
# insert enough entries in a table to trigger a compaction
# drop the table
# stop metastore
# restart metastore with compactor on

Result:  NPE in the compactor threads.  I suspect this would also happen if the inserts and drops were done in between a run of the compactor, but I haven't proven it."
HIVE-10560,LLAP: a different NPE in shuffle,"Lots of those in Query 1 logs; ran just now on 8 daemons on recent version.
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.unregisterDag(ShuffleHandler.java:437)
        at org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.queryComplete(QueryTracker.java:81)
        at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.queryComplete(ContainerRunnerImpl.java:214)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.queryComplete(LlapDaemon.java:271)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.queryComplete(LlapDaemonProtocolServerImpl.java:94)
        at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(LlapDaemonProtocolProtos.java:12278)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:972)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2088)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2084)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2082)
{noformat}"
HIVE-10538,Fix NPE in FileSinkOperator from hashcode mismatch,"A Null Pointer Exception occurs when in FileSinkOperator when using bucketed tables and distribute by with multiFileSpray enabled. The following snippet query reproduces this issue:

{code}
set hive.enforce.bucketing = true;
set hive.exec.reducers.max = 20;

create table bucket_a(key int, value_a string) clustered by (key) into 256 buckets;
create table bucket_b(key int, value_b string) clustered by (key) into 256 buckets;
create table bucket_ab(key int, value_a string, value_b string) clustered by (key) into 256 buckets;

-- Insert data into bucket_a and bucket_b

insert overwrite table bucket_ab
select a.key, a.value_a, b.value_b from bucket_a a join bucket_b b on (a.key = b.key) distribute by key;
{code}

The following stack trace is logged.

{code}
2015-04-29 12:54:12,841 FATAL [pool-110-thread-1]: ExecReducer (ExecReducer.java:reduce(255)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":""113"",""_col1"":""val_113""}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)
	... 8 more
{code}"
HIVE-10527,NPE in SparkUtilities::isDedicatedCluster [Spark Branch],We should add {{spark.master}} to HiveConf when it doesn't exist.
HIVE-10500,Repeated deadlocks in underlying RDBMS cause transaction or lock failure,"In some cases in a busy system, deadlocks in the metastore RDBMS can cause failures in Hive locks and transactions when using DbTxnManager"
HIVE-10483,insert overwrite partition deadlocks on itself with DbTxnManager,"insert overwrite table ta partition(part=xxxx) select xxx from tb join ta where part=xxxx

It seems like the Shared conflicts with the Exclusive lock for Insert Overwrite even though both are part of the same txn.
More precisely insert overwrite requires X lock on partition and the read side needs an S lock on the query.

A simpler case is
insert overwrite table ta partition(part=xxxx) select * from ta"
HIVE-10470,LLAP: NPE in IO when returning 0 rows with no projection,"Looks like a trivial fix, unless I'm missing something. I may do it later if you don't ;)

{noformat}
aused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.EncodedTreeReaderFactory.createEncodedTreeReader(EncodedTreeReaderFactory.java:1764)
	at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.decodeBatch(OrcEncodedDataConsumer.java:92)
	at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.decodeBatch(OrcEncodedDataConsumer.java:39)
	at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:116)
	at org.apache.hadoop.hive.llap.io.decode.EncodedDataConsumer.consumeData(EncodedDataConsumer.java:36)
	at org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:329)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:299)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:55)
	at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37)
	... 4 more
{noformat}

Running q file
{noformat}
SET hive.vectorized.execution.enabled=true;

SET hive.llap.io.enabled=false;

SET hive.exec.orc.default.row.index.stride=1000;
SET hive.optimize.index.filter=true;

DROP TABLE orc_llap;

CREATE TABLE orc_llap(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN)
    STORED AS ORC tblproperties (""orc.compress""=""ZLIB"");

insert into table orc_llap
select ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2
from alltypesorc limit 10;

SET hive.llap.io.enabled=true;

select count(*) from orc_llap where cint < 60000000;

DROP TABLE orc_llap;
{noformat}"
HIVE-10456,Grace Hash Join should not load spilled partitions on abort,Grace Hash Join loads the spilled partitions to complete the join in closeOp(). This should not happen when closeOp with abort is invoked. Instead it should clean up all the spilled data.
HIVE-10453,HS2 leaking open file descriptors when using UDFs,"1. create a custom function by
CREATE FUNCTION myfunc AS 'someudfclass' using jar 'hdfs:///tmp/myudf.jar';
2. Create a simple jdbc client, just do 
connect, 
run simple query which using the function such as:
select myfunc(col1) from sometable
3. Disconnect.
Check open file for HiveServer2 by:
lsof -p HSProcID | grep myudf.jar
You will see the leak as:
{noformat}
java      28718 ychen  txt      REG                1,4        741 212977666 /private/var/folders/6p/7_njf13d6h144wldzbbsfpz80000gp/T/1bfe3de0-ac63-4eba-a725-6a9840f1f8d5_resources/myudf.jar
java      28718 ychen  330r     REG                1,4        741 212977666 /private/var/folders/6p/7_njf13d6h144wldzbbsfpz80000gp/T/1bfe3de0-ac63-4eba-a725-6a9840f1f8d5_resources/myudf.jar
{noformat}
"
HIVE-10446,Hybrid Hybrid Grace Hash Join : java.lang.IllegalArgumentException in Kryo while spilling big table,"TPC-DS Q85 fails with Kryo exception when spilling big table data.

Query 
{code}
select  substr(r_reason_desc,1,20) as r
       ,avg(wr_return_ship_cost) wq
       ,avg(wr_refunded_cash) ref
       ,avg(wr_fee) fee
 from web_returns, customer_demographics cd1,
      customer_demographics cd2, customer_address, date_dim, reason 
 where 
   cd1.cd_demo_sk = web_returns.wr_refunded_cdemo_sk 
   and cd2.cd_demo_sk = web_returns.wr_returning_cdemo_sk
   and customer_address.ca_address_sk = web_returns.wr_refunded_addr_sk
   and reason.r_reason_sk = web_returns.wr_reason_sk
   and cd1.cd_marital_status = cd2.cd_marital_status
   and cd1.cd_education_status = cd2.cd_education_status
group by r_reason_desc
order by r, wq, ref, fee
limit 100
{code}

Plan 
{code}
OK
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE), Map 5 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE), Map 7 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
      DagName: mmokhtar_20150422165209_d8eb5634-c19f-4576-9525-cad248c7ca37:5
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: web_returns
                  filterExpr: (((wr_refunded_addr_sk is not null and wr_reason_sk is not null) and wr_refunded_cdemo_sk is not null) and wr_returning_cdemo_sk is not null) (type: boolean)
                  Statistics: Num rows: 2062802370 Data size: 185695406284 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (((wr_refunded_addr_sk is not null and wr_reason_sk is not null) and wr_refunded_cdemo_sk is not null) and wr_returning_cdemo_sk is not null) (type: boolean)
                    Statistics: Num rows: 1875154723 Data size: 51267313780 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: wr_refunded_cdemo_sk (type: int), wr_refunded_addr_sk (type: int), wr_returning_cdemo_sk (type: int), wr_reason_sk (type: int), wr_fee (type: float), wr_return_ship_cost (type: float), wr_refunded_cash (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Statistics: Num rows: 1875154723 Data size: 51267313780 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col1 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0, _col2, _col3, _col4, _col5, _col6
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 1875154688 Data size: 45003712512 Basic stats: COMPLETE Column stats: COMPLETE
                        HybridGraceHashJoin: true
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          keys:
                            0 _col3 (type: int)
                            1 _col0 (type: int)
                          outputColumnNames: _col0, _col2, _col4, _col5, _col6, _col9
                          input vertices:
                            1 Map 5
                          Statistics: Num rows: 1875154688 Data size: 219393098496 Basic stats: COMPLETE Column stats: COMPLETE
                          HybridGraceHashJoin: true
                          Map Join Operator
                            condition map:
                                 Inner Join 0 to 1
                            keys:
                              0 _col0 (type: int)
                              1 _col0 (type: int)
                            outputColumnNames: _col2, _col4, _col5, _col6, _col9, _col11, _col12
                            input vertices:
                              1 Map 6
                            Statistics: Num rows: 1875154688 Data size: 547545168896 Basic stats: COMPLETE Column stats: COMPLETE
                            HybridGraceHashJoin: true
                            Map Join Operator
                              condition map:
                                   Inner Join 0 to 1
                              keys:
                                0 _col2 (type: int), _col11 (type: string), _col12 (type: string)
                                1 _col0 (type: int), _col1 (type: string), _col2 (type: string)
                              outputColumnNames: _col4, _col5, _col6, _col9
                              input vertices:
                                1 Map 7
                              Statistics: Num rows: 402058172 Data size: 43824340748 Basic stats: COMPLETE Column stats: COMPLETE
                              HybridGraceHashJoin: true
                              Select Operator
                                expressions: _col9 (type: string), _col5 (type: float), _col6 (type: float), _col4 (type: float)
                                outputColumnNames: _col0, _col1, _col2, _col3
                                Statistics: Num rows: 402058172 Data size: 43824340748 Basic stats: COMPLETE Column stats: COMPLETE
                                Group By Operator
                                  aggregations: avg(_col1), avg(_col2), avg(_col3)
                                  keys: _col0 (type: string)
                                  mode: hash
                                  outputColumnNames: _col0, _col1, _col2, _col3
                                  Statistics: Num rows: 10975 Data size: 1064575 Basic stats: COMPLETE Column stats: COMPLETE
                                  Reduce Output Operator
                                    key expressions: _col0 (type: string)
                                    sort order: +
                                    Map-reduce partition columns: _col0 (type: string)
                                    Statistics: Num rows: 10975 Data size: 1064575 Basic stats: COMPLETE Column stats: COMPLETE
                                    value expressions: _col1 (type: struct<count:bigint,sum:double,input:float>), _col2 (type: struct<count:bigint,sum:double,input:float>), _col3 (type: struct<count:bigint,sum:double,input:float>)
            Execution mode: vectorized
        Map 4
            Map Operator Tree:
                TableScan
                  alias: customer_address
                  filterExpr: ca_address_sk is not null (type: boolean)
                  Statistics: Num rows: 40000000 Data size: 40595195284 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ca_address_sk is not null (type: boolean)
                    Statistics: Num rows: 40000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ca_address_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 40000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 40000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5
            Map Operator Tree:
                TableScan
                  alias: reason
                  filterExpr: r_reason_sk is not null (type: boolean)
                  Statistics: Num rows: 72 Data size: 14400 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: r_reason_sk is not null (type: boolean)
                    Statistics: Num rows: 72 Data size: 7272 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: r_reason_sk (type: int), r_reason_desc (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 72 Data size: 7272 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 72 Data size: 7272 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 6
            Map Operator Tree:
                TableScan
                  alias: cd1
                  filterExpr: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                  Statistics: Num rows: 1920800 Data size: 718379200 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                    Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cd_demo_sk (type: int), cd_marital_status (type: string), cd_education_status (type: string)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string), _col2 (type: string)
            Execution mode: vectorized
        Map 7
            Map Operator Tree:
                TableScan
                  alias: cd1
                  filterExpr: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                  Statistics: Num rows: 1920800 Data size: 718379200 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                    Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cd_demo_sk (type: int), cd_marital_status (type: string), cd_education_status (type: string)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: string), _col2 (type: string)
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: string), _col2 (type: string)
                        Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), avg(VALUE._col1), avg(VALUE._col2)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 25 Data size: 3025 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: substr(_col0, 1, 20) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)
                    sort order: ++++
                    Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                    TopN Hash Memory Usage: 0.04
        Reducer 3
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: double), KEY.reducesinkkey2 (type: double), KEY.reducesinkkey3 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 100
                  Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 100
      Processor Tree:
        ListSink
{code}

Exception 
{code}
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:290)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: output cannot be null.
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:411)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.process(VectorMapJoinOperator.java:287)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:138)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:114)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:97)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:162)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
	... 18 more
Caused by: java.lang.IllegalArgumentException: output cannot be null.
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:601)
	at org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.add(ObjectContainer.java:101)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.spillBigTableRow(MapJoinOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.spillBigTableRow(VectorMapJoinOperator.java:307)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:390)
	... 27 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_3652_3_04 [Map 1] killed/failed due to:null]Vertex killed, vertexName=Reducer 3, vertexId=vertex_1426707664723_3652_3_06, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_3652_3_06 [Reducer 3] killed/failed due to:null]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1426707664
{code}"
HIVE-10428,NPE in RegexSerDe using HCat,"When HCatalog calls to table with ""org.apache.hadoop.hive.serde2.RegexSerDe"", when doing Hcatalog call to get read the table, it throws exception:

{noformat}
15/04/21 14:07:31 INFO security.TokenCache: Got dt for hdfs://hdpsecahdfs; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:hdpsecahdfs, Ident: (HDFS_DELEGATION_TOKEN token 1478 for haha)
15/04/21 14:07:31 INFO mapred.FileInputFormat: Total input paths to process : 1
Splits len : 1
SplitInfo : [hdpseca03.seca.hwxsup.com, hdpseca04.seca.hwxsup.com, hdpseca05.seca.hwxsup.com]
15/04/21 14:07:31 INFO mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.RegexSerDe with properties {name=casetest.regex_table, numFiles=1, columns.types=string,string, serialization.format=1, columns=id,name, rawDataSize=0, numRows=0, output.format.string=%1$s %2$s, serialization.lib=org.apache.hadoop.hive.serde2.RegexSerDe, COLUMN_STATS_ACCURATE=true, totalSize=25, serialization.null.format=\N, input.regex=([^ ]*) ([^ ]*), transient_lastDdlTime=1429590172}
15/04/21 14:07:31 WARN serde2.RegexSerDe: output.format.string has been deprecated
Exception in thread ""main"" java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
	at com.google.common.base.Splitter.split(Splitter.java:371)
	at org.apache.hadoop.hive.serde2.RegexSerDe.initialize(RegexSerDe.java:155)
	at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:49)
	at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:518)
	at org.apache.hive.hcatalog.mapreduce.InternalUtil.initializeDeserializer(InternalUtil.java:156)
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.createDeserializer(HCatRecordReader.java:127)
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.initialize(HCatRecordReader.java:92)
	at HCatalogSQLMR.main(HCatalogSQLMR.java:81)
{noformat}"
HIVE-10408,LLAP: NPE in scheduler in case of rejected tasks,"{noformat}
java.lang.NullPointerException
        at org.apache.tez.dag.app.rm.LlapTaskSchedulerService.deallocateTask(LlapTaskSchedulerService.java:388)
        at org.apache.tez.dag.app.rm.TaskSchedulerEventHandler.handleTASucceeded(TaskSchedulerEventHandler.java:339)
        at org.apache.tez.dag.app.rm.TaskSchedulerEventHandler.handleEvent(TaskSchedulerEventHandler.java:224)
        at org.apache.tez.dag.app.rm.TaskSchedulerEventHandler$1.run(TaskSchedulerEventHandler.java:493)
{noformat}

The query, running alone on 10-node cluster, dumped 1000 mappers into running; with 3 completed it failed with that."
HIVE-10403,Add n-way join support for Hybrid Grace Hash Join,Currently Hybrid Grace Hash Join only supports 2-way join (one big table and one small table). This task will enable n-way join (one big table and multiple small tables).
HIVE-10333,LLAP: NPE due to failure to find position,"{noformat}
java.lang.NullPointerException 
at org.apache.hadoop.hive.ql.io.orc.InStream.findCompressedPosition(InStream.java:793) 
at org.apache.hadoop.hive.ql.io.orc.InStream.uncompressStream(InStream.java:611) 
at org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:346) 
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:290) 
at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:48) 
at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37) ... 4 more 
{noformat}

Example query {noformat}
select * from lineitem where l_orderkey = 1212000001;
{noformat}
"
HIVE-10302,Load small tables (for map join) in executor memory only once [Spark Branch],"Usually there are multiple cores in a Spark executor, and thus it's possible that multiple map-join tasks can be running in the same executor (concurrently or sequentially). Currently, each task will load its own copy of the small tables for map join into memory, ending up with inefficiency. Ideally, we only load the small tables once and share them among the tasks running in that executor."
HIVE-10284,enable container reuse for grace hash join ,
HIVE-10273,Union with partition tables which have no data fails with NPE,"As shown in the test case in the patch below, when we have partitioned tables which have no data, we fail with an NPE with the following stack trace:

{code}
NullPointerException null
java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
  at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:357)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:321)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:307)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:847)
  at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:468)
  at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:223)
  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10170)
  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
  at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
{code}"
HIVE-10225,CLI JLine does not flush history on quit/Ctrl-C,"Hive CLI is not saving history, if hive cli is terminated using a Ctrl-C or ""quit;"".

HIVE-9310 fixed it for the case where one exits with Ctrl-D (EOF), but not for the above ways of exiting.
"
HIVE-10123,Hybrid grace Hash join : Use estimate key count from stats to initialize BytesBytesMultiHashMap,"Hybrid grace Hash join is not using estimated number of rows from the statistics to initialize BytesBytesMultiHashMap. 

Add some logging to BytesBytesMultiHashMap to track get probes and use msec for expandAndRehash as us overflow."
HIVE-10095,format_number udf throws NPE,"For example
{code}
select format_number(cast(null as int), 0);
FAILED: NullPointerException null
{code}"
HIVE-10093,Unnecessary HMSHandler initialization for default MemoryTokenStore on HS2,"When the HiveAuthFactory is constructed in HS2, it initializes a HMSHandler unnecessarily right before the call to: HadoopThriftAuthBridge.startDelegationTokenSecretManager().  If the DelegationTokenStore is configured to be a memoryTokenStore, this step is not needed.

Side effect is creation of useless derby database file on HiveServer2 in secure clusters, causing confusion.  This could potentially be skipped if MemoryTokenStore is used."
HIVE-10072,Add vectorization support for Hybrid Grace Hash Join,This task is to enable vectorization support for Hybrid Grace Hash Join feature.
HIVE-10050,Support overriding memory configuration for AM launched for TempletonControllerJob,The MR AM launched for the TempletonControllerJob does not do any heavy lifting and therefore can be configured to use a small memory footprint ( as compared to potentially using the default footprint for most MR jobs on a cluster ). 
HIVE-10045,LLAP : NPE in DiskRangeList helper init,"{noformat}
hive> select count(*) from store_sales where ss_item_sk not in (select i_item_sk from item);
Warning: Map Join MAPJOIN[32][bigTable=store_sales] in task 'Map 1' is a cross product
Query ID = gunther_20150320161251_1fb94086-90aa-4fd2-b4b2-fca245850238
Total jobs = 1
Launching Job 1 out of 1


Status: Running (Executing on YARN cluster with App id application_1424502260528_1355)

--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1                 KILLED    174          0        0      174       0     174
Map 3 ..........   SUCCEEDED      1          1        0        0       0       0
Map 4                 FAILED      1          0        0        1       4       0
Reducer 2             KILLED      1          0        0        1       0       1
Reducer 5             KILLED      1          0        0        1       0       1
--------------------------------------------------------------------------------
VERTICES: 01/05  [>>--------------------------] 0%    ELAPSED TIME: 0.62 s
--------------------------------------------------------------------------------
Status: Failed
Vertex failed, vertexName=Map 4, vertexId=vertex_1424502260528_1355_5_00, diagnostics=[Task failed, taskId=task_1424502260528_1355_5_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:330)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:71)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:308)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: java.io.IOException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:126)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:113)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:61)
	... 16 more
Caused by: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:249)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.nextCvb(LlapInputFormat.java:201)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:140)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.next(LlapInputFormat.java:96)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
	... 22 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.common.DiskRangeList$DiskRangeListMutateHelper.<init>(DiskRangeList.java:183)
	at org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:220)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:272)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:44)
	at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37)
	... 4 more
{noformat}"
HIVE-10018,Activating SQLStandardAuth results in NPE [hbase-metastore branch],Setting the config to run SQLStandardAuth and then doing even simple SQL statements results in an NPE.
HIVE-10010,Alter table results in NPE [hbase-metastore branch],"Doing an alter table results in:

{code}
2015-03-18 10:45:54,189 ERROR [main]: exec.DDLTask (DDLTask.java:failed(512)) - java.lang.NullPointerException
    at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)
    at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)
    at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)
    at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)
    at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)
    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)
    at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)
{code}"
HIVE-10006,RSC has memory leak while execute multi queries.[Spark Branch],"While execute query with RSC, MapWork/ReduceWork number is increased all the time, and lead to OOM at the end."
HIVE-9976,Possible race condition in DynamicPartitionPruner for <200ms tasks,"Race condition in the DynamicPartitionPruner between DynamicPartitionPruner::processVertex() and DynamicPartitionpruner::addEvent() for tasks which respond with both the result and success in a single heartbeat sequence.

{code}
2015-03-16 07:05:01,589 ERROR [InputInitializer [Map 1] #0] tez.DynamicPartitionPruner: Expecting: 1, received: 0
2015-03-16 07:05:01,590 ERROR [Dispatcher thread: Central] impl.VertexImpl: Vertex Input: store_sales initializer failed, vertex=vertex_1424502260528_1113_4_04 [Map 1]
org.apache.tez.dag.app.dag.impl.AMUserCodeException: org.apache.hadoop.hive.ql.metadata.HiveException: Incorrect event count in dynamic parition pruning
{code}

!llap_vertex_200ms.png!

All 4 upstream vertices of Map 1 need to finish within ~200ms to trigger this, which seems to be consistently happening with LLAP."
HIVE-9953,fix NPE in WindowingTableFunction,"WindowingTableFunction line 1193
{code}
// now
return (s1 == null && s2 == null) || s1.equals(s2);

// should be
return (s1 == null && s2 == null) || (s1 != null && s1.equals(s2));
{code}"
HIVE-9952,fix NPE in CorrelationUtilities,CorrelationUtilities.isNullOperator will throw NPE if operator is null
HIVE-9936,fix potential NPE in DefaultUDAFEvaluatorResolver,"In some cases DefaultUDAFEvaluatorResolver calls new AmbiguousMethodException(udafClass, null, null)  (line 94)
This will throw NPE because AmbiguousMethodException calls argTypeInfos.toString()
argTypeInfos is the second parameter and it can not be null."
HIVE-9929,StatsUtil#getAvailableMemory could return negative value,"In MAPREDUCE-5785, the default value of mapreduce.map.memory.mb is set to -1. We need fix StatsUtil#getAvailableMemory not to return negative value."
HIVE-9886,Hive on tez: NPE when converting join to SMB in sub-query,"{code}
set hive.auto.convert.sortmerge.join = true;

create table t1(
id string,
od string);

create table t2(
id string,
od string);

select vt1.id from
(select rt1.id from
(select t1.id, row_number() over (partition by id order by od desc) as row_no from t1) rt1
where rt1.row_no=1) vt1
join
(select rt2.id from
(select t2.id, row_number() over (partition by id order by od desc) as row_no from t2) rt2
where rt2.row_no=1) vt2
where vt1.id=vt2.id;
{code}

throws NPE:

{code}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.getValueObjectInspectors(AbstractMapJoinOperator.java:96)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:167)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:72)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:89)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:66)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:410)
	at org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:89)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)
	... 14 more
{code}
"
HIVE-9860,MapredLocalTask/SecureCmdDoAs leaks local files,The class {{SecureCmdDoAs}} creates a temp file but does not clean it up.
HIVE-9839,HiveServer2 leaks OperationHandle on async queries which fail at compile phase,"Using beeline to connect to HiveServer2.And type the following:
drop table if exists table_not_exists;
select * from table_not_exists;

There will be an OperationHandle object staying in HiveServer2's memory for ever even after quit from beeline .

"
HIVE-9816,LLAP: Distinguish between memory used by executors and the cache,
HIVE-9811,Hive on Tez leaks WorkMap objects,"TezTask doesn't fully clean gWorkMap, so as result Hive leaks WorkMap objects."
HIVE-9791,insert into table throws NPE,"to reproduce NPE run the following
{code}
create table a as select 'A' letter;
OK

insert into table a select 'B' letter;
FAILED: NullPointerException null

-- works fine if add ""from <table>"" to select statement
insert into table a select 'B' letter from dual;
OK
{code}"
HIVE-9665,Parallel move task optimization causes race condition,"The change in HIVE-8042 doesn't actually work. Running it at scale produces race conditions which lead to broken thrift messages and OOMs. E.g.:

{noformat}
java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:353)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:215)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:1122)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:1108)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:131)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)
	at com.sun.proxy.$Proxy9.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1064)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1006)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:250)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:72)
java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:353)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:215)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:1122)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:1108)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)
	at com.sun.proxy.$Proxy9.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1064)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1006)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:250)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:72)
java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:353)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:215)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:1122)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:1108)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:131)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)
	at com.sun.proxy.$Proxy9.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1064)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1006)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:250)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:72)
{noformat}"
HIVE-9622,Getting NPE when trying to restart HS2 when metastore is configured to use org.apache.hadoop.hive.thrift.DBTokenStore,"# Configure the cluster to use kerberos for HS2 and Metastore.
## http://www.cloudera.com/content/cloudera/en/documentation/cdh4/v4-3-0/CDH4-Security-Guide/cdh4sg_topic_9_1.html
## http://www.cloudera.com/content/cloudera/en/documentation/cdh4/v4-6-0/CDH4-Security-Guide/cdh4sg_topic_9_2.html
# Set hive metastore delegation token to org.apache.hadoop.hive.thrift.DBTokenStore in hive-site.xml
{code}
<property>
     <name>hive.cluster.delegation.token.store.class</name>
     <value>org.apache.hadoop.hive.thrift.DBTokenStore</value>
</property>
{code}
# Then trying to restart hive service, HS2 fails to start the NPE below: 
{code}
9:43:10.711 AM	ERROR	org.apache.hive.service.cli.thrift.ThriftCLIService	
Error: 
org.apache.thrift.transport.TTransportException: Failed to start token manager
	at org.apache.hive.service.auth.HiveAuthFactory.<init>(HiveAuthFactory.java:107)
	at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:51)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to initialize master key
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.startThreads(TokenStoreDelegationTokenSecretManager.java:223)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server.startDelegationTokenSecretManager(HadoopThriftAuthBridge20S.java:438)
	at org.apache.hive.service.auth.HiveAuthFactory.<init>(HiveAuthFactory.java:105)
	... 2 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.startThreads(TokenStoreDelegationTokenSecretManager.java:221)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnRawStore(DBTokenStore.java:145)
	at org.apache.hadoop.hive.thrift.DBTokenStore.addMasterKey(DBTokenStore.java:41)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.logUpdateMasterKey(TokenStoreDelegationTokenSecretManager.java:203)
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:339)
	... 9 more
9:43:10.719 AM	INFO	org.apache.hive.service.server.HiveServer2	
SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down HiveServer2 at a1909.halxg.cloudera.com/10.20.202.109
************************************************************/
{code}

The problem appears that we didn't pass a {{RawStore}} object in the following:

https://github.com/apache/hive/blob/trunk/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java#L111"
HIVE-9617,UDF from_utc_timestamp throws NPE if the second argument is null,"UDF from_utc_timestamp throws NPE if the second argument is null
{code}
select from_utc_timestamp('2015-02-06 10:30:00', cast(null as string));
FAILED: NullPointerException null
{code}"
HIVE-9513,NULL POINTER EXCEPTION,"NPE duting parsing  of :

{noformat}
select * from (
     select * from ( select 1 as id , ""foo"" as str_1 from staging.dual ) f
  union	all
     select * from ( select 2 as id , ""bar"" as str_2 from staging.dual ) g
) e ;
{noformat}"
HIVE-9404,NPE in org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct(),"{noformat}
Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct(TxnHandler.java:1015)

	at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkRetryable(TxnHandler.java:906)

	at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:238)

	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5321)

{noformat}"
HIVE-9361,Intermittent NPE in SessionHiveMetaStoreClient.alterTempTable,"it's happening at 
{noformat}
    MetaStoreUtils.updateUnpartitionedTableStatsFast(newtCopy,
        wh.getFileStatusesForSD(newtCopy.getSd()), false, true);
{noformat}

other methods in this class call getWh() to get Warehouse so this likely explains why it's intermittent."
HIVE-9277,Hybrid Hybrid Grace Hash Join,"We are proposing an enhanced hash join algorithm called _“hybrid hybrid grace hash join”_.

We can benefit from this feature as illustrated below:
* The query will not fail even if the estimated memory requirement is slightly wrong
* Expensive garbage collection overhead can be avoided when hash table grows
* Join execution using a Map join operator even though the small table doesn't fit in memory as spilling some data from the build and probe sides will still be cheaper than having to shuffle the large fact table

The design was based on Hadoop’s parallel processing capability and significant amount of memory available."
HIVE-9234,HiveServer2 leaks FileSystem objects in FileSystem.CACHE,"Running over extended period (48+ hrs), we've noticed HiveServer2 leaking FileSystem objects in FileSystem.CACHE. Linked jiras were previous attempts to fix it, but the issue still seems to be there. A workaround is to disable the caching (by setting {{fs.hdfs.impl.disable.cache}} and {{fs.file.impl.disable.cache}} to {{true}}), but creating new FileSystem objects is expensive."
HIVE-9111,Potential NPE in OrcStruct for list and map types,Currently getters in OrcStruct class for list and map object inspectors does not have null checks which may throw NPE when UDFs like size() is used on list or map column.
HIVE-9073,NPE when using custom windowing UDAFs,"From the hive-user email group:

{noformat}
While executing a simple select query using a custom windowing UDAF I created I am constantly running into this error.
 
Error: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:409)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
        ... 9 more
Caused by: java.lang.RuntimeException: Reduce operator initialization failed
        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:173)
        ... 14 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:647)
        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getWindowFunctionInfo(FunctionRegistry.java:1875)
        at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.streamingPossible(WindowingTableFunction.java:150)
        at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.setCanAcceptInputAsStream(WindowingTableFunction.java:221)
        at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.initializeStreaming(WindowingTableFunction.java:266)
        at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.initializeStreaming(PTFOperator.java:292)
        at org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:86)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:166)
        ... 14 more
 
Just wanted to check if any of you have faced this earlier. Also when I try to run the Custom UDAF on another server it works fine. The only difference I can see it that the hive version I am using on my local machine is 0.13.1 where it is working and on the other machine it is 0.13.0 where I see the above mentioned error. I am not sure if this was a bug which was fixed in the later release but I just wanted to confirm the same.
{noformat}"
HIVE-8811,Dynamic partition pruning can result in NPE during query compilation,Bug in tarjan's algo results in incorrect strongly connected components. I've seen this manifest itself as an NPE in TezCompiler.
HIVE-8798,Some Oracle deadlocks not being caught in TxnHandler,Oracle seems to give different error codes and different error messages at different times for deadlocks.  There are still some error codes/messages we are missing in TxnHandler.
HIVE-8794,Hive on Tez leaks AMs when killed before first dag is run,The shutdown hook that guards against this kind of leakage is only set up when the TezJobMonitor class is loaded. If you kill the shell before that - that might be too late.
HIVE-8778,ORC split elimination can cause NPE when column statistics is null,Row group elimination has protection for NULL statistics values in RecordReaderImpl.evaluatePredicate() which then calls evaluatePredicateRange(). But split elimination directly calls evaluatePredicateRange() without NULL protection. This can lead to NullPointerException when a column is NULL in entire stripe. 
HIVE-8747,Estimate number of rows for table with 0 rows overflows resulting in an in-efficient plan ,"ship_mode table has 0 rows.

Query 
{code}
select count(*) 
from
          web_sales
         ,date_dim
 	  ,ship_mode
     where
 web_sales.ws_sold_date_sk = date_dim.d_date_sk
 	and web_sales.ws_ship_mode_sk = ship_mode.sm_ship_mode_sk
        and d_year = 2002
 	and sm_carrier in ('DIAMOND','AIRBORNE')
{code}

Explain 
{code}
STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE)
        Map 4 <- Map 3 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
      DagName: mmokhtar_20141105180404_59e6fb65-529f-4eaa-9446-7f34d12bffac:30
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: ship_mode
                  filterExpr: ((sm_carrier) IN ('DIAMOND', 'AIRBORNE') and sm_ship_mode_sk is not null) (type: boolean)
                  Statistics: Num rows: 0 Data size: 45 Basic stats: PARTIAL Column stats: COMPLETE
                  Filter Operator
                    predicate: ((sm_carrier) IN ('DIAMOND', 'AIRBORNE') and sm_ship_mode_sk is not null) (type: boolean)
                    Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: sm_ship_mode_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0
                          1
                        keys:
                          0 _col1 (type: int)
                          1 _col0 (type: int)
                        input vertices:
                          0 Map 4
                        Statistics: Num rows: 9223372036854775807 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                        Select Operator
                          Statistics: Num rows: 9223372036854775807 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                          Group By Operator
                            aggregations: count()
                            mode: hash
                            outputColumnNames: _col0
                            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              sort order:
                              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                              value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 3
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 652 Data size: 5216 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 652 Data size: 2608 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 652 Data size: 2608 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4
            Map Operator Tree:
                TableScan
                  alias: web_sales
                  filterExpr: (ws_sold_date_sk is not null and ws_ship_mode_sk is not null) (type: boolean)
                  Statistics: Num rows: 143966864 Data size: 19577477788 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (ws_sold_date_sk is not null and ws_ship_mode_sk is not null) (type: boolean)
                    Statistics: Num rows: 143948856 Data size: 1151518824 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ws_sold_date_sk (type: int), ws_ship_mode_sk (type: int)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 143948856 Data size: 1151518824 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0 {_col1}
                          1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1
                        input vertices:
                          1 Map 3
                        Statistics: Num rows: 1284818 Data size: 5139272 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col1 (type: int)
                          sort order: +
                          Map-reduce partition columns: _col1 (type: int)
                          Statistics: Num rows: 1284818 Data size: 5139272 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: bigint)
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
{code}"
HIVE-8711,"DB deadlocks not handled in TxnHandler for Postgres, Oracle, and SQLServer","TxnHandler.detectDeadlock has code to catch deadlocks in MySQL and Derby.  But it does not detect a deadlock for Postgres, Oracle, or SQLServer"
HIVE-8689,handle overflows in statistics better,"Improve overflow checks in StatsAnnotation optimizer.
"
HIVE-8671,Overflow in estimate row count and data size with fetch column stats,"Overflow in row counts and data size for several TPC-DS queries.
Interestingly the operators which have overflow end up running with a small parallelism.

For instance Reducer 2 has an overflow but it only runs with parallelism of 2.
{code}
       Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string), KEY._col3 (type: string), KEY._col4 (type: float)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775341 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col3 (type: string), _col3 (type: string)
                  sort order: ++
                  Map-reduce partition columns: _col3 (type: string)
                  Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775341 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float), _col5 (type: double)
            Execution mode: vectorized
{code}

{code}
VERTEX       TOTAL_TASKS    DURATION_SECONDS     CPU_TIME_MILLIS INPUT_RECORDS   OUTPUT_RECORDS 
Map 1                 62               26.41           1,779,510   211,978,502       60,628,390
Map 5                  1                4.28               6,950       138,098          138,098
Map 6                  1                2.44               3,910            31               31
Reducer 2              2               22.69              61,320    60,628,390           69,182
Reducer 3              1                2.63               3,910        69,182              100
Reducer 4              1                1.01               1,180           100              100
{code}

Query
{code}
explain  
select  i_item_desc 
      ,i_category 
      ,i_class 
      ,i_current_price
      ,i_item_id
      ,sum(ws_ext_sales_price) as itemrevenue 
      ,sum(ws_ext_sales_price)*100/sum(sum(ws_ext_sales_price)) over
          (partition by i_class) as revenueratio
from	
	web_sales
    	,item 
    	,date_dim
where 
	web_sales.ws_item_sk = item.i_item_sk 
  	and item.i_category in ('Jewelry', 'Sports', 'Books')
  	and web_sales.ws_sold_date_sk = date_dim.d_date_sk
	and date_dim.d_date between '2001-01-12' and '2001-02-11'
group by 
	i_item_id
        ,i_item_desc 
        ,i_category
        ,i_class
        ,i_current_price
order by 
	i_category
        ,i_class
        ,i_item_id
        ,i_item_desc
        ,revenueratio
limit 100
{code}

Explain 
{code}
STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 1 <- Map 5 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
      DagName: mmokhtar_20141019164343_854cb757-01bd-40cb-843e-9ada7c5e6f38:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: web_sales
                  filterExpr: ws_item_sk is not null (type: boolean)
                  Statistics: Num rows: 21594638446 Data size: 2850189889652 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ws_item_sk is not null (type: boolean)
                    Statistics: Num rows: 21594638446 Data size: 172746300152 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ws_item_sk (type: int), ws_ext_sales_price (type: float), ws_sold_date_sk (type: int)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 21594638446 Data size: 172746300152 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0 {_col0} {_col1}
                          1 
                        keys:
                          0 _col2 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0, _col1
                        input vertices:
                          1 Map 6
                        Statistics: Num rows: 24145061366 Data size: 193160490928 Basic stats: COMPLETE Column stats: COMPLETE
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          condition expressions:
                            0 {_col1}
                            1 {_col1} {_col2} {_col3} {_col4} {_col5}
                          keys:
                            0 _col0 (type: int)
                            1 _col0 (type: int)
                          outputColumnNames: _col1, _col6, _col7, _col8, _col9, _col10
                          input vertices:
                            1 Map 5
                          Statistics: Num rows: 25381041158 Data size: 11929089344260 Basic stats: COMPLETE Column stats: COMPLETE
                          Select Operator
                            expressions: _col6 (type: string), _col7 (type: string), _col10 (type: string), _col9 (type: string), _col8 (type: float), _col1 (type: float)
                            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                            Statistics: Num rows: 25381041158 Data size: 11929089344260 Basic stats: COMPLETE Column stats: COMPLETE
                            Group By Operator
                              aggregations: sum(_col5)
                              keys: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float)
                              mode: hash
                              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                              Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                              Reduce Output Operator
                                key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float)
                                sort order: +++++
                                Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float)
                                Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                                value expressions: _col5 (type: double)
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((i_category) IN ('Jewelry', 'Sports', 'Books') and i_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 462000 Data size: 663862160 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((i_category) IN ('Jewelry', 'Sports', 'Books') and i_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 231000 Data size: 109491664 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int), i_item_id (type: string), i_item_desc (type: string), i_current_price (type: float), i_class (type: string), i_category (type: string)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      Statistics: Num rows: 231000 Data size: 109491664 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 231000 Data size: 109491664 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string), _col2 (type: string), _col3 (type: float), _col4 (type: string), _col5 (type: string)
            Execution mode: vectorized
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: (d_date BETWEEN '2001-01-12' AND '2001-02-11' and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (d_date BETWEEN '2001-01-12' AND '2001-02-11' and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Select Operator
                        expressions: _col0 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 36524 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                        Group By Operator
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 36524 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                          Dynamic Partitioning Event Operator
                            Target Input: web_sales
                            Partition key expr: ws_sold_date_sk
                            Statistics: Num rows: 36524 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                            Target column: ws_sold_date_sk
                            Target Vertex: Map 1
            Execution mode: vectorized
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string), KEY._col3 (type: string), KEY._col4 (type: float)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col3 (type: string), _col3 (type: string)
                  sort order: ++
                  Map-reduce partition columns: _col3 (type: string)
                  Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float), _col5 (type: double)
            Execution mode: vectorized
        Reducer 3 
            Reduce Operator Tree:
              Extract
                Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                PTF Operator
                  Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float), _col0 (type: string), _col5 (type: double), ((_col5 * 100.0) / _wcol0) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col1 (type: string), _col2 (type: string), _col4 (type: string), _col0 (type: string), _col6 (type: double)
                      sort order: +++++
                      Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                      TopN Hash Memory Usage: 0.04
                      value expressions: _col3 (type: float), _col5 (type: double)
        Reducer 4 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey3 (type: string), KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), VALUE._col0 (type: float), KEY.reducesinkkey2 (type: string), VALUE._col1 (type: double), KEY.reducesinkkey4 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 100
                  Statistics: Num rows: 100 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 100 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized

  Stage: Stage-0
    Fetch Operator
      limit: 100
      Processor Tree:
        ListSink

{code}"
HIVE-8628,NPE in case of shuffle join in tez,"test throws NullPointerException:
{noformat}
Vertex failed, vertexName=Reducer 2, vertexId=vertex_1413774081318_0803_5_03, diagnostics=[Task failed, taskId=task_1413774081318_0803_5_03_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
]], Vertex failed as one or more tasks failed. failedTasks:1]
Vertex killed, vertexName=Reducer 3, vertexId=vertex_1413774081318_0803_5_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0]
DAG failed due to vertex failure. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
{noformat}

{code:sql}
set hive.auto.convert.join=false;
select c.c_first_name, c.c_last_name, cd.cd_gender, hd.hd_buy_potential from customer c left outer join customer_demographics cd on cd.cd_demo_sk = c.c_current_cdemo_sk left outer join household_demographics hd on hd.hd_demo_sk = c.c_current_hdemo_sk where c.c_customer_sk < 1000;
{code}

Plan (auto.convert.join=false, vectorization on, cbo on,execution.engine=tez)
{noformat}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE)
        Reducer 3 <- Map 5 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)
      DagName: hrt_qa_20141020210707_53fdc731-2d96-455e-8c20-ce7fec75f01f:6
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: c
                  filterExpr: (c_customer_sk < 1000) (type: boolean)
                  Statistics: Num rows: 100000 Data size: 4679516 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (c_customer_sk < 1000) (type: boolean)
                    Statistics: Num rows: 33333 Data size: 1559823 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: c_current_cdemo_sk (type: int), c_current_hdemo_sk (type: int), c_first_name (type: string), c_last_name (type: string)
                      outputColumnNames: _col1, _col2, _col3, _col4
                      Statistics: Num rows: 33333 Data size: 1559823 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col1 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col1 (type: int)
                        Statistics: Num rows: 33333 Data size: 1559823 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col2 (type: int), _col3 (type: string), _col4 (type: string)
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: cd
                  Statistics: Num rows: 1920800 Data size: 5893494 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: cd_demo_sk (type: int), cd_gender (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1920800 Data size: 5893494 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 1920800 Data size: 5893494 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: hd
                  Statistics: Num rows: 7200 Data size: 840 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: hd_demo_sk (type: int), hd_buy_potential (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 7200 Data size: 840 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 7200 Data size: 840 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col1 (type: string)
            Execution mode: vectorized
        Reducer 2 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Right Outer Join0 to 1
                condition expressions:
                  0 {VALUE._col0}
                  1 {VALUE._col1} {VALUE._col2} {VALUE._col3}
                outputColumnNames: _col1, _col4, _col5, _col6
                Statistics: Num rows: 2112880 Data size: 6482843 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col4 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col4 (type: int)
                  Statistics: Num rows: 2112880 Data size: 6482843 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: string), _col5 (type: string), _col6 (type: string)
        Reducer 3 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                condition expressions:
                  0 {VALUE._col1} {VALUE._col4} {VALUE._col5}
                  1 {VALUE._col0}
                outputColumnNames: _col1, _col5, _col6, _col8
                Statistics: Num rows: 2324168 Data size: 7131127 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col5 (type: string), _col6 (type: string), _col1 (type: string), _col8 (type: string)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Statistics: Num rows: 2324168 Data size: 7131127 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 2324168 Data size: 7131127 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
{noformat}"
HIVE-8579,Guaranteed NPE in DDLSemanticAnalyzer,"This was added by [~jdere] in HIVE-8411. I don't fully understand the code (i.e. what it means when desc is null) but I'm sure, Jason, you can fix it without much trouble?

{code}
if (desc == null || !AlterTableDesc.doesAlterTableTypeSupportPartialPartitionSpec(desc.getOp())) {
  throw new SemanticException( ErrorMsg.ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED, desc.getOp().name());
        } else if (!conf.getBoolVar(HiveConf.ConfVars.DYNAMICPARTITIONING)) {
          throw new SemanticException(ErrorMsg.DYNAMIC_PARTITION_DISABLED);
        }
{code}

You check for whether {{desc}} is null but then use it to do {{desc.getOp()}}."
HIVE-8576,Guaranteed NPE in StatsRulesProcFactory,"Code looks like this:

{code}
    private List<Integer> getPrimaryKeyCandidates(List<Operator<? extends OperatorDesc>> ops) {
      List<Integer> result = Lists.newArrayList();
      if (ops != null || !ops.isEmpty()) {
        for (int i = 0; i < ops.size(); i++) {
{code}

The {{||}} should be {{&&}} as the current {{if}} statement can be rewritten as {{!(ops == null && ops.isEmpty())}} which doesn't make sense.

Or am I missing something?"
HIVE-8563,Running annotate_stats_join_pkfk.q in TestMiniTezCliDriver is causing NPE,"I added a test case as part of HIVE-8549 to annotate_stats_join_pkfk.q. This test case fails with NullPointerException when we run using TestMiniTezCliDriver. Here is the stack trace
{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.plan.PlanUtils.getFieldSchemasFromRowSchema(PlanUtils.java:548)
        at org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.process(ReduceSinkMapJoinProc.java:239)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)
        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)
        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)
        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)
        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)
        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)
        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)
        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:367)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10057)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1070)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1132)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1007)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:997)

{code}"
HIVE-8551,NPE in FunctionRegistry (affects CBO in negative tests),
HIVE-8549,NPE in PK-FK inference when one side of join is complex tree,"HIVE-8168 added PK-FK inference from column stats. But when one side of join is complex tree which propagates FK, relationship inference fails with NPE.

{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.getSelectivity(StatsRulesProcFactory.java:1293)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.inferPKFKRelationship(StatsRulesProcFactory.java:1250)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.process(StatsRulesProcFactory.java:1067)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:248)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:120)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:99)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10039)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1129)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:994)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)

{code}"
HIVE-8408,hcat cli throws NPE when authorizer using new api is enabled,"Hcat cli tries to authorize actions using the authorizer configured for hive-cli. When authorizer based on new authorization interface is used, it throws a NPE as it checks only for the old api based authorization implementation.
"
HIVE-8378,NPE in TezTask due to null counters,The counters variable iterated over on line 177 can be null apparently.
HIVE-8372,Potential NPE in Tez MergeFileRecordProcessor,MergeFileRecordProcessor retrieves map work from cache. This map work can be instance of merge file work. When the merge file work already exists in the cache casting the map work to merge file work is missing which will result in NullPointerException.
HIVE-8361,NPE in PTFOperator when there are empty partitions,"Here is a simple query to reproduce this:
{code}
select sum(p_size) over (partition by p_mfgr )
from part where p_mfgr = 'some non existent mfgr';
{code}"
HIVE-8332,Reading an ACID table with vectorization on results in NPE,"On a transactional table, insert some data, then with vectorization turned on do a select.  The result is:
{code}
Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$1.getObjectInspector(OrcInputFormat.java:1137) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.<init>(VectorizedOrcAcidRowReader.java:61) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1041) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:246)
	... 25 more
{code}"
HIVE-8281,NPE with dynamic partition pruning on Tez,Dynamic partition pruning can generate incorrect query plans during join algorithm selection.
HIVE-8272,Query with particular decimal expression causes NPE during execution initialization,"
Query:
{code}
select 
  cast(sum(dc)*100 as decimal(11,3)) as c1
  from somedecimaltable
  order by c1
  limit 100;
{code}

Fails during execution initialization due to *null* ExprNodeDesc.

Noticed while trying to simplify a Vectorization issue and realized it was a more general issue.

{code}
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:154)
	... 22 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(ReduceSinkOperator.java:215)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:380)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:464)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:420)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:427)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:380)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:464)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:420)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:380)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:464)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:420)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:193)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:380)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:380)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:133)
	... 22 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getExprString(ExprNodeGenericFuncDesc.java:154)
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getExprString(ExprNodeGenericFuncDesc.java:154)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(ReduceSinkOperator.java:148)
	... 38 more
{code}"
HIVE-8227,NPE w/ hive on tez when doing unions on empty tables,We're looking at aliasToWork.values() to determine input paths etc. This can contain nulls when we're scanning empty tables.
HIVE-8203,ACID operations result in NPE when run through HS2,"When accessing Hive via HS2, any operation requiring the DbTxnManager results in an NPE."
HIVE-8156,Vectorized reducers need to avoid memory build-up during a single key,"When encountering a skewed key with a large number of values, the vectorized reducer will not release memory within the loop."
HIVE-8104,Insert statements against ACID tables NPE when vectorization is on,Doing an insert against a table that is using ACID format with the transaction manager set to DbTxnManager and vectorization turned on results in an NPE.  
HIVE-8090,Potential null pointer reference in WriterImpl#StreamFactory#createStream(),"{code}
      switch (kind) {
...
      default:
        modifiers = null;
        break;
      }

      BufferedStream result = streams.get(name);
      if (result == null) {
        result = new BufferedStream(name.toString(), bufferSize,
            codec == null ? codec : codec.modify(modifiers));
{code}
In case modifiers is null and codec is ZlibCodec, there would be NPE in ZlibCodec#modify(EnumSet<Modifier> modifiers) :
{code}
    for (Modifier m : modifiers) {
{code}"
HIVE-8078,ORC Delta encoding corrupts data when delta overflows long,"There is an issue with the integer encoding that can cause corruption in certain cases.
The following 3 longs cause this failure.
4513343538618202711
2911390882471569739
-9181829309989854913

I believe that even though the numbers are in decreasing order, the delta between the last two numbers overflows causing a positive delta, in this case the last digit ends up being corrupted (the delta is applied for the wrong sign resulting in -3442132998776557225 instead of -9181829309989854913.
"
HIVE-8008,NPE while reading null decimal value,"Say you have this table {{dec_test}}:
{code}
dec                 	decimal(10,0)       	                    
{code}

If the table has a row that is 9999999999.5, and if we do

{code}
select * from dec_test;
{code}

it will crash with NPE:

{code}
2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)
  at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)
  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)
  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)
  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)
  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)
  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)
  ... 12 more
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)
  at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)
  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)
  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)
  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)
  ... 19 more
{code}"
HIVE-7992,StatsRulesProcFactory should gracefully handle overflows,"When StatsRulesProcFactory overflows it sets data size to 0 and as a result the Vertex will ask for a single task, this results in a fairly slow running query, most likely the overflow is a result of higher than usual number of rows.

The class should detect an overflow and set a flag when an overflow occurs, if an overflow occurs StatsRulesProcFactory should request the maximum number of tasks for the vertex."
HIVE-7991,Incorrect calculation of number of rows in JoinStatsRule.process results in overflow,"This loop results in adding the parent twice incase of a 3 way join of store_sales  x date_dim x store

{code}
         for (int pos = 0; pos < parents.size(); pos++) {
            ReduceSinkOperator parent = (ReduceSinkOperator) jop.getParentOperators().get(pos);

            Statistics parentStats = parent.getStatistics();
            List<ExprNodeDesc> keyExprs = parent.getConf().getKeyCols();

            // Parent RS may have column statistics from multiple parents.
            // Populate table alias to row count map, this will be used later to
            // scale down/up column statistics based on new row count
            // NOTE: JOIN with UNION as parent of RS will not have table alias
            // propagated properly. UNION operator does not propagate the table
            // alias of subqueries properly to expression nodes. Hence union20.q
            // will have wrong number of rows.
            Set<String> tableAliases = StatsUtils.getAllTableAlias(parent.getColumnExprMap());
            for (String tabAlias : tableAliases) {
              rowCountParents.put(tabAlias, parentStats.getNumRows());
            }
{code}

In the first join we have rowCountParents with {store_sales=120464862, date_dim=36524} which is correct.
For the second join result rowCountParents ends up with {store=212, store_sales=120464862, date_dim=120464862} where it should be {store=212, store_sales=120464862, date_dim=36524}.
The result of this is that computeNewRowCount ends up multiplying row count of store_sales x store_sales which makes the number of rows really high and eventually over flow.

Plan snippet : 
{code}
   Map 1
            Map Operator Tree:
                TableScan
                  alias: store_sales
                  filterExpr: (((ss_sold_date_sk is not null and ss_store_sk is not null) and ss_item_sk is not null) and ss_sold_date BETWEEN '1999-06-01' AND '2000-05-31') (type: boolean)
                  Statistics: Num rows: 110339135 Data size: 4817453454 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((ss_sold_date_sk is not null and ss_store_sk is not null) and ss_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 107740258 Data size: 2124353556 Basic stats: COMPLETE Column stats: COMPLETE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      condition expressions:
                        0 {ss_sold_date_sk} {ss_item_sk} {ss_store_sk} {ss_quantity} {ss_sales_price} {ss_sold_date}
                        1 {d_date_sk} {d_month_seq} {d_year} {d_moy} {d_qoy}
                      keys:
                        0 ss_sold_date_sk (type: int)
                        1 d_date_sk (type: int)
                      outputColumnNames: _col0, _col2, _col7, _col10, _col13, _col23, _col27, _col30, _col33, _col35, _col37
                      input vertices:
                        1 Map 6
                      Statistics: Num rows: 120464862 Data size: 26984129088 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0 {_col0} {_col2} {_col7} {_col10} {_col13} {_col23} {_col27} {_col30} {_col33} {_col35} {_col37}
                          1 {s_store_sk} {s_store_id}
                        keys:
                          0 _col7 (type: int)
                          1 s_store_sk (type: int)
                        outputColumnNames: _col0, _col2, _col7, _col10, _col13, _col23, _col27, _col30, _col33, _col35, _col37, _col58, _col59
                        input vertices:
                          1 Map 5
                        Statistics: Num rows: 17886616227069518 Data size: 5866810122478801920 Basic stats: COMPLETE Column stats: COMPLETE
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          condition expressions:
                            0 {_col0} {_col2} {_col7} {_col10} {_col13} {_col23} {_col27} {_col30} {_col33} {_col35} {_col37} {_col58} {_col59}
                            1 {i_item_sk} {i_brand} {i_class} {i_category} {i_product_name}
                          keys:
                            0 _col2 (type: int)
                            1 i_item_sk (type: int)
                          outputColumnNames: _col0, _col2, _col7, _col10, _col13, _col23, _col27, _col30, _col33, _col35, _col37, _col58, _col59, _col90, _col98, _col100, _col102, _col111
                          input vertices:
                            1 Map 7
                          Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                          Filter Operator
                            predicate: (((((_col0 = _col27) and (_col2 = _col90)) and (_col7 = _col58)) and _col30 BETWEEN 1193 AND 1204) and _col23 BETWEEN '1999-06-01' AND '2000-05-31') (type: boolean)
                            Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                            Select Operator
                              expressions: _col102 (type: string), _col100 (type: string), _col98 (type: string), _col111 (type: string), _col33 (type: int), _col37 (type: int), _col35 (type: int), _col59 (type: string), _col13 (type: float), _col10 (type: int)
                              outputColumnNames: _col102, _col100, _col98, _col111, _col33, _col37, _col35, _col59, _col13, _col10
                              Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                              Group By Operator
                                aggregations: sum(COALESCE((_col13 * _col10),0))
                                keys: _col102 (type: string), _col100 (type: string), _col98 (type: string), _col111 (type: string), _col33 (type: int), _col37 (type: int), _col35 (type: int), _col59 (type: string), '0' (type: string)
                                mode: hash
                                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                                Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                                Reduce Output Operator
                                  key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: int), _col6 (type: int), _col7 (type: string), _col8 (type: string)
                                  sort order: +++++++++
                                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: int), _col6 (type: int), _col7 (type: string), _col8 (type: string)
                                  Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                                  value expressions: _col9 (type: double)
{code}"
HIVE-7987,Storage based authorization  - NPE for drop view,"When storage based authorization is enabled, NullPointerException is thrown for 'drop view'.
"
HIVE-7975,HS2 memory optimization: Internalizing instance fields of Thrift-generated metastore API classes,"We should internalize the String-based instance fields of the metastore API classes FieldSchema, Partition, SerDeInfo, and StorageDescriptor in order to save memory. In a test environment with data consisting of about 1800 partitions, the proposed changes are able to save about 24% of old generation memory during a complex query. See details in the attached document."
HIVE-7904,Missing null check cause NPE when updating join column stats in statistics annotation,Column stats updation in join stats rule annotation can cause NPE if column stats is missing from one relation. 
HIVE-7851,Fix NPE in split generation on Tez 0.5,
HIVE-7829,Entity.getLocation can throw an NPE,It's possible for the getDataLocation methods which Entity.getLocation calls to return null and as such NPE
HIVE-7738,tez select sum(decimal) from union all of decimal and null throws NPE,"if run this query using tez engine then hive will throw NPE
{code}
select sum(a) from (
  select cast(1.1 as decimal) a from dual
  union all
  select cast(null as decimal) a from dual
) t;
{code}

{code}
hive> select sum(a) from (
    >   select cast(1.1 as decimal) a from dual
    >   union all
    >   select cast(null as decimal) a from dual
    > ) t;
Query ID = apivovarov_20140814200909_438385b2-4147-47bc-98a0-a01567bbb5c5
Total jobs = 1
Launching Job 1 out of 1


Status: Running (application id: application_1407388228332_5616)

Map 1: -/-	Map 4: -/-	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 0/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 0/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Status: Failed
Vertex failed, vertexName=Map 1, vertexId=vertex_1407388228332_5616_1_02, diagnostics=[Task failed, taskId=task_1407388228332_5616_1_02_000000, diagnostics=[AttemptID:attempt_1407388228332_5616_1_02_000000_0 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more

Container released by application, AttemptID:attempt_1407388228332_5616_1_02_000000_1 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more

Container released by application, AttemptID:attempt_1407388228332_5616_1_02_000000_2 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more

Container released by application, AttemptID:attempt_1407388228332_5616_1_02_000000_3 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more
], Vertex failed as one or more tasks failed. failedTasks:1]
Vertex killed, vertexName=Reducer 3, vertexId=vertex_1407388228332_5616_1_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0]
DAG failed due to vertex failure. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
{code}
"
HIVE-7697,PlanUtils.getTableDesc uses printStackTrace and returns null,PlanUtils.getTableDesc uses printStackTrace and returns null. Calls of this are not expecting null.
HIVE-7641,INSERT ... SELECT with no source table leads to NPE,"When no source table is provided for an INSERT statement Hive fails with NPE. 

{code}
0: jdbc:hive2://localhost:11050/default> create table test_tbl(i int);
No rows affected (0.333 seconds)
0: jdbc:hive2://localhost:11050/default> insert into table test_tbl select 1;
Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)

-- Get a NPE even when using incorrect syntax (no TABLE keyword)
0: jdbc:hive2://localhost:11050/default> insert into test_tbl select 1;
Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)

-- Works when a source table is provided
0: jdbc:hive2://localhost:11050/default> insert into table test_tbl select 1 from foo;
No rows affected (5.751 seconds)
{code}"
HIVE-7599,NPE in MergeTask#main() when -format is absent,"When '-format' is absent from commandline, the following call would result in NPE (format is initialized to null):
{code}
    if (format.equals(""rcfile"")) {
      mergeWork = new MergeWork(inputPaths, new Path(outputDir), RCFileInputFormat.class);
{code}"
HIVE-7574,CommonJoinOperator.checkAndGenObject calls LOG.Trace per row from probe side in a HashMap join consuming 4% of the CPU,"In Map join Log4JLogger.trace takes 4% of the CPU time as it gets called per row from the probe side by CommonJoinOperator.genAllOneUniqueJoinObject.

Fix is to remove the logging code code below from CommonJoinOperator.genAllOneUniqueJoinObject:
{code}
if (allOne) {
        LOG.info(""calling genAllOneUniqueJoinObject"");
        genAllOneUniqueJoinObject();
        LOG.info(""called genAllOneUniqueJoinObject"");
      } else {
        LOG.trace(""calling genUniqueJoinObject"");
        genUniqueJoinObject(0, 0);
        LOG.trace(""called genUniqueJoinObject"");
      }
{code}

And 
{code}
    if (!hasEmpty && !mayHasMoreThanOne) {
        LOG.trace(""calling genAllOneUniqueJoinObject"");
        genAllOneUniqueJoinObject();
        LOG.trace(""called genAllOneUniqueJoinObject"");
      } else if (!hasEmpty && !hasLeftSemiJoin) {
        LOG.trace(""calling genUniqueJoinObject"");
        genUniqueJoinObject(0, 0);
        LOG.trace(""called genUniqueJoinObject"");
      } else {
        LOG.trace(""calling genObject"");
        genJoinObject();
        LOG.trace(""called genObject"");
      }
{code}


This is the call stack 
{code}
Stack Trace	Sample Count	Percentage(%)
hadoop.hive.ql.exec.MapJoinOperator.processOp(Object, int)	388	75.486
   hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject()	121	23.541
      hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject()	92	17.899
      commons.logging.impl.Log4JLogger.trace(Object)	20	3.891
         log4j.Category.log(String, Priority, Object, Throwable)	20	3.891
            log4j.Category.getEffectiveLevel()	10	1.946
{code}"
HIVE-7498,NPE on show grant for global privilege,"{noformat}
2014-07-24 11:10:05,961 ERROR exec.DDLTask (DDLTask.java:failed(501)) - java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.compareTo(HivePrivilegeObject.java:66)
	at org.apache.hadoop.hive.ql.exec.DDLTask$2.compare(DDLTask.java:3156)
	at org.apache.hadoop.hive.ql.exec.DDLTask$2.compare(DDLTask.java:3153)
	at java.util.Arrays.mergeSort(Arrays.java:1270)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.mergeSort(Arrays.java:1281)
	at java.util.Arrays.sort(Arrays.java:1210)
	at java.util.Collections.sort(Collections.java:157)
	at org.apache.hadoop.hive.ql.exec.DDLTask.writeGrantInfo(DDLTask.java:3153)
	at org.apache.hadoop.hive.ql.exec.DDLTask.showGrants(DDLTask.java:606)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:455)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1513)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1280)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1094)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:918)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:908)
{noformat}
Seemed regression from HIVE-7026"
HIVE-7459,Fix NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat,
HIVE-7409,Add workaround for a deadlock issue of Class.getAnnotation() ,"[JDK-7122142|https://bugs.openjdk.java.net/browse/JDK-7122142] mentions that there is a race condition in getAnnotations. This problem can lead deadlock. The fix on JDK will be merged on jdk8, but hive supports jdk6/jdk7 currently. Therefore, we should add workaround to avoid the issue."
HIVE-7393,Tez jobs sometimes fail with NPE processing input splits,"Input files are either ORC or RC format.  Only occurs on occasion - if the query is repeated it is likely to complete successfully.

{noformat}
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Grouping splits in Tez
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Desired splits: 408 too large.  Desired splitLength: 614866 Min splitLength: 16777216 New desired splits: 15 Total length: 250865685 Original splits: 13
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Using original number of splits: 13 desired splits: 15
2014-07-11 15:31:45,381 INFO [AsyncDispatcher event handler] org.apache.tez.dag.history.HistoryEventHandler: [HISTORY][DAG:dag_1405114778353_0004_1][Event:VERTEX_INITIALIZED]: vertexName=Reducer 4, vertexId=vertex_1405114778353_0004_1_09, initRequestedTime=1405117905313, initedTime=1405117905381, numTasks=999, processorName=org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor, additionalInputsCount=0
2014-07-11 15:31:45,381 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: vertex_1405114778353_0004_1_09 [Reducer 4] transitioned from NEW to INITED due to event V_INIT
2014-07-11 15:31:45,383 ERROR [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Vertex Input: csb initializer failed
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:275)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)
	at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)
	at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}
"
HIVE-7353,HiveServer2 using embedded MetaStore leaks JDOPersistanceManager,"While using embedded metastore, while creating background threads to run async operations, HiveServer2 ends up creating new instances of JDOPersistanceManager which are cached in JDOPersistanceManagerFactory. Even when the background thread is killed by the thread pool manager, the JDOPersistanceManager are never GCed because they are cached by JDOPersistanceManagerFactory."
HIVE-7234,Select on decimal column throws NPE,"Select on decimal column throws NPE for values greater than maximum permissible value (9999999999)

Steps to repro:

DROP TABLE IF EXISTS DECIMAL;

CREATE TABLE DECIMAL (dec decimal);

// Content of decimal_10_0.txt => ""9999999999.999""
LOAD DATA LOCAL INPATH '../../data/files/decimal_10_0.txt' OVERWRITE INTO TABLE DECIMAL;

SELECT dec FROM DECIMAL; => throws NPE

DROP TABLE DECIMAL;"
HIVE-7226,Windowing Streaming mode causes NPE for empty partitions,Change in HIVE-7062 doesn't handle empty partitions properly. StreamingState is not correctly initialized for empty partition
HIVE-7210,"NPE with ""No plan file found"" when running Driver instances on multiple threads","Informatica has a multithreaded application running multiple instances of CLIDriver.  When running concurrent queries they sometimes hit the following error:

{noformat}
2014-05-30 10:24:59 <pool-10-thread-1> INFO: Hadoop_Native_Log :INFO org.apache.hadoop.hive.ql.exec.Utilities: No plan file found: hdfs://ICRHHW21NODE1:8020/tmp/hive-qamercury/hive_2014-05-30_10-24-57_346_890014621821056491-2/-mr-10002/6169987c-3263-4737-b5cb-38daab882afb/map.xml
2014-05-30 10:24:59 <pool-10-thread-1> INFO: Hadoop_Native_Log :INFO org.apache.hadoop.mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/qamercury/.staging/job_1401360353644_0078
2014-05-30 10:24:59 <pool-10-thread-1> INFO: Hadoop_Native_Log :ERROR org.apache.hadoop.hive.ql.exec.Task: Job Submission failed with exception 'java.lang.NullPointerException(null)'
java.lang.NullPointerException
                at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)
                at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:271)
                at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:520)
                at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:512)
                at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)
                at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
                at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
                at java.security.AccessController.doPrivileged(Native Method)
                at javax.security.auth.Subject.doAs(Subject.java:415)
                at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
                at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
                at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
                at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
                at java.security.AccessController.doPrivileged(Native Method)
                at javax.security.auth.Subject.doAs(Subject.java:415)
                at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
                at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
                at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
                at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:420)
                at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:136)
                at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
                at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
                at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)
                at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)
                at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)
                at org.apache.hadoop.hive.ql.Driver.run(Driver.java:912)
                at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
                at com.informatica.platform.dtm.executor.hive.impl.AbstractHiveDriverBaseImpl.run(AbstractHiveDriverBaseImpl.java:86)
                at com.informatica.platform.dtm.executor.hive.MHiveDriver.executeQuery(MHiveDriver.java:126)
                at com.informatica.platform.dtm.executor.hive.task.impl.HiveTaskHandlerImpl.executeQuery(HiveTaskHandlerImpl.java:358)
                at com.informatica.platform.dtm.executor.hive.task.impl.HiveTaskHandlerImpl.executeScript(HiveTaskHandlerImpl.java:247)
                at com.informatica.platform.dtm.executor.hive.task.impl.HiveTaskHandlerImpl.executeMainScript(HiveTaskHandlerImpl.java:194)
                at com.informatica.platform.ldtm.executor.common.workflow.taskhandler.impl.BaseTaskHandlerImpl.run(BaseTaskHandlerImpl.java:126)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:744)
{noformat}"
HIVE-7202,DbTxnManager deadlocks in hcatalog.cli.TestSematicAnalysis.testAlterTblFFpart(),"select * from HIVE_LOCKS produces

{noformat}
6                   |1                   |0                   |default                                                                                                                         |junit_sem_analysis                                                                                                              |NULL                                                                                                                            |w|r|1402354627716       |NULL                |unknown                                                                                                                         |ekoifman.local                                                                                                                  
6                   |2                   |0                   |default                                                                                                                         |junit_sem_analysis                                                                                                              |b=2010-10-10                                                                                                                    |w|e|1402354627716       |NULL                |unknown                                                                                                                         |ekoifman.local                                                                                                                  

2 rows selected
{noformat}

easiest way to repro this is to add
    hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
    hiveConf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, ""org.apache.hadoop.hive.ql.lockmgr.DbTxnManager"");

in HCatBaseTest.setUpHiveConf()"
HIVE-7155,WebHCat controller job exceeds container memory limit,"Submit a Hive query on a large table via WebHCat results in failure because the WebHCat controller job is killed by Yarn since it exceeds the memory limit (set by mapreduce.map.memory.mb, defaults to 1GB):
{code}
 INSERT OVERWRITE TABLE Temp_InjusticeEvents_2014_03_01_00_00 SELECT * from Stage_InjusticeEvents where LogTimestamp > '2014-03-01 00:00:00' and LogTimestamp <= '2014-03-01 01:00:00';
{code}

We could increase mapreduce.map.memory.mb to solve this problem, but this way we are changing this setting system wise.

We need to provide a WebHCat configuration to overwrite mapreduce.map.memory.mb when submitting the controller job.

"
HIVE-7109,Resource leak in HBaseStorageHandler,"The ""preCreateTable"" method in the HBaseStorageHandler checks that the HBase table is still online by creating a new instance of HTable

{code}
// ensure the table is online
new HTable(hbaseConf, tableDesc.getName());
{code}

However this instance is never closed. So if this test succeeds, we would have a resource leak in the code."
HIVE-6984,Analyzing partitioned table with NULL values for the partition column failed with NPE,"The following describes how to produce the bug:
{code}
hive> desc test2;
name                	string              	                    
age                 	int                 	                    

hive> select * from test2;
6666666666666666666	NULL
5555555555555555555	NULL
tom	15
john	NULL
mayr	40
	30
	NULL

hive> create table test3(name string) partitioned by (age int);

hive> from test2 insert overwrite table test3 partition(age) select test2.name, test2.age;
Loading data to table default.test3 partition (age=null)
	Loading partition {age=40}
	Loading partition {age=__HIVE_DEFAULT_PARTITION__}
	Loading partition {age=30}
	Loading partition {age=15}
Partition default.test3{age=15} stats: [numFiles=1, numRows=1, totalSize=4, rawDataSize=3]
Partition default.test3{age=30} stats: [numFiles=1, numRows=1, totalSize=1, rawDataSize=0]
Partition default.test3{age=40} stats: [numFiles=1, numRows=1, totalSize=5, rawDataSize=4]
Partition default.test3{age=__HIVE_DEFAULT_PARTITION__} stats: [numFiles=1, numRows=4, totalSize=46, rawDataSize=42]

hive> analyze table test3 partition(age) compute statistics;
...
Task with the most failures(4): 
-----
Diagnostic Messages for this Task:
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
	at org.apache.hado

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{code}

The following is the stack trace in mapper log:
{code}
2014-04-28 15:39:25,073 FATAL org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)
	... 9 more
{code}"
HIVE-6888,Hive leaks MapWork objects via Utilities::gWorkMap,"When running multiple queries with hive on a single Application Master, we found that hive leaks a large number of MapWork objects which accumulate in the AM"
HIVE-6468,HS2 & Metastore using SASL out of memory error when curl sends a get request,"We see an out of memory error when we run simple beeline calls.
(The hive.server2.transport.mode is binary)

curl localhost:10000

Exception in thread ""pool-2-thread-8"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:181)
	at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)
	at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:189)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)"
HIVE-6430,MapJoin hash table has large memory overhead,"Right now, in some queries, I see that storing e.g. 4 ints (2 for key and 2 for row) can take several hundred bytes, which is ridiculous. I am reducing the size of MJKey and MJRowContainer in other jiras, but in general we don't need to have java hash table there.  We can either use primitive-friendly hashtable like the one from HPPC (Apache-licenced), or some variation, to map primitive keys to single row storage structure without an object per row (similar to vectorization)."
HIVE-5364,NPE on some queries from partitioned orc table,"If you create a partitioned ORC table with:

{code}
create table A
...
PARTITIONED BY (
year int,
month int,
day int)
{code}

This query will fail:
select count from A where where year=2013 and month=9 and day=15;
"
HIVE-5296,Memory leak: OOM Error after multiple open/closed JDBC connections. ,"Multiple connections to Hiveserver2, all of which are closed and disposed of properly show the Java heap size to grow extremely quickly. 

This issue can be recreated using the following code

{code}

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.Properties;

import org.apache.hive.service.cli.HiveSQLException;
import org.apache.log4j.Logger;

/*
 * Class which encapsulates the lifecycle of a query or statement.
 * Provides functionality which allows you to create a connection
 */

public class HiveClient {
	
	Connection con;
	Logger logger;
	private static String driverName = ""org.apache.hive.jdbc.HiveDriver"";	
	private String db;
	
	
	public HiveClient(String db)
	{		
		logger = Logger.getLogger(HiveClient.class);
		this.db=db;
		
		try{
			 Class.forName(driverName);
		}catch(ClassNotFoundException e){
			logger.info(""Can't find Hive driver"");
		}
		
		String hiveHost = GlimmerServer.config.getString(""hive/host"");
		String hivePort = GlimmerServer.config.getString(""hive/port"");
		String connectionString = ""jdbc:hive2://""+hiveHost+"":""+hivePort +""/default"";
		logger.info(String.format(""Attempting to connect to %s"",connectionString));
		try{			
			con = DriverManager.getConnection(connectionString,"""","""");									
		}catch(Exception e){
			logger.error(""Problem instantiating the connection""+e.getMessage());
		}		
	}
			
	public int update(String query) 
	{
		Integer res = 0;
		Statement stmt = null;
		try{			
			stmt = con.createStatement();
			String switchdb = ""USE ""+db;
			logger.info(switchdb);		
			stmt.executeUpdate(switchdb);
			logger.info(query);
			res = stmt.executeUpdate(query);
			logger.info(""Query passed to server"");	
			stmt.close();
		}catch(HiveSQLException e){
			logger.info(String.format(""HiveSQLException thrown, this can be valid, "" +
					""but check the error: %s from the query %s"",query,e.toString()));
		}catch(SQLException e){
			logger.error(String.format(""Unable to execute query SQLException %s. Error: %s"",query,e));
		}catch(Exception e){
			logger.error(String.format(""Unable to execute query %s. Error: %s"",query,e));
		}
		
		if(stmt!=null)
			try{
				stmt.close();
			}catch(SQLException e){
				logger.error(""Cannot close the statment, potentially memory leak ""+e);
			}
		
		return res;
	}
	
	public void close()
	{
		if(con!=null){
			try {
				con.close();
			} catch (SQLException e) {				
				logger.info(""Problem closing connection ""+e);
			}
		}
	}
	
	
	
}
{code}

And by creating and closing many HiveClient objects. The heap space used by the hiveserver2 runjar process is seen to increase extremely quickly, without such space being released."
HIVE-5196,"ThriftCLIService.java uses stderr to print the stack trace, it should use the logger instead.","ThriftCLIService.java uses stderr to print the stack trace, it should use the logger instead. Using e.printStackTrace is not suitable for production."
HIVE-5061,Row sampling throws NPE when used in sub-query,"select * from (select * from src TABLESAMPLE (1 ROWS)) x;

{noformat}
ava.lang.NullPointerException
	at org.apache.hadoop.hive.ql.parse.SplitSample.getTargetSize(SplitSample.java:103)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.sampleSplits(CombineHiveInputFormat.java:487)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:405)
	at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1025)
	at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1017)
	at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:928)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:881)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:881)
	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:855)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:426)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:144)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1424)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1204)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1009)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:878)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
{noformat}"
HIVE-5048,StorageBasedAuthorization provider causes an NPE when asked to authorize from client side.,"StorageBasedAuthorizationProvider(henceforth referred to as SBAP) is a HiveMetastoreAuthorizationProvider (henceforth referred to as HMAP, and HiveAuthorizationProvider as HAP) that was introduced as part of HIVE-3705.

As long as it's used as a HMAP, i.e. from the metastore-side, as was its initial implementation intent, everything's great. However, HMAP extends HAP, and there is no reason SBAP shouldn't be expected to work as a HAP as well. However, it uses a wh variable that is never initialized if it is called as a HAP, and hence, it will always fail when authorize is called on it.

We should change SBAP so that it correctly initiazes wh so that it can be run as a HAP as well.
"
HIVE-5028,Some tests with fail OutOfMemoryError PermGen Space on Hadoop2,"{noformat}
java.lang.OutOfMemoryError: PermGen space
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:181)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:349)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:978)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:890)
        at org.apache.hcatalog.cli.HCatDriver.run(HCatDriver.java:43)
        at org.apache.hcatalog.hbase.TestHBaseBulkOutputFormat.bulkModeAbortTest(TestHBaseBulkOutputFormat.java:540)
{noformat}"
HIVE-4963,Support in memory PTF partitions,"PTF partitions apply the defensive mode of assuming that partitions will not fit in memory. Because of this there is a significant deserialization overhead when accessing elements. 

Allow the user to specify that there is enough memory to hold partitions through a 'hive.ptf.partition.fits.in.mem' option.  

Savings depends on partition size and in case of windowing the number of UDAFs and the window ranges. For eg for the following (admittedly extreme) case the PTFOperator exec times went from 39 secs to 8 secs.
 
{noformat}
select t, s, i, b, f, d,
min(t) over(partition by 1 rows between unbounded preceding and current row), 
min(s) over(partition by 1 rows between unbounded preceding and current row), 
min(i) over(partition by 1 rows between unbounded preceding and current row), 
min(b) over(partition by 1 rows between unbounded preceding and current row) 
from over10k
{noformat}"
HIVE-4935,Potential NPE in MetadataOnlyOptimizer,"In MetadataOnlyOptimizer.TableScanProcessor.process, it is possible that we consider a TableScanOperator as ""MayBeMetadataOnly"" when this TS does not have a conf. In MetadataOnlyOptimizer.MetadataOnlyTaskDispatcher.dispatch(Node, Stack<Node>, Object...), when we convert this TS, we want to get the alias from its conf...."
HIVE-4920,PTest2 handle Spot Price increases gracefully and improve rsync paralllelsim,"We should handle spot price increases more gracefully and parallelize rsync to slaves better

NO PRECOMMIT TESTS"
HIVE-4883,TestHadoop20SAuthBridge tests fail sometimes because of race condition,"TestHadoop20SAuthBridge tests testSaslWithHiveMetaStore and testMetastoreProxyUser sometimes fail. I have seen this more often on mac and windows, but this can happen on linux as well.

The problem is that metastore is started in a different thread and these unit tests actually rely on the metastore having initialized DelegationTokenSecretManager in HadoopThriftAuthBridge20S as part of the metastore startup (HiveMetaStore.startMetaStore )

"
HIVE-4798,NPE when we call isSame from an instance of ExprNodeConstantDesc with null value,"Take a look at the code
{code}
  @Override
  public boolean isSame(Object o) {
    if (!(o instanceof ExprNodeConstantDesc)) {
      return false;
    }
    ExprNodeConstantDesc dest = (ExprNodeConstantDesc) o;
    if (!typeInfo.equals(dest.getTypeInfo())) {
      return false;
    }
    if (!value.equals(dest.getValue())) {
      return false;
    }

    return true;
  }
{\code}

value is an Object."
HIVE-4761,ZooKeeperHiveLockManage.unlockPrimitive has race condition with threads,"In unlockPrimitive, we check to see if children exist and if not delete the parent node. If two threads do this at the same time it's possible for two threads to call Zookeeper.delete() on the same node."
HIVE-4679,WebHCat can deadlock Hadoop if the number of concurrently running tasks if higher or equal than the number of mappers,"o In the current Templeton design, each time a Job is submitted thru the REST API (it can be Pig/Hive or MR job), it will consume one Hadoop map slot. Given that the number of map slots is finite in the cluster (16 node cluster will have 32 map slots), in some circumstances, a user can deadlock the cluster if Templeton job submission pipeline takes over all map slots (Templeton map tasks will wait for the actual underlying jobs to complete, what will never happen, given that Hadoop has no free map slots to schedule new tasks).

o HCat queries use a different mechanism and do not contribute to the deadlock."
HIVE-4540,JOIN-GRP BY-DISTINCT fails with NPE when mapjoin.mapreduce=true,"If the mapjoin.mapreduce optimization kicks in on a query of this form:

{noformat}
select count(distinct a.v) 
from a join b on (a.k = b.k)
group by a.g
{noformat}

The planer will NPE in the metadataonly optimizer."
HIVE-4526,auto_sortmerge_join_9.q throws NPE but test is succeeded,"auto_sortmerge_join_9.q

{noformat}
    [junit] Running org.apache.hadoop.hive.cli.TestCliDriver
    [junit] Begin query: auto_sortmerge_join_9.q
    [junit] Deleted file:/home/navis/apache/oss-hive/build/ql/test/data/warehouse/tbl1
    [junit] Deleted file:/home/navis/apache/oss-hive/build/ql/test/data/warehouse/tbl2
    [junit] org.apache.hadoop.hive.ql.metadata.HiveException: Failed with exception nulljava.lang.NullPointerException
    [junit] 	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRowInspectorFromPartitionedTable(FetchOperator.java:252)
    [junit] 	at org.apache.hadoop.hive.ql.exec.FetchOperator.getOutputObjectInspector(FetchOperator.java:605)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.initializeOperators(MapredLocalTask.java:393)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:277)
    [junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:676)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] 
    [junit] 	at org.apache.hadoop.hive.ql.exec.FetchOperator.getOutputObjectInspector(FetchOperator.java:631)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.initializeOperators(MapredLocalTask.java:393)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:277)
    [junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:676)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] org.apache.hadoop.hive.ql.metadata.HiveException: Failed with exception nulljava.lang.NullPointerException
    [junit] 	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRowInspectorFromPartitionedTable(FetchOperator.java:252)
    [junit] 	at org.apache.hadoop.hive.ql.exec.FetchOperator.getOutputObjectInspector(FetchOperator.java:605)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.initializeOperators(MapredLocalTask.java:393)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:277)
    [junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:676)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] 
    [junit] 	at org.apache.hadoop.hive.ql.exec.FetchOperator.getOutputObjectInspector(FetchOperator.java:631)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.initializeOperators(MapredLocalTask.java:393)
    [junit] 	at org.apache.hadoop.hive.ql.exec.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:277)
    [junit] 	at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:676)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] Running: diff -a /home/navis/apache/oss-hive/build/ql/test/logs/clientpositive/auto_sortmerge_join_9.q.out /home/navis/apache/oss-hive/ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out
    [junit] Done query: auto_sortmerge_join_9.q elapsedTime=178s
    [junit] Cleaning up TestCliDriver
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 184.24 sec
{noformat}
"
HIVE-4502,NPE - subquery smb joins fails,Found this issue while running some SMB joins. Attaching test case that causes this error.
HIVE-4421,Improve memory usage by ORC dictionaries,"Currently, for tables with many string columns, it is possible to significantly underestimate the memory used by the ORC dictionaries and cause the query to run out of memory in the task. "
HIVE-4398,HS2 Resource leak: operation handles not cleaned when originating session is closed,"
In HS2 closing of sessions doesn't lead to closing of all the operation handles that the session had opened. This JIRA is meant to address this issue."
HIVE-4375,Single sourced multi insert consists of native and non-native table mixed throws NPE,"CREATE TABLE src_x1(key string, value string);
CREATE TABLE src_x2(key string, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (""hbase.columns.mapping"" = "":key,cf:string"");

explain
from src a
insert overwrite table src_x1
select key,value where a.key > 0 AND a.key < 50
insert overwrite table src_x2
select key,value where a.key > 50 AND a.key < 100;

throws,

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.addStatsTask(GenMRFileSink1.java:236)
	at org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.process(GenMRFileSink1.java:126)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:87)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:55)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:101)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genMapRedTasks(SemanticAnalyzer.java:8354)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8759)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:279)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
{noformat}"
HIVE-4342,NPE for query involving UNION ALL with nested JOIN and UNION ALL,"UNION ALL query with JOIN in first part and another UNION ALL in second part gives NPE.

bq. JOIN
UNION ALL
bq. UNION ALL

Attachments:
1. HiveCommands.txt : command script to setup schema for query under consideration.
2. sourceData1.txt and sourceData2.txt : required for above command script.
3. Query.txt : Exact query which produces NPE.

NOTE: you will need to update path to sourceData1.txt and sourceData2.txt in the HiveCommands.txt to suit your environment.

Attached files contain the schema and exact query which fails on Hive 0.9.
It is worthwhile to note that the same query executes successfully on Hive 0.7."
HIVE-4327,NPE in constant folding with decimal,"The query:

SELECT dec * cast('123456789012345678901234567890.1234567' as decimal) FROM DECIMAL_PRECISION LIMIT 1

fails with an NPE while constant folding. This only happens when the decimal is out of range of max precision.
"
HIVE-4248,Implement a memory manager for ORC,"With the large default stripe size (256MB) and dynamic partitions, it is quite easy for users to run out of memory when writing ORC files. We probably need a solution that keeps track of the total number of concurrent ORC writers and divides the available heap space between them. "
HIVE-4186,NPE in ReduceSinkDeDuplication,"When you have a sequence of RedueSinks on constants you get this error:
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.ReduceSinkDeDuplication$ReduceSinkDeduplicateProcFactory$ReducerReducerProc.getPartitionAndKeyColumnMapping(ReduceSinkDeDuplication.java:416)
{noformat}

The e.g. to generate this si:
{noformat}
select p_name from (select p_name from part distribute by 1 sort by 1) p distribute by 1 sort by 1
{noformat}

Sorry for the contrived e.g., but this actually happens when we stack windowing clauses (see PTF-Windowing branch)"
HIVE-4154,NPE reading column of empty string from ORC file,"If a String column contains only empty strings, a null pointer exception is throws from the RecordReaderImpl for ORC."
HIVE-4151,HiveProfiler NPE with ScriptOperator,
HIVE-4119,ANALYZE TABLE ... COMPUTE STATISTICS FOR COLUMNS fails with NPE if the table is empty,"ANALYZE TABLE ... COMPUTE STATISTICS FOR COLUMNS fails with NPE if the table is empty


{code}
hive -e ""create table empty_table (i int); select compute_stats(i, 16) from empty_table""


java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt(PrimitiveObjectInspectorUtils.java:535)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFLongStatsEvaluator.iterate(GenericUDAFComputeStats.java:477)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:139)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1099)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:558)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1132)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:558)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt(PrimitiveObjectInspectorUtils.java:535)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFLongStatsEvaluator.iterate(GenericUDAFComputeStats.java:477)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:139)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1099)
	... 15 more
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1132)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:558)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt(PrimitiveObjectInspectorUtils.java:535)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFLongStatsEvaluator.iterate(GenericUDAFComputeStats.java:477)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:139)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1099)
	... 15 more

{code}
"
HIVE-4079,Altering a view partition fails with NPE,"Altering a view partition e.g. to add partition parameters, fails with a null pointer exception in the ObjectStore class.

Currently, this is only possible using the metastore Thrift API and there are no testcases for it."
HIVE-4033,NPE at runtime while selecting virtual column after joining three tables on different keys,This is due to hasVC flag in MapOperator is incorrectly initialized.
HIVE-4029,Hive Profiler dies with NPE,"Steps to reproduce:

{noformat}
$ git clone https://github.com/apache/hive.git hive-profiler-npe
Initialized empty Git repository in /home/brock/hive-profiler-npe/.git/
remote: Counting objects: 73654, done.
remote: Compressing objects: 100% (15383/15383), done.
remote: Total 73654 (delta 44331), reused 71338 (delta 43054)
Receiving objects: 100% (73654/73654), 42.78 MiB | 1.69 MiB/s, done.
Resolving deltas: 100% (44331/44331), done.
$ cd hive-profiler-npe/
$ ant clean package
$ cd build/dist/
$ ./bin/hive
hive> DROP TABLE IF EXISTS users;
hive> CREATE TABLE users (
user string,
passwd string,
uid int,
gid int,
name string,
home string,
shell string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
STORED AS TEXTFILE;

hive> LOAD DATA LOCAL INPATH '/etc/passwd' INTO TABLE users;

hive> set hive.exec.operator.hooks=org.apache.hadoop.hive.ql.profiler.HiveProfiler;
set hive.exec.operator.hooks=org.apache.hadoop.hive.ql.profiler.HiveProfiler
hive> set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.HiveProfilerResultsHook;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.HiveProfilerResultsHook
hive> SET hive.exec.mode.local.auto=false;
SET hive.exec.mode.local.auto=false
hive> SET hive.task.progress=true;
SET hive.task.progress=true
hive> 
    > select count(1) from users;
select count(1) from users
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201302131617_0022, Tracking URL = http://localhost.localdomain:50030/jobdetails.jsp?jobid=job_201302131617_0022
Kill Command = /usr/local/hadoop-1.0.4/libexec/../bin/hadoop job  -kill job_201302131617_0022
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-02-16 10:24:14,215 Stage-1 map = 0%,  reduce = 0%
2013-02-16 10:24:44,354 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201302131617_0022 with errors
Error during job, obtaining debugging information...
Job Tracking URL: http://localhost.localdomain:50030/jobdetails.jsp?jobid=job_201302131617_0022
Examining task ID: task_201302131617_0022_m_000002 (and more) from job job_201302131617_0022

Task with the most failures(4): 
-----
Task ID:
  task_201302131617_0022_m_000000

URL:
  http://localhost.localdomain:50030/taskdetails.jsp?jobid=job_201302131617_0022&tipid=task_201302131617_0022_m_000000
-----
Diagnostic Messages for this Task:
java.lang.RuntimeException: Hive Runtime Error while closing operators
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:227)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.profiler.HiveProfilePublisher.publishStat(HiveProfilePublisher.java:85)
	at org.apache.hadoop.hive.ql.profiler.HiveProfiler.close(HiveProfiler.java:110)
	at org.apache.hadoop.hive.ql.exec.Operator.closeOperatorHooks(Operator.java:452)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:605)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:194)
	... 8 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec

{noformat}

"
HIVE-3996,Correctly enforce the memory limit on the multi-table map-join,"Currently with HIVE-3784, the joins are converted to map-joins based on checks of the table size against the config variable: hive.auto.convert.join.noconditionaltask.size. 

However, the current implementation will also merge multiple mapjoin operators into a single task regardless of whether the sum of the table sizes will exceed the configured value.
"
HIVE-3927,Potential overflow with new RCFileCat column sizes options,"The uncompressed/compressed sizes of columns may fit into ints for a single block of an RC file, but the same does not hold when they are summed across the file.  Should update the array which aggregates this sum to be an array of longs."
HIVE-3913,Possible deadlock in ZK lock manager,"ZK Hive lock manager can get into a state when the connection is closed, but no reconnection is attempted."
HIVE-3872,MAP JOIN  for VIEW throws NULL pointer exception error,"I have created a view  as shown below. 

CREATE VIEW V1 AS
select /*+ MAPJOIN(t1) ,MAPJOIN(t2)  */ t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3 from TABLE1 t1 join TABLE t2 on ( t1.f2= t2.f2 and t1.f3 = t2.f3 and t1.f4 = t2.f4 ) group by t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3

View get created successfully however when I execute below mentioned SQL or any SQL on the view  get NULLPOINTER exception error

hive> select count (*) from V1;
FAILED: NullPointerException null
hive>

Is there anything wrong with the view creation ?

Next I created view without MAPJOIN hints 

CREATE VIEW V1 AS
select  t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3 from TABLE1 t1 join TABLE t2 on ( t1.f2= t2.f2 and t1.f3 = t2.f3 and t1.f4 = t2.f4 ) group by t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3

Before executing select SQL I excute set  hive.auto.convert.join=true; 

I am getting beloow mentioned warnings
java.lang.InstantiationException: org.apache.hadoop.hive.ql.parse.ASTNodeOrigin
Continuing ...
java.lang.RuntimeException: failed to evaluate: <unbound>=Class.new();
Continuing ...


And I see from log that total 5 mapreduce jobs are started however when don't set auto.convert.join to true, I see only 3 mapreduce jobs getting invoked.
Total MapReduce jobs = 5
Ended Job = 1116112419, job is filtered out (removed at runtime).
Ended Job = -33256989, job is filtered out (removed at runtime).
WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files."
HIVE-3846,alter view rename NPEs with authorization on.,
HIVE-3795,NPE in SELECT when WHERE-clause is an and/or/not operation involving null,"Sometimes users forget to quote date constants in queries. For example, SELECT * FROM some_table WHERE ds >= 2012-12-10 and ds <= 2012-12-12; . In such cases, if the WHERE-clause contains and/or/not operation, it would throw NPE exception. That's because PcrExprProcFactory in ql/optimizer forgot to check null. "
HIVE-3760,TestNegativeMinimrCliDriver_mapreduce_stack_trace.q fails on hadoop-1,"Actually functionality is working correctly, but incorrect include/exclude macro in test directive is making its .q.out comparison against incorrect results."
HIVE-3723,Hive Driver leaks ZooKeeper connections,"In certain error cases (i.e.: statement fails to compile, semantic errors) the hive driver leaks zookeeper connections.

This can be seen in the TestNegativeCliDriver test which accumulates a large number of open file handles and fails if the max allowed number of file handles isn't at least 2048."
HIVE-3697,External JAR files on HDFS can lead to race condition with hive.downloaded.resources.dir,"I've seen situations where utilizing JAR files on HDFS can cause job failures via CNFE or JVM crashes. 

This is difficult to replicate, seems to be related to JAR size, latency between client and HDFS cluster, but I've got some example stack traces below. Seems that the calls made to FileSystem (copyToLocal) which are static and will be executed to delete the current local copy can cause the file(s) to be removed during job processing.

We should consider changing the default for hive.downloaded.resources.dir to include some level of uniqueness per job. We should not consider hive.session.id however, as execution of multiple statements via the same user/session which might access the same JAR files will utilize the same session.

A proposal might be to utilize System.nanoTime() -- which might be enough to avoid the issue, although it's not perfect (depends on JVM and system for level of precision) as part of the default (/tmp/${user.name}/resources/System.nanoTime()/). 

If anyone else has hit this, would like to capture environment information as well. Perhaps there is something else at play here. 

Here are some examples of the errors:

for i in {0..2}; do hive -S -f query.q& done
[2] 48405
[3] 48406
[4] 48407
% #
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGBUS (0x7) at pc=0x00007fb10bd931f0, pid=48407, tid=140398456698624
#
# JRE version: 6.0_31-b04
# Java VM: Java HotSpot(TM) 64-Bit Server VM (20.6-b01 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libzip.so+0xb1f0]  __int128+0x60
#
# An error report file with more information is saved as:
# /home/.../hs_err_pid48407.log
#
# If you would like to submit a bug report, please visit:
#   http://java.sun.com/webapps/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
java.lang.NoClassDefFoundError: com/example/udf/Lower
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.getUdfClass(FunctionTask.java:105)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.createFunction(FunctionTask.java:75)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:63)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1331)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1117)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:950)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:341)
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:439)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:449)
        at org.apache.hadoop.hive.cli.CliDriver.processInitFiles(CliDriver.java:485)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:692)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Caused by: java.lang.ClassNotFoundException: com.example.udf.Lower
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        ... 24 more
FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.FunctionTask

Another:
for i in {0..2}; do hive -S -f query.q& done
[1] 16294 
[2] 16295 
[3] 16296 
[]$ Couldn't create directory /tmp/ctm/resources/
Couldn't create directory /tmp/ctm/resources/"
HIVE-3659,TestHiveHistory::testQueryloglocParentDirNotExist Test fails on Windows because of some resource leaks in ZK,"Hive uses ZK for locking. In some test cases, ZK is not behaving well. In thread dumps, I saw it is waiting for locks to be released but they were not getting released. Hive tries to release locks but keeps failing, it eventually times out for its release attempts, which in default settings takes 10 mins. This is also the cause of why some queries take extra-ordinarily long to run. I suggest to disable ZK locking till ZK is certified for windows.

In this test case, I don’t see a requirement to use ZK so I am disabling the HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY to work around the issue.
"
HIVE-3636,Catch the NPe when using ^D to exit from CLI,The exit patch is just a quick hack to catch the NPE in order to allow ^D to exit hive without a stacktrace.
HIVE-3582,NPE in union processing followed by lateral view followed by 2 group bys,"EXPLAIN 
SELECT e.key, e.arr_ele, count(1) FROM (
  SELECT d.key as key, d.arr_ele as arr_ele, d.value  as value, count(1) as cnt FROM (
    SELECT c.arr_ele as arr_ele, a.key as key, a.value as value FROM (
      SELECT key, value, array(1,2,3) as arr
      FROM src

      UNION ALL
   
      SELECT key, value, array(1,2,3) as arr
      FROM srcpart
      WHERE ds = '2008-04-08' and hr='12'
    ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele
  ) d group by d.key, d.arr_ele, d.value
) e group by e.key, e.arr_ele;"
HIVE-3525,Avro Maps with Nullable Values fail with NPE,"When working against current trunk@1393794, using a backing Avro schema that has a Map field with nullable values causes a NPE on deserialization when the map contains a null value."
HIVE-3497,Avoid NPE in skewed information read,Table.java reads skwed information is not safe. Add check to make is safe
HIVE-3481,<Resource leak>: Hiveserver is not closing the existing driver handle before executing the next command. It results in to file handle leaks.,Close the driver object if it exists before creating another driver object. Bunch of HiveServer & JDBC related unit tests are failing because of these file handle leaks.
HIVE-3480,<Resource leak>: Fix the file handle leaks in Symbolic & Symlink related input formats.,Noticed these file handle leaks while fixing the Symlink related unit test failures on Windows.
HIVE-3452,Missing column causes null pointer exception,"select * from src where src = 'alkdfaj';
FAILED: SemanticException null"
HIVE-3431,Avoid race conditions while downloading resources from non-local filesystem,"""add resource <remote-uri>"" command downloads the resource file to location specified by conf ""hive.downloaded.resources.dir"" in local file system. But when the command above is executed concurrently to hive-server for same file, some client fails by VM crash, which is caused by overwritten file by other requests.

So there should be a configuration to provide per request location for add resource command, something like ""set hiveconf:hive.downloaded.resources.dir=temporary"""
HIVE-3317,Fix “TestDosToUnix” unit tests on Windows by closing the leaking file handle in DosToUnix.java.,Windows can’t delete the files if there are any open file handles on it so it is required to close them properly after completing the validation in DosToUnix utilities.
HIVE-3303,Fix error code inconsistency bug in mapreduce_stack_trace.q and mapreduce_stack_trace_turnoff.q when running hive on hadoop23,"when running hive on hadoop23, mapreduce_stack_trace.q and mapreduce_stack_trace_turnoff.q are having inconsistent error code diffs:

[junit] diff -a /home/cloudera/Code/hive/build/ql/test/logs/clientnegative/mapreduce_stack_trace.q.out /home/cloudera/Code/hive/ql/src/test/results/clientnegative/mapreduce_stack_trace.q.out
[junit] < FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
[junit] > FAILED: Execution Error, return code 20000 from org.apache.hadoop.hive.ql.exec.MapRedTask. Unable to initialize custom script.


[junit] diff -a /home/cloudera/Code/hive/build/ql/test/logs/clientnegative/mapreduce_stack_trace_turnoff.q.out /home/cloudera/Code/hive/ql/src/test/results/clientnegative/mapreduce_stack_trace_turnoff.q.out
[junit] 5c5
[junit] < FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
[junit] —
[junit] > FAILED: Execution Error, return code 20000 from org.apache.hadoop.hive.ql.exec.MapRedTask. Unable to initialize custom script

The error code 20000(which indicates unable to initialize custom script) could not be retrieved. 

"
HIVE-3302,Race condition in query plan for merging at the end of a query,"In the query plan that's used to merge files at the end of a query, the dependency tree looks something like:
                   MoveTask(1)
                  /           \
...ConditionalTask             MoveTask(2)...
                  \           /
                   MergeTask

Here MoveTask(1) moves the partition data to a temporary location, and MoveTask(2) moves it to the final location.

However if there are dynamic partitions generated and some of these partitions are merged and others are moved, the dependency tree is changed at runtime to:
...ConditionalTask           MoveTask(2)...
                  \         /
                   MergeTask
                            \
                             MoveTask(1)

This produces a race condition between the two MoveTasks where if MoveTask(2) runs before MoveTask(1) the partitions moved by MoveTask(1) will get moved to an intermediate location and never moved to the final location.  In this case those partitions are quietly lost."
HIVE-3301,Fix quote printing bug in mapreduce_stack_trace.q testcase failure when running hive on hadoop23,"When running hive on hadoop0.23, mapreduce_stack_trace.q is failing due to quote printing bug:

quote is printed as: '&quot;', instead of ""

Seems not able to state the bug clearly in html:

quote is printed as 'address sign' + 'quot' + semicolon
not the expected 'quote sign'"
HIVE-3265,HiveHistory.printRowCount() throws NPE,
HIVE-3232,Resource Leak: Fix the File handle leak in EximUtil.java,"1) Not closing the file handle EximUtil after reading the metadata from the file.
2) Nit: Get the path from URI to handle the Windows paths.
"
HIVE-3225,NPE on a join query with authorization enabled,"when performing a join query which filters by a non-existent partition in the where clause (ie):

select t1.a as a1, t2.a as a2 from t1 join t2 on t1.a=t2.a where t2.part=""non-existent"";

It returns an NPE. It seems that the partition since non-existent is not part of the list of inputs (or maybe optimized out?). But the TableScanOperator still has a reference to it which causes an NPE after tableUsePartLevelAuth.get() returns null.


FAILED: Hive Internal Error: java.lang.NullPointerException(null)java.lang.NullPointerException        at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:617)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:486)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:336)        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:909)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:689)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:557)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
"
HIVE-3205,Bucketed mapjoin on partitioned table which has no partition throws NPE,"{code}
create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;
create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;

set hive.optimize.bucketmapjoin = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;
{code}

throws NPE
{noformat}
2012-06-28 08:59:13,459 ERROR ql.Driver (SessionState.java:printError(400)) - FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer$BucketMapjoinOptProc.process(BucketMapJoinOptimizer.java:269)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.transform(BucketMapJoinOptimizer.java:100)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:87)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7564)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:50)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
{noformat}"
HIVE-3203,Drop partition throws NPE if table doesn't exist,"ALTER TABLE t1 DROP PARTITION (part = '1');

This throws an NPE if t1 doesn't exist.  A SemanticException would be cleaner."
HIVE-3191,timestamp - timestamp causes null pointer exception,"select tts.rnum, tts.cts - tts.cts from cert.tts tts

Error: Query returned non-zero code: 12, cause: FAILED: Hive Internal Error: java.lang.NullPointerException(null)
SQLState:  42000
ErrorCode: 12

create table if not exists CERT.TTS ( RNUM int , CTS timestamp) 
stored as sequencefile;"
HIVE-3153,Release codecs and output streams between flushes of RCFile,"Currently, RCFile writer holds a compression codec per a file and a compression output stream per a column. Especially for queries that use dynamic partitions this quickly consumes a lot of memory.

I'd like flushRecords to get a codec from the pool and create the compression output stream in flushRecords."
HIVE-3098,Memory leak from large number of FileSystem instances in FileSystem.CACHE,"The problem manifested from stress-testing HCatalog 0.4.1 (as part of testing the Oracle backend).

The HCatalog server ran out of memory (-Xmx2048m) when pounded by 60-threads, in under 24 hours. The heap-dump indicates that hadoop::FileSystem.CACHE had 1000000 instances of FileSystem, whose combined retained-mem consumed the entire heap.

It boiled down to hadoop::UserGroupInformation::equals() being implemented such that the ""Subject"" member is compared for equality (""==""), and not equivalence ("".equals()""). This causes equivalent UGI instances to compare as unequal, and causes a new FileSystem instance to be created and cached.

The UGI.equals() is so implemented, incidentally, as a fix for yet another problem (HADOOP-6670); so it is unlikely that that implementation can be modified.

The solution for this is to check for UGI equivalence in HCatalog (i.e. in the Hive metastore), using an cache for UGI instances in the shims.

I have a patch to fix this. I'll upload it shortly. I just ran an overnight test to confirm that the memory-leak has been arrested."
HIVE-3008,Memory leak in TUGIContainingTransport,Identical bug as in THRIFT-1468
HIVE-2956,[hive] Provide error message when using UDAF in the place of UDF instead of throwing NPE,"For example, 

{code}
hive> select distinct deptno, sum(deptno) from emp;
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:214)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:767)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:888)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:165)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:7755)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:7713)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:2793)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggr1MR(SemanticAnalyzer.java:3651)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:6125)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6762)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7531)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
{code}

Trivial.. but people always reports this confused by esoteric custom function names."
HIVE-2933,analyze command throw NPE when table doesn't exists,analyze command throw NPE when table doesn't exists
HIVE-2929,race condition in DAG execute tasks for hive,"select ...
(
SubQuery involving MapReduce
union all
SubQuery involving MapReduce
);

or 

select ...
(SubQuery involving MapReduce)
join
(SubQuery involving MapReduce)
;

If both the subQueries finish at nearly the same time, there is a race 
condition in which the results of the subQuery finishing last will be completely missed.
"
HIVE-2920,TestStatsPublisherEnhanced throws NPE on JDBC connection failure,
HIVE-2858,Cache remote map reduce job stack traces for additional logging,"Currently we are parsing the task logs for failed jobs for information to display to the user in the CLI.  In addition, we could parse those logs for stack traces and store e them in the SessionState.  This way, when we log failed queries, these will give us a decent idea of why those queries failed."
HIVE-2800,"NPE in ""create index"" without comment clause in external metastore","This happens only when using external metastore (with --hiveconf hive.metastore.uris=thrift://localhost:8088 --hiveconf hive.metastore.local=false). Also if I gave a comment in the statement, this exception go away.

Here is the statement:
create index test111 on table hcat_test(name) as 'compact' with deferred rebuild;

Here is the stack:
2012-02-10 17:07:42,612 ERROR exec.Task (SessionState.java:printError(380)) - FAILED: Error in metadata: java.lang.NullPointerException
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.Hive.createIndex(Hive.java:725)
        at org.apache.hadoop.hive.ql.exec.DDLTask.createIndex(DDLTask.java:822)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1291)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1082)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:933)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.NullPointerException
        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:185)
        at org.apache.hadoop.hive.metastore.api.Index.write(Index.java:1032)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_index_args.write(ThriftHiveMetastore.java:47518)
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_add_index(ThriftHiveMetastore.java:1675)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.add_index(ThriftHiveMetastore.java:1666)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createIndex(HiveMetaStoreClient.java:853)
        at org.apache.hadoop.hive.ql.metadata.Hive.createIndex(Hive.java:722)
        ... 17 more"
HIVE-2754,NPE in union with lateral view,
HIVE-2718,NPE in union followed by join,
HIVE-2706,StackOverflowError when using custom UDF after adding archive after adding jars,"When a custom UDF is used in a query after add an archive, such as a zip file, after adding jars, the XMLEncoder enters an infinite loop when serializing the map reduce task, as part of sending it to be executed. This results in a stack overflow error."
HIVE-2666,StackOverflowError when using custom UDF in map join,"When a custom UDF is used as part of a join which is converted to a map join, the XMLEncoder enters an infinite loop when serializing the map reduce task for the second time, as part of sending it to be executed.  This results in a stack overflow error."
HIVE-2581,explain task: getJSONPlan throws a NPE if the ast is null,
HIVE-2510,Hive throws Null Pointer Exception upon CREATE TABLE <db_name>.<table_name> ....     if the given <db_name> doesn't exist,
HIVE-2473,Hive throws an NPE when $HADOOP_HOME points to a tarball install directory that contains a build/ subdirectory.,
HIVE-2402,Function like with empty string is throwing null pointer exception,"select emp.ename from emp where ename like ''
This query is throwing null pointer exception"
HIVE-2334,DESCRIBE TABLE causes NPE when hive.cli.print.header=true,
HIVE-2182,Avoid null pointer exception when executing UDF,"For using UDF's executed following steps

{noformat}
add jar /home/udf/udf.jar;
create temporary function grade as 'udf.Grade';
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;
{noformat}

But from the above steps if we miss the first step (add jar) and execute remaining steps

{noformat}
create temporary function grade as 'udf.Grade';
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;
{noformat}

In tasktracker it is throwing this exception
{noformat}
Caused by: java.lang.RuntimeException: Map operator initialization failed
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121)
		 ... 18 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
		 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:126)
		 at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:878)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:904)
		 at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
		 at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
		 at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:444)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:98)
		 ... 18 more
Caused by: java.lang.NullPointerException
		 at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:107)
		 ... 31 more
{noformat}
Instead of null pointer exception it should throw meaning full exception"
HIVE-2159,"TableSample(percent ) uses one intermediate size to be int, which overflows for large sampled size, making the sampling never triggered.",
HIVE-2157,NPE in MapJoinObjectKey,
HIVE-2145,NPE during parsing order-by expression,"The following query throws NPE, where it should have throw parsing exception. 

hive> select key, count(1) cnt from src group by key order by count(1) limit 10;
select key, count(1) cnt from src group by key order by count(1) limit 10;
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:640)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:761)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:156)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6830)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6788)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:4303)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:5461)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6022)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:6607)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:790)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:209)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:286)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:514)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)


We should still allow functions in the order by clause (as in MySQL) but fix the NPE."
HIVE-2082,Reduce memory consumption in preparing MapReduce job,"Hive client side consume a lot of memory when the number of input partitions is large. One reason is that each partition maintains a list of FieldSchema which are intended to deal with schema evolution. However they are not used currently and Hive uses the table level schema for all partitions. This will be fixed in HIVE-2050. The memory consumption by this part will be reduced by almost half (1.2GB to 700BM for 20k partitions). 

Another large chunk of memory consumption is in the MapReduce job setup phase when a PartitionDesc is created from each Partition object. A property object is maintained in PartitionDesc which contains a full list of columns and types. Due to the same reason, these should be the same as in the table level schema. Also the deserializer initialization takes large amount of memory, which should be avoided. My initial testing for these optimizations cut the memory consumption in half (700MB to 300MB for 20k partitions). "
HIVE-2060,CLI local mode hit NPE when exiting by ^D,CLI gets an NPE when running in local mode and hit an ^D to exit it. 
HIVE-2045,TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() throws Null Pointer Exception in some cases,"1) In TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() is doing null check for the tokenizer.
If tokenizer is null, fillTokenizer() method is called to get the tokenizer object. But fillTokenizer() method also can update the tokenizer with NULL , so NULL check should be done before using the tokenizer.

2) Also improved some logging in TCTLSeparatedProtocol.java"
HIVE-2031,Correct the exception message for the better traceability for the scenario load into the partitioned table having 2  partitions by specifying only one partition in the load statement. ," Load into the partitioned table having 2 partitions by specifying only one partition in the load statement is failing and logging the following exception message.

{noformat}
 org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)
	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)
	at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)
	at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{noformat}

This needs to be corrected in such a way what is the actual root cause for this."
HIVE-1959,Potential memory leak when same connection used for long time. TaskInfo and QueryInfo objects are getting accumulated on executing more queries on the same connection.,*org.apache.hadoop.hive.ql.history.HiveHistory$TaskInfo* and *org.apache.hadoop.hive.ql.history.HiveHistory$QueryInfo* these two objects are getting accumulated on executing more number of queries on the same connection. These objects are getting released only when the connection is closed.
HIVE-1908,FileHandler leak on partial iteration of the resultset. ,"If the ""resultset"" is not iterated completely ,  one filehandler is leaking

Ex: We need only first row. This case one resource is leaking

{code}

ResultSet resultSet = createStatement.executeQuery(""select * from sampletable"");

if (resultSet.next())
{
	System.out.println(resultSet.getString(1)+""   ""+resultSet.getString(2));
} 

{code}


Command used for checking the filehandlers
{code}
lsof -p {hive_process_id} > runjarlsof.txt
{code}

"
HIVE-1884,Potential risk of resource leaks in Hive,"h3.There are couple of resource leaks.
h4.For example,

In CliDriver.java, Method :- processReader() the buffered reader is not closed.

h3.Also there are risk(s) of  resource(s) getting leaked , in such cases we need to re factor the code to move closing of resources in finally block.

h4. For Example :- 

In Throttle.java   Method:- checkJobTracker() , the following code snippet might cause resource leak.

{code}
InputStream in = url.openStream();
in.read(buffer);
in.close();
{code}


Ideally and as per the best coding practices it should be like below

{code}

InputStream in=null;
try   {
        in = url.openStream();
        int numRead = in.read(buffer);
}
finally {
       IOUtils.closeStream(in);
}

{code}

Similar cases, were found in ExplainTask.java, DDLTask.java etc.Need to re factor all such occurrences.


"
HIVE-1844,Hanging hive client caused by TaskRunner's OutOfMemoryError,
HIVE-1758,optimize group by hash map memory,"Group By map side's hash map consumes a lot of memory, thereby decreasing its effectiveness.

We can use some of the optimizations from map-join to reduce the memory footprint:

  class KeyWrapper {
    int hashcode;
    ArrayList<Object> keys;
    // decide whether this is already in hashmap (keys in hashmap are deepcopied
    // version, and we need to use 'currentKeyObjectInspector').
    boolean copy = false;

1. Changes keys to Array
2. Optimize the scenario when keys is of a small size (1,2) etc

Let us start profiling it and take it from there"
HIVE-1678,NPE in MapJoin ,"The query with two map joins and a group by fails with following NPE:

Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)
        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)
"
HIVE-1547,Unarchiving operation throws NPE,"Unarchiving a partition throws a null pointer exception similar to the following:

2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

This error seems to be DFS specific, as local file system in the unit tests don't catch this."
HIVE-1320,NPE with lineage in a query of union alls on joins.,"The following query generates a NPE in the lineage ctx code

EXPLAIN
INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;

The stack trace is:

FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx$Index.mergeDependency(LineageCtx.java:116)
at org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory$UnionLineage.process(OpProcFactory.java:396)
at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
at org.apache.hadoop.hive.ql.optimizer.lineage.Generator.transform(Generator.java:72)
at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:83)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5976)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:48)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
"
HIVE-1316,Increase the memory limit for CLI client,"Hive CLI client stores the plan in memory and serialize it to XML file. the serialization takes a lot of memory. for large queries, we've seen OOM exception and GC overhead limit reached. It would be good to increase the max heap size of CLI client and disable GC overhead limit. "
HIVE-1308,<boolean> = <boolean> throws NPE,"Workaround is to just use <boolean> or NOT <boolean>

{code}
hive> select true=true from src;
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper.<init>(GenericUDFUtils.java:212)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:138)
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:587)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:708)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6136)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1831)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1663)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:4911)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:5421)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5952)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:304)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:377)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:303)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{code}"
HIVE-1205,RowContainer should flush out  dummy rows when the table desc is null,"When a skew key found, row container will flush all rows to hdfs. 
But if the table desc is null, it actually flush an empty file. It should flush out a file with dummy rows."
HIVE-1188,NPE when running TestJdbcDriver/TestHiveServer,"{noformat}
% ant test -Dtestcase=TestJdbcDriver

BUILD FAILED
/Users/carl/Projects/hive/hd11/hive/build.xml:154: The following error occurred while executing this line:
/Users/carl/Projects/hive/hd11/hive/build.xml:93: The following error occurred while executing this line:
/Users/carl/Projects/hive/hd11/hive/contrib/build.xml:77: java.lang.NullPointerException
	at java.util.Arrays$ArrayList.<init>(Arrays.java:3357)
	at java.util.Arrays.asList(Arrays.java:3343)
	at org.apache.hadoop.hive.ant.QTestGenTask.execute(QTestGenTask.java:248)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.Target.execute(Target.java:390)
	at org.apache.tools.ant.Target.performTasks(Target.java:411)
	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360)
	at org.apache.tools.ant.helper.SingleCheckExecutor.executeTargets(SingleCheckExecutor.java:38)
	at org.apache.tools.ant.Project.executeTargets(Project.java:1212)
	at org.apache.tools.ant.taskdefs.Ant.execute(Ant.java:441)
	at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:302)
	at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:221)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:68)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.MacroInstance.execute(MacroInstance.java:398)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.Target.execute(Target.java:390)
	at org.apache.tools.ant.Target.performTasks(Target.java:411)
	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360)
	at org.apache.tools.ant.Project.executeTarget(Project.java:1329)
	at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
	at org.apache.tools.ant.Project.executeTargets(Project.java:1212)
	at org.apache.tools.ant.Main.runBuild(Main.java:801)
	at org.apache.tools.ant.Main.startAnt(Main.java:218)
	at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)
	at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)
{noformat}

TestHiveServer throws the same error.
"
HIVE-1185,Fix RCFile resource leak when opening a non-RCFile,"See HADOOP-5476 for the bug in SequenceFile. We should do the same thing in RCFile.
"
HIVE-1097,groupby_bigdata.q sometimes throws out of memory exception,"I would get out of memory errors like the following when running groupby_bigdata.q.

{code}
  
    [junit] plan = /data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/scratchdir/plan38413.xml
    [junit] Exception in thread ""Thread-15"" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.util.Arrays.copyOf(Arrays.java:2882)
    [junit]     at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
    [junit]     at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)
    [junit]     at java.lang.StringBuffer.append(StringBuffer.java:224)
    [junit]     at java.io.StringWriter.write(StringWriter.java:84)
    [junit]     at java.io.PrintWriter.newLine(PrintWriter.java:436)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:585)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:696)
    [junit]     at java.lang.Throwable.printStackTrace(Throwable.java:512)
    [junit]     at org.apache.hadoop.util.StringUtils.stringifyException(StringUtils.java:60)
    [junit]     at org.apache.hadoop.hive.ql.exec.ScriptOperator$StreamThread.run(ScriptOperator.java:561)
    [junit] Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.nio.HeapCharBuffer.<init>(HeapCharBuffer.java:39)
    [junit]     at java.nio.CharBuffer.allocate(CharBuffer.java:312)
    [junit]     at java.nio.charset.CharsetEncoder.isLegalReplacement(CharsetEncoder.java:319)
    [junit]     at java.nio.charset.CharsetEncoder.replaceWith(CharsetEncoder.java:267)
    [junit]     at java.nio.charset.CharsetEncoder.<init>(CharsetEncoder.java:186)
    [junit]     at java.nio.charset.CharsetEncoder.<init>(CharsetEncoder.java:209)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.<init>(ISO_8859_1.java:116)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.<init>(ISO_8859_1.java:113)
    [junit]     at sun.nio.cs.ISO_8859_1.newEncoder(ISO_8859_1.java:46)
    [junit]     at java.lang.StringCoding$StringEncoder.<init>(StringCoding.java:215)
    [junit]     at java.lang.StringCoding$StringEncoder.<init>(StringCoding.java:207)
    [junit]     at java.lang.StringCoding.encode(StringCoding.java:266)
    [junit]     at java.lang.String.getBytes(String.java:947)
    [junit]     at java.io.UnixFileSystem.getLength(Native Method)
    [junit]     at java.io.File.length(File.java:848)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.<init>(RawLocalFileSystem.java:375)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:359)
    [junit]     at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)
    [junit]     at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:643)
    [junit]     at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:114)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:680)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:936)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] Traceback (most recent call last):
    [junit]   File ""../data/scripts/dumpdata_script.py"", line 6, in <module>
    [junit]     print 20000 * i + k

{code}"
HIVE-1064,NPE when operating HiveCLI in distributed mode,"{code}
hive> select id, name from tab_a;
select id, name from tab_a;
10/01/18 03:55:59 INFO parse.ParseDriver: Parsing command: select id, name from tab_a
10/01/18 03:55:59 INFO parse.ParseDriver: Parse Completed
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Completed phase 1 of Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Get metadata for source tables
10/01/18 03:55:59 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
10/01/18 03:55:59 INFO metastore.ObjectStore: ObjectStore, initialize called
10/01/18 03:56:03 INFO metastore.ObjectStore: Initialized ObjectStore
10/01/18 03:56:03 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=tab_a
10/01/18 03:56:03 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for subqueries
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for destination tables
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed getting MetaData in Semantic Analysis
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for FS(2)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for SEL(1)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for TS(0)
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed plan generation
10/01/18 03:56:04 INFO ql.Driver: Semantic Analysis Completed
10/01/18 03:56:04 INFO ql.Driver: Starting command: select id, name from tab_a
Total MapReduce jobs = 1
10/01/18 03:56:04 INFO ql.Driver: Total MapReduce jobs = 1
Launching Job 1 out of 1
10/01/18 03:56:04 INFO ql.Driver: Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
10/01/18 03:56:04 INFO exec.ExecDriver: Number of reduce tasks is set to 0 since there's no reduce operator
FAILED: Unknown exception : null
10/01/18 03:56:04 ERROR ql.Driver: FAILED: Unknown exception : null
java.lang.NullPointerException
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:288)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:475)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:103)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:64)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:589)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:469)
	at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:329)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:317)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
	at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)

hive> 
{code}"
HIVE-1011,GenericUDTFExplode() throws NPE when given nulls,"When explode() is called on rows where the array column is null, a null pointer exception is thrown.

For example consider having  a table named test with the following rows

{code}
key     value
1       [1,2,3]
2       null 
{code}

Then running the query

{code}
SELECT explode(value) AS myCol FROM test;

or

SELECT * FROM test LATERAL VIEW explode(value) myTab AS myCol;
{code}

will throw a null pointer exception when explode() gets the null value from the 2nd row.

Possible options are:

1. Treat null values as an error and make the user fix null values in data
2. explode(null) generates a null output value.
3. explode(null) generates no output values

#2 means that explode(null) and explode(array(null)) will behave identically. Because of that, I think #3 makes the most sense."
HIVE-996,"""describe function"" throws NPE when when called on UDTF or UDAF","{noformat}
hive> describe function explode;
describe function explode;
FAILED: Error in metadata: java.lang.NullPointerException
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive> describe function sum;
describe function sum;
FAILED: Error in metadata: java.lang.NullPointerException
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
hive> describe function conv;
describe function conv;
OK
conv(num, from_base, to_base) - convert num from from_base to to_base
Time taken: 0.042 seconds
{noformat}"
HIVE-963,no out of memory errors for skewed join,"Currently, in case of skew, hive runs out of memory.
A simpler fix would be to use JDBM to store data and use that.

It can be configurable and JDBM should only be triggered if the number of values for a given key exceed a given number."
HIVE-879,null pointer for empty dir ,
HIVE-878,Update the hash table entry before flushing in Group By hash aggregation,"This is a newly introduced bug from r796133.
We should first update the aggregation, and then we can flush the hash table. Otherwise the entry that we update might be already out of the hash table.
"
HIVE-876,UDFOPNegative should deal with NULL gracefully,"UDFOPNegative is throwing out NullPointerException. It should return NULL for that.
"
HIVE-865,mapjoin: memory leak for same key with very large number of values,
HIVE-864,mapjoin : memory leak,
HIVE-795,Return better error messaging from HiveServer ,"If an exception is thrown on the Hive server (i.e., when an invalid query is passed), the Hive server throws a HiveServerExcpeption to the client with a message like ""Query returned non-zero code: 10"". A more informative description of the cause of the error should be returned."
HIVE-790,race condition related to ScriptOperator + UnionOperator,"ScriptOperator uses a second thread to output the rows to the children operators. In a corner case which contains a union, 2 threads might be outputting data into the same operator hierarchy and caused race conditions.

{code}
CREATE TABLE tablea (cola STRING);
SELECT *
FROM (
    SELECT TRANSFORM(cola)
    USING 'cat'
    AS cola
    FROM tablea
  UNION ALL
    SELECT cola as cola
    FROM tablea
) a;
{code}
"
HIVE-689,dump more memory stat periodically,"Usually we see out of memory errors for all kinds of reasons - it is very difficult to track what was the reason for that.
We should dump more statistics at that point - currently, we dont even know whether we died because the current process was bad or
was it due to some other process on that machine"
HIVE-575,map join runs out of memory for a key with very large number of values,
HIVE-435,Empty passwd param causing NPE in ExecDriver,HIVE-403 can cause NPE if the password param is empty. 
HIVE-372,Nested UDFs cause _very_ high memory usage when processing query,"When nesting UDFs, the Hive Query processor takes a large amount of time+memory to process the query. For example, I ran something along the lines of:

select trim( trim( trim(trim( trim( trim( trim( trim( trim(column))))))))) from test_table;

This query needs 10GB+ of memory to process before it'll launch the job. The amount of memory increases exponentially with each nested UDF.

Obviously, I am using trim() in this case as a simple example that causes the same problem to occur. In my actual use-case I had a bunch of nested regexp_replaces."
HIVE-340,[hive] null pointer exception with nulls in map-side aggregation,
HIVE-320,Issuing queries with COUNT(DISTINCT) on a column that may contain null values hits a NPE,"When issuing a query that may contain a null value, I get a NPE. 

E.g. if 'middle_name' potentially holds null values,

select count(distinct middle_name) from people; will fail with the below exception.

Other queries that work with the same input set:
select distinct middle_name from people;
select count(1), middle_name from people group by middle_name;

java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:169)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2198)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:424)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:164)
	... 2 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:376)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:477)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:420)
	... 3 more

"
HIVE-250,shared memory java dbm for map-side joins,"can use either:
- sdbm: http://freshmeat.net/projects/solingerjavasdbm/
- jdbm: http://sourceforge.net/projects/jdbm/

both need modifications to use file mmaps instead of regular file io. will do some testing to see if there's a major difference between the two."
HIVE-230,While loading a table from a query that returns empty data results in null pointer exception ,"If the select query returns zero rows then the insert will fail with null pointer exception
INSERT OVERWRITE TABLE test_pc SELECT a.userid, a.ip FROM test_pc2 a WHERE (userid=595058415);

2009-01-13 10:16:21,396 ERROR exec.MoveTask (SessionState.java:printError(254)) - Failed with exception null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:127)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:212)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:174)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:207)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:305)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:166)
	at org.apache.hadoop.mapred.JobShell.run(JobShell.java:194)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.mapred.JobShell.main(JobShell.java:220)"
HIVE-220,incorrect log directory in TestMTQueries causing null pointer exception,mistyped  ')' on line 38 of TestMTQueries.java causing null pointer exception.
HIVE-129,Fix aux.jar packaging to work properly with 0.17 and 0.18 versions of hadoop,"ant -lib testlibs -Dhadoop.version=""0.17"" clean-test test

leads to failures in

input16.q
input16_cc.q
input3.q

as TestSerDe cannot be located."
