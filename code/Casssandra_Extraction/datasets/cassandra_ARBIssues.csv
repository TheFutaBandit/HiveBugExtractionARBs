Bug_ID,Bug_Summary,Bug_Description
CASSANDRA-20121,Preclude InMemoryJournal from saving Unitialized commands; remove invalidated commands in postExecute,
CASSANDRA-20116,Avoid prepared statement invalidation race when committing schema changes,"We currently invalidate the prepared statement cache before making the schema change “live"", creating a small window where we might re-prepare a statement with the old schema. This causes us to use the wrong epoch when coordinating a request, making replicas think the coordinator is behind and failing the request until the node is bounced (or prepared statement cache invalidated)"
CASSANDRA-20065,Accord's ConfigService lock is held over large areas which cause deadlocks and performance issues,"Accord has a AbstractConfigurationService that locks on “this” and does this over large ares, even when the lock is not needed.  This has been found to cause issues as messaging gets blocked on this causing timeouts.

To better improve things, we should lower the locking to what is needed and avoid locking while triggering callbacks"
CASSANDRA-19934,NPE in AffectedRangesImpl#intersects when other has different keyspaces than local,"This error was found on Accord branch.

{code}
 org.apache.cassandra.simulator.SimulationException: Failed on seed 0xfa3d51da237d56e5; Failure creating the simulation
Caused by: java.lang.AssertionError: Errors detected during simulation
	Suppressed: java.lang.NullPointerException
		at org.apache.cassandra.tcm.sequences.LockedRanges$AffectedRangesImpl.intersects(LockedRanges.java:337)
		at org.apache.cassandra.tcm.sequences.LockedRanges.intersects(LockedRanges.java:97)
		at org.apache.cassandra.tcm.transformations.PrepareLeave.execute(PrepareLeave.java:108)
		at org.apache.cassandra.tcm.AbstractLocalProcessor.executeStrictly(AbstractLocalProcessor.java:167)
		at org.apache.cassandra.tcm.AbstractLocalProcessor.commit(AbstractLocalProcessor.java:77)
		at org.apache.cassandra.distributed.test.log.TestProcessor.commit(TestProcessor.java:62)
		at org.apache.cassandra.tcm.Commit$Handler.doVerb(Commit.java:368)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:102)
		at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:70)
		at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:56)
		at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:122)
{code}

In debugger I see the following

{code}
other = {LockedRanges$AffectedRangesImpl@92088} ""AffectedRangesImpl{map={ReplicationParams{class=org.apache.cassandra.locator.MetaStrategy, dc0=2, dc2=2, dc1=2}=[(-9223372036854775808,-9223372036854775808]]}}""
 map = {HashMap@93801}  size = 1
  {ReplicationParams@92097} ""ReplicationParams{class=org.apache.cassandra.locator.MetaStrategy, dc0=2, dc2=2, dc1=2}"" -> {HashSet@92098}  size = 1
{code}

{code}
map = {HashMap@92092}  size = 3
 {ReplicationParams@93810} ""ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}"" -> {HashSet@93811}  size = 3
 {ReplicationParams@93812} ""ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}"" -> {HashSet@93813}  size = 4
 {ReplicationParams@93814} ""ReplicationParams{class=org.apache.cassandra.locator.NetworkTopologyStrategy, dc0=3, dc2=3, dc1=3}"" -> {HashSet@93815}  size = 7
{code}

Locally we are missing the meta keyspace so can’t find it from the map and NPE.

The reason for this is that the current node is starting up

{code}
StorageService.instance.operationMode == STARTING
{code}"
CASSANDRA-19868,Fix NPE in InformDurableSerializers,"Currently, R/W concurrent workload may throw:
{code}
java.lang.NullPointerException: null
	at org.apache.cassandra.service.accord.serializers.CommandSerializers$TimestampSerializer.serialize(CommandSerializers.java:97)
	at org.apache.cassandra.service.accord.serializers.InformDurableSerializers$1.serializeBody(InformDurableSerializers.java:39)
	at org.apache.cassandra.service.accord.serializers.InformDurableSerializers$1.serializeBody(InformDurableSerializers.java:35)
	at org.apache.cassandra.service.accord.serializers.TxnRequestSerializer.serialize(TxnRequestSerializer.java:46)
	at org.apache.cassandra.service.accord.serializers.TxnRequestSerializer.serialize(TxnRequestSerializer.java:31)
	at org.apache.cassandra.net.Message$Serializer.serialize(Message.java:833)
	at org.apache.cassandra.distributed.impl.Instance.serializeMessage(Instance.java:445)
	at org.apache.cassandra.distributed.impl.Instance.lambda$registerOutboundFilter$5(Instance.java:388)
	at org.apache.cassandra.net.OutboundSink$Filtered.accept(OutboundSink.java:54)
	at org.apache.cassandra.net.OutboundSink.accept(OutboundSink.java:70)
	at org.apache.cassandra.net.MessagingService.send(MessagingService.java:534)
	at org.apache.cassandra.net.MessagingService.send(MessagingService.java:472)
	at org.apache.cassandra.service.accord.AccordMessageSink.send(AccordMessageSink.java:228)
	at accord.local.Node.send(Node.java:526)
	at accord.local.Node.lambda$send$8(Node.java:464)
	at java.base/java.lang.Iterable.forEach(Iterable.java:75)
	at accord.local.Node.send(Node.java:464)
	at accord.coordinate.MaybeRecover.onDone(MaybeRecover.java:109)
	at accord.coordinate.ReadCoordinator.handle(ReadCoordinator.java:241)
	at accord.coordinate.ReadCoordinator.onSuccess(ReadCoordinator.java:145)
	at accord.coordinate.ReadCoordinator.onSuccess(ReadCoordinator.java:43)
{code}
"
CASSANDRA-19866,Fix Journal segment allocation/switch race condition,"Concurrent r/w workload is currently throwing:
{code}
java.lang.IllegalArgumentException: Can not reference segment 1724695101990
        at org.apache.cassandra.journal.Segments.isFlushed(Segments.java:189)
        at org.apache.cassandra.journal.Journal.isFlushed(Journal.java:199)
        at org.apache.cassandra.journal.Journal$FlusherCallbacks.submit(Journal.java:151)
        at org.apache.cassandra.journal.Journal.onFlush(Journal.java:204)
        at org.apache.cassandra.service.accord.AccordJournal.appendCommand(AccordJournal.java:264)
        at org.apache.cassandra.service.accord.AccordCommandStore.appendCommands(AccordCommandStore.java:579)
        at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:277)
        at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:303)
        at org.apache.cassandra.service.accord.async.AsyncOperation.onLoaded(AsyncOperation.java:169)
        at accord.utils.async.AsyncCallbacks.lambda$inExecutorService$0(AsyncCallbacks.java:36)
        at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code}"
CASSANDRA-19836,[Analytics] Fix NPE when writing UDT values,"When UDT field values are set to null, the bulk writer throws NPE, e.g. the stacktrace below. Although it is on the boolean type, the NPE can be thrown on all other types whenever the value is null.

{code:java}
Caused by: java.lang.NullPointerException
  at org.apache.cassandra.spark.data.types.Boolean.setInnerValue(Boolean.java:91)
  at org.apache.cassandra.spark.data.complex.CqlUdt.setInnerValue(CqlUdt.java:534)
  at org.apache.cassandra.spark.data.complex.CqlUdt.toUserTypeValue(CqlUdt.java:522)
  at org.apache.cassandra.spark.data.complex.CqlUdt.convertForCqlWriter(CqlUdt.java:169)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.maybeConvertUdt(RecordWriter.java:450)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.getBindValuesForColumns(RecordWriter.java:432)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.writeRow(RecordWriter.java:415)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.write(RecordWriter.java:202)
{code}

"
CASSANDRA-19794,NPE on Directory access during Memtable flush fails ShortPaxosSimulationTest,"Run {{ShortPaxosSimulationTest}} w/ the following arguments on trunk:

{noformat}
PaxosSimulationRunner.main(new String[] { ""run"", ""-n"", ""3..6"", ""-t"", ""1000"", ""-c"", ""2"", ""--cluster-action-limit"", ""2"", ""-s"", ""30"", ""--seed"", ""0xe0247e19a75e3bba"" });
{noformat}

You should see a failure, starting with...

{noformat}
[junit-timeout] WARN  [OptionalTasks:1] node5 2024-07-22 15:46:00,210 LegacyStateListener.java:158 - Token -6148914691236517205 changing ownership from /127.0.0.1:7012 to /127.0.0.6:7012
[junit-timeout] WARN  [OptionalTasks:1] node6 2024-07-22 15:46:00,259 SystemKeyspace.java:1287 - Using stored Gossip Generation 1577894856 as it is greater than current system time 1577894855.  See CASSANDRA-3654 if you experience problems
[junit-timeout] WARN  [OptionalTasks:1] node6 2024-07-22 15:46:00,277 LegacyStateListener.java:158 - Token -6148914691236517205 changing ownership from /127.0.0.1:7012 to /127.0.0.6:7012
[junit-timeout] ERROR [isolatedExecutor:3] node6 2024-07-22 15:46:00,469 ReconfigureCMS.java:184 - Could not finish adding the node to the Cluster Metadata Service
[junit-timeout] java.lang.IllegalStateException: Can not commit transformation: ""SERVER_ERROR""(class java.lang.NullPointerException).
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.lambda$commit$6(ClusterMetadataService.java:491)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:535)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:488)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.ReconfigureCMS.executeNext(ReconfigureCMS.java:179)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.InProgressSequences.resume(InProgressSequences.java:200)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.InProgressSequences.finishInProgressSequences(InProgressSequences.java:72)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.reconfigureCMS(ClusterMetadataService.java:372)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.ensureCMSPlacement(ClusterMetadataService.java:379)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.BootstrapAndReplace.executeNext(BootstrapAndReplace.java:274)
[junit-timeout] 	at org.apache.cassandra.simulator.cluster.OnClusterReplace$ExecuteNextStep.lambda$new$f5e64c00$1(OnClusterReplace.java:162)
[junit-timeout] 	at org.apache.cassandra.distributed.api.IInvokableInstance.unsafeRunOnThisThread(IInvokableInstance.java:85)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.SimulatedActionTask.lambda$asSafeRunnable$0(SimulatedActionTask.java:83)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.SimulatedActionTask$1.run(SimulatedActionTask.java:93)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:318)
[junit-timeout] 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout] 	at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}

...and underneath that...

{noformat}
[junit-timeout] Thread[ScheduledTasks:1,5,node3]
[junit-timeout] java.lang.NullPointerException
[junit-timeout] 	at org.apache.cassandra.utils.btree.AbstractBTreeMap.get(AbstractBTreeMap.java:92)
[junit-timeout] 	at org.apache.cassandra.tcm.membership.Directory.endpoint(Directory.java:312)
[junit-timeout] 	at org.apache.cassandra.tcm.transformations.cms.AdvanceCMSReconfiguration.executeRemove(AdvanceCMSReconfiguration.java:242)
[junit-timeout] 	at org.apache.cassandra.tcm.transformations.cms.AdvanceCMSReconfiguration.execute(AdvanceCMSReconfiguration.java:123)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.ReconfigureCMS.applyTo(ReconfigureCMS.java:149)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadata.writePlacementAllSettled(ClusterMetadata.java:275)
[junit-timeout] 	at org.apache.cassandra.db.DiskBoundaryManager.getLocalRanges(DiskBoundaryManager.java:158)
[junit-timeout] 	at org.apache.cassandra.db.DiskBoundaryManager.getDiskBoundaryValue(DiskBoundaryManager.java:121)
[junit-timeout] 	at org.apache.cassandra.db.DiskBoundaryManager.getDiskBoundaries(DiskBoundaryManager.java:65)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.getDiskBoundaries(ColumnFamilyStore.java:3676)
[junit-timeout] 	at org.apache.cassandra.db.compaction.CompactionStrategyManager.maybeReloadDiskBoundaries(CompactionStrategyManager.java:587)
[junit-timeout] 	at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:899)
[junit-timeout] 	at org.apache.cassandra.db.lifecycle.Tracker.notify(Tracker.java:558)
[junit-timeout] 	at org.apache.cassandra.db.lifecycle.Tracker.notifySwitched(Tracker.java:547)
[junit-timeout] 	at org.apache.cassandra.db.lifecycle.Tracker.switchMemtable(Tracker.java:390)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore$Flush.<init>(ColumnFamilyStore.java:1248)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:1074)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtableIfCurrent(ColumnFamilyStore.java:1055)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.signalFlushRequired(ColumnFamilyStore.java:1482)
[junit-timeout] 	at org.apache.cassandra.db.memtable.AbstractAllocatorMemtable.flushIfPeriodExpired(AbstractAllocatorMemtable.java:240)
[junit-timeout] 	at org.apache.cassandra.db.memtable.AbstractAllocatorMemtable$1.runMayThrow(AbstractAllocatorMemtable.java:221)
[junit-timeout] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.SimulatedExecution$1.call(SimulatedExecution.java:212)
[junit-timeout] 	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:585)
[junit-timeout] 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout] 	at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}

Reverting the changes from CASSANDRA-19705 allows the test to complete successfully, which makes sense, as {{ensureCMSPlacement()}} shows up in the trace above."
CASSANDRA-19785,Possible memory leak in BTree.FastBuilder ,"We are having a problem with the heap growing in size, This is a large cluster > 1,000 nodes across a large number of dc’s. This is running version 4.0.11.

 

Each node has a 32GB heap, and the amount used continues to grow until it reaches 30GB, it then struggles with multiple Full GC pauses, as can be seen here:

!image-2024-07-19-08-44-56-714.png!

We took 2 heap dumps on one node a few days after it was restarted, and the heap had grown by 2.7GB

 

9{^}th{^} July

!image-2024-07-19-08-45-17-289.png!

11{^}th{^} July

!image-2024-07-19-08-45-33-933.png!

This can be seen as mainly an increase of memory used by FastThreadLocalThread, increasing from 5.92GB to 8.53GB

!image-2024-07-19-08-45-50-383.png!

!image-2024-07-19-08-46-06-919.png!

Looking deeper into this it can be seen that the growing heap is contained within the threads for the MutationStage, Native-transport-Requests, ReadStage etc. We would expect the memory used within these threads to be short lived, and not grow as time goes on.  We recently increased the size of theses threadpools, and that has increased the size of the problem.

 

Top memory usage for FastThreadLocalThread

9{^}th{^} July

!image-2024-07-19-08-46-42-979.png!

11{^}th{^} July

!image-2024-07-19-08-46-56-594.png!
This has led us to investigate whether there could be a memory leak, and we have found the following issues within the retained references in BTree.FastBuilder objects. The issue appears to stem from the reset() method, which does not properly clear all buffers.  We are not really sure how the BTree.FastBuilder works, but this this is our analysis of where a leak might occur.

 

Specifically:

Leaf Buffer Not Being Cleared:
When leaf().count is 0, the statement Arrays.fill(leaf().buffer, 0, leaf().count, null); does not clear the buffer because the end index is 0. This leaves the buffer with references to potentially large objects, preventing garbage collection and increasing heap usage.

Branch inUse Property:
If the inUse property of the branch is set to false elsewhere in the code, the while loop while (branch != null && branch.inUse) does not execute, resulting in uncleared branch buffers and retained references.

 

This is based on the following observations:

    Heap Dumps: Analysis of heap dumps shows that leaf().count is often 0, and as a result, the buffer is not being cleared, leading to high heap utilization.

!image-2024-07-19-08-47-19-517.png!

    Remote Debugging: Debugging sessions indicate that the drain() method sets count to 0, and the inUse flag for the parent branch is set to false, preventing the while loop in reset() from clearing the branch buffers.

!image-2024-07-19-08-47-34-582.png!

 "
CASSANDRA-19783,In-jvm dtest to detect InstanceClassLoaderLeaks,"It is currently very easy to add dependencies/code that cause in-jvm dtest {{InstanceClassLoader}} leaks, and hard to find them. These patches add a WeakHashSet that allows us to count the number of reachable InstanceClassLoaders in the in-jvm dtest API, so that we can use it in the ResourceLeakTest in the actual Cassandra branches in CI (it only takes 2 iterations to find the leak recently introduced by the inclusion of the {{oshi.jna}}  library, which was fixed in [https://github.com/apache/cassandra-in-jvm-dtest-api/pull/38])

Notes:

The trunk patch +requires+ the changes made in the in-jvm-dtest-api project in order to compile.

Additionally, ResourceLeakTest#looperEverythingTest (the newly-enabled test) won't fail once we pick up the changes made to include `osha.jna` in the in-jvm-dtest-api project, so I added some ""DO NOT COMMIT"" code there to prove the test can in fact detect the leak.

 

NOTE: I'm removing the `DO NOT COMMIT` code that demonstrates the test actually fails when there's a leak. If you want to try it out, you can change the line 202 in ResourceLeakTest.java to this: 

 
{code:java}
            try (Cluster cluster = (Cluster) builder.withNodes(numClusterNodes).withConfig(updater)
                    .withSharedClasses(builder.getSharedClasses().and(name -> !name.startsWith(""oshi.jna."")))
                    .start())
{code}
This removes the `oshi.jna` classes from the shared class loader, which demonstrates the failure."
CASSANDRA-19752,Debian packaging fails after openjdk-8* and java8* removed from bullseye,"No candidates for {{`openjdk-8-jdk | java8-jdk`}}

Failure occurs at the {{mk-build-deps}} step…
{noformat}
Broken cassandra-build-deps:amd64 Depends on openjdk-8-jdk:amd64 < none @un H >
     Removing cassandra-build-deps:amd64 because I can't find openjdk-8-jdk:amd64
Broken cassandra-build-deps:amd64 Depends on java8-jdk:amd64 < none @un H >
     Removing cassandra-build-deps:amd64 because I can't find java8-jdk:amd64
     Or group remove for cassandra-build-deps:amd64
…
mk-build-deps: Unable to install cassandra-build-deps at /usr/bin/mk-build-deps line 457.
mk-build-deps: Unable to install all build-dep packages
{noformat}
ref: https://ci-cassandra.apache.org/job/Cassandra-3.0-artifacts/jdk=jdk_1.8_latest,label=cassandra/428/console "
CASSANDRA-19632,wrap tracing logs in isTraceEnabled across the codebase,"Our usage of logger.isTraceEnabled across the codebase is inconsistent. This would also fix issues similar in e.g. CASSANDRA-19429 as [~rustyrazorblade] suggested.

We should fix this at least in trunk and 5.0 (not critical though) and probably come up with a checkstyle rule to prevent not calling isTraceEnabled while logging with TRACE level. "
CASSANDRA-19605,Accord: NPE in RangeDeps.forEach,"{code}
java.lang.NullPointerException
	accord.primitives.RangeDeps.visitTxnIdxsForRangeIndex(RangeDeps.java:249)
	accord.utils.CheckpointIntervalArray.forEach(CheckpointIntervalArray.java:219)
	accord.utils.CheckpointIntervalArray.forEach(CheckpointIntervalArray.java:127)
	accord.utils.CheckpointIntervalArray.forEach(CheckpointIntervalArray.java:97)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:178)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:202)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:281)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:273)
	org.apache.cassandra.service.accord.AccordSafeCommandStore.registerHistoricalTransactions(AccordSafeCommandStore.java:190)
{code}"
CASSANDRA-19440,Non-serial writes can race with Accord topology changes,"Accord and Paxos handle these, but non-SERIAL writes don't check for this condition and can't retry the portions of the write that failed on the correct system until the entire thing succeeds."
CASSANDRA-19391,Flush metadata snapshot table on every write,"We depend on the latest snapshot when starting up, flushing avoids gaps between latest snapshot and the most recent local log entry"
CASSANDRA-19365,invalid EstimatedHistogramReservoirSnapshot::getValue values due to race condition in DecayingEstimatedHistogramReservoir,"`DecayingEstimatedHistogramReservoir` has a race condition between `update` and `rescaleIfNeeded`.
A sample which ends up (`update`) in an already scaled decayingBucket (`rescaleIfNeeded`) may still use a non-scaled weight because `decayLandmark` has not been updated yet at the moment of `update`.
 
The observed consequence was flooding of the cluster with speculative retries (we happened to hit low-percentile buckets with overweight samples, which drove p99 below true p50 for a long time).

Please note that despite the manifestation being similar to CASSANDRA-19330, these are two distinct bugs in their own right.

This bug affects versions 4.0+
On 3.11 there's locking in DEHR. I did not check earlier versions."
CASSANDRA-19361,fix node info NPE when ClusterMetadata is null,"h3. How

 
I create an ensemble with 3 nodes(It works well), then I add the fourth node to join the party. 
when executing nodetool info, get the following exception:
{code:java}
➜  bin ./nodetool info

java.lang.NullPointerException at org.apache.cassandra.service.StorageService.operationMode(StorageService.java:3744) at org.apache.cassandra.service.StorageService.isBootstrapFailed(StorageService.java:3810) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)   

➜  bin ./nodetool info 

WARN  [InternalResponseStage:152] 2024-02-02 11:45:15,731 RemoteProcessor.java:213 - Got error from /127.0.0.4:7000: TIMEOUT when sending TCM_COMMIT_REQ, retrying on CandidateIterator{candidates=[/127.0.0.4:7000], checkLive=true} error: null -- StackTrace -- java.lang.NullPointerException at org.apache.cassandra.service.StorageService.getLocalHostId(StorageService.java:1904) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71) at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:260){code}
server 1 cannot execute node info and cql shell, server 2 and 3 can do it. Try to query the system prefix tables, I attach stack error log for the further debugging. Cannot find a way to recover. After deleting data(losing all data), restart and everything became OK
{code:java}
➜  bin ./nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load  Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.2  ?     16      51.2%             6d194555-f6eb-41d0-c000-000000000002  rack1
DN  127.0.0.4  ?     16      48.8%             6d194555-f6eb-41d0-c000-000000000001  rack1{code}
h3. When

 
It was introduced by the Patch: CEP-21. Anyway, the NPE check is needed to protect its propagation anywhere
{code:java}
Implementation of Transactional Cluster Metadata as described in CEP-21
Hash: ae084237
 
code diff:
 
    public String getLocalHostId()
     {
-        UUID id = getLocalHostUUID();
-        return id != null ? id.toString() : null;
+        return getLocalHostUUID().toString();
     }
 
     public UUID getLocalHostUUID()
     {
-        UUID id = getTokenMetadata().getHostId(FBUtilities.getBroadcastAddressAndPort());
-        if (id != null)
-            return id;
-        // this condition is to prevent accessing the tables when the node is not started yet, and in particular,
-        // when it is not going to be started at all (e.g. when running some unit tests or client tools).
-        else if ((DatabaseDescriptor.isDaemonInitialized() || DatabaseDescriptor.isToolInitialized()) && CommitLog.instance.isStarted())
-            return SystemKeyspace.getLocalHostId();
-
-        return null;
+        // Metadata collector requires using local host id, and flush of IndexInfo may race with
+        // creation and initialization of cluster metadata service. Metadata collector does accept
+        // null localhost ID values, it's just that TokenMetadata was created earlier.
+        ClusterMetadata metadata = ClusterMetadata.currentNullable();
+        if (metadata == null || metadata.directory.peerId(getBroadcastAddressAndPort()) == null)
+            return null;
+        return metadata.directory.peerId(getBroadcastAddressAndPort()).toUUID();
     } {code}"
CASSANDRA-19336,Repair causes out of memory,"CASSANDRA-14096 introduced {{repair_session_space}} as a limit for the memory usage for Merkle tree calculations during repairs. This limit is applied to the set of Merkle trees built for a received validation request ({{{}VALIDATION_REQ{}}}), divided by the replication factor so as not to overwhelm the repair coordinator, who will have requested RF sets of Merkle trees. That way the repair coordinator should only use {{repair_session_space}} for the RF Merkle trees.

However, a repair session without {{{}-pr-{}}}/{{{}-partitioner-range{}}} will send RF*RF validation requests, because the repair coordinator node has RF-1 replicas and is also the replica of RF-1 nodes. Since all the requests are sent at the same time, at some point the repair coordinator can have up to RF*{{{}repair_session_space{}}} worth of Merkle trees if none of the validation responses is fully processed before the last response arrives.

Even worse, if the cluster uses virtual nodes, many nodes can be replicas of the repair coordinator, and some nodes can be replicas of multiple token ranges. It would mean that the repair coordinator can send more than RF or RF*RF simultaneous validation requests.

For example, in an 11-node cluster with RF=3 and 256 tokens, we have seen a repair session involving 44 groups of ranges to be repaired. This produces 44*3=132 validation requests contacting all the nodes in the cluster. When the responses for all these requests start to arrive to the coordinator, each containing up to {{repair_session_space}}/3 of Merkle trees, they accumulate quicker than they are consumed, greatly exceeding {{repair_session_space}} and OOMing the node."
CASSANDRA-19269,NPE in MetaStrategy.java when collecting metrics,"Now that the metrics exporter has been removed in CASSANDRA-18743, I thought I'd take a stab at setting up a metrics collector through DropWizard.  I have a POC of an agent that exposes a Prometheus endpoint using the Prometheus simpleclient here: https://github.com/rustyrazorblade/cassandra-prometheus-exporter

There's build instructions in the README - it's just a simple agent on a background thread.

Unfortunately when trying to view the metrics, Cassandra throws an NPE exception, stack trace is here:

{code:java}
WARN  [qtp104447770-28] 2024-01-12 14:26:50,102 ServletHandler.java:522 - /metrics
java.lang.NullPointerException: null
	at org.apache.cassandra.locator.MetaStrategy.getReplicationFactor(MetaStrategy.java:64)
	at org.apache.cassandra.metrics.KeyspaceMetrics.lambda$new$12(KeyspaceMetrics.java:224)
	at org.apache.cassandra.metrics.KeyspaceMetrics$1.getValue(KeyspaceMetrics.java:344)
	at org.apache.cassandra.metrics.KeyspaceMetrics$1.getValue(KeyspaceMetrics.java:338)
	at org.apache.cassandra.metrics.StorageMetrics.lambda$static$0(StorageMetrics.java:40)
	at org.apache.cassandra.metrics.StorageMetrics.lambda$createSummingGauge$2(StorageMetrics.java:55)
	at java.base/java.util.stream.ReferencePipeline$5$1.accept(ReferencePipeline.java:229)
	at com.google.common.collect.CollectSpliterators$1.lambda$forEachRemaining$1(CollectSpliterators.java:128)
	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at com.google.common.collect.CollectSpliterators$1.forEachRemaining(CollectSpliterators.java:128)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.LongPipeline.reduce(LongPipeline.java:474)
	at java.base/java.util.stream.LongPipeline.sum(LongPipeline.java:432)
	at org.apache.cassandra.metrics.StorageMetrics.lambda$createSummingGauge$3(StorageMetrics.java:56)
	at io.prometheus.client.dropwizard.DropwizardExports.fromGauge(DropwizardExports.java:47)
	at io.prometheus.client.dropwizard.DropwizardExports.collect(DropwizardExports.java:133)
	at io.prometheus.client.Collector.collect(Collector.java:45)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:204)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.<init>(CollectorRegistry.java:162)
	at io.prometheus.client.CollectorRegistry.filteredMetricFamilySamples(CollectorRegistry.java:140)
	at io.prometheus.client.exporter.MetricsServlet.doGet(MetricsServlet.java:43)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:648)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:455)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1072)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:382)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1006)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:365)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:485)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:926)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:988)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}

Related [discussion thread|https://lists.apache.org/thread/lppjlxrd91qn0r0dkz80r5y843jwl1qo]."
CASSANDRA-19255,StorageService.getRangeToEndpointMap() MBean operation is running into NPE for LocalStrategy keysapces,"When the StorageService's MBean operation getRangeToEndpointMap is called for LocalStrategy keyspaces, then it is running into NPE. It is working in earlier major version, but failing in trunk. It can be reproduced in local using JConsole or using a tool like `jmxterm` (unfortunately these tools are not giving full stacktrace). Observed the same behavior with getRangeToEndpointWithPortMap operation too."
CASSANDRA-19253,(Accord) NPE while trying to serialize FoundKnownMap as value is null half the time but unexpected while serializing,"{code}
java.lang.NullPointerException
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serializedSize(CommandSerializers.java:277)
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serializedSize(CommandSerializers.java:253)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serializedSize(CheckStatusSerializers.java:75)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serializedSize(CheckStatusSerializers.java:54)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$2.serializedSize(CheckStatusSerializers.java:115)
{code}

{code}
java.lang.NullPointerException
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serialize(CommandSerializers.java:257)
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serialize(CommandSerializers.java:253)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serialize(CheckStatusSerializers.java:58)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serialize(CheckStatusSerializers.java:54)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$2.serialize(CheckStatusSerializers.java:91)
{code}
"
CASSANDRA-19182,IR may leak SSTables with pending repair when coming from streaming,"There is a race condition where SSTables from streaming may race with pending repair cleanup in compaction causing us to cleanup the pending repair state in compaction while the SSTables are being added to it; this leads to IR failing in the future when those files get selected for repair.

This problem was hard to track down as the in-memory state was wiped, so we don’t have any details.  To better aid these types of investigation we should make sure the repair vtables get updated when IR session failures are submitted"
CASSANDRA-19177,SAI query timeouts can cause resource leaks,There are several places in the SAI query path where a query timeout can result in a resource not being closed correctly. We need to make sure that wherever QueryContext.checkpoint is called we catch the resulting exception and close any open resources.
CASSANDRA-19169,Don't NPE when initializing CFSs for local system keyspaces with UCS,"When UnifiedCompactionStrategy is used as the default, NPEs are thrown when
flushing the system keyspace tables early during startup. The system keyspace is
initialised before the cluster metadata, but UCS currently tries to access the
current epoch when initialising the shard manager, to determine whether the
local ranges are out of date. This isn't necessary for the system keyspaces as
they use LocalStrategy and cover the whole token space."
CASSANDRA-19166,StackOverflowError on ALTER after many previous schema changes,"Since 4.1, TableMetadataRefCache re-wraps its fields in Collections.unmodifiableMap on every local schema update. This causes TableMetadataRefCache's Map fields to reference chains of nested UnmodifiableMaps. Eventually, this leads to a StackOverflowError on get(), which has to traverse lots of these maps to fetch the actual value.

https://github.com/apache/cassandra/blob/4059faf5b948c5a285c25fb0f2e4c4288ee7c305/src/java/org/apache/cassandra/schema/TableMetadataRefCache.java#L53

The issue goes away on restart, since TableMetadataRefCache is reloaded from disk.

See CASSANDRA-17044, when TableMetadataRefCache was introduced. This issue was discovered on a real test cluster where schema changes were failing, via a heap dump."
CASSANDRA-19138,Flush sealed period/snapshot tables on every write,"These tables are low throughput, so flushing on every write would not have a significant perf impact, but would improve availability of snapshots for recovery and replay at startup"
CASSANDRA-19125,Investigate increased memory usage in tests with TCM,A few tests started failing with OOM after we committed TCM - we should investigate why/if we use more memory
CASSANDRA-19105,Fix CQLSH Capture to save query results only and not trace details when Tracing is on,"When using *Tracing* in CQLSH, it's sometimes helpful to use *Capture* to avoid paging through output when you want to see the trace results.  However, the trace results are also incorrectly captured to the output file.

According to *Help Capture* (below), only the query result should be saved. The correct behavior should display the trace results and only capture the query results.

        {_}Help capture message{_}:  Only query result output is captured. Errors and output from cqlsh-only commands will still be shown in the cqlsh session.

Example:
{quote}> TRACING ON;

> select * from system_schema.columns LIMIT 1;

> CAPTURE 'foo'
{quote}
this should display only the tracing information
{quote}> CAPTURE OFF
{quote}
displays results and then the tracing
{quote}> TRACING OFF
{quote}
displays only the results"
CASSANDRA-19082,Histogram overflow causes client timeouts and message drops,"Hi,

We have recently noticed that sometimes this exception happens on our Cassandra cluster: 
{code:java}
ERROR [ScheduledTasks:1] 2023-11-24 06:24:12,680 CassandraDaemon.java:244 - Exception in thread Thread[ScheduledTasks:1,5,main]
java.lang.IllegalStateException: Unable to compute when histogram overflowed
        at org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir$EstimatedHistogramReservoirSnapshot.getMean(DecayingEstimatedHistogramReservoir.java:472)
        at org.apache.cassandra.net.MessagingService.getDroppedMessagesLogs(MessagingService.java:1272)
        at org.apache.cassandra.net.MessagingService.logDroppedMessages(MessagingService.java:1244)
        at org.apache.cassandra.net.MessagingService.access$200(MessagingService.java:84)
        at org.apache.cassandra.net.MessagingService$4.run(MessagingService.java:512)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84)
        at java.lang.Thread.run(Thread.java:750)
 {code}
It happens on all 6 nodes at the same time. Also we see increased client timeouts and dropped READ and READ_RESPONSE messages. Our Cassandra is 3.11.16, 2 DC setup, 6 node in each DC. RF is 3. I have searched issues but could not find exactly same issue causing messages to be dropped. Any suggestion would be appreciated. "
CASSANDRA-19065,Test Failure: org.apache.cassandra.distributed.upgrade.MixedModeTTLOverflowUpgradeTest.testTTLOverflowDuringUpgrade-_jdk11,"Failed in Circle:

[https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20534/tests]

{noformat}
junit.framework.AssertionFailedError: Error in test '4.1.4 -> [5.1]' while upgrading to '5.1'; successful upgrades [4.0.12 -> [5.1]]
	at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:397)
	at org.apache.cassandra.distributed.upgrade.MixedModeTTLOverflowUpgradeTest.testTTLOverflow(MixedModeTTLOverflowUpgradeTest.java:134)
	at org.apache.cassandra.distributed.upgrade.MixedModeTTLOverflowUpgradeTest.testTTLOverflowDuringUpgrade(MixedModeTTLOverflowUpgradeTest.java:49)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.utils.Throwables.unchecked(Throwables.java:308)
	at org.apache.cassandra.utils.Throwables.cleaned(Throwables.java:327)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:550)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.handleMutation(CommitLogReplayer.java:521)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readMutation(CommitLogReader.java:478)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readSection(CommitLogReader.java:397)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:244)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:147)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayFiles(CommitLogReplayer.java:195)
	at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:223)
	at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:204)
	at org.apache.cassandra.tcm.Startup.initializeFromGossip(Startup.java:259)
	at org.apache.cassandra.tcm.Startup.initialize(Startup.java:115)
	at org.apache.cassandra.distributed.impl.Instance.partialStartup(Instance.java:711)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$7(Instance.java:615)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)
	at org.apache.cassandra.utils.memory.SlabAllocator.getRegion(SlabAllocator.java:139)
	at org.apache.cassandra.utils.memory.SlabAllocator.allocate(SlabAllocator.java:104)
	at org.apache.cassandra.utils.memory.MemtableBufferAllocator$1.allocate(MemtableBufferAllocator.java:40)
	at org.apache.cassandra.utils.memory.ByteBufferCloner.clone(ByteBufferCloner.java:77)
	at org.apache.cassandra.utils.memory.ByteBufferCloner.clone(ByteBufferCloner.java:63)
	at org.apache.cassandra.utils.memory.ByteBufferCloner.clone(ByteBufferCloner.java:46)
	at org.apache.cassandra.db.memtable.SkipListMemtable.put(SkipListMemtable.java:115)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1451)
	at org.apache.cassandra.db.CassandraTableWriteHandler.write(CassandraTableWriteHandler.java:38)
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:626)
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:483)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer$MutationInitiator$1.runMayThrow(CommitLogReplayer.java:316)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
	at org.apache.cassandra.concurrent.FutureTask$3.call(FutureTask.java:130)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
{noformat}"
CASSANDRA-18927,Fix potential race condition in IndexViewManager during invalidation,"There is a potential race condition in the {{IndexViewManager.invalidate}} method:
{code:java}
public void invalidate()
{
    View currentView = view.get();

    for (SSTableIndex index : currentView)
    {
        index.markObsolete();
    }

    view.set(new View(context, Collections.emptyList()));
} {code}
We should {{getAndSet}} the view before marking the indexes as obsolete. This would avoid indexes potentially being made obsolete when being accessed. "
CASSANDRA-18926,SAI in-memory index does not include maximum term size check when adding terms,"The {{SSTableIndexWriter}} rejects terms that exceed a maximum term size with a no-spam warning, but the {{TrieMemoryIndex}} does not do this. 

We should check term sizes when rows are added and issue client warnings when this happens. This still needs to happen in the {{SSTableIndexWriter}} to handle terms during an initial index build. "
CASSANDRA-18921,Describe statement may include inconsistent schema and schema version for paging,"When the {{DescribeStatement}} is executed, it initially gets the current schema snapshot and the schema version with two separate unsynchronized calls. When the schema is being modified around that time, the schema snapshot and schema version may diverge which will result in failing or inconsistent paging. This is not super important but it is easy to fix.
"
CASSANDRA-18913,Gossip NPE due to shutdown event corrupting empty statuses,"When an instance either disables gossip or shuts down we send a gossip shutdown message, peers ignore it if the endpoint isn’t known, else it mutates its local copy of the state to mark shutdown…
When an instance restarts it populates gossip with the endpoints found in peers, but the state is empty (not null)

So, there is a fun timing bug…

* stop node1
* start node1; at this point all known endpoints before exist in gossip but are empty
* node2 shutdown (gossip shutdown or node, doesn’t matter)
* node1 sees the shutdown before gossip messages, and gets corruptted
* node3 tries to join the cluster, fails due to node1 being corrupted

There are 2 different patterns the NPE can happen with, in this example node1 and node3 will have different stack traces

{code}
org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
	Suppressed: java.lang.NullPointerException: Unable to get HOST_ID; HOST_ID is not defined, given EndpointState: HeartBeatState = HeartBeat: generation = 0, version = 2147483647, AppStateMap = {STATUS=Value(shutdown,true,37), RPC_READY=Value(false,38), STATUS_WITH_PORT=Value(shutdown,true,36)}
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1218)
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1208)
		at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:3279)
		at org.apache.cassandra.service.StorageService.onChange(StorageService.java:2756)
		at org.apache.cassandra.gms.Gossiper.markAsShutdown(Gossiper.java:611)
		at org.apache.cassandra.gms.GossipShutdownVerbHandler.doVerb(GossipShutdownVerbHandler.java:39)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)
	Suppressed: java.lang.NullPointerException: Unable to get HOST_ID; HOST_ID is not defined, given EndpointState: HeartBeatState = HeartBeat: generation = 0, version = 2147483647, AppStateMap = {STATUS=Value(shutdown,true,37), RPC_READY=Value(false,38), STATUS_WITH_PORT=Value(shutdown,true,36)}
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1218)
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1208)
		at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:3279)
		at org.apache.cassandra.service.StorageService.onChange(StorageService.java:2756)
		at org.apache.cassandra.gms.Gossiper.doOnChangeNotifications(Gossiper.java:1762)
		at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:3793)
		at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1465)
		at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1678)
		at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)

{code}"
CASSANDRA-18910,Debian packaging broken by quilt?,"Something has changed in the docker image that is breaking the debian packaging in all versions, similar to this:

{quote}
dpkg-buildpackage: info: source package cassandra
dpkg-buildpackage: info: source version 4.1.4-20231004git486acc68f1
dpkg-buildpackage: info: source distribution unstable
dpkg-buildpackage: info: source changed by build <build@1518e06a5507>
dpkg-buildpackage: info: host architecture amd64
 dpkg-source --tar-ignore=.git --before-build .
 fakeroot debian/rules clean
QUILT_PATCHES=debian/patches \
        quilt --quiltrc /dev/null pop -a -R || test $? = 1
No patch removed
make: *** [/usr/share/quilt/quilt.make:23: unpatch] Error 1
dpkg-buildpackage: error: fakeroot debian/rules clean subprocess returned exit status 2
{quote}"
CASSANDRA-18882,Update the slf4j-api library in order to avoid potential leaks,"{color:red}>{color}The proposed update is 1.7.25 (current) > 1.7.36 (latest)

The following commits are included in the newer slf4j-api and that currently look very useful for us:
 - [SLF4J-469|https://www.mail-archive.com/slf4j-dev@qos.ch/msg02569.html] Potential memory leaks if there is no underlying implementation
 - [SLF4j-466|https://www.mail-archive.com/slf4j-dev@qos.ch/msg02499.html] Add test for all happy flow cases
 - [SLF4J-460|https://www.mail-archive.com/search?l=slf4j-dev@qos.ch&q=subject:%22%5C%5Bslf4j%5C-dev%5C%5D+%5C%5BJIRA%5C%5D+%5C%28SLF4J%5C-460%5C%29+EventRecodingLogger+debug+logs+as%09TRACE%22&o=newest&f=1)] EventRecodingLogger debug logs as TRACE"
CASSANDRA-18841,InstanceClassLoader leak in 5.0/trunk,"Something in the 5.0/trunk branches has caused an in-jvm dtest InstanceClassLoader leak - it appears to have something to do with the Mutual TLS Authenticator (f078c02cb58bddd735490b07548f7352f0eb09aa) but nothing in that commit, so far, has stood out as causing issues.

The culprit class appears to be {{io.netty.util.internal.InternalThreadLocalMap}}, which seems to no be removed when the threads stops for some reason."
CASSANDRA-18840,Leakage of references to SSTable on unsuccessful operations,"This is a little bit tricky to describe correctly as I can talk about the symptoms only. I hit this issue when testing CASSANDRA-18781.

In a nutshell, when we go to bulkload an SSTable, it opens it in SSTableLoader. If bulkloading fails on server side and exception is propagated to the client, on releasing of references, it fails on this assert (1). This practically means that we are leaking resources as something still references that SSTable but it was not tidied up (on failure). On a happy path, it is all de-referenced correctly.

I think that this might have implications beyond SSTable loading, e.g. this could happen upon streaming too.

(1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java#L245"
CASSANDRA-18762,Repair triggers OOM with direct buffer memory,"We are seeing repeated failures of nodes with 16GB of heap on a VM with 32GB of physical RAM due to direct memory.  This seems to be related to CASSANDRA-15202 which moved Merkel trees off-heap in 4.0.   Using Cassandra 4.0.6 with Java 11.
{noformat}
2023-08-09 04:30:57,470 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e55a3b0-366d-11ee-a644-d91df26add5e] Received merkle tree for table_a from /169.102.200.241:7000
2023-08-09 04:30:57,567 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e0d2900-366d-11ee-a644-d91df26add5e] Received merkle tree for table_b from /169.93.192.29:7000
2023-08-09 04:30:57,568 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e1dcad0-366d-11ee-a644-d91df26add5e] Received merkle tree for table_c from /169.104.171.134:7000
2023-08-09 04:30:57,591 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e69a0e0-366d-11ee-a644-d91df26add5e] Received merkle tree for table_b from /169.79.232.67:7000
2023-08-09 04:30:57,876 [INFO ] [Service Thread] cluster_id=101 ip_address=169.0.0.1 GCInspector.java:294 - G1 Old Generation GC in 282ms. Compressed Class Space: 8444560 -> 8372152; G1 Eden Space: 7809794048 -> 0; G1 Old Gen: 1453478400 -> 820942800; G1 Survivor Space: 419430400 -> 0; Metaspace: 80411136 -> 80176528
2023-08-09 04:30:58,387 [ERROR] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 JVMStabilityInspector.java:102 - OutOfMemory error letting the JVM handle the error:
java.lang.OutOfMemoryError: Direct buffer memory
at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:318)
at org.apache.cassandra.utils.MerkleTree.allocate(MerkleTree.java:742)
at org.apache.cassandra.utils.MerkleTree.deserializeOffHeap(MerkleTree.java:780)
at org.apache.cassandra.utils.MerkleTree.deserializeTree(MerkleTree.java:751)
at org.apache.cassandra.utils.MerkleTree.deserialize(MerkleTree.java:720)
at org.apache.cassandra.utils.MerkleTree.deserialize(MerkleTree.java:698)
at org.apache.cassandra.utils.MerkleTrees$MerkleTreesSerializer.deserialize(MerkleTrees.java:416)
at org.apache.cassandra.repair.messages.ValidationResponse$1.deserialize(ValidationResponse.java:100)
at org.apache.cassandra.repair.messages.ValidationResponse$1.deserialize(ValidationResponse.java:84)
at org.apache.cassandra.net.Message$Serializer.deserializePost40(Message.java:782)
at org.apache.cassandra.net.Message$Serializer.deserialize(Message.java:642)
at org.apache.cassandra.net.InboundMessageHandler$LargeMessage.deserialize(InboundMessageHandler.java:364)
at org.apache.cassandra.net.InboundMessageHandler$LargeMessage.access$1100(InboundMessageHandler.java:317)
at org.apache.cassandra.net.InboundMessageHandler$ProcessLargeMessage.provideMessage(InboundMessageHandler.java:504)
at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:429)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:834)no* further _formatting_ is done here{noformat}
 
-XX:+AlwaysPreTouch
-XX:+CrashOnOutOfMemoryError
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:+ParallelRefProcEnabled
-XX:+PerfDisableSharedMem
-XX:+ResizeTLAB
-XX:+UseG1GC
-XX:+UseNUMA
-XX:+UseTLAB
-XX:+UseThreadPriorities
-XX:-UseBiasedLocking
-XX:CompileCommandFile=/opt/nosql/clusters/cassandra-101/conf/hotspot_compiler
-XX:G1RSetUpdatingPauseTimePercent=5
-XX:G1ReservePercent=20
-XX:HeapDumpPath=/opt/nosql/data/cluster_101/cassandra-1691623098-pid2804737.hprof
-XX:InitiatingHeapOccupancyPercent=70
-XX:MaxGCPauseMillis=200
-XX:StringTableSize=60013
-Xlog:gc*:file=/opt/nosql/clusters/cassandra-101/logs/gc.log:time,uptime:filecount=10,filesize=10485760
-Xms16G
-Xmx16G
-Xss256k
 
From our Prometheus metrics, the behavior shows the direct buffer memory ramping up until it reaches the max and then causes an OOM.  It would appear that direct memory is never being released by the JVM until its exhausted.
 
!Cluster-dm-metrics.PNG!

An Eclipse Memory Analyzer

Class Histogram:
||Class Name||Objects||Shallow Heap||Retained Heap||
|java.lang.Object[]|445,014|42,478,160|>= 4,603,280,344| |
|io.netty.util.concurrent.FastThreadLocalThread|167|21,376|>= 4,467,294,736|

Leaks: Problem Suspect 1
The thread *io.netty.util.concurrent.FastThreadLocalThread @ 0x501dd5930 AntiEntropyStage:1* keeps local variables with total size *4,295,042,472 (84.00%)* bytes."
CASSANDRA-18736,Streaming exception race creates corrupt transaction log files that prevent restart,"On restart, Cassandra logs this message and terminates.
{code:java}
ERROR 2023-07-17T17:17:22,931 [main] org.apache.cassandra.db.lifecycle.LogTransaction:561 - Unexpected disk state: failed to read transaction log [nb_txn_stream_39d5f6b0-fb81-11ed-8f46-e97b3f61511e.log in /datadir1/keyspace/table-c9527530a0d611e8813f034699fc9043]
Files and contents follow:
/datadir1/keyspace/table-c9527530a0d611e8813f034699fc9043/nb_txn_stream_39d5f6b0-fb81-11ed-8f46-e97b3f61511e.log
        ABORT:[,0,0][737437348]
                ***This record should have been the last one in all replicas
        ADD:[/datadir1/keyspace/table-c9527530a0d611e8813f034699fc9043/nb-284490-big-,0,8][2493503833]
{code}
The root cause is a race during streaming exception handling.

Although concurrent modification of to the {{LogTransaction}} was added for CASSANDRA-16225, there is nothing to prevent usage after the transaction is completed (committed/aborted) once it has been processed by {{TransactionTidier}} (after the last reference is released). Before the transaction is tidied, the {{LogFile}} keeps a list of records that are checked for completion before adding new entries. In {{TransactionTidier}} {{LogFile.records}} are cleared as no longer needed, however the LogTransaction/LogFile is still accessible to the stream.

The changes in CASSANDRA-17273 added a parallel set of {{onDiskRecords}} that could be used to reliably recreate the transaction log at any new datadirs the same as the existing
datadirs - regardless of the effect of {{LogTransaction.untrackNew/LogFile.remove}}

If a streaming exception causes the LogTransaction to be aborted and tidied just before {{SimpleSSTableMultiWriter}} calls trackNew to add a new sstable. At the time of the call, the {{LogFile}} will not contain any {{LogReplicas}},
{{LogFile.records}} will be empty, and {{LogFile.onDiskRecords}} will contain an {{ABORT}}.

When {{LogTransaction.trackNew/LogFile.add}} is called, the check for completed transaction fails as records is empty, there are no replicas on the datadir, so {{maybeCreateReplicas}} creates a new txnlog file replica containing ABORT, then
appends an ADD record.

The LogFile has already been tidied after the abort so the txnlog file is not removed and sits on disk until a restart, causing the faiulre.

There is a related exception caused with a different interleaving of aborts, after an sstable is added, however this is just a nuisance in the logs as the LogRelica is already created with an {{ADD}} record first.
{code:java}
java.lang.AssertionError: [ADD:[/datadir1/keyspace/table/nb-23314378-big-,0,8][1869379820]] is not tracked by 55be35b0-35d1-11ee-865d-8b1e3c48ca06
        at org.apache.cassandra.db.lifecycle.LogFile.remove(LogFile.java:388)
        at org.apache.cassandra.db.lifecycle.LogTransaction.untrackNew(LogTransaction.java:158)
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.untrackNew(LifecycleTransaction.java:577)
        at org.apache.cassandra.db.streaming.CassandraStreamReceiver$1.untrackNew(CassandraStreamReceiver.java:149)
        at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.abort(SimpleSSTableMultiWriter.java:95)
        at org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter.abort(RangeAwareSSTableWriter.java:191)
        at org.apache.cassandra.db.streaming.CassandraCompressedStreamReader.read(CassandraCompressedStreamReader.java:115)
        at org.apache.cassandra.db.streaming.CassandraIncomingFile.read(CassandraIncomingFile.java:85)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:53)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:38)
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:53)
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:172)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code}"
CASSANDRA-18733,Waiting indefinitely on ReceivedMessage response in StreamSession#receive() can cause deadlock,"I've observed in a recent stack trace from a node running 4.1 what looks like a deadlock around the {{StreamSession}} monitor lock when {{StreamSession#receive()}} waits via {{syncUninteruptibly()}} for a response to a control message.

{noformat}
""Messaging-EventLoop-3-10"" #320 daemon prio=5 os_prio=0 cpu=57979617.98ms elapsed=5587916.03s tid=0x00007f056e88ae00 nid=0x80ec waiting for monitor entry  [0x00007f056d277000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:524)
        - waiting to lock <0x00000006816fae70> (a org.apache.cassandra.streaming.StreamSession)
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:690)
        at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel.onMessageComplete(StreamingMultiplexedChannel.java:264)
        at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel.lambda$sendMessage$1(StreamingMultiplexedChannel.java:233)
        at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$$Lambda$2029/0x00000008007a0c40.operationComplete(Unknown Source)
        at org.apache.cassandra.utils.concurrent.ListenerList.notifyListener(ListenerList.java:134)
        at org.apache.cassandra.utils.concurrent.ListenerList.notifyListener(ListenerList.java:148)
        at org.apache.cassandra.utils.concurrent.ListenerList$GenericFutureListenerList.notifySelf(ListenerList.java:190)
        at org.apache.cassandra.utils.concurrent.ListenerList.lambda$notifyExclusive$0(ListenerList.java:124)
        at org.apache.cassandra.utils.concurrent.ListenerList$$Lambda$950/0x0000000800666040.accept(Unknown Source)
        at org.apache.cassandra.utils.concurrent.IntrusiveStack.forEach(IntrusiveStack.java:195)
        at org.apache.cassandra.utils.concurrent.ListenerList.notifyExclusive(ListenerList.java:124)
        at org.apache.cassandra.utils.concurrent.ListenerList.notify(ListenerList.java:96)
        at org.apache.cassandra.utils.concurrent.AsyncFuture.trySet(AsyncFuture.java:104)
        at org.apache.cassandra.utils.concurrent.AbstractFuture.tryFailure(AbstractFuture.java:148)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.tryFailure(AsyncPromise.java:139)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:1009)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:870)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
        at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(java.base@11.0.16/Thread.java:829)
{noformat}

It seems that while {{receive()} is holding the monitor lock on {{StreamSession}}, the callback that executes on a different thread for the control message it sends carries an error. This error, when handled in {{onError()}}, then calls {{closeSession()}}, which tries to acquire the monitor lock already held in {{receive()}}.

{noformat}
""Stream-Deserializer-/100.70.229.6:7000-de724029"" #1919000 daemon prio=5 os_prio=0 cpu=224.66ms elapsed=1604976.92s tid=0x00007f0561c66500 nid=0xe2a2 waiting on condition  [0x00007f0830947000]
   java.lang.Thread.State: WAITING (parking)
        at jdk.internal.misc.Unsafe.park(java.base@11.0.16/Native Method)
        at java.util.concurrent.locks.LockSupport.park(java.base@11.0.16/LockSupport.java:323)
        at org.apache.cassandra.utils.concurrent.WaitQueue$Standard$AbstractSignal.await(WaitQueue.java:289)
        at org.apache.cassandra.utils.concurrent.WaitQueue$Standard$AbstractSignal.await(WaitQueue.java:282)
        at org.apache.cassandra.utils.concurrent.Awaitable$AsyncAwaitable.await(Awaitable.java:306)
        at org.apache.cassandra.utils.concurrent.AsyncFuture.await(AsyncFuture.java:154)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.await(AsyncPromise.java:244)
        at org.apache.cassandra.net.AsyncChannelPromise.await(AsyncChannelPromise.java:127)
        at org.apache.cassandra.net.AsyncChannelPromise.await(AsyncChannelPromise.java:34)
        at org.apache.cassandra.utils.concurrent.Awaitable$Defaults.awaitUninterruptibly(Awaitable.java:186)
        at org.apache.cassandra.utils.concurrent.AbstractFuture.awaitUninterruptibly(AbstractFuture.java:482)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.awaitUninterruptibly(AsyncPromise.java:254)
        at org.apache.cassandra.net.AsyncChannelPromise.awaitUninterruptibly(AsyncChannelPromise.java:133)
        at org.apache.cassandra.net.AsyncChannelPromise.awaitUninterruptibly(AsyncChannelPromise.java:34)
        at org.apache.cassandra.utils.concurrent.Future.syncUninterruptibly(Future.java:94)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.syncUninterruptibly(AsyncPromise.java:186)
        at org.apache.cassandra.net.AsyncChannelPromise.syncUninterruptibly(AsyncChannelPromise.java:121)
        at org.apache.cassandra.net.AsyncChannelPromise.syncUninterruptibly(AsyncChannelPromise.java:34)
        at org.apache.cassandra.streaming.StreamSession.receive(StreamSession.java:1054)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:628)
        - locked <0x00000006816fae70> (a org.apache.cassandra.streaming.StreamSession)
        at org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:76)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(java.base@11.0.16/Thread.java:829)
{noformat}

The most straightforward way to fix this might be to put an upper bound on the wait in {{receive()}}, along w/ some logging at WARN if it times out. This would at least allow us to make progress eventually and close the session properly. The syncUninterruptibly() was intended to avoid a race during shutdown, IIRC, and unless the timeout is comically low, it shouldn't compromise that.

The problem can usually be fixed by bouncing affected nodes, who will usually present w/ an increasing backlog of unrepaired data and a log message that looks something like this, in addition to a number of streaming errors:

{noformat}
INFO  2023-08-04T10:45:54,845 [NettyStreaming-Outbound-/<ip:port>] org.apache.cassandra.streaming.async.StreamingMultiplexedChannel:359 - [Stream #10465c10-3108-11ee-af8a-3f74fd21ad9d] waiting to acquire a permit to begin streaming <data directory>-nb-101920-big-Data.db. This message logs every 3 minutes
{noformat}"
CASSANDRA-18716,In-Memory write support for a vector search index,This ticket will introduce the write side in-memory components of the vector index.
CASSANDRA-18710,Test failure: org.apache.cassandra.io.DiskSpaceMetricsTest.testFlushSize-.jdk17 (from org.apache.cassandra.io.DiskSpaceMetricsTest-.jdk17),"Seen here:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1644/testReport/org.apache.cassandra.io/DiskSpaceMetricsTest/testFlushSize__jdk17/]
h3.  
{code:java}
Error Message
expected:<7200.0> but was:<1367.83970468544>

Stacktrace
junit.framework.AssertionFailedError: expected:<7200.0> but was:<1367.83970468544> at org.apache.cassandra.io.DiskSpaceMetricsTest.testFlushSize(DiskSpaceMetricsTest.java:119) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-18558,remove dh_python use from debian packaging,"It looks like dh_python2 has been removed from debian, but it also looks like we don't need it:

{noformat}
E: dh_python2 dh_python2:408: no package to act on (python-foo or one with ${python:Depends} in Depends)
{noformat}"
CASSANDRA-18556,Leak detected issue in cassandra 3.11.1,"We are running 36 nodes Cassandra cluster. All nodes have the some config, Cassandra version is 3.11.1. Suddenly, we are seeing fluctuation in storage available graphs for the 2 nodes. Attaching the last 7 days used storage graph for the Cassandra nodes. 

Further investigation, we saw errors in the logs of these 2 Cassandra node logs. 

{code:java}
ERROR [CompactionExecutor:164360] 2023-05-04 00:30:58,094 CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:164360,1,main]
java.lang.IllegalArgumentException: null
    at java.nio.Buffer.position(Buffer.java:244) ~[na:1.8.0_131]
    at org.apache.cassandra.io.util.SafeMemoryWriter.reallocate(SafeMemoryWriter.java:59) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.util.SafeMemoryWriter.setCapacity(SafeMemoryWriter.java:68) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.IndexSummaryBuilder.prepareToCommit(IndexSummaryBuilder.java:250) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.doPrepare(BigTableWriter.java:524) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter$TransactionalProxy.doPrepare(BigTableWriter.java:364) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.format.SSTableWriter.prepareToCommit(SSTableWriter.java:281) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:379) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:111) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:121) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.1.jar:3.11.1]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
WARN  [CompactionExecutor:164909] 2023-05-04 00:31:54,573 IndexSummaryBuilder.java:115 - min_index_interval of 128 is too low for 4298508892 expected keys of avg size 64; using interval of 145 instead
ERROR [Reference-Reaper:1] 2023-05-04 00:32:02,326 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4f374614) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1106289026:Memory@[7fa6ab9ec010..7fa72ad798d0) was not released before the reference was garbage collected {code}
Compactions also running continually run on these nodes.

I am unable to understand the root cause of it. Any help is appreciated."
CASSANDRA-18552,Debian packaging source should exclude git subdirectory,This balloons the source up to 400+MB instead of the ~13MB necessary.
CASSANDRA-18522,LEAK DETECTED: org.apache.cassandra.io.util.FileHandle$Cleanup in TestSecondaryIndexes.test_failing_manual_rebuild_index,"A leak was detected in CI run: https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2344/workflows/1f57b9a0-3fc9-49c7-a821-52e24b025056/jobs/22547/parallel-runs/4/steps/4-106{{{}

==================================== ERRORS ===================================={}}}
{{_ ERROR at teardown of TestSecondaryIndexes.test_failing_manual_rebuild_index __}}
{{Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [Reference-Reaper] 2023-05-05 17:57:29,429 Ref.java:237 - LEAK DETECTED: a reference (class org.apache.cassandra.io.util.FileHandle$Cleanup@1392096802:/tmp/dtest-nl3k0_2v/test/node1/data0/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-1-big-Index.db) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1392096802:/tmp/dtest-nl3k0_2v/test/node1/data0/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-1-big-Index.db was not released before the reference was garbage collected', [node1] 'ERROR [Reference-Reaper] 2023-05-05 17:57:29,430 Ref.java:237 - LEAK DETECTED: a reference (class org.apache.cassandra.io.util.FileHandle$Cleanup@1379143890:/tmp/dtest-nl3k0_2v/test/node1/data1/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-2-big-Index.db) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1379143890:/tmp/dtest-nl3k0_2v/test/node1/data1/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-2-big-Index.db was not released before the reference was garbage collected']}}
{{==============}}"
CASSANDRA-18505,NPE when deserializing malformed collections from client,"When deserializing collections sent from the client, if an element in the collection is incorrectly serialized, Collections.getValue can return null if the length of the element is negative.  Currently this isn't detected and serialization continues, calling validate and throwing an NPE in serializers that don't handle null value buffers.

Detect the malformed input and throw a better MarshalException so it will be converted to an InvalidRequestException for the client.
"
CASSANDRA-18485,CEP-15: (C*) Enhance in-memory FileSystem to work with mmap and support tests to add custom logic,"The Simulator currently uses JimFS for its FileSystem, but this lacks any way to add custom logic when file operations are performed; it also lacks mmap support which requires all disk access logic to provide non-mmap solutions as well.

As part of the Simulator work, testing disk corruption will require resolving both these issues, so will need a new FileSystem to rule them all…

This ticket is to define the new FileSystem and add the integration to Simulator, JVM-Dtest, and Unit tests, but does not directly add the fault injections that Simulator will be doing, that will be follow up work.

Goals:
* FileSystem that works for unit, jvm-dtest, and simulator tests
* FileSystem that allows tests to intercept file operations"
CASSANDRA-18461,CEP-21 Avoid NPE when getting dc/rack for not yet registered endpoints,"If a snitch is asked for location info for a node not yet added to the cluster, it should not NPE. In future, it may be desirable to fine tune the actual behaviour, but for now returning a default would be an improvement.
"
CASSANDRA-18443,Deadlock updating sstable metadata if disk boundaries need reloading,"{{CompactionStrategyManager.handleNotification}} holds the read lock while processing notifications. When handling metadata changed notifications, an extra call is made to maybeReloadDiskBoundaries which tries to grab the write lock and deadlocks the thread.

Partial stacktrace
{code}
        at jdk.internal.misc.Unsafe.park(java.base@11.0.16/Native Method)
        - parking to wait for  <0x00000005cc000078> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.maybeReloadDiskBoundaries(CompactionStrategyManager.java:495)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.getCompactionStrategyFor(CompactionStrategyManager.java:343)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleMetadataChangedNotification(CompactionStrategyManager.java:796)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:838)
        at org.apache.cassandra.db.lifecycle.Tracker.notifySSTableMetadataChanged(Tracker.java:482)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:838)
{code}

Deadlocking with the read lock held blocks the SlabpoolCleaner while notifying ColumnFamilyStore so memtables are prevented from being flushed and recycled, causing any thread applying a mutation to the database (at least GossipStage and MutationStage) to be considered down by peers and/or back up with pending requests.

All the cases investigated were during single sstable upleveling by {{org.apache.cassandra.db.compaction.SingleSSTableLCSTask}} added in CASSANDRA-12526.

Other less critical work was also affected, JMX calls to get estimated remaining compaction tasks, the index summary manager redistributing summaries, the StatusLogger trying to log dropped messages, and the ValidationManager.

Workaround is to reboot the affected host.

The fix is to just remove the redundant disk boundary reload check on that path.
"
CASSANDRA-18422,"CEP-15 (Accord) Original and recover coordinators may hit a race condition with PreApply where reads and writes are interleaved, causing one of the coordinators to see the writes from the other","While verifying CASSANDRA-18364 I saw the following history violation in simulator

{code}
[junit-timeout] Testcase: simulationTest(org.apache.cassandra.simulator.test.ShortAccordSimulationTest)-.jdk1.8:        Caused an ERROR
[junit-timeout] Failed on seed 0xadaca81151490353
[junit-timeout] org.apache.cassandra.simulator.SimulationException: Failed on seed 0xadaca81151490353
[junit-timeout] Caused by: java.lang.AssertionError: History violations detected
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.logAndThrow(PaxosSimulation.java:315)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.isDone(PaxosSimulation.java:278)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation$2.hasNext(PaxosSimulation.java:249)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.run(PaxosSimulation.java:224)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AbstractPairOfSequencesPaxosSimulation.run(AbstractPairOfSequencesPaxosSimulation.java:297)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation.run(PairOfSequencesAccordSimulation.java:62)
[junit-timeout]         at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:374)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:39)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:30)
[junit-timeout]         at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:355)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner.main(AccordSimulationRunner.java:76)
[junit-timeout]         at org.apache.cassandra.simulator.test.ShortAccordSimulationTest.simulationTest(ShortAccordSimulationTest.java:32)
[junit-timeout]         Suppressed: org.apache.cassandra.simulator.paxos.HistoryViolation: Inconsistent sequences on 1: [2, 0, 1, 6, 8, 9, 13, 14, 16, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 40, 43, 47, 48, 49, 54, 56, 57, 58, 60, 64, 68, 70, 71, 74, 76, 79, 80, 83, 85, 87, 87] vs [2, 0, 1, 6, 8, 9, 13, 14, 16, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 40, 43, 47, 48, 49, 54, 56, 57, 58, 60, 64, 68, 70, 71, 74, 76, 79, 80, 83, 85, 87]+90
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier$Register.updateSequence(StrictSerializabilityVerifier.java:607)
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier$Register.access$100(StrictSerializabilityVerifier.java:576)
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier.apply(StrictSerializabilityVerifier.java:825)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.lambda$close$0(StrictSerializabilityValidator.java:66)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator.convertHistoryViolation(StrictSerializabilityValidator.java:89)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator.access$200(StrictSerializabilityValidator.java:27)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.close(StrictSerializabilityValidator.java:66)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.LoggingHistoryValidator$1.close(LoggingHistoryValidator.java:63)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.verify(PairOfSequencesAccordSimulation.java:218)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:135)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.accept(PairOfSequencesAccordSimulation.java:171)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:83)
[junit-timeout]                 at org.apache.cassandra.simulator.systems.SimulatedActionCallable$1.run(SimulatedActionCallable.java:47)
[junit-timeout]                 at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:317)
[junit-timeout]                 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout]                 at java.lang.Thread.run(Thread.java:750)
{code}

Adding logging to track message passing, reads, and writes, I have the following ordering

{code}
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,208 CS:[2] OP:0xea64a268 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,209 CS:[2] OP:0x9761fb36 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,210 CS:[2] OP:0xe62230f2 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,211 CS:[2] OP:0xc5563e5d send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,212 CS:[2] OP:0xc5563e5d send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,212 CS:[2] OP:0xc5563e5d send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,334 CS:[2] OP:0xf8562cfb reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,338 CS:[2] OP:0xcfd2540f reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,338 CS:[2] OP:0xc4cf5af8 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,343 CS:[2] OP:0xb60153ab reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,344 CS:[2] OP:0xac20a1d6 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,345 CS:[2] OP:0xa73f7484 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,349 CS:[2] OP:0xff574276 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,349 CS:[2] OP:0xff574276 Performing read
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,351 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_READ_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 Performing coordinated write
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,360 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,363 CS:[2] OP:0x8bdb6795 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,363 CS:[2] OP:0x8bdb6795 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,364 CS:[2] OP:0x92e94460 Performing write
[MutationStage:4]     node3 2023-04-03 12:54:30,364 Performing write: pre
[MutationStage:4]     node3 2023-04-03 12:54:30,365 Performing write: post
[ReadStage:1]         node3 2023-04-03 12:54:30,365 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,365 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,369 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,369 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,369 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_READ_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,370 CS:[2] OP:0xa59dc286 Performing write
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,374 CS:[2] OP:0xab0f3ca4 Performing write
[MutationStage:1]     node2 2023-04-03 12:54:30,374 Performing write: pre
[MutationStage:1]     node2 2023-04-03 12:54:30,375 Performing write: post
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,376 Performing coordinated write
[MutationStage:3]     node1 2023-04-03 12:54:30,376 Performing write: pre
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,382 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_APPLY_RSP))
[MutationStage:3]     node1 2023-04-03 12:54:30,382 Performing write: post
{code}

(The transaction has a returning select and an auto-read, which is why there are double logs for reads)

Here we see the following timing

{code}
T00 node3 starts txn
T01 node3 sends COMMIT
T02 node2 starts recover
T03 all nodes ack to the pending recover
T04 node2 sends COMMIT
T05 node3 performs reads needed for txn
T06 node3 sends read results to node2
T07 node2 performs write locally and send APPLY
T08 node3 performs write
T09 node3 performs reads needed for txn
T10 node3 send reads to node3
T11 node3 performs write and sends APPLY
T12 node3 ACKs APPLY to node2
{code}

Given the fact the simulator got a response back and didn’t get a preempt, this implies that the original coordinator was able to complete the full transaction without issues and reply back, but the reads/writes were interleaved between node3 and node2 causing the second write to observe the first write"
CASSANDRA-18336,Do not remove SSTables when cause of FSReadError is OutOfMemoryError while using best_effort disk failure policy,"1.When this exception occurs in the system
{code:java}
// 
ERROR [CompactionExecutor:351627] 2023-02-21 17:59:20,721 CassandraDaemon.java:581 - Exception in thread Thread[CompactionExecutor:351627,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Map failed
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:167)
    at org.apache.cassandra.io.util.MmappedRegions$State.add(MmappedRegions.java:310)
    at org.apache.cassandra.io.util.MmappedRegions$State.access$400(MmappedRegions.java:246)
    at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:170)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61)
    at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104)
    at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:365)
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:337)
    at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:172)
    at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:124)
    at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:64)
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:137)
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:193)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:77)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:100)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:298)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.IOException: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1016)
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:163)
    ... 23 common frames omitted
Caused by: java.lang.OutOfMemoryError: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map0(Native Method)
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1013)


{code}
2.Restart the node, Verifying logfile transaction ,All sstables are deleted
{code:java}
// code placeholder
INFO  [main] 2023-02-21 18:00:23,350 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Index.db 
INFO  [main] 2023-02-21 18:00:23,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_c923b230-b077-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,510 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,518 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,543 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,546 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,550 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,590 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,606 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,610 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,685 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,687 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,688 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,727 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,775 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,787 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,020 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,022 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,077 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,078 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,092 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,097 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,098 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,118 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,133 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,484 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,490 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,492 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,502 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Index.db 
INFO  [main] 2023-02-21 18:00:48,045 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Filter.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Summary.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,166 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,202 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,841 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Data.db 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Filter.db 
INFO  [main] 2023-02-21 18:01:22,825 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Index.db 
INFO  [main] 2023-02-21 18:01:24,891 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Summary.db 
INFO  [main] 2023-02-21 18:01:24,892 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,190 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,462 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Index.db 
INFO  [main] 2023-02-21 18:02:02,466 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Filter.db 
INFO  [main] 2023-02-21 18:02:02,467 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,763 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Summary.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,765 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Index.db 
INFO  [main] 2023-02-21 18:02:05,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Filter.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Summary.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Data.db 
INFO  [main] 2023-02-21 18:02:41,367 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:41,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:41,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Index.db 
INFO  [main] 2023-02-21 18:02:42,034 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Filter.db 
INFO  [main] 2023-02-21 18:02:42,049 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Index.db 
INFO  [main] 2023-02-21 18:04:02,784 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Filter.db 
INFO  [main] 2023-02-21 18:04:02,785 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,889 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Index.db 
INFO  [main] 2023-02-21 18:04:03,384 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Filter.db 
INFO  [main] 2023-02-21 18:04:03,418 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Data.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Summary.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:38,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Filter.db 
INFO  [main] 2023-02-21 18:04:38,293 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Index.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Summary.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Data.db 
INFO  [main] 2023-02-21 18:05:28,014 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Digest.crc32 
INFO  [main] 2023-02-21 18:05:28,015 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Statistics.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-TOC.txt 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Filter.db 
INFO  [main] 2023-02-21 18:05:28,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Index.db 
INFO  [main] 2023-02-21 18:05:28,277 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Data.db 
INFO  [main] 2023-02-21 18:06:17,552 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,553 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:17,554 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:17,566 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Index.db 
INFO  [main] 2023-02-21 18:06:17,567 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Filter.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Data.db 
INFO  [main] 2023-02-21 18:06:24,899 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:24,900 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:24,932 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Filter.db 
INFO  [main] 2023-02-21 18:06:24,949 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Index.db 
INFO  [main] 2023-02-21 18:06:29,880 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Data.db 
INFO  [main] 2023-02-21 18:08:11,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Index.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Filter.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,718 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Data.db 
INFO  [main] 2023-02-21 18:08:22,177 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Filter.db 
INFO  [main] 2023-02-21 18:08:22,212 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Index.db 
INFO  [main] 2023-02-21 18:08:22,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Summary.db 
INFO  [main] 2023-02-21 18:08:22,642 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Data.db 
INFO  [main] 2023-02-21 18:09:16,035 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:16,036 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:16,163 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Index.db 
INFO  [main] 2023-02-21 18:09:16,302 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Filter.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Summary.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Data.db 
INFO  [main] 2023-02-21 18:09:30,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Filter.db 
INFO  [main] 2023-02-21 18:09:30,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Index.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Summary.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,487 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,692 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,741 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,742 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Filter.db 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Index.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Summary.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,760 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Index.db 
INFO  [main] 2023-02-21 18:10:18,081 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Filter.db 
INFO  [main] 2023-02-21 18:10:18,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Data.db 
INFO  [main] 2023-02-21 18:11:06,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Summary.db 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:06,080 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Index.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Filter.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Data.db 
INFO  [main] 2023-02-21 18:11:16,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Summary.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:16,902 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:16,903 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Index.db 
INFO  [main] 2023-02-21 18:11:17,170 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Filter.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Summary.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Data.db 
INFO  [main] 2023-02-21 18:11:59,054 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:59,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Index.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Filter.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Summary.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Data.db 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Index.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Filter.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Data.db 
INFO  [main] 2023-02-21 18:12:42,749 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Filter.db 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Index.db 
INFO  [main] 2023-02-21 18:12:42,820 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,821 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Data.db 
INFO  [main] 2023-02-21 18:12:55,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:55,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Filter.db 
INFO  [main] 2023-02-21 18:12:55,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Index.db 
INFO  [main] 2023-02-21 18:12:57,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Data.db 
INFO  [main] 2023-02-21 18:15:07,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Summary.db 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:07,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Index.db 
INFO  [main] 2023-02-21 18:15:07,909 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Filter.db 
INFO  [main] 2023-02-21 18:15:07,950 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,262 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,327 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,550 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,856 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,857 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Data.db 
INFO  [main] 2023-02-21 18:16:19,652 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Summary.db 
INFO  [main] 2023-02-21 18:16:19,653 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Digest.crc32 
INFO  [main] 2023-02-21 18:16:19,654 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Statistics.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-TOC.txt 
INFO  [main] 2023-02-21 18:16:19,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,772 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Data.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Summary.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,232 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Data.db 
INFO  [main] 2023-02-21 18:17:17,432 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:17,449 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Index.db 
INFO  [main] 2023-02-21 18:17:17,969 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Filter.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Index.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Filter.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Summary.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Index.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Filter.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Data.db 
INFO  [main] 2023-02-21 18:18:47,122 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Index.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Filter.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,411 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,413 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,634 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,636 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Data.db 
INFO  [main] 2023-02-21 18:19:17,136 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:17,137 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Index.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Filter.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Data.db 
INFO  [main] 2023-02-21 18:19:36,881 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Filter.db 
INFO  [main] 2023-02-21 18:19:36,884 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Index.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Data.db 
INFO  [main] 2023-02-21 18:19:45,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:45,482 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Filter.db 
INFO  [main] 2023-02-21 18:19:45,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Index.db 
INFO  [main] 2023-02-21 18:19:45,639 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Summary.db 
INFO  [main] 2023-02-21 18:19:45,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Data.db 
INFO  [main] 2023-02-21 18:20:48,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:48,587 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:48,619 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Index.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Filter.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Summary.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Data.db 
INFO  [main] 2023-02-21 18:20:51,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:51,052 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Filter.db 
INFO  [main] 2023-02-21 18:20:51,061 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Index.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Summary.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Summary.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,142 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Data.db 
INFO  [main] 2023-02-21 18:21:22,503 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Summary.db 
INFO  [main] 2023-02-21 18:21:22,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Filter.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Index.db 
INFO  [main] 2023-02-21 18:21:22,660 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Data.db 
INFO  [main] 2023-02-21 18:22:11,241 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Index.db 
INFO  [main] 2023-02-21 18:22:11,328 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Filter.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Data.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Index.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Filter.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Summary.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Data.db 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Index.db 
INFO  [main] 2023-02-21 18:22:51,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Filter.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Summary.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,456 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:54,646 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,648 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:22:54,650 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Index.db 
INFO  [main] 2023-02-21 18:22:54,656 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,673 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,694 Keyspace.java:386 - Creating replication strategy kairosdb params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}
INFO  [main] 2023-02-21 18:22:54,715 ColumnFamilyStore.java:385 - Initializing kairosdb.data_points
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:54,720 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830647-big (179.084MiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830662-big (4.039MiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830663-big (3.589MiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830661-big (39.789MiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830664-big (6.007MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830583-big (190.543MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830440-big (191.089MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:54,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830513-big (194.560MiB)
INFO  [main] 2023-02-21 18:22:54,947 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,977 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_index-8742543087ba11eba3799bdca9e7ad04/mc-1-big (7.580MiB)
INFO  [main] 2023-02-21 18:22:55,023 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_time_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,054 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26770-big (0.075KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,070 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26769-big (0.052KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,077 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26768-big (2.671MiB)
INFO  [main] 2023-02-21 18:22:55,131 ColumnFamilyStore.java:385 - Initializing kairosdb.row_keys
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,135 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-796510-big (7.682MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,190 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-769597-big (50.002MiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,203 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-75-big (87.496MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,209 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-256221-big (51.492MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,211 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-550752-big (50.323MiB)
INFO  [main] 2023-02-21 18:22:55,357 ColumnFamilyStore.java:385 - Initializing kairosdb.service_index
INFO  [main] 2023-02-21 18:22:55,381 ColumnFamilyStore.java:385 - Initializing kairosdb.spec
INFO  [main] 2023-02-21 18:22:55,393 ColumnFamilyStore.java:385 - Initializing kairosdb.string_index
INFO  [main] 2023-02-21 18:22:55,409 ColumnFamilyStore.java:385 - Initializing kairosdb.tag_indexed_row_keys
INFO  [main] 2023-02-21 18:22:55,419 Keyspace.java:386 - Creating replication strategy system_auth params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 18:22:55,425 ColumnFamilyStore.java:385 - Initializing system_auth.network_permissions
INFO  [main] 2023-02-21 18:22:55,440 ColumnFamilyStore.java:385 - Initializing system_auth.resource_role_permissons_index
INFO  [main] 2023-02-21 18:22:55,457 ColumnFamilyStore.java:385 - Initializing system_auth.role_members
INFO  [main] 2023-02-21 18:22:55,473 ColumnFamilyStore.java:385 - Initializing system_auth.role_permissions
INFO  [main] 2023-02-21 18:22:55,485 ColumnFamilyStore.java:385 - Initializing system_auth.roles
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,518 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_auth/roles-5bc52802de2535edaeab188eecebb090/mc-1-big (0.100KiB)
INFO  [main] 2023-02-21 18:22:55,543 Keyspace.java:386 - Creating replication strategy system_distributed params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}
INFO  [main] 2023-02-21 18:22:55,558 ColumnFamilyStore.java:385 - Initializing system_distributed.parent_repair_history
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,577 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-49-big (1.398KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,585 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-44-big (1.376KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,591 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-24-big (0.863KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,593 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-5-big (0.644KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,594 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-12-big (1.130KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,595 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-16-big (0.990KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-39-big (1.646KiB)
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-57-big (2.019KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,605 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-68-big (0.920KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,606 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-107-big (0.728KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,607 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-43-big (0.592KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,608 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-31-big (1.451KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-80-big (0.944KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-106-big (0.589KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,622 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-88-big (0.935KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-20-big (1.151KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-76-big (1.481KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,624 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/mc-1-big (16.284KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-95-big (3.737KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-84-big (1.031KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,633 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-64-big (1.132KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,636 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-100-big (0.561KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,656 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-99-big (1.159KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,683 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-104-big (0.720KiB)
INFO  [main] 2023-02-21 18:22:55,733 ColumnFamilyStore.java:385 - Initializing system_distributed.repair_history
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-45-big (2.741KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-25-big (1.789KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-89-big (1.814KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-112-big (1.327KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,749 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-32-big (3.927KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,756 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-67-big (3.370KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,757 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-81-big (3.249KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,758 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-110-big (1.354KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-44-big (1.250KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-50-big (3.340KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-93-big (2.319KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-85-big (1.789KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,770 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-113-big (1.334KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,773 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-101-big (3.239KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,774 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-21-big (2.331KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,785 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-40-big (4.339KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,789 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-17-big (2.308KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,794 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-106-big (0.766KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,797 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-13-big (2.822KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-105-big (2.460KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-6-big (1.802KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,805 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-74-big (2.896KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,808 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/mc-1-big (0.812KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,811 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-59-big (5.973KiB) {code}
3.Bugs can be reproduced.Just set  vm.max_ map_ count as a small value, and then trigger OOM, and restart the node."
CASSANDRA-18332,Backport CASSANDRA-17205 to 4.0 branch (strong ref leak),"See description in CASSANDRA-17205; this should have been applied on 4.0 and merged up but was overlooked.

 

Also double-check that strong leaks are logged at ERROR instead of WARN on both 4.0, 4.1, and trunk (see [comment|https://issues.apache.org/jira/browse/CASSANDRA-18176?focusedCommentId=17687184&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17687184])"
CASSANDRA-18314,Lift MessagingService.minimum_version to 40,"MessagingService's VERSION_30 and VERSION_3014 don't have to be supported in Cassandra 5.0 anymore.

(Cassandra 5.0 currently is still using VERSION_40)

Patch: https://github.com/apache/cassandra/compare/trunk...thelastpickle:cassandra:mck/18314/trunk

Raises the question whether backward compatibility to the previous major is defined by the Cassandra version or by the internal component version (MessagingService)."
CASSANDRA-18311,BufferPool incorrectly counts memoryInUse when putUnusedPortion is used,"The counter is incorrectly decremented by the size of the unused portion of the provided buffer.
It should be decremented by the number of bytes actually returned to the pool (that may be different than ""size""). The number should be calculated as a difference between original and resulting buffer capacity."
CASSANDRA-18212,Reduce memory allocations of calls to ByteBufer.duplicate() made in org.apache.cassandra.transport.CBUtil#writeValue,"Currently, org.apache.cassandra.transport.CBUtil#writeValue(java.nio.ByteBuffer, io.netty.buffer.ByteBuf) calls ByteBufer.duplicate() and is found to be 20% of memory allocations in production. No changes have been made to reduce this but there is discussion related to the issue in CASSANDRA-13741. Attached below are the performance result for disk based queries and mostly memory based queries. "
CASSANDRA-18200,Cassandra messaging to self changed behavior,"During testing of Cassandra on AWS, we noticed some behavior changes between Cassandra 3.11 and Cassandra 4.0 when it comes to messaging.
When performing a range query with consistency local_quorum, Cassandra sents a request to itself and some peers.
In case of Cassandra 4.0, it's trying to connect to itself using the broadcast_address while in Cassandra 3.11 it's connecting using the local address (see [https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L152].

This translation seems to be missing in Cassandra 4.X. I think the best place to fix it would be here (see attached file): [https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/net/OutboundConnectionSettings.java#L451]"
CASSANDRA-18184,'Maximum memory usage reached' chunk cache log message doesn't specify which cache is exhausted,"With Cassandra 4.0.x, we are seeing this cassandra.log message very frequently on the majority of our Cassandra 4.0 clusters:

    _[INFO ] [epollEventLoopGroup-5-3] cluster_id=99 ip_address=127.0.0.50  NoSpamLogger.java:92 - Maximum memory usage reached (128.000MiB), cannot allocate chunk of 8.000MiB_

It took me several weeks to track down what it means, until I saw this start-up message

    _BufferPools.java:49 - Global buffer pool limit is 2.000GiB for chunk-cache and 128.000MiB for networking_

This maximum memory usage warning would benefit from clarifying that its the {*}network cache{*}, not the *off-heap chunk* *cache* which is exhausted.  With 'chunk cache' in both warning messages, they're too easily confused.

 "
CASSANDRA-18153,"Memtable being flushed without hostId in version ""me"" and newer during CommitLogReplay","On ticket CASSANDRA-16619 some files were changed to allow Cassandra to store HostID in the new ""me"" SSTable version.

But SSTables flushed during CommitLogReplay miss this HostID info.

 

In the next Cassandra startup, if these SSTables were still present, system.log will show:


{{WARN Origin of 3 sstables is unknown or doesn't match the local node; commitLogIntervals for them were ignored}}

{{WARN }}{{{}Origin of 3 sstables is unknown or doesn't match the local node; commitLogIntervals for them were ignored{}}}{{{}{}}}{{ }}

 

And debug.log will show a list of SSTables, witch can include ""md"" and ""me"" version (before upgradesstables):

 

{{Ignored commitLogIntervals from the following sstables: [/var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/me-3-big-Data.db, /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-1-big-Data.db, /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-2-big-Data.db]}}

 

https://issues.apache.org/jira/browse/CASSANDRA-16619"
CASSANDRA-18118,Do not leak 2015 memtable synthetic Epoch,"This [Epoch|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/db/rows/EncodingStats.java#L48] can [leak|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/db/Memtable.java#L392] affecting all the timestamps logic.  It has been observed in a production env it can i.e. prevent proper sstable and tombstone cleanup.

To reproduce create the following table:
{noformat}
drop keyspace test;
create keyspace test WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};
CREATE TABLE test.test (
    key text PRIMARY KEY,
    id text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '2', 'tombstone_compaction_interval': '3000', 'tombstone_threshold': '0.1', 'unchecked_tombstone_compaction': 'true'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.0
    AND default_time_to_live = 10
    AND gc_grace_seconds = 10
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE INDEX id_idx ON test.test (id);
{noformat}

And stress load it with:
{noformat}
insert into test.test (key,id) values('$RANDOM_UUID $RANDOM_UUID', 'eaca36a1-45f1-469c-a3f6-3ba54220363f') USING TTL 10
{noformat}

Notice how all inserts have a 10s TTL, the default 10s TTL and gc_grace is also at 10s. This is to speed up the repro:
- Run the load for a couple minutes and track sstables disk usage. You will see it does only increase, nothing gets cleaned up and it doesn't stop growing (notice all this is well past the 10s gc_grace and TTL)
- Running a flush and a compaction while under load against the keyspace, table or index doesn't solve the issue.
- Stopping the load and running a compaction doesn't solve the issue. Flushing does though.
- On the original observation where TTL was around 600s and gc_grace around 1800s we could get GBs of sstables that weren't cleaned up or compacted away after hours of work.
- Reproduction can also happen on plain sstables by repeatedly inserting/deleting/overwriting the same values over and over again without 2i indices or TTL being involved.

The problem seems to be [EncodingStats|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/db/rows/EncodingStats.java#L48] using a synthetic Epoch in 2015 which plays nice with Vint serialization.  Unfortunately {{Memtable}} is using that to keep track of the {{minTimestamp}} which can leak the 2015 Epoch. This confuses any logic consuming that timestamp. In this particular case purge and fully expired sstables weren't properly detected.
"
CASSANDRA-18113,fqltool dump results NPE when null value inserted using prepared query,"Enable fullquerylog, prepare insert statement and bind it with a null value and execute it. Executing fqltool dump after insert will result into NullPointerException.
 
Stept to reproduce:
    * Create cluster using ccm
    * Create  a table something like:
{code:java}
    CREATE TABLE ks1.t2 (
        id int PRIMARY KEY,
        value text
    ) ;
 {code}
    * Execute below code
{code:java}
    try (CqlSession cqlSession = CqlSession.builder().build()) {
        PreparedStatement preparedStatement = cqlSession.prepare(""INSERT INTO ks1.t2 (id, value) VALUES (?, ?)"");
        cqlSession.execute(preparedStatement.bind(6, null));
    }
{code}
    * Now running fqltool dump. It will run into NPE
 
 
Stack trace:
{code:java}
error: null
-- StackTrace --
java.lang.NullPointerException
    at net.openhft.chronicle.bytes.BytesStore.wrap(BytesStore.java:76)
    at net.openhft.chronicle.bytes.Bytes.wrapForRead(Bytes.java:179)
    at org.apache.cassandra.fqltool.commands.Dump.appendValuesToStringBuilder(Dump.java:222)
    at org.apache.cassandra.fqltool.commands.Dump.dumpQuery(Dump.java:179)
    at org.apache.cassandra.fqltool.commands.Dump.lambda$dump$0(Dump.java:123)
    at net.openhft.chronicle.queue.impl.single.StoreTailer.readDocument(StoreTailer.java:111)
    at org.apache.cassandra.fqltool.commands.Dump.dump(Dump.java:148)
    at org.apache.cassandra.fqltool.commands.Dump.run(Dump.java:68)
    at org.apache.cassandra.fqltool.FullQueryLogTool.main(FullQueryLogTool.java:65)
{code}"
CASSANDRA-18079,Print exception message without stacktrace when nodetool commands fail on probe.getOwnershipWithPort(),"When status, ring or describecluster nodetool commands are executed while a node which is queried is not fully bootstrapped / started, it can throw this exception:
{code:java}
cassandra_node_4  | error: No nodes present in the cluster. Has this node finished starting up?
cassandra_node_4  | -- StackTrace --
cassandra_node_4  | java.lang.RuntimeException: No nodes present in the cluster. Has this node finished starting up?
cassandra_node_4  | 	at org.apache.cassandra.dht.Murmur3Partitioner.describeOwnership(Murmur3Partitioner.java:303)
cassandra_node_4  | 	at org.apache.cassandra.service.StorageService.getOwnershipWithPort(StorageService.java:5751)
cassandra_node_4  | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
cassandra_node_4  | 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
cassandra_node_4  | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
cassandra_node_4  | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
cassandra_node_4  | 	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
cassandra_node_4  | 	at jdk.internal.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
cassandra_node_4  | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
cassandra_node_4  | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
cassandra_node_4  | 	at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:260)
cassandra_node_4  | 	at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
cassandra_node_4  | 	at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
cassandra_node_4  | 	at java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
cassandra_node_4  | 	at java.management/com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
cassandra_node_4  | 	at java.management/com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
cassandra_node_4  | 	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:641)
cassandra_node_4  | 	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
cassandra_node_4  | 	at java.management/com.sun.jmx.remote.security.MBeanServerAccessController.getAttribute(MBeanServerAccessController.java:320)
cassandra_node_4  | 	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1443)
cassandra_node_4  | 	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1307)
cassandra_node_4  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
cassandra_node_4  | 	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1406)
cassandra_node_4  | 	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:637)
cassandra_node_4  | 	at java.base/jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
cassandra_node_4  | 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
cassandra_node_4  | 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
cassandra_node_4  | 	at java.rmi/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:359)
cassandra_node_4  | 	at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:200)
cassandra_node_4  | 	at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:197)
cassandra_node_4  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
cassandra_node_4  | 	at java.rmi/sun.rmi.transport.Transport.serviceCall(Transport.java:196)
cassandra_node_4  | 	at java.rmi/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:562)
cassandra_node_4  | 	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:796)
cassandra_node_4  | 	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:677)
cassandra_node_4  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
cassandra_node_4  | 	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:676)
cassandra_node_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
cassandra_node_4  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
cassandra_node_4  | 	at java.base/java.lang.Thread.run(Thread.java:829)

{code}
The message as such is ok, it is more about the way we inform a user. There should not be stacktrace visible. Why it should? As a user I am not interested in that. All I want to see is one-liner about what happened.

In the code, for example for ""status"", look at this (1). When line 77 fails, it will go to the catch block and there is probe.getOwnershipWithPort() called. However, when that one fails (as shown above), that exception propagates to nodetool which will eventually log it with stacktrace as well. We should wrap this one more time and write exception message only and exit(1).

(1) [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/Status.java#L75-L89]"
CASSANDRA-18058,In-memory index and query path,An in-memory index using the in-memory trie structure introduced with CASSANDRA-17240 along with a query path implementation to perform index queries from the in-memory index.
CASSANDRA-17955,Race condition on repair snapshots,"If an endpoint is convicted and that endpoint is a coordinator then ActiveRepairService::removeParentRepairSession is called.

The issue is that this occurs on clearSnapshotExecutor and can happen while RepairMessageVerbHandler is in process of taking a snapshot. So then you get a race condition and clearSnapshot will throw a java.nio.file.DirectoryNotEmptyException

 
{code:java}
public static void deleteRecursiveWithThrottle(File dir, RateLimiter rateLimiter)
{
    if (dir.isDirectory())
    {
        String[] children = dir.list();
        for (String child : children)
            deleteRecursiveWithThrottle(new File(dir, child), rateLimiter);
    }

    // The directory is now empty so now it can be smoked
    deleteWithConfirmWithThrottle(dir, rateLimiter);
} {code}
Due to the directory not being empty when it goes to remove the directory at the end."
CASSANDRA-17906,Test splits generated with --only-resource-intensive-tests only work on hosts with >= 27GB memory,"As the title says, [here|https://github.com/apache/cassandra-builds/blob/trunk/build-scripts/cassandra-dtest-pytest.sh#L87] we use the ""only resource intensive"" flag to filter to just those tests, but machines with less than 27G will fail [this one|https://github.com/apache/cassandra-dtest/blob/trunk/conftest.py#L113]."
CASSANDRA-17882,Fix flaky test_no_base_column_in_view_pk_complex_timestamp_with_flush,"It seems flaky lately on 4.1 per Butler:

[https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-4.1/failure/materialized_views_test/TestMaterializedViews/test_no_base_column_in_view_pk_complex_timestamp_with_flush]

I was not able to reproduce it on 4.0 and 4.1 in Circle. 
h3.  
{code:java}
Error Message
AssertionError: Expected [[1, 1, 1, None, None, None]] from SELECT * FROM t, but got []

Stacktrace
self = <materialized_views_test.TestMaterializedViews object at 0x7fc5fb98a880> @flaky @since('3.0') def test_no_base_column_in_view_pk_complex_timestamp_with_flush(self): > self._test_no_base_column_in_view_pk_complex_timestamp(flush=True) materialized_views_test.py:1451: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ materialized_views_test.py:1541: in _test_no_base_column_in_view_pk_complex_timestamp assert_one(session, ""SELECT * FROM t"", [1, 1, 1, None, None, None]) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
{code}
 "
CASSANDRA-17852,"WEBSITE - Community page - encourage users to search answered questions on Stack Overflow, Stack Exchange",We need to encourage users to first search the {{[cassandra]}} tag on [Stack Overflow|http://stackoverflow.com/questions/tagged/cassandra] and [Stack Exchange|https://dba.stackexchange.com/questions/tagged/cassandra] before asking a question so contributors are not repeatedly asking the same questions over and over.
CASSANDRA-17840,IndexOutOfBoundsException in Paging State Version Inference (V3 State Received on V4 Connection),"In {{PagingState.java}}, {{index}} is an integer field, and we add long values to it without a {{Math.toIntExact}} check. While we’re checking for negative return values returned by {{getUnsignedVInt}}, there's a chance that the value returned by it is so large that addition operation would cause integer overflow, or the value itself is large enough to cause overflow."
CASSANDRA-17828,"Read/Write/Truncate throw RequestFailure in a race condition with callback timeouts, should return Timeout instead","There is an edge case with write timeout where the condition gets signaled on the timeouts and this happens before await times out, this triggers us to send a Failure rather than a Timeout
"
CASSANDRA-17822,NPE in org.apache.cassandra.cql3.Attributes.getTimeToLive,"{code}
java.lang.NullPointerException
at org.apache.cassandra.cql3.Attributes.getTimeToLive(Attributes.java:129)
       at org.apache.cassandra.cql3.statements.ModificationStatement.getTimeToLive(ModificationStatement.java:237)
       at org.apache.cassandra.cql3.statements.ModificationStatement.makeUpdateParameters(ModificationStatement.java:833)
       at org.apache.cassandra.cql3.statements.ModificationStatement.makeUpdateParameters(ModificationStatement.java:799)
       at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:736)
       at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:689)
       at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:470)
       at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:454)
       at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:255)
       at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:716)
       at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:678)
       at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:160)
       at org.apache.cassandra.transport.Message$Request.execute(Message.java:242)
       at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:142)
       at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:158)
       at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:181)
       at org.apache.cassandra.transport.Dispatcher.lambda$dispatch$1(Dispatcher.java:108)
       at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
       at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
       at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
       at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
       at java.base/java.lang.Thread.run(Thread.java:834)
{code}"
CASSANDRA-17801,NPE bug in streaming checking if SSTable is being repaired,"Streaming hit a race condition where a SSTable was being repaired, but the moment we try to check the repair ID the repair was over

{code}
java.lang.NullPointerException
at org.apache.cassandra.db.streaming.CassandraStreamManager.lambda$null$0(CassandraStreamManager.java:110)
at com.google.common.collect.Iterators$5.computeNext(Iterators.java:639)
at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:141)
at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:136)
at org.apache.cassandra.db.streaming.CassandraStreamManager.lambda$createOutgoingStreams$1(CassandraStreamManager.java:121)
at org.apache.cassandra.db.ColumnFamilyStore.select(ColumnFamilyStore.java:2000)
at org.apache.cassandra.db.ColumnFamilyStore.selectAndReference(ColumnFamilyStore.java:1976)
at org.apache.cassandra.db.streaming.CassandraStreamManager.createOutgoingStreams(CassandraStreamManager.java:96)
at org.apache.cassandra.streaming.StreamSession.getOutgoingStreamsForRanges(StreamSession.java:481)
at org.apache.cassandra.streaming.StreamSession.addTransferRanges(StreamSession.java:440)
at org.apache.cassandra.streaming.StreamSession.lambda$null$6(StreamSession.java:816)
at java.base/java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.cassandra.streaming.StreamSession.lambda$processStreamRequests$7(StreamSession.java:812)
at java.base/java.util.Map.forEach(Map.java:661)
at org.apache.cassandra.streaming.StreamSession.processStreamRequests(StreamSession.java:808)
at org.apache.cassandra.streaming.StreamSession.prepareAsync(StreamSession.java:740)
at org.apache.cassandra.streaming.StreamSession.lambda$prepare$3(StreamSession.java:720)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:834)
{code}
"
CASSANDRA-17792,Fix race condition on updating cdc size and advancing to next segment,"org.apache.cassandra.distributed.test.cdc.ToggleCDCOnRepairEnabledTest is a bit flaky on [trunk. 

As [this run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=17666] shows it was flaky since it was introduced a month ago as part of

CASSANDRA-17666 but the flakiness is so low that if we don't run it in a loop it is hard to hit it. 

Both tests in the test class can fail with the same exception:
{code:java}
org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1056) at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1042) at org.apache.cassandra.distributed.test.cdc.ToggleCDCOnRepairEnabledTest.testCDCOnRepairEnabled(ToggleCDCOnRepairEnabledTest.java:95) at org.apache.cassandra.distributed.test.cdc.ToggleCDCOnRepairEnabledTest.testCDCOnRepairIsEnabled(ToggleCDCOnRepairEnabledTest.java:40) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Suppressed: java.lang.NullPointerException at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC$CDCSizeTracker.recalculateOverflowSize(CommitLogSegmentManagerCDC.java:390) at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:81) at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:47) at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:57) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829){code}
CC [~ycai] , [~jmckenzie] "
CASSANDRA-17731,"Clean up ScheduledExecutors, CommitLog, and MessagingService shutdown for in-JVM dtests","There appear to be two problems w/ the way we shut down {{ScheduledExecutors}} in Instance in 4.0+:

1.) We do it twice, Ince as part of a larger batch of shutdown activity, and then again in its own {{parallelRun()}} block.
2.) It happens before {{MessagingService}} shuts down, but some messaging-related threads (see {{StreamSession#closeSession()}}) can submit tasks to {{nonPeriodicTasks}}.

We should do it once, and do it after the {{MessagingService}} has properly shut down."
CASSANDRA-17678,Cassandra ||system logs|| Unable to compute ceiling for max when histogram overflowed ,"Hi Team,

 

Please help to suggest how to fix the issue as our Cassandra DB is misbehaving.

 

There is a total of 8 Node clusters of Cassandra out of which 2 nodes are misbehaving and getting down over and again. while checking ./nodetool status two nodes always get down within 1 or 2 days.

And we have noticed that Cassandra's services also not running on these two faulty nodes.

Logs:

ERROR [CompactionExecutor:9433] 2022-06-02 16:32:29,997 CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:9433,1,main]
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
    at org.apache.cassandra.utils.EstimatedHistogram.rawMean(EstimatedHistogram.java:231) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.utils.EstimatedHistogram.mean(EstimatedHistogram.java:220) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.io.sstable.metadata.StatsMetadata.getEstimatedDroppableTombstoneRatio(StatsMetadata.java:115) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.io.sstable.format.SSTableReader.getEstimatedDroppableTombstoneRatio(SSTableReader.java:1926) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:423) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:99) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:183) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:153) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_191]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_191]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_191]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_191]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.3.jar:3.11.3]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_191]
ERROR [CompactionExecutor:9435] 2022-06-02 16:32:29,997 CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:9435,1,main]
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
    at org.apache.cassandra.utils.EstimatedHistogram.rawMean(EstimatedHistogram.java:231) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.utils.EstimatedHistogram.mean(EstimatedHistogram.java:220) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.io.sstable.metadata.StatsMetadata.getEstimatedDroppableTombstoneRatio(StatsMetadata.java:115) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.io.sstable.format.SSTableReader.getEstimatedDroppableTombstoneRatio(SSTableReader.java:1926) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:423) ~[apache-cassandra-3.11.3.jar:3.11.3]
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:99) ~[apache-cassandra-3.11.3.jar:3.11.3]"
CASSANDRA-17668,Fix leak of non-standard Java types in our Exceptions as clients using JMX are unable to handle them,"This is a continuation of CASSANDRA-17638 where we fixed leaks introduced during development of 4.1 to ensure no regressions.

This ticket is to fix a few leakages which are there since previous major versions, not 4.1 regressions. 

{_}setRepairSessionMaxTreeDepth{_}(exists since 3.0) and _setRepairSessionSpaceInMegabytes(since 4.0)_

 in the DatabaseDescriptor. 

checkValidForByteConversion and _validateMaxConcurrentAutoUpgradeTasksConf (both since 4.0)_

 are used in both setters and on startup. They shouldn't throw ConfigurationException in the setters. 

There might be more but those are at least a few obvious I found in the DatabaseDescriptor.

CC [~dcapwell] "
CASSANDRA-17638,Clients using JMX are unable to handle non-standard java types but we leak this into our Exceptions,"This is follow up ticket on CASSANDRA-17527 and CASSANDRA-17580. 
CASSANDRA-17580 was decided to be a backup non-default option for users who want to be 100% sure there are no leaked Cassandra exceptions. In that sense I will fix here all known ConfigurationExceptions that need to be thrown as IllegalArgumentException for the default behavior as we were always supposed to do that. "
CASSANDRA-17620,Incremental repair leaks SomeRepairFailedException after switch away from flatMap,"This is an extension of CASSANDRA-17549.

Incremental repair used to do a flatMap where it returned a failed future of SomeRepairFailedException; this would not trigger our failure logging to log it due to the fact the executor didn’t know about it and RepairRunnable knew to ignore that error, a change was made to use map and throw the exception, and later that behavior was updated to trigger uncaughtException handling which leaks this exception.

To roll back to previous behavior, we should switch back to flatMap to avoid this logging"
CASSANDRA-17607,Fix org.apache.cassandra.db.commitlog.BatchCommitLogTest.testOutOfOrderFlushRecovery[3]-cdc,"[https://ci-cassandra.apache.org/job/Cassandra-trunk/1130/testReport/org.apache.cassandra.db.commitlog/BatchCommitLogTest/testOutOfOrderFlushRecovery_3__cdc/]

 
h3.  
{code:java}
Stacktrace
java.io.UncheckedIOException at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:768) at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:753) at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:255) at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:297) at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:304) at org.apache.cassandra.io.util.File.delete(File.java:158) at org.apache.cassandra.io.util.File.delete(File.java:167) at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:691) at org.apache.cassandra.db.commitlog.CommitLogSegment.discard(CommitLogSegment.java:449) at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.discardAvailableSegment(AbstractCommitLogSegmentManager.java:519) at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.shutdown(AbstractCommitLogSegmentManager.java:506) at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.shutdown(CommitLogSegmentManagerCDC.java:157) at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.stopUnsafe(AbstractCommitLogSegmentManager.java:452) at org.apache.cassandra.db.commitlog.CommitLog.stopUnsafe(CommitLog.java:504) at org.apache.cassandra.db.commitlog.CommitLog.resetUnsafe(CommitLog.java:470) at org.apache.cassandra.db.commitlog.CommitLogTest.testOutOfOrderFlushRecovery(CommitLogTest.java:991) at org.apache.cassandra.db.commitlog.CommitLogTest.testOutOfOrderFlushRecovery(CommitLogTest.java:1057) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Caused by: java.nio.file.NoSuchFileException: build/test/cassandra/commitlog/CommitLog-7-1651753848882.log at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:249) at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:105) at java.base/java.nio.file.Files.delete(Files.java:1142) at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:250)
{code}
 "
CASSANDRA-17605,Fix dtest-offheap.materialized_views_test.TestMaterializedViews.test_mv_with_default_ttl_with_flush,"h3.  

https://ci-cassandra.apache.org/job/Cassandra-4.1/5/testReport/dtest-offheap.materialized_views_test/TestMaterializedViews/test_mv_with_default_ttl_with_flush_2/
{code:java}
Error Message
AssertionError: Expected [[1, 6, 1]] from SELECT * FROM t, but got [[1, None, 1]]

Stacktrace
self = <materialized_views_test.TestMaterializedViews object at 0x7f8b1fd21cd0> @since('3.0') def test_mv_with_default_ttl_with_flush(self): > self._test_mv_with_default_ttl(True) materialized_views_test.py:1320: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ materialized_views_test.py:1416: in _test_mv_with_default_ttl assert_one(session, ""SELECT * FROM t"", [1, 6, 1]) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ session = <cassandra.cluster.Session object at 0x7f8b1f167f70> query = 'SELECT * FROM t', expected = [1, 6, 1], cl = None def assert_one(session, query, expected, cl=None): """""" Assert query returns one row. @param session Session to use @param query Query to run @param expected Expected results from query @param cl Optional Consistency Level setting. Default ONE Examples: assert_one(session, ""LIST USERS"", ['cassandra', True]) assert_one(session, query, [0, 0]) """""" simple_query = SimpleStatement(query, consistency_level=cl) res = session.execute(simple_query) list_res = _rows_to_list(res) > assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res) E AssertionError: Expected [[1, 6, 1]] from SELECT * FROM t, but got [[1, None, 1]] tools/assertions.py:132: AssertionError
{code}
 "
CASSANDRA-17596,Fix NPE in SimpleBuilders.ParititionUpdateBulder.RTBuilder.build,"These two arrays are not initialised (1) which means that if I do not set start nor end, when ClusteringBound.create is called, it will be null, but it will fail here (2) as values will be null.

The fix consists of check if values are null and if they are, we build that bound immediately.

(1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SimpleBuilders.java#L257-L258
(2) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ClusteringBound.java#L128"
CASSANDRA-17595,nodetool enablefullquerylog can NPE when directory has no files,"fqltool_test.py::TestFQLTool::test_unclean_enable found an issue with org.apache.cassandra.utils.binlog.BinLog#deleteRecursively, it assumes listing files returns empty list rather than null and throws a NPE

{code}
E           -- StackTrace --
E           java.lang.NullPointerException
E           	at org.apache.cassandra.utils.binlog.BinLog.deleteRecursively(BinLog.java:490)
E           	at org.apache.cassandra.utils.binlog.BinLog.cleanDirectory(BinLog.java:477)
E           	at org.apache.cassandra.utils.binlog.BinLog$Builder.build(BinLog.java:436)
E           	at org.apache.cassandra.fql.FullQueryLogger.enable(FullQueryLogger.java:106)
E           	at org.apache.cassandra.service.StorageService.enableFullQueryLogger(StorageService.java:6495)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E           	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.lang.reflect.Method.invoke(Method.java:498)
E           	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E           	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.lang.reflect.Method.invoke(Method.java:498)
E           	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:276)
E           	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
E           	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
E           	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
E           	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
E           	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
E           	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
E           	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
E           	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
E           	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
E           	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
E           	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
E           	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
{code}"
CASSANDRA-17553,NPE in RangeTombstoneTest.overlappingRangeTest-compression,"The trace looks something like this on trunk:

{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.db.RangeTombstoneTest.overlappingRangeTest(RangeTombstoneTest.java:437)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}

See https://ci-cassandra.apache.org/job/Cassandra-trunk/1078/testReport/org.apache.cassandra.db/RangeTombstoneTest/overlappingRangeTest_compression/"
CASSANDRA-17552,"LongBufferPoolTest failing, several data races in BufferPool","{{LongBufferPoolTest}} fails pretty consistently on my local laptop.

I identified 3 different failure modes:
 
{noformat}
ERROR [test:1] 2022-04-13 16:29:03,064 LongBufferPoolTest.java:588 - Got throwable null, current chunk [slab java.nio.DirectByteBuffer[pos=0 lim=131072 cap=131072], slots bitmap 1111111111111111111111111111111111111111111111111111111111111111, capacity 131072, free 131072]
java.lang.AssertionError
    at org.apache.cassandra.utils.memory.BufferPool$Chunk.get(BufferPool.java:1315)
    at org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.get(BufferPool.java:576)
    at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGetInternal(BufferPool.java:900)
    at org.apache.cassandra.utils.memory.BufferPool$LocalPool.lambda$new$0(BufferPool.java:739)
    at org.apache.cassandra.utils.memory.BufferPool$LocalPool.addChunkFromParent(BufferPool.java:952)
    at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGetInternal(BufferPool.java:907)
    at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGet(BufferPool.java:893)
    at org.apache.cassandra.utils.memory.BufferPool$LocalPool.access$000(BufferPool.java:710)
    at org.apache.cassandra.utils.memory.BufferPool.tryGet(BufferPool.java:205)
    at org.apache.cassandra.utils.memory.LongBufferPoolTest$2.testOne(LongBufferPoolTest.java:513)
    at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:575)
    at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:553)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:748)
{noformat}
 
{noformat}
ERROR [main] 2022-04-13 16:30:27,139 LongBufferPoolTest.java:614 - Test failed - null
java.lang.AssertionError: null
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$Debug.check(LongBufferPoolTest.java:106)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest.testAllocate(LongBufferPoolTest.java:288)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest.main(LongBufferPoolTest.java:607)
{noformat}

{noformat}
ERROR [test:1] 2022-04-13 16:36:54,093 LongBufferPoolTest.java:580 - Got exception null, current chunk null
java.lang.NullPointerException
	at org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.add(BufferPool.java:513)
	at org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.access$2200(BufferPool.java:480)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.addChunk(BufferPool.java:963)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.addChunkFromParent(BufferPool.java:956)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGetInternal(BufferPool.java:907)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGet(BufferPool.java:893)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.access$000(BufferPool.java:710)
	at org.apache.cassandra.utils.memory.BufferPool.tryGet(BufferPool.java:205)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$2.testOne(LongBufferPoolTest.java:512)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:575)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:553)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

Branch: cassadra 4.0, commit d1270c204f31578212bfca5860ab46abeaec22b9 

So far I've found the following problems with the code (this list might not be complete):

Problem 1:
{{LocalPool}}  documentation states that allocations from the local pool can be done by a single thread only, but releases can be done by any thread. This means {{LocalPool}} is shared between threads and should be thread safe.
Unfortunately the implementation is far from thread safe, because {{LocalPool}} has mutable and unsynchronized state in {{MicroQueueOfChunks}}. 

Possible problem 2: 
There seems to be an assumption that the {{Chunk}} may be released only when no more allocations are going on from it. However, I believe this assumption does not hold and I can't see code enforcing that assumption. Because {{release}} can be called by a different thread than the owner, it may clear the owner and immediately clear the {{freeSlots}} bitmap in line 1150, despite the fact that a concurrent allocation is still in progress. Clearing the flags in the wrong moment would cause the assertion in line 1315 to fail.
"
CASSANDRA-17543,ReadRepairQueryTypesTest.testUnrestrictedQueryOnSkinnyTable[8: strategy=NONE coordinator=1 flush=false paging=false] times out sporadically,"org.apache.cassandra.distributed.test.ReadRepairQueryTypesTest.testUnrestrictedQueryOnSkinnyTable[8: strategy=NONE coordinator=1 flush=false paging=false]

{noformat}
Error Message
Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
Stacktrace
junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1388)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1388)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1388)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.apache.cassandra.anttasks.TestHelper.execute(TestHelper.java:53)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1388)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}

See https://ci-cassandra.apache.org/job/Cassandra-trunk/1075/testReport/org.apache.cassandra.distributed.test/ReadRepairQueryTypesTest/testUnrestrictedQueryOnSkinnyTable_8__strategy_NONE_coordinator_1_flush_false_paging_false_/"
CASSANDRA-17527,Clients using JMX are unable to handle non-standard java types but we leak this into our interfaces,"JMX clients only work if-and-only-if they have the same types defined by the interface, so non-standard java (or javax) types should never be used; we currently rely on humans to block this in review, which has lead to a few interfaces exposing Cassandra types"
CASSANDRA-17472,"On CFS.flushLargestMemtable, log candidate at info instead of debug","When flushing the largest memtable in {{{}ColumnFamilyStore.flushLargestMemtable{}}}, we should log at info that a memtable was flushed due to this case rather than at debug. We don't expect this to trigger often and this information is useful for operators and shouldn't require elevated logging to see it."
CASSANDRA-17443,testUnwriteableFlushRecovery is never called,`testUnwriteableFlushRecovery` of `CommitLogTest` was implemented 6 years ago and seems to became a dead code immediately. It should be either enabled back or removed. The source of the test is at https://github.com/apache/cassandra/blob/2b2c6decfafc6235ad537e72073fab2fd4467e2f/test/unit/org/apache/cassandra/db/commitlog/CommitLogTest.java#L932
CASSANDRA-17347,Instance failed to start up due to NPE in StartupClusterConnectivityChecker,"Instance is crashing during startup due to a NPE in StartupClusterConnectivityChecker with stack trace:
{noformat}
java.lang.NullPointerException: element cannot be mapped to a null key
        at java.util.Objects.requireNonNull(Objects.java:228)
        at java.util.stream.Collectors.lambda$groupingBy$45(Collectors.java:907)
        at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
        at org.apache.cassandra.net.StartupClusterConnectivityChecker.execute(StartupClusterConnectivityChecker.java:173)
{noformat}"
CASSANDRA-17335,Fix race condition bug during local session repair,"The in-JVM dtest {{RepairErrorsTest#testNoSuchRepairSessionAnticompaction}} seems to be flaky, as it's shown by [this repeated run|https://app.circleci.com/pipelines/github/adelapena/cassandra/1280/workflows/8a4e04cb-64cc-46a3-9e1e-c946dfafc7fa/jobs/12114] on trunk, which hits 18 failures in 500 iterations. The config for CircleCI was generated with:
{code}
.circleci/generate.sh -m \
  -e REPEATED_UTEST_TARGET=test-jvm-dtest-some \
  -e REPEATED_UTEST_COUNT=500 \
  -e REPEATED_UTEST_CLASS=org.apache.cassandra.distributed.test.RepairErrorsTest
{code}
This was discovered while testing CASSANDRA-16878, on [this CI run|https://app.circleci.com/pipelines/github/adelapena/cassandra/1268/workflows/aef1c703-c816-40f8-8e07-9055027d6403/jobs/12000].

The error consists on a failed assertion when grepping the logs in search of an error message."
CASSANDRA-17331,For rpm packaging replace centos8 docker images with almalinux,"Centos went EOL, and CI started failing today with
{code}
Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist
{code}
ref: https://ci-cassandra.apache.org/job/Cassandra-trunk-artifacts/1010/jdk=jdk_1.8_latest,label=cassandra/consoleFull

Switching to almalinux seems one common path forward…"
CASSANDRA-17295,Make vtables accessible via internode messaging,"As an extension of CASSANDRA-15399 and needed to solve CASSANDRA-15566, we need the ability to perform queries against vtables using internode messaging; currently this is limited to CQL which isn’t exposed in internode"
CASSANDRA-17252,ConnectionLimitHandler may leaks connection count if remote connection drops,"In some cases, Netty does not return the original IP used for per-IP counting when the channel becomes inactive,
which throws an NPE before decrementing the active per-IP count.


{code:java}
java.lang.NullPointerException
at org.apache.cassandra.transport.ConnectionLimitHandler.channelInactive(ConnectionLimitHandler.java:101)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
       at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
       at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)
       at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
       at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
       at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
       at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
       at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)
       at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
       at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
       at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
       at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
       at java.base/java.lang.Thread.run(Thread.java:834)
{code}
"
CASSANDRA-17239,Race in CompactionExecutorTest,"CompactionExecutorTest has a race between the runnable/callable under test completing
and the {{afterExecute}} method stashing it for the test.  Replace the wait/sleep loop
with a {{SimpleCondition}} that is signaled once the test task throwable has been recorded.

This seems fairly hard to hit but has happened on CI.  It took about 2600 iterations on my MacBook to trigger, but you can artificially hit frequently by adding a sleep at the start of the afterExecute method.
"
CASSANDRA-17205,File leaks will not be be detected and released due to strong self-references in the tidier,"LogTransaction.SSTableTidier holds a reference to a {{Tracker}} which holds references to both a {{ColumnFamilyStore}} and a {{View}}, both of which hold refs to SSTableReaders. As per the comment at the top of the SSTableTidier:
{quote}// must not retain a reference to the SSTableReader, else leak detection cannot kick in
{quote}
We shouldn't hold a reference to the Tracker here; long running unit tests w/-Dcassandra.debugrefcount=true had this pop up.

{code}ERROR [Strong-Reference-Leak-Detector:1] 2020-10-27T01:10:12,421 NoSpamLogger.java:97 - Strong self-ref loop detected{code}"
CASSANDRA-17174,Harden resource management on SSTable components to prevent further leaks,"We've seen resource leaks pop up w/histogram overflows repeatedly; the code in {{BigTableWriter.openEarly()}} and {{BigTableWriter.openFinal()}} doesn't appropriately catch and handle any exceptions during creation before things are registered with a {{LifecycleTransaction}} so any errors there will leak.

We should clean that up."
CASSANDRA-17103,CEP-15 (C*): Messaging and storage engine integration,"This work encompasses implementing Accord’s storage and networking interfaces within Cassandra, so that messages may be sent around the cluster and executed"
CASSANDRA-17088,Fix test org.apache.cassandra.net.MessagingServiceTest,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/1062/workflows/3642831f-81d0-4c4f-8bb7-9444e11b58b4/jobs/7084

{code}
junit.framework.AssertionFailedError
	at org.apache.cassandra.net.MessagingServiceTest.listen(MessagingServiceTest.java:343)
	at org.apache.cassandra.net.MessagingServiceTest.listenRequiredSecureConnection(MessagingServiceTest.java:277)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}

And

{code}
junit.framework.AssertionFailedError
	at org.apache.cassandra.net.MessagingServiceTest.listen(MessagingServiceTest.java:343)
	at org.apache.cassandra.net.MessagingServiceTest.listenOptionalSecureConnection(MessagingServiceTest.java:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}"
CASSANDRA-17068,Failed inbound internode authentication failures generate ugly warning with stack trace,Inbound connections can come from anywhere and the warning / stack trace is unhelpful so I'd like to downgrade to a simple single-line INFO message when authorization fails to reduce clutter in the logs.
CASSANDRA-17054,"v4+ protocol did not clean up client warnings, which caused leaking the state","If you perform a query in v5, this will cause the STARTUP message to be handled in the netty thread, but the way this is done is by calling an internal API to dispatcher which requires the caller to clean up; but we do not clean up; at this point the netty thread will have a ClientWarn state populated.  If you now perform the same query again, but with the v3 protocol, this will pick up the state and try to serialize it, causing a client error (in java as java rejects the output from the server) saying that v3 may not have client warnings."
CASSANDRA-17049,Fix rare NPE caused by batchlog replay / node decomission races,"Batchlog replay process collects addresses of the hosts that have been hinted to, so it can flush hints for them to disk before confirming deletion of the replayed batches. If a node has been decommissioned during replay, however, when the time comes to flush the hints at the very end of replay, {{StorageService.getHostIdForEndpoint()}} will return {{null}} for its address, which will, down the line, cause {{HintsCatalog::get()}} to be invoked with a {{null}} host id argument, causing an NPE.

The simple fix is to check returned host ids for addresses for nulls, and collect hinted host ids instead of hinted addresses."
CASSANDRA-17041,Fix resource leak due to Files.list,"Files.list will opend dir and we should close it.

 

see jdk:

the
 * \{@code try}-with-resources construct should be used to ensure that the
 * stream's \{@link Stream#close close} method is invoked after the stream
 * operations are completed.

 

 "
CASSANDRA-17033,MessagingServiceTest listenOptionalSecureConnection and listenRequiredSecureConnection fail sporadically,"This popped up in a recent CircleCI run: https://app.circleci.com/pipelines/github/maedhroz/cassandra/351/workflows/f8dbf599-df72-4982-8a12-a72a5b8ddd3b/jobs/2195

I was able to reproduce it locally on trunk as well in a single run, although it happens perhaps once per 3-4 runs of the MessagingServiceTest suite. It looks like we open the sockets in an InboundSockets container, await on the future that returns, and then assert that all those sockets are open. It’s not clear to me yet why this assertion fails sporadically, or what state changes exactly are racing."
CASSANDRA-17025,Direct memory OOM ,"Version: 3.11.11

Cluster: 9 nodes (1 dc, 3 racks) on aws r5.4xlarge nodes (16 vCPU, 128GB mem)

Heap size: 20G

Direct memory buffer: 24G

 

We stood up this cluster a few months ago in order to migrate an old 2.0 cluster. Since then, after about 7-10 days a node will begin to experience long old gc cycles  before logging many  ""java.lang.OutOfMemoryError: Direct buffer memory"" exceptions. When the long gc cycles start, the entire cluster becomes unresponsive (our application is unable to make queries to any node).

Restarting cassandra on the failing node resolves the problem, then we have to restart every other node in the cluster to prevent them from getting into the same state.

 

We have attempted to:
 * Increase -XX:MaxDirectMemorySize
 * Increase -Djdk.nio.maxCachedBufferSize
 * Update cassandra from 3.11.10 to 3.11.11

None of these have resolved the problem.

Since the last failure, we have increased -XX:MaxDirectMemorySize again, and are waiting to see if that has any effect.

 

Old gc collections from system.log:
{noformat}
INFO  [Service Thread] 2021-10-04 15:24:04,973 GCInspector.java:285 - G1 Old Generation GC in 4447ms.  Compressed Class Space: 6683064 -> 6677952; G1 Eden Space: 16777216 -> 0; G1 Old Gen: 5431375384 -> 745163912; G1 Survivor Space: 419430400 -> 0; Metaspace: 54716768 ->
...
INFO  [Service Thread] 2021-10-04 15:24:06,985 GCInspector.java:285 - G1 Old Generation GC in 1901ms.  G1 Old Gen: 745163912 -> 745168360;
...
INFO  [Service Thread] 2021-10-04 15:24:09,306 GCInspector.java:285 - G1 Old Generation GC in 2046ms.  G1 Eden Space: 528482304 -> 0; G1 Old Gen: 759785184 -> 761229864;
...
INFO  [Service Thread] 2021-10-04 15:24:14,749 GCInspector.java:285 - G1 Old Generation GC in 5403ms.  G1 Old Gen: 761229864 -> 762168640;
...
INFO  [Service Thread] 2021-10-04 15:24:16,782 GCInspector.java:285 - G1 Old Generation GC in 1949ms.  G1 Old Gen: 762168640 -> 762167640;
...
INFO  [Service Thread] 2021-10-04 15:25:09,406 GCInspector.java:285 - G1 Old Generation GC in 52302ms.  G1 Eden Space: 8388608 -> 0; G1 Old Gen: 762167640 -> 762168160;
...
INFO  [Service Thread] 2021-10-04 15:25:15,011 GCInspector.java:285 - G1 Old Generation GC in 5522ms.  G1 Eden Space: 192937984 -> 0; G1 Old Gen: 762168160 -> 770098088;
...
INFO  [Service Thread] 2021-10-04 15:25:31,453 GCInspector.java:285 - G1 Old Generation GC in 16310ms.  G1 Eden Space: 201326592 -> 0; G1 Old Gen: 770098088 -> 769228400;
...
INFO  [Service Thread] 2021-10-04 15:25:33,597 GCInspector.java:285 - G1 Old Generation GC in 1984ms.  G1 Eden Space: 352321536 -> 0; G1 Old Gen: 750824952 -> 751118968;
...
INFO  [Service Thread] 2021-10-04 15:25:50,152 GCInspector.java:285 - G1 Old Generation GC in 16411ms.  G1 Eden Space: 8388608 -> 0; G1 Old Gen: 751118968 -> 751645056;
{noformat}
Example of direct memory oom from system.log
{noformat}
INFO  [ScheduledTasks:1] 2021-10-04 15:25:31,484 MessagingService.java:1246 - READ messages were dropped in last 5000 ms: 2 internal and 7 cross node. Mean internal dropped latency: 47238 ms and Mean cross-node dropped latency: 45171 ms
INFO  [ScheduledTasks:1] 2021-10-04 15:25:31,484 MessagingService.java:1246 - COUNTER_MUTATION messages were dropped in last 5000 ms: 1 internal and 60 cross node. Mean internal dropped latency: 7289 ms and Mean cross-node dropped latency: 9509 ms
ERROR [CounterMutationStage-464] 2021-10-04 15:25:31,484 JVMStabilityInspector.java:94 - OutOfMemory error letting the JVM handle the error:
java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:695) ~[na:1.8.0_292]
        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) ~[na:1.8.0_292]
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[na:1.8.0_292]
        at org.apache.cassandra.utils.memory.BufferPool.allocate(BufferPool.java:114) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.utils.memory.BufferPool.access$1000(BufferPool.java:50) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.utils.memory.BufferPool$LocalPool.allocate(BufferPool.java:408) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.utils.memory.BufferPool$LocalPool.access$000(BufferPool.java:335) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.utils.memory.BufferPool.takeFromPool(BufferPool.java:126) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.utils.memory.BufferPool.get(BufferPool.java:98) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.cache.ChunkCache.load(ChunkCache.java:156) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.cache.ChunkCache.load(ChunkCache.java:39) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache.lambda$new$0(BoundedLocalCache.java:2949) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$15(BoundedLocalCache.java:1807) ~[caffeine-2.2.6.jar:na]
        at java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1892) ~[na:1.8.0_292]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:1805) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:1788) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:97) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:66) ~[caffeine-2.2.6.jar:na]
        at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.rebuffer(ChunkCache.java:236) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.rebuffer(ChunkCache.java:214) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.io.util.RandomAccessReader.reBufferAt(RandomAccessReader.java:65) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:207) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.io.util.FileHandle.createReader(FileHandle.java:150) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getFileDataInput(SSTableReader.java:1801) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:103) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:49) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:72) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:65) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.initializeIterator(UnfilteredRowIteratorWithLowerBound.java:108) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.partitionLevelDeletion(LazilyInitializedUnfilteredRowIterator.java:81) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.partitionLevelDeletion(UnfilteredRowIteratorWithLowerBound.java:170) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:762) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:673) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.CounterMutation.updateWithCurrentValuesFromCFS(CounterMutation.java:258) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.CounterMutation.processModifications(CounterMutation.java:202) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.CounterMutation.applyCounterMutation(CounterMutation.java:123) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.service.StorageProxy$9.runMayThrow(StorageProxy.java:1650) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2771) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.service.StorageProxy$3.apply(StorageProxy.java:160) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:1201) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:1627) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:48) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_292]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.11.jar:3.11.11]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:113) [apache-cassandra-3.11.11.jar:3.11.11]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_292]
{noformat}
Example of full gc from from gc.log
{noformat}
2021-10-04T15:25:46.467+0000: 494369.005: [Full GC (System.gc())  727M->726M(20480M), 1.8053704 secs]
   [Eden: 8192.0K(12288.0M)->0.0B(12288.0M) Survivors: 0.0B->0.0B Heap: 727.3M(20480.0M)->726.2M(20480.0M)], [Metaspace: 53437K->53437K(1099776K)]
Heap after GC invocations=14150 (full 57):
 garbage-first heap   total 20971520K, used 743652K [0x00000002c0000000, 0x00000002c0805000, 0x00000007c0000000)
  region size 8192K, 0 young (0K), 0 survivors (0K)
 Metaspace       used 53437K, capacity 55970K, committed 56528K, reserved 1099776K
  class space    used 6522K, capacity 6987K, committed 7120K, reserved 1048576K
}
 [Times: user=2.99 sys=0.00, real=1.81 secs]
2021-10-04T15:25:48.273+0000: 494370.811: Total time for which application threads were stopped: 1.8099675 seconds, Stopping threads took: 0.0003245 seconds
{Heap before GC invocations=14150 (full 57):
 garbage-first heap   total 20971520K, used 743652K [0x00000002c0000000, 0x00000002c0805000, 0x00000007c0000000)
  region size 8192K, 1 young (8192K), 0 survivors (0K)
 Metaspace       used 53437K, capacity 55970K, committed 56528K, reserved 1099776K
  class space    used 6522K, capacity 6987K, committed 7120K, reserved 1048576K
{noformat}"
CASSANDRA-16865,Avoid logging full stack trace when index summary redistribution is cancelled,"When a compaction process is interrupted in an expected fashion, we don't need to dump full stack. Clutters up the logs."
CASSANDRA-16854,Exclude Jackson 1.x dependency that leaks via old hadoop-core dependency,"build.xml has a dependency for an old hadoop-core version (1.0.3). This is likely needed for some Hadoop compatibility code under `src/java/org/apache/cassandra/hadoop`. Since 1.0.3 was released in 2012, its dependencies are very old; in particular it depends on ""jackson-mapper-asl"" 1.0.1 (from 2009!).
An earlier issue CASSANDRA-15867 referenced this dependency as well (but did not actually remove it for some reason, which marked as resolved).

Although `hadoop-core` dependency is marked as ""provided"" (and should then not be included in distributed version) it seems best to avoid downloading it during build to ""build/lib/jars"". This can be done by adding 2 exclusions for ""hadoop-core"" and ""hadoop-minicluster"" dependencies in build.xml.
I will provide a patch.

I also tried updating ""hadoop-core"" to the latest public version (1.2.1), which would have upgraded jackson-mapper-asl to 1.8.8, but that breaks the build due to some other version incompatibility (asm, possibly). If anyone wants to tackle that issue it could be a good follow-up task; ""hadoop-core"" itself has been moved to ""hadoop-client"" it seems (and there's ""hadoop-commons"" too... confusing).

 "
CASSANDRA-16805,NPE when running in-jvm upgrade dtest in CircleCI,"CASSANDRA-16649 changed the upgrade dtest to test all upgrade paths. When running from trunk, the dtest jar of the lower version should present. 
 
CircleCI config is not updated to build the cassandra-4.0 dtest jar, so that most of the upgrade dtest fail. Add the 4.0 version to the dtest jar build task should fix. "
CASSANDRA-16721,Repaired data tracking on a read coordinator is susceptible to races between local and remote requests,"At read time on a coordinator which is also a replica, the local and remote reads can race such that the remote responses are received while the local read is executing. If the remote responses are mismatching, triggering a {{DigestMismatchException}} and subsequent round of full data reads and read repair, the local runnable may find the {{isTrackingRepairedStatus}} flag flipped mid-execution.  If this happens after a certain point in execution, it would mean
that the RepairedDataInfo instance in use is the singleton null object {{RepairedDataInfo.NULL_REPAIRED_DATA_INFO}}. If this happens, it can lead to an NPE when calling {{RepairedDataInfo::extend}} when the local results are iterated."
CASSANDRA-16681,org.apache.cassandra.utils.memory.LongBufferPoolTest - tests are flaky,"Jenkins history:

[https://jenkins-cm4.apache.org/job/Cassandra-4.0/50/testReport/junit/org.apache.cassandra.utils.memory/LongBufferPoolTest/testPoolAllocateWithRecyclePartially/history/]

Fails being run in a loop in CircleCI:

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/844/workflows/945011f4-00ac-4678-89f6-5c0db0a40169/jobs/5008

 "
CASSANDRA-16673,Avoid race in AbstractReplicationStrategy endpoint caching,"We should make sure we track which ringVersion we are caching in AbstractReplicationStrategy to avoid a race where we might return the wrong EndpointsForRange.

{code}
Caused by: java.lang.IllegalArgumentException: 9010454139840013625 is not contained within (9223372036854775801,-4611686018427387905]
	at org.apache.cassandra.locator.EndpointsForRange.forToken(EndpointsForRange.java:59)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalReplicasForToken(AbstractReplicationStrategy.java:104)
	at org.apache.cassandra.locator.ReplicaLayout.forTokenReadLiveSorted(ReplicaLayout.java:330)
	at org.apache.cassandra.locator.ReplicaPlans.forRead(ReplicaPlans.java:594)
{code}"
CASSANDRA-16668,Intermittent failure of SEPExecutorTest.changingMaxWorkersMeetsConcurrencyGoalsTest caused by race condition when shrinking maximum pool size to zero,"A difficult-to-hit race condition exists in changingMaxWorkersMeetsConcurrencyGoalsTest when changing the maximum pool size from 0 -> 4 which results in the test failing like so:

{{junit.framework.AssertionFailedError: Test tasks did not hit max concurrency goal expected:<true> but was:<false>junit.framework.AssertionFailedError: Test tasks did not hit max concurrency goal expected:<true> but was:<false> at org.apache.cassandra.concurrent.SEPExecutorTest.assertMaxTaskConcurrency(SEPExecutorTest.java:198) at org.apache.cassandra.concurrent.SEPExecutorTest.changingMaxWorkersMeetsConcurrencyGoalsTest(SEPExecutorTest.java:132)}}

I can hit this issue maybe 2/3 times for every 100 invocations of the unit test.

The issue that causes the failure is that if tasks are still enqueued when the maximum pool size is set to zero and if all of the SEPWorker threads enter the STOP state before the pool size is bumped to 4, then no SEPWorker threads will be spun up to service the task queue. This causes the above error.

Why don't we spin up SEPWorker threads when enqueing tasks? Because of the guard logic in addTask: [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/concurrent/SEPExecutor.java#L113,L121]

In this scenario taskPermits will not be zero (because we have tasks on the queue) so we never call {{maybeStartSpinningWorker()}}.

A trick to make this issue much easier to hit is to insert a {{Thread.sleep(500)}} immediately after setting the pool size to zero. This has the effect of guaranteeing that all SEPWorker threads will be STOP'd before enqueueing more work.

Here's a fix that attempts to spin up an SEPWorker whenever we grow the number of work permits: https://github.com/mfleming/cassandra/commit/071516d29e41da9924af24e8002822d3c6af0e01"
CASSANDRA-16607,Fix flaky test testRequestResponse – org.apache.cassandra.net.MockMessagingServiceTest,"https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/659/tests/

{code}
Error
expected:<1> but was:<0>
Stacktrace
junit.framework.AssertionFailedError: expected:<1> but was:<0>
	at org.apache.cassandra.net.MockMessagingServiceTest.testRequestResponse(MockMessagingServiceTest.java:81)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Standard Output
INFO  [main] 2021-04-15 08:22:46,838 YamlConfigurationLoader.java:93 - Configuration location: file:/home/cassandra/cassandra/test/conf/cassandra.yaml
DEBUG [main] 2021-04-15 08:22:46,840 YamlConfigurationLoader.java:112 - Loading settings from file:/home/cassandra/cassandra/test/conf/cassandra.yaml
DEBUG [main] 2021-04-15 08:22:46,899 InternalLoggerFactory.java:63 - Using SLF4J as the default logging framework
DEBUG [main] 2021-04-15 08:22:46,911 PlatformDependent0.java:417 - -Dio.netty.noUnsaf
...[truncated 61235 chars]...
te NORMAL, token [a57d4b7f61f49471614b7ac41f16477e]
DEBUG [main] 2021-04-15 08:22:49,840 StorageService.java:2674 - New node /127.0.0.1:7069 at token a57d4b7f61f49471614b7ac41f16477e
DEBUG [main] 2021-04-15 08:22:49,848 StorageService.java:2727 - Node /127.0.0.1:7069 state NORMAL, token [a57d4b7f61f49471614b7ac41f16477e]
INFO  [main] 2021-04-15 08:22:49,848 StorageService.java:2730 - Node /127.0.0.1:7069 state jump to NORMAL
DEBUG [main] 2021-04-15 08:22:49,849 StorageService.java:1619 - NORMAL
{code}"
CASSANDRA-16588,NPE getting host_id in Gossiper.isSafeForStartup,"As seen here: https://ci-cassandra.apache.org/job/Cassandra-devbranch/604/testReport/junit/org.apache.cassandra.distributed.upgrade/MixedModeGossipTest/testStatusFieldShouldExistInOldVersionNodesEdgeCase/

{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.isSafeForStartup(Gossiper.java:952)
	at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:657)
	at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:933)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:784)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:729)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$10(Instance.java:541)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

I believe what is happening is a GossipDigestAck has been queued to ack the shutdown state from the node on the seed, but isn't actually sent until the node has restarted and gone into shadow.  Since the ack contains the node's IP, it assumes a host_id will be there but since this is not an actual shadow response, it is not."
CASSANDRA-16585,Periodic failures in *RepairCoordinator*Test caused by race condition with nodetool repair,"Periodic failures in *RepairCoordinator*Test cause errors such as

FullRepairCoordinatorNeighbourDownTest#validationParticipentCrashesAndComesBack[DATACENTER_AWARE/true] 

{code}
nodetool command [repair, distributed_test_keyspace, validationparticipentcrashesandcomesback_full_datacenter_aware_true, --dc-parallel, --full] Error message 'Some repair failed' does not contain any of [/127.0.0.2:7012 died]
stdout:
[2021-04-07 22:45:24,887] Starting repair command #10 (f129cb60-97f2-11eb-9316-794aa6ab8411), repairing keyspace distributed_test_keyspace with repair options (parallelism: dc_parallel, primary range: false, incremental: false, job threads: 1, ColumnFamilies: [validationparticipentcrashesandcomesback_full_datacenter_aware_true], dataCenters: [], hosts: [], previewKind: NONE, # of ranges: 2, pull repair: false, force repair: false, optimise streams: false, ignore unreplicated keyspaces: false)
[2021-04-07 22:45:32,864] Repair command #10 failed with error Repair session f1342ba0-97f2-11eb-9316-794aa6ab8411 for range [(-1,9223372036854775805], (9223372036854775805,-1]] failed with error Endpoint /127.0.0.2:7012 died
[2021-04-07 22:45:32,887] After waiting for poll interval of 1 seconds queried for parent session status and discovered repair failed.
[2021-04-07 22:45:32,887] Repair command #10 finished with error
[2021-04-07 22:45:32,887] Some repair failed
[2021-04-07 22:45:32,888] Repair command #10 finished with error

stderr:
error: Some repair failed
-- StackTrace --
java.io.IOException: Some repair failed
at org.apache.cassandra.tools.RepairRunner.queryForCompletedRepair(RepairRunner.java:167)
at org.apache.cassandra.tools.RepairRunner.run(RepairRunner.java:72)
at org.apache.cassandra.tools.NodeProbe.repairAsync(NodeProbe.java:431)
at org.apache.cassandra.tools.nodetool.Repair.execute(Repair.java:171)
at org.apache.cassandra.tools.NodeTool$NodeToolCmd.runInternal(NodeTool.java:358)
at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:343)
at org.apache.cassandra.tools.NodeTool.execute(NodeTool.java:246)
at org.apache.cassandra.distributed.impl.Instance$DTestNodeTool.execute(Instance.java:836)
at org.apache.cassandra.distributed.impl.Instance.lambda$nodetoolResult$38(Instance.java:746)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:834)


Notifications:
Notification{type=START, src=repair:10, message=Starting repair command #10 (f129cb60-97f2-11eb-9316-794aa6ab8411), repairing keyspace distributed_test_keyspace with repair options (parallelism: dc_parallel, primary range: false, incremental: false, job threads: 1, ColumnFamilies: [validationparticipentcrashesandcomesback_full_datacenter_aware_true], dataCenters: [], hosts: [], previewKind: NONE, # of ranges: 2, pull repair: false, force repair: false, optimise streams: false, ignore unreplicated keyspaces: false)}
Notification{type=ERROR, src=repair:10, message=Repair command #10 failed with error Repair session f1342ba0-97f2-11eb-9316-794aa6ab8411 for range [(-1,9223372036854775805], (9223372036854775805,-1]] failed with error Endpoint /127.0.0.2:7012 died}
Notification{type=COMPLETE, src=repair:10, message=Repair command #10 finished with error}
Error:
java.io.IOException: Some repair failed
at org.apache.cassandra.tools.RepairRunner.queryForCompletedRepair(RepairRunner.java:167)
at org.apache.cassandra.tools.RepairRunner.run(RepairRunner.java:72)
at org.apache.cassandra.tools.NodeProbe.repairAsync(NodeProbe.java:431)
at org.apache.cassandra.tools.nodetool.Repair.execute(Repair.java:171)
at org.apache.cassandra.tools.NodeTool$NodeToolCmd.runInternal(NodeTool.java:358)
at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:343)
at org.apache.cassandra.tools.NodeTool.execute(NodeTool.java:246)
at org.apache.cassandra.distributed.impl.Instance$DTestNodeTool.execute(Instance.java:836)
at org.apache.cassandra.distributed.impl.Instance.lambda$nodetoolResult$38(Instance.java:746)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:834)
{code}

Seems there is a race condition in nodetool repair where we query the error state before we get the notification, then we throw a generic error rather than the specific error."
CASSANDRA-16576,jflex NPE (StateSetEnumerator.reset(…)) on arm64 and jdk11,"NPE thrown from `ant generate-jflex-java` on jdk11 and arm64.

Upstream: https://github.com/jflex-de/jflex/issues/910 
{code}
 java.lang.NullPointerException
 	at jflex.StateSetEnumerator.reset(StateSetEnumerator.java:40)
 	at jflex.NFA.containsFinal(NFA.java:295)
 	at jflex.NFA.getDFA(NFA.java:539)
 	at jflex.Main.generate(Main.java:80)
 	at jflex.anttask.JFlexTask.execute(JFlexTask.java:65)
 	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:292)
 	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
 	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:99)
 	at org.apache.tools.ant.Task.perform(Task.java:350)
 	at org.apache.tools.ant.Target.execute(Target.java:449)
 	at org.apache.tools.ant.Target.performTasks(Target.java:470)
 	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1388)
 	at org.apache.tools.ant.Project.executeTarget(Project.java:1361)
 	at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
 	at org.apache.tools.ant.Project.executeTargets(Project.java:1251)
 	at org.apache.tools.ant.Main.runBuild(Main.java:834)
 	at org.apache.tools.ant.Main.startAnt(Main.java:223)
 	at org.apache.tools.ant.launch.Launcher.run(Launcher.java:284)
 	at org.apache.tools.ant.launch.Launcher.main(Launcher.java:101)
{code}

Failures
- https://ci-cassandra.apache.org/view/Cassandra%204.0/job/Cassandra-trunk-artifacts/jdk=jdk_11_latest,label=cassandra-arm64/505/consoleFull
- https://ci-cassandra.apache.org/view/Cassandra%204.0/job/Cassandra-trunk-artifacts/501/jdk=jdk_11_latest,label=cassandra-arm64/console
- https://ci-cassandra.apache.org/view/Cassandra%204.0/job/Cassandra-trunk-artifacts/497/jdk=jdk_11_latest,label=cassandra-arm64/console"
CASSANDRA-16554,Race between secondary index building and active compactions tracking,"There is a race condition between the secondary index build compaction task and the active compactions tracking, especially when incremental repair is running. 
It could result into 2 exceptions. 


{code:java}
Caused by: java.util.NoSuchElementException
	at org.apache.cassandra.utils.AbstractIterator.next(AbstractIterator.java:64)
	at org.apache.cassandra.io.sstable.ReducingKeyIterator.next(ReducingKeyIterator.java:117)
	at org.apache.cassandra.index.internal.CollatedViewIndexBuilder.build(CollatedViewIndexBuilder.java:74)
	at org.apache.cassandra.db.compaction.CompactionManager$14.run(CompactionManager.java:1688)
{code}


{code:java}
Caused by: java.io.EOFException
	at org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:180)
	at org.apache.cassandra.utils.vint.VIntCoding.readUnsignedVInt(VIntCoding.java:68)
	at org.apache.cassandra.io.util.RebufferingInputStream.readUnsignedVInt(RebufferingInputStream.java:243)
	at org.apache.cassandra.db.RowIndexEntry$Serializer.readPosition(RowIndexEntry.java:364)
	at org.apache.cassandra.db.RowIndexEntry$Serializer.skip(RowIndexEntry.java:369)
	at org.apache.cassandra.io.sstable.KeyIterator.computeNext(KeyIterator.java:110)
{code}

In the first exception, the iterator returns true for the call of `hasNext`, but the following call of `next` throws. 
In the second exception, the file wrapper object returns false for the call of `isEOF`, but the following call that reads from it throws EOFException. 
The exceptions can be constantly reproduced by the test in the patch. 

The root cause of the exception is from the thread-unsafe lazy initialization found in `ReducingKeyIterator` and `KeyIterator`. When the `maybeInit` methods of both classes are called from multiple threads, it is likely to instantiate 2 instances and mess up the internal state. Those iterators might not be considered being used in a multiple thread environment when being added to the codebase initially. However, the `CollatedViewIndexBuilder` contains the reference to those 2 iterator, and it, as a `CompactionInfo.Holder`, is added to active compactions to be accessed from other threads, e.g. by calling `ActiveCompactions#getCompactionsForSSTable`. The implementation of `getCompactionInfo` in `CollatedViewIndexBuilder` publishes the reference to the `ReducingKeyIterator` and transitively `KeyIterator` to the other threads. Therefore, the other threads can possibly race. 

For the instance of NSEE, thread 1 calls the `hasNext` on the `ReducingKeyIterator`, it initialize the internal merge iterator. The call returns true. Right after, there is a thread 2 that calls `ActiveCompactions#getCompactionsForSSTable` and it sees the instance of `ReducingKeyIterator` is not initialized yet, so run `maybeInit` again. The reference of `iter` is replaced with the second instance. Now, thread 1 calls `next` using the new instance that does not has the correct state (expecting HAS_NEXT). So it again calls `hasNext`, which fetches the next element from the merged iterator. Those 2 `ReducingKeyIterator` share the same instances of `KeyIterator`. If the key iterators are already at the end, calling `hasNext` returns false and we get the NSEE. 

Besides the explicit NSEE exception, with the above reasoning, it is also possible to have the unexpected behavior that skips one item from the merge iterator. It leads to no 2i is built for that missing partition. Such case is totally hidden since no exception is ever thrown. 

To fix the unexpected behaviors, we need to correctly lazy initialize the iterators. 

Looking at the implementations of `CompactionInfo.Holder`, `ScrubInfo` and `VerifyInfo` also publishes the non-thread-safe `RandomAccessReader` when creating the compaction info object. According to the code inspection, there is a rare chance that a thread is calling `getFilePointer` from the file object and another thread closes the file, which can produce a NPE. When closing the file, the internal states, e.g. buffer is set to null. I did not add the fix in this patch, as it is never spotted. "
CASSANDRA-16552,"Anticompaction appears to race with Compaction, preventing forward compaction progress after an incremental repair","While testing 4.0-rc1 on a 12 i3en.2xlarge x 2 region (AWS us-east-1 and eu-west-1) cluster I attempted to run {{nodetool repair}} while the cluster was taking moderate read/write load. 

The first time it worked as expected, but when I ran an incremental run the second time multiple nodes got stuck trying to compact the unrepaired sstables. They are now spinning with:
{noformat}
$ nt compactionstats
pending tasks: 827
- acceptance_josephl.acceptance_josephl_cass4: 827

$ nt tpstats            
Pool Name                    Active Pending Completed Blocked All time blocked
RequestResponseStage         0      0       422359133 0       0               
MutationStage                0      0       164540628 0       0               
ReadStage                    0      0       198857844 0       0               
CompactionExecutor           0      0       60782     0       0    


$ tail system.log
DEBUG [CompactionExecutor:684] 2021-03-31 15:13:59,902 LeveledManifest.java:292 - L0 is too far behind, performing size-tiering there first
DEBUG [CompactionExecutor:684] 2021-03-31 15:13:59,908 LeveledManifest.java:292 - L0 is too far behind, performing size-tiering there first
WARN  [CompactionExecutor:684] 2021-03-31 15:13:59,912 LeveledCompactionStrategy.java:154 - Could not acquire references for compacting SSTables [BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11eba
fb40b81cbd6fb3d/na-4826-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4872-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acc
eptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4849-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4874-big-Data.db'), BigTableReader(path='/mnt/dat
a/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4841-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4897-big-D
ata.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4924-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e79
0917c11ebafb40b81cbd6fb3d/na-4837-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4926-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_j
osephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4729-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4723-big-Data.db'), BigTableReader(path
='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4875-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-
4922-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4920-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cas
s4-6144e790917c11ebafb40b81cbd6fb3d/na-4869-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4823-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/ac
ceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4846-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4873-big-Data.db'), BigTableR
eader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4840-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cb
d6fb3d/na-4833-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4829-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_j
osephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4726-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4923-big-Data.db'), BigTableReader(path='/mnt/data/cassand
ra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4925-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4905-big-Data.db'),
BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4876-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11eb
afb40b81cbd6fb3d/na-4901-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4732-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/ac
ceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4909-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4915-big-Data.db'), BigTableReader(path='/mnt/da
ta/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4921-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4860-big-
Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4693-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e7
90917c11ebafb40b81cbd6fb3d/na-4694-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4692-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_
josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4691-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4696-big-Data.db'), BigTableReader(pat
h='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4697-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na
-4700-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4698-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_ca
ss4-6144e790917c11ebafb40b81cbd6fb3d/na-4688-big-Data.db'), BigTableReader(path='/mnt/data/cassandra/data/acceptance_josephl/acceptance_josephl_cass4-6144e790917c11ebafb40b81cbd6fb3d/na-4689-big-Data.db')] which is not a problem per se,unless it happens
frequently, in which case it must be reported. Will retry later.
{noformat}

I've attached some starting breadcrumbs. I believe the issue is a potential race in [marking sstables for compaction|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L152-L161] getting null back from [tryModify|https://github.com/apache/cassandra/blob/d42087a63309178b96909c012dd0073fe0b6ea11/src/java/org/apache/cassandra/db/lifecycle/Tracker.java#L100] which I think can only happen under a [small number of circumstances|https://github.com/apache/cassandra/blob/d42087a63309178b96909c012dd0073fe0b6ea11/src/java/org/apache/cassandra/db/lifecycle/View.java#L269]. From the initial investigation it does appear that only the unrepaired products get into this state.

I have a heap dump containing the View state but it contains potentially sensitive infrastructure details so if you're debugging just message me in slack and I can send it to you directly.

The following mitigation appears to unstick the nodes via a forced full compaction:
{noformat}
nodetool stop COMPACTION; nodetool compact <ks> <table>
{noformat}
I'm not confident in this mitigation though.
"
CASSANDRA-16546,Flaky testRecoverOverflowedExpirationWithSSTableScrub,"See [here|https://ci-cassandra.apache.org/job/Cassandra-trunk-test/627/jdk=jdk_11_latest,label=cassandra,split=4/testReport/junit/org.apache.cassandra.cql3.validation.operations/TTLTest/testRecoverOverflowedExpirationWithSSTableScrub/]"
CASSANDRA-16523,Improve handling of unflushed hint files,In some situations hint files can have a run of trailing zeros at the end - for example when hard-rebooting machines. These zeros can be safely ignored and we should avoid invoking the disk failure policy in these cases.
CASSANDRA-16477,Fix centos packaging for arm64,The cryptography python library (needed by urllib3) does not install on arm64 using centos 7 and python2.
CASSANDRA-16471,DataOutputBuffer#scratchBuffer can use off-heap or on-heap memory as a means to control memory allocations,"While running workflows to compare 3.0 with trunk we found that allocations and GC are significantly higher for a write mostly workload (22% read, 3% delete, 75% write); below is what we saw for a 2h run

Allocations
30: 1.64TB
40: 2.99TB

GC Events
30: 7.39k events
40: 13.93k events

When looking at the allocation output we saw the follow for memory allocations

!https://issues.apache.org/jira/secure/attachment/13021238/Screen%20Shot%202021-02-25%20at%203.34.28%20PM.png!

Here we see that org.apache.cassandra.io.util.DataOutputBuffer#expandToFit is around 52% of the memory allocations.  When looking at this logic I see that allocations are on-heap and constantly throw away the buffer (as a means to allow GC to clean up).

With the patch, allocations/gc are the following

Allocations
30: 1.64TB
40 w/ patch: 1.77TB
40: 2.99TB

GC Events
30: 7.39k events
40 w/ patch: 8k events
40: 13.93k events


With the patch only 0.8% allocations

!https://issues.apache.org/jira/secure/attachment/13021239/Screen%20Shot%202021-02-25%20at%204.14.19%20PM.png!"
CASSANDRA-16453,NPE in Slice#make on RT + partition deletion reconciliation on timestamp tie,"There’s an NPE in Slice#make on RT + partition deletion reconciliation.

Minimal repro:
{code:java}
    try (Cluster cluster = init(builder().withNodes(3).start()))
        {
            cluster.schemaChange(withKeyspace(""CREATE TABLE distributed_test_keyspace.table_0 (pk0 bigint,ck0 bigint,regular0 bigint,regular1 bigint,regular2 bigint, PRIMARY KEY (pk0, ck0)) WITH  CLUSTERING ORDER BY (ck0 ASC);""));
            long pk = 0L;
            cluster.coordinator(1).execute(""DELETE FROM distributed_test_keyspace.table_0 USING TIMESTAMP 100230 WHERE pk0=? AND ck0>?;"", ConsistencyLevel.ALL, pk, 2L);
            cluster.get(3).executeInternal(""DELETE FROM distributed_test_keyspace.table_0 USING TIMESTAMP 100230 WHERE pk0=?;"", pk);
            cluster.coordinator(1).execute(""SELECT * FROM distributed_test_keyspace.table_0 WHERE pk0=? AND ck0>=? AND ck0<?;"",
                                                     ConsistencyLevel.ALL, pk, 1L, 3L);
        }
{code}
Details:
{code:java}
java.lang.AssertionError: Error merging RTs on distributed_test_keyspace.table_0: merged=null, versions=[Marker EXCL_START_BOUND(2)@100230/1613500432, Marker EXCL_START_BOUND(2)@100230/1613500432, null], sources={[Full(/127.0.0.1:7012,(-3074457345618258603,3074457345618258601]), Full(/127.0.0.2:7012,(-3074457345618258603,3074457345618258601]), Full(/127.0.0.3:7012,(-3074457345618258603,3074457345618258601])]}, debug info:
 /127.0.0.1:7012 => [distributed_test_keyspace.table_0] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | [regular0 regular1 regular2]] repaired_digest= repaired_digest_conclusive==true
    Marker EXCL_START_BOUND(2)@100230/1613500432
    Marker EXCL_END_BOUND(3)@100230/1613500432,
/127.0.0.2:7012 => [distributed_test_keyspace.table_0] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | [regular0 regular1 regular2]] repaired_digest= repaired_digest_conclusive==true
    Marker EXCL_START_BOUND(2)@100230/1613500432
    Marker EXCL_END_BOUND(3)@100230/1613500432,
/127.0.0.3:7012 => [distributed_test_keyspace.table_0] key=0 partition_deletion=deletedAt=100230, localDeletion=1613500432 columns=[[] | [regular0 regular1 regular2]] repaired_digest= repaired_digest_conclusive==true
{code}
Exception:
{code:java}
java.lang.NullPointerException
	at org.apache.cassandra.db.Slice.make(Slice.java:74)
	at org.apache.cassandra.service.reads.repair.RowIteratorMergeListener.closeOpenMarker(RowIteratorMergeListener.java:351)
	at org.apache.cassandra.service.reads.repair.RowIteratorMergeListener.onMergedRangeTombstoneMarkers(RowIteratorMergeListener.java:315)
	at org.apache.cassandra.service.reads.DataResolver$2$1.onMergedRangeTombstoneMarkers(DataResolver.java:378)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer.getReduced(UnfilteredRowIterators.java:592)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer.getReduced(UnfilteredRowIterators.java:541)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:219)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:523)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:391)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
	at org.apache.cassandra.db.transform.FilteredRows.isEmpty(FilteredRows.java:50)
	at org.apache.cassandra.db.transform.EmptyPartitionsDiscarder.applyToPartition(EmptyPartitionsDiscarder.java:27)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:97)
	at org.apache.cassandra.service.StorageProxy$6.hasNext(StorageProxy.java:1908)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:93)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:777)
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:425)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:402)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:250)
	at org.apache.cassandra.distributed.impl.Coordinator.lambda$executeWithPagingWithResult$2(Coordinator.java:162)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{code}

This behaviour is new to 4.0 and was introduced in [CASSANDRA-15369]. The difference with 3.0 is that in 3.0 {{RangeTombstoneMarker merged}} would be {{null}}, so we would never be hitting the code path where some of the sources is opening/closing marker, and instead will fall through to opening/closing of deletions below. I've checked code in 15369 and it looks like this condition is a new edge case in otherwise correct code. Since the goal was to avoid generating range tombstones on ties with partition deletion, fix for this issue is also consistent with that goal. 

In other words, on 3.0 given
{code}
cluster.coordinator(1).execute(""INSERT INTO distributed_test_keyspace.tbl0 (pk, ck, value) VALUES (?,?,?) USING TIMESTAMP 1"", ConsistencyLevel.ALL, pk, 1L, 1L, 1L);
cluster.coordinator(1).execute(""DELETE FROM distributed_test_keyspace.tbl0 USING TIMESTAMP 2 WHERE pk=? AND ck>?;"", ConsistencyLevel.ALL, pk, 2L);            
{code}

We would RR:
{code}
Mutation(keyspace='distributed_test_keyspace', key='0000000000000000', modifications=[
  [distributed_test_keyspace.table_0] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | [regular0 regular1 regular2]]
    Marker EXCL_START_BOUND(2)@100230/1615295010
    Marker EXCL_END_BOUND(3)@100230/1615295010
])
mutation = Mutation(keyspace='distributed_test_keyspace', key='0000000000000000', modifications=[
  [distributed_test_keyspace.table_0] key=0 partition_deletion=deletedAt=100230, localDeletion=1615295010 columns=[[] | [regular0 regular1 regular2]]
    Marker EXCL_START_BOUND(2)@100230/1615295010
    Marker EXCL_END_BOUND(3)@100230/1615295010
])
Mutation(keyspace='distributed_test_keyspace', key='0000000000000000', modifications=[
  [distributed_test_keyspace.table_0] key=0 partition_deletion=deletedAt=100230, localDeletion=1615295010 columns=[[] | [regular0 regular1 regular2]]
])
{code}

And on 4.0: 
{code}
Mutation(keyspace='distributed_test_keyspace', key='0000000000000000', modifications=[
  [distributed_test_keyspace.tbl0] key=0 partition_deletion=deletedAt=2, localDeletion=1615295072 columns=[[] | [value]]
])
Mutation(keyspace='distributed_test_keyspace', key='0000000000000000', modifications=[
  [distributed_test_keyspace.tbl0] key=0 partition_deletion=deletedAt=2, localDeletion=1615295072 columns=[[] | [value]]
])
{code}"
CASSANDRA-16446,Parent repair sessions leak may lead to node long pauses,"{{ActiveRepairService}} keeps  a map `parentRepairSessions`. If these sessions leak, that map can grow to a size when a node restarts {{ActiveRepairService.onRestart()}} triggers a cleanup of sessions that can pause nodes in a cluster for a long time.

The proposed solution is for repairs to cleanup these sessions on all nodes on completion by sending a CLEANUP message to involved nodes. Tests rely on a new {{parentRepairSessionsCount()}} method on the parent repair sessions MBean to keep track of these."
CASSANDRA-16437,fault occurred in a recent unsafe memory access operation,"Hi, testing Cassandra 4~beta4 and JDK 11, all nodes crashed with error of :
{code}
ERROR [MutationStage-55] 2021-02-10 01:07:15,015 AbstractLocalAwareExecutorService.java:166 - Uncaught exception on thread Thread[MutationStage-55,5,main]
java.lang.RuntimeException: java.lang.InternalError: a fault occurred in a recent unsafe memory access operation
        at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:104)
        at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:44)
        at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:432)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InternalError: a fault occurred in a recent unsafe memory access operation
{code}

Using Azul Zing JDK:
zing-zst-8c.dkms.2.6.32-5.22.8.0.4.el.x86_64
zing-jdk11.0.0-20.08.101.0-1.x86_64

OS: CentOS 7, kernel 5.0.1 

The crash accrued after some stress tests on cluster.

Thank you, Yakir Gibraltar"
CASSANDRA-16435,Reverse iteration with paging may throw if the page boundary coincides with open tombstone boundary,"If sstable contains a tombstone that has an open bound which coincides with current page bound, we’ll generate an impossible empty slice range. 
 
Minimal repro:
 
{code}
        try (Cluster cluster = init(builder().withNodes(3).start()))
        {
            cluster.schemaChange(""CREATE TABLE "" + KEYSPACE + "".tbl (pk int, ck int, regular int, PRIMARY KEY (pk, ck))"");
            cluster.coordinator(1).execute(""DELETE FROM "" + KEYSPACE + "".tbl WHERE pk = 1 AND ck > 1 AND ck < 10"", ConsistencyLevel.ALL);
            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tbl (pk, ck, regular) values (1,1,1)"", ConsistencyLevel.ALL);
            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tbl (pk, ck, regular) values (1,2,2)"", ConsistencyLevel.ALL);
            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tbl (pk, ck, regular) values (1,3,3)"", ConsistencyLevel.ALL);
            cluster.stream().forEach(n -> {
                n.nodetool(""flush"");
            });
            Iterator<Object[]> iter = cluster.coordinator(1).executeWithPaging(""SELECT pk,ck,regular FROM "" + KEYSPACE + "".tbl "" +
                                                                               ""WHERE pk=? AND ck>=? ORDER BY ck DESC;"",
                                                                               ConsistencyLevel.QUORUM, 1,
                                                                               1,1);
            while (iter.hasNext())
            {
                System.out.println(Arrays.toString(iter.next()));
            }
        }
 {code}
Stack trace: 
 
{code}
Caused by: java.lang.IllegalArgumentException: [1, 1)
at com.google.common.base.Preconditions.checkArgument(Preconditions.java:141)
at org.apache.cassandra.db.Slices.with(Slices.java:66)
at org.apache.cassandra.db.columniterator.SSTableReversedIterator$ReverseReader.setIterator(SSTableReversedIterator.java:140)
at org.apache.cassandra.db.columniterator.SSTableReversedIterator$ReverseReader.setForSlice(SSTableReversedIterator.java:134)
at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:118)
at org.apache.cassandra.db.columniterator.SSTableReversedIterator.<init>(SSTableReversedIterator.java:52)
at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:75)
at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:67)
at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.initializeIterator(UnfilteredRowIteratorWithLowerBound.java:100)
at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48)
at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.partitionLevelDeletion(LazilyInitializedUnfilteredRowIterator.java:81)
at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.partitionLevelDeletion(UnfilteredRowIteratorWithLowerBound.java:161)
at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:672)
at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:566)
at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:400)
at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:460)
at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2011)
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2725)
... 5 common frames omitted
{code}"
CASSANDRA-16430,Cassandra JVM Memory Pools -Par Survivor Space UsedPoolMemory,"I have a 9 node cluster of cassandra, with 2gb of data on each node in our production environment. We are continuously facing below warnings from one of our monitoring tool, but there is no suspicious error from system.log file. Also there is sufficient memory available for Heap/server.

 
 warn - Cassandra JVM Memory Pools -Par Survivor Space UsedPoolMemory
 Datasource: Cassandra JVM Memory Pools -Par Survivor Space
 Datapoint: UsedPoolMemory
 Level: warn
 Start: 2021-02-08 10:11:17 GMT
 Duration: 1h 0m
 Value: 100.0
 ClearValue: 1.3631
 Reason: UsedPoolMemory is not >= 90: the current value is 1.3631

The java memory pool Par Survivor Space is now using 100.0 % of the maximum.

 We see the below message in debug.log file.

2021-02-08 08:43:51,310 INFO  [ReadStage-1] NoSpamLogger.java:91 - Maximum memory usage reached (512.000MiB), cannot allocate chunk of 1.000MiB

2021-02-08 08:47:28,344 DEBUG [COMMIT-LOG-ALLOCATOR] AbstractCommitLogSegmentManager.java:109 - No segments in reserve; creating a fresh one

 Please help us to fix this issue ASAP.

 Thanks!"
CASSANDRA-16427,In-JVM dtest paging does not handle Group By correctly,"In-JVM dtest paging is using a pager that disregards the type of the executed query, resulting into `GROUP BY` queries being executed like normal SELECT queries without GROUP BY clause."
CASSANDRA-16412,nodetool status doesn't work without system_traces,"bq. Error: The node does not have system_traces yet, probably still bootstrapping

There's no reason we can't show status because of this, we can simply log that the effective ownership is unknown, like we used to, rather than forcing a user to go to another machine to find out what's going on in the ring.
"
CASSANDRA-16396,Update debian packaging for python3,Currently we require 'python >= 2.7' but we need to allow the 'python3' package in order to avoid python 2.
CASSANDRA-16394,Fix schema aggreement race conditions in in-JVM dtests ,"There there are two race conditions in in-JVM dtest schema agreement, which are causing test failures:

1. First is caused by the fact we’re starting waiting for schema propagation already after the schema agreement was reached (which was resulting into us endlessly waiting for an agreement that has already been established);
 2. The other one was because the callback to notify about successful agreement can be triggered already after the other node has notified about it, and control flow might have moved cluster to a different configuration.

Example of exception:
{code:java}
Caused by: java.lang.IllegalStateException: Schema agreement not reached
	at org.apache.cassandra.distributed.impl.AbstractCluster$ChangeMonitor.waitForCompletion(AbstractCluster.java:?)
	at org.apache.cassandra.distributed.impl.AbstractCluster.lambda$schemaChange$5(AbstractCluster.java:?)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:?)
	at java.util.concurrent.FutureTask.run(FutureTask.java:?)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:?)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:?)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:?)
	at java.lang.Thread.run(Thread.java:?)
{code}"
CASSANDRA-16330,Fix failing upgrade dtest test_basic_paging - upgrade_tests.paging_test.TestPagingDataNodes,"test_basic_paging - upgrade_tests.paging_test.TestPagingDataNodes
upgrade_tests/paging_test.py

https://app.circleci.com/pipelines/github/dcapwell/cassandra/843/workflows/9545f259-0a61-4ba8-8dea-485a33136032/jobs/4964

{code}
        #4.0 doesn't support compact storage
>       if version_string == 'trunk' or version_string >= MAJOR_VERSION_4:

upgrade_tests/paging_test.py:476: 
{code}

{code}
self = LooseVersion ('4.0')
other = LooseVersion ('clone:/home/cassandra/cassandra')

    def _cmp (self, other):
        if isinstance(other, str):
            other = LooseVersion(other)
    
        if self.version == other.version:
            return 0
>       if self.version < other.version:
E       TypeError: '<' not supported between instances of 'int' and 'str'
{code}"
CASSANDRA-16307,GROUP BY queries with paging can return deleted data,"{{GROUP BY}} queries using paging and CL>ONE/LOCAL_ONE. This dtest reproduces the problem:
{code:java}
try (Cluster cluster = init(Cluster.create(2)))
{
    cluster.schemaChange(withKeyspace(""CREATE TABLE %s.t (pk int, ck int, PRIMARY KEY (pk, ck))""));
    ICoordinator coordinator = cluster.coordinator(1);
    coordinator.execute(withKeyspace(""INSERT INTO %s.t (pk, ck) VALUES (0, 0)""), ConsistencyLevel.ALL);
    coordinator.execute(withKeyspace(""INSERT INTO %s.t (pk, ck) VALUES (1, 1)""), ConsistencyLevel.ALL);
    
    cluster.get(1).executeInternal(withKeyspace(""DELETE FROM %s.t WHERE pk=0 AND ck=0""));
    cluster.get(2).executeInternal(withKeyspace(""DELETE FROM %s.t WHERE pk=1 AND ck=1""));
    String query = withKeyspace(""SELECT * FROM %s.t GROUP BY pk"");
    Iterator<Object[]> rows = coordinator.executeWithPaging(query, ConsistencyLevel.ALL, 1);
    assertRows(Iterators.toArray(rows, Object[].class));
}
{code}
Using a 2-node cluster and RF=2, the test inserts two partitions in both nodes. Then it locally deletes each row in a separate node, so each node sees a different partition alive, but reconciliation should produce no alive partitions. However, a {{GROUP BY}} query using a page size of 1 wrongly returns one of the rows.

This has been detected during CASSANDRA-16180, and it is probably related to CASSANDRA-15459, which solved a similar problem for group-by queries with limit, instead of paging."
CASSANDRA-16299,org.apache.cassandra.streaming.LongStreamingTest fail with NPE,"{code}
java.lang.NullPointerException
	at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.awaitTermination(AbstractCommitLogSegmentManager.java:509)
	at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.stopUnsafe(AbstractCommitLogSegmentManager.java:427)
	at org.apache.cassandra.db.commitlog.CommitLog.stopUnsafe(CommitLog.java:467)
	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:731)
	at org.apache.cassandra.streaming.LongStreamingTest.setup(LongStreamingTest.java:56)
{code}"
CASSANDRA-16294,Potential NPE in JVMStabilityInspector,"On either a FileNotFoundException or SocketException, JVMStabilityInspector checks the error message for the string ""Too many open files"". However, both of these exceptions have a constructor which sets a null message, which can lead to NPE if handled."
CASSANDRA-16276,Drain and/or shutdown might throw because of slow messaging service shutdown,"If we invoke nodetool drain before shutdown, it sometimes fails to shut down messaging service in time (in this case - timing out the shutdown of the eventloopgroup by Netty). But, not before we manage to set isShutdown of StorageService to true, despite aborting further drain logic (including shutting down mutation stages).

Then, via on shutdown hook, we invoke drain() method again, implicitly. We see that the mutation stage is not shutdown and proceed to assert that isShutdown == false, failing that assertion and triggering a second error log message.

The patch merely ensures that any exception thrown by MS shutdown is captured so that drain logic can complete the first time around."
CASSANDRA-16266,Stress testing a mixed cluster with C* 2.1.0 (seed) and 2.0.0 causes NPE,"Steps to reproduce: 
 # setup a mixed cluster with C* 2.1.0 (seed node) and C* 2.0.0
 # run the stress testing tool, e.g.,

{code:java}
/cassandra/tools/bin/cassandra-stress write n=1000 -rate threads=50 -node 250.16.238.1,250.16.238.2{code}
NPE: 
{code:java}
ERROR [InternalResponseStage:2] 2020-07-22 08:29:36,170 CassandraDaemon.java (line 186) Exception in thread Thread[InternalResponseStage:2,5,main]
java.lang.NullPointerException
  at org.apache.cassandra.serializers.BooleanSerializer.deserialize(BooleanSerializer.java:33)
  at org.apache.cassandra.serializers.BooleanSerializer.deserialize(BooleanSerializer.java:24)
  at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
  at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:106)
  at org.apache.cassandra.config.CFMetaData.fromSchemaNoColumnsNoTriggers(CFMetaData.java:1555)
  at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1642)
  at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:305)
  at org.apache.cassandra.db.DefsTables.mergeColumnFamilies(DefsTables.java:270)
  at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:183)
  at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:66)
  at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:46)
  at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
{code}
Root cause: incompatible data

In the `CFMetaData` class of version 2.0.0, there is a boolean field named `replicate_on_write`. In the same class of version 2.1.0, however, this field no longer exists. When serializing this class in function `toSchemaNoColumnsNoTriggers`, it will first write all of its fields into a `RowMutation` (in 2.0.0) / `Mutation` (in 2.1.0) class, and then serialize this “Mutation” like class in the same way. In 2.0.0 the `replicate_on_write` field gets serialized at [https://github.com/apache/cassandra/blob/03045ca22b11b0e5fc85c4fabd83ce6121b5709b/src/java/org/apache/cassandra/config/CFMetaData.java#L1514] .

When deserializing this class in function `fromSchemaNoColumnsNoTriggers`, it reads all its fields from a map-like class `UntypedResultSet.Row`. In 2.0.0 the `replicate_on_write` field gets deserialized at [https://github.com/apache/cassandra/blob/03045ca22b11b0e5fc85c4fabd83ce6121b5709b/src/java/org/apache/cassandra/config/CFMetaData.java#L1555] .

The problem is that the existence of the key is not checked, and the map returns a `null` value because the message from 2.1.0 doesn’t contain the `replicate_on_write` key, which leads to the NullPointerException.

 "
CASSANDRA-16261,Prevent unbounded number of flushing tasks,"The cleaner thread is not prevented from queueing an unbounded number of flushing tasks for memtables that are almost empty.

This patch adds a mechanism to track the number of pending flushing
tasks in the memtable cleaner. Above the maximum number (2x the flushing
threads by default), only memtables using at least MCT memory will be
flushed, where MCT stands for Memory Cleanup Threshold.

This patch also fixes a possible problem tracking the memory marked as
""reclaiming"" in the memtable allocators and pool. Writes that complete
only after a memtable has been scheduled for flushing, did not report
their memory as reclaiming. Normally this should be a small value of no
consequence, but if the flushing tasks are blocked for a long period,
and there is a sufficient number of writes, or these writes use
a sufficiently large quantity of memory, this would cause the memtable
cleaning algorithm to schedule repeated flushing tasks because the used
memory is always > reclaiming memory + MCT."
CASSANDRA-16242,LEAK DETECTED:  was not released before the reference was garbage collected,"Hello,

I am getting the following errors and I am running 3.11.5
{code:java}
LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@356cb07f) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1709177197:Memory@[7e7a3b64b910..7e7a3b96b910) was not released before the reference was garbage collected


ERROR [CompactionExecutor:3763] 2020-11-02 02:34:53,316  CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:3763,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Map failed
        at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:157) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions$State.add(MmappedRegions.java:310) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions$State.access$400(MmappedRegions.java:246) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:170) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:362) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:290) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:180) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:135) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:65) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:142) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_60]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_60]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_60]
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:939) ~[na:1.8.0_60]
        at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:153) ~[apache-cassandra-3.11.5.jar:3.11.5]
        ... 23 common frames omitted
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method) ~[na:1.8.0_60]
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:936) ~[na:1.8.0_60]
        ... 24 common frames omitted
WARN  [GossipTasks:1] 2020-11-02 02:34:53,302  FailureDetector.java:278 - Not marking nodes down due to local pause of 7468254535 > 5000000000
ERROR [Reference-Reaper] 2020-11-02 02:34:53,208  Ref.java:229 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@33fef449) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1098210945:/data/cassandra/data/elvisevallogks/decision_detail-7351f6205f3811ea922ce1efeeba3e49/md-3695-big-Index.db was not released before the reference was garbage collected
ERROR [CompactionExecutor:3762] 2020-11-02 02:34:53,208  CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:3762,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Map failed
        at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:157) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions$State.add(MmappedRegions.java:310) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions$State.access$400(MmappedRegions.java:246) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:181) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:362) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:290) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:180) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:135) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:65) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:142) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_60]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_60]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_60]
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:939) ~[na:1.8.0_60]
        at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:153) ~[apache-cassandra-3.11.5.jar:3.11.5]
        ... 23 common frames omitted
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method) ~[na:1.8.0_60]
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:936) ~[na:1.8.0_60]
        ... 24 common frames omitted
INFO  [Service Thread] 2020-11-02 02:34:53,877  StatusLogger.java:51 - MemtablePostFlush                 0         0           3596         0                 0 {code}
 

These are the settings
{code:java}
# /etc/security/limits-cassandra.conf
# Cassandra-specfic parameters
# add
cassandra        soft    memlock         unlimited
cassandra        hard    memlock         unlimited
cassandra        soft    nproc           65535
#change
*                soft    nofile          655350
*                hard    nofile          655350 {code}
For cassandra process
{code:java}
cat /proc/25586/limits
Limit                     Soft Limit           Hard Limit           Units
Max cpu time              unlimited            unlimited            seconds
Max file size             unlimited            unlimited            bytes
Max data size             unlimited            unlimited            bytes
Max stack size            8388608              unlimited            bytes
Max core file size        0                    0                    bytes
Max resident set          unlimited            unlimited            bytes
Max processes             65535                692334               processes
Max open files            655350               655350               files
Max locked memory         unlimited            unlimited            bytes
Max address space         unlimited            unlimited            bytes
Max file locks            unlimited            unlimited            locks
Max pending signals       692334               692334               signals
Max msgqueue size         819200               819200               bytes
Max nice priority         0                    0
Max realtime priority     0                    0
Max realtime timeout      unlimited            unlimited            us {code}

 {{vm.max_map_count is set to 1048575}}
{code:java}
sysctl vm.max_map_count
vm.max_map_count = 1048575 {code}
 "
CASSANDRA-16214,Improve Logging Downstream of DataOutputBuffer Overflows,"There are a number of codepaths that buffer intermediate representations of a partition in memory in a {{DataOutputBuffer}}. Compactions and reads (involving multiple SSTables) can both produce partitions whose serialized size is larger than {{MAX_ARRAY_SIZE}}, in which case a {{RuntimeException}} and really not much else makes it to the logs. We should be able to improve this so that at least the offending keyspace, table, and partition are logged."
CASSANDRA-16209,Log Warning Rather than Verbose Trace when Preview Repair Validation Conflicts with Incremental Repair,"When a preview repair on repaired data identifies which SSTables to validate, it might come across an SSTable that's still pending for an in-progress incremental repair session. It makes sense that we immediately fail the preview repair in that case, but the resulting error and verbose stack trace in the logs is a bit too severe a reaction. We should downgrade this to a simple warning message."
CASSANDRA-16207,NPE when calling broadcast address on unintialized node,"When trying to run upgrades, sometimes we’re calling broadcasts addrerss on an uninitialised new node:

{code}
java.lang.IllegalStateException: Can't use shut down instances, delegate is null
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegate(AbstractCluster.java:163)
	at org.apache.cassandra.distributed.impl.DelegatingInvokableInstance.broadcastAddress(DelegatingInvokableInstance.java:53) 
	at org.apache.cassandra.distributed.impl.Instance$2.allowIncomingMessage(Instance.java:278) 
	at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:1031) ~[dtest-3.0.19.jar:?]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:213) 
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:182) 
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93) 
{code}"
CASSANDRA-16195,Fix flaky test test_expiration_overflow_policy_cap - ttl_test.TestTTL,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/622/workflows/adcd463c-156a-43c7-a9bc-7f3e4938dbe8/jobs/3514

{code}
>           assert warning, 'Log message should be print for CAP and CAP_NOWARN policy'
E           AssertionError: Log message should be print for CAP and CAP_NOWARN policy
E           assert []

ttl_test.py:410: AssertionError
{code}"
CASSANDRA-16145,dtest: Fix redundant TTL overflow policy tests,"While looking into CASSANDRA-15996 I came across what I think is a copy/paste mistake resulting in redundant tests, and not running the other variant. 

Spinning out here to fix independently of the flaky test."
CASSANDRA-16131,NPE in StreamingMessage type lookup,"Found while investigating https://issues.apache.org/jira/browse/CASSANDRA-15965

{noformat}
java.lang.NullPointerException: null
	at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:51)
	at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:172)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

There is a null in the zero.index of the type map. We can clean this up, handle invalid ids in a uniform manner, and add a test."
CASSANDRA-16119,MockSchema's SSTableReader creation leaks FileHandle and Channel instances,"{{MockSchema}} creates {{SSTableReader}} instances for testing, but when it does, it doesn’t seem to ever close the {{FileHandle}} and {{Channel}} instances from which copies are made for the actual readers. ({{FileHandle}} itself also internally copies the channel on creation.) This can trigger leak detection, although perhaps not reliably, from tests like {{AntiCompactionTest}}. A couple well-placed {{try-with-resources}} blocks should help us avoid this (and shouldn't risk closing anything too early, since the close methods for handles and channels seem only to do reference bookkeeping anyway).


Example:

{noformat}
[junit-timeout] ERROR 16:35:47,747 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@487c0fdb) to class org.apache.cassandra.io.util.FileHandle$Cleanup@2072030898:/var/folders/4d/zfjs7m7s6x5_l93k33r5k6680000gn/T/mocksegmentedfile0tmp was not released before the reference was garbage collected
[junit-timeout] ERROR 16:35:47,747 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@487c0fdb:
[junit-timeout] Thread[main,5,main]
[junit-timeout] 	at java.lang.Thread.getStackTrace(Thread.java:1559)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:249)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:179)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:101)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.SharedCloseableImpl.<init>(SharedCloseableImpl.java:30)
[junit-timeout] 	at org.apache.cassandra.io.util.FileHandle.<init>(FileHandle.java:74)
[junit-timeout] 	at org.apache.cassandra.io.util.FileHandle.<init>(FileHandle.java:50)
[junit-timeout] 	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:389)
[junit-timeout] 	at org.apache.cassandra.schema.MockSchema.sstable(MockSchema.java:124)
[junit-timeout] 	at org.apache.cassandra.schema.MockSchema.sstable(MockSchema.java:83)
{noformat}"
CASSANDRA-16106,BufferOverflow exception while writing response to buffer,"Was running a benchmark at LOCAL_ONE and eventually saw the below exception; this is related to CASSANDRA-16097 as it was found during the same test.

{code}
message=""...SMALL_MESSAGES-1bb47c27 dropping message of type HINT_RSP due to error""
exception=""java.nio.BufferOverflowException
	at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52)
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:153)
	at org.apache.cassandra.utils.vint.VIntCoding.writeUnsignedVInt(VIntCoding.java:191)
	at org.apache.cassandra.io.util.DataOutputPlus.writeUnsignedVInt(DataOutputPlus.java:55)
	at org.apache.cassandra.net.Message$Serializer.serializeHeaderPost40(Message.java:688)
	at org.apache.cassandra.net.Message$Serializer.serializePost40(Message.java:758)
	at org.apache.cassandra.net.Message$Serializer.serialize(Message.java:618)
	at org.apache.cassandra.net.OutboundConnection$EventLoopDelivery.doRun(OutboundConnection.java:813)
	at org.apache.cassandra.net.OutboundConnection$Delivery.run(OutboundConnection.java:687)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)""
{code}

{code}
message=""...-SMALL_MESSAGES-e72423f4 dropping message of type MUTATION_RSP due to error""
exception=""java.nio.BufferOverflowException
	at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52)
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:153)
	at org.apache.cassandra.utils.vint.VIntCoding.writeUnsignedVInt(VIntCoding.java:191)
	at org.apache.cassandra.io.util.DataOutputPlus.writeUnsignedVInt(DataOutputPlus.java:55)
	at org.apache.cassandra.net.Message$Serializer.serializeParams(Message.java:1112)
	at org.apache.cassandra.net.Message$Serializer.serializeHeaderPost40(Message.java:689)
	at org.apache.cassandra.net.Message$Serializer.serializePost40(Message.java:758)
	at org.apache.cassandra.net.Message$Serializer.serialize(Message.java:618)
	at org.apache.cassandra.net.OutboundConnection$EventLoopDelivery.doRun(OutboundConnection.java:813)
	at org.apache.cassandra.net.OutboundConnection$Delivery.run(OutboundConnection.java:687)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)""
{code}"
CASSANDRA-16071,max_compaction_flush_memory_in_mb is interpreted as bytes,"In CASSANDRA-12662, [~scottcarey] [reported|https://issues.apache.org/jira/browse/CASSANDRA-12662?focusedCommentId=17070055&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17070055] that the {{max_compaction_flush_memory_in_mb}} setting gets incorrectly interpreted in bytes rather than megabytes as its name implies.

{quote}
1.  the setting 'max_compaction_flush_memory_in_mb' is a misnomer, it is actually memory in BYTES.  If you take it at face value, and set it to say, '512' thinking that means 512MB,  you will produce a million temp files rather quickly in a large compaction, which will exhaust even large values of max_map_count rapidly, and get the OOM: Map Error issue above and possibly have a very difficult situation to get a cluster back into a place where nodes aren't crashing while initilaizing or soon after.  This issue is minor if you know about it in advance and set the value IN BYTES.
{quote}

"
CASSANDRA-16062,Avoid NPE in getCompactionInfo,We currently initialize {{sstables}} after calling {{beginCompaction(this)}} in the {{CompactionIterator}} constructor which creates a window where we can get NPE creating the {{CompactionInfo}} for cancelling compactions
CASSANDRA-16060,Cassandra crashes with OutOfMemory Exception,"Out Cassandra instance has run perfectly for almost 5 years, but aout a month ago, the performance has dropped significantly. Reads are incredibly slow or time out, making the cluster almost useless. The load on the nodes has skyrocketed to almost 100% CPU usage. On top of that, the nodes crash with OutOfMemoryError. 
I also have [heap dumps|https://arcelormittal-my.sharepoint.com/:f:/g/personal/sidtdke_sidmar_be/Es7PaQq15jdKgIvDfMFfBRoBR6sr60t156AxbA0dFZvzJg?e=rohgHn]. 
Cassandra version: 3.11.1
Java version: OpenJDK 1.8.0_201

We have upgraded our nodes to version 3.11.7 and applied the recommended OS/system setting, but this has not improved performance.

{code:java}
ERROR [ReadStage-9] 2020-08-10 14:56:01,399 JVMStabilityInspector.java:142 - JVM state determined to be unstable.  Exiting forcefully due to:
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[na:1.8.0_201]
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) ~[na:1.8.0_201]
	at org.apache.cassandra.io.util.DataOutputBuffer.reallocate(DataOutputBuffer.java:159) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.io.util.DataOutputBuffer.doFlush(DataOutputBuffer.java:119) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:132) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:151) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.utils.ByteBufferUtil.writeWithVIntLength(ByteBufferUtil.java:296) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.marshal.AbstractType.writeValue(AbstractType.java:413) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.Cell$Serializer.serialize(Cell.java:210) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.lambda$serializeRowBody$0(UnfilteredSerializer.java:248) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$120/205506350.accept(Unknown Source) ~[na:na]
	at org.apache.cassandra.utils.btree.BTree.applyForwards(BTree.java:1242) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.utils.btree.BTree.apply(BTree.java:1197) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.BTreeRow.apply(BTreeRow.java:172) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serializeRowBody(UnfilteredSerializer.java:236) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:205) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:137) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:125) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:137) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:92) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:79) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:308) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:167) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:160) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:156) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:346) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_201]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) ~[apache-cassandra-3.11.1.jar:3.11.1]
{code}
"
CASSANDRA-16055,"StorageService exposes ensureTraceKeyspace that is used by jvm-dtest, but it should be removed and use MigrationManager.evolveSystemKeyspace instead","Jvm dtest uses StorageService.ensureTraceKeyspace to register the tracing keyspace, but doesn’t register the other distributed key spaces.  This method should not be used by dtest and instead should use MigrationManager.evolveSystemKeyspace as it registers all keyspaces"
CASSANDRA-15996,Fix flaky python dtest test_expiration_overflow_policy_capnowarn - ttl_test.TestTTL,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/361/workflows/3a42fa45-1f60-4c95-86a4-15a6773e384e/jobs/1860

{code}
>           assert warning, 'Log message should be print for CAP and CAP_NOWARN policy'
E           AssertionError: Log message should be print for CAP and CAP_NOWARN policy
E           assert []
{code}"
CASSANDRA-15979,CircleCI UnitTests error - java.lang.OutOfMemoryError: Java heap space,"We persistently see on the latest CircleCI trunk the following error:

(MIDRES)

 
{code:java}
BUILD FAILED
/tmp/cassandra/build.xml:1982: The following error occurred while executing this line:
/tmp/cassandra/build.xml:1866: java.lang.OutOfMemoryError: Java heap space
 at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
 at java.lang.StringBuffer.<init>(StringBuffer.java:128)
 at com.sun.org.apache.xml.internal.utils.FastStringBuffer.getString(FastStringBuffer.java:872)
 at com.sun.org.apache.xml.internal.dtm.ref.sax2dtm.SAX2DTM2.getStringValueX(SAX2DTM2.java:2937)
 at com.sun.org.apache.xalan.internal.xsltc.dom.DOMAdapter.getStringValueX(DOMAdapter.java:284)
 at junit_frames.template$dot$5()
 at junit_frames.applyTemplates5()
 at junit_frames.package()
 at junit_frames.template$dot$0()
 at junit_frames.applyTemplates()
 at junit_frames.applyTemplates()
 at junit_frames.transform()
 at com.sun.org.apache.xalan.internal.xsltc.runtime.AbstractTranslet.transform(AbstractTranslet.java:620)
 at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:730)
 at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:343)
 at org.apache.tools.ant.taskdefs.optional.TraXLiaison.transform(TraXLiaison.java:201)
 at org.apache.tools.ant.taskdefs.XSLTProcess.process(XSLTProcess.java:870)
 at org.apache.tools.ant.taskdefs.XSLTProcess.execute(XSLTProcess.java:408)
 at org.apache.tools.ant.taskdefs.optional.junit.AggregateTransformer.transform(AggregateTransformer.java:281)
 at org.apache.tools.ant.taskdefs.optional.junit.XMLResultAggregator.execute(XMLResultAggregator.java:157)
 at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:292)
 at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:99)
 at org.apache.tools.ant.Task.perform(Task.java:350)
 at org.apache.tools.ant.taskdefs.Sequential$$Lambda$149/1543351283.accept(Unknown Source)
 at java.util.Vector.forEach(Vector.java:1275)
 at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:67)
 at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:292)
 at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Total time: 2 minutes 52 seconds
Exited with code exit status 1
CircleCI received exit code 1
{code}
 

*Example:* [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/285/workflows/f2722016-2353-4c38-9fd2-8614f6609f55/jobs/1648/parallel-runs/16?filterBy=FAILED]"
CASSANDRA-15978,CircleCI Unit Tests issue on latest trunk - java.lang.OutOfMemoryError: Java heap space ,"We persistently see on the latest CircleCI trunk the following error:

 
{code:java}
BUILD FAILED
/tmp/cassandra/build.xml:1982: The following error occurred while executing this line:
/tmp/cassandra/build.xml:1866: java.lang.OutOfMemoryError: Java heap space
 at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)
 at java.lang.StringBuffer.<init>(StringBuffer.java:128)
 at com.sun.org.apache.xml.internal.utils.FastStringBuffer.getString(FastStringBuffer.java:872)
 at com.sun.org.apache.xml.internal.dtm.ref.sax2dtm.SAX2DTM2.getStringValueX(SAX2DTM2.java:2937)
 at com.sun.org.apache.xalan.internal.xsltc.dom.DOMAdapter.getStringValueX(DOMAdapter.java:284)
 at junit_frames.template$dot$5()
 at junit_frames.applyTemplates5()
 at junit_frames.package()
 at junit_frames.template$dot$0()
 at junit_frames.applyTemplates()
 at junit_frames.applyTemplates()
 at junit_frames.transform()
 at com.sun.org.apache.xalan.internal.xsltc.runtime.AbstractTranslet.transform(AbstractTranslet.java:620)
 at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:730)
 at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:343)
 at org.apache.tools.ant.taskdefs.optional.TraXLiaison.transform(TraXLiaison.java:201)
 at org.apache.tools.ant.taskdefs.XSLTProcess.process(XSLTProcess.java:870)
 at org.apache.tools.ant.taskdefs.XSLTProcess.execute(XSLTProcess.java:408)
 at org.apache.tools.ant.taskdefs.optional.junit.AggregateTransformer.transform(AggregateTransformer.java:281)
 at org.apache.tools.ant.taskdefs.optional.junit.XMLResultAggregator.execute(XMLResultAggregator.java:157)
 at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:292)
 at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:99)
 at org.apache.tools.ant.Task.perform(Task.java:350)
 at org.apache.tools.ant.taskdefs.Sequential$$Lambda$149/1543351283.accept(Unknown Source)
 at java.util.Vector.forEach(Vector.java:1275)
 at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:67)
 at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:292)
 at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Total time: 2 minutes 52 seconds
Exited with code exit status 1
CircleCI received exit code 1
{code}
 

*Example:* https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/285/workflows/f2722016-2353-4c38-9fd2-8614f6609f55/jobs/1648/parallel-runs/16?filterBy=FAILED"
CASSANDRA-15959,org.apache.cassandra.cql3.validation.operations.TTLTest testCapWarnExpirationOverflowPolicy,"Build: https://ci-cassandra.apache.org/job/Cassandra-trunk-test/194/testReport/junit/org.apache.cassandra.cql3.validation.operations/TTLTest/testCapWarnExpirationOverflowPolicy/
junit.framework.AssertionFailedError
	at org.apache.cassandra.cql3.validation.operations.TTLTest.checkTTLIsCapped(TTLTest.java:321)
	at org.apache.cassandra.cql3.validation.operations.TTLTest.verifyData(TTLTest.java:303)
	at org.apache.cassandra.cql3.validation.operations.TTLTest.testCapExpirationDateOverflowPolicy(TTLTest.java:248)
	at org.apache.cassandra.cql3.validation.operations.TTLTest.testCapExpirationDateOverflowPolicy(TTLTest.java:199)
	at org.apache.cassandra.cql3.validation.operations.TTLTest.testCapWarnExpirationOverflowPolicy(TTLTest.java:140)"
CASSANDRA-15949,NPE thrown while updating speculative execution time if table is removed during task execution,"CASSANDRA-14338 fixed the scheduling the speculation retry threshold calculation, but if the task happens to be scheduled while a table is being dropped, it triggers an NPE. 


ERROR 2020-07-14T11:34:55,762 [OptionalTasks:1] org.apache.cassandra.service.CassandraDaemon:446 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException: null
       at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:444) ~[cassandra-4.0.0.jar:4.0.0]
       at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:346) ~[cassandra-4.0.0.jar:4.0.0]
       at org.apache.cassandra.db.Keyspace.open(Keyspace.java:139) ~[cassandra-4.0.0.jar:4.0.0]
       at org.apache.cassandra.db.Keyspace.open(Keyspace.java:116) ~[cassandra-4.0.0.jar:4.0.0]
       at org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:102) ~[cassandra-4.0.0.jar:4.0.0]
       at org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:99) ~[cassandra-4.0.0.jar:4.0.0]
       at com.google.common.collect.Iterables$5.lambda$forEach$0(Iterables.java:704) ~[guava-27.0-jre.jar:?]
       at com.google.common.collect.IndexedImmutableSet.forEach(IndexedImmutableSet.java:45) ~[guava-27.0-jre.jar:?]
       at com.google.common.collect.Iterables$5.forEach(Iterables.java:704) ~[guava-27.0-jre.jar:?]
       at org.apache.cassandra.service.CassandraDaemon.lambda$setup$2(CassandraDaemon.java:412) ~[cassandra-4.0.0.jar:4.0.0]
       at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118) [cassandra-4.0.0.jar:4.0.0]
       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
       at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
       at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
       at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-all-4.1.37.Final.jar:4.1.37.Final]
       at java.lang.Thread.run(Thread.java:834) [?:?]"
CASSANDRA-15946,NPE when sending REQUEST_RSP from 3.0 to 4.0 in in-jvm dtests,"There is a communication problem when testing upgrades using in-JVM dtest between Cassandra 3 and 4. 

In a method {{registerInboundFilter}} of {{Instance}}, we get a message which was just received and we prepare it for filtering as part of which, we serialize the payload again. This is fine when dealing with incoming Cassandra 4 message, because we can serialize it. However when we get the Cassandra 3 message, which uses a different protocol, and we get something like {{REQUEST_RSP}}, we can surely deserialize it through some special deserialization path, but we cannot serialize the payload for it as there is no serializer defined for {{REQUEST_RSP}} - no wonder, why would Cassandra 4.0 need to be able to serialize Cassandra 3.0 payloads?

{code}
java.lang.NullPointerException: null
	at org.apache.cassandra.net.Message$Serializer.serializePost40(Message.java:760)
	at org.apache.cassandra.net.Message$Serializer.serialize(Message.java:618)
	at org.apache.cassandra.distributed.impl.Instance.serializeMessage(Instance.java:267)
	at org.apache.cassandra.distributed.impl.Instance.lambda$registerInboundFilter$4(Instance.java:234)
	at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:62)
	at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:49)
	at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:93)
	at org.apache.cassandra.distributed.impl.Instance.lambda$null$6(Instance.java:305)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:137)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{code}
"
CASSANDRA-15917,LEAK Detected in Cassandra,"Hi

We are using cassandra in docker environment. So 1 out of 3 cassandra, cassandra docker is restarting continuously. This issue is happening in production setup. 

Then checked system logs and debug logs and found these traceback occur multiple times
{code:java}
ERROR [Reference-Reaper] 2020-06-29 14:40:20,800 Ref.java:229 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@53581121) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@194489638:/var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-626-big-Index.db was not released before the reference was garbage collected
DEBUG [SSTableBatchOpen:8] 2020-06-29 14:40:23,698 SSTableReader.java:515 - Opening /var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-240-big (0.050KiB)
ERROR [SSTableBatchOpen:5] 2020-06-29 14:40:20,801 DebuggableThreadPoolExecutor.java:239 - Error in ThreadPoolExecutor
 java.lang.NullPointerException: null
 at org.apache.cassandra.io.sstable.format.SSTableReader.getFilename(SSTableReader.java:697) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:543) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:385) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.SSTableReader$3.run(SSTableReader.java:570) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_232]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_232]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_232]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_232]
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
 at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_232]
{code}
 

 

Can somebody help me out here as it is customer deployed setup"
CASSANDRA-15916,LEAK Detected in Cassandra,"Hi

We are using cassandra docker environment. So 1 out of 3 cassandra, cassandra docker is restarting continuously. This issue is happening in production setup. 

Then checked system logs and debug logs and found these traceback occur multiple times
ERROR [Reference-Reaper] 2020-06-29 14:40:20,800 Ref.java:229 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@53581121) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@194489638:/var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-626-big-Index.db was not released before the reference was garbage collectedDEBUG [SSTableBatchOpen:8] 2020-06-29 14:40:23,698 SSTableReader.java:515 - Opening /var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-240-big (0.050KiB)ERROR [SSTableBatchOpen:5] 2020-06-29 14:40:20,801 DebuggableThreadPoolExecutor.java:239 - Error in ThreadPoolExecutor
java.lang.NullPointerException: null
        at org.apache.cassandra.io.sstable.format.SSTableReader.getFilename(SSTableReader.java:697) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:543) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:385) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader$3.run(SSTableReader.java:570) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_232]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_232]"
CASSANDRA-15915,LEAK Detected in Cassandra,"Hi

We are using cassandra docker environment. So 1 out of 3 cassandra, cassandra docker is restarting continuously. This issue is happening in production setup. 

Then checked system logs and debug logs and found these traceback occur multiple times
ERROR [Reference-Reaper] 2020-06-29 14:40:20,800 Ref.java:229 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@53581121) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@194489638:/var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-626-big-Index.db was not released before the reference was garbage collectedDEBUG [SSTableBatchOpen:8] 2020-06-29 14:40:23,698 SSTableReader.java:515 - Opening /var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-240-big (0.050KiB)ERROR [SSTableBatchOpen:5] 2020-06-29 14:40:20,801 DebuggableThreadPoolExecutor.java:239 - Error in ThreadPoolExecutor
java.lang.NullPointerException: null
        at org.apache.cassandra.io.sstable.format.SSTableReader.getFilename(SSTableReader.java:697) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:543) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:385) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader$3.run(SSTableReader.java:570) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_232]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_232]"
CASSANDRA-15914,LEAK Detected in Cassandra,"Hi

We are using cassandra docker environment. So 1 out of 3 cassandra, cassandra docker is restarting continuously. This issue is happening in production setup. 

Then checked system logs and debug logs and found these traceback occur multiple times
ERROR [Reference-Reaper] 2020-06-29 14:40:20,800 Ref.java:229 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@53581121) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@194489638:/var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-626-big-Index.db was not released before the reference was garbage collectedDEBUG [SSTableBatchOpen:8] 2020-06-29 14:40:23,698 SSTableReader.java:515 - Opening /var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/md-240-big (0.050KiB)ERROR [SSTableBatchOpen:5] 2020-06-29 14:40:20,801 DebuggableThreadPoolExecutor.java:239 - Error in ThreadPoolExecutor
java.lang.NullPointerException: null
        at org.apache.cassandra.io.sstable.format.SSTableReader.getFilename(SSTableReader.java:697) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:543) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:385) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader$3.run(SSTableReader.java:570) ~[apache-cassandra-3.11.5.jar:3.11.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_232]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_232]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_232]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_232]"
CASSANDRA-15908,Improve messaging on indexing frozen collections,"When attempting to create an index on a frozen collection the error message produced can be improved to provide more detail about the problem and possible workarounds. Currently, a user will receive a message indicating ""...Frozen collections only support full() indexes"" which is not immediately clear for users new to Cassandra indexing and datatype compatibility.

Here is an example:
{code:java}
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

cqlsh> CREATE TABLE test.mytable ( id int primary key, addresses frozen<set<text>> );

cqlsh> CREATE INDEX mytable_addresses_idx on test.mytable (addresses);
 InvalidRequest: Error from server: code=2200 [Invalid query] message=""Cannot create values() index on frozen column addresses. Frozen collections only support full() indexes""{code}
 

I'm proposing possibly enhancing the messaging to something like this.
{quote}Cannot create values() index on frozen column addresses. Frozen collections only support indexes on the entire data structure due to immutability constraints of being frozen, wrap your frozen column with the full() target type to index properly.
{quote}"
CASSANDRA-15898,cassandra 3.11.4 deadlock,"We are running apache-cassandra-3.11.4, 10 node cluster with -Xms32G -Xmx32G -Xmn8G using CMS.
after running couple of days one of the node become unresponsive and threaddump (jstack -F) shows deadlock.


Found one Java-level deadlock:
=============================

""Native-Transport-Requests-144"": waiting to lock Monitor@0x00007cd5142e4d08 (Object@0x00007f6e00348268, a java/io/ExpiringCache),
 which is held by ""CompactionExecutor:115134""
""CompactionExecutor:115134"": waiting to lock Monitor@0x00007f6bcaf130f8 (Object@0x00007f6dff31faa0, a ch/qos/logback/core/joran/spi/ConfigurationWatchList),
 which is held by ""Native-Transport-Requests-144""

Found a total of 1 deadlock.

I have seen this couple of time now with different nodes with following in system.log

IndexSummaryRedistribution.java:77 - Redistributing index summaries
 NoSpamLogger.java:91 - Maximum memory usage reached (536870912), cannot allocate chunk of 1048576

also lookin in gc log there has not been a ParNew collection for last 10hrs, only CMS collections.

1739842.375: [GC (CMS Final Remark) [YG occupancy: 2712269 K (7549760 K)]
1739842.375: [Rescan (parallel) , 0.0614157 secs]
1739842.437: [weak refs processing, 0.0000994 secs]
1739842.437: [class unloading, 0.0231076 secs]
1739842.460: [scrub symbol table, 0.0061049 secs]
1739842.466: [scrub string table, 0.0043847 secs][1 CMS-remark: 17696837K(25165824K)] 20409107K(32715584K), 0.0953750 secs] [Times: user=2.95 sys=0.00, real=0.09 secs]
1739842.471: [CMS-concurrent-sweep-start]
1739848.572: [CMS-concurrent-sweep: 6.101/6.101 secs] [Times: user=6.13 sys=0.00, real=6.10 secs]
1739848.573: [CMS-concurrent-reset-start]
1739848.645: [CMS-concurrent-reset: 0.072/0.072 secs] [Times: user=0.08 sys=0.00, real=0.08 secs]
1739858.653: [GC (CMS Initial Mark) [1 CMS-initial-mark: 17696837K(25165824K)] 
20409111K(32715584K), 0.0584838 secs] [Times: user=2.68 sys=0.00, real=0.06 secs]
1739858.713: [CMS-concurrent-mark-start]
1739860.496: [CMS-concurrent-mark: 1.784/1.784 secs] [Times: user=84.77 sys=0.00, real=1.79 secs]
1739860.497: [CMS-concurrent-preclean-start]
1739860.566: [CMS-concurrent-preclean: 0.070/0.070 secs] [Times: user=0.07 sys=0.00, real=0.07 secs]
1739860.567: [CMS-concurrent-abortable-preclean-start]CMS: abort preclean due to time
1739866.333: [CMS-concurrent-abortable-preclean: 5.766/5.766 secs] [Times: user=5.80 sys=0.00, real=5.76 secs]

Java HotSpot(TM) 64-Bit Server VM (25.162-b12) for linux-amd64 JRE (1.8.0_162-b12)
Memory: 4k page, physical 792290076k(2780032k free), swap 16777212k(16693756k free)

CommandLine flags:
-XX:+AlwaysPreTouch
-XX:CICompilerCount=15
-XX:+CMSClassUnloadingEnabled
-XX:+CMSEdenChunksRecordAlways
-XX:CMSInitiatingOccupancyFraction=40
-XX:+CMSParallelInitialMarkEnabled
-XX:+CMSParallelRemarkEnabled
-XX:CMSWaitDuration=10000
-XX:ConcGCThreads=50
-XX:+CrashOnOutOfMemoryError
-XX:GCLogFileSize=10485760
-XX:+HeapDumpOnOutOfMemoryError
-XX:InitialHeapSize=34359738368
-XX:InitialTenuringThreshold=1
-XX:+ManagementServer
-XX:MaxHeapSize=34359738368
-XX:MaxNewSize=8589934592
-XX:MaxTenuringThreshold=1
-XX:MinHeapDeltaBytes=196608
-XX:NewSize=8589934592
-XX:NumberOfGCLogFiles=10
-XX:OldPLABSize=16
-XX:OldSize=25769803776
-XX:OnOutOfMemoryError=kill -9 %p
-XX:ParallelGCThreads=50
-XX:+PerfDisableSharedMem
-XX:+PrintGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+ResizeTLAB
-XX:StringTableSize=1000003
-XX:SurvivorRatio=8
-XX:ThreadPriorityPolicy=42
-XX:ThreadStackSize=256
-XX:-UseBiasedLocking
-XX:+UseCMSInitiatingOccupancyOnly
-XX:+UseConcMarkSweepGC
-XX:+UseCondCardMark
-XX:+UseFastUnorderedTimeStamps
-XX:+UseGCLogFileRotation
-XX:+UseNUMA
-XX:+UseNUMAInterleaving
-XX:+UseParNewGC
-XX:+UseTLAB
-XX:+UseThreadPriorities"
CASSANDRA-15880,Memory leak in CompressedChunkReader,"CompressedChunkReader uses java.lang.ThreadLocal to reuse ByteBuffer for compressed data. ByteBuffers leak due to peculiar ThreadLocal quality.
ThreadLocals are stored in a map, where the key is a weak reference to a ThreadLocal and the value is the user's object (ByteBuffer in this case). When a last strong reference to a ThreadLocal is lost, weak reference to ThreadLocal (key) is removed but the value (ByteBuffer) is kept until cleaned by ThreadLocal heuristic expunge mechanism. See ThreadLocal's ""stale entries"" for details.

When a number of long-living threads is high enough this results in thousands of ByteBuffers stored as stale entries in ThreadLocals. In a not-so-lucky scenario we get OutOfMemoryException."
CASSANDRA-15869,MemoryOutputStream overflow on large bloom filter,"Since CASSANDRA-9067, Cassandra will use {{MemoryOutputStream}} to reconstruct BF Memory without re-ordering bytes, see {{OffsetBitSet#deserialize}}. But {{MemoryOutputStream}} use {{INT}} to track position and will overflow when BF size exceeds 2GB.
{code:java|title=stacktrace}
error: Illegal bounds [-2147483648..-2147483584); size: 4588588016
-- StackTrace --
java.lang.AssertionError: Illegal bounds [-2147483648..-2147483584); size: 4588588016
	at org.apache.cassandra.io.util.Memory.checkBounds(Memory.java:185)
	at org.apache.cassandra.io.util.Memory.setBytes(Memory.java:138)
	at org.apache.cassandra.io.util.MemoryOutputStream.write(MemoryOutputStream.java:45)
{code}"
CASSANDRA-15861,Mutating sstable component may race with entire-sstable-streaming(ZCS) causing checksum validation failure,"Flaky dtest: [test_dead_sync_initiator - repair_tests.repair_test.TestRepair|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/143/testReport/junit/dtest.repair_tests.repair_test/TestRepair/test_dead_sync_initiator/]
{code:java|title=stacktrace}
Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [Stream-Deserializer-127.0.0.1:7000-570871f3] 2020-06-03 04:05:19,081 CassandraEntireSSTableStreamReader.java:145 - [Stream 6f1c3360-a54f-11ea-a808-2f23710fdc90] Error while reading sstable from stream for table = keyspace1.standard1
org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /home/cassandra/cassandra/cassandra-dtest/tmp/dtest-te4ty0r9/test/node3/data0/keyspace1/standard1-5f5ab140a54f11eaa8082f23710fdc90/na-2-big-Statistics.db
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.maybeValidateChecksum(MetadataSerializer.java:219)
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:198)
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:129)
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.mutate(MetadataSerializer.java:226)
	at org.apache.cassandra.db.streaming.CassandraEntireSSTableStreamReader.read(CassandraEntireSSTableStreamReader.java:140)
	at org.apache.cassandra.db.streaming.CassandraIncomingFile.read(CassandraIncomingFile.java:78)
	at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:49)
	at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:36)
	at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:49)
	at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:181)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Checksums do not match for /home/cassandra/cassandra/cassandra-dtest/tmp/dtest-te4ty0r9/test/node3/data0/keyspace1/standard1-5f5ab140a54f11eaa8082f23710fdc90/na-2-big-Statistics.db
{code}
 

In the above test, it executes ""nodetool repair"" on node1 and kills node2 during repair. At the end, node3 reports checksum validation failure on sstable transferred from node1.
{code:java|title=what happened}
1. When repair started on node1, it performs anti-compaction which modifies sstable's repairAt to 0 and pending repair id to session-id.
2. Then node1 creates {{ComponentManifest}} which contains file lengths to be transferred to node3.
3. Before node1 actually sends the files to node3, node2 is killed and node1 starts to broadcast repair-failure-message to all participants in {{CoordinatorSession#fail}}
4. Node1 receives its own repair-failure-message and fails its local repair sessions at {{LocalSessions#failSession}} which triggers async background compaction.
5. Node1's background compaction will mutate sstable's repairAt to 0 and pending repair id to null via  {{PendingRepairManager#getNextRepairFinishedTask}}, as there is no more in-progress repair.
6. Node1 actually sends the sstable to node3 where the sstable's STATS component size is different from the original size recorded in the manifest.
7. At the end, node3 reports checksum validation failure when it tries to mutate sstable level and ""isTransient"" attribute in {{CassandraEntireSSTableStreamReader#read}}.
{code}
Currently, entire-sstable-streaming requires sstable components to be immutable, because \{{ComponentManifest}}
with component sizes are sent before sending actual files. This isn't a problem in legacy streaming as STATS file length didn't matter.

 

Ideally it will be great to make sstable STATS metadata immutable, just like other sstable components, so we don't have to worry this special case.

I can think of 2 ways:
 # Make STATS mutation as a proper compaction to create hard link on the compacting sstable components with a new descriptor, except STATS files which will be copied entirely. Then mutation will be applied on the new STATS file. At the end, old sstable will be released. This ensures all sstable components are immutable and shouldn't make these special compaction tasks slower.
 # Change STATS metadata format to use fixed length encoding for repair info"
CASSANDRA-15848,Fully purged static row causes NPE in repaired data tracking,"During repaired data tracking, if the result of applying the purge function to a static row is null, an exception is thrown from RepairedDataInfo. This will cause a read exception from the replica and could lead to unavailable results if hit on multiple replicas. A workaround is to disable repaired data tracking."
CASSANDRA-15838,Add deb and rpm packaging to artifacts test script,"Currently we have no way of testing debian or rpm packaging. This is a problem, it hurts when cutting releases as well as afterwards (like CASSANDRA-15830). 

This ticket is to add testing of debian or rpm packaging into the artifacts test script.

This will also provide ""nightly"" build artifacts for the deb and rpm packages."
CASSANDRA-15800,BinLog deadlock on stopping when the sample queue is full,"A deadlock can happen when 1) the BinLog is being stoped and 2) the BinLog's internal sample queue is full. 

When stopping, BinLog first set the flag shouldContinue to false, so that the internal consumer thread stop consuming. It is possible to leave the queue being full.
Then, the BinLog puts one extra object NO_OP into the sample queue. However, the queue is already full, so the put operation blocks, and the stop method never returns. 
Therefore, we got a deadlock.

BinLog is used by Cassandra 40 new features such as audit logging and full query logging. 
If such deadlock happens, the thread cannot be not joined and the referenced items in the queue are never released, hence memory leak."
CASSANDRA-15727,Internode messaging connection setup between 4.0 and legacy SSL 3.0 fails if initial connection version incorrect,"This was discovered while testing upgrading an SSL enabled cluster from 3.0 to 4.0.  The 3.0 cluster was configured to only listen on the ssl storage port. When the upgraded 4.0 node started it received a gossip messsage that triggered a shadow round before it had correctly set the messaging versions for the other endpoints.

Sending the message created the connection, but because the endpoint defaulted to {{VERSION_40}} the initial connect attempt was to the regular {{storage_port}}.  The 3.0 node was only listening on the {{ssl_storage_port}}, so the connection was refused and the {{OutboundConnection.onFailure}} handler was called.  As the shadow
gossip round had queued up a message, the {{hasPending}} branch was followed and the connection was rescheduled, however the port is never recalculated as the original settings are used so it always fails.

Meanwhile, the node discovered information about peers through inbound connection and gossip updating the messaging version for the endpoint which could have been used to make a valid connection."
CASSANDRA-15726,buffer pool may throw NPE with concurrent release due to in-progress tiny pool eviction,"This can be reproduced by running {{LongBufferPoolTest}}, 1 out 5 runs..
{code:java}
java.lang.NullPointerException
	at org.apache.cassandra.utils.memory.BufferPool$Chunk.access$1300(BufferPool.java:836)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.lambda$remove$1(BufferPool.java:716)
	at org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.removeIf(BufferPool.java:460)
	at org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.access$1500(BufferPool.java:304)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.remove(BufferPool.java:716)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.put(BufferPool.java:590)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.recycle(BufferPool.java:709)
	at org.apache.cassandra.utils.memory.BufferPool$Chunk.recycle(BufferPool.java:909)
	at org.apache.cassandra.utils.memory.BufferPool$Chunk.tryRecycle(BufferPool.java:903)
	at org.apache.cassandra.utils.memory.BufferPool$Chunk.release(BufferPool.java:896)
	at org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.removeIf(BufferPool.java:465)
	at org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.access$1500(BufferPool.java:304)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.addChunk(BufferPool.java:736)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.addChunkFromParent(BufferPool.java:725)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGetInternal(BufferPool.java:691)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGet(BufferPool.java:679)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.access$000(BufferPool.java:518)
	at org.apache.cassandra.utils.memory.BufferPool.tryGet(BufferPool.java:120)                       
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$2.testOne(LongBufferPoolTest.java:497)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:558)
	at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:538)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{code}
The cause is that:
 * When evicting a normal chunk from a full MicroQueueOfChunks, local pool will try to remove corresponding tiny chunks, via {{MicroQueueOfChunks#removeIf}}.
 * If matching tiny chunk is found, tiny {{chunk.release()}} is called immediately before moving null chunk to the back of the queue.
 * Due to concurrent release from different threads, tiny {{chunk.release()}} may cause its parent normal chunk, aka. the evicted chunk in #1, to be removed from local pool and causes tiny pool to remove corresponding tiny chunks again in {{LocalPool#remove()}}.
 * In {{MicroQueueOfChunks#removeIf}}, due to previous in-progress {{removeIf}}, it throws NPE as it violate MicroQueueOfChunks's assumption which requires null chunks to be put at the back of queue.

 
|[patch|https://github.com/apache/cassandra/pull/537]|[CI|https://circleci.com/workflow-run/a97317a0-ef21-4c01-9a97-82eaf28d7faf]|

The fix is to put null chunks to the back of queue before releasing any chunks."
CASSANDRA-15700,Performance regression on internode messaging,"Me and [~jasonstack] have been investigating a performance regression affecting 4.0 during a 3 nodes, RF 3 write throughput test with a timeseries like workload, as shown in this plot, where blue is 3.11 and orange is 4.0:

!Oss40vsOss311.png|width=389,height=214!

 It's been a bit of a long investigation, but two clues ended up standing out:
1) An abnormal number of expired messages on 4.0 (as shown in the attached  system log), while 3.11 has almost none.
2) An abnormal GC activity (as shown in the attached gc log).

Turns out the two are related, as the [on expired callback|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundConnection.java#L462] creates a huge amount of strings in the {{id()}} call. The next question is what causes all those message expirations; we thoroughly reviewed the internode messaging code and the only issue we could find so far is related to the ""batch pruning"" calls [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundMessageQueue.java#L81] and [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundMessageQueue.java#L188]: it _seems_ too much time is spent on those, causing the event loop to fall behind in processing the rest of the messages, which will end up being expired. This is supported by the analysis of the collapsed stacks (after fixing the GC issue):
{noformat}
(tprint (top-aggregated-calls oss40nogc ""EventLoopDelivery:doRun"" 5))
org/apache/cassandra/net/OutboundConnection$EventLoopDelivery:doRun 3456
org/apache/cassandra/net/OutboundMessageQueue:access$600 1621
org/apache/cassandra/net/PrunableArrayQueue:prune 1621
org/apache/cassandra/net/OutboundMessageQueue$WithLock:close 1621
org/apache/cassandra/net/OutboundMessageQueue:pruneInternalQueueWithLock 1620
{noformat}

Those are the top 5 sampled calls from {{EventLoopDelivery#doRun()}} which spends half of its time pruning. But only a tiny portion of such pruning time is spent actually expiring:
{noformat}
(tprint (top-aggregated-calls oss40nogc ""OutboundMessageQueue:pruneInternalQueueWithLock"" 5))
org/apache/cassandra/net/OutboundMessageQueue:pruneInternalQueueWithLock 1900
org/apache/cassandra/net/PrunableArrayQueue:prune 1894
org/apache/cassandra/net/OutboundMessageQueue$1Pruner:onPruned 147
org/apache/cassandra/net/OutboundConnection$$Lambda$444/740904487:accept 147
org/apache/cassandra/net/OutboundConnection:onExpired 147
{noformat}

And indeed, the {{PrunableArrayQueue:prune()}} self time is dominant:
{noformat}
(tprint (top-self-calls oss40nogc ""PrunableArrayQueue:prune"" 5))
org/apache/cassandra/net/PrunableArrayQueue:prune 1718
org/apache/cassandra/net/OutboundConnection:releaseCapacity 27
java/util/concurrent/ConcurrentHashMap:replaceNode 19
java/util/concurrent/ConcurrentLinkedQueue:offer 16
java/util/concurrent/LinkedBlockingQueue:offer 15
{noformat}

That said, before proceeding with a PR to fix those issues, I'd like to understand: what's the reason to prune so often, rather than just when polling the message during delivery? If there's a reason I'm missing, let's talk about how to optimize pruning, otherwise let's get rid of that."
CASSANDRA-15692,NPE in ConnectionBurnTest,"https://ci-cassandra.apache.org/job/Cassandra-trunk-test-burn/30/testReport/org.apache.cassandra.net/ConnectionBurnTest/test/

{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.getCurrentPosition(AbstractCommitLogSegmentManager.java:515)
	at org.apache.cassandra.db.commitlog.CommitLog.getCurrentPosition(CommitLog.java:223)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:403)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:602)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:576)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:567)
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:445)
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:340)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:138)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:115)
	at org.apache.cassandra.db.Keyspace.openAndGetStore(Keyspace.java:173)
	at org.apache.cassandra.index.IndexRegistry.obtain(IndexRegistry.java:251)
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:154)
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:134)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepareRestrictions(SelectStatement.java:1084)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:958)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:948)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:923)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:523)
	at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:252)
	at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:297)
	at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:307)
	at org.apache.cassandra.db.SystemKeyspace.getPreferredIP(SystemKeyspace.java:857)
	at org.apache.cassandra.net.OutboundConnectionSettings.connectTo(OutboundConnectionSettings.java:448)
	at org.apache.cassandra.net.OutboundConnectionSettings.withDefaults(OutboundConnectionSettings.java:470)
	at org.apache.cassandra.net.OutboundConnection.<init>(OutboundConnection.java:291)
	at org.apache.cassandra.net.Connection.<init>(Connection.java:81)
	at org.apache.cassandra.net.ConnectionBurnTest$Test.<init>(ConnectionBurnTest.java:216)
	at org.apache.cassandra.net.ConnectionBurnTest$Test.<init>(ConnectionBurnTest.java:144)
	at org.apache.cassandra.net.ConnectionBurnTest$Test$Builder.build(ConnectionBurnTest.java:179)
	at org.apache.cassandra.net.ConnectionBurnTest.test(ConnectionBurnTest.java:640)
	at org.apache.cassandra.net.ConnectionBurnTest.test(ConnectionBurnTest.java:658)
{noformat}

Test History:
https://ci-cassandra.apache.org/job/Cassandra-trunk-test-burn/lastCompletedBuild/testReport/org.apache.cassandra.net/ConnectionBurnTest/test/history/"
CASSANDRA-15687,Regression in traceOutgoingMessage printing message size in tracing,"The internode messaging refactor (CASSANDRA-15066) introduced a tiny regression to tracing. Previously CASSANDRA-13028 had changed the message emitted by traceOutgoingMessage to include the number of bytes.
{code}
""Sending %s message to %s""
{code}
to
{code}
""Sending %s message to %s message size %d bytes""
{code}

But the change was dropped during the refactor, it's easy to put it back."
CASSANDRA-15683,Fix NPE in SimpleReadWriteTest after test framework behavior change,"In JVM dtests, all exceptions thrown in a request execution used to be always wrapped in a RuntimeException.

After CASSANDRA-15650 changes, the behavior has been changed to: If the exception is a RuntimeException, just rethrow it, otherwise wrap in RuntimeException, as you can see in: [https://github.com/apache/cassandra/commit/dfc279a22a5563ac7a832a586914d5410426e9b7#diff-0b019281b7e97248577c82af0e663ef4R211]

This causes the tests that were always extracting the cause from the wrapping RuntimeException before, to check the root cause of the error, to throw a NPE when they call {{getCause()}}, tests such as {{SimpleReadWriteTest#readWithSchemaDisagreement}} and {{SimpleReadWriteTest#writeWithSchemaDisagreement}}.

Can be fixed by simply not unwrapping the cause in those tests, use the thrown exception directly, if the behavior of ""not always wrapping in RuntimeException"" is agreed to be correct."
CASSANDRA-15672," Testsuite: org.apache.cassandra.repair.consistent.CoordinatorMessagingTest Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.878 sec","The following failure was observed:

 Testsuite: org.apache.cassandra.repair.consistent.CoordinatorMessagingTest Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.878 sec
[junit-timeout] 
[junit-timeout] Testcase: testMockedMessagingPrepareFailureP1(org.apache.cassandra.repair.consistent.CoordinatorMessagingTest):	FAILED
[junit-timeout] null
[junit-timeout] junit.framework.AssertionFailedError
[junit-timeout] 	at org.apache.cassandra.repair.consistent.CoordinatorMessagingTest.testMockedMessagingPrepareFailure(CoordinatorMessagingTest.java:206)
[junit-timeout] 	at org.apache.cassandra.repair.consistent.CoordinatorMessagingTest.testMockedMessagingPrepareFailureP1(CoordinatorMessagingTest.java:154)
[junit-timeout] 
[junit-timeout] 

Seen on Java8"
CASSANDRA-15667,"StreamResultFuture check for completeness is inconsistent, leading to races","{{StreamResultFuture#maybeComplete()}} uses {{StreamCoordinator#hasActiveSessions()}} to determine if all sessions are completed, but then accesses each session state via {{StreamCoordinator#getAllSessionInfo()}}: this is inconsistent, as the former relies on the actual {{StreamSession}} state, while the latter on the {{SessionInfo}} state, and the two are concurrently updated with no coordination whatsoever.

This leads to races, i.e. apparent in some dtest spurious failures, such as {{TestBootstrap.resumable_bootstrap_test}} in CASSANDRA-15614 cc [~e.dimitrova]."
CASSANDRA-15666,Race condition when completing stream sessions,"{{StreamSession#prepareAsync()}} executes, as the name implies, asynchronously from the IO thread: this opens up for race conditions between the sending of the {{PrepareSynAckMessage}} and the call to {{StreamSession#maybeCompleted()}}. I.e., the following could happen:
1) Node A sends {{PrepareSynAckMessage}} from the {{prepareAsync()}} thread.
2) Node B receives it and starts streaming.
3) Node A receives the streamed file and sends {{ReceivedMessage}}.
4) At this point, if this was the only file to stream, both nodes are ready to close the session via {{maybeCompleted()}}, but:
a) Node A will call it twice from both the IO thread and the thread at #1, closing the session and its channels.
b) Node B will attempt to send a {{CompleteMessage}}, but will fail because the session has been closed in the meantime.

There are other subtle variations of the pattern above, depending on the order of concurrently sent/received messages.

I believe the best fix would be to modify the message exchange so that:
1) Only the ""follower"" is allowed to send the {{CompleteMessage}}.
2) Only the ""initiator"" is allowed to close the session and its channels after receiving the {{CompleteMessage}}.

By doing so, the message exchange logic would be easier to reason about, which is overall a win anyway."
CASSANDRA-15555,Lower the amount of garbage ChecksummingTransformerTest generates by reusing memory,"When the test finds a issue and attempts to shrink, a ton of garbage is generated which cause the test to then fail with GC issues.  Since the test is a quick theories test, we can rewrite the generates to reuse memory rather than creating new objects for each iteration.  This will also let us move away from String which requires multiple copies in the test and gets rid of the getBytes() call which is dependent on the system its called on.

|[Circle CI|https://circleci.com/gh/dcapwell/cassandra/tree/quickTheoryMemory]|"
CASSANDRA-15499,Internode message builder does not add trace header,"The messages built with the {{Builder}} ({{org.apache.cassandra.net.Message.Builder}}) do not have the trace header when tracing is enabled. 
Consequently, no tracing session gets propagated to other nodes, and the tracing function is broken. 

The set of static {{out*}} methods provided (to create an out-bounding message) in Message do not have the issue. They can properly add the trace header when necessary. 
To be clear, only the {{Builder}} missed adding the tracing header and it should be fixed to be consistent with the {{out*}} methods."
CASSANDRA-15478,Improved Messaging ,"Added a page on improved messaging.
https://github.com/apache/cassandra/pull/407"
CASSANDRA-15470,Potential Overflow in DatabaseDescriptor Functions That Convert Between KB/MB & Bytes,"{{DatabaseDescriptor}} has several functions that convert between user supplied sizes in KB/MB and bytes. These are implemented without much consistency and, while unlikely, several have the potential to overflow since validation on the input is missing. Meanwhile, some widen the number to a long correctly. Options include: widening in all places or simply doing better validation on start up — currently only the lower bound of the valid range is checked for many of these fields.

List of Affected {{DatabaseDescriptor}} Methods:
 * {{getColumnIndexSize}}
 * {{getColumnIndexCacheSize}}
 * {{getBatchSizeWarnThreshold}}
 * {{getNativeTransportFrameBlockSize}}
 * {{getRepairSessionSpaceInMegabytes}}
 * {{getNativeTransportMaxFrameSize}}"
CASSANDRA-15441,"Bump generations and document changes to system_distributed and system_traces in 3.0, 3.11",We should document all the changes to distributed system keyspaces and assign unique generations to them. In 3.0 and 3.11 this is just a documentation issue.
CASSANDRA-15398,Fix system_traces creation timestamp; optimise system keyspace upgrades,"We have introduced changes to system_traces tables in 3.0 (removal of default_time_to_live, lowering of bloom_filter_fp_chance). We did not, however, bump the timestamp with which we add the tables to schema, still defaulting to 0. As a result, for clusters that upgraded from 2.1/2.2, on bounce we would always detect a mismatch between actual and desired table definitions, always try to reconcile it by issuing migration tasks, but have them never override the existing definitions in place.

Additionally, prior to 2.0.2 (CASSANDRA-6016) we’d use a ‘real’ timestamp, so for clusters that started on even earlier versions of C* (say, 1.2), a bump to the timestamp by 1 would be insufficient, and a larger generation is necessary (I picked Jan 1 2020 as cut-off date).

The patch also optimises the process of upgrading replicated system tables. Instead of issuing a migration task for every table that changed for every node, we batch all changes into a single schema migration task."
CASSANDRA-15379,Flush with fast compressors by default,"[~josnyder] and I have been testing out CASSANDRA-14482 (Zstd compression) on some of our most dense clusters and have been observing close to 50% reduction in footprint with Zstd on some of our workloads! Unfortunately though we have been running into an issue where the flush might take so long (Zstd is slower to compress than LZ4) that we can actually block the next flush and cause instability.

Internally we are working around this with a very simple patch which flushes SSTables as the default compression strategy (LZ4) regardless of the table params. This is a simple solution but I think the ideal solution though might be for the flush compression strategy to be configurable separately from the table compression strategy (while defaulting to the same thing). Instead of adding yet another compression option to the yaml (like hints and commitlog) I was thinking of just adding it to the table parameters and then adding a {{default_table_parameters}} yaml option like:
{noformat}

# Default table properties to apply on freshly created tables. The currently supported defaults are:
# * compression       : How are SSTables compressed in general (flush, compaction, etc ...)
# * flush_compression : How are SSTables compressed as they flush
# supported
default_table_parameters:
  compression:
    class_name: 'LZ4Compressor'
    parameters:
      chunk_length_in_kb: 16
  flush_compression:
    class_name: 'LZ4Compressor'
    parameters:
      chunk_length_in_kb: 4
{noformat}

This would have the nice effect as well of giving our configuration a path forward to providing user specified defaults for table creation (so e.g. if a particular user wanted to use a different default chunk_length_in_kb they can do that).

So the proposed (~mandatory) scope is:
* Flush with a faster compression strategy

I'd like to implement the following at the same time:
* Per table flush compression configuration
* Ability to default the table flush and compaction compression in the yaml."
CASSANDRA-15371,Incorrect messaging service version breaks in-JVM upgrade tests on trunk,"The in-JVM upgrade tests on trunk currently fail because the messaging
 version for internode messaging is selected as {{MessagingService.current_version}},
 a regression from the implementation in CASSANDRA-15078."
CASSANDRA-15368,Failing to flush Memtable without terminating process results in permanent data loss,"{{Memtable}} do not contain records that cover a precise contiguous range of {{ReplayPosition}}, since there are only weak ordering constraints when rolling over to a new {{Memtable}} - the last operations for the old {{Memtable}} may obtain their {{ReplayPosition}} after the first operations for the new {{Memtable}}.

Unfortunately, we treat the {{Memtable}} range as contiguous, and invalidate the entire range on flush.  Ordinarily we only invalidate records when all prior {{Memtable}} have also successfully flushed.  However, in the event of a flush that does not terminate the process (either because of disk failure policy, or because it is a software error), the later flush is able to invalidate the region of the commit log that includes records that should have been flushed in the prior {{Memtable}}

More problematically, this can also occur on restart without any associated flush failure, as we use commit log boundaries written to our flushed sstables to filter {{ReplayPosition}} on recovery, which is meant to replicate our {{Memtable}} flush behaviour above.  However, we do not know that earlier flushes have completed, and they may complete successfully out-of-order.  So any flush that completes before the process terminates, but began after another flush that _doesn’t_ complete before the process terminates, has the potential to cause permanent data loss.
"
CASSANDRA-15367,Memtable memory allocations may deadlock,"* Under heavy contention, we guard modifications to a partition with a mutex, for the lifetime of the memtable.
* Memtables block for the completion of all {{OpOrder.Group}} started before their flush began
* Memtables permit operations from this cohort to fall-through to the following Memtable, in order to guarantee a precise commitLogUpperBound
* Memtable memory limits may be lifted for operations in the first cohort, since they block flush (and hence block future memory allocation)

With very unfortunate scheduling
* A contended partition may rapidly escalate to a mutex
* The system may reach memory limits that prevent allocations for the new Memtable’s cohort (C2) 
* An operation from C2 may hold the mutex when this occurs
* Operations from a prior Memtable’s cohort (C1), for a contended partition, may fall-through to the next Memtable
* The operations from C1 may execute after the above is encountered by those from C2
"
CASSANDRA-15340,Resource leak in CompressedSequentialWriter,"In CompressedSequentialWriter, we reallocate the {{compressed}} buffer if the existing buffer is not large enough. These buffers are usually direct byte buffers, and we don't explicitly release their memory here, which delays release until the buffer is gc'd"
CASSANDRA-15327,Deleted data can re-appear if range movement streaming time exceeds gc_grace_seconds,"Hey,

We've come across a scenario in production (noticed on Cassandra 2.2.14) where data that is deleted from Cassandra at consistency {{ALL}} can be resurrected.  I've added a reproduction in a comment.

If a {{delete}} is issued during a range movement (i.e. bootstrap, decommission, move), and {{gc_grace_seconds}} is surpassed before the stream is finished, then the tombstones from the {{delete}} can be purged from the recipient node before the data is streamed. Once the move is complete, the data now exists on the recipient node without a tombstone.

We noticed this because our bootstrapping time occasionally exceeds our configured gc_grace_seconds, so we lose the consistency guarantee.  As an operator, it would be great to not have to worry about this edge case.

I've attached a patch that we have tested and successfully used in production, and haven't noticed any ill effects.  Happy to submit patches for more recent versions, I'm not sure how cleanly this will actually merge since there was some refactoring to this logic in 3.x."
CASSANDRA-15326,Unable to compute ceiling for max when histogram overflowed,"I have 9 cassandra nodes. When I create a keyspace that has 15 tables and the record count of all table are about 8.2 billion. I imported data through my java loaders, and I found out the system.log has error message. What happened and how can I solve the error?

Exception in thread Thread[CompactionExecutor:113041,1,main] 
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
	at org.apache.cassandra.utils.EstimatedHistogram.rawMean(EstimatedHistogram.java:231) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.utils.EstimatedHistogram.mean(EstimatedHistogram.java:220) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.io.sstable.metadata.StatsMetadata.getEstimatedDroppableTombstoneRatio(StatsMetadata.java:115) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.io.sstable.format.SSTableReader.getEstimatedDroppableTombstoneRatio(SSTableReader.java:1926) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:424) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:99) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:183) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:153) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:260) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_191]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_191]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_191]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_191]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.4.jar:3.11.4]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_191]"
CASSANDRA-15325,Unable to compute ceiling for max when histogram overflowed,"I have 9 cassandra nodes. When I create a keyspace that has 15 tables and the record count of all table are about 8.2 billion. I imported data through my java loaders, and I found out the system.log has error message. What happened and how can I solve the error?"
CASSANDRA-15323,MessagingServiceTest failed with method listenRequiredSecureConnectionWithBroadcastAddr    ON MAC OS  ,"when I do unit test on mac os for cassandra4.0 tag version , I found that the unit test failed when doing MessagingServiceTest  on method listenRequiredSecureConnectionWithBroadcastAddr. 

I found out that it is because that the mac os can not get connect to ip address 127.0.0.2 on default. 
so when you doing : ant test -Dtest.name=MessagingServiceTest -Dtest.methods=listenRequiredSecureConnectionWithBroadcastAddr

you can get a bind exception : can not assign request address. 
 !exception.png! 

what to do with it ,you can just set the 127.0.0.2  by :
sudo ifconfig lo0 alias 127.0.0.2 netmask 0xFFFFFFFF

then the unit can run successfully ."
CASSANDRA-15295,Running into deadlock when do CommitLog initialization,"Recently, I found a cassandra(3.11.4) node stuck in STARTING status for a long time.
 I used jstack to saw what happened. The main thread stuck in *AbstractCommitLogSegmentManager.awaitAvailableSegment*
 !screenshot-1.png! 

The strange thing is COMMIT-LOG-ALLOCATOR thread state was runnable but it was not actually running.  
 !screenshot-2.png! 

And then I used pstack to troubleshoot. I found COMMIT-LOG-ALLOCATOR block on java class initialization.
  !screenshot-3.png! 

This is a deadlock obviously. CommitLog waits for a CommitLogSegment when initializing. In this moment, the CommitLog class is not initialized and the main thread holds the class lock. After that, COMMIT-LOG-ALLOCATOR creates a CommitLogSegment with exception and call *CommitLog.handleCommitError*(static method).  COMMIT-LOG-ALLOCATOR will block on this line because CommitLog class is still initializing.



 

 "
CASSANDRA-15242,Race condition between flushing and compaction stalls compaction indefinitely,"Seen on Cassandra 3.11.4 with OpenJDK 8u212, although I've seen this a few times before, also on 3.11.3. It's a rare issue so I've not bothered with trying to trace it until now.
{noformat}
DEBUG [NativePoolCleaner] 2019-07-18 01:12:41,799 ColumnFamilyStore.java:1325 - Flushing largest CFS(Keyspace='keyspacename', ColumnFamily='tablename') to free up room. Used total: 0.10/0.33, live: 0.10/0.33, flushing: 0.00/0.00, this: 0.09/0.19
DEBUG [NativePoolCleaner] 2019-07-18 01:12:41,800 ColumnFamilyStore.java:935 - Enqueuing flush of tablename: 267.930MiB (9%) on-heap, 575.580MiB (19%) off-heap
DEBUG [PerDiskMemtableFlushWriter_0:204] 2019-07-18 01:12:42,480 Memtable.java:456 - Writing Memtable-tablename@498336646(520.721MiB serialized bytes, 870200 ops, 9%/19% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]
INFO  [Service Thread] 2019-07-18 01:12:43,616 GCInspector.java:284 - G1 Young Generation GC in 227ms.  G1 Eden Space: 14713618432 -> 0; G1 Old Gen: 13240876928 -> 13259198848; G1 Survivor Space: 276824064 -> 268435456;
INFO  [Service Thread] 2019-07-18 01:12:56,251 GCInspector.java:284 - G1 Young Generation GC in 206ms.  G1 Eden Space: 14713618432 -> 0; G1 Old Gen: 13259198848 -> 13285123456; G1 Survivor Space: 268435456 -> 285212672;
DEBUG [PerDiskMemtableFlushWriter_0:204] 2019-07-18 01:12:56,693 Memtable.java:485 - Completed flushing /cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db (524.023MiB) for commitlog position CommitLogPosition(segmentId=1563386911266, position=32127822)
DEBUG [MemtableFlushWriter:204] 2019-07-18 01:12:57,620 ColumnFamilyStore.java:1233 - Flushed to [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] (1 sstables, 518.714MiB), biggest 518.714MiB, smallest 518.714MiB
WARN  [CompactionExecutor:1617] 2019-07-18 01:12:57,628 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
{noformat}
This final line then starts repeating about once per minute:
{noformat}
WARN  [CompactionExecutor:1610] 2019-07-18 01:13:18,898 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1611] 2019-07-18 01:14:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1622] 2019-07-18 01:15:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1436] 2019-07-18 01:16:15,073 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1618] 2019-07-18 01:16:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1611] 2019-07-18 01:17:18,900 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1606] 2019-07-18 01:18:18,900 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1630] 2019-07-18 01:19:18,902 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1627] 2019-07-18 01:20:18,904 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1638] 2019-07-18 01:21:18,904 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1631] 2019-07-18 01:22:18,905 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1636] 2019-07-18 01:22:58,220 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1625] 2019-07-18 01:23:18,905 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.{noformat}
It will keep going like this for days, until restarted, but compaction won't run until then, so sstables pile up."
CASSANDRA-15214,Internode messaging catches OOMs and does not rethrow,"Netty (at least, and perhaps elsewhere in Executors) catches all exceptions, so presently there is no way to ensure that an OOM reaches the JVM handler to trigger a crash/heapdump.

It may be that the simplest most consistent way to do this would be to have a single thread spawned at startup that waits for any exceptions we must propagate to the Runtime.

We could probably submit a patch upstream to Netty, but for a guaranteed future proof approach, it may be worth paying the cost of a single thread."
CASSANDRA-15201,LegacyLayout does not handle paging states that cross a collection column,"{{LegacyLayout.decodeBound}} assumes there is only a single extra component, referring to a column name.  In fact an encoded page boundary may include a collection column, and this occurs as a matter of course when paging a table whose last column is a collection."
CASSANDRA-15176,Fix PagingState deserialization when the state was serialized using protocol version different from current session's,"3.0 and native protocol V4 introduced a change to how {{PagingState}} is serialized. Unfortunately that can break requests during upgrades: since paging states are opaque, it's possible for a client to receive a paging state encoded as V3 on a 2.1 node, and then send it to a 3.0 node on a V4 session. The version of the current session will be used to deserialize the paging state, instead of the actual version used to serialize it, and the request will fail.

This is obviously sub-optimal, but also avoidable. This JIRA fixes one half of the problem: 3.0 failing to deserialize 'mislabeled' paging states. We can do this by inspecting the byte buffer to verify if it's been indeed serialized with the protocol version used by the session, and if not, use the other method of deserialization.

It should be noted that we list this as a 'known limitation' somewhere, but really this is an upgrade-blocking bug for some users of C*."
CASSANDRA-15165,Reference-Reaper detected leak while running FramingTest unit test cases,"Reference-Reaper detected leak while running FramingTest unit test cases. Here are the leak details:

{code}
[junit-timeout] ERROR [Reference-Reaper] 2019-06-17 01:44:53,812 Ref.java:228 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@15460327) to @876994034 was not released before the reference was garbage collected
[junit-timeout] ERROR [Reference-Reaper] 2019-06-17 01:44:53,812 Ref.java:259 - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@15460327:
[junit-timeout] Thread[main,5,main]
[junit-timeout] 	at java.lang.Thread.getStackTrace(Thread.java:1559)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:249)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:179)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:101)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$Chunk.setAttachment(BufferPool.java:960)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$Chunk.set(BufferPool.java:1100)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$Chunk.get(BufferPool.java:1090)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGetInternal(BufferPool.java:721)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGet(BufferPool.java:706)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.get(BufferPool.java:656)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.access$000(BufferPool.java:535)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool.getAtLeast(BufferPool.java:129)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.sequenceOfMessages(FramingTest.java:413)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testRandomSequenceOfMessages(FramingTest.java:265)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testSomeMessages(FramingTest.java:259)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testRandomLegacy(FramingTest.java:243)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testRandomLegacy(FramingTest.java:234)
[junit-timeout] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit-timeout] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[junit-timeout] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout] 	at java.lang.reflect.Method.invoke(Method.java:498)
[junit-timeout] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
[junit-timeout] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
[junit-timeout] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
[junit-timeout] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
[junit-timeout] 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
[junit-timeout] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
[junit-timeout] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
[junit-timeout] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
[junit-timeout] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
[junit-timeout] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
[junit-timeout] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
[junit-timeout] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
[junit-timeout] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
[junit-timeout] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
[junit-timeout] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
[junit-timeout] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)
[junit-timeout] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:534)
[junit-timeout] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1196)
[junit-timeout] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1041)
{code}
“sliceAndConsume"" method of ShareableBytes increases reference count. Reference acquired testRandomSequenceOfMessages method is not released which is causing the leak."
CASSANDRA-15164,Overflowed Partition Cell Histograms Can Prevent Compactions from Executing,"Hi, we are running 6 node Cassandra cluster in production with 3 seed node but from last night one of our seed nodes is continuously throwing an error like this;-

cassandra.protocol.ServerError: <Error from server: code=0000 [Server error] message=""java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed"">

For a cluster to be up and running I Drained this node.

Can somebody help me out with this?

 

Any help or lead would be appreciated 

 

Note : We are using Cassandra version 3.7"
CASSANDRA-15163,"Untangle RepairMessage sub-hierarchy of messages, use new messaging (more) correctly","There is currently a nested sub-hierarchy of messages used by repair - in {{RepairMessage}} - all sharing one verb, all sharing one deserialiser, all sharing one verb handler. The first two can and should be addressed, and it’s mostly a mechanical task to do so. The last one should be tackled separately, when we get to refactor repair some day (and it could definitely use some love).

The proposed patch follows-up the work done in CASSANDRA-15066 and makes repair use the new messaging service in a more correct way, making verbs and deserialization more explicit. It wasn't included into the original commit so that to not make the single commit larger that it needed to be."
CASSANDRA-15140,Utest of Paging test occurs Runtime exceptions ,"when we run ant test of the trunk branch to make a full test of all unit test, there will occurs a RuntimeException. but when you do a specify ut for the PagingTest run :ant test -Dname=PagingTest, there 
will not be any exception here.
----
java.lang.RuntimeException: Unable to gossip with any peers
        at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1625)
        at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:552)
        at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:840)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:698)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:649)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:388)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:510)
        at org.apache.cassandra.service.EmbeddedCassandraService.start(EmbeddedCassandraService.java:50)
        at org.apache.cassandra.cql3.PagingTest.setup(PagingTest.java:63)

Here is the steps to reproduce the exceptions:
1. cd to the cassandra dir and switch to the trunk branch ;
2.ant test "
CASSANDRA-15126,Resource leak when queries apply a RowFilter,"RowFilter.CQLFilter optionally removes those partitions that have no matching results, but fails to close the iterator representing that partition’s unfiltered results, leaking resources when this happens."
CASSANDRA-15088,PagingTest failure on trunk,"[Example failure|https://circleci.com/gh/emolsson/cassandra/19]
{noformat}
java.lang.RuntimeException: Unable to gossip with any peers
	at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1546)
	at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:553)
	at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:841)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:699)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:650)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:379)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:501)
	at org.apache.cassandra.service.EmbeddedCassandraService.start(EmbeddedCassandraService.java:50)
	at org.apache.cassandra.cql3.PagingTest.setup(PagingTest.java:63)
{noformat}

Running the test case by itself won't reproduce the issue:
{noformat}
ant test -Dtest.name=PagingTest
{noformat}

But running it in parallel with other tests will:
{noformat}
ant test -Dtest.name=cql3/*
{noformat}

From the logs the following can be observed:
{noformat}
INFO  [main] 2019-04-11 15:32:29,916 Node configuration:[...seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1:7017}; storage_port=7027]

...

DEBUG [main] 2019-04-17 10:11:55,418 connection attempt 0 to 127.0.0.1:7044 (GOSSIP)
DEBUG [main] 2019-04-17 10:11:55,420 creating outbound bootstrap to peer 127.0.0.1:7044, compression: false, encryption: disabled, coalesce: DISABLED, protocolVersion: 12
DEBUG [ScheduledFastTasks:1] 2019-04-17 10:11:55,538 connection attempt 1 to 127.0.0.1:7044 (GOSSIP)
DEBUG [ScheduledFastTasks:1] 2019-04-17 10:11:55,538 creating outbound bootstrap to peer 127.0.0.1:7044, compression: false, encryption: disabled, coalesce: DISABLED, protocolVersion: 12
{noformat}

It seems like we have an offset issue of 10 between seed/storage_port.
The port 7044 does not seem to be defined anywhere though."
CASSANDRA-15078,Support cross version messaging in in-jvm upgrade dtests,
CASSANDRA-15069,Tombstone/Partition not purged after gc_grace_seconds,"During a tombstone purge (reducing gc_grace_seconds to zero and running `nodetool garbagecollect`), I noticed that when doing a dump of the SSTable, sometimes, there are a few partitions that do not get completely purged, even with gc_grace_seconds set to zero. I was able to replicate this in a small test dataset, from which I have attached the SSTable files and the schema to this ticket so that you can verify this as well. 
Doing a dump of the mc-51-big-Data.db file, you'll notice the following partition:

{
    ""partition"" : {
      ""key"" : [ ""96"" ],
      ""position"" : 285,
      ""deletion_info"" : { ""marked_deleted"" : ""2019-03-14T21:31:55.244490Z"", ""local_delete_time"" : ""2019-03-14T21:31:55Z"" }
    },
    ""rows"" : [ ]
  },

As you can see, the rows were removed correctly by the garbagecollect, but the partition record, continues there, and never goes away.
From the client side, no data is returned, so it's good there. But regardless of that, this partition should not be present in the SSTable file."
CASSANDRA-15066,Improvements to Internode Messaging,"CASSANDRA-8457 introduced asynchronous networking to internode messaging, but there have been several follow-up endeavours to improve some semantic issues.  CASSANDRA-14503 and CASSANDRA-13630 are the latest such efforts, and were combined some months ago into a single overarching refactor of the original work, to address some of the issues that have been discovered.  Given the criticality of this work to the project, we wanted to bring some more eyes to bear to ensure the release goes ahead smoothly.  In doing so, we uncovered a number of issues with messaging, some of which long standing, that we felt needed to be addressed.  This patch widens the scope of CASSANDRA-14503 and CASSANDRA-13630 in an effort to close the book on the messaging service, at least for the foreseeable future.

The patch includes a number of clarifying refactors that touch outside of the {{net.async}} package, and a number of semantic changes to the {{net.async}} packages itself.  We believe it clarifies the intent and behaviour of the code while improving system stability, which we will outline in comments below.

https://github.com/belliottsmith/cassandra/tree/messaging-improvements"
CASSANDRA-15065,Remove traces of dclocal_read_repair_chance from dtests as it is not used anymore,"This test as of now fails consistently because property ""dclocal_read_repair_chance"" does not exist anymore. It was marked as ""not used"" as part of CASSANDRA-13910 and it fails in current trunk in

 
 snitch_test.py TestDynamicEndpointSnitchtest multidatacenter_local_quorum 
 In test, it was set to 0.0 too so I am not sure what is the benefit of having that property there (even it is not recognised anymore) 
{code:java}
./src/java/org/apache/cassandra/schema/SchemaKeyspace.java:129: + ""dclocal_read_repair_chance double,"" // no longer used, left for drivers' sake
./src/java/org/apache/cassandra/schema/SchemaKeyspace.java:196: + ""dclocal_read_repair_chance double,"" // no longer used, left for drivers' sake
./src/java/org/apache/cassandra/schema/SchemaKeyspace.java:560: .add(""dclocal_read_repair_chance"", 0.0) // no longer used, left for drivers' sake{code}
{code:java}
E   cassandra.protocol.SyntaxException: <Error from server: code=2000 [Syntax error in CQL query] message=""Unknown property 'dclocal_read_repair_chance'"">{code}
 "
CASSANDRA-15061,"Dtests: tests are failing on too powerful machines, setting more memory per node in dtests","While running dtests on 32 cores and 64 GB of memory on c5.9xlarge some tests are failing because they are not able to handle the stress cassandra-stress is generating for them.

For all examples, there is e.g. this one (1) where we test that a cluster is able to cope with a boostrapping node. The problem is that node1 is bombed with cassandra-stress and it is eventually killed and test fails as such before even proceeding to test itself.

It was said to me that dtests in circleci are running in containers with 8 cores and 16GB or RAM and I simulated this on my machine (-Dcassandra.available_processors=8). The core problem is that nodes do not have enough memory - Xmx and Xms is set to only 512MB and that is very low figure and they are eventually killed.

Proposed solutions:

1) Run dtests on less powerful machines so it can not handle stress high enough so underlying nodes would be killed (rather strange idea)

2) Increase memory for node - this should be configurable, I saw that 1GB helps but there are still some timeouts, 2GB is better. 4GB would be the best.

3) Fix the test in such way it does not fail with 512MB.

 

1) is not viable to me, 3) takes a lot of time to go through and does not actually solve anything and it would be very cumbersome and clunky to go through all tests to set them like that. 2) seems to be the best approach but there is not any way I am aware of how to add more memory to every node all at once as node and cluster start / creation is scattered all over the project.

I have raised the issue here (2) too.

Do you guys think that if we manage to somehow fix this in CCM, we could introduce some switch / flag to dtests as how much memory a node in a cluster should run with?

(1) [https://github.com/apache/cassandra-dtest/blob/master/bootstrap_test.py#L419-L470]

(2) [https://github.com/riptano/ccm/issues/696]"
CASSANDRA-15059,Gossiper#markAlive can race with Gossiper#markDead,"The Gossiper class is not threadsafe and assumes all state changes happen in a single thread (the gossip stage). Gossiper#convict, however, can be called from the GossipTasks thread. This creates a race where calls to Gossiper#markAlive and Gossiper#markDead can interleave, corrupting gossip state. Gossiper#assassinateEndpoint has a similar problem, being called from the mbean server thread."
CASSANDRA-15027,Handle IR prepare phase failures less race prone by waiting for all results,"Handling incremental repairs as a coordinator begins by sending a {{PrepareConsistentRequest}} message to all participants, which may also include the coordinator itself. Participants will run anti-compactions upon receiving such a message and report the result of the operation back to the coordinator.

Once we receive a failure response from any of the participants, we fail-fast in {{CoordinatorSession.handlePrepareResponse()}}, which will in turn completes the {{prepareFuture}} that {{RepairRunnable}} is blocking on. Then the repair command will terminate with an error status, as expected.

The issue is that in case the node will both be coordinator and participant, we may end up with a local session and submitted anti-compactions, which will be executed without any coordination with the coordinator session (on same node). This may result in situations where running repair commands right after another, may cause overlapping execution of anti-compactions that will cause the following (misleading) message to show up in the logs and will cause the repair to fail again:
 ""Prepare phase for incremental repair session %s has failed because it encountered intersecting sstables belonging to another incremental repair session (%s). This is by starting an incremental repair session before a previous one has completed. Check nodetool repair_admin for hung sessions and fix them."""
CASSANDRA-15025,Avoid NPE in RepairRunnable.recordFailure,"failureMessage parameter in {{RepairRunnable.recordFailure}} can be null, avoid this happening and make sure we log the actual exception"
CASSANDRA-15009,In-JVM Testing tooling for paging,Add distributed pager to in-jvm distributed tests to allow realistic pager tests.
CASSANDRA-15002,Avoid leaking threads when anticompaction fails,"If anticompaction fails on a node, a message is sent to all repair participants that this session is failed. If the other participants successfully finish their anticompactions we will not shut down the executor in `LocalSessions` since we can't change the state from ""FAILED"" to ""PREPARED"" and throw exception before calling shutdown."
CASSANDRA-14953,Failed to reclaim the memory and too many MemtableReclaimMemory pending task,"We found that Cassandra has a lot of write accumulation in the production environment, and our business has experienced a lot of write failures.
 Through the system.log, it was found that MemtableReclaimMemory was pending at the beginning, and then a large number of MutationStage stacks appeared at a certain moment.
 Finally, the heap memory is full, the GC time reaches tens of seconds, the node status is DN through nodetool, but the Cassandra process is still running.We killed the node and restarted the node, and the above situation disappeared.

 

Also the number of Active MemtableReclaimMemory threads seems to stay at 1.

(you can see the 1.PNG)

a large number of MutationStage stacks appeared at a certain moment.

(you can see the 2.PNG)

 

long GC time:

 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Old Generation GC in 87121ms. G1 Old Gen: 51175946656 -> 50082999760;
 - MutationStage 128 11931622 1983820772 0 0
 - CounterMutationStage 0 0 0 0 0
 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Young Generation GC in {color:#FF0000}969ms{color}. G1 Eden Space: 1090519040 -> 0; G1 Old Gen: 50082999760 -> 51156741584;
 - MutationStage 128 11953653 1983820772 0 0
 - CounterMutationStage 0 0 0 0 0
 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Old Generation GC in {color:#FF0000}84785ms{color}. G1 Old Gen: 51173518800 -> 50180911432;
 - MutationStage 128 11967484 1983820772 0 0
 - CounterMutationStage 0 0 0 0 0
 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Young Generation GC in 611ms. G1 Eden Space: 989855744 -> 0; G1 Old Gen: 50180911432 -> 51153989960;
 - MutationStage 128 11975849 1983820772 0 0
 - CounterMutationStage 0 0 0 0 0
 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Old Generation GC in {color:#FF0000}85845ms{color}. G1 Old Gen: 51170767176 -> 50238295416;
 - MutationStage 128 11978192 1983820772 0 0
 - CounterMutationStage 0 0 0 0 0
 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Young Generation GC in 602ms. G1 Eden Space: 939524096 -> 0; G1 Old Gen: 50238295416 -> 51161042296;
 - MutationStage 128 11994295 1983820772 0 0
 - CounterMutationStage 0 0 0 0 0
 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Old Generation GC in {color:#FF0000}85307ms{color}. G1 Old Gen: 51177819512 -> 50288829624; Metaspace: 36544536 -> 36525696
 - MutationStage 128 12001932 1983820772 0 0
 - CounterMutationStage 0 0 0 0 0
66 - MutationStage 128 12004395 1983820772 0 0
66 - CounterMutationStage 0 0 0 0 0
 - MemtableReclaimMemory 1 156 24565 0 0
66 - MemtableReclaimMemory 1 156 24565 0 0
 - G1 Young Generation GC in 610ms. G1 Eden Space: 889192448 -> 0; G1 Old Gen: 50288829624 -> 51178022072;
 - MutationStage 128 12023677 1983820772 0 0

Why is this happening? "
CASSANDRA-14952,NPE when using allocate_tokens_for_keyspace and add new DC,"Received following NPE while bootstrapping very first node in the new datacenter with {{allocate_tokens_for_keyspace}} yaml option
{code:java}
INFO  21:44:13 JOINING: getting bootstrap token
Exception (java.lang.NullPointerException) encountered during startup: null
java.lang.NullPointerException
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.getStrategy(TokenAllocation.java:208)
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.getStrategy(TokenAllocation.java:170)
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.allocateTokens(TokenAllocation.java:55)
	at org.apache.cassandra.dht.BootStrapper.allocateTokens(BootStrapper.java:206)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:173)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:854)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:579)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:351)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:586)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:714)

{code}
Please find reproducible steps here:
 1. Set {{allocate_tokens_for_keyspace}} property with {{Networktopologystrategy}} say {{{{Networktopologystrategy, 'dc1' : 1, 'dc2' : 1}}}}
 2. Start first node in {{dc1}}
 3. Now bootstrap second node in {{dc2,}} it will throw above exception.

RCA:
 [doAddEndpoint|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/locator/TokenMetadata.java#L1325] is invoked from the [bootstrap|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L1254] and at this time [local node's rack information|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/locator/TokenMetadata.java#L1276] is available

However with have {{allocate_tokens_for_keyspace}} option, daemon tries to access rack information even before calling [bootstrap|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L1241] function, at [this place|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L878] which results in NPE

Fix:
 Since this is applicable to only very first node for new dc, we can check for {{null}} as:
{code:java}
diff --git a/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java b/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
index 8d8a6ffeca..e162757d95 100644
--- a/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
+++ b/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
@@ -205,7 +205,11 @@ public class TokenAllocation
         final int replicas = rs.getReplicationFactor(dc);
 
         Topology topology = tokenMetadata.getTopology();
-        int racks = topology.getDatacenterRacks().get(dc).asMap().size();
+        int racks = 1;
+        if (topology.getDatacenterRacks().get(dc) != null)
+        {
+            racks = topology.getDatacenterRacks().get(dc).asMap().size();
+        }
 
         if (racks >= replicas)
         {
{code}
Let me know your comments."
CASSANDRA-14920,Some comparisons used for verifying paging queries in dtests only test the column names and not values,"The implementation of {{PageAssertionMixin::assertEqualIgnoreOrder}} introduced in CASSANDRA-14134 can't be used to compare expected and actual results when the row data is represented by a {{dict}}. The underlying {{list_to_hashed_dict}} function expected lists of values and when it encounters a dict, it constructs its normalized list using only the keys. So the actual result values may be completely wrong, but as long as the field names are the same the equality check will pass. This affects only {{paging_tests.py}} and {{upgrade_tests/paging_test.py}}, and looks like it maybe a leftover from an earlier dev iteration, as some tests in the affected fixtures are already using the alternative (and correct) {{assertions.py::assert_lists_equal_ignoring_order}}.
"
CASSANDRA-14919,Regression in paging queries in mixed version clusters ,"The changes to handling legacy bounds in CASSANDRA-14568/CASSANDRA-14749/CASSANDRA-14912 break paging queries where the coordinator is a legacy node and the replica is an upgraded node. 

The long-held assumption made by {{LegacyLayout::decodeBound}} that ""There can be more components than the clustering size only in the case this is the bound of a collection range tombstone."" is not true as serialized paged read commands may also include these type of bounds in their {{SliceQueryFilter}}. The additional checks the more recent tickets add cause such queries to error when processed by a 3.0 replica."
CASSANDRA-14909,Netty IOExceptions caused by unclean client disconnects being logged at INFO instead of TRACE,"Observed spam logs on 3.0.17 cluster with redundant Netty IOExceptions caused due to client-side disconnections.

{code:java}
INFO  [epollEventLoopGroup-2-28] 2018-11-20 23:23:04,386 Message.java:619 - Unexpected exception during request; channel = [id: 0x12995bc1, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xxx.xxx:33754]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
	at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}


{code:java}
INFO  [epollEventLoopGroup-2-23] 2018-11-20 13:16:33,263 Message.java:619 - Unexpected exception during request; channel = [id: 0x98bd7c0e, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xx.xx:33350]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
	at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

[CASSANDRA-7849|https://issues.apache.org/jira/browse/CASSANDRA-7849] addresses this for JAVA IO Exception like ""java.io.IOException: Connection reset by peer"", but not for Netty IOException since the exception message in Netty includes method name."
CASSANDRA-14900,DigestMismatchException log messages should be at TRACE,"DigestMismatchException log messages should probably be at TRACE. These log messages about normal digest mismatches that include scary stacktraces:

{noformat}
DEBUG [ReadRepairStage:40] 2017-10-24 19:45:50,349  ReadCallback.java:242 - Digest mismatch:
org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(-786225366477494582, 31302e33322e37382e31332d6765744469736b5574696c50657263656e742d736463) (943070f62d72259e3c25be0c6f76e489 vs f4c7c7675c803e0028992e11e0bbc5a0)
        at org.apache.cassandra.service.DigestResolver.compareResponses(DigestResolver.java:92) ~[cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.run(ReadCallback.java:233) ~[cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{noformat}"
CASSANDRA-14884,"Move TWCS message ""No compaction necessary for bucket size"" to Trace level","When using TWCS, this message sometimes spams the debug logs:

DEBUG [CompactionExecutor:4993|https://datastax.jira.com/wiki/display/CompactionExecutor/4993] 2018-04-20 00:41:13,795 TimeWindowCompactionStrategy.java:304 - No compaction necessary for bucket size 1 , key 1521763200000, now 1524182400000

The similar message is already at trace level for LCS, so this patch changes the message from TWCS to trace as well."
CASSANDRA-14878,Race condition when setting bootstrap flags,"{{StorageService#bootstrap()}} is supposed to wait for bootstrap to finish, but Guava calls the future listeners [after|https://github.com/google/guava/blob/ec2dedebfa359991cbcc8750dc62003be63ec6d3/guava/src/com/google/common/util/concurrent/AbstractFuture.java#L890] unparking its waiters, which causes a race on when the {{bootstrapFinished()}} will be executed, making it non-deterministic."
CASSANDRA-14855,"Message Flusher scheduling fell off the event loop, resulting in out of memory","We recently had a production issue where about 10 nodes in a 96 node cluster ran out of heap. 

From heap dump analysis, I believe there is enough evidence to indicate `queued` data member of the Flusher got too big, resulting in out of memory.
Below are specifics on what we found from the heap dump (relevant screenshots attached):
* non-empty ""queued"" data member of Flusher having retaining heap of 0.5GB, and multiple such instances.
* ""running"" data member of Flusher having ""true"" value
* Size of scheduledTasks on the eventloop was 0.

We suspect something (maybe an exception) caused the Flusher running state to continue to be true, but was not able to schedule itself with the event loop.
Could not find any ERROR in the system.log, except for following INFO logs around the incident time.


{code:java}
INFO [epollEventLoopGroup-2-4] 2018-xx-xx xx:xx:xx,592 Message.java:619 - Unexpected exception during request; channel = [id: 0x8d288811, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.x.xx:18886]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
 at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

I would like to pursue the following proposals to fix this issue:
# ImmediateFlusher: Backport trunk's ImmediateFlusher ( [CASSANDRA-13651|https://issues.apache.org/jira/browse/CASSANDRA-13651] https://github.com/apache/cassandra/commit/96ef514917e5a4829dbe864104dbc08a7d0e0cec)  to 3.0.x and maybe to other versions as well, since ImmediateFlusher seems to be more robust than the existing Flusher as it does not depend on any running state/scheduling.
# Make ""queued"" data member of the Flusher bounded to avoid any potential of causing out of memory due to otherwise unbounded nature.


"
CASSANDRA-14841,"Don't write to system_distributed.repair_history, system_traces.sessions, system_traces.events in mixed version 3.X/4.0 clusters","When upgrading from 3.x to 4.0 I get exceptions in the old nodes once the first 4.0 node starts up. I have tested to upgrade from both 3.0.15 and 3.11.3 and get the same problem.

 
{noformat}
2018-10-22T11:12:05.060+0200 ERROR [MessagingService-Incoming-/10.216.193.244] CassandraDaemon.java:228 Exception in thread Thread[MessagingService-Incoming-/10.216.193.244,5,main]
java.lang.RuntimeException: Unknown column coordinator_port during deserialization
at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:452) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.filter.ColumnFilter$Serializer.deserialize(ColumnFilter.java:482) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:760) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:697) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.3.jar:3.11.3]{noformat}
I think it was introduced by CASSANDRA-7544.

 "
CASSANDRA-14834,Avoid keeping StreamingTombstoneHistogramBuilder.Spool in memory during the whole compaction,Since CASSANDRA-13444 {{StreamingTombstoneHistogramBuilder.Spool}} is allocated to keep around an array with 131072 * 2 * 2 integers *per written sstable* during the whole compaction. With LCS at times creating 1000s of sstables during a compaction it kills the node.
CASSANDRA-14807,Avoid querying “self” through messaging service when collecting full data during read repair,"Currently, when collecting full requests during read-repair, we go through the messaging service instead of executing the query locally.

||[patch|https://github.com/apache/cassandra/pull/278]||[dtest-patch|https://github.com/apache/cassandra-dtest/pull/39]||

|[utest|https://circleci.com/gh/ifesdjeen/cassandra/641]|[dtest-vnode|https://circleci.com/gh/ifesdjeen/cassandra/640]|[dtest-novnode|https://circleci.com/gh/ifesdjeen/cassandra/639]|"
CASSANDRA-14775,StreamingTombstoneHistogramBuilder overflows if > 2B in a single bucket/sstable,"This may be unlikely, but is certainly not impossible.  In this event, the count for the bucket will be reset to zero, and the time distorted to 1s in the future.  If MAX_DELETION_TIME were encountered through overflow, this might result in a bucket with NO_DELETION_TIME.
"
CASSANDRA-14773,Overflow of 32-bit integer during compaction.,"In scope of CASSANDRA-13444 the compaction was significantly improved from CPU and memory perspective. Hovewer this improvement introduces the bug in rounding. When rounding the expriration time which is close to  *Cell.MAX_DELETION_TIME*(it is just *Integer.MAX_VALUE*) the math overflow happens(because in scope of -CASSANDRA-13444-) data type for point was changed from Long to Integer in order to reduce memory footprint), as result point became negative and acts as silent poison for internal structures of StreamingTombstoneHistogramBuilder like *DistanceHolder* and *DataHolder*. Then depending of point intervals:
 * The TombstoneHistogram produces wrong values when interval of points is less then binSize, it is not critical.
 * Compaction crashes with ArrayIndexOutOfBoundsException if amount of point intervals is great then  binSize, this case is very critical.

 

This is pull request [https://github.com/apache/cassandra/pull/273] that reproduces the issue and provides the fix. 

 

The stacktrace when running(on codebase without fix) *testMathOverflowDuringRoundingOfLargeTimestamp* without -ea JVM flag
{noformat}
java.lang.ArrayIndexOutOfBoundsException
at java.lang.System.arraycopy(Native Method)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$DistanceHolder.add(StreamingTombstoneHistogramBuilder.java:208)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushValue(StreamingTombstoneHistogramBuilder.java:140)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$$Lambda$1/1967205423.consume(Unknown Source)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$Spool.forEach(StreamingTombstoneHistogramBuilder.java:574)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushHistogram(StreamingTombstoneHistogramBuilder.java:124)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.build(StreamingTombstoneHistogramBuilder.java:184)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilderTest.testMathOverflowDuringRoundingOfLargeTimestamp(StreamingTombstoneHistogramBuilderTest.java:183)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
at org.junit.runner.JUnitCore.run(JUnitCore.java:159)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)

{noformat}
 

The stacktrace when running(on codebase without fix)  *testMathOverflowDuringRoundingOfLargeTimestamp* with enabled -ea JVM flag
{noformat}
ava.lang.AssertionError: point2 should follow point1

at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$DataHolder.merge(StreamingTombstoneHistogramBuilder.java:324)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.mergeBin(StreamingTombstoneHistogramBuilder.java:158)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushValue(StreamingTombstoneHistogramBuilder.java:145)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$$Lambda$1/1967205423.consume(Unknown Source)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder$Spool.forEach(StreamingTombstoneHistogramBuilder.java:574)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.flushHistogram(StreamingTombstoneHistogramBuilder.java:124)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilder.build(StreamingTombstoneHistogramBuilder.java:184)
at org.apache.cassandra.utils.streamhist.StreamingTombstoneHistogramBuilderTest.testMathOverflowDuringRoundingOfLargeTimestamp(StreamingTombstoneHistogramBuilderTest.java:183)

{noformat}
 "
CASSANDRA-14764,"Test Messaging Refactor with: 12 Node Breaking Point, compression=none, encryption=none, coalescing=off","*Setup:*
 * Cassandra: 12 (2*6) node i3.xlarge AWS instance (4 cpu cores, 30GB ram) running cassandra trunk off of jasobrown/14503 jdd7ec5a2 (Jasons patched internode messaging branch) vs the same footprint running 3.0.17
 * Two datacenters with 100ms latency between them
 * No compression, encryption, or coalescing turned on

*Test #1:*

ndbench sent 1.5k QPS at a coordinator level to one datacenter (RF=3*2 = 6 so 3k global replica QPS) of 4kb single partition BATCH mutations at LOCAL_ONE. This represents about 250 QPS per coordinator in the first datacenter or 60 QPS per core. The goal was to observe P99 write and read latencies under various QPS.

*Result:*

The good news is since the CASSANDRA-14503 changes, instead of keeping the mutations on heap we put the message into hints instead and don't run out of memory. The bad news is that the {{MessagingService-NettyOutbound-Thread's}} would occasionally enter a degraded state where they would just spin on a core. I've attached flame graphs showing the CPU state as [~jasobrown] applied fixes to the {{OutboundMessagingConnection}} class.


 *Follow Ups:*
[~jasobrown] has committed a number of fixes onto his {{jasobrown/14503-collab}} branch including:
1. Limiting the amount of time spent dequeuing messages if they are expired (previously if messages entered the queue faster than we could dequeue them we'd just inifinte loop on the consumer side)
2. Don't call {{dequeueMessages}} from within {{dequeueMessages}} created callbacks.

We're continuing to use CPU flamegraphs to figure out where we're looping and fixing bugs as we find them."
CASSANDRA-14742,Race Condition in batchlog replica collection,"When we collect nodes for it in {{StorageProxy#getBatchlogReplicas}}, we already filter out down replicas; subsequently they get picked up and taken for liveAndDown.

There's a possible race condition due to picking tokens from token metadata twice (once in {{StorageProxy#getBatchlogReplicas}} and second one in {{ReplicaPlan#forBatchlogWrite}})"
CASSANDRA-14710,Use quilt to patch cassandra.in.sh in Debian packaging,"While working on CASSANDRA-14707, I found the debian/cassandra.in.sh file is outdated and is missing some elements from bin/cassandra.in.sh. This should not be a separately maintained file, so let's use quilt to patch the few bits that need to be updated on Debian package installations.
 * rm debian/cassandra.in.sh
 * create quilt patch for path updates needed
 * update debian/cassandra.install to install our patched bin/cassandra.in.sh"
CASSANDRA-14681,SafeMemoryWriterTest doesn't compile on trunk,"{{SafeMemoryWriterTest}} references {{sun.misc.VM}}, which doesn't exist in Java 11, so the build fails.

Proposed patch makes the test work against Java 8 + 11."
CASSANDRA-14601,[dtest] test_failure_threshold_deletions - paging_test.TestPagingWithDeletions,"failing dtest on 3.11 only. Error output from pytest:

{noformat}
        except ReadFailure as exc:
            if supports_v5_protocol:
>               assert exc.error_code_map is not None
E               assert None is not None
E                +  where None = ReadFailure('Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 r...d 2 failures"" info={\'consistency\': \'ALL\', \'required_responses\': 2, \'received_responses\': 0, \'failures\': 2}',).error_code_map

paging_test.py:3447: AssertionError
{noformat}
"
CASSANDRA-14589,CommitLogReplayer.handleReplayError should print stack traces ,"handleReplayError does not accept an explicit Throwable parameter, so callers only integrate the exception’s message text into the log entry.  This means a loss of debug information for operators.

Note, this was fixed by CASSANDRA-8844 for 3.x+, only 3.0.x is affected."
CASSANDRA-14571,Fix race condition in MV build/propagate when there is existing data in the base table,"CASSANDRA-13426 exposed a race in MV initialisation and building, which now breaks, consistently, {{materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows}}.

CASSANDRA-14168 is also directly related."
CASSANDRA-14558,dtest: log-watching thread leak and thread starvation,"We get occasional build timeouts on b.a.o after pytest becomes unresponsive for over 20 minutes. This will result in a thread dump like this one:

[https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-2.2-dtest/117/consoleFull]

If you look for ""Log-watching thread starting"" messages and the dump, it becomes quite obvious whats the issue here.

I had a quick look at the python3 / pytest related changes in CASSANDRA-14134 and it seems that some of the handling around dtest_setup's {{log_watch_thread}} var has been changed in a way that would prevent eventually yielding the allocated thread."
CASSANDRA-14540,Internode messaging handshake sends wrong messaging version number,"With the refactor of internode messaging to netty in 4.0, we abstracted the protocol handshakes messages into a class and handlers. There is a bug when the initiator of the connection sends, in the third message of the handshake, it's own default protocol version number ({{MessagingService.current_version}}), rather than the negotiated version. This was not causing any obvious problems when CASSANDRA-8457 was initially committed, but the bug is exposed after CASSANDRA-7544. The problem is during rolling upgrades of 3.0/3.X to 4.0, nodes cannot correctly connect. "
CASSANDRA-14529,nodetool import row cache invalidation races with adding sstables to tracker,"CASSANDRA-6719 introduced {{nodetool import}} with row cache invalidation, which [occurs before adding new sstables to the tracker|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SSTableImporter.java#L137-L178]. Stale reads will result after a read is interleaved with the read row's invalidation and adding the containing file to the tracker.  "
CASSANDRA-14528,Provide stacktraces for various error logs,"We should reintroduce some stack traces that have gone missing since CASSANDRA-13723 (ba87ab4e954ad2). The cleanest way would probably to use {{String.format}} for any custom messages, e.g. {{logger.error(String.format(""Error using param {}"", param), e)}}, so we make this more implicit and robust for coming api changes."
CASSANDRA-14507,OutboundMessagingConnection backlog is not fully written in case of race conditions,"The {{OutboundMessagingConnection}} writes into a backlog queue before the connection handshake is successfully completed, and then writes such backlog to the channel as soon as the successful handshake moves the channel state to {{READY}}.
This is unfortunately race prone, as the following could happen:
1) One or more writer threads see the channel state as {{NOT_READY}} in {{#sendMessage()}} and are about to enqueue to the backlog, but they get descheduled by the OS.
2) The handshake thread is scheduled by the OS and moves the channel state to {{READY}}, emptying the backlog.
3) The writer threads are scheduled back and add to the backlog, but the channel state is {{READY}} at this point, so those writes would sit in the backlog and expire.

Please note a similar race condition exists between {{OutboundMessagingConnection#sendMessage()}} and {{MessageOutHandler#channelWritabilityChanged()}}, which is way more serious as the channel writability could frequently change, luckily it looks like {{ChannelWriter#write()}} never gets invoked with {{checkWritability}} at {{true}} (so writes never go to the backlog when the channel is not writable)."
CASSANDRA-14503,Internode connection management is race-prone,"Following CASSANDRA-8457, internode connection management has been rewritten to rely on Netty, but the new implementation in {{OutboundMessagingConnection}} seems quite race prone to me, in particular on those two cases:

* {{#finishHandshake()}} racing with {{#close()}}: i.e. in such case the former could run into an NPE if the latter nulls the {{channelWriter}} (but this is just an example, other conflicts might happen).
* Connection timeout and retry racing with state changing methods: {{connectionRetryFuture}} and {{connectionTimeoutFuture}} are cancelled when handshaking or closing, but there's no guarantee those will be actually cancelled (as they might be already running), so they might end up changing the connection state concurrently with other methods (i.e. by unexpectedly closing the channel or clearing the backlog).

Overall, the thread safety of {{OutboundMessagingConnection}} is very difficult to assess given the current implementation: I would suggest to refactor it into a single-thread model, where all connection state changing actions are enqueued on a single threaded scheduler, so that state transitions can be clearly defined and checked."
CASSANDRA-14488,Avoid unneeded memory allocations / cpu for disabled log levels,"add debug and trace log guard when the parameters creation implies memory allocations and / or cpu.

Especially for StreamingInboundHandler/NettyStreamingMessageSender where createLogTag is allocating 64*2+UUID#toString bytes."
CASSANDRA-14485,Optimize internode messaging protocol,"There's some dead wood and places for optimization in the internode messaging protocol. Currently, we include the sender's \{{IPAddressAndPort}} in *every* internode message, even though we already sent that in the handshake that established the connection/session. Further, there are several places where we can use vints instead of a fixed, 4-byte integer value- especially as those values will almost always be less than one byte."
CASSANDRA-14474,Cassandra going down with `java.lang.OutOfMemoryError: Map failed`,"From last couple of days, we are seeing this error in production and causing to down the dse/cassandra. 
DSE-5.1.2 & APACHE CASSANDRA-3.1.11

ERROR [COMMIT-LOG-ALLOCATOR] 2018-05-29 04:16:33,582 JVMStabilityInspector.java:142 - JVM state determined to be unstable. Exiting forcefully due to:
java.lang.OutOfMemoryError: Map failed
at sun.nio.ch.FileChannelImpl.map0(Native Method) ~[na:1.8.0_121]
at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:937) ~[na:1.8.0_121]
at org.apache.cassandra.db.commitlog.MemoryMappedSegment.createBuffer(MemoryMappedSegment.java:56) ~[cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:170) ~[cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at org.apache.cassandra.db.commitlog.MemoryMappedSegment.<init>(MemoryMappedSegment.java:45) ~[cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at org.apache.cassandra.db.commitlog.CommitLogSegment.createSegment(CommitLogSegment.java:124) ~[cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard.createSegment(CommitLogSegmentManagerStandard.java:78) ~[cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1.runMayThrow(AbstractCommitLogSegmentManager.java:108) ~[cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) [cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [cassandra-all-3.11.0.1758.jar:3.11.0.1758]
at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
INFO [main] 2018-05-29 04:39:26,515 DseModule.java:90 - Loading DSE module
INFO [main] 2018-05-29 04:39:26,734 YamlConfigurationLoader.java:89 - Configuration location: file:/etc/dse/cassandra/cassandra.yaml
INFO [main] 2018-05-29 04:39:27,039 DseConfigYamlLoader.java:38 - Loading settings from file:/etc/dse/dse.yaml
INFO [main] 2018-05-29 04:39:27,137 DseConfig.java:382 - Load of settings is done.
INFO [main] 2018-05-29 04:39:27,138 YamlConfigurationLoader.java:89 - Configuration location: file:/etc/dse/cassandra/cassandra.yaml"
CASSANDRA-14472,Too many LEAK DETECTED errors in the logs.,"We are seeing too many leak detected errors in the system log (Cassandra 3.11.0 & dse (5.1.2).

```

ERROR [Reference-Reaper:1] 2018-05-28 21:59:42,783  Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@733e48f6) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1977222631:Memory@[7dda86406000..7dda86596000) was not released before the reference was garbage collected

 

 

 

 

ERROR [Reference-Reaper:1] 2018-05-28 22:02:13,183  Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@463fffb6) to class org.apache.cassandra.io.util.FileHandle$Cleanup@304923348:/app/db/data2/odp_raw/rolling_device_usage_hour-4afe7060339611e897fbafb645229968/mc-152546-big-Index.db was not released before the reference was garbage collected

ERROR [Reference-Reaper:1] 2018-05-28 22:02:13,183  Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3071c683) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@847483493:Memory@[7dda8d406000..7dda8d596000) was not released before the reference was garbage collected

ERROR [Reference-Reaper:1] 2018-05-28 22:02:13,183  Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@bf809cc) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@2142654469:[Memory@[0..84), Memory@[0..528)] was not released before the reference was garbage collected

 ```"
CASSANDRA-14444,Got NPE when querying Cassandra 3.11.2,"We just upgraded our Cassandra cluster from 2.2.6 to 3.11.2

After upgrading, we immediately got exceptions in Cassandra like this one: 

 
{code}
ERROR [Native-Transport-Requests-1] 2018-05-11 17:10:21,994 QueryMessage.java:129 - Unexpected error during query
java.lang.NullPointerException: null
at org.apache.cassandra.dht.RandomPartitioner.getToken(RandomPartitioner.java:248) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.dht.RandomPartitioner.decorateKey(RandomPartitioner.java:92) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.config.CFMetaData.decorateKey(CFMetaData.java:666) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.service.pager.PartitionRangeQueryPager.<init>(PartitionRangeQueryPager.java:44) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.db.PartitionRangeReadCommand.getPager(PartitionRangeReadCommand.java:268) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.statements.SelectStatement.getPager(SelectStatement.java:475) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:288) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.2.jar:3.11.2]
at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.2.jar:3.11.2]
at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]
{code}
 

The table schema is like:
{code}
CREATE TABLE example.example_table (
 id bigint,
 hash text,
 json text,
 PRIMARY KEY (id, hash)
) WITH COMPACT STORAGE
{code}
 

The query is something like:
{code}
""select * from example.example_table;"" // (We do know this is bad practise, and we are trying to fix that right now)
{code}
with fetch-size as 200, using DataStax Java driver. 

This table contains about 20k rows. 

 

Actually, the fix is quite simple, 

 
{code}
--- a/src/java/org/apache/cassandra/service/pager/PagingState.java
+++ b/src/java/org/apache/cassandra/service/pager/PagingState.java
@@ -46,7 +46,7 @@ public class PagingState

public PagingState(ByteBuffer partitionKey, RowMark rowMark, int remaining, int remainingInPartition)
 {
- this.partitionKey = partitionKey;
+ this.partitionKey = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
 this.rowMark = rowMark;
 this.remaining = remaining;
 this.remainingInPartition = remainingInPartition;
{code}
 

""partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;"" is in 2.2.6 and 2.2.8. But it was removed for some reason. 

The interesting part is that, we have: 
{code}
public final ByteBuffer partitionKey; // Can be null for single partition queries.
{code}
It seems ""partitionKey"" could be null.

Thanks a lot. 

 

 

 "
CASSANDRA-14426,cassandra-stress throws NPE if insert section isn't specified in user profile,"When user profile file is used, and insert section isn't specified, then cassandra-stress is using default values instead.

Since support for LWTs was added, absence of the insert section lead to throwing of NullPointerException when generating inserts:

{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.stress.StressProfile.getInsert(StressProfile.java:546)
	at org.apache.cassandra.stress.StressProfile.printSettings(StressProfile.java:126)
	at org.apache.cassandra.stress.settings.StressSettings.lambda$printSettings$1(StressSettings.java:311)
	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.apache.cassandra.stress.settings.StressSettings.printSettings(StressSettings.java:311)
	at org.apache.cassandra.stress.Stress.run(Stress.java:108)
	at org.apache.cassandra.stress.Stress.main(Stress.java:63)
{noformat}

Fix is trivial, and will be provided as PR"
CASSANDRA-14385,Fix Some Potential NPE ,"We have developed a static analysis tool [NPEDetector|https://github.com/lujiefsi/NPEDetector] to find some potential NPE. Our analysis shows that some callees may return null in corner case(e.g. node crash , IO exception), some of their callers have  _!=null_ check but some do not have. In this issue we post a patch which can add  !=null  based on existed !=null  check. For example:

Calle Schema#getView may return null:
{code:java}
public ViewMetadata getView(String keyspaceName, String viewName)
{
    assert keyspaceName != null;
    KeyspaceMetadata ksm = keyspaces.getNullable(keyspaceName);
    return (ksm == null) ? null : ksm.views.getNullable(viewName);//may return null
}
{code}
 it have 4 callers, 3 of them have !=null check, like its caller MigrationManager#announceViewDrop have !=null check()
{code:java}
public static void announceViewDrop(String ksName, String viewName, boolean announceLocally) throws ConfigurationException
{
   ViewMetadata view = Schema.instance.getView(ksName, viewName);
    if (view == null)//null pointer checker
        throw new ConfigurationException(String.format(""Cannot drop non existing materialized view '%s' in keyspace '%s'."", viewName,     ksName));
   KeyspaceMetadata ksm = Schema.instance.getKeyspaceMetadata(ksName);

   logger.info(""Drop table '{}/{}'"", view.keyspace, view.name);
   announce(SchemaKeyspace.makeDropViewMutation(ksm, view, FBUtilities.timestampMicros()), announceLocally);
}
{code}
but caller MigrationManager#announceMigration does not have 

We add !=null check based on MigrationManager#announceViewDrop:
{code:java}
if (current == null)
    throw new InvalidRequestException(""There is no materialized view in keyspace "" + keyspace());
{code}
But due to we are not very  familiar with CASSANDRA, hope some expert can review it.

Thanks!!!!

 "
CASSANDRA-14370,Reduce level of log from debug to trace in CommitLogSegmentManager.java,"[{{AbstractCommitLogSegmentManager.java:112}}|https://github.com/apache/cassandra/blob/2402acd47e3bb514981cde742b7330666c564869/src/java/org/apache/cassandra/db/commitlog/AbstractCommitLogSegmentManager.java#L112]
It's changed to trace() in cassandra-3.0 with CASSANDRA-10241:https://github.com/pauloricardomg/cassandra/commit/3ef1b18fa76dce7cd65b73977fc30e51301f3fed#diff-d07279710c482983e537aed26df80400

but not in cassandra-3.11 and trunk. I think it makes sense to make them consistent and downgrade to {{trace()}}."
CASSANDRA-14318,Fix query pager DEBUG log leak causing hit in paged reads throughput,"Debug logging can involve in many cases (especially very low latency ones) a very important overhead on the read path in 2.2 as we've seen when upgrading clusters from 2.0 to 2.2.

The performance impact was especially noticeable on the client side metrics, where p99 could go up to 10 times higher, while ClientRequest metrics recorded by Cassandra didn't show any overhead.

Below shows latencies recorded on the client side with debug logging on first, and then without it :

!debuglogging.png!  

We generated a flame graph before turning off debug logging that shows the read call stack is dominated by debug logging : 

!flame_graph_snapshot.png!

I've attached the original flame graph for exploration.

Once disabled, the new flame graph shows that the read call stack gets extremely thin, which is further confirmed by client recorded metrics : 

!flame22 nodebug sjk svg.png!

The query pager code has been reworked since 3.0 and it looks like log.debug() calls are gone there, but for 2.2 users and to prevent such issues to appear with default settings, I really think debug logging should be disabled by default."
CASSANDRA-14164,Calling StorageService.loadNewSSTables function results in deadlock with compaction background task,"Tested on version 2.2.11 (but seems like trunck 3.x is still the same for the related code path), using nodetool refresh for restoring a snapshot

Calling StorageService.loadNewSSTables function results in deadlock with compaction background task.
because  : 
From StorageService class , function public void loadNewSSTables(String ksName, String cfName) a call is made to ColumnFamilyStore class , function public static synchronized void loadNewSSTables(String ksName, String cfName) and then a call to Keyspace class, function public static Keyspace open(String keyspaceName)
getting to the function private static Keyspace open(String keyspaceName, Schema schema, boolean loadSSTables)
finally trying to get a lock by synchronized (Keyspace.class)

So inside the ColumnFamilyStore class lock, there is an attempt to get the lock on the Keyspace.class

Now at the same time I have the thread OptionalTasks executing the ColumnFamilyStore.getBackgroundCompactionTaskSubmitter() task.

The thread task is also calling Keyspace.open function, already progressed as far as getting the lock on Keyspace class.
But then the call also initializes the column families and thus is calling on class ColumnFamilyStore the public static synchronized ColumnFamilyStore createColumnFamilyStore ...

Result : the external call on loadNewSSTables blocks the internal compaction background task.

So function 1 locks A and then B
And function 2 locks B and then A
leading to deadlock (due to incorrect order of locking objects)

Regards,
Ignace"
CASSANDRA-14155,[TRUNK] Gossiper somewhat frequently hitting an NPE on node startup with dtests at org.apache.cassandra.gms.Gossiper.isSafeForStartup(Gossiper.java:769),"Gossiper is somewhat frequently hitting an NPE on node startup with dtests at org.apache.cassandra.gms.Gossiper.isSafeForStartup(Gossiper.java:769)

{code}
test teardown failure
Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [main] 2018-01-08 21:41:01,832 CassandraDaemon.java:675 - Exception encountered during startup
java.lang.NullPointerException: null
        at org.apache.cassandra.gms.Gossiper.isSafeForStartup(Gossiper.java:769) ~[main/:na]
        at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:511) ~[main/:na]
        at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:761) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:621) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:568) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:360) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:658) [main/:na], ERROR [main] 2018-01-08 21:41:01,832 CassandraDaemon.java:675 - Exception encountered during startup
java.lang.NullPointerException: null
        at org.apache.cassandra.gms.Gossiper.isSafeForStartup(Gossiper.java:769) ~[main/:na]
        at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:511) ~[main/:na]
        at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:761) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:621) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:568) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:360) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:658) [main/:na]]
{code}"
CASSANDRA-14148,test_no_base_column_in_view_pk_complex_timestamp_with_flush - materialized_views_test.TestMaterializedViews frequently fails in CI,"test_no_base_column_in_view_pk_complex_timestamp_with_flush - materialized_views_test.TestMaterializedViews frequently fails in CI

self = <materialized_views_test.TestMaterializedViews object at 0x7f849b25cf60>

    @since('3.0')
    def test_no_base_column_in_view_pk_complex_timestamp_with_flush(self):
>       self._test_no_base_column_in_view_pk_complex_timestamp(flush=True)

materialized_views_test.py:970: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
materialized_views_test.py:1066: in _test_no_base_column_in_view_pk_complex_timestamp
    assert_one(session, ""SELECT * FROM t"", [1, 1, None, None, None, 1])
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

session = <cassandra.cluster.Session object at 0x7f849b379390>
query = 'SELECT * FROM t', expected = [1, 1, None, None, None, 1], cl = None

    def assert_one(session, query, expected, cl=None):
        """"""
        Assert query returns one row.
        @param session Session to use
        @param query Query to run
        @param expected Expected results from query
        @param cl Optional Consistency Level setting. Default ONE
    
        Examples:
        assert_one(session, ""LIST USERS"", ['cassandra', True])
        assert_one(session, query, [0, 0])
        """"""
        simple_query = SimpleStatement(query, consistency_level=cl)
        res = session.execute(simple_query)
        list_res = _rows_to_list(res)
>       assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
E       AssertionError: Expected [[1, 1, None, None, None, 1]] from SELECT * FROM t, but got []"
CASSANDRA-14136,MemtableFlushWriter DecoratedKey Exception,"Running into this issue on my cluster periodically for different tables. After this error is encountered, all the post flushes stop and eventually the system runs out of memory.
On a restart all the commit logs get played normally and things go back to normal. 

I'm unable to understand the scenario, but the issue is recreating every few days.

{code}DEBUG [MemtableFlushWriter:884] 2017-12-26 18:19:40,883 Memtable.java:401 - Completed flushing /mnt/DATA/cassandra/data/products/products_by_hierarchy5storeid_pascdesc-411cabe0632411e7b25a1b665c06298b/.id
x_hierarchy1category/mc-2050-big-Data.db (508.127KiB) for commitlog position ReplayPosition(segmentId=1513929386900,
 position=19110822)
DEBUG [MemtableFlushWriter:884] 2017-12-26 18:19:41,150 Memtable.java:368 - Writing Memtable-products_by_hierarchy5storeid_pascdesc.idx_hierarchy3category@551487729(545.926KiB serialized bytes, 324073 ops, 0%/0% of on/off-heap limit)
ERROR [MemtableFlushWriter:884] 2017-12-26 18:19:41,316 CassandraDaemon.java:205 - Exception in thread Thread[MemtableFlushWriter:884,5,main]
java.lang.RuntimeException: Last written key DecoratedKey(CU00328612, 43553030333238363132) >= current key DecoratedKey(^@^@^@^@^@^@^@^@^@^@, 43553030333238363838) writing into /mnt/DATA/cassandra/data/products/products_by_hierarchy5storeid_pascdesc-411cabe0632411e7b25a1b665c06298b/.idx_hierarchy3category/mc-2134-big-Data.db
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.beforeAppend(BigTableWriter.java:106) ~[apache-cassandra-3.0.9.jar:3.0.9]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:145) ~[apache-cassandra-3.0.9.jar:3.0.9]
        at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:45) ~[apache-cassandra-3.0.9.jar:3.0.9]
        at org.apache.cassandra.io.sstable.SSTableTxnWriter.append(SSTableTxnWriter.java:52) ~[apache-cassandra-3.0.9.jar:3.0.9]
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:394) ~[apache-cassandra-3.0.9.jar:3.0.9]
        at org.apache.cassandra.db.Memtable.flush(Memtable.java:332) ~[apache-cassandra-3.0.9.jar:3.0.9]
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1054) ~[apache-cassandra-3.0.9.jar:3.0.9]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_112]
{code}"
CASSANDRA-14103,Fix potential race during compaction strategy reload,"When the compaction strategies are reloaded after disk boundary changes (CASSANDRA-13948), it's possible that a recently finished SSTable is added twice to the compaction strategy: once when the compaction strategies are reloaded due to the disk boundary change ({{maybeReloadDiskBoundarie}}), and another when the {{CompactionStrategyManager}} is processing the {{SSTableAddedNotification}}.

This should be quite unlikely because a compaction must finish as soon as the disk boundary changes, and even if it happens most compaction strategies would not be affected by it since they deduplicate sstables internally, but we should protect against such scenario. 

For more context see [this comment|https://issues.apache.org/jira/browse/CASSANDRA-13948?focusedCommentId=16280448&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16280448] from Marcus."
CASSANDRA-14096,Cassandra 3.11.1 Repair Causes Out of Memory,"Number of nodes: 9
System resources: 8 Core, 16GB RAM
Replication factor: 3
Number of vnodes: 256

We get out of memory errors while repairing (incremental or full) our keyspace. I had tried to increase node's memory from 16GB to 32GB but result did not change. Repairing tables one by one in our keyspace was not completed successfully for all tables too. 

Only subrange repair with cassandra-reaper worked for me.

Here is the output of heap utils before oom:

{code}

ERROR [MessagingService-Incoming-/192.168.199.121] 2017-12-05 11:38:08,121 JVMStabilityInspector.java:142 - JVM state determined to be unstable.  Exiting forcefully due to:
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.gms.GossipDigestSerializationHelper.deserialize(GossipDigestSyn.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:95) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.1.jar:3.11.1]
{code}

{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      31105265     1493052720  org.apache.cassandra.utils.MerkleTree$Inner
   2:      31134570      996306240  org.apache.cassandra.utils.MerkleTree$Leaf
   3:      31195121      748682904  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   4:      22885384      667447608  [B
   5:        214550       18357360  [C
   6:        364637       17502576  java.nio.HeapByteBuffer
   7:         46525        9566496  [J
   8:        111024        5306976  [Ljava.lang.Object;
   9:        132674        5306960  org.apache.cassandra.db.rows.BufferCell
  10:        210309        5047416  java.lang.String
  11:         59984        3838976  org.apache.cassandra.utils.btree.BTreeSearchIterator
  12:        101181        3237792  java.util.HashMap$Node
  13:         27158        2719216  [I
  14:         60181        2407240  java.util.TreeMap$Entry
  15:         65998        2111936  org.apache.cassandra.db.rows.BTreeRow
  16:         62387        2023784  [Ljava.nio.ByteBuffer;
  17:         19086        1750464  [Ljava.util.HashMap$Node;
  18:         63466        1523184  javax.management.ObjectName$Property
  19:         61553        1477272  org.apache.cassandra.db.BufferClustering
  20:         29274        1405152  org.apache.cassandra.utils.MerkleTree
  21:         34602        1384080  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$100/78247817
  22:         40972        1311104  java.util.concurrent.ConcurrentHashMap$Node
  23:         39172        1253504  java.util.RandomAccessSubList
  24:         51657        1239768  org.apache.cassandra.db.LivenessInfo
  25:         19013        1216832  java.nio.DirectByteBuffer
  26:         28178        1127120  org.apache.cassandra.db.PreHashedDecoratedKey
  27:         32407        1033120  [Ljavax.management.ObjectName$Property;
  28:         42090        1010160  java.util.EnumMap$EntryIterator$Entry
  29:         40878         981072  java.util.Arrays$ArrayList
  30:         19721         946608  java.util.HashMap
  31:          8359         932600  java.lang.Class
  32:         37277         894648  org.apache.cassandra.dht.Range
  33:         26897         860704  org.apache.cassandra.db.rows.EncodingStats
  34:         19958         798320  org.apache.cassandra.utils.MergeIterator$Candidate
  35:         31281         750744  java.util.ArrayList
  36:         23291         745312  org.apache.cassandra.utils.MerkleTree$TreeRange
  37:         21650         692800  java.util.AbstractList$ListItr
  38:         27675         664200  java.lang.Long
  39:         16204         648160  javax.management.ObjectName
  40:         36873         589968  org.apache.cassandra.utils.WrappedInt
  41:          4100         557600  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference
  42:         21651         519624  java.util.SubList$1
  43:         12275         491000  java.math.BigInteger
  44:          8657         484792  org.apache.cassandra.utils.memory.BufferPool$Chunk
  45:         14732         471424  java.util.ArrayList$Itr
  46:          5371         429680  java.lang.reflect.Constructor
  47:         12640         404480  com.codahale.metrics.LongAdder
  48:         16156         387744  com.sun.jmx.mbeanserver.NamedObject
  49:         16133         387192  com.sun.jmx.mbeanserver.StandardMBeanSupport
  50:          9536         381440  org.apache.cassandra.db.EmptyIterators$EmptyUnfilteredRowIterator
  51:          6035         337960  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator
  52:          6031         337736  org.apache.cassandra.db.transform.UnfilteredRows
  53:          8298         331920  org.apache.cassandra.db.rows.BTreeRow$Builder
  54:          5182         331648  sun.security.provider.SHA2$SHA256
  55:         10356         331392  org.apache.cassandra.utils.btree.BTree$$Lambda$192/259279152
  56:          8145         325800  org.apache.cassandra.db.rows.SerializationHelper
  57:          8144         325760  org.apache.cassandra.io.sstable.SSTableIdentityIterator
  58:          8144         325760  org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator
  59:           176         319536  [Ljava.util.concurrent.ConcurrentHashMap$Node;
  60:          9716         310912  java.net.InetAddress$InetAddressHolder
  61:          7770         310800  com.github.benmanes.caffeine.cache.NodeFactory$SStMW
  62:         18470         295520  org.apache.cassandra.db.rows.CellPath$SingleItemCellPath
  63:          2505         276784  [S
  64:          5646         271008  com.codahale.metrics.EWMA
  65:         11258         270192  java.util.concurrent.ConcurrentLinkedDeque$Node
  66:          8248         263936  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1
  67:         10618         254832  java.lang.Double
  68:          7921         253472  org.apache.cassandra.cache.ChunkCache$Buffer
  69:          7773         248736  org.apache.cassandra.cache.ChunkCache$Key
  70:         10296         247104  org.apache.cassandra.dht.Token$KeyBound
  71:          6096         243816  [Lorg.apache.cassandra.db.transform.Transformation;
  72:          6035         241400  org.apache.cassandra.db.rows.Row$Merger
  73:          6034         241360  org.apache.cassandra.db.rows.RangeTombstoneMarker$Merger
  74:          6034         241360  org.apache.cassandra.db.rows.Row$Merger$ColumnDataReducer
  75:          9969         239256  org.apache.cassandra.db.RowIndexEntry
  76:          9699         232776  java.net.Inet4Address
  77:          5750         230000  org.apache.cassandra.utils.concurrent.Ref$State
  78:         13690         219040  java.util.concurrent.atomic.AtomicInteger
  79:          9091         218184  org.apache.cassandra.gms.GossipDigest
  80:         12392         216040  [Ljava.lang.Class;
  81:          5289         211560  org.apache.cassandra.utils.MergeIterator$ManyToOne
  82:         13079         209264  java.lang.Object
  83:          5183         207320  org.apache.cassandra.repair.Validator$CountingDigest
  84:          8157         195768  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxGauge
  85:          6035         193120  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer
  86:          6023         192736  org.apache.cassandra.db.LivenessInfo$ExpiringLivenessInfo
  87:          5745         183840  com.google.common.collect.RegularImmutableList
  88:          6035         180640  [Lorg.apache.cassandra.db.rows.Row;
  89:          6034         180600  [Lorg.apache.cassandra.db.rows.RangeTombstoneMarker;
  90:          6033         180576  [Lorg.apache.cassandra.db.DeletionTime;
  91:          7464         179136  org.apache.cassandra.db.rows.BTreeRow$$Lambda$109/2102075500
  92:          5288         171488  [Lorg.apache.cassandra.utils.MergeIterator$Candidate;
  93:          5331         170592  com.google.common.collect.Iterators$11
  94:          5183         165856  java.security.MessageDigest$Delegate
  95:          5178         165696  com.google.common.collect.Iterators$7
  96:          5157         165024  org.apache.cassandra.utils.MerkleTree$RowHash
  97:           169         163280  [Lio.netty.util.Recycler$DefaultHandle;
  98:          2304         147456  io.netty.buffer.PoolSubpage
  99:          4608         147456  java.util.EnumMap$EntryIterator
 100:          6034         144816  org.apache.cassandra.db.rows.Row$Merger$CellReducer
 101:          1595         140360  java.lang.reflect.Method
 102:          2893         138864  java.util.TreeMap
 103:          5750         138000  org.apache.cassandra.utils.concurrent.Ref
 104:          8453         135248  org.apache.cassandra.db.rows.BTreeRow$Builder$CellResolver
 105:          5613         134712  java.util.concurrent.atomic.AtomicLong
 106:          5509         132216  org.apache.cassandra.utils.btree.BTree$FiltrationTracker
 107:          5179         124296  com.google.common.collect.Iterables$6
 108:          5179         124296  com.google.common.collect.Iterables$8
 109:          5179         124296  com.google.common.collect.Iterators$5
 110:          5179         124296  com.google.common.collect.Iterators$8
 111:          5177         124248  com.google.common.collect.Iterables$2
 112:          5159         123816  sun.security.jca.GetInstance$Instance
 113:          2577         123696  java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync
 114:          2399         115152  org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir
 115:          4643         111432  org.apache.cassandra.db.DeletionTime
 116:          4490         107760  org.apache.cassandra.db.Columns
 117:          2673         106920  java.util.EnumMap
 118:          4202         100848  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxCounter
 119:          6095          97520  org.apache.cassandra.db.transform.BaseIterator$Stop
 120:          4041          96984  java.util.concurrent.ConcurrentLinkedDeque
 121:          4033          96792  org.apache.cassandra.utils.concurrent.Ref$GlobalState
 122:          1882          90336  com.codahale.metrics.Meter
 123:          5596          89536  java.util.concurrent.atomic.AtomicLongArray
 124:          1845          88560  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxTimer
 125:          5179          82864  com.google.common.collect.Iterables$3
 126:          2050          82000  org.apache.cassandra.utils.btree.BTree$Builder
 127:          1111          71104  java.nio.DirectByteBufferR
 128:          1713          68520  java.util.LinkedHashMap$Entry
 129:          2115          67680  io.netty.util.Recycler$DefaultHandle
 130:          1687          67480  java.lang.ref.SoftReference
 131:          1519          66968  [Ljava.lang.String;
 132:          2724          65376  org.apache.cassandra.db.PartitionColumns
 133:          1598          63920  org.apache.cassandra.io.util.MmappedRegions$State
 134:          2572          61728  java.util.concurrent.locks.ReentrantReadWriteLock
 135:          3736          59776  java.util.concurrent.atomic.AtomicBoolean
 136:           154          59136  io.netty.util.concurrent.FastThreadLocalThread
 137:          1835          58720  org.apache.cassandra.utils.MergeIterator$TrivialOneToOne
 138:          1794          57408  org.apache.cassandra.gms.EndpointState
 139:           896          57344  org.apache.cassandra.config.ColumnDefinition
 140:          1385          55400  sun.misc.Cleaner
 141:          2302          55248  org.apache.cassandra.db.commitlog.CommitLogPosition
 142:          1713          54816  java.io.FileDescriptor
 143:           802          51328  sun.nio.ch.FileChannelImpl
 144:          2137          51288  org.apache.cassandra.db.rows.Row$Deletion
 145:           400          51200  org.apache.cassandra.io.sstable.format.big.BigTableReader
 146:          1584          50688  java.lang.StackTraceElement
 147:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
 148:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
 149:          1579          50528  java.lang.ref.WeakReference
 150:          1563          50016  org.apache.cassandra.io.util.Memory
 151:          1559          49888  java.util.concurrent.locks.ReentrantLock$NonfairSync
 152:            60          48760  [D
 153:           867          48552  java.lang.invoke.MemberName
 154:          1176          47040  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper
 155:          1176          47040  org.apache.cassandra.net.MessageIn
 156:          1938          46512  org.apache.cassandra.db.rows.ComplexColumnData
 157:          1157          46280  com.google.common.util.concurrent.AbstractFuture$Sync
 158:          1893          45432  java.util.concurrent.Executors$RunnableAdapter
 159:           400          44800  org.apache.cassandra.io.sstable.metadata.StatsMetadata
 160:           605          43560  java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask
 161:          2713          43408  com.codahale.metrics.Counter
 162:          1794          43056  org.apache.cassandra.gms.HeartBeatState
 163:          1033          41320  org.apache.cassandra.db.rows.BTreeRow$Builder$ComplexColumnDeletion
 164:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock
 165:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter
 166:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock
 167:           616          39424  com.google.common.collect.MapMakerInternalMap$Segment
 168:          1611          38664  com.codahale.metrics.Histogram
 169:          1611          38664  com.codahale.metrics.Timer
 170:          2410          38560  java.util.concurrent.atomic.AtomicReference
 171:           601          38464  java.util.concurrent.ConcurrentHashMap
 172:          1601          38424  org.apache.cassandra.io.util.ChannelProxy
 173:          1587          38088  org.apache.cassandra.cache.KeyCacheKey
 174:          1583          37992  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
 175:           945          37800  org.apache.cassandra.metrics.LatencyMetrics
 176:          1557          37368  org.apache.cassandra.gms.VersionedValue
 177:          1157          37024  java.lang.ThreadLocal$ThreadLocalMap$Entry
 178:          1540          36960  java.util.concurrent.LinkedBlockingQueue$Node
 179:          1525          36600  org.apache.cassandra.repair.NodePair
 180:           151          36240  org.apache.cassandra.metrics.TableMetrics
 181:          1490          35760  java.util.concurrent.ConcurrentLinkedQueue$Node
 182:          2213          35408  java.util.TreeMap$KeySet
 183:           868          34720  java.util.HashMap$ValueIterator
 184:           863          34520  java.lang.invoke.MethodType
 185:           710          34080  org.apache.cassandra.metrics.RestorableMeter$RestorableEWMA
 186:           418          33696  [Ljava.lang.ThreadLocal$ThreadLocalMap$Entry;
 187:           809          32360  sun.nio.ch.FileChannelImpl$Unmapper
 188:          1344          32256  com.google.common.util.concurrent.ExecutionList
 189:          1342          32208  org.apache.cassandra.utils.Pair
 190:          2012          32192  java.lang.Integer
 191:           800          32000  org.apache.cassandra.io.util.FileHandle
 192:          1333          31992  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxHistogram
 193:          1324          31776  [Lorg.apache.cassandra.dht.Range;
 194:           948          30336  org.apache.cassandra.db.partitions.AbstractBTreePartition$Holder
 195:          1223          29352  java.lang.StringBuilder
 196:           898          28736  sun.security.util.DerInputBuffer
 197:           898          28736  sun.security.util.DerValue
 198:          1196          28704  javax.management.openmbean.CompositeDataSupport
 199:          1176          28224  org.apache.cassandra.concurrent.ExecutorLocals
 200:          1176          28224  org.apache.cassandra.net.MessageDeliveryTask
 201:           866          27712  java.lang.invoke.MethodType$ConcurrentWeakInternSet$WeakEntry
 202:          1143          27432  org.apache.cassandra.repair.SyncStat
 203:           685          27400  org.apache.cassandra.io.sstable.IndexInfo
 204:          1109          26616  org.apache.cassandra.utils.Interval
 205:           828          26496  org.apache.cassandra.utils.MergeIterator$OneToOne
 206:           816          26112  java.lang.ref.ReferenceQueue
 207:           800          25600  org.apache.cassandra.io.util.FileHandle$Cleanup
 208:           982          23568  java.util.Collections$UnmodifiableRandomAccessList
 209:           716          22912  org.apache.cassandra.db.context.CounterContext$ContextState
 210:           941          22584  org.apache.cassandra.utils.MerkleTrees
 211:           400          22400  org.apache.cassandra.io.compress.CompressionMetadata
 212:           400          22400  org.apache.cassandra.io.sstable.IndexSummary
 213:           400          22400  org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier
 214:           553          22120  org.apache.cassandra.db.SerializationHeader
 215:           389          21784  sun.nio.cs.UTF_8$Encoder
 216:           160          21760  io.netty.util.internal.InternalThreadLocalMap
 217:           898          21552  sun.security.util.DerInputStream
 218:           445          21360  org.apache.cassandra.repair.RepairJob
 219:           885          21240  [Lsun.security.x509.AVA;
 220:           885          21240  sun.security.x509.AVA
 221:           885          21240  sun.security.x509.RDN
 222:           878          21072  org.apache.cassandra.repair.TreeResponse
 223:           855          20520  java.util.concurrent.ConcurrentSkipListMap$Node
 224:           628          20096  java.util.Hashtable$Entry
 225:           349          20024  [Z
 226:           621          19872  java.io.File
 227:          1233          19728  java.util.TreeMap$Values
 228:          1212          19392  java.util.Optional
 229:           404          19392  org.apache.cassandra.io.sstable.Descriptor
 230:           604          19328  [Lcom.codahale.metrics.Histogram;
 231:           802          19248  sun.nio.ch.NativeThreadSet
 232:           801          19224  org.apache.cassandra.io.util.MmappedRegions
 233:           399          19152  org.apache.cassandra.io.sstable.format.big.BigFormat$BigVersion
 234:           798          19152  org.apache.cassandra.io.util.ChannelProxy$Cleanup
 235:           798          19152  org.apache.cassandra.utils.EstimatedHistogram
 236:           788          18912  org.apache.cassandra.metrics.ClearableHistogram
 237:           766          18384  com.google.common.collect.SingletonImmutableList
 238:           762          18288  org.apache.cassandra.gms.GossipDigestSyn
 239:           569          18208  java.nio.DirectByteBuffer$Deallocator
 240:           569          18208  org.apache.cassandra.db.filter.ColumnFilter
 241:           300          18000  [Ljava.lang.ref.SoftReference;
 242:           160          17920  org.apache.cassandra.config.CFMetaData
 243:           744          17856  java.util.concurrent.CopyOnWriteArrayList
 244:           442          17680  java.util.HashMap$EntryIterator
 245:           221          17680  org.apache.cassandra.io.sstable.format.big.BigTableScanner
 246:           225          17464  [Ljava.lang.StackTraceElement;
 247:          1084          17344  java.util.EnumMap$EntrySet
 248:           424          16960  org.apache.cassandra.utils.btree.NodeCursor
 249:            32          16896  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
 250:           300          16800  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
 251:             1          16400  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node;
 252:           512          16384  org.apache.cassandra.repair.RepairJobDesc
 253:           154          16016  com.google.common.collect.MapMakerInternalMap
 254:           500          16000  java.lang.invoke.DirectMethodHandle
 255:           400          16000  org.apache.cassandra.io.sstable.BloomFilterTracker
 256:           998          15968  org.antlr.runtime.BitSet
 257:           664          15936  com.google.common.collect.ImmutableMapEntry$TerminalEntry
 258:           398          15920  java.util.WeakHashMap$Entry
 259:           392          15680  java.lang.ref.Finalizer
 260:           325          15600  java.util.concurrent.ConcurrentSkipListMap
 261:           487          15584  org.apache.cassandra.schema.CompressionParams
 262:           485          15520  sun.security.util.ObjectIdentifier
 263:           483          15456  org.apache.cassandra.db.partitions.AtomicBTreePartition
 264:           161          15456  org.apache.cassandra.schema.TableParams
 265:           170          15440  [Ljava.util.WeakHashMap$Entry;
 266:           384          15360  io.netty.buffer.PoolChunkList
 267:           382          15280  org.apache.cassandra.repair.RemoteSyncTask
 268:           941          15056  org.apache.cassandra.utils.MerkleTrees$TokenRangeComparator
 269:           622          14928  java.util.Collections$1
 270:           622          14928  org.apache.cassandra.db.RowIndexEntry$Serializer
 271:           930          14880  java.util.concurrent.locks.ReentrantLock
 272:           464          14848  org.apache.cassandra.cql3.ColumnIdentifier
 273:           925          14800  java.util.HashSet
 274:           264          14784  java.util.LinkedHashMap
 275:           151          14496  org.apache.cassandra.db.ColumnFamilyStore
 276:           604          14496  org.apache.cassandra.metrics.TableMetrics$TableHistogram
 277:           301          14448  ch.qos.logback.classic.Logger
 278:           355          14200  org.apache.cassandra.metrics.RestorableMeter
 279:           442          14144  org.apache.cassandra.io.util.RandomAccessReader
 280:           430          14056  [Lcom.google.common.collect.ImmutableMapEntry;
 281:           433          13856  com.google.common.collect.MapMakerInternalMap$StrongEntry
 282:           433          13856  com.google.common.collect.MapMakerInternalMap$WeakValueReference
 283:           855          13680  java.nio.channels.spi.AbstractInterruptibleChannel$1
 284:            34          13600  org.apache.cassandra.net.IncomingTcpConnection
 285:           333          13320  com.google.common.collect.RegularImmutableSortedMap
 286:           818          13088  java.lang.ref.ReferenceQueue$Lock
 287:           201          12864  java.net.URL
 288:           803          12848  sun.nio.ch.FileDispatcherImpl
 289:           401          12832  org.apache.cassandra.utils.BloomFilter
 290:           200          12800  java.util.regex.Matcher
 291:           400          12800  org.apache.cassandra.cache.ChunkCache$CachingRebufferer
 292:           400          12800  org.apache.cassandra.io.util.CompressedChunkReader$Mmap
 293:           400          12800  org.apache.cassandra.io.util.MmapRebufferer
 294:           799          12784  org.apache.cassandra.io.util.MmappedRegions$Tidier
 295:           799          12784  org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy
 296:           399          12768  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy
 297:           797          12752  java.util.Collections$SingletonSet
 298:           396          12672  java.util.UUID
 299:           784          12544  java.util.HashMap$KeySet
 300:           521          12504  java.util.concurrent.ConcurrentLinkedQueue
 301:           154          12320  org.apache.cassandra.db.rows.RowAndDeletionMergeIterator
 302:           170          12240  java.lang.reflect.Field
 303:           507          12168  org.apache.cassandra.db.BufferDecoratedKey
 304:           151          12080  org.apache.cassandra.db.Memtable
 305:           302          12080  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategyOptions
 306:           376          12032  java.lang.invoke.LambdaForm$Name
 307:           213          11928  sun.security.ssl.CipherSuite
 308:            27          11880  org.apache.cassandra.net.OutboundTcpConnection
 309:           738          11808  java.util.HashMap$Values
 310:           208          11648  java.lang.Package
 311:           242          11616  org.apache.cassandra.utils.IntervalTree$IntervalNode
 312:           128          11264  [Lio.netty.buffer.PoolSubpage;
 313:           699          11184  java.util.HashMap$EntrySet
 314:           155          11160  org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater
 315:           344          11008  java.util.concurrent.ConcurrentSkipListMap$HeadIndex
 316:           341          10912  sun.misc.FDBigInteger
 317:           227          10896  sun.security.x509.X500Name
 318:           453          10872  org.apache.cassandra.utils.DefaultValue
 319:           333          10656  com.google.common.collect.RegularImmutableSortedSet
 320:           265          10600  java.util.Formatter$FormatSpecifier
 321:           263          10520  [Ljava.util.Formatter$Flags;
 322:           433          10392  org.apache.cassandra.cql3.ColumnIdentifier$InternedKey
 323:            72          10368  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong
 324:           324          10368  sun.security.x509.AlgorithmId
 325:           320          10240  io.netty.util.internal.chmv8.LongAdderV8
 326:           633          10128  java.util.concurrent.atomic.AtomicReferenceArray
 327:           180          10080  java.lang.invoke.MethodTypeForm
 328:           156           9984  io.netty.util.Recycler$Stack
 329:           416           9984  java.lang.ThreadLocal$ThreadLocalMap
 330:           622           9952  org.apache.cassandra.dht.Range$1
 331:           154           9856  org.apache.cassandra.cql3.UpdateParameters
 332:           244           9760  java.util.HashMap$KeyIterator
 333:           304           9728  java.util.concurrent.locks.AbstractQueuedSynchronizer$Node
 334:           302           9664  org.apache.cassandra.metrics.TableMetrics$TableMetricNameFactory
 335:           302           9664  org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator
 336:           400           9600  [Lorg.apache.cassandra.io.util.Memory;
 337:           400           9600  org.apache.cassandra.utils.StreamingHistogram
 338:           399           9576  [Ljava.lang.AutoCloseable;
 339:            25           9400  java.lang.Thread
 340:           195           9360  org.apache.cassandra.net.MessageOut
 341:           292           9344  [Lcom.codahale.metrics.Timer;
 342:            16           9216  io.netty.util.internal.shaded.org.jctools.queues.MpscChunkedArrayQueue
 343:           381           9144  org.apache.cassandra.repair.RepairResult
 344:           362           8688  com.google.common.util.concurrent.ExecutionList$RunnableExecutorPair
 345:            68           8680  [Ljava.util.Hashtable$Entry;
 346:           271           8672  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxMeter
 347:           108           8640  sun.security.x509.X509CertImpl
 348:           269           8608  javax.management.MBeanAttributeInfo
 349:           215           8600  com.google.common.collect.RegularImmutableMap
 350:           215           8600  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator
 351:           151           8456  org.apache.cassandra.db.compaction.CompactionStrategyManager
 352:           260           8320  javax.management.MBeanParameterInfo
 353:           142           7952  java.beans.MethodDescriptor
 354:           331           7944  java.util.Collections$SingletonList
 355:           494           7904  com.google.common.base.Present
 356:           164           7872  java.util.WeakHashMap
 357:           227           7768  [Lsun.security.x509.RDN;
 358:           483           7728  org.apache.cassandra.utils.CounterId
 359:           318           7632  java.util.Collections$SetFromMap
 360:           318           7632  java.util.Formatter$FixedString
 361:           156           7488  org.apache.cassandra.utils.concurrent.OpOrder$Group
 362:           187           7480  com.google.common.util.concurrent.ListenableFutureTask
 363:           308           7392  org.apache.cassandra.utils.btree.BTreeSet
 364:           306           7344  java.beans.MethodRef
 365:           304           7296  org.apache.cassandra.io.util.MmappedRegions$Region
 366:           302           7248  org.apache.cassandra.utils.TopKSampler
 367:           151           7248  org.apache.cassandra.utils.memory.SlabAllocator
 368:           148           7104  java.lang.invoke.LambdaForm
 369:           292           7008  org.apache.cassandra.metrics.TableMetrics$TableTimer
 370:           155           6904  [Ljava.lang.invoke.LambdaForm$Name;
 371:           121           6776  jdk.internal.org.objectweb.asm.Item
 372:           169           6760  java.security.AccessControlContext
 373:           280           6720  java.util.Date
 374:           168           6720  java.util.IdentityHashMap
 375:           209           6688  org.apache.cassandra.db.ClusteringComparator
 376:           278           6672  com.google.common.collect.ImmutableSortedAsList
 377:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet
 378:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet$1
 379:           404           6464  java.util.concurrent.CopyOnWriteArraySet
 380:           200           6400  java.util.Formatter
 381:           400           6400  org.apache.cassandra.io.sstable.format.SSTableReader$UniqueIdentifier
 382:           399           6384  org.apache.cassandra.utils.obs.OffHeapBitSet
 383:            23           6368  [[S
 384:           394           6304  org.apache.cassandra.db.commitlog.IntervalSet
 385:           262           6288  java.util.concurrent.CopyOnWriteArrayList$COWIterator
 386:           156           6240  org.apache.cassandra.cql3.QueryOptions$DefaultQueryOptions
 387:           111           6216  sun.security.util.MemoryCache$SoftCacheEntry
 388:           155           6200  javax.management.MBeanOperationInfo
 389:           155           6200  org.apache.cassandra.db.Mutation
 390:           155           6200  org.apache.cassandra.db.partitions.PartitionUpdate
 391:           155           6200  org.apache.cassandra.utils.memory.AbstractAllocator$CloningBTreeRowBuilder
 392:           193           6176  org.apache.cassandra.net.OutboundTcpConnection$QueuedMessage
 393:           200           6160  [Ljava.util.Formatter$FormatString;
 394:           154           6160  java.util.Collections$SingletonMap
 395:           154           6160  org.apache.cassandra.db.rows.BTreeRow$$Lambda$122/418553968
 396:           154           6160  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$125/1196438970
 397:           152           6080  org.apache.cassandra.db.lifecycle.View
 398:           253           6072  java.util.concurrent.ConcurrentSkipListMap$Index
 399:           189           6048  org.apache.cassandra.repair.ValidationTask
 400:           108           6048  sun.security.x509.X509CertInfo
 401:           251           6024  javax.management.ImmutableDescriptor
 402:            62           5952  java.util.jar.JarFile$JarFileEntry
 403:            82           5904  java.beans.PropertyDescriptor
 404:           244           5856  org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$111/177399658
 405:           243           5832  org.apache.cassandra.cql3.functions.FunctionName
 406:            52           5824  sun.nio.ch.SocketChannelImpl
 407:            90           5760  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$99/328488350
 408:           240           5736  [Lorg.apache.cassandra.db.marshal.AbstractType;
 409:           179           5728  org.apache.cassandra.auth.DataResource
 410:            89           5696  org.apache.cassandra.utils.btree.NodeBuilder
 411:           355           5680  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1
 412:           229           5496  org.apache.cassandra.db.MutableDeletionInfo
 413:           227           5448  java.security.Provider$ServiceKey
 414:           224           5376  com.google.common.collect.SingletonImmutableSet
 415:            74           5328  ch.qos.logback.classic.spi.LoggingEvent
 416:            95           5320  java.security.Provider$Service
 417:           165           5280  java.lang.invoke.BoundMethodHandle$Species_L
 418:           106           5272  [Ljavax.management.MBeanAttributeInfo;
 419:           109           5232  java.util.concurrent.ThreadPoolExecutor$Worker
 420:           325           5200  org.apache.cassandra.utils.concurrent.WaitQueue
 421:           108           5184  javax.management.MBeanInfo
 422:           210           5040  com.google.common.collect.RegularImmutableAsList
 423:           210           5040  com.google.common.collect.RegularImmutableMap$EntrySet
 424:           208           4992  java.util.concurrent.ConcurrentHashMap$KeySetView
 425:           155           4960  org.apache.cassandra.db.commitlog.CommitLogSegment$Allocation
 426:           154           4928  [Lcom.google.common.collect.MapMakerInternalMap$Segment;
 427:           308           4928  org.apache.cassandra.db.Columns$$Lambda$121/617875913
 428:           154           4928  org.apache.cassandra.db.rows.EncodingStats$Collector
 429:           154           4928  org.apache.cassandra.io.util.DataOutputBufferFixed
 430:           102           4896  java.util.TimSort
 431:           152           4864  org.apache.cassandra.db.lifecycle.Tracker
 432:           202           4848  org.apache.cassandra.db.lifecycle.SSTableIntervalTree
 433:           121           4840  java.io.ObjectStreamField
 434:           151           4832  org.apache.cassandra.db.compaction.CompactionLogger
 435:            99           4752  javax.management.Notification
 436:           198           4752  org.apache.cassandra.db.ClusteringBound
 437:           198           4752  org.apache.cassandra.db.rows.ComplexColumnData$Builder
 438:           180           4744  [Ljava.security.ProtectionDomain;
 439:            63           4536  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterator
 440:            40           4480  java.net.SocksSocketImpl
 441:           275           4400  java.util.Formatter$Flags
 442:           273           4368  java.lang.Byte
 443:            32           4352  io.netty.buffer.PoolArena$DirectArena
 444:            32           4352  io.netty.buffer.PoolArena$HeapArena
 445:           181           4344  java.lang.invoke.LambdaForm$NamedFunction
 446:             6           4320  [Ljdk.internal.org.objectweb.asm.Item;
 447:            90           4320  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$313/480779282
 448:           108           4320  org.apache.cassandra.db.CachedHashDecoratedKey
 449:           178           4272  org.apache.cassandra.gms.GossipDigestAck
 450:           177           4248  java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
 451:           131           4192  com.sun.jmx.mbeanserver.ConvertingMethod
 452:           128           4096  java.lang.NoSuchMethodException
 453:           256           4096  java.lang.Short
 454:            70           3920  sun.misc.URLClassPath$JarLoader
 455:            60           3840  java.util.jar.JarFile
 456:            80           3840  java.util.logging.LogManager$LoggerWeakRef
 457:           160           3840  org.apache.cassandra.db.Serializers
 458:           160           3840  org.apache.cassandra.db.Serializers$NewFormatSerializer
 459:           160           3840  org.apache.cassandra.io.sstable.IndexInfo$Serializer
 460:           160           3840  org.apache.cassandra.schema.Indexes
 461:            53           3816  java.util.regex.Pattern
 462:            95           3800  sun.security.rsa.RSAPublicKeyImpl
 463:           158           3792  com.sun.jmx.mbeanserver.PerInterface$MethodAndSig
 464:            59           3776  java.text.DateFormatSymbols
 465:           155           3720  org.apache.cassandra.utils.memory.ContextAllocator
 466:           154           3696  [Lorg.apache.cassandra.db.Directories$DataDirectory;
 467:           154           3696  com.google.common.collect.Collections2$TransformedCollection
 468:           154           3696  org.apache.cassandra.cql3.statements.UpdatesCollector
 469:           154           3696  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter
 470:           154           3696  org.apache.cassandra.db.rows.Rows$$Lambda$120/877468788
 471:           151           3624  [Ljava.io.File;
 472:           151           3624  org.apache.cassandra.db.Directories
 473:           151           3624  org.apache.cassandra.db.Memtable$ColumnsCollector
 474:           151           3624  org.apache.cassandra.index.SecondaryIndexManager
 475:           151           3624  org.apache.cassandra.metrics.TableMetrics$10
 476:           151           3624  org.apache.cassandra.metrics.TableMetrics$11
 477:           151           3624  org.apache.cassandra.metrics.TableMetrics$12
 478:           151           3624  org.apache.cassandra.metrics.TableMetrics$14
 479:           151           3624  org.apache.cassandra.metrics.TableMetrics$15
 480:           151           3624  org.apache.cassandra.metrics.TableMetrics$16
 481:           151           3624  org.apache.cassandra.metrics.TableMetrics$17
 482:           151           3624  org.apache.cassandra.metrics.TableMetrics$19
 483:           151           3624  org.apache.cassandra.metrics.TableMetrics$2
 484:           151           3624  org.apache.cassandra.metrics.TableMetrics$21
 485:           151           3624  org.apache.cassandra.metrics.TableMetrics$23
 486:           151           3624  org.apache.cassandra.metrics.TableMetrics$24
 487:           151           3624  org.apache.cassandra.metrics.TableMetrics$25
 488:           151           3624  org.apache.cassandra.metrics.TableMetrics$27
 489:           151           3624  org.apache.cassandra.metrics.TableMetrics$29
 490:           151           3624  org.apache.cassandra.metrics.TableMetrics$3
 491:           151           3624  org.apache.cassandra.metrics.TableMetrics$30
 492:           151           3624  org.apache.cassandra.metrics.TableMetrics$31
 493:           151           3624  org.apache.cassandra.metrics.TableMetrics$32
 494:           151           3624  org.apache.cassandra.metrics.TableMetrics$33
 495:           151           3624  org.apache.cassandra.metrics.TableMetrics$34
 496:           151           3624  org.apache.cassandra.metrics.TableMetrics$4
 497:           151           3624  org.apache.cassandra.metrics.TableMetrics$5
 498:           151           3624  org.apache.cassandra.metrics.TableMetrics$6
 499:           151           3624  org.apache.cassandra.metrics.TableMetrics$7
 500:           151           3624  org.apache.cassandra.metrics.TableMetrics$8
 501:           151           3624  org.apache.cassandra.metrics.TableMetrics$9
 502:           113           3616  [Lorg.apache.cassandra.utils.memory.BufferPool$Chunk;
 503:           113           3616  org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef
 504:           225           3600  org.apache.cassandra.cql3.FieldIdentifier
 505:           149           3576  org.apache.cassandra.cql3.restrictions.RestrictionSet
 506:           221           3536  java.util.zip.CRC32
 507:            63           3528  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController
 508:            63           3528  org.apache.cassandra.repair.Validator
 509:            12           3480  [Ljava.util.concurrent.RunnableScheduledFuture;
 510:           108           3456  java.util.Collections$SynchronizedMap
 511:           143           3432  com.google.common.util.concurrent.Futures$CombinedFuture$2
 512:           143           3432  java.util.LinkedList$Node
 513:           107           3424  java.io.IOException
 514:            37           3384  [Lorg.apache.cassandra.io.sstable.IndexInfo;
 515:            60           3360  org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
 516:           122           3344  [Ljavax.management.MBeanParameterInfo;
 517:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$31/1914108708
 518:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$32/1889757798
 519:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$33/1166106620
 520:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$34/221861886
 521:            41           3328  [Ljava.lang.invoke.MethodHandle;
 522:            32           3328  java.io.ObjectStreamClass
 523:           208           3328  org.apache.cassandra.utils.concurrent.Refs
 524:            69           3312  com.google.common.util.concurrent.Futures$CombinedFuture
 525:           103           3296  org.apache.cassandra.schema.CompactionParams
 526:           137           3288  java.util.ArrayDeque
 527:            24           3264  com.codahale.metrics.Striped64$Cell
 528:           203           3248  org.apache.cassandra.io.util.DataOutputBuffer$GrowingChannel
 529:           135           3240  com.sun.jmx.remote.internal.ArrayNotificationBuffer$NamedNotification
 530:           101           3232  java.util.Vector
 531:           101           3232  org.apache.cassandra.schema.SpeculativeRetryParam
 532:           132           3168  org.apache.cassandra.db.view.TableViews
 533:            79           3160  com.google.common.collect.SingletonImmutableBiMap
 534:            98           3136  org.xml.sax.helpers.LocatorImpl
 535:            98           3136  sun.security.x509.BasicConstraintsExtension
 536:            78           3120  java.security.ProtectionDomain
 537:           129           3096  com.google.common.collect.RegularImmutableMap$NonTerminalMapEntry
 538:            77           3080  sun.nio.cs.UTF_8$Decoder
 539:            64           3072  org.apache.cassandra.db.compaction.CompactionIterator$Purger
 540:            64           3072  org.apache.cassandra.db.transform.UnfilteredPartitions
 541:            96           3072  sun.security.x509.SubjectKeyIdentifierExtension
 542:            24           3032  [Ljava.beans.MethodDescriptor;
 543:            92           3024  [Ljavax.management.MBeanOperationInfo;
 544:            94           3008  java.util.AbstractList$Itr
 545:            91           2912  com.codahale.metrics.Timer$Context
 546:           121           2904  org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate
 547:            60           2880  java.util.zip.Inflater
 548:            45           2880  javax.management.openmbean.OpenMBeanAttributeInfoSupport
 549:           118           2832  java.util.regex.Pattern$1
 550:           118           2832  sun.reflect.generics.tree.SimpleClassTypeSignature
 551:            88           2816  sun.security.x509.KeyUsageExtension
 552:           175           2800  org.apache.cassandra.gms.GossipDigestAck2
 553:           113           2712  org.apache.cassandra.utils.memory.BufferPool$LocalPool
 554:            37           2664  java.util.logging.Logger
 555:           111           2664  sun.security.util.Cache$EqualByteArray
 556:            55           2640  java.util.Hashtable
 557:           163           2608  java.util.IdentityHashMap$KeySet
 558:           162           2592  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable
 559:           108           2592  org.apache.cassandra.dht.LocalPartitioner$LocalToken
 560:            18           2592  sun.reflect.MethodAccessorGenerator
 561:           108           2592  sun.security.util.BitArray
 562:           108           2592  sun.security.x509.CertificateValidity
 563:           138           2584  [Lcom.sun.jmx.mbeanserver.MXBeanMapping;
 564:           107           2568  java.net.InetSocketAddress$InetSocketAddressHolder
 565:            64           2560  com.google.common.collect.Multimaps$UnmodifiableMultimap
 566:            64           2560  java.util.ArrayList$SubList
 567:            64           2560  java.util.ArrayList$SubList$1
 568:            64           2560  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1
 569:           160           2560  org.apache.cassandra.schema.Triggers
 570:            64           2560  org.apache.cassandra.utils.OverlapIterator
 571:            53           2544  java.util.concurrent.LinkedBlockingQueue
 572:           155           2480  org.apache.cassandra.utils.btree.UpdateFunction$Simple
 573:           155           2480  org.apache.cassandra.utils.concurrent.OpOrder
 574:            44           2464  java.lang.Class$ReflectionData
 575:           154           2464  java.util.concurrent.ConcurrentSkipListSet
 576:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$117/1004624941
 577:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$119/1364111969
 578:           154           2464  org.apache.cassandra.utils.WrappedBoolean
 579:           102           2448  org.apache.cassandra.schema.CachingParams
 580:            76           2432  java.security.CodeSource
 581:           151           2416  org.apache.cassandra.db.Memtable$StatsCollector
 582:           151           2416  org.apache.cassandra.utils.memory.EnsureOnHeap$NoOp
 583:            75           2400  java.util.LinkedList
 584:            50           2400  org.apache.cassandra.cql3.restrictions.StatementRestrictions
 585:            99           2376  sun.security.x509.CertificateExtensions
 586:            74           2368  java.io.ObjectStreamClass$WeakClassKey
 587:            98           2352  java.lang.Class$AnnotationData
 588:           147           2352  java.util.concurrent.ConcurrentHashMap$ValuesView
 589:            98           2352  java.util.jar.Attributes$Name
 590:            73           2336  java.util.regex.Pattern$Curly
 591:            97           2328  com.google.common.collect.ImmutableMapKeySet
 592:            48           2304  com.google.common.collect.HashMultimap
 593:            96           2304  com.google.common.collect.ImmutableMapKeySet$1
 594:            16           2304  io.netty.channel.epoll.EpollEventLoop
 595:           144           2304  org.apache.cassandra.db.ColumnFamilyStore$3
 596:            96           2304  org.apache.cassandra.metrics.KeyspaceMetrics$17
 597:            72           2304  sun.reflect.ClassFileAssembler
 598:            70           2240  java.util.concurrent.ConcurrentHashMap$ReservationNode
 599:            70           2240  java.util.logging.LogManager$LogNode
 600:            70           2240  org.apache.cassandra.utils.MerkleTree$TreeRangeIterator
 601:            91           2200  [Lcom.github.benmanes.caffeine.cache.RemovalCause;
 602:            91           2184  com.github.benmanes.caffeine.SingleConsumerQueue$Node
 603:            39           2184  org.apache.cassandra.db.marshal.UserType
 604:            90           2160  [Lcom.github.benmanes.caffeine.cache.Node;
 605:           118           2160  [Lsun.reflect.generics.tree.TypeArgument;
 606:            90           2160  com.github.benmanes.caffeine.cache.BoundedLocalCache$AddTask
 607:            90           2160  java.lang.StringBuffer
 608:            67           2144  java.util.TreeMap$ValueIterator
 609:            89           2136  java.lang.RuntimePermission
 610:            89           2136  org.apache.cassandra.io.compress.CompressionMetadata$Chunk
 611:            53           2120  sun.security.ec.NamedCurve
 612:            66           2112  java.io.FilePermission
 613:            66           2112  java.util.zip.ZipCoder
 614:            52           2080  sun.nio.ch.SocketAdaptor
 615:            37           2072  javax.management.MBeanServerNotification
 616:            37           2072  org.apache.cassandra.db.RowIndexEntry$IndexedEntry
 617:            86           2064  javax.management.openmbean.TabularDataSupport
 618:           129           2064  sun.security.x509.KeyIdentifier
 619:            64           2048  com.google.common.util.concurrent.Futures$ChainingListenableFuture
 620:           128           2048  java.lang.Character
 621:            64           2048  org.apache.cassandra.db.partitions.PurgeFunction$$Lambda$104/2021147872
 622:            64           2048  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2
 623:            64           2048  sun.misc.FloatingDecimal$ASCIIToBinaryBuffer
 624:            84           2016  java.security.Provider$UString
 625:            18           2016  java.util.GregorianCalendar
 626:            62           1984  org.apache.cassandra.utils.MerkleTrees$TreeRangeIterator
 627:            27           1944  sun.reflect.DelegatingClassLoader
 628:           120           1920  com.codahale.metrics.Striped64$HashCode
 629:            80           1920  java.util.regex.Pattern$GroupTail
 630:            34           1904  org.apache.cassandra.cql3.statements.SelectStatement
 631:            79           1896  com.google.common.collect.ImmutableList$1
 632:            79           1896  java.util.regex.Pattern$GroupHead
 633:            59           1888  java.util.RegularEnumSet
 634:           118           1888  sun.reflect.generics.tree.ClassTypeSignature
 635:           118           1888  sun.security.x509.SerialNumber
 636:            13           1872  java.text.DecimalFormat
 637:            39           1872  sun.util.locale.LocaleObjectCache$CacheEntry
 638:            10           1832  [[B
 639:            57           1824  org.apache.cassandra.cql3.functions.CastFcts$JavaFunctionWrapper
 640:            75           1800  java.util.regex.Pattern$Single
 641:            56           1792  java.lang.Throwable
 642:             8           1792  jdk.internal.org.objectweb.asm.MethodWriter
 643:            74           1776  com.google.common.util.concurrent.Futures$6
 644:           111           1776  java.util.LinkedHashMap$LinkedValues
 645:            44           1760  java.io.ObjectStreamClass$FieldReflectorKey
 646:            36           1728  org.apache.cassandra.concurrent.SEPWorker
 647:            72           1728  sun.reflect.ByteVectorImpl
 648:           108           1728  sun.security.x509.CertificateAlgorithmId
 649:           108           1728  sun.security.x509.CertificateSerialNumber
 650:           108           1728  sun.security.x509.CertificateVersion
 651:           108           1728  sun.security.x509.CertificateX509Key
 652:            18           1728  sun.util.calendar.Gregorian$Date
 653:           107           1712  java.net.InetSocketAddress
 654:             4           1696  [Ljava.lang.Thread;
 655:            53           1696  java.security.spec.EllipticCurve
 656:            30           1688  [Ljava.lang.reflect.Method;
 657:             6           1680  java.util.concurrent.ConcurrentHashMap$CounterCell
 658:            52           1664  java.lang.invoke.DirectMethodHandle$Special
 659:            52           1664  sun.nio.ch.SocketAdaptor$SocketInputStream
 660:            68           1632  org.apache.cassandra.cql3.Constants$Marker
 661:            68           1632  sun.reflect.NativeConstructorAccessorImpl
 662:           101           1616  org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/673586830
 663:            40           1600  ch.qos.logback.core.joran.event.StartEvent
 664:            40           1600  com.sun.jmx.mbeanserver.PerInterface
 665:            40           1600  sun.management.DiagnosticCommandArgumentInfo
 666:            99           1584  org.apache.cassandra.db.marshal.AbstractType$$Lambda$4/495702238
 667:            49           1568  java.io.DataOutputStream
 668:            49           1568  java.nio.channels.Channels$1
 669:            65           1560  java.security.spec.ECPoint
 670:            39           1560  org.apache.cassandra.io.util.SafeMemory
 671:            65           1560  org.apache.cassandra.utils.btree.TreeBuilder
 672:            64           1536  org.apache.cassandra.db.compaction.CompactionIterator$GarbageSkipper
 673:            63           1512  com.google.common.util.concurrent.Futures$1
 674:            63           1512  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$EQRestriction
 675:            63           1512  org.apache.cassandra.db.compaction.CompactionManager$13
 676:            47           1504  org.apache.cassandra.cql3.statements.ParsedStatement$Prepared
 677:            47           1504  org.apache.cassandra.io.util.DataOutputBuffer$1$1
 678:            93           1488  java.util.Collections$UnmodifiableSet
 679:            61           1464  java.util.regex.Pattern$Slice
 680:            60           1440  java.util.zip.ZStreamRef
 681:            51           1408  [Ljava.io.ObjectStreamField;
 682:            16           1392  [Ljava.lang.Byte;
 683:             1           1376  [Lsun.misc.FDBigInteger;
 684:            43           1376  java.util.regex.Pattern$Branch
 685:            43           1376  org.apache.cassandra.concurrent.NamedThreadFactory
 686:            34           1360  ch.qos.logback.core.status.InfoStatus
 687:            17           1360  java.net.URI
 688:            34           1360  org.apache.cassandra.cql3.selection.Selection$SimpleSelection
 689:            61           1352  [Ljava.lang.reflect.Type;
 690:            24           1344  java.util.ResourceBundle$CacheKey
 691:            24           1344  javax.management.openmbean.CompositeType
 692:            72           1336  [Ljavax.management.openmbean.CompositeData;
 693:            33           1320  sun.security.x509.AuthorityKeyIdentifierExtension
 694:            79           1312  [Ljava.security.Principal;
 695:            54           1296  ch.qos.logback.classic.spi.StackTraceElementProxy
 696:            23           1288  java.net.SocketPermission
 697:            39           1280  [Ljava.math.BigInteger;
 698:            40           1280  ch.qos.logback.core.joran.event.EndEvent
 699:            16           1280  com.google.common.cache.LocalCache$Segment
 700:            20           1280  org.apache.cassandra.db.RowIndexEntry$ShallowIndexedEntry
 701:            43           1272  [Ljava.util.regex.Pattern$Node;
 702:            53           1272  sun.nio.ch.Util$BufferCache
 703:            79           1264  java.security.ProtectionDomain$Key
 704:            39           1248  java.lang.Thread$WeakClassKey
 705:            38           1240  [Ljava.lang.reflect.Field;
 706:            14           1232  org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor
 707:            38           1216  java.security.Permissions
 708:            50           1200  org.apache.cassandra.cql3.restrictions.ClusteringColumnRestrictions
 709:            50           1200  org.apache.cassandra.cql3.restrictions.IndexRestrictions
 710:            25           1200  org.apache.cassandra.metrics.ClientRequestMetrics
 711:             2           1184  [Lcom.github.benmanes.caffeine.cache.NodeFactory;
 712:            37           1184  java.net.Socket
 713:            49           1176  org.apache.cassandra.cql3.restrictions.PartitionKeySingleRestrictionSet
 714:            21           1176  sun.util.calendar.ZoneInfo
 715:            52           1168  [Lorg.apache.cassandra.cql3.ColumnSpecification;
 716:            24           1152  java.beans.BeanDescriptor
 717:            24           1152  java.lang.management.MemoryUsage
 718:            72           1152  org.apache.cassandra.db.ColumnFamilyStore$1
 719:            36           1152  org.apache.cassandra.io.util.SafeMemory$MemoryTidy
 720:            24           1152  org.hyperic.sigar.FileSystem
 721:            36           1152  sun.reflect.generics.repository.ClassRepository
 722:            20           1120  javax.management.openmbean.ArrayType
 723:            35           1120  org.apache.cassandra.cql3.ResultSet$ResultMetadata
 724:            69           1104  com.google.common.util.concurrent.Futures$8
 725:            69           1104  com.google.common.util.concurrent.Futures$CombinedFuture$1
 726:            46           1104  org.apache.cassandra.metrics.DefaultNameFactory
 727:            69           1104  sun.reflect.DelegatingConstructorAccessorImpl
 728:             3           1080  [Ljava.lang.Integer;
 729:            27           1080  com.google.common.collect.HashBiMap$BiEntry
 730:            27           1080  org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy
 731:            45           1080  sun.reflect.generics.factory.CoreReflectionFactory
 732:            24           1064  [Ljava.beans.PropertyDescriptor;
 733:             2           1056  [Ljava.lang.Long;
 734:             2           1056  [Ljava.lang.Short;
 735:            26           1040  java.math.BigDecimal
 736:            43           1032  io.netty.channel.ChannelOption
 737:            43           1032  java.io.ExpiringCache$Entry
 738:            64           1024  org.apache.cassandra.db.compaction.AbstractCompactionStrategy$ScannerList
 739:            64           1024  org.apache.cassandra.db.compaction.CompactionIterator$1
 740:            64           1024  org.apache.cassandra.repair.RepairJob$3
 741:            63           1008  org.apache.cassandra.repair.RepairJob$2
 742:            12            960  [Lcom.google.common.collect.HashBiMap$BiEntry;
 743:            24            960  java.beans.GenericBeanInfo
 744:            30            960  java.security.Provider$EngineDescription
 745:            40            960  java.util.regex.Pattern$BitClass
 746:            20            960  org.antlr.runtime.CommonToken
 747:            30            960  org.apache.cassandra.cql3.ColumnSpecification
 748:            40            960  org.apache.cassandra.cql3.statements.SelectStatement$Parameters
 749:            60            960  org.cliffc.high_scale_lib.Counter
 750:            20            960  org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
 751:            40            960  org.codehaus.jackson.map.type.ClassKey
 752:            40            960  org.xml.sax.helpers.AttributesImpl
 753:            46            944  [Lsun.reflect.generics.tree.FormalTypeParameter;
 754:            39            936  java.util.regex.Pattern$5
 755:             8            928  [Lorg.apache.cassandra.db.ClusteringBound;
 756:            29            928  java.security.BasicPermissionCollection
 757:            29            928  org.apache.cassandra.io.util.DataInputPlus$DataInputStreamPlus
 758:            23            920  org.codehaus.jackson.map.type.SimpleType
 759:            19            912  sun.management.DiagnosticCommandInfo
 760:            28            896  java.io.DataInputStream
 761:            18            864  net.jpountz.lz4.LZ4BlockOutputStream
 762:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$26/843299092
 763:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$27/605982374
 764:            54            864  org.apache.cassandra.config.ColumnDefinition$1
 765:            18            864  org.apache.cassandra.utils.SlidingTimeRate
 766:            36            864  sun.reflect.Label$PatchInfo
 767:            27            864  sun.reflect.generics.reflectiveObjects.TypeVariableImpl
 768:            36            864  sun.reflect.generics.tree.ClassSignature
 769:            44            856  [Ljavax.management.MBeanConstructorInfo;
 770:            21            840  com.sun.jmx.mbeanserver.MXBeanSupport
 771:            35            840  net.jpountz.xxhash.StreamingXXHash32JNI
 772:            35            840  sun.reflect.generics.scope.ClassScope
 773:            21            840  sun.util.locale.BaseLocale$Key
 774:             2            832  [Lorg.antlr.runtime.BitSet;
 775:            13            832  com.google.common.util.concurrent.SmoothRateLimiter$SmoothBursty
 776:            13            832  java.text.DecimalFormatSymbols
 777:            38            824  [Lsun.reflect.generics.tree.FieldTypeSignature;
 778:            34            816  org.apache.cassandra.cql3.selection.SelectionColumnMapping
 779:             6            816  org.apache.cassandra.metrics.KeyspaceMetrics
 780:            25            800  java.util.PropertyPermission
 781:            20            800  org.cliffc.high_scale_lib.NonBlockingHashMap
 782:            14            784  java.util.HashMap$TreeNode
 783:            14            784  org.apache.cassandra.cql3.statements.UpdateStatement
 784:            32            768  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$IdentityMapping
 785:            32            768  io.netty.channel.unix.FileDescriptor
 786:            16            768  java.util.ResourceBundle$BundleReference
 787:            24            768  java.util.ResourceBundle$LoaderReference
 788:            16            768  net.jpountz.lz4.LZ4BlockInputStream
 789:            32            768  org.apache.cassandra.cql3.functions.CastFcts$CastAsTextFunction
 790:            32            768  sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl
 791:            24            768  sun.security.x509.OIDMap$OIDInfo
 792:            23            736  javax.management.MBeanConstructorInfo
 793:            23            736  sun.management.MappedMXBeanType$BasicMXBeanType
 794:            30            720  com.google.common.collect.ImmutableEntry
 795:            30            720  java.io.ObjectStreamClass$EntryFuture
 796:            15            720  java.lang.management.PlatformComponent
 797:             9            720  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor
 798:             9            720  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
 799:             1            720  org.apache.cassandra.config.Config
 800:            18            720  org.apache.cassandra.metrics.ThreadPoolMetrics
 801:            22            704  com.sun.jmx.mbeanserver.WeakIdentityHashMap$IdentityWeakReference
 802:            11            704  java.text.SimpleDateFormat
 803:            29            696  org.apache.cassandra.net.MessagingService$Verb
 804:            36            688  [Lsun.reflect.generics.tree.ClassTypeSignature;
 805:            43            688  java.util.regex.Pattern$BranchConn
 806:            17            680  sun.reflect.UnsafeQualifiedStaticLongFieldAccessorImpl
 807:            29            672  [Ljava.lang.reflect.TypeVariable;
 808:            28            672  ch.qos.logback.core.spi.ContextAwareBase
 809:            28            672  java.util.regex.Pattern$Ctype
 810:            28            672  java.util.regex.Pattern$Start
 811:             4            672  jdk.internal.org.objectweb.asm.ClassWriter
 812:            42            672  org.apache.cassandra.config.ColumnDefinition$Raw$Literal
 813:            42            672  org.apache.cassandra.io.sstable.format.big.BigTableScanner$EmptySSTableScanner
 814:            28            672  sun.nio.ch.SocketOptionRegistry$RegistryKey
 815:            12            672  sun.security.ssl.CipherSuite$BulkCipher
 816:            41            656  ch.qos.logback.core.joran.spi.ElementPath
 817:            27            648  java.io.FilePermissionCollection
 818:            27            648  org.apache.cassandra.cql3.selection.RawSelector
 819:            27            648  sun.reflect.generics.tree.FormalTypeParameter
 820:            16            640  io.netty.util.collection.IntObjectHashMap
 821:             8            640  java.util.concurrent.ThreadPoolExecutor
 822:            40            640  java.util.jar.Attributes
 823:             8            640  java.util.zip.ZipEntry
 824:            10            640  jdk.internal.org.objectweb.asm.Label
 825:            20            640  org.apache.cassandra.cql3.functions.BytesConversionFcts$2
 826:            20            640  org.apache.cassandra.db.compaction.OperationType
 827:             3            624  [Ljava.lang.invoke.LambdaForm;
 828:            13            624  java.nio.HeapCharBuffer
 829:            26            624  java.security.spec.ECFieldF2m
 830:            26            624  java.util.regex.Pattern$Ques
 831:            39            624  org.apache.cassandra.serializers.TupleSerializer
 832:            39            624  org.apache.cassandra.serializers.UserTypeSerializer
 833:            27            616  [Ljava.lang.reflect.Constructor;
 834:            19            608  java.io.FileInputStream
 835:            19            608  java.rmi.server.UID
 836:            19            608  java.util.Locale
 837:            19            608  org.apache.cassandra.schema.IndexMetadata
 838:            19            608  sun.management.DiagnosticCommandImpl$Wrapper
 839:            19            608  sun.util.locale.BaseLocale
 840:            15            600  java.lang.ClassNotFoundException
 841:            25            600  java.lang.invoke.Invokers
 842:            25            600  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$HoldCounter
 843:            25            600  org.apache.cassandra.gms.ApplicationState
 844:            25            600  sun.reflect.NativeMethodAccessorImpl
 845:            25            600  sun.reflect.annotation.AnnotationInvocationHandler
 846:            18            576  ch.qos.logback.core.joran.event.BodyEvent
 847:            12            576  java.io.ObjectInputStream$FilterValues
 848:            24            576  jdk.internal.org.objectweb.asm.ByteVector
 849:            12            576  org.apache.cassandra.db.marshal.MapType
 850:             9            576  org.apache.cassandra.metrics.ConnectionMetrics
 851:            24            576  org.apache.cassandra.metrics.ThreadPoolMetricNameFactory
 852:            35            560  ch.qos.logback.core.joran.spi.ElementSelector
 853:            14            560  io.netty.util.Recycler$WeakOrderQueue
 854:            10            560  java.util.zip.ZipFile$ZipFileInflaterInputStream
 855:            10            560  java.util.zip.ZipFile$ZipFileInputStream
 856:            14            560  javax.management.openmbean.SimpleType
 857:            10            560  sun.invoke.util.Wrapper
 858:            23            552  [Ljava.net.InetAddress;
 859:             3            552  [Lorg.apache.cassandra.net.MessagingService$Verb;
 860:            23            552  ch.qos.logback.core.pattern.LiteralConverter
 861:            23            552  io.netty.util.internal.logging.Slf4JLogger
 862:            23            552  org.codehaus.jackson.map.SerializationConfig$Feature
 863:             2            544  [Ljava.lang.Character;
 864:            17            544  io.netty.util.concurrent.DefaultPromise
 865:            34            544  java.io.FilePermission$1
 866:            17            544  java.nio.channels.ClosedChannelException
 867:            17            544  java.util.concurrent.atomic.AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl
 868:            34            544  net.jpountz.xxhash.StreamingXXHash32$1
 869:            17            544  org.apache.cassandra.transport.Message$Type
 870:            17            544  sun.reflect.MethodAccessorGenerator$1
 871:            17            544  sun.security.x509.DistributionPoint
 872:            17            544  sun.security.x509.URIName
 873:            22            528  java.net.URLClassLoader$1
 874:            22            528  org.apache.cassandra.cql3.CQL3Type$Native
 875:            33            528  sun.reflect.DelegatingMethodAccessorImpl
 876:            13            520  com.google.common.base.Stopwatch
 877:            13            520  io.netty.channel.unix.Errors$NativeIoException
 878:            13            520  java.lang.invoke.MethodHandleImpl$IntrinsicMethodHandle
 879:            13            520  java.text.DigitList
 880:             4            512  com.google.common.cache.LocalCache
 881:            16            512  io.netty.channel.epoll.IovArray
 882:            16            512  java.lang.NoSuchFieldException
 883:            32            512  java.util.TreeSet
 884:            16            512  java.util.concurrent.Semaphore$NonfairSync
 885:            16            512  sun.security.ssl.CipherSuite$KeyExchange
 886:            21            504  java.util.Locale$LocaleKey
 887:             9            504  java.util.concurrent.ConcurrentHashMap$ValueIterator
 888:            21            504  org.apache.cassandra.cql3.functions.AggregateFcts$24
 889:             9            504  org.apache.cassandra.net.RateBasedBackPressureState
 890:            21            504  sun.security.x509.AVAKeyword
 891:            31            496  sun.security.x509.GeneralName
 892:            19            488  [Lsun.management.DiagnosticCommandArgumentInfo;
 893:            20            480  java.io.ObjectStreamClass$2
 894:            12            480  java.lang.UNIXProcess$ProcessPipeInputStream
 895:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$22
 896:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$23
 897:            20            480  org.apache.cassandra.cql3.functions.BytesConversionFcts$1
 898:            20            480  org.apache.cassandra.dht.LocalPartitioner
 899:            15            480  org.apache.cassandra.index.internal.composites.RegularColumnIndex
 900:             6            480  org.apache.cassandra.repair.RepairSession
 901:            20            480  org.yaml.snakeyaml.tokens.Token$ID
 902:             6            480  sun.net.www.protocol.jar.URLJarFile
 903:            30            480  sun.security.x509.GeneralNames
 904:             6            456  [Lsun.invoke.util.Wrapper;
 905:            19            456  ch.qos.logback.classic.spi.ClassPackagingData
 906:            19            456  java.lang.Class$1
 907:            19            456  java.util.regex.Pattern$Dollar
 908:             5            448  [[Ljava.lang.Object;
 909:             7            448  java.security.SecureRandom
 910:            28            448  java.util.LinkedHashSet
 911:             8            448  javax.management.openmbean.OpenMBeanParameterInfoSupport
 912:             8            448  jdk.internal.org.objectweb.asm.AnnotationWriter
 913:            14            448  jdk.internal.org.objectweb.asm.Type
 914:            14            448  sun.security.x509.CRLDistributionPointsExtension
 915:            11            440  java.lang.ClassLoader$NativeLibrary
 916:            11            440  sun.security.ec.ECPublicKeyImpl
 917:             9            432  com.sun.jna.Function
 918:            27            432  java.security.spec.ECFieldFp
 919:            18            432  java.text.DateFormat$Field
 920:            18            432  java.util.Collections$UnmodifiableCollection$1
 921:            18            432  org.apache.cassandra.exceptions.ExceptionCode
 922:            18            432  org.apache.cassandra.io.util.WrappedDataOutputStreamPlus
 923:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$1
 924:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$2
 925:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$3
 926:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$4
 927:             9            432  org.apache.cassandra.net.OutboundTcpConnectionPool
 928:            18            432  org.cliffc.high_scale_lib.NonBlockingHashMap$NBHMEntry
 929:            13            416  io.netty.util.Recycler$WeakOrderQueue$Link
 930:            13            416  java.lang.invoke.SimpleMethodHandle
 931:            13            416  java.security.AlgorithmParameters
 932:            13            416  java.util.Stack
 933:             4            416  sun.net.www.protocol.file.FileURLConnection
 934:            17            408  org.apache.cassandra.utils.IntegerInterval
 935:            17            408  org.codehaus.jackson.map.DeserializationConfig$Feature
 936:            10            400  java.io.ObjectStreamClass$FieldReflector
 937:            10            400  java.lang.invoke.DirectMethodHandle$Accessor
 938:            10            400  javax.crypto.CryptoPermission
 939:            10            400  sun.reflect.generics.repository.MethodRepository
 940:             7            392  java.util.Calendar$Builder
 941:             1            392  org.apache.cassandra.utils.memory.MemtableCleanerThread
 942:             8            384  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong;
 943:             1            384  ch.qos.logback.core.AsyncAppenderBase$Worker
 944:             4            384  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap
 945:            16            384  io.netty.channel.epoll.EpollEventArray
 946:            12            384  java.io.EOFException
 947:             1            384  java.lang.ref.Finalizer$FinalizerThread
 948:             8            384  java.net.SocketInputStream
 949:             8            384  java.net.SocketOutputStream
 950:            12            384  java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue
 951:            12            384  java.util.concurrent.atomic.AtomicLongFieldUpdater$CASUpdater
 952:             1            384  java.util.logging.LogManager$Cleaner
 953:            16            384  javax.management.StandardMBean
 954:            16            384  org.apache.cassandra.cql3.Attributes
 955:            16            384  org.apache.cassandra.cql3.Constants$Setter
 956:            16            384  org.apache.cassandra.cql3.Operations
 957:            12            384  org.apache.cassandra.cql3.SingleColumnRelation
 958:             8            384  org.apache.cassandra.hints.HintsStore
 959:            16            384  org.apache.cassandra.metrics.TableMetrics$35
 960:             1            384  org.apache.cassandra.net.MessagingService$SocketThread
 961:            16            384  org.apache.cassandra.schema.TableParams$Option
 962:             1            384  org.apache.cassandra.thrift.ThriftServer$ThriftServerThread
 963:            16            384  sun.misc.MetaIndex
 964:            16            384  sun.nio.ch.OptionKey
 965:             3            384  sun.nio.fs.UnixFileAttributes
 966:            12            384  sun.nio.fs.UnixPath
 967:             1            376  java.lang.ref.Reference$ReferenceHandler
 968:            16            368  [Ljava.security.cert.Certificate;
 969:            17            368  [Ljavax.management.MBeanNotificationInfo;
 970:            23            368  java.lang.ThreadLocal
 971:             3            360  [Lorg.apache.cassandra.gms.ApplicationState;
 972:            15            360  com.sun.jmx.remote.util.ClassLogger
 973:             9            360  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit
 974:             9            360  java.io.BufferedInputStream
 975:            15            360  java.io.ObjectStreamClass$ClassDataSlot
 976:            15            360  java.net.InetAddress
 977:             9            360  org.apache.cassandra.db.marshal.SetType
 978:            15            360  org.apache.cassandra.utils.memory.SlabAllocator$Region
 979:            11            352  java.lang.ClassLoader$1
 980:            11            352  java.util.concurrent.SynchronousQueue
 981:            11            352  org.apache.cassandra.db.ConsistencyLevel
 982:             4            352  sun.rmi.transport.ConnectionInputStream
 983:             7            336  [Ljavax.management.openmbean.OpenType;
 984:             7            336  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CompositeMapping
 985:            14            336  java.lang.invoke.LambdaFormEditor$Transform$Kind
 986:             6            336  java.nio.DirectLongBufferU
 987:            21            336  java.util.Collections$UnmodifiableCollection
 988:             7            336  java.util.Properties
 989:             6            336  org.apache.cassandra.concurrent.SEPExecutor
 990:             6            336  sun.management.MemoryPoolImpl
 991:             5            328  [Ljava.io.ObjectInputStream$HandleTable$HandleList;
 992:            16            328  [Ljava.lang.management.PlatformComponent;
 993:             4            320  [Lio.netty.buffer.PoolArena;
 994:            10            320  [Ljava.lang.invoke.LambdaForm$BasicType;
 995:            10            320  java.io.FileOutputStream
 996:             8            320  java.io.ObjectOutputStream$HandleTable
 997:            10            320  java.lang.OutOfMemoryError
 998:            10            320  java.lang.StringCoding$StringEncoder
 999:            10            320  java.lang.reflect.WeakCache$CacheValue
1000:            10            320  java.security.cert.PolicyQualifierInfo
1001:             8            320  org.apache.cassandra.db.marshal.ListType
1002:            20            320  org.apache.cassandra.dht.LocalPartitioner$1
1003:             8            320  org.apache.cassandra.gms.ArrayBackedBoundedStats
1004:             8            320  org.apache.cassandra.gms.ArrivalWindow
1005:            10            320  sun.reflect.generics.tree.MethodTypeSignature
1006:             8            320  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler
1007:            10            320  sun.security.util.DisabledAlgorithmConstraints$KeySizeConstraint
1008:            13            312  [Ljava.net.InetSocketAddress;
1009:            13            312  com.sun.jna.Pointer
1010:            13            312  java.lang.management.ManagementPermission
1011:            19            304  sun.reflect.BootstrapConstructorAccessorImpl
1012:             1            296  com.github.benmanes.caffeine.SingleConsumerQueue
1013:             1            296  com.github.benmanes.caffeine.cache.BoundedBuffer$RingBuffer
1014:             4            288  [Lch.qos.logback.classic.spi.StackTraceElementProxy;
1015:            12            288  [Lcom.codahale.metrics.Striped64$Cell;
1016:            12            288  ch.qos.logback.core.joran.spi.HostClassAndPropertyDouble
1017:             1            288  com.github.benmanes.caffeine.cache.LocalCacheFactory$SSLiMW
1018:             6            288  com.google.common.collect.HashBiMap
1019:             9            288  com.google.common.collect.RegularImmutableSet
1020:             4            288  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8
1021:            12            288  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ListenerWrapper
1022:             6            288  java.io.BufferedReader
1023:            12            288  java.lang.ProcessEnvironment$Variable
1024:             9            288  java.lang.reflect.Proxy$Key1
1025:             9            288  java.util.concurrent.CountDownLatch$Sync
1026:             9            288  java.util.concurrent.SynchronousQueue$TransferStack$SNode
1027:             9            288  java.util.logging.Level
1028:            18            288  java.util.regex.Pattern$Begin
1029:            12            288  org.apache.cassandra.concurrent.Stage
1030:            18            288  org.apache.cassandra.io.util.DataOutputStreamPlus$2
1031:             4            288  org.apache.cassandra.locator.TokenMetadata
1032:             9            288  org.apache.commons.lang3.JavaVersion
1033:             6            288  sun.nio.cs.StreamDecoder
1034:            18            288  sun.reflect.Label
1035:             4            288  sun.rmi.transport.ConnectionOutputStream
1036:             9            288  sun.security.jca.ProviderConfig
1037:             7            280  java.net.SocketTimeoutException
1038:             7            280  org.apache.cassandra.streaming.messages.StreamMessage$Type
1039:             7            280  org.apache.thrift.transport.TTransportException
1040:             7            280  sun.misc.FloatingDecimal$BinaryToASCIIBuffer
1041:             7            280  sun.rmi.transport.tcp.TCPEndpoint
1042:             1            272  [Lorg.codehaus.jackson.sym.Name;
1043:            17            272  com.sun.proxy.$Proxy3
1044:            17            272  net.jpountz.lz4.LZ4HCJNICompressor
1045:            17            272  org.apache.cassandra.cql3.Constants$Value
1046:            17            272  sun.reflect.ClassDefiner$1
1047:            17            272  sun.security.x509.DNSName
1048:             3            264  [[D
1049:            11            264  com.google.common.collect.ImmutableMapValues
1050:            11            264  java.net.StandardSocketOptions$StdSocketOption
1051:            11            264  java.rmi.server.ObjID
1052:            11            264  java.util.regex.Pattern$SliceI
1053:            11            264  org.apache.cassandra.io.sstable.Component
1054:            11            264  org.apache.cassandra.io.sstable.Component$Type
1055:            11            264  org.apache.cassandra.metrics.DroppedMessageMetrics
1056:            11            264  org.apache.cassandra.metrics.TableMetrics$36
1057:            11            264  org.apache.cassandra.net.MessagingService$DroppedMessages
1058:            11            264  sun.rmi.transport.ObjectEndpoint
1059:            11            264  sun.security.util.DisabledAlgorithmConstraints$DisabledConstraint
1060:            10            256  [Ljava.io.ObjectStreamClass$ClassDataSlot;
1061:             8            256  com.google.common.cache.LocalCache$StrongEntry
1062:            16            256  io.netty.channel.epoll.EpollEventLoop$1
1063:            16            256  io.netty.channel.epoll.EpollEventLoop$2
1064:            16            256  io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator
1065:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$2
1066:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$DefaultThreadProperties
1067:             8            256  java.util.Collections$UnmodifiableMap
1068:            16            256  java.util.concurrent.Semaphore
1069:             8            256  javax.management.MBeanNotificationInfo
1070:             8            256  org.apache.cassandra.cql3.functions.CastFcts$JavaCounterFunctionWrapper
1071:             8            256  org.apache.cassandra.db.ClusteringPrefix$Kind
1072:             8            256  org.apache.cassandra.repair.messages.RepairMessage$Type
1073:             8            256  sun.management.NotificationEmitterSupport$ListenerInfo
1074:             8            256  sun.misc.ProxyGenerator$PrimitiveTypeInfo
1075:             8            256  sun.misc.URLClassPath$JarLoader$2
1076:             8            256  sun.security.x509.CertificatePoliciesExtension
1077:             6            240  [Ljava.lang.invoke.BoundMethodHandle$SpeciesData;
1078:            10            240  com.sun.org.apache.xerces.internal.impl.XMLScanner$NameType
1079:            10            240  java.io.BufferedOutputStream
1080:             6            240  java.lang.UNIXProcess
1081:            10            240  java.nio.file.StandardOpenOption
1082:            10            240  java.security.CryptoPrimitive
1083:             3            240  java.util.concurrent.ScheduledThreadPoolExecutor
1084:            15            240  java.util.regex.Pattern$Dot
1085:            10            240  org.apache.cassandra.auth.Permission
1086:             5            240  org.apache.cassandra.config.ViewDefinition
1087:             5            240  org.apache.cassandra.db.lifecycle.LogRecord
1088:             5            240  org.apache.cassandra.db.view.View
1089:             6            240  org.apache.cassandra.metrics.SEPMetrics
1090:             6            240  org.apache.cassandra.schema.KeyspaceMetadata
1091:            10            240  org.codehaus.jackson.JsonParser$Feature
1092:            10            240  org.yaml.snakeyaml.events.Event$ID
1093:            15            240  org.yaml.snakeyaml.nodes.Tag
1094:             6            240  sun.management.MemoryPoolImpl$CollectionSensor
1095:             6            240  sun.management.MemoryPoolImpl$PoolSensor
1096:             5            240  sun.misc.URLClassPath
1097:            10            240  sun.reflect.generics.scope.MethodScope
1098:            15            240  sun.reflect.generics.tree.TypeVariableSignature
1099:            10            240  sun.rmi.runtime.Log$LoggerLog
1100:            10            240  sun.security.x509.Extension
1101:             5            240  sun.util.locale.provider.LocaleResources$ResourceReference
1102:             8            232  [Ljava.lang.Boolean;
1103:             2            224  [Lorg.codehaus.jackson.map.SerializationConfig$Feature;
1104:             7            224  [Lsun.nio.fs.NativeBuffer;
1105:             7            224  com.google.common.util.concurrent.MoreExecutors$DirectExecutorService
1106:             4            224  java.io.ObjectInputStream$BlockDataInputStream
1107:            14            224  java.rmi.server.Operation
1108:             7            224  java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
1109:             7            224  java.util.regex.Pattern$BnM
1110:             7            224  org.codehaus.jackson.JsonGenerator$Feature
1111:             4            224  org.codehaus.jackson.map.introspect.AnnotatedClass
1112:             4            224  org.codehaus.jackson.map.introspect.BasicBeanDescription
1113:             7            224  sun.nio.fs.NativeBuffer
1114:             7            224  sun.reflect.annotation.AnnotationType
1115:             4            224  sun.rmi.transport.Target
1116:             7            224  sun.security.x509.NetscapeCertTypeExtension
1117:             9            216  java.lang.ProcessEnvironment$Value
1118:             9            216  java.util.Collections$SynchronizedSet
1119:             9            216  java.util.logging.Level$KnownLevel
1120:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$1
1121:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$2
1122:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$3
1123:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$4
1124:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$5
1125:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$6
1126:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$7
1127:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$8
1128:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$9
1129:             3            216  sun.security.provider.NativePRNG$RandomIO
1130:             9            216  sun.util.logging.PlatformLogger$Level
1131:             7            208  [Ljava.lang.invoke.LambdaForm$NamedFunction;
1132:             2            208  [Lorg.apache.cassandra.cql3.CQL3Type$Native;
1133:            13            208  com.google.common.util.concurrent.RateLimiter$SleepingStopwatch$1
1134:             2            208  java.lang.invoke.InnerClassLambdaMetafactory
1135:            13            208  sun.nio.ch.SocketAdaptor$2
1136:             2            200  [Ljava.text.DateFormat$Field;
1137:             5            200  io.netty.channel.group.DefaultChannelGroup
1138:             5            200  java.lang.invoke.BoundMethodHandle$SpeciesData
1139:             5            200  java.lang.invoke.DirectMethodHandle$Constructor
1140:             5            200  java.util.stream.StreamOpFlag
1141:             5            200  org.apache.cassandra.cql3.statements.SelectStatement$RawStatement
1142:             5            200  org.apache.cassandra.db.view.ViewBuilder
1143:             5            200  sun.rmi.transport.WeakRef
1144:             6            192  [Ljava.rmi.server.Operation;
1145:             3            192  [Lorg.apache.cassandra.db.ConsistencyLevel;
1146:             4            192  [[Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
1147:             3            192  ch.qos.logback.classic.PatternLayout
1148:             6            192  ch.qos.logback.core.util.CachingDateFormatter
1149:             8            192  com.google.common.cache.LocalCache$AccessQueue$1
1150:             4            192  com.google.common.collect.TreeMultimap
1151:             6            192  java.lang.ProcessBuilder
1152:             6            192  java.lang.invoke.LambdaForm$BasicType
1153:             8            192  java.lang.invoke.MethodHandleImpl$Intrinsic
1154:             8            192  java.math.RoundingMode
1155:            12            192  java.util.concurrent.ConcurrentSkipListMap$EntrySet
1156:             4            192  java.util.concurrent.locks.ReentrantReadWriteLock$FairSync
1157:             8            192  java.util.regex.Pattern$7
1158:             8            192  javax.crypto.CryptoPermissionCollection
1159:             4            192  javax.management.openmbean.TabularType
1160:             3            192  jdk.internal.org.objectweb.asm.FieldWriter
1161:             4            192  jdk.internal.org.objectweb.asm.Frame
1162:             8            192  jdk.net.SocketFlow$Status
1163:             6            192  org.apache.cassandra.db.Keyspace
1164:             4            192  org.apache.cassandra.db.RangeTombstoneList
1165:             8            192  org.apache.cassandra.db.WriteType
1166:             8            192  org.apache.cassandra.serializers.MapSerializer
1167:             8            192  org.apache.cassandra.serializers.SetSerializer
1168:             8            192  org.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State
1169:             8            192  org.apache.cassandra.service.StorageService$Mode
1170:             3            192  org.apache.cassandra.utils.MerkleTree$TreeDifference
1171:             6            192  org.apache.commons.lang3.text.StrBuilder
1172:             8            192  org.yaml.snakeyaml.scanner.Constant
1173:            12            192  sun.nio.ch.SocketAdaptor$1
1174:             6            192  sun.rmi.runtime.NewThreadAction
1175:             2            192  sun.security.provider.Sun
1176:             6            192  sun.security.util.MemoryCache
1177:             8            192  sun.security.x509.PolicyInformation
1178:             2            176  [Lorg.apache.cassandra.transport.Message$Type;
1179:             2            176  [Lorg.codehaus.jackson.map.DeserializationConfig$Feature;
1180:            10            176  [Lsun.reflect.generics.tree.TypeSignature;
1181:            11            176  java.text.NumberFormat$Field
1182:            11            176  java.util.LinkedHashMap$LinkedEntrySet
1183:            11            176  java.util.concurrent.SynchronousQueue$TransferStack
1184:             2            176  javax.management.remote.rmi.NoCallStackClassLoader
1185:             2            176  org.apache.cassandra.db.commitlog.MemoryMappedSegment
1186:            11            176  sun.security.ec.ECParameters
1187:             1            168  [[Ljava.math.BigInteger;
1188:             7            168  ch.qos.logback.classic.Level
1189:             3            168  ch.qos.logback.classic.encoder.PatternLayoutEncoder
1190:             7            168  com.google.common.collect.ImmutableEnumSet
1191:             7            168  com.sun.management.VMOption$Origin
1192:             7            168  com.sun.org.apache.xerces.internal.util.FeatureState
1193:             7            168  java.lang.invoke.MethodHandles$Lookup
1194:             7            168  java.net.NetPermission
1195:             7            168  java.util.BitSet
1196:             3            168  javax.management.openmbean.OpenMBeanOperationInfoSupport
1197:             7            168  javax.security.auth.AuthPermission
1198:             7            168  org.apache.cassandra.cql3.Constants$Type
1199:             7            168  org.apache.cassandra.db.Directories$FileAction
1200:             7            168  org.apache.cassandra.utils.concurrent.SimpleCondition
1201:             7            168  org.apache.cassandra.utils.progress.ProgressEventType
1202:             7            168  org.codehaus.jackson.annotate.JsonMethod
1203:             7            168  sun.nio.fs.NativeBuffer$Deallocator
1204:             7            168  sun.rmi.server.LoaderHandler$LoaderKey
1205:             3            168  sun.rmi.transport.tcp.TCPChannel
1206:             3            168  sun.rmi.transport.tcp.TCPConnection
1207:             3            168  sun.security.provider.SHA
1208:             7            168  sun.security.x509.NetscapeCertTypeExtension$MapEntry
1209:             4            160  [F
1210:             2            160  ch.qos.logback.core.rolling.RollingFileAppender
1211:            10            160  io.netty.util.internal.ConcurrentSet
1212:             4            160  java.io.ObjectOutputStream$BlockDataOutputStream
1213:             5            160  java.io.SerializablePermission
1214:             5            160  java.lang.StringCoding$StringDecoder
1215:             5            160  javax.management.StandardEmitterMBean
1216:             5            160  org.apache.cassandra.db.marshal.CompositeType
1217:             5            160  org.apache.cassandra.repair.RepairRunnable$1
1218:             5            160  org.apache.cassandra.transport.ProtocolVersion
1219:             5            160  org.apache.cassandra.transport.messages.ResultMessage$Kind
1220:             5            160  org.apache.cassandra.utils.CassandraVersion
1221:             4            160  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotV
1222:             5            160  sun.rmi.transport.StreamRemoteCall
1223:             5            160  sun.security.ssl.CipherSuite$MacAlg
1224:            10            160  sun.security.x509.CertificatePolicyId
1225:             5            160  sun.util.locale.provider.LocaleProviderAdapter$Type
1226:             6            144  [Ljava.io.Closeable;
1227:             2            144  [Ljava.math.BigDecimal;
1228:             1            144  [Ljava.util.concurrent.ForkJoinTask$ExceptionNode;
1229:             1            144  [Lorg.codehaus.jackson.sym.CharsToNameCanonicalizer$Bucket;
1230:             3            144  ch.qos.logback.classic.pattern.DateConverter
1231:             3            144  ch.qos.logback.classic.pattern.ExtendedThrowableProxyConverter
1232:             3            144  ch.qos.logback.classic.spi.ThrowableProxy
1233:             6            144  com.google.common.collect.AbstractMultimap$EntrySet
1234:             6            144  com.sun.org.apache.xerces.internal.util.Status
1235:             6            144  java.io.InputStreamReader
1236:             3            144  java.lang.ThreadGroup
1237:             6            144  java.lang.UNIXProcess$$Lambda$15/1221027335
1238:             6            144  java.lang.UNIXProcess$ProcessPipeOutputStream
1239:             9            144  java.util.concurrent.CountDownLatch
1240:             6            144  java.util.regex.Pattern$CharProperty$1
1241:             2            144  org.antlr.runtime.RecognizerSharedState
1242:             6            144  org.apache.cassandra.cql3.CFName
1243:             6            144  org.apache.cassandra.cql3.WhereClause
1244:             6            144  org.apache.cassandra.db.filter.DataLimits$Kind
1245:             6            144  org.apache.cassandra.db.view.ViewManager
1246:             3            144  org.apache.cassandra.locator.SimpleStrategy
1247:             3            144  org.apache.cassandra.metrics.CacheMetrics
1248:             6            144  org.apache.cassandra.metrics.SEPMetrics$1
1249:             6            144  org.apache.cassandra.metrics.SEPMetrics$2
1250:             6            144  org.apache.cassandra.metrics.SEPMetrics$3
1251:             6            144  org.apache.cassandra.metrics.SEPMetrics$4
1252:             6            144  org.apache.cassandra.schema.KeyspaceParams
1253:             6            144  org.apache.cassandra.schema.ReplicationParams
1254:             6            144  org.apache.cassandra.service.ActiveRepairService$1
1255:             6            144  org.apache.cassandra.service.ActiveRepairService$2
1256:             6            144  org.apache.cassandra.streaming.StreamSession$State
1257:             6            144  org.codehaus.jackson.annotate.JsonAutoDetect$Visibility
1258:             6            144  org.github.jamm.MemoryMeter$Guess
1259:             6            144  sun.misc.PerfCounter
1260:             6            144  sun.security.ssl.ProtocolVersion
1261:             6            144  sun.security.util.DisabledAlgorithmConstraints$Constraint$Operator
1262:             4            128  [Lcom.google.common.cache.LocalCache$Segment;
1263:             4            128  [Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
1264:             2            128  [Lorg.apache.cassandra.concurrent.Stage;
1265:             2            128  [Lorg.apache.cassandra.io.sstable.Component$Type;
1266:             2            128  ch.qos.logback.core.rolling.FixedWindowRollingPolicy
1267:             4            128  ch.qos.logback.core.rolling.helper.FileNamePattern
1268:             8            128  com.google.common.cache.LocalCache$AccessQueue
1269:             8            128  com.google.common.cache.LocalCache$StrongValueReference
1270:             4            128  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$ArrayMapping
1271:             2            128  java.io.ExpiringCache$1
1272:             4            128  java.io.ObjectInputStream$HandleTable
1273:             4            128  java.io.ObjectInputStream$PeekInputStream
1274:             4            128  java.lang.UNIXProcess$Platform
1275:             2            128  java.lang.invoke.InvokerBytecodeGenerator
1276:             4            128  java.util.Random
1277:             4            128  java.util.concurrent.ExecutionException
1278:             4            128  net.jpountz.util.Native$OS
1279:             4            128  org.apache.cassandra.cql3.functions.CastFcts$CassandraFunctionWrapper
1280:             4            128  org.apache.cassandra.db.marshal.ReversedType
1281:             1            128  org.apache.cassandra.io.compress.CompressedSequentialWriter
1282:             1            128  org.apache.cassandra.io.sstable.format.big.BigTableWriter
1283:             4            128  org.apache.cassandra.io.util.SequentialWriterOption
1284:             4            128  org.apache.cassandra.locator.PendingRangeMaps
1285:             2            128  org.apache.cassandra.metrics.CASClientRequestMetrics
1286:             8            128  org.apache.cassandra.serializers.MapSerializer$$Lambda$24/2072313080
1287:             8            128  sun.net.www.ParseUtil
1288:             4            128  sun.rmi.transport.LiveRef
1289:             8            128  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$292/1509453068
1290:             4            128  sun.security.ssl.CipherSuite$PRF
1291:             4            128  sun.security.x509.ExtendedKeyUsageExtension
1292:             3            120  [Lorg.codehaus.jackson.annotate.JsonMethod;
1293:             1            120  [[Ljava.lang.String;
1294:             5            120  ch.qos.logback.core.pattern.parser.TokenStream$TokenizerState
1295:             5            120  ch.qos.logback.core.subst.Token$Type
1296:             5            120  ch.qos.logback.core.util.AggregationType
1297:             3            120  com.google.common.collect.AbstractMapBasedMultimap$AsMap
1298:             5            120  com.sun.org.apache.xerces.internal.util.PropertyState
1299:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State
1300:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State
1301:             3            120  java.lang.invoke.BoundMethodHandle$Species_LL
1302:             3            120  java.lang.invoke.MethodHandleImpl$AsVarargsCollector
1303:             5            120  java.util.stream.StreamOpFlag$Type
1304:             3            120  org.apache.cassandra.cache.AutoSavingCache
1305:             5            120  org.apache.cassandra.config.Config$DiskFailurePolicy
1306:             5            120  org.apache.cassandra.cql3.VariableSpecifications
1307:             5            120  org.apache.cassandra.cql3.statements.IndexTarget$Type
1308:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Status
1309:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Type
1310:             3            120  org.apache.cassandra.db.lifecycle.LogTransaction$SSTableTidier
1311:             3            120  org.apache.cassandra.index.internal.composites.ClusteringColumnIndex
1312:             5            120  org.apache.cassandra.schema.CompactionParams$Option
1313:             1            120  org.apache.cassandra.service.StorageService
1314:             5            120  org.apache.cassandra.utils.NativeLibrary$OSType
1315:             5            120  org.yaml.snakeyaml.DumperOptions$ScalarStyle
1316:             5            120  sun.misc.FloatingDecimal$PreparedASCIIToBinaryBuffer
1317:             5            120  sun.security.jca.ServiceId
1318:             5            120  sun.security.util.DisabledAlgorithmConstraints
1319:             2            112  [Ljava.lang.invoke.MethodType;
1320:             2            112  [Ljava.security.CryptoPrimitive;
1321:             2            112  [Ljava.util.List;
1322:             2            112  [Lorg.apache.cassandra.auth.Permission;
1323:             2            112  [Lorg.apache.cassandra.db.PartitionPosition;
1324:             3            112  [Lorg.apache.cassandra.transport.ProtocolVersion;
1325:             7            112  com.google.common.util.concurrent.MoreExecutors$ListeningDecorator
1326:             2            112  com.sun.management.GcInfo
1327:             2            112  io.netty.buffer.PooledByteBufAllocator
1328:             7            112  java.util.concurrent.ConcurrentHashMap$EntrySetView
1329:             2            112  org.apache.cassandra.cql3.statements.DeleteStatement
1330:             2            112  org.apache.cassandra.db.compaction.LeveledCompactionStrategy
1331:             2            112  org.apache.cassandra.repair.LocalSyncTask
1332:             7            112  org.apache.cassandra.serializers.ListSerializer
1333:             2            112  org.apache.cassandra.utils.memory.MemtablePool$SubPool
1334:             7            112  sun.security.provider.NativePRNG
1335:             1            104  com.codahale.metrics.ThreadLocalRandom
1336:             1            104  io.netty.channel.epoll.EpollServerSocketChannel
1337:             1            104  org.apache.cassandra.db.ColumnIndex
1338:             1            104  sun.rmi.server.LoaderHandler$Loader
1339:             2             96  [Lcom.google.common.cache.LocalCache$EntryFactory;
1340:             6             96  [Ljava.io.ObjectStreamClass$MemberSignature;
1341:             2             96  [Ljava.util.concurrent.TimeUnit;
1342:             1             96  [Lorg.apache.cassandra.db.compaction.OperationType;
1343:             2             96  [Lorg.apache.cassandra.repair.messages.RepairMessage$Type;
1344:             1             96  [Lorg.yaml.snakeyaml.tokens.Token$ID;
1345:             1             96  [[J
1346:             1             96  ch.qos.logback.classic.LoggerContext
1347:             3             96  ch.qos.logback.classic.pattern.FileOfCallerConverter
1348:             3             96  ch.qos.logback.classic.pattern.LevelConverter
1349:             3             96  ch.qos.logback.classic.pattern.LineOfCallerConverter
1350:             3             96  ch.qos.logback.classic.pattern.LineSeparatorConverter
1351:             3             96  ch.qos.logback.classic.pattern.MessageConverter
1352:             3             96  ch.qos.logback.classic.pattern.ThreadConverter
1353:             3             96  ch.qos.logback.core.joran.action.AppenderRefAction
1354:             4             96  ch.qos.logback.core.pattern.parser.Token
1355:             2             96  ch.qos.logback.core.recovery.ResilientFileOutputStream
1356:             2             96  ch.qos.logback.core.rolling.helper.DateTokenConverter
1357:             4             96  ch.qos.logback.core.subst.Token
1358:             2             96  ch.qos.logback.core.util.InvocationGate
1359:             4             96  com.google.common.cache.LocalCache$WriteQueue$1
1360:             4             96  com.google.common.collect.AbstractIterator$State
1361:             4             96  com.google.common.collect.Iterators$12
1362:             4             96  com.googlecode.concurrentlinkedhashmap.LinkedDeque
1363:             3             96  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$EnumMapping
1364:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$MBeanInfoMap
1365:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$PerInterfaceMap
1366:             1             96  com.sun.net.ssl.internal.ssl.Provider
1367:             3             96  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap
1368:             3             96  io.netty.buffer.EmptyByteBuf
1369:             3             96  java.io.ByteArrayInputStream
1370:             6             96  java.io.FileInputStream$1
1371:             4             96  java.io.ObjectOutputStream$ReplaceTable
1372:             6             96  java.lang.UNIXProcess$$Lambda$16/1801942731
1373:             6             96  java.net.Socket$2
1374:             6             96  java.net.Socket$3
1375:             4             96  java.net.URLClassLoader$2
1376:             4             96  java.nio.file.FileVisitResult
1377:             4             96  java.text.Normalizer$Form
1378:             6             96  java.util.LinkedHashMap$LinkedKeySet
1379:             2             96  java.util.concurrent.ArrayBlockingQueue
1380:             3             96  java.util.concurrent.ConcurrentHashMap$ForwardingNode
1381:             3             96  java.util.concurrent.locks.ReentrantLock$FairSync
1382:             4             96  java.util.stream.StreamShape
1383:             4             96  javax.management.NotificationBroadcasterSupport$ListenerInfo
1384:             4             96  org.apache.cassandra.auth.IRoleManager$Option
1385:             4             96  org.apache.cassandra.config.CFMetaData$Flag
1386:             4             96  org.apache.cassandra.config.ColumnDefinition$Kind
1387:             4             96  org.apache.cassandra.config.Config$CommitFailurePolicy
1388:             4             96  org.apache.cassandra.config.Config$DiskAccessMode
1389:             4             96  org.apache.cassandra.config.Config$MemtableAllocationType
1390:             4             96  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption
1391:             1             96  org.apache.cassandra.cql3.Cql_Parser
1392:             4             96  org.apache.cassandra.db.SystemKeyspace$BootstrapState
1393:             2             96  org.apache.cassandra.db.compaction.LeveledManifest
1394:             4             96  org.apache.cassandra.db.context.CounterContext$Relationship
1395:             4             96  org.apache.cassandra.db.lifecycle.LogTransaction$Obsoletion
1396:             4             96  org.apache.cassandra.dht.Bounds
1397:             4             96  org.apache.cassandra.hints.HintsDispatcher$Callback$Outcome
1398:             4             96  org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys
1399:             4             96  org.apache.cassandra.io.sstable.format.SSTableReader$OpenReason
1400:             4             96  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason
1401:             4             96  org.apache.cassandra.io.sstable.metadata.MetadataType
1402:             2             96  org.apache.cassandra.io.util.FileHandle$Builder
1403:             2             96  org.apache.cassandra.locator.LocalStrategy
1404:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$1
1405:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$10
1406:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$11
1407:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$12
1408:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$13
1409:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$14
1410:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$15
1411:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$16
1412:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$2
1413:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$3
1414:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$4
1415:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$5
1416:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$6
1417:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$7
1418:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$8
1419:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$9
1420:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$KeyspaceMetricNameFactory
1421:             6             96  org.apache.cassandra.schema.Functions
1422:             4             96  org.apache.cassandra.schema.SpeculativeRetryParam$Kind
1423:             6             96  org.apache.cassandra.schema.Tables
1424:             6             96  org.apache.cassandra.schema.Views
1425:             4             96  org.apache.cassandra.transport.Event$Type
1426:             1             96  org.apache.cassandra.triggers.CustomClassLoader
1427:             4             96  org.apache.cassandra.utils.AbstractIterator$State
1428:             4             96  org.apache.cassandra.utils.AsymmetricOrdering$Op
1429:             3             96  org.apache.cassandra.utils.NoSpamLogger
1430:             4             96  org.apache.cassandra.utils.SortedBiMultiValMap
1431:             4             96  org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State
1432:             2             96  org.codehaus.jackson.map.MapperConfig$Base
1433:             4             96  org.yaml.snakeyaml.nodes.NodeId
1434:             2             96  sun.management.GarbageCollectorImpl
1435:             2             96  sun.management.GcInfoBuilder
1436:             4             96  sun.misc.FormattedFloatingDecimal$Form
1437:             1             96  sun.misc.Launcher$AppClassLoader
1438:             4             96  sun.net.www.MessageHeader
1439:             1             96  sun.nio.ch.ServerSocketChannelImpl
1440:             2             96  sun.nio.cs.StreamEncoder
1441:             6             96  sun.rmi.transport.Transport$$Lambda$295/399097450
1442:             3             96  sun.rmi.transport.Transport$1
1443:             1             96  sun.security.ec.SunEC
1444:             1             96  sun.security.jca.ProviderList$1
1445:             1             96  sun.security.rsa.SunRsaSign
1446:             3             96  sun.security.ssl.ProtocolList
1447:             4             88  [Ljava.util.Map$Entry;
1448:             1             88  [Lnet.jpountz.lz4.LZ4Compressor;
1449:             1             88  [Lorg.apache.cassandra.exceptions.ExceptionCode;
1450:             1             88  [Lsun.security.util.ObjectIdentifier;
1451:             1             88  [[Ljava.lang.Byte;
1452:             1             88  java.util.jar.JarVerifier
1453:             1             88  org.apache.cassandra.concurrent.JMXConfigurableThreadPoolExecutor
1454:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CacheCleanupExecutor
1455:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CompactionExecutor
1456:             1             88  org.apache.cassandra.db.compaction.CompactionManager$ValidationExecutor
1457:             1             88  org.apache.cassandra.gms.Gossiper
1458:             1             88  org.apache.cassandra.io.sstable.IndexSummaryBuilder
1459:             1             88  org.apache.cassandra.io.sstable.metadata.MetadataCollector
1460:             1             88  sun.misc.Launcher$ExtClassLoader
1461:             1             80  [Lio.netty.util.concurrent.SingleThreadEventExecutor;
1462:             2             80  [Ljava.lang.management.MemoryUsage;
1463:             2             80  [Ljava.util.stream.StreamOpFlag$Type;
1464:             5             80  [Lorg.apache.cassandra.config.ColumnDefinition;
1465:             2             80  [Lorg.apache.cassandra.config.Config$DiskFailurePolicy;
1466:             1             80  [Lorg.apache.cassandra.cql3.Operator;
1467:             1             80  [Lorg.apache.cassandra.schema.TableParams$Option;
1468:             2             80  [Lorg.apache.cassandra.transport.messages.ResultMessage$Kind;
1469:             2             80  [Lorg.codehaus.jackson.annotate.JsonAutoDetect$Visibility;
1470:             1             80  [Lsun.security.ssl.CipherSuite$KeyExchange;
1471:             1             80  ch.qos.logback.classic.AsyncAppender
1472:             2             80  ch.qos.logback.classic.filter.ThresholdFilter
1473:             1             80  ch.qos.logback.classic.turbo.ReconfigureOnChangeFilter
1474:             2             80  ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy
1475:             2             80  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$TabularMapping
1476:             1             80  com.sun.jmx.remote.util.ClassLoaderWithRepository
1477:             5             80  com.sun.proxy.$Proxy1
1478:             5             80  io.netty.channel.group.DefaultChannelGroup$1
1479:             2             80  io.netty.channel.unix.Errors$NativeConnectException
1480:             2             80  io.netty.util.Signal
1481:             2             80  java.io.ExpiringCache
1482:             2             80  java.util.Locale$Category
1483:             5             80  java.util.logging.SimpleFormatter
1484:             2             80  java.util.regex.Pattern$Loop
1485:             5             80  javax.security.auth.x500.X500Principal
1486:             1             80  org.apache.cassandra.concurrent.StageManager$TracingExecutor
1487:             1             80  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$SMAwareReconfigureOnChangeFilter
1488:             1             80  org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter
1489:             1             80  org.apache.cassandra.io.sstable.SSTableRewriter
1490:             5             80  org.apache.cassandra.repair.RepairSession$1
1491:             2             80  org.codehaus.jackson.sym.CharsToNameCanonicalizer
1492:             2             80  sun.management.MemoryManagerImpl
1493:             2             80  sun.reflect.UnsafeQualifiedStaticObjectFieldAccessorImpl
1494:             1             80  sun.reflect.misc.MethodUtil
1495:             2             80  sun.rmi.server.LoaderHandler$LoaderEntry
1496:             2             80  sun.rmi.server.UnicastServerRef
1497:             2             80  sun.rmi.server.UnicastServerRef2
1498:             2             80  sun.security.provider.DSAPublicKeyImpl
1499:             5             80  sun.security.util.DisabledAlgorithmConstraints$Constraints
1500:             2             80  sun.util.logging.resources.logging
1501:             1             72  [Ljava.lang.invoke.LambdaFormEditor$Transform$Kind;
1502:             4             72  [Ljava.nio.file.LinkOption;
1503:             3             72  [Ljava.util.concurrent.ConcurrentHashMap$CounterCell;
1504:             1             72  [Ljavax.management.openmbean.SimpleType;
1505:             2             72  [Lsun.security.jca.ProviderConfig;
1506:             1             72  ch.qos.logback.core.ConsoleAppender
1507:             3             72  ch.qos.logback.core.joran.action.NOPAction
1508:             3             72  ch.qos.logback.core.joran.action.PropertyAction
1509:             3             72  ch.qos.logback.core.pattern.FormatInfo
1510:             3             72  ch.qos.logback.core.rolling.helper.CompressionMode
1511:             3             72  ch.qos.logback.core.spi.FilterReply
1512:             3             72  ch.qos.logback.core.subst.Tokenizer$TokenizerState
1513:             3             72  com.github.benmanes.caffeine.cache.AccessOrderDeque
1514:             3             72  com.github.benmanes.caffeine.cache.Caffeine$Strength
1515:             3             72  com.google.common.base.CharMatcher$13
1516:             3             72  com.google.common.base.CharMatcher$RangesMatcher
1517:             3             72  com.google.common.collect.AbstractMapBasedMultimap$KeySet
1518:             1             72  io.netty.channel.DefaultChannelHandlerContext
1519:             1             72  io.netty.channel.DefaultChannelPipeline$HeadContext
1520:             1             72  io.netty.channel.DefaultChannelPipeline$TailContext
1521:             1             72  io.netty.channel.epoll.EpollServerSocketChannelConfig
1522:             3             72  java.io.ObjectStreamClass$ExceptionInfo
1523:             3             72  java.lang.UNIXProcess$LaunchMechanism
1524:             3             72  java.lang.annotation.RetentionPolicy
1525:             3             72  java.nio.file.FileTreeWalker$EventType
1526:             3             72  java.rmi.dgc.VMID
1527:             3             72  java.security.SecurityPermission
1528:             3             72  java.util.Base64$Encoder
1529:             1             72  java.util.ResourceBundle$RBClassLoader
1530:             3             72  java.util.concurrent.atomic.AtomicMarkableReference$Pair
1531:             3             72  java.util.jar.Manifest
1532:             1             72  java.util.logging.LogManager$RootLogger
1533:             1             72  java.util.logging.LogRecord
1534:             3             72  java.util.stream.Collector$Characteristics
1535:             3             72  java.util.stream.MatchOps$MatchKind
1536:             3             72  javax.crypto.CryptoPermissions
1537:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader
1538:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader$ClassLoaderWrapper
1539:             3             72  javax.security.auth.Subject$SecureSet
1540:             3             72  org.apache.cassandra.auth.DataResource$Level
1541:             3             72  org.apache.cassandra.config.ColumnDefinition$ClusteringOrder
1542:             3             72  org.apache.cassandra.config.Config$InternodeCompression
1543:             3             72  org.apache.cassandra.config.Config$UserFunctionTimeoutPolicy
1544:             3             72  org.apache.cassandra.config.ReadRepairDecision
1545:             3             72  org.apache.cassandra.cql3.AssignmentTestable$TestResult
1546:             1             72  org.apache.cassandra.cql3.Cql_Lexer
1547:             3             72  org.apache.cassandra.cql3.ResultSet$Flag
1548:             3             72  org.apache.cassandra.db.Conflicts$Resolution
1549:             3             72  org.apache.cassandra.db.Directories$FileType
1550:             3             72  org.apache.cassandra.db.commitlog.CommitLogSegment$CDCState
1551:             1             72  org.apache.cassandra.db.compaction.CompactionIterator
1552:             3             72  org.apache.cassandra.db.lifecycle.SSTableSet
1553:             3             72  org.apache.cassandra.db.marshal.AbstractType$ComparisonType
1554:             3             72  org.apache.cassandra.db.monitoring.MonitoringState
1555:             3             72  org.apache.cassandra.db.rows.SerializationHelper$Flag
1556:             1             72  org.apache.cassandra.io.util.SequentialWriter
1557:             3             72  org.apache.cassandra.locator.TokenMetadata$Topology
1558:             3             72  org.apache.cassandra.metrics.CacheMetrics$1
1559:             3             72  org.apache.cassandra.metrics.CacheMetrics$6
1560:             3             72  org.apache.cassandra.metrics.CacheMetrics$7
1561:             3             72  org.apache.cassandra.metrics.StreamingMetrics
1562:             3             72  org.apache.cassandra.repair.RepairParallelism
1563:             3             72  org.apache.cassandra.repair.SystemDistributedKeyspace$RepairState
1564:             3             72  org.apache.cassandra.repair.messages.ValidationComplete
1565:             3             72  org.apache.cassandra.schema.CompactionParams$TombstoneOption
1566:             3             72  org.apache.cassandra.schema.IndexMetadata$Kind
1567:             3             72  org.apache.cassandra.service.CacheService$CacheType
1568:             3             72  org.apache.cassandra.streaming.StreamEvent$Type
1569:             3             72  org.apache.cassandra.transport.Server$LatestEvent
1570:             3             72  org.apache.cassandra.utils.BiMultiValMap
1571:             3             72  org.apache.cassandra.utils.NoSpamLogger$Level
1572:             3             72  org.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle
1573:             1             72  org.apache.commons.lang3.builder.ToStringStyle$DefaultToStringStyle
1574:             1             72  org.apache.commons.lang3.builder.ToStringStyle$MultiLineToStringStyle
1575:             1             72  org.apache.commons.lang3.builder.ToStringStyle$NoFieldNameToStringStyle
1576:             1             72  org.apache.commons.lang3.builder.ToStringStyle$ShortPrefixToStringStyle
1577:             1             72  org.apache.commons.lang3.builder.ToStringStyle$SimpleToStringStyle
1578:             1             72  org.apache.thrift.server.TThreadPoolServer$Args
1579:             3             72  org.yaml.snakeyaml.DumperOptions$FlowStyle
1580:             3             72  org.yaml.snakeyaml.DumperOptions$LineBreak
1581:             3             72  org.yaml.snakeyaml.introspector.BeanAccess
1582:             3             72  sun.misc.FloatingDecimal$ExceptionalBinaryToASCIIBuffer
1583:             3             72  sun.misc.ObjectInputFilter$Status
1584:             3             72  sun.misc.Signal
1585:             3             72  sun.nio.fs.UnixFileAttributeViews$Basic
1586:             3             72  sun.rmi.transport.SequenceEntry
1587:             3             72  sun.security.provider.NativePRNG$Variant
1588:             3             72  sun.security.ssl.CipherSuite$CipherType
1589:             3             72  sun.security.ssl.CipherSuiteList
1590:             1             72  sun.util.locale.provider.JRELocaleProviderAdapter
1591:             3             72  sun.util.resources.ParallelListResourceBundle$KeySet
1592:             2             64  [Ljava.lang.UNIXProcess$LaunchMechanism;
1593:             2             64  [Ljava.lang.annotation.RetentionPolicy;
1594:             3             64  [Ljava.security.CodeSigner;
1595:             3             64  [Ljava.security.cert.X509Certificate;
1596:             2             64  [Ljava.util.stream.Collector$Characteristics;
1597:             2             64  [Lorg.apache.cassandra.config.CFMetaData$Flag;
1598:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$ClusteringOrder;
1599:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$Kind;
1600:             2             64  [Lorg.apache.cassandra.config.Config$CommitFailurePolicy;
1601:             2             64  [Lorg.apache.cassandra.config.Config$InternodeCompression;
1602:             2             64  [Lorg.apache.cassandra.config.Config$MemtableAllocationType;
1603:             2             64  [Lorg.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption;
1604:             2             64  [Lorg.apache.cassandra.cql3.ResultSet$Flag;
1605:             2             64  [Lorg.apache.cassandra.db.SystemKeyspace$BootstrapState;
1606:             2             64  [Lorg.apache.cassandra.io.sstable.metadata.MetadataType;
1607:             2             64  [Lorg.apache.cassandra.schema.CompactionParams$TombstoneOption;
1608:             2             64  [Lorg.apache.cassandra.schema.IndexMetadata$Kind;
1609:             2             64  [Lorg.apache.cassandra.transport.Event$Type;
1610:             2             64  [Lorg.yaml.snakeyaml.nodes.NodeId;
1611:             2             64  ch.qos.logback.classic.joran.action.LevelAction
1612:             2             64  ch.qos.logback.core.joran.spi.ConsoleTarget
1613:             2             64  ch.qos.logback.core.rolling.helper.Compressor
1614:             2             64  ch.qos.logback.core.rolling.helper.IntegerTokenConverter
1615:             4             64  ch.qos.logback.core.spi.FilterAttachableImpl
1616:             1             64  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus
1617:             2             64  com.github.benmanes.caffeine.cache.References$WeakKeyReference
1618:             1             64  com.github.benmanes.caffeine.cache.stats.CacheStats
1619:             1             64  com.google.common.cache.CacheStats
1620:             4             64  com.google.common.cache.LocalCache$WriteQueue
1621:             2             64  com.google.common.util.concurrent.Striped$LargeLazyStriped
1622:             4             64  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher
1623:             2             64  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CollectionMapping
1624:             1             64  com.sun.jmx.remote.internal.ArrayNotificationBuffer
1625:             2             64  com.sun.management.GarbageCollectionNotificationInfo
1626:             2             64  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property
1627:             1             64  io.netty.channel.ChannelOutboundBuffer
1628:             4             64  io.netty.util.concurrent.FastThreadLocal
1629:             4             64  java.io.ObjectInputStream$ValidationList
1630:             2             64  java.io.PrintStream
1631:             2             64  java.lang.ClassValue$Entry
1632:             2             64  java.lang.NoSuchMethodError
1633:             2             64  java.lang.VirtualMachineError
1634:             2             64  java.lang.ref.ReferenceQueue$Null
1635:             2             64  java.net.Inet6Address
1636:             2             64  java.net.Inet6Address$Inet6AddressHolder
1637:             2             64  java.util.ResourceBundle$Control$1
1638:             2             64  java.util.concurrent.ConcurrentLinkedQueue$Itr
1639:             2             64  java.util.jar.Manifest$FastInputStream
1640:             1             64  javax.management.remote.rmi.RMIConnectionImpl
1641:             1             64  javax.management.remote.rmi.RMIConnectorServer
1642:             4             64  javax.security.auth.login.AppConfigurationEntry$LoginModuleControlFlag
1643:             4             64  org.apache.cassandra.concurrent.SEPWorker$Work
1644:             2             64  org.apache.cassandra.cql3.functions.TokenFct
1645:             2             64  org.apache.cassandra.db.commitlog.CommitLogDescriptor
1646:             2             64  org.apache.cassandra.db.lifecycle.LogFile
1647:             2             64  org.apache.cassandra.db.lifecycle.LogTransaction
1648:             2             64  org.apache.cassandra.io.sstable.format.SSTableFormat$Type
1649:             2             64  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxIntTracker
1650:             2             64  org.apache.cassandra.io.util.SafeMemoryWriter
1651:             1             64  org.apache.cassandra.locator.DynamicEndpointSnitch
1652:             1             64  org.apache.cassandra.metrics.ViewWriteMetrics
1653:             1             64  org.apache.cassandra.net.MessagingService
1654:             2             64  org.apache.cassandra.service.ClientState
1655:             2             64  org.apache.cassandra.service.GCInspector$GCState
1656:             1             64  org.apache.cassandra.service.GCInspector$State
1657:             1             64  org.apache.cassandra.thrift.CustomTThreadPoolServer
1658:             1             64  org.apache.cassandra.utils.SigarLibrary
1659:             4             64  org.apache.cassandra.utils.SortedBiMultiValMap$1
1660:             4             64  org.codehaus.jackson.map.introspect.AnnotationMap
1661:             4             64  sun.net.www.protocol.jar.Handler
1662:             4             64  sun.rmi.server.MarshalOutputStream$1
1663:             2             64  sun.rmi.transport.DGCImpl$LeaseInfo
1664:             2             64  sun.rmi.transport.tcp.TCPTransport
1665:             2             64  sun.security.ssl.EphemeralKeyManager$EphemeralKeyPair
1666:             2             64  sun.security.ssl.SSLSessionContextImpl
1667:             2             64  sun.security.x509.PrivateKeyUsageExtension
1668:             2             64  sun.security.x509.SubjectAlternativeNameExtension
1669:             2             64  sun.util.locale.provider.LocaleServiceProviderPool
1670:             1             56  [Lcom.sun.org.apache.xerces.internal.impl.XMLScanner$NameType;
1671:             1             56  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit;
1672:             1             56  [Ljava.lang.Runnable;
1673:             1             56  [Ljava.nio.file.StandardOpenOption;
1674:             2             56  [Ljdk.internal.org.objectweb.asm.Type;
1675:             1             56  [Lorg.apache.commons.lang3.JavaVersion;
1676:             1             56  [Lorg.codehaus.jackson.JsonParser$Feature;
1677:             1             56  [Lorg.yaml.snakeyaml.events.Event$ID;
1678:             1             56  [Lsun.util.logging.PlatformLogger$Level;
1679:             1             56  [[I
1680:             1             56  com.sun.jmx.remote.internal.ServerNotifForwarder
1681:             1             56  io.netty.util.concurrent.ScheduledFutureTask
1682:             1             56  java.lang.invoke.LambdaFormEditor$Transform
1683:             1             56  java.util.concurrent.ConcurrentHashMap$KeyIterator
1684:             1             56  java.util.logging.ConsoleHandler
1685:             1             56  java.util.logging.LogManager
1686:             1             56  javax.management.remote.JMXConnectionNotification
1687:             1             56  javax.management.remote.rmi.RMIJRMPServerImpl
1688:             1             56  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache
1689:             1             56  org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
1690:             1             56  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions
1691:             1             56  org.apache.cassandra.cql3.CqlLexer$DFA1
1692:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA14
1693:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA22
1694:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA24
1695:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA28
1696:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA30
1697:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA37
1698:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA44
1699:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA9
1700:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA1
1701:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA15
1702:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA153
1703:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA154
1704:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA172
1705:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA174
1706:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA176
1707:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA178
1708:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA181
1709:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA189
1710:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA194
1711:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA195
1712:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA204
1713:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA44
1714:             1             56  org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard
1715:             1             56  org.apache.cassandra.db.commitlog.PeriodicCommitLogService
1716:             1             56  org.apache.cassandra.db.compaction.CompactionController
1717:             1             56  org.apache.cassandra.db.lifecycle.LifecycleTransaction
1718:             1             56  org.apache.cassandra.io.compress.CompressionMetadata$Writer
1719:             1             56  org.apache.cassandra.metrics.CacheMissMetrics
1720:             1             56  org.codehaus.jackson.map.ObjectMapper
1721:             1             56  org.codehaus.jackson.map.ser.StdSerializerProvider
1722:             1             56  org.codehaus.jackson.sym.BytesToNameCanonicalizer
1723:             1             56  org.hyperic.sigar.SigarLoader
1724:             1             56  sun.rmi.runtime.Log$InternalStreamHandler
1725:             1             48  [Lcom.sun.beans.util.Cache$CacheEntry;
1726:             1             48  [Lcom.sun.management.VMOption$Origin;
1727:             1             48  [Ljava.beans.WeakIdentityMap$Entry;
1728:             3             48  [Ljava.lang.annotation.Annotation;
1729:             1             48  [Ljava.lang.invoke.MethodHandleImpl$Intrinsic;
1730:             1             48  [Ljava.math.RoundingMode;
1731:             2             48  [Ljava.nio.file.FileVisitOption;
1732:             1             48  [Ljdk.net.SocketFlow$Status;
1733:             2             48  [Lorg.apache.cassandra.config.Config$CommitLogSync;
1734:             1             48  [Lorg.apache.cassandra.cql3.Constants$Type;
1735:             1             48  [Lorg.apache.cassandra.db.ClusteringPrefix$Kind;
1736:             1             48  [Lorg.apache.cassandra.db.Directories$FileAction;
1737:             1             48  [Lorg.apache.cassandra.db.WriteType;
1738:             2             48  [Lorg.apache.cassandra.exceptions.RequestFailureReason;
1739:             2             48  [Lorg.apache.cassandra.net.RateBasedBackPressure$Flow;
1740:             1             48  [Lorg.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State;
1741:             1             48  [Lorg.apache.cassandra.service.StorageService$Mode;
1742:             1             48  [Lorg.apache.cassandra.streaming.messages.StreamMessage$Type;
1743:             1             48  [Lorg.apache.cassandra.utils.progress.ProgressEventType;
1744:             1             48  [Lorg.codehaus.jackson.JsonGenerator$Feature;
1745:             1             48  [Lsun.security.x509.NetscapeCertTypeExtension$MapEntry;
1746:             1             48  ch.qos.logback.classic.jmx.JMXConfigurator
1747:             3             48  ch.qos.logback.classic.pattern.EnsureExceptionHandling
1748:             3             48  ch.qos.logback.classic.spi.PackagingDataCalculator
1749:             1             48  ch.qos.logback.core.joran.action.DefinePropertyAction
1750:             1             48  ch.qos.logback.core.joran.spi.InterpretationContext
1751:             1             48  ch.qos.logback.core.joran.spi.Interpreter
1752:             2             48  ch.qos.logback.core.rolling.helper.RenameUtil
1753:             3             48  ch.qos.logback.core.spi.LogbackLock
1754:             2             48  ch.qos.logback.core.subst.Node$Type
1755:             2             48  ch.qos.logback.core.util.FileSize
1756:             2             48  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format
1757:             3             48  com.google.common.cache.LocalCache$LocalLoadingCache
1758:             1             48  com.google.common.collect.EmptyImmutableListMultimap
1759:             2             48  com.google.common.collect.HashBiMap$Inverse
1760:             1             48  com.google.common.collect.ImmutableListMultimap
1761:             2             48  com.google.common.collect.ImmutableMultimap$Values
1762:             2             48  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry
1763:             1             48  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$Mappings
1764:             2             48  com.sun.jmx.mbeanserver.WeakIdentityHashMap
1765:             2             48  com.sun.jmx.remote.internal.ServerNotifForwarder$IdAndFilter
1766:             1             48  com.sun.jna.NativeLibrary
1767:             3             48  com.sun.org.apache.xerces.internal.impl.dv.dtd.ListDatatypeValidator
1768:             2             48  io.netty.buffer.PooledByteBufAllocator$PoolThreadLocalCache
1769:             2             48  io.netty.channel.VoidChannelPromise
1770:             2             48  io.netty.util.Recycler$2
1771:             2             48  io.netty.util.UniqueName
1772:             1             48  io.netty.util.concurrent.GlobalEventExecutor
1773:             3             48  io.netty.util.internal.TypeParameterMatcher$ReflectiveMatcher
1774:             2             48  java.io.ByteArrayOutputStream
1775:             2             48  java.io.File$PathStatus
1776:             3             48  java.io.FileOutputStream$1
1777:             2             48  java.io.OutputStreamWriter
1778:             2             48  java.io.SerialCallbackContext
1779:             3             48  java.lang.Boolean
1780:             3             48  java.lang.Float
1781:             3             48  java.lang.InheritableThreadLocal
1782:             1             48  java.lang.invoke.BoundMethodHandle$Species_L4
1783:             2             48  java.lang.invoke.ConstantCallSite
1784:             2             48  java.lang.invoke.InfoFromMemberName
1785:             2             48  java.lang.invoke.InnerClassLambdaMetafactory$ForwardingMethodGenerator
1786:             2             48  java.lang.management.MemoryType
1787:             2             48  java.lang.reflect.ReflectPermission
1788:             2             48  java.net.InetAddress$Cache
1789:             2             48  java.net.InetAddress$Cache$Type
1790:             2             48  java.net.InetAddress$CacheEntry
1791:             1             48  java.net.NetworkInterface
1792:             2             48  java.net.ServerSocket
1793:             2             48  java.net.SocketPermissionCollection
1794:             2             48  java.net.StandardProtocolFamily
1795:             3             48  java.nio.channels.FileChannel$MapMode
1796:             2             48  java.nio.charset.CoderResult
1797:             3             48  java.nio.charset.CodingErrorAction
1798:             2             48  java.rmi.dgc.Lease
1799:             2             48  java.security.AllPermissionCollection
1800:             3             48  java.text.AttributedCharacterIterator$Attribute
1801:             3             48  java.util.Base64$Decoder
1802:             2             48  java.util.PropertyPermissionCollection
1803:             3             48  java.util.TreeMap$EntrySet
1804:             2             48  java.util.concurrent.Executors$DefaultThreadFactory
1805:             3             48  java.util.concurrent.atomic.AtomicMarkableReference
1806:             2             48  java.util.logging.Logger$LoggerBundle
1807:             1             48  java.util.regex.Pattern$GroupCurly
1808:             2             48  java.util.regex.Pattern$Prolog
1809:             2             48  javax.management.MBeanServerInvocationHandler
1810:             1             48  javax.management.remote.rmi.RMIConnectionImpl$RMIServerCommunicatorAdmin
1811:             1             48  javax.security.auth.SubjectDomainCombiner$WeakKeyValueMap
1812:             1             48  org.antlr.runtime.ANTLRStringStream
1813:             2             48  org.apache.cassandra.cache.AutoSavingCache$2
1814:             2             48  org.apache.cassandra.config.Config$CommitLogSync
1815:             2             48  org.apache.cassandra.config.Config$DiskOptimizationStrategy
1816:             2             48  org.apache.cassandra.config.ParameterizedClass
1817:             2             48  org.apache.cassandra.cql3.Sets$Marker
1818:             2             48  org.apache.cassandra.cql3.Sets$Setter
1819:             2             48  org.apache.cassandra.cql3.functions.FunctionCall
1820:             2             48  org.apache.cassandra.cql3.statements.Bound
1821:             2             48  org.apache.cassandra.db.Directories$OnTxnErr
1822:             2             48  org.apache.cassandra.db.Memtable$LastCommitLogPosition
1823:             2             48  org.apache.cassandra.db.ReadCommand$Kind
1824:             2             48  org.apache.cassandra.db.aggregation.AggregationSpecification$Kind
1825:             1             48  org.apache.cassandra.db.commitlog.CommitLogArchiver
1826:             1             48  org.apache.cassandra.db.compaction.CompactionInfo
1827:             2             48  org.apache.cassandra.db.filter.ClusteringIndexFilter$Kind
1828:             2             48  org.apache.cassandra.db.lifecycle.LifecycleTransaction$State
1829:             2             48  org.apache.cassandra.db.lifecycle.LogReplica
1830:             2             48  org.apache.cassandra.db.rows.Unfiltered$Kind
1831:             2             48  org.apache.cassandra.exceptions.RequestFailureReason
1832:             1             48  org.apache.cassandra.gms.FailureDetector
1833:             2             48  org.apache.cassandra.hints.HintsDispatcher$Action
1834:             1             48  org.apache.cassandra.hints.HintsService
1835:             2             48  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason
1836:             1             48  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter
1837:             1             48  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxLongTracker
1838:             2             48  org.apache.cassandra.io.util.NIODataInputStream
1839:             1             48  org.apache.cassandra.locator.NetworkTopologyStrategy
1840:             3             48  org.apache.cassandra.metrics.CacheMetrics$2
1841:             3             48  org.apache.cassandra.metrics.CacheMetrics$3
1842:             3             48  org.apache.cassandra.metrics.CacheMetrics$4
1843:             3             48  org.apache.cassandra.metrics.CacheMetrics$5
1844:             2             48  org.apache.cassandra.metrics.TableMetrics$Sampler
1845:             1             48  org.apache.cassandra.net.MessagingService$2
1846:             1             48  org.apache.cassandra.net.RateBasedBackPressure
1847:             2             48  org.apache.cassandra.net.RateBasedBackPressure$Flow
1848:             1             48  org.apache.cassandra.repair.messages.RepairOption
1849:             2             48  org.apache.cassandra.schema.CachingParams$Option
1850:             2             48  org.apache.cassandra.schema.KeyspaceParams$Option
1851:             1             48  org.apache.cassandra.service.ActiveRepairService$ParentRepairSession
1852:             2             48  org.apache.cassandra.streaming.ProgressInfo$Direction
1853:             2             48  org.apache.cassandra.transport.Event$StatusChange$Status
1854:             2             48  org.apache.cassandra.transport.Message$Direction
1855:             2             48  org.apache.cassandra.utils.ChecksumType$3
1856:             2             48  org.apache.cassandra.utils.Throwables$FileOpType
1857:             2             48  org.apache.cassandra.utils.btree.BTree$Dir
1858:             2             48  org.apache.cassandra.utils.concurrent.WaitQueue$RegisteredSignal
1859:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotE
1860:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotK
1861:             1             48  org.codehaus.jackson.JsonFactory
1862:             1             48  org.codehaus.jackson.map.DeserializationConfig
1863:             1             48  org.codehaus.jackson.map.SerializationConfig
1864:             2             48  org.codehaus.jackson.map.deser.std.CalendarDeserializer
1865:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$BooleanDeserializer
1866:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ByteDeserializer
1867:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$CharacterDeserializer
1868:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$DoubleDeserializer
1869:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$FloatDeserializer
1870:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$IntegerDeserializer
1871:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$LongDeserializer
1872:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ShortDeserializer
1873:             2             48  org.codehaus.jackson.map.ser.StdSerializers$BooleanSerializer
1874:             1             48  org.hyperic.sigar.FileSystemMap
1875:             1             48  org.hyperic.sigar.Sigar
1876:             2             48  sun.management.ManagementFactoryHelper$1
1877:             2             48  sun.misc.NativeSignalHandler
1878:             2             48  sun.misc.URLClassPath$FileLoader
1879:             3             48  sun.nio.fs.UnixFileAttributes$UnixAsBasicFileAttributes
1880:             2             48  sun.rmi.server.UnicastServerRef$1
1881:             3             48  sun.rmi.server.WeakClassHashMap$ValueCell
1882:             2             48  sun.security.jca.ProviderList
1883:             2             48  sun.security.jca.ProviderList$3
1884:             2             48  sun.security.provider.DSAParameters
1885:             2             48  sun.security.ssl.SSLAlgorithmConstraints
1886:             3             48  sun.security.util.AlgorithmDecomposer
1887:             2             48  sun.security.util.DisabledAlgorithmConstraints$UsageConstraint
1888:             2             48  sun.security.util.DisabledAlgorithmConstraints$jdkCAConstraint
1889:             3             48  sun.security.x509.RFC822Name
1890:             3             48  sun.text.normalizer.NormalizerBase$QuickCheckResult
1891:             1             48  sun.text.resources.FormatData
1892:             1             48  sun.text.resources.en.FormatData_en
1893:             1             48  sun.text.resources.en.FormatData_en_US
1894:             1             40  [Lch.qos.logback.core.pattern.parser.TokenStream$TokenizerState;
1895:             1             40  [Lch.qos.logback.core.subst.Token$Type;
1896:             1             40  [Lch.qos.logback.core.util.AggregationType;
1897:             1             40  [Lcom.google.common.collect.SortedLists$KeyPresentBehavior;
1898:             2             40  [Lcom.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry;
1899:             1             40  [Lcom.sun.org.apache.xerces.internal.util.Status;
1900:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State;
1901:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State;
1902:             1             40  [Ljava.lang.management.MemoryPoolMXBean;
1903:             2             40  [Ljava.util.logging.Handler;
1904:             1             40  [Ljava.util.stream.StreamOpFlag;
1905:             1             40  [Lorg.apache.cassandra.cql3.statements.IndexTarget$Type;
1906:             1             40  [Lorg.apache.cassandra.db.filter.DataLimits$Kind;
1907:             1             40  [Lorg.apache.cassandra.db.lifecycle.LogRecord$Type;
1908:             1             40  [Lorg.apache.cassandra.schema.CompactionParams$Option;
1909:             1             40  [Lorg.apache.cassandra.streaming.StreamSession$State;
1910:             1             40  [Lorg.apache.cassandra.utils.NativeLibrary$OSType;
1911:             1             40  [Lorg.github.jamm.MemoryMeter$Guess;
1912:             1             40  [Lorg.yaml.snakeyaml.DumperOptions$ScalarStyle;
1913:             1             40  [Lsun.security.jca.ServiceId;
1914:             1             40  [Lsun.security.util.DisabledAlgorithmConstraints$Constraint$Operator;
1915:             1             40  [Lsun.util.locale.provider.LocaleProviderAdapter$Type;
1916:             1             40  [[Ljava.lang.invoke.LambdaForm$Name;
1917:             1             40  ch.qos.logback.core.BasicStatusManager
1918:             1             40  ch.qos.logback.core.joran.spi.ConfigurationWatchList
1919:             1             40  com.google.common.collect.AbstractMapBasedMultimap$2
1920:             1             40  com.google.common.collect.AbstractMapBasedMultimap$WrappedSet
1921:             1             40  com.google.common.collect.EmptyImmutableSortedMap
1922:             1             40  com.sun.beans.finder.MethodFinder$1
1923:             1             40  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor
1924:             1             40  com.sun.jmx.mbeanserver.JmxMBeanServer
1925:             1             40  com.sun.jmx.mbeanserver.MBeanServerDelegateImpl
1926:             1             40  io.netty.channel.AbstractChannel$CloseFuture
1927:             1             40  io.netty.channel.DefaultChannelPipeline
1928:             1             40  io.netty.channel.epoll.AbstractEpollServerChannel$EpollServerSocketUnsafe
1929:             1             40  java.beans.WeakIdentityMap$Entry
1930:             1             40  java.lang.reflect.Proxy$Key2
1931:             1             40  java.rmi.NoSuchObjectException
1932:             1             40  java.util.ResourceBundle$1
1933:             1             40  javax.crypto.CryptoAllPermission
1934:             1             40  net.jpountz.lz4.LZ4Factory
1935:             1             40  org.antlr.runtime.CommonTokenStream
1936:             1             40  org.apache.cassandra.concurrent.SharedExecutorPool
1937:             1             40  org.apache.cassandra.config.TransparentDataEncryptionOptions
1938:             1             40  org.apache.cassandra.cql3.CqlLexer
1939:             1             40  org.apache.cassandra.db.commitlog.CommitLog
1940:             1             40  org.apache.cassandra.db.compaction.CompactionTask
1941:             1             40  org.apache.cassandra.exceptions.RepairException
1942:             1             40  org.apache.cassandra.io.sstable.format.big.BigTableWriter$TransactionalProxy
1943:             1             40  org.apache.cassandra.locator.GossipingPropertyFileSnitch
1944:             1             40  org.apache.cassandra.net.MessagingService$1
1945:             1             40  org.apache.cassandra.net.MessagingService$3
1946:             1             40  org.apache.cassandra.streaming.management.StreamEventJMXNotifier
1947:             1             40  org.apache.cassandra.transport.Server
1948:             1             40  org.apache.cassandra.utils.NoSpamLogger$NoSpamLogStatement
1949:             1             40  org.apache.cassandra.utils.memory.SlabPool
1950:             1             40  org.codehaus.jackson.map.util.StdDateFormat
1951:             1             40  sun.management.DiagnosticCommandImpl
1952:             1             40  sun.management.MappedMXBeanType$CompositeDataMXBeanType
1953:             1             40  sun.management.MappedMXBeanType$MapMXBeanType
1954:             1             40  sun.nio.cs.StandardCharsets$Aliases
1955:             1             40  sun.nio.cs.StandardCharsets$Cache
1956:             1             40  sun.nio.cs.StandardCharsets$Classes
1957:             1             40  sun.security.ssl.SSLContextImpl$DefaultSSLContext
1958:             1             32  [Lch.qos.logback.core.rolling.helper.CompressionMode;
1959:             1             32  [Lch.qos.logback.core.spi.FilterReply;
1960:             1             32  [Lch.qos.logback.core.subst.Tokenizer$TokenizerState;
1961:             1             32  [Lcom.github.benmanes.caffeine.cache.Caffeine$Strength;
1962:             1             32  [Lcom.google.common.base.Predicates$ObjectPredicate;
1963:             1             32  [Lcom.google.common.cache.LocalCache$Strength;
1964:             1             32  [Lcom.google.common.collect.AbstractIterator$State;
1965:             1             32  [Lcom.google.common.collect.MapMakerInternalMap$Strength;
1966:             1             32  [Lcom.google.common.collect.SortedLists$KeyAbsentBehavior;
1967:             1             32  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus;
1968:             1             32  [Lcom.sun.beans.util.Cache$Kind;
1969:             1             32  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap;
1970:             2             32  [Ljava.lang.Enum;
1971:             1             32  [Ljava.lang.OutOfMemoryError;
1972:             1             32  [Ljava.lang.ThreadGroup;
1973:             1             32  [Ljava.lang.UNIXProcess$Platform;
1974:             1             32  [Ljava.lang.management.MemoryManagerMXBean;
1975:             1             32  [Ljava.nio.file.FileTreeWalker$EventType;
1976:             1             32  [Ljava.nio.file.FileVisitResult;
1977:             1             32  [Ljava.text.Normalizer$Form;
1978:             1             32  [Ljava.util.concurrent.atomic.AtomicReference;
1979:             1             32  [Ljava.util.stream.MatchOps$MatchKind;
1980:             1             32  [Ljava.util.stream.StreamShape;
1981:             1             32  [Lnet.jpountz.util.Native$OS;
1982:             1             32  [Lorg.apache.cassandra.auth.DataResource$Level;
1983:             1             32  [Lorg.apache.cassandra.auth.IRoleManager$Option;
1984:             1             32  [Lorg.apache.cassandra.config.Config$DiskAccessMode;
1985:             1             32  [Lorg.apache.cassandra.config.Config$UserFunctionTimeoutPolicy;
1986:             1             32  [Lorg.apache.cassandra.config.ReadRepairDecision;
1987:             1             32  [Lorg.apache.cassandra.cql3.AssignmentTestable$TestResult;
1988:             1             32  [Lorg.apache.cassandra.cql3.statements.StatementType;
1989:             1             32  [Lorg.apache.cassandra.db.Conflicts$Resolution;
1990:             1             32  [Lorg.apache.cassandra.db.Directories$FileType;
1991:             1             32  [Lorg.apache.cassandra.db.commitlog.CommitLogSegment$CDCState;
1992:             1             32  [Lorg.apache.cassandra.db.context.CounterContext$Relationship;
1993:             1             32  [Lorg.apache.cassandra.db.lifecycle.SSTableSet;
1994:             1             32  [Lorg.apache.cassandra.db.marshal.AbstractType$ComparisonType;
1995:             1             32  [Lorg.apache.cassandra.db.marshal.CollectionType$Kind;
1996:             1             32  [Lorg.apache.cassandra.db.monitoring.MonitoringState;
1997:             1             32  [Lorg.apache.cassandra.db.rows.SerializationHelper$Flag;
1998:             1             32  [Lorg.apache.cassandra.hints.HintsDispatcher$Callback$Outcome;
1999:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReader$OpenReason;
2000:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason;
2001:             1             32  [Lorg.apache.cassandra.repair.RepairParallelism;
2002:             1             32  [Lorg.apache.cassandra.repair.SystemDistributedKeyspace$RepairState;
2003:             1             32  [Lorg.apache.cassandra.schema.SpeculativeRetryParam$Kind;
2004:             1             32  [Lorg.apache.cassandra.service.CacheService$CacheType;
2005:             1             32  [Lorg.apache.cassandra.streaming.StreamEvent$Type;
2006:             1             32  [Lorg.apache.cassandra.utils.AbstractIterator$State;
2007:             1             32  [Lorg.apache.cassandra.utils.AsymmetricOrdering$Op;
2008:             1             32  [Lorg.apache.cassandra.utils.NoSpamLogger$Level;
2009:             1             32  [Lorg.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State;
2010:             1             32  [Lorg.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle;
2011:             2             32  [Lorg.codehaus.jackson.type.JavaType;
2012:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$FlowStyle;
2013:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$LineBreak;
2014:             1             32  [Lorg.yaml.snakeyaml.introspector.BeanAccess;
2015:             1             32  [Lsun.misc.FormattedFloatingDecimal$Form;
2016:             1             32  [Lsun.misc.ObjectInputFilter$Status;
2017:             1             32  [Lsun.security.provider.NativePRNG$Variant;
2018:             1             32  [Lsun.security.ssl.CipherSuite$CipherType;
2019:             1             32  [Lsun.security.ssl.CipherSuite$PRF;
2020:             1             32  [[Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
2021:             1             32  ch.qos.logback.classic.joran.JoranConfigurator
2022:             1             32  ch.qos.logback.classic.joran.action.ConfigurationAction
2023:             1             32  ch.qos.logback.classic.joran.action.EvaluatorAction
2024:             1             32  ch.qos.logback.classic.joran.action.LoggerAction
2025:             1             32  ch.qos.logback.classic.joran.action.LoggerContextListenerAction
2026:             1             32  ch.qos.logback.classic.joran.action.ReceiverAction
2027:             1             32  ch.qos.logback.classic.joran.action.RootLoggerAction
2028:             1             32  ch.qos.logback.classic.sift.SiftAction
2029:             1             32  ch.qos.logback.classic.spi.LoggerContextVO
2030:             1             32  ch.qos.logback.core.helpers.CyclicBuffer
2031:             1             32  ch.qos.logback.core.joran.action.AppenderAction
2032:             1             32  ch.qos.logback.core.joran.action.ConversionRuleAction
2033:             1             32  ch.qos.logback.core.joran.action.IncludeAction
2034:             1             32  ch.qos.logback.core.joran.action.NestedBasicPropertyIA
2035:             1             32  ch.qos.logback.core.joran.action.NestedComplexPropertyIA
2036:             1             32  ch.qos.logback.core.joran.action.NewRuleAction
2037:             1             32  ch.qos.logback.core.joran.action.ParamAction
2038:             1             32  ch.qos.logback.core.joran.action.ShutdownHookAction
2039:             1             32  ch.qos.logback.core.joran.action.StatusListenerAction
2040:             1             32  ch.qos.logback.core.joran.action.TimestampAction
2041:             1             32  ch.qos.logback.core.joran.conditional.ElseAction
2042:             1             32  ch.qos.logback.core.joran.conditional.IfAction
2043:             1             32  ch.qos.logback.core.joran.conditional.ThenAction
2044:             1             32  ch.qos.logback.core.joran.spi.SimpleRuleStore
2045:             2             32  ch.qos.logback.core.spi.AppenderAttachableImpl
2046:             1             32  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache
2047:             1             32  com.github.benmanes.caffeine.cache.FrequencySketch
2048:             2             32  com.google.common.base.Joiner
2049:             2             32  com.google.common.base.Predicates$InPredicate
2050:             1             32  com.google.common.collect.AbstractMapBasedMultimap$NavigableKeySet
2051:             1             32  com.google.common.collect.EmptyImmutableBiMap
2052:             2             32  com.google.common.util.concurrent.Striped$2
2053:             2             32  com.sun.beans.WeakCache
2054:             1             32  com.sun.beans.finder.BeanInfoFinder
2055:             1             32  com.sun.jmx.mbeanserver.Repository
2056:             1             32  com.sun.jmx.remote.internal.ArrayQueue
2057:             1             32  com.sun.jmx.remote.security.JMXSubjectDomainCombiner
2058:             1             32  com.sun.org.apache.xerces.internal.impl.XMLEntityScanner$1
2059:             2             32  com.sun.org.apache.xerces.internal.impl.dv.dtd.ENTITYDatatypeValidator
2060:             2             32  com.sun.proxy.$Proxy5
2061:             1             32  io.netty.bootstrap.ServerBootstrap$ServerBootstrapAcceptor
2062:             1             32  io.netty.channel.epoll.EpollEventLoopGroup
2063:             2             32  io.netty.channel.group.ChannelMatchers$ClassMatcher
2064:             1             32  io.netty.util.concurrent.DefaultThreadFactory
2065:             2             32  io.netty.util.internal.logging.Slf4JLoggerFactory
2066:             1             32  java.beans.ThreadGroupContext
2067:             1             32  java.beans.ThreadGroupContext$1
2068:             2             32  java.io.ObjectStreamClass$1
2069:             2             32  java.io.ObjectStreamClass$3
2070:             2             32  java.io.ObjectStreamClass$4
2071:             2             32  java.io.ObjectStreamClass$5
2072:             1             32  java.io.UnixFileSystem
2073:             1             32  java.lang.ArithmeticException
2074:             1             32  java.lang.ArrayIndexOutOfBoundsException
2075:             1             32  java.lang.ClassCastException
2076:             1             32  java.lang.Exception
2077:             1             32  java.lang.NullPointerException
2078:             2             32  java.lang.Shutdown$Lock
2079:             1             32  java.lang.UnsupportedOperationException
2080:             1             32  java.lang.reflect.WeakCache
2081:             1             32  java.lang.reflect.WeakCache$CacheKey
2082:             2             32  java.nio.ByteOrder
2083:             1             32  java.nio.channels.NotYetConnectedException
2084:             1             32  java.text.DontCareFieldPosition
2085:             2             32  java.util.Hashtable$EntrySet
2086:             1             32  java.util.PriorityQueue
2087:             1             32  java.util.TreeMap$EntryIterator
2088:             1             32  java.util.TreeMap$KeyIterator
2089:             1             32  java.util.concurrent.CancellationException
2090:             1             32  java.util.concurrent.ConcurrentSkipListMap$KeyIterator
2091:             2             32  java.util.concurrent.ConcurrentSkipListMap$KeySet
2092:             1             32  java.util.concurrent.FutureTask
2093:             1             32  java.util.concurrent.ThreadLocalRandom
2094:             2             32  java.util.logging.ErrorManager
2095:             1             32  java.util.logging.LogManager$SystemLoggerContext
2096:             1             32  java.util.regex.Pattern$3
2097:             1             32  javax.crypto.spec.RC5ParameterSpec
2098:             2             32  javax.management.NotificationFilterSupport
2099:             1             32  javax.management.remote.JMXServiceURL
2100:             1             32  javax.security.auth.Subject
2101:             1             32  net.jpountz.xxhash.XXHashFactory
2102:             1             32  org.apache.cassandra.auth.CassandraRoleManager
2103:             1             32  org.apache.cassandra.batchlog.BatchlogManager
2104:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache
2105:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache$1
2106:             1             32  org.apache.cassandra.config.Schema
2107:             1             32  org.apache.cassandra.cql3.QueryOptions$SpecificOptions
2108:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$4
2109:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$5
2110:             2             32  org.apache.cassandra.db.RangeSliceVerbHandler
2111:             1             32  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool
2112:             1             32  org.apache.cassandra.db.compaction.CompactionManager
2113:             2             32  org.apache.cassandra.db.lifecycle.LogReplicaSet
2114:             2             32  org.apache.cassandra.db.lifecycle.LogTransaction$TransactionTidier
2115:             1             32  org.apache.cassandra.db.marshal.AsciiType
2116:             1             32  org.apache.cassandra.db.marshal.PartitionerDefinedOrder
2117:             2             32  org.apache.cassandra.db.rows.CellPath$EmptyCellPath
2118:             2             32  org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer
2119:             1             32  org.apache.cassandra.hints.HintsBuffer
2120:             1             32  org.apache.cassandra.hints.HintsBufferPool
2121:             1             32  org.apache.cassandra.hints.HintsDispatchExecutor
2122:             1             32  org.apache.cassandra.hints.HintsDispatchTrigger
2123:             1             32  org.apache.cassandra.index.internal.composites.CollectionKeyIndex
2124:             1             32  org.apache.cassandra.io.compress.CompressedSequentialWriter$TransactionalProxy
2125:             1             32  org.apache.cassandra.io.compress.LZ4Compressor
2126:             1             32  org.apache.cassandra.io.sstable.IndexSummaryManager
2127:             1             32  org.apache.cassandra.metrics.CQLMetrics
2128:             2             32  org.apache.cassandra.metrics.ClientMetrics$$Lambda$278/1979648826
2129:             1             32  org.apache.cassandra.metrics.CommitLogMetrics
2130:             1             32  org.apache.cassandra.metrics.CompactionMetrics
2131:             2             32  org.apache.cassandra.metrics.TableMetrics$AllTableMetricNameFactory
2132:             2             32  org.apache.cassandra.net.ResponseVerbHandler
2133:             1             32  org.apache.cassandra.repair.RepairRunnable
2134:             2             32  org.apache.cassandra.schema.Types
2135:             1             32  org.apache.cassandra.security.EncryptionContext
2136:             1             32  org.apache.cassandra.service.ActiveRepairService
2137:             1             32  org.apache.cassandra.service.CassandraDaemon
2138:             1             32  org.apache.cassandra.service.NativeTransportService
2139:             1             32  org.apache.cassandra.thrift.TCustomServerSocket
2140:             1             32  org.apache.cassandra.thrift.ThriftServer
2141:             1             32  org.apache.cassandra.utils.ExpiringMap
2142:             2             32  org.apache.cassandra.utils.IntegerInterval$Set
2143:             1             32  org.apache.cassandra.utils.ResourceWatcher$WatchedResource
2144:             1             32  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder
2145:             1             32  org.apache.cassandra.utils.btree.BTree$1
2146:             1             32  org.apache.cassandra.utils.btree.TreeBuilder$1
2147:             1             32  org.apache.cassandra.utils.concurrent.WaitQueue$TimedSignal
2148:             1             32  org.apache.cassandra.utils.memory.BufferPool$GlobalPool
2149:             1             32  org.apache.thrift.protocol.TBinaryProtocol$Factory
2150:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$2
2151:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$3
2152:             1             32  org.codehaus.jackson.map.deser.BeanDeserializerFactory$ConfigImpl
2153:             1             32  org.codehaus.jackson.map.deser.StdDeserializerProvider
2154:             1             32  org.codehaus.jackson.map.introspect.VisibilityChecker$Std
2155:             2             32  org.codehaus.jackson.map.ser.StdSerializers$NumberSerializer
2156:             2             32  org.codehaus.jackson.map.ser.std.StdKeySerializer
2157:             1             32  org.codehaus.jackson.map.type.TypeFactory
2158:             2             32  org.codehaus.jackson.map.util.RootNameLookup
2159:             1             32  org.github.jamm.MemoryMeter
2160:             1             32  sun.instrument.InstrumentationImpl
2161:             1             32  sun.management.GcInfoCompositeData
2162:             1             32  sun.management.MappedMXBeanType$InProgress
2163:             1             32  sun.nio.ch.ServerSocketAdaptor
2164:             2             32  sun.nio.ch.SocketDispatcher
2165:             1             32  sun.nio.cs.StandardCharsets
2166:             1             32  sun.nio.fs.LinuxFileSystem
2167:             1             32  sun.reflect.UnsafeIntegerFieldAccessorImpl
2168:             1             32  sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl
2169:             2             32  sun.rmi.server.UnicastRef
2170:             2             32  sun.rmi.server.UnicastRef2
2171:             2             32  sun.rmi.transport.DGCImpl$1
2172:             1             32  sun.rmi.transport.proxy.RMIMasterSocketFactory
2173:             1             32  sun.rmi.transport.tcp.TCPTransport$AcceptLoop
2174:             1             32  sun.security.provider.SecureRandom
2175:             2             32  sun.security.ssl.SSLAlgorithmDecomposer
2176:             1             32  sun.security.ssl.X509TrustManagerImpl
2177:             1             32  sun.security.validator.SimpleValidator
2178:             1             32  sun.security.x509.AuthorityInfoAccessExtension
2179:             1             32  sun.security.x509.IssuerAlternativeNameExtension
2180:             1             32  sun.security.x509.PolicyMappingsExtension
2181:             1             32  sun.util.locale.provider.LocaleResources
2182:             1             24  [Lch.qos.logback.core.joran.spi.ConsoleTarget;
2183:             1             24  [Lch.qos.logback.core.subst.Node$Type;
2184:             1             24  [Lcom.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format;
2185:             1             24  [Lcom.github.benmanes.caffeine.cache.Buffer;
2186:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledTicker;
2187:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledWriter;
2188:             1             24  [Lcom.github.benmanes.caffeine.cache.SingletonWeigher;
2189:             1             24  [Lcom.github.benmanes.caffeine.cache.stats.DisabledStatsCounter;
2190:             1             24  [Lcom.google.common.base.Functions$IdentityFunction;
2191:             1             24  [Lcom.google.common.cache.CacheBuilder$NullListener;
2192:             1             24  [Lcom.google.common.cache.CacheBuilder$OneWeigher;
2193:             1             24  [Lcom.google.common.collect.GenericMapMaker$NullListener;
2194:             1             24  [Lcom.google.common.collect.Maps$EntryFunction;
2195:             1             24  [Lcom.google.common.util.concurrent.MoreExecutors$DirectExecutor;
2196:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener;
2197:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher;
2198:             1             24  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property;
2199:             1             24  [Ljava.io.File$PathStatus;
2200:             1             24  [Ljava.lang.ClassValue$Entry;
2201:             1             24  [Ljava.lang.management.MemoryType;
2202:             1             24  [Ljava.net.InetAddress$Cache$Type;
2203:             1             24  [Ljava.net.InterfaceAddress;
2204:             1             24  [Ljava.net.StandardProtocolFamily;
2205:             1             24  [Ljava.rmi.server.ObjID;
2206:             1             24  [Ljava.util.Comparators$NaturalOrderComparator;
2207:             1             24  [Ljava.util.Locale$Category;
2208:             1             24  [Ljava.util.concurrent.ExecutorService;
2209:             1             24  [Ljava.util.concurrent.ThreadPoolExecutor;
2210:             1             24  [Ljavax.net.ssl.KeyManager;
2211:             1             24  [Ljavax.net.ssl.TrustManager;
2212:             1             24  [Lorg.apache.cassandra.concurrent.ExecutorLocal;
2213:             1             24  [Lorg.apache.cassandra.config.Config$DiskOptimizationStrategy;
2214:             1             24  [Lorg.apache.cassandra.config.Config$RequestSchedulerId;
2215:             1             24  [Lorg.apache.cassandra.cql3.QueryProcessor$InternalStateInstance;
2216:             1             24  [Lorg.apache.cassandra.cql3.Term;
2217:             1             24  [Lorg.apache.cassandra.cql3.statements.Bound;
2218:             1             24  [Lorg.apache.cassandra.db.Directories$OnTxnErr;
2219:             1             24  [Lorg.apache.cassandra.db.ReadCommand$Kind;
2220:             1             24  [Lorg.apache.cassandra.db.aggregation.AggregationSpecification$Kind;
2221:             1             24  [Lorg.apache.cassandra.db.filter.ClusteringIndexFilter$Kind;
2222:             1             24  [Lorg.apache.cassandra.db.rows.Unfiltered$Kind;
2223:             1             24  [Lorg.apache.cassandra.hints.HintsDispatcher$Action;
2224:             1             24  [Lorg.apache.cassandra.io.compress.BufferType;
2225:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableFormat$Type;
2226:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason;
2227:             1             24  [Lorg.apache.cassandra.metrics.TableMetrics$Sampler;
2228:             1             24  [Lorg.apache.cassandra.schema.CachingParams$Option;
2229:             1             24  [Lorg.apache.cassandra.schema.KeyspaceParams$Option;
2230:             1             24  [Lorg.apache.cassandra.streaming.ProgressInfo$Direction;
2231:             1             24  [Lorg.apache.cassandra.transport.Event$StatusChange$Status;
2232:             1             24  [Lorg.apache.cassandra.transport.Message$Direction;
2233:             1             24  [Lorg.apache.cassandra.utils.ChecksumType;
2234:             1             24  [Lorg.apache.cassandra.utils.Throwables$FileOpType;
2235:             1             24  [Lorg.apache.cassandra.utils.btree.BTree$Dir;
2236:             1             24  [Lsun.launcher.LauncherHelper;
2237:             1             24  [Lsun.security.ssl.EphemeralKeyManager$EphemeralKeyPair;
2238:             1             24  ch.qos.logback.classic.joran.action.ConsolePluginAction
2239:             1             24  ch.qos.logback.classic.joran.action.ContextNameAction
2240:             1             24  ch.qos.logback.classic.joran.action.InsertFromJNDIAction
2241:             1             24  ch.qos.logback.classic.joran.action.JMXConfiguratorAction
2242:             1             24  ch.qos.logback.classic.spi.TurboFilterList
2243:             1             24  ch.qos.logback.classic.util.ContextSelectorStaticBinder
2244:             1             24  ch.qos.logback.classic.util.LogbackMDCAdapter
2245:             1             24  ch.qos.logback.core.joran.action.ContextPropertyAction
2246:             1             24  ch.qos.logback.core.joran.spi.CAI_WithLocatorSupport
2247:             1             24  ch.qos.logback.core.joran.spi.EventPlayer
2248:             1             24  com.clearspring.analytics.stream.cardinality.RegisterSet
2249:             1             24  com.codahale.metrics.MetricRegistry
2250:             1             24  com.github.benmanes.caffeine.cache.BoundedBuffer
2251:             1             24  com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask
2252:             1             24  com.github.benmanes.caffeine.cache.DisabledTicker
2253:             1             24  com.github.benmanes.caffeine.cache.DisabledWriter
2254:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$1
2255:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$10
2256:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$100
2257:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$101
2258:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$102
2259:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$103
2260:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$104
2261:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$105
2262:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$106
2263:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$107
2264:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$108
2265:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$109
2266:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$11
2267:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$110
2268:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$111
2269:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$112
2270:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$113
2271:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$114
2272:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$115
2273:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$116
2274:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$117
2275:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$118
2276:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$119
2277:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$12
2278:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$120
2279:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$121
2280:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$122
2281:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$123
2282:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$124
2283:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$125
2284:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$126
2285:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$127
2286:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$128
2287:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$129
2288:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$13
2289:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$130
2290:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$131
2291:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$132
2292:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$133
2293:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$134
2294:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$135
2295:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$136
2296:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$137
2297:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$138
2298:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$139
2299:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$14
2300:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$140
2301:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$141
2302:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$142
2303:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$143
2304:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$144
2305:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$15
2306:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$16
2307:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$17
2308:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$18
2309:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$19
2310:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$2
2311:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$20
2312:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$21
2313:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$22
2314:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$23
2315:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$24
2316:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$25
2317:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$26
2318:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$27
2319:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$28
2320:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$29
2321:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$3
2322:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$30
2323:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$31
2324:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$32
2325:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$33
2326:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$34
2327:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$35
2328:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$36
2329:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$37
2330:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$38
2331:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$39
2332:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$4
2333:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$40
2334:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$41
2335:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$42
2336:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$43
2337:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$44
2338:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$45
2339:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$46
2340:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$47
2341:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$48
2342:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$49
2343:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$5
2344:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$50
2345:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$51
2346:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$52
2347:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$53
2348:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$54
2349:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$55
2350:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$56
2351:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$57
2352:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$58
2353:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$59
2354:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$6
2355:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$60
2356:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$61
2357:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$62
2358:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$63
2359:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$64
2360:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$65
2361:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$66
2362:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$67
2363:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$68
2364:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$69
2365:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$7
2366:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$70
2367:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$71
2368:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$72
2369:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$73
2370:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$74
2371:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$75
2372:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$76
2373:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$77
2374:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$78
2375:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$79
2376:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$8
2377:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$80
2378:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$81
2379:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$82
2380:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$83
2381:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$84
2382:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$85
2383:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$86
2384:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$87
2385:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$88
2386:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$89
2387:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$9
2388:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$90
2389:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$91
2390:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$92
2391:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$93
2392:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$94
2393:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$95
2394:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$96
2395:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$97
2396:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$98
2397:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$99
2398:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$1
2399:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$2
2400:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$3
2401:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$4
2402:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$5
2403:             1             24  com.github.benmanes.caffeine.cache.SingletonWeigher
2404:             1             24  com.github.benmanes.caffeine.cache.stats.DisabledStatsCounter
2405:             1             24  com.google.common.base.CharMatcher$Or
2406:             1             24  com.google.common.base.Functions$IdentityFunction
2407:             1             24  com.google.common.base.Joiner$1
2408:             1             24  com.google.common.base.Joiner$MapJoiner
2409:             1             24  com.google.common.base.Predicates$ObjectPredicate$1
2410:             1             24  com.google.common.base.Predicates$ObjectPredicate$2
2411:             1             24  com.google.common.base.Predicates$ObjectPredicate$3
2412:             1             24  com.google.common.base.Predicates$ObjectPredicate$4
2413:             1             24  com.google.common.cache.CacheBuilder$NullListener
2414:             1             24  com.google.common.cache.CacheBuilder$OneWeigher
2415:             1             24  com.google.common.cache.LocalCache$EntryFactory$1
2416:             1             24  com.google.common.cache.LocalCache$EntryFactory$2
2417:             1             24  com.google.common.cache.LocalCache$EntryFactory$3
2418:             1             24  com.google.common.cache.LocalCache$EntryFactory$4
2419:             1             24  com.google.common.cache.LocalCache$EntryFactory$5
2420:             1             24  com.google.common.cache.LocalCache$EntryFactory$6
2421:             1             24  com.google.common.cache.LocalCache$EntryFactory$7
2422:             1             24  com.google.common.cache.LocalCache$EntryFactory$8
2423:             1             24  com.google.common.cache.LocalCache$Strength$1
2424:             1             24  com.google.common.cache.LocalCache$Strength$2
2425:             1             24  com.google.common.cache.LocalCache$Strength$3
2426:             1             24  com.google.common.collect.ByFunctionOrdering
2427:             1             24  com.google.common.collect.ConcurrentHashMultiset
2428:             1             24  com.google.common.collect.EmptyImmutableSortedSet
2429:             1             24  com.google.common.collect.GenericMapMaker$NullListener
2430:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$1
2431:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$2
2432:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$3
2433:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$4
2434:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$5
2435:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$6
2436:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$7
2437:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$8
2438:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$1
2439:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$2
2440:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$3
2441:             1             24  com.google.common.collect.Maps$EntryFunction$1
2442:             1             24  com.google.common.collect.Maps$EntryFunction$2
2443:             1             24  com.google.common.collect.Sets$3
2444:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$1
2445:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$2
2446:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$3
2447:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$1
2448:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$2
2449:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$3
2450:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$4
2451:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$5
2452:             1             24  com.google.common.util.concurrent.Futures$1$1
2453:             1             24  com.google.common.util.concurrent.Futures$ChainingListenableFuture$1
2454:             1             24  com.google.common.util.concurrent.MoreExecutors$DirectExecutor
2455:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$KeySetView
2456:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener
2457:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$1
2458:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$2
2459:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$3
2460:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$KeySet
2461:             1             24  com.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher
2462:             1             24  com.sun.beans.util.Cache$Kind$1
2463:             1             24  com.sun.beans.util.Cache$Kind$2
2464:             1             24  com.sun.beans.util.Cache$Kind$3
2465:             1             24  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport
2466:             1             24  com.sun.jmx.mbeanserver.MXBeanLookup
2467:             1             24  com.sun.jmx.remote.internal.ArrayNotificationBuffer$ShareBuffer
2468:             1             24  com.sun.jna.Structure$3
2469:             1             24  com.sun.org.apache.xerces.internal.impl.Constants$ArrayEnumeration
2470:             1             24  io.netty.buffer.UnpooledByteBufAllocator
2471:             1             24  io.netty.channel.AdaptiveRecvByteBufAllocator
2472:             1             24  io.netty.channel.SucceededChannelFuture
2473:             1             24  io.netty.channel.unix.Socket
2474:             1             24  io.netty.util.concurrent.FailedFuture
2475:             1             24  java.lang.ClassValue$Version
2476:             1             24  java.lang.Package$1
2477:             1             24  java.lang.ProcessEnvironment$StringEnvironment
2478:             1             24  java.lang.invoke.MethodHandleImpl$4
2479:             1             24  java.lang.invoke.MethodType$ConcurrentWeakInternSet
2480:             1             24  java.math.MutableBigInteger
2481:             1             24  java.net.Inet4AddressImpl
2482:             1             24  java.net.InterfaceAddress
2483:             1             24  java.nio.file.FileVisitOption
2484:             1             24  java.nio.file.LinkOption
2485:             1             24  java.security.CodeSigner
2486:             1             24  java.security.Policy$PolicyInfo
2487:             1             24  java.security.Policy$UnsupportedEmptyCollection
2488:             1             24  java.util.Collections$EmptyMap
2489:             1             24  java.util.Collections$UnmodifiableList
2490:             1             24  java.util.Comparators$NaturalOrderComparator
2491:             1             24  java.util.Currency
2492:             1             24  java.util.Locale$Cache
2493:             1             24  java.util.OptionalLong
2494:             1             24  java.util.ResourceBundle$Control$CandidateListCache
2495:             1             24  java.util.Vector$1
2496:             1             24  java.util.concurrent.Executors$DelegatedScheduledExecutorService
2497:             1             24  java.util.concurrent.TimeUnit$1
2498:             1             24  java.util.concurrent.TimeUnit$2
2499:             1             24  java.util.concurrent.TimeUnit$3
2500:             1             24  java.util.concurrent.TimeUnit$4
2501:             1             24  java.util.concurrent.TimeUnit$5
2502:             1             24  java.util.concurrent.TimeUnit$6
2503:             1             24  java.util.concurrent.TimeUnit$7
2504:             1             24  java.util.logging.LogManager$5
2505:             1             24  java.util.logging.LogManager$LoggerContext
2506:             1             24  java.util.logging.LoggingPermission
2507:             1             24  java.util.regex.Pattern$SingleI
2508:             1             24  javax.crypto.spec.RC2ParameterSpec
2509:             1             24  javax.management.NotificationBroadcasterSupport
2510:             1             24  javax.net.ssl.SSLContext
2511:             1             24  org.antlr.runtime.CharStreamState
2512:             1             24  org.apache.cassandra.auth.CassandraAuthorizer
2513:             1             24  org.apache.cassandra.auth.CassandraRoleManager$Role
2514:             1             24  org.apache.cassandra.auth.PasswordAuthenticator
2515:             1             24  org.apache.cassandra.cache.ChunkCache
2516:             1             24  org.apache.cassandra.config.Config$1
2517:             1             24  org.apache.cassandra.config.Config$RequestSchedulerId
2518:             1             24  org.apache.cassandra.config.RequestSchedulerOptions
2519:             1             24  org.apache.cassandra.cql3.Attributes$Raw
2520:             1             24  org.apache.cassandra.cql3.ColumnConditions
2521:             1             24  org.apache.cassandra.cql3.CqlParser
2522:             1             24  org.apache.cassandra.cql3.ErrorCollector
2523:             1             24  org.apache.cassandra.cql3.Lists$Marker
2524:             1             24  org.apache.cassandra.cql3.Maps$DiscarderByKey
2525:             1             24  org.apache.cassandra.cql3.Maps$Marker
2526:             1             24  org.apache.cassandra.cql3.Maps$Setter
2527:             1             24  org.apache.cassandra.cql3.Operator$1
2528:             1             24  org.apache.cassandra.cql3.Operator$10
2529:             1             24  org.apache.cassandra.cql3.Operator$11
2530:             1             24  org.apache.cassandra.cql3.Operator$12
2531:             1             24  org.apache.cassandra.cql3.Operator$13
2532:             1             24  org.apache.cassandra.cql3.Operator$14
2533:             1             24  org.apache.cassandra.cql3.Operator$15
2534:             1             24  org.apache.cassandra.cql3.Operator$2
2535:             1             24  org.apache.cassandra.cql3.Operator$3
2536:             1             24  org.apache.cassandra.cql3.Operator$4
2537:             1             24  org.apache.cassandra.cql3.Operator$5
2538:             1             24  org.apache.cassandra.cql3.Operator$6
2539:             1             24  org.apache.cassandra.cql3.Operator$7
2540:             1             24  org.apache.cassandra.cql3.Operator$8
2541:             1             24  org.apache.cassandra.cql3.Operator$9
2542:             1             24  org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance
2543:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$1
2544:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$10
2545:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$11
2546:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$12
2547:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$13
2548:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$14
2549:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$15
2550:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$16
2551:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$17
2552:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$18
2553:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$19
2554:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$2
2555:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$20
2556:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$21
2557:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$3
2558:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$4
2559:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$5
2560:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$6
2561:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$7
2562:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$8
2563:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$9
2564:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$3
2565:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$4
2566:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$1
2567:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$10
2568:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$11
2569:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$12
2570:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$2
2571:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$3
2572:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$6
2573:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$7
2574:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$8
2575:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$9
2576:             1             24  org.apache.cassandra.cql3.functions.UuidFcts$1
2577:             1             24  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$InRestrictionWithMarker
2578:             1             24  org.apache.cassandra.cql3.restrictions.TermSlice
2579:             1             24  org.apache.cassandra.cql3.restrictions.TokenRestriction$SliceRestriction
2580:             1             24  org.apache.cassandra.cql3.statements.StatementType$1
2581:             1             24  org.apache.cassandra.cql3.statements.StatementType$2
2582:             1             24  org.apache.cassandra.cql3.statements.StatementType$3
2583:             1             24  org.apache.cassandra.cql3.statements.StatementType$4
2584:             1             24  org.apache.cassandra.db.BlacklistedDirectories
2585:             1             24  org.apache.cassandra.db.Clustering$1
2586:             1             24  org.apache.cassandra.db.Clustering$2
2587:             1             24  org.apache.cassandra.db.Slice$1
2588:             1             24  org.apache.cassandra.db.commitlog.CommitLog$Configuration
2589:             1             24  org.apache.cassandra.db.compaction.CompactionLogger$CompactionLogSerializer
2590:             1             24  org.apache.cassandra.db.filter.DataLimits$1
2591:             1             24  org.apache.cassandra.db.filter.DataLimits$CQLLimits
2592:             1             24  org.apache.cassandra.db.marshal.AsciiType$1
2593:             1             24  org.apache.cassandra.db.marshal.BooleanType
2594:             1             24  org.apache.cassandra.db.marshal.ByteType
2595:             1             24  org.apache.cassandra.db.marshal.BytesType
2596:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$1
2597:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$2
2598:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$3
2599:             1             24  org.apache.cassandra.db.marshal.CounterColumnType
2600:             1             24  org.apache.cassandra.db.marshal.DecimalType
2601:             1             24  org.apache.cassandra.db.marshal.DoubleType
2602:             1             24  org.apache.cassandra.db.marshal.DurationType
2603:             1             24  org.apache.cassandra.db.marshal.EmptyType
2604:             1             24  org.apache.cassandra.db.marshal.FloatType
2605:             1             24  org.apache.cassandra.db.marshal.InetAddressType
2606:             1             24  org.apache.cassandra.db.marshal.Int32Type
2607:             1             24  org.apache.cassandra.db.marshal.IntegerType
2608:             1             24  org.apache.cassandra.db.marshal.LongType
2609:             1             24  org.apache.cassandra.db.marshal.ShortType
2610:             1             24  org.apache.cassandra.db.marshal.SimpleDateType
2611:             1             24  org.apache.cassandra.db.marshal.TimeType
2612:             1             24  org.apache.cassandra.db.marshal.TimeUUIDType
2613:             1             24  org.apache.cassandra.db.marshal.TimestampType
2614:             1             24  org.apache.cassandra.db.marshal.TypeParser
2615:             1             24  org.apache.cassandra.db.marshal.UTF8Type
2616:             1             24  org.apache.cassandra.db.marshal.UUIDType
2617:             1             24  org.apache.cassandra.db.transform.Stack
2618:             1             24  org.apache.cassandra.dht.Murmur3Partitioner
2619:             1             24  org.apache.cassandra.dht.Murmur3Partitioner$1
2620:             1             24  org.apache.cassandra.hints.HintsCatalog
2621:             1             24  org.apache.cassandra.hints.HintsWriteExecutor
2622:             1             24  org.apache.cassandra.io.compress.BufferType$1
2623:             1             24  org.apache.cassandra.io.compress.BufferType$2
2624:             1             24  org.apache.cassandra.io.util.ChecksumWriter
2625:             1             24  org.apache.cassandra.io.util.SequentialWriter$TransactionalProxy
2626:             1             24  org.apache.cassandra.io.util.SsdDiskOptimizationStrategy
2627:             1             24  org.apache.cassandra.locator.ReconnectableSnitchHelper
2628:             1             24  org.apache.cassandra.metrics.AuthMetrics
2629:             1             24  org.apache.cassandra.metrics.BufferPoolMetrics
2630:             1             24  org.apache.cassandra.metrics.CassandraMetricsRegistry
2631:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$1
2632:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$2
2633:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$3
2634:             1             24  org.apache.cassandra.metrics.CompactionMetrics$3
2635:             1             24  org.apache.cassandra.metrics.HintedHandoffMetrics
2636:             1             24  org.apache.cassandra.metrics.MessagingMetrics
2637:             1             24  org.apache.cassandra.net.MessagingService$Verb$1
2638:             1             24  org.apache.cassandra.net.MessagingService$Verb$10
2639:             1             24  org.apache.cassandra.net.MessagingService$Verb$11
2640:             1             24  org.apache.cassandra.net.MessagingService$Verb$12
2641:             1             24  org.apache.cassandra.net.MessagingService$Verb$13
2642:             1             24  org.apache.cassandra.net.MessagingService$Verb$2
2643:             1             24  org.apache.cassandra.net.MessagingService$Verb$3
2644:             1             24  org.apache.cassandra.net.MessagingService$Verb$4
2645:             1             24  org.apache.cassandra.net.MessagingService$Verb$5
2646:             1             24  org.apache.cassandra.net.MessagingService$Verb$6
2647:             1             24  org.apache.cassandra.net.MessagingService$Verb$7
2648:             1             24  org.apache.cassandra.net.MessagingService$Verb$8
2649:             1             24  org.apache.cassandra.net.MessagingService$Verb$9
2650:             1             24  org.apache.cassandra.service.CacheService
2651:             1             24  org.apache.cassandra.service.GCInspector
2652:             1             24  org.apache.cassandra.service.PendingRangeCalculatorService
2653:             1             24  org.apache.cassandra.service.QueryState
2654:             1             24  org.apache.cassandra.service.StartupChecks
2655:             1             24  org.apache.cassandra.service.StartupChecks$8
2656:             1             24  org.apache.cassandra.streaming.StreamManager
2657:             1             24  org.apache.cassandra.thrift.Cassandra$Processor
2658:             1             24  org.apache.cassandra.tracing.TracingImpl
2659:             1             24  org.apache.cassandra.transport.ConnectionLimitHandler
2660:             1             24  org.apache.cassandra.transport.Frame$Compressor
2661:             1             24  org.apache.cassandra.transport.Frame$Decompressor
2662:             1             24  org.apache.cassandra.transport.Frame$Encoder
2663:             1             24  org.apache.cassandra.transport.Message$Dispatcher
2664:             1             24  org.apache.cassandra.transport.Message$ProtocolDecoder
2665:             1             24  org.apache.cassandra.transport.Message$ProtocolEncoder
2666:             1             24  org.apache.cassandra.transport.RequestThreadPoolExecutor
2667:             1             24  org.apache.cassandra.transport.Server$ConnectionTracker
2668:             1             24  org.apache.cassandra.transport.Server$EventNotifier
2669:             1             24  org.apache.cassandra.transport.Server$Initializer
2670:             1             24  org.apache.cassandra.triggers.TriggerExecutor
2671:             1             24  org.apache.cassandra.utils.ChecksumType$1
2672:             1             24  org.apache.cassandra.utils.ChecksumType$2
2673:             1             24  org.apache.cassandra.utils.ConcurrentBiMap
2674:             1             24  org.apache.cassandra.utils.ExpiringMap$1
2675:             1             24  org.apache.cassandra.utils.HistogramBuilder
2676:             1             24  org.apache.cassandra.utils.IntervalTree
2677:             1             24  org.apache.cassandra.utils.JMXServerUtils$Registry
2678:             1             24  org.apache.cassandra.utils.concurrent.OpOrder$Barrier
2679:             1             24  org.apache.cassandra.utils.memory.BufferPool$Debug
2680:             1             24  org.apache.cassandra.utils.progress.jmx.JMXProgressSupport
2681:             1             24  org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport
2682:             1             24  org.codehaus.jackson.map.deser.BeanDeserializerFactory
2683:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory
2684:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory$ConfigImpl
2685:             1             24  org.codehaus.jackson.map.ser.impl.FailingSerializer
2686:             1             24  org.codehaus.jackson.map.ser.impl.SerializerCache
2687:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$BooleanArraySerializer
2688:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$DoubleArraySerializer
2689:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$FloatArraySerializer
2690:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$IntArraySerializer
2691:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$LongArraySerializer
2692:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$ShortArraySerializer
2693:             1             24  org.slf4j.helpers.FormattingTuple
2694:             1             24  org.slf4j.impl.StaticLoggerBinder
2695:             1             24  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.PercentEscaper
2696:             1             24  sun.instrument.TransformerManager
2697:             1             24  sun.launcher.LauncherHelper
2698:             1             24  sun.management.CompilationImpl
2699:             1             24  sun.management.GarbageCollectionNotifInfoCompositeData
2700:             1             24  sun.management.MemoryImpl
2701:             1             24  sun.management.OperatingSystemImpl
2702:             1             24  sun.management.RuntimeImpl
2703:             1             24  sun.management.ThreadImpl
2704:             1             24  sun.management.VMManagementImpl
2705:             1             24  sun.misc.JarIndex
2706:             1             24  sun.net.ProgressMonitor
2707:             1             24  sun.net.sdp.SdpProvider
2708:             1             24  sun.net.www.protocol.http.Handler
2709:             1             24  sun.nio.cs.ISO_8859_1
2710:             1             24  sun.nio.cs.US_ASCII
2711:             1             24  sun.nio.cs.UTF_16
2712:             1             24  sun.nio.cs.UTF_16BE
2713:             1             24  sun.nio.cs.UTF_16LE
2714:             1             24  sun.nio.cs.UTF_8
2715:             1             24  sun.rmi.runtime.RuntimeUtil$1
2716:             1             24  sun.rmi.server.LoaderHandler$1
2717:             1             24  sun.rmi.transport.DGCImpl
2718:             1             24  sun.rmi.transport.Target$$Lambda$338/684260999
2719:             1             24  sun.security.provider.certpath.X509CertPath
2720:             1             24  sun.security.ssl.SunX509KeyManagerImpl
2721:             1             24  sun.security.validator.EndEntityChecker
2722:             1             24  sun.security.x509.AccessDescription
2723:             1             24  sun.security.x509.CertificatePolicyMap
2724:             1             24  sun.util.locale.BaseLocale$Cache
2725:             1             24  sun.util.locale.provider.CalendarDataProviderImpl
2726:             1             24  sun.util.locale.provider.CalendarProviderImpl
2727:             1             24  sun.util.locale.provider.CurrencyNameProviderImpl
2728:             1             24  sun.util.locale.provider.DateFormatSymbolsProviderImpl
2729:             1             24  sun.util.locale.provider.DecimalFormatSymbolsProviderImpl
2730:             1             24  sun.util.locale.provider.NumberFormatProviderImpl
2731:             1             24  sun.util.logging.PlatformLogger
2732:             1             24  sun.util.logging.PlatformLogger$JavaLoggerProxy
2733:             1             24  sun.util.resources.LocaleData$1
2734:             1             16  [Lch.qos.logback.classic.spi.ThrowableProxy;
2735:             1             16  [Ljava.beans.EventSetDescriptor;
2736:             1             16  [Ljava.lang.Double;
2737:             1             16  [Ljava.lang.Float;
2738:             1             16  [Ljava.lang.Throwable;
2739:             1             16  [Ljava.net.NetworkInterface;
2740:             1             16  [Ljava.net.URL;
2741:             1             16  [Ljava.nio.file.attribute.FileAttribute;
2742:             1             16  [Ljava.security.Provider;
2743:             1             16  [Ljava.text.FieldPosition;
2744:             1             16  [Ljavax.security.cert.X509Certificate;
2745:             1             16  [Lnet.jpountz.lz4.LZ4JNI;
2746:             1             16  [Lnet.jpountz.lz4.LZ4Utils;
2747:             1             16  [Lnet.jpountz.util.ByteBufferUtils;
2748:             1             16  [Lnet.jpountz.util.Native;
2749:             1             16  [Lnet.jpountz.util.SafeUtils;
2750:             1             16  [Lnet.jpountz.xxhash.XXHashJNI;
2751:             1             16  [Lorg.apache.cassandra.db.rows.Cell;
2752:             1             16  [Lorg.apache.cassandra.db.transform.Stack$MoreContentsHolder;
2753:             1             16  [Lorg.codehaus.jackson.map.AbstractTypeResolver;
2754:             1             16  [Lorg.codehaus.jackson.map.Deserializers;
2755:             1             16  [Lorg.codehaus.jackson.map.KeyDeserializers;
2756:             1             16  [Lorg.codehaus.jackson.map.Serializers;
2757:             1             16  [Lorg.codehaus.jackson.map.deser.BeanDeserializerModifier;
2758:             1             16  [Lorg.codehaus.jackson.map.deser.ValueInstantiators;
2759:             1             16  [Lorg.codehaus.jackson.map.introspect.AnnotationMap;
2760:             1             16  [Lorg.codehaus.jackson.map.ser.BeanSerializerModifier;
2761:             1             16  [Lsun.instrument.TransformerManager$TransformerInfo;
2762:             1             16  ch.qos.logback.classic.selector.DefaultContextSelector
2763:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$1
2764:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$2
2765:             1             16  ch.qos.logback.core.joran.spi.DefaultNestedComponentRegistry
2766:             1             16  ch.qos.logback.core.joran.util.ConfigurationWatchListUtil
2767:             1             16  com.codahale.metrics.Clock$UserTimeClock
2768:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$1
2769:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$2
2770:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$3
2771:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$4
2772:             1             16  com.codahale.metrics.Striped64$ThreadHashCode
2773:             1             16  com.codahale.metrics.ThreadLocalRandom$1
2774:             1             16  com.github.benmanes.caffeine.SingleConsumerQueue$$Lambda$80/692511295
2775:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$79/608770405
2776:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache$$Lambda$81/1858886571
2777:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$EntrySetView
2778:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$KeySetView
2779:             1             16  com.github.benmanes.caffeine.cache.BoundedWeigher
2780:             1             16  com.github.benmanes.caffeine.cache.Caffeine$$Lambda$77/2064869182
2781:             1             16  com.google.common.base.Absent
2782:             1             16  com.google.common.base.CharMatcher$1
2783:             1             16  com.google.common.base.CharMatcher$15
2784:             1             16  com.google.common.base.CharMatcher$2
2785:             1             16  com.google.common.base.CharMatcher$3
2786:             1             16  com.google.common.base.CharMatcher$4
2787:             1             16  com.google.common.base.CharMatcher$5
2788:             1             16  com.google.common.base.CharMatcher$6
2789:             1             16  com.google.common.base.CharMatcher$7
2790:             1             16  com.google.common.base.CharMatcher$8
2791:             1             16  com.google.common.base.Equivalence$Equals
2792:             1             16  com.google.common.base.Equivalence$Identity
2793:             1             16  com.google.common.base.Predicates$NotPredicate
2794:             1             16  com.google.common.base.Predicates$OrPredicate
2795:             1             16  com.google.common.base.Suppliers$SupplierOfInstance
2796:             1             16  com.google.common.base.Ticker$1
2797:             1             16  com.google.common.cache.CacheBuilder$1
2798:             1             16  com.google.common.cache.CacheBuilder$2
2799:             1             16  com.google.common.cache.CacheBuilder$3
2800:             1             16  com.google.common.cache.LocalCache$1
2801:             1             16  com.google.common.cache.LocalCache$2
2802:             1             16  com.google.common.cache.LocalCache$LocalManualCache
2803:             1             16  com.google.common.collect.ComparatorOrdering
2804:             1             16  com.google.common.collect.EmptyImmutableSet
2805:             1             16  com.google.common.collect.Iterators$1
2806:             1             16  com.google.common.collect.Iterators$2
2807:             1             16  com.google.common.collect.MapMakerInternalMap$1
2808:             1             16  com.google.common.collect.MapMakerInternalMap$2
2809:             1             16  com.google.common.collect.Multisets$5
2810:             1             16  com.google.common.collect.NaturalOrdering
2811:             1             16  com.google.common.collect.ReverseOrdering
2812:             1             16  com.google.common.io.ByteStreams$1
2813:             1             16  com.google.common.util.concurrent.Futures$4
2814:             1             16  com.google.common.util.concurrent.Futures$7
2815:             1             16  com.google.common.util.concurrent.Runnables$1
2816:             1             16  com.google.common.util.concurrent.Striped$5
2817:             1             16  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingQueue
2818:             1             16  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ResourceContext$1
2819:             1             16  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory
2820:             1             16  com.sun.jmx.mbeanserver.DescriptorCache
2821:             1             16  com.sun.jmx.mbeanserver.MBeanAnalyzer$MethodOrder
2822:             1             16  com.sun.jmx.mbeanserver.MBeanInstantiator
2823:             1             16  com.sun.jmx.mbeanserver.MXBeanIntrospector
2824:             1             16  com.sun.jmx.mbeanserver.SecureClassLoaderRepository
2825:             1             16  com.sun.jmx.mbeanserver.StandardMBeanIntrospector
2826:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$5
2827:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BroadcasterQuery
2828:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BufferListener
2829:             1             16  com.sun.jmx.remote.internal.ServerCommunicatorAdmin$Timeout
2830:             1             16  com.sun.jmx.remote.internal.ServerNotifForwarder$NotifForwarderBufferFilter
2831:             1             16  com.sun.jmx.remote.protocol.iiop.IIOPProxyImpl
2832:             1             16  com.sun.jmx.remote.security.SubjectDelegator
2833:             1             16  com.sun.jna.Native$1
2834:             1             16  com.sun.jna.Native$2
2835:             1             16  com.sun.jna.Native$7
2836:             1             16  com.sun.jna.Structure$1
2837:             1             16  com.sun.jna.Structure$2
2838:             1             16  com.sun.jna.VarArgsChecker$RealVarArgsChecker
2839:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDDatatypeValidator
2840:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDREFDatatypeValidator
2841:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NMTOKENDatatypeValidator
2842:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NOTATIONDatatypeValidator
2843:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.StringDatatypeValidator
2844:             1             16  com.sun.org.apache.xerces.internal.utils.SecuritySupport
2845:             1             16  com.sun.proxy.$Proxy2
2846:             1             16  com.sun.proxy.$Proxy4
2847:             1             16  com.sun.proxy.$Proxy7
2848:             1             16  io.netty.buffer.ByteBufUtil$1
2849:             1             16  io.netty.buffer.ByteBufUtil$2
2850:             1             16  io.netty.channel.ChannelFutureListener$1
2851:             1             16  io.netty.channel.ChannelFutureListener$2
2852:             1             16  io.netty.channel.ChannelFutureListener$3
2853:             1             16  io.netty.channel.ChannelMetadata
2854:             1             16  io.netty.channel.ChannelOutboundBuffer$1
2855:             1             16  io.netty.channel.DefaultChannelPipeline$1
2856:             1             16  io.netty.channel.DefaultMessageSizeEstimator
2857:             1             16  io.netty.channel.DefaultMessageSizeEstimator$HandleImpl
2858:             1             16  io.netty.channel.DefaultSelectStrategy
2859:             1             16  io.netty.channel.DefaultSelectStrategyFactory
2860:             1             16  io.netty.channel.group.ChannelMatchers$1
2861:             1             16  io.netty.channel.group.ChannelMatchers$InvertMatcher
2862:             1             16  io.netty.util.Recycler$1
2863:             1             16  io.netty.util.Recycler$3
2864:             1             16  io.netty.util.concurrent.DefaultPromise$CauseHolder
2865:             1             16  io.netty.util.concurrent.GlobalEventExecutor$1
2866:             1             16  io.netty.util.concurrent.GlobalEventExecutor$TaskRunner
2867:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$1
2868:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$PowerOfTwoEventExecutorChooser
2869:             1             16  io.netty.util.concurrent.RejectedExecutionHandlers$1
2870:             1             16  io.netty.util.concurrent.SingleThreadEventExecutor$1
2871:             1             16  io.netty.util.internal.NoOpTypeParameterMatcher
2872:             1             16  java.io.DeleteOnExitHook$1
2873:             1             16  java.io.FileDescriptor$1
2874:             1             16  java.io.ObjectInputStream$$Lambda$293/697818519
2875:             1             16  java.io.ObjectInputStream$1
2876:             1             16  java.lang.ApplicationShutdownHooks$1
2877:             1             16  java.lang.CharacterDataLatin1
2878:             1             16  java.lang.ClassValue$Identity
2879:             1             16  java.lang.ProcessBuilder$NullInputStream
2880:             1             16  java.lang.ProcessBuilder$NullOutputStream
2881:             1             16  java.lang.Runtime
2882:             1             16  java.lang.String$CaseInsensitiveComparator
2883:             1             16  java.lang.System$2
2884:             1             16  java.lang.Terminator$1
2885:             1             16  java.lang.UNIXProcess$$Lambda$13/1784131088
2886:             1             16  java.lang.UNIXProcess$$Lambda$14/2143582219
2887:             1             16  java.lang.UNIXProcess$Platform$$Lambda$10/616881582
2888:             1             16  java.lang.invoke.MemberName$Factory
2889:             1             16  java.lang.invoke.MethodHandleImpl$2
2890:             1             16  java.lang.invoke.MethodHandleImpl$3
2891:             1             16  java.lang.management.PlatformComponent$1
2892:             1             16  java.lang.management.PlatformComponent$10
2893:             1             16  java.lang.management.PlatformComponent$11
2894:             1             16  java.lang.management.PlatformComponent$12
2895:             1             16  java.lang.management.PlatformComponent$13
2896:             1             16  java.lang.management.PlatformComponent$14
2897:             1             16  java.lang.management.PlatformComponent$15
2898:             1             16  java.lang.management.PlatformComponent$2
2899:             1             16  java.lang.management.PlatformComponent$3
2900:             1             16  java.lang.management.PlatformComponent$4
2901:             1             16  java.lang.management.PlatformComponent$5
2902:             1             16  java.lang.management.PlatformComponent$6
2903:             1             16  java.lang.management.PlatformComponent$7
2904:             1             16  java.lang.management.PlatformComponent$8
2905:             1             16  java.lang.management.PlatformComponent$9
2906:             1             16  java.lang.ref.Reference$1
2907:             1             16  java.lang.ref.Reference$Lock
2908:             1             16  java.lang.reflect.Proxy$KeyFactory
2909:             1             16  java.lang.reflect.Proxy$ProxyClassFactory
2910:             1             16  java.lang.reflect.ReflectAccess
2911:             1             16  java.math.BigDecimal$1
2912:             1             16  java.net.InetAddress$2
2913:             1             16  java.net.URLClassLoader$7
2914:             1             16  java.nio.Bits$1
2915:             1             16  java.nio.Bits$1$1
2916:             1             16  java.nio.charset.CoderResult$1
2917:             1             16  java.nio.charset.CoderResult$2
2918:             1             16  java.nio.file.Files$AcceptAllFilter
2919:             1             16  java.rmi.server.RMIClassLoader$2
2920:             1             16  java.security.AllPermission
2921:             1             16  java.security.ProtectionDomain$2
2922:             1             16  java.security.ProtectionDomain$JavaSecurityAccessImpl
2923:             1             16  java.text.DontCareFieldPosition$1
2924:             1             16  java.util.Collections$EmptyEnumeration
2925:             1             16  java.util.Collections$EmptyIterator
2926:             1             16  java.util.Collections$EmptyList
2927:             1             16  java.util.Collections$EmptySet
2928:             1             16  java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet
2929:             1             16  java.util.Currency$CurrencyNameGetter
2930:             1             16  java.util.EnumMap$1
2931:             1             16  java.util.ResourceBundle$Control
2932:             1             16  java.util.Spliterators$EmptySpliterator$OfDouble
2933:             1             16  java.util.Spliterators$EmptySpliterator$OfInt
2934:             1             16  java.util.Spliterators$EmptySpliterator$OfLong
2935:             1             16  java.util.Spliterators$EmptySpliterator$OfRef
2936:             1             16  java.util.TreeMap$EntrySpliterator$$Lambda$68/1819038759
2937:             1             16  java.util.WeakHashMap$KeySet
2938:             1             16  java.util.concurrent.Executors$FinalizableDelegatedExecutorService
2939:             1             16  java.util.concurrent.ThreadPoolExecutor$AbortPolicy
2940:             1             16  java.util.jar.JarVerifier$3
2941:             1             16  java.util.jar.JavaUtilJarAccessImpl
2942:             1             16  java.util.logging.LoggingProxyImpl
2943:             1             16  java.util.regex.Pattern$4
2944:             1             16  java.util.regex.Pattern$LastNode
2945:             1             16  java.util.regex.Pattern$Node
2946:             1             16  java.util.stream.Collectors$$Lambda$178/1708585783
2947:             1             16  java.util.stream.Collectors$$Lambda$179/2048467502
2948:             1             16  java.util.stream.Collectors$$Lambda$180/1269763229
2949:             1             16  java.util.stream.Collectors$$Lambda$221/1489469437
2950:             1             16  java.util.stream.Collectors$$Lambda$222/431613642
2951:             1             16  java.util.stream.Collectors$$Lambda$223/1098744211
2952:             1             16  java.util.stream.Collectors$$Lambda$247/1746129463
2953:             1             16  java.util.stream.Collectors$$Lambda$60/1724814719
2954:             1             16  java.util.stream.Collectors$$Lambda$61/1718322084
2955:             1             16  java.util.stream.Collectors$$Lambda$62/24039137
2956:             1             16  java.util.stream.Collectors$$Lambda$63/992086987
2957:             1             16  java.util.stream.LongPipeline$$Lambda$189/1888591113
2958:             1             16  java.util.stream.LongPipeline$$Lambda$325/1014276638
2959:             1             16  java.util.zip.ZipFile$1
2960:             1             16  javax.crypto.JceSecurityManager
2961:             1             16  javax.management.JMX
2962:             1             16  javax.management.MBeanServerBuilder
2963:             1             16  javax.management.NotificationBroadcasterSupport$1
2964:             1             16  javax.management.remote.JMXPrincipal
2965:             1             16  javax.management.remote.rmi.RMIConnectionImpl_Stub
2966:             1             16  javax.management.remote.rmi.RMIServerImpl_Stub
2967:             1             16  javax.xml.parsers.SecuritySupport
2968:             1             16  net.jpountz.lz4.LZ4JNICompressor
2969:             1             16  net.jpountz.lz4.LZ4JNIFastDecompressor
2970:             1             16  net.jpountz.lz4.LZ4JNISafeDecompressor
2971:             1             16  net.jpountz.xxhash.StreamingXXHash32JNI$Factory
2972:             1             16  net.jpountz.xxhash.StreamingXXHash64JNI$Factory
2973:             1             16  net.jpountz.xxhash.XXHash32JNI
2974:             1             16  net.jpountz.xxhash.XXHash64JNI
2975:             1             16  org.apache.cassandra.auth.AllowAllAuthenticator$Negotiator
2976:             1             16  org.apache.cassandra.auth.AllowAllInternodeAuthenticator
2977:             1             16  org.apache.cassandra.auth.AuthCache$1
2978:             1             16  org.apache.cassandra.auth.AuthMigrationListener
2979:             1             16  org.apache.cassandra.auth.CassandraRoleManager$$Lambda$264/195066780
2980:             1             16  org.apache.cassandra.auth.CassandraRoleManager$1
2981:             1             16  org.apache.cassandra.auth.CassandraRoleManager$2
2982:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$265/385180766
2983:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$266/694021194
2984:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$267/767298601
2985:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$268/274090580
2986:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$269/1588510401
2987:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$270/331234425
2988:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$271/996989596
2989:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$272/1507030140
2990:             1             16  org.apache.cassandra.batchlog.Batch$Serializer
2991:             1             16  org.apache.cassandra.batchlog.BatchRemoveVerbHandler
2992:             1             16  org.apache.cassandra.batchlog.BatchStoreVerbHandler
2993:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$258/2042553130
2994:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$290/1638031626
2995:             1             16  org.apache.cassandra.cache.AutoSavingCache$1
2996:             1             16  org.apache.cassandra.cache.ChunkCache$$Lambda$78/420307438
2997:             1             16  org.apache.cassandra.cache.NopCacheProvider$NopCache
2998:             1             16  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$1
2999:             1             16  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1
3000:             1             16  org.apache.cassandra.concurrent.StageManager$1
3001:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$213/1328645530
3002:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$214/2107098463
3003:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$232/1529326426
3004:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$233/570714518
3005:             1             16  org.apache.cassandra.config.CFMetaData$Builder$$Lambda$30/671596011
3006:             1             16  org.apache.cassandra.config.CFMetaData$Serializer
3007:             1             16  org.apache.cassandra.config.ColumnDefinition$$Lambda$25/207471778
3008:             1             16  org.apache.cassandra.config.DatabaseDescriptor$1
3009:             1             16  org.apache.cassandra.config.Schema$$Lambda$262/956354740
3010:             1             16  org.apache.cassandra.config.Schema$$Lambda$263/2080528880
3011:             1             16  org.apache.cassandra.cql3.ColumnConditions$$Lambda$116/841977955
3012:             1             16  org.apache.cassandra.cql3.Constants$1
3013:             1             16  org.apache.cassandra.cql3.Constants$NullLiteral
3014:             1             16  org.apache.cassandra.cql3.Constants$UnsetLiteral
3015:             1             16  org.apache.cassandra.cql3.Cql_Parser$1
3016:             1             16  org.apache.cassandra.cql3.IfExistsCondition
3017:             1             16  org.apache.cassandra.cql3.IfNotExistsCondition
3018:             1             16  org.apache.cassandra.cql3.QueryOptions$Codec
3019:             1             16  org.apache.cassandra.cql3.QueryProcessor
3020:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$17/951221468
3021:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$18/1046545660
3022:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$19/1545827753
3023:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$20/1611832218
3024:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$21/2027317551
3025:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$22/273077527
3026:             1             16  org.apache.cassandra.cql3.QueryProcessor$MigrationSubscriber
3027:             1             16  org.apache.cassandra.cql3.ResultSet$Codec
3028:             1             16  org.apache.cassandra.cql3.ResultSet$ResultMetadata$Codec
3029:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$41/1614133563
3030:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$42/839771540
3031:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$43/1751403001
3032:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$44/1756819670
3033:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$45/178604517
3034:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$46/1543518287
3035:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$47/464872674
3036:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$48/1659286984
3037:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$49/1793899405
3038:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager
3039:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$1
3040:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$2
3041:             1             16  org.apache.cassandra.cql3.restrictions.RestrictionSet$1
3042:             1             16  org.apache.cassandra.cql3.selection.Selection$1
3043:             1             16  org.apache.cassandra.cql3.statements.CreateTableStatement$$Lambda$23/1470868839
3044:             1             16  org.apache.cassandra.db.CBuilder$1
3045:             1             16  org.apache.cassandra.db.Clustering$Serializer
3046:             1             16  org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer
3047:             1             16  org.apache.cassandra.db.ClusteringPrefix$Serializer
3048:             1             16  org.apache.cassandra.db.ColumnFamilyStore$$Lambda$190/1269783694
3049:             1             16  org.apache.cassandra.db.ColumnFamilyStore$2
3050:             1             16  org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily
3051:             1             16  org.apache.cassandra.db.Columns$$Lambda$205/2092785251
3052:             1             16  org.apache.cassandra.db.Columns$Serializer
3053:             1             16  org.apache.cassandra.db.CounterMutation$CounterMutationSerializer
3054:             1             16  org.apache.cassandra.db.CounterMutationVerbHandler
3055:             1             16  org.apache.cassandra.db.DataRange$Serializer
3056:             1             16  org.apache.cassandra.db.DecoratedKey$1
3057:             1             16  org.apache.cassandra.db.DefinitionsUpdateVerbHandler
3058:             1             16  org.apache.cassandra.db.DeletionPurger$$Lambda$105/2116697030
3059:             1             16  org.apache.cassandra.db.DeletionTime$Serializer
3060:             1             16  org.apache.cassandra.db.Directories$3
3061:             1             16  org.apache.cassandra.db.Directories$DataDirectory
3062:             1             16  org.apache.cassandra.db.EmptyIterators$EmptyPartitionIterator
3063:             1             16  org.apache.cassandra.db.HintedHandOffManager
3064:             1             16  org.apache.cassandra.db.Keyspace$1
3065:             1             16  org.apache.cassandra.db.MigrationRequestVerbHandler
3066:             1             16  org.apache.cassandra.db.Mutation$MutationSerializer
3067:             1             16  org.apache.cassandra.db.MutationVerbHandler
3068:             1             16  org.apache.cassandra.db.PartitionPosition$RowPositionSerializer
3069:             1             16  org.apache.cassandra.db.PartitionRangeReadCommand$Deserializer
3070:             1             16  org.apache.cassandra.db.ReadCommand$1
3071:             1             16  org.apache.cassandra.db.ReadCommand$1WithoutPurgeableTombstones$$Lambda$110/208106294
3072:             1             16  org.apache.cassandra.db.ReadCommand$2
3073:             1             16  org.apache.cassandra.db.ReadCommand$3
3074:             1             16  org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer
3075:             1             16  org.apache.cassandra.db.ReadCommand$LegacyRangeSliceCommandSerializer
3076:             1             16  org.apache.cassandra.db.ReadCommand$LegacyReadCommandSerializer
3077:             1             16  org.apache.cassandra.db.ReadCommand$Serializer
3078:             1             16  org.apache.cassandra.db.ReadCommandVerbHandler
3079:             1             16  org.apache.cassandra.db.ReadQuery$1
3080:             1             16  org.apache.cassandra.db.ReadRepairVerbHandler
3081:             1             16  org.apache.cassandra.db.ReadResponse$1
3082:             1             16  org.apache.cassandra.db.ReadResponse$LegacyRangeSliceReplySerializer
3083:             1             16  org.apache.cassandra.db.ReadResponse$Serializer
3084:             1             16  org.apache.cassandra.db.SchemaCheckVerbHandler
3085:             1             16  org.apache.cassandra.db.SerializationHeader$Serializer
3086:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer
3087:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Group$$Lambda$106/1952605049
3088:             1             16  org.apache.cassandra.db.SizeEstimatesRecorder
3089:             1             16  org.apache.cassandra.db.Slice$Serializer
3090:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices
3091:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices$1
3092:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices
3093:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices$1
3094:             1             16  org.apache.cassandra.db.Slices$Serializer
3095:             1             16  org.apache.cassandra.db.SnapshotCommandSerializer
3096:             1             16  org.apache.cassandra.db.StorageHook$1
3097:             1             16  org.apache.cassandra.db.SystemKeyspace$$Lambda$186/1473888912
3098:             1             16  org.apache.cassandra.db.TruncateResponse$TruncateResponseSerializer
3099:             1             16  org.apache.cassandra.db.TruncateVerbHandler
3100:             1             16  org.apache.cassandra.db.TruncationSerializer
3101:             1             16  org.apache.cassandra.db.WriteResponse
3102:             1             16  org.apache.cassandra.db.WriteResponse$Serializer
3103:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$1
3104:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$Serializer
3105:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$$Lambda$72/500233312
3106:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1
3107:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogService$1
3108:             1             16  org.apache.cassandra.db.commitlog.CommitLog$$Lambda$227/2024217158
3109:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$1
3110:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$CommitLogPositionSerializer
3111:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$$Lambda$228/1186545861
3112:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$MutationInitiator
3113:             1             16  org.apache.cassandra.db.commitlog.CommitLogSegment$$Lambda$175/1833918497
3114:             1             16  org.apache.cassandra.db.commitlog.IntervalSet$1
3115:             1             16  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool$1
3116:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$184/889018651
3117:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$185/638825183
3118:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$242/1509719872
3119:             1             16  org.apache.cassandra.db.compaction.CompactionManager$1
3120:             1             16  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController$$Lambda$307/363853319
3121:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$133/1728760599
3122:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$134/703363283
3123:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$172/1546684896
3124:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$85/654029265
3125:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$86/2030162789
3126:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$87/1306548322
3127:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$88/973942848
3128:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$89/558033602
3129:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$90/1361733480
3130:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$91/999951331
3131:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$92/1918201666
3132:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$93/1181004273
3133:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$95/1423931162
3134:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$96/1090942546
3135:             1             16  org.apache.cassandra.db.compaction.LeveledManifest$1
3136:             1             16  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy$1
3137:             1             16  org.apache.cassandra.db.context.CounterContext
3138:             1             16  org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer
3139:             1             16  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter$NamesDeserializer
3140:             1             16  org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer
3141:             1             16  org.apache.cassandra.db.filter.ColumnFilter$Serializer
3142:             1             16  org.apache.cassandra.db.filter.DataLimits$Serializer
3143:             1             16  org.apache.cassandra.db.filter.RowFilter$CQLFilter
3144:             1             16  org.apache.cassandra.db.filter.RowFilter$Serializer
3145:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$58/435914790
3146:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$59/1273958371
3147:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$64/731243659
3148:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$66/1037955032
3149:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$70/331596257
3150:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$165/1814072734
3151:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$203/2022031193
3152:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$204/1336053009
3153:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$140/1142908098
3154:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$141/423008343
3155:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$142/88843440
3156:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$177/1035048662
3157:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$162/1676168006
3158:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$166/1882192501
3159:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$168/700891016
3160:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$52/894421232
3161:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$54/276869158
3162:             1             16  org.apache.cassandra.db.lifecycle.Tracker$$Lambda$170/1786214274
3163:             1             16  org.apache.cassandra.db.marshal.CollectionType$CollectionPathSerializer
3164:             1             16  org.apache.cassandra.db.monitoring.ApproximateTime$$Lambda$108/2001863314
3165:             1             16  org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer
3166:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$$Lambda$107/2345640
3167:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer
3168:             1             16  org.apache.cassandra.db.rows.AbstractTypeVersionComparator
3169:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$118/474868079
3170:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$123/164389557
3171:             1             16  org.apache.cassandra.db.rows.Cell$$Lambda$101/1913147328
3172:             1             16  org.apache.cassandra.db.rows.Cell$Serializer
3173:             1             16  org.apache.cassandra.db.rows.ColumnData$$Lambda$28/494077446
3174:             1             16  org.apache.cassandra.db.rows.EncodingStats$Serializer
3175:             1             16  org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer
3176:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer
3177:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$194/5263871
3178:             1             16  org.apache.cassandra.db.view.View$$Lambda$219/1557380482
3179:             1             16  org.apache.cassandra.dht.BootStrapper$StringSerializer
3180:             1             16  org.apache.cassandra.dht.Murmur3Partitioner$2
3181:             1             16  org.apache.cassandra.dht.StreamStateStore
3182:             1             16  org.apache.cassandra.dht.Token$TokenSerializer
3183:             1             16  org.apache.cassandra.gms.EchoMessage
3184:             1             16  org.apache.cassandra.gms.EchoMessage$EchoMessageSerializer
3185:             1             16  org.apache.cassandra.gms.EndpointStateSerializer
3186:             1             16  org.apache.cassandra.gms.GossipDigestAck2Serializer
3187:             1             16  org.apache.cassandra.gms.GossipDigestAck2VerbHandler
3188:             1             16  org.apache.cassandra.gms.GossipDigestAckSerializer
3189:             1             16  org.apache.cassandra.gms.GossipDigestAckVerbHandler
3190:             1             16  org.apache.cassandra.gms.GossipDigestSerializer
3191:             1             16  org.apache.cassandra.gms.GossipDigestSynSerializer
3192:             1             16  org.apache.cassandra.gms.GossipDigestSynVerbHandler
3193:             1             16  org.apache.cassandra.gms.GossipShutdownVerbHandler
3194:             1             16  org.apache.cassandra.gms.Gossiper$1
3195:             1             16  org.apache.cassandra.gms.Gossiper$GossipTask
3196:             1             16  org.apache.cassandra.gms.HeartBeatStateSerializer
3197:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueFactory
3198:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueSerializer
3199:             1             16  org.apache.cassandra.hints.EncodedHintMessage$Serializer
3200:             1             16  org.apache.cassandra.hints.Hint$Serializer
3201:             1             16  org.apache.cassandra.hints.HintMessage$Serializer
3202:             1             16  org.apache.cassandra.hints.HintResponse
3203:             1             16  org.apache.cassandra.hints.HintResponse$Serializer
3204:             1             16  org.apache.cassandra.hints.HintVerbHandler
3205:             1             16  org.apache.cassandra.hints.HintsBuffer$$Lambda$327/1070755303
3206:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$244/955891688
3207:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$245/1579667951
3208:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$246/2099786968
3209:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$282/2033605821
3210:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$283/1986677941
3211:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$284/355640298
3212:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$250/1791992279
3213:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$251/1557383930
3214:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$252/763495689
3215:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$318/991892116
3216:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$322/1059094831
3217:             1             16  org.apache.cassandra.hints.HintsWriteExecutor$FsyncWritersTask$$Lambda$289/2053564305
3218:             1             16  org.apache.cassandra.index.Index$CollatedViewIndexBuildingSupport
3219:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$152/111521464
3220:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$153/118079547
3221:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$182/992085984
3222:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$188/887656608
3223:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$312/1070341018
3224:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$1
3225:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$2
3226:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$3
3227:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$4
3228:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$5
3229:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$6
3230:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$7
3231:             1             16  org.apache.cassandra.index.transactions.UpdateTransaction$1
3232:             1             16  org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer
3233:             1             16  org.apache.cassandra.io.compress.SnappyCompressor
3234:             1             16  org.apache.cassandra.io.sstable.Descriptor$$Lambda$71/999647352
3235:             1             16  org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer
3236:             1             16  org.apache.cassandra.io.sstable.IndexSummaryManager$1
3237:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$73/1687768728
3238:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$74/15478307
3239:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$75/1394837936
3240:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$1
3241:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$Equals
3242:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThan
3243:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThanOrEqualTo
3244:             1             16  org.apache.cassandra.io.sstable.format.SSTableReadsListener$1
3245:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$160/1520196427
3246:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$311/1357900831
3247:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat
3248:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory
3249:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory
3250:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$150/504911193
3251:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$151/451889382
3252:             1             16  org.apache.cassandra.io.sstable.metadata.CompactionMetadata$CompactionMetadataSerializer
3253:             1             16  org.apache.cassandra.io.sstable.metadata.StatsMetadata$StatsMetadataSerializer
3254:             1             16  org.apache.cassandra.io.sstable.metadata.ValidationMetadata$ValidationMetadataSerializer
3255:             1             16  org.apache.cassandra.io.util.DataOutputBuffer$1
3256:             1             16  org.apache.cassandra.io.util.DataOutputStreamPlus$1
3257:             1             16  org.apache.cassandra.io.util.FileHandle$$Lambda$158/795408782
3258:             1             16  org.apache.cassandra.io.util.MmappedRegions$State$$Lambda$197/1396226930
3259:             1             16  org.apache.cassandra.io.util.Rebufferer$1
3260:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$1
3261:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$2
3262:             1             16  org.apache.cassandra.locator.EndpointSnitchInfo
3263:             1             16  org.apache.cassandra.locator.PendingRangeMaps$1
3264:             1             16  org.apache.cassandra.locator.PendingRangeMaps$2
3265:             1             16  org.apache.cassandra.locator.PendingRangeMaps$3
3266:             1             16  org.apache.cassandra.locator.PendingRangeMaps$4
3267:             1             16  org.apache.cassandra.locator.PropertyFileSnitch
3268:             1             16  org.apache.cassandra.locator.PropertyFileSnitch$1
3269:             1             16  org.apache.cassandra.locator.SimpleSeedProvider
3270:             1             16  org.apache.cassandra.locator.TokenMetadata$1
3271:             1             16  org.apache.cassandra.metrics.BufferPoolMetrics$1
3272:             1             16  org.apache.cassandra.metrics.CQLMetrics$1
3273:             1             16  org.apache.cassandra.metrics.CQLMetrics$2
3274:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$82/1609657810
3275:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$83/2101898459
3276:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$84/342161168
3277:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$1
3278:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$2
3279:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$3
3280:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$4
3281:             1             16  org.apache.cassandra.metrics.ClientMetrics
3282:             1             16  org.apache.cassandra.metrics.CompactionMetrics$1
3283:             1             16  org.apache.cassandra.metrics.CompactionMetrics$2
3284:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$1
3285:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$2
3286:             1             16  org.apache.cassandra.metrics.TableMetrics$1
3287:             1             16  org.apache.cassandra.metrics.TableMetrics$13
3288:             1             16  org.apache.cassandra.metrics.TableMetrics$18
3289:             1             16  org.apache.cassandra.metrics.TableMetrics$20
3290:             1             16  org.apache.cassandra.metrics.TableMetrics$22
3291:             1             16  org.apache.cassandra.metrics.TableMetrics$26
3292:             1             16  org.apache.cassandra.metrics.TableMetrics$28
3293:             1             16  org.apache.cassandra.metrics.ViewWriteMetrics$1
3294:             1             16  org.apache.cassandra.net.IAsyncCallback$1
3295:             1             16  org.apache.cassandra.net.MessagingService$4
3296:             1             16  org.apache.cassandra.net.MessagingService$5
3297:             1             16  org.apache.cassandra.net.MessagingService$CallbackDeterminedSerializer
3298:             1             16  org.apache.cassandra.notifications.SSTableDeletingNotification
3299:             1             16  org.apache.cassandra.repair.NodePair$NodePairSerializer
3300:             1             16  org.apache.cassandra.repair.RepairJobDesc$RepairJobDescSerializer
3301:             1             16  org.apache.cassandra.repair.RepairMessageVerbHandler
3302:             1             16  org.apache.cassandra.repair.messages.AnticompactionRequest$AnticompactionRequestSerializer
3303:             1             16  org.apache.cassandra.repair.messages.CleanupMessage$CleanupMessageSerializer
3304:             1             16  org.apache.cassandra.repair.messages.PrepareMessage$PrepareMessageSerializer
3305:             1             16  org.apache.cassandra.repair.messages.RepairMessage$RepairMessageSerializer
3306:             1             16  org.apache.cassandra.repair.messages.SnapshotMessage$SnapshotMessageSerializer
3307:             1             16  org.apache.cassandra.repair.messages.SyncComplete$SyncCompleteSerializer
3308:             1             16  org.apache.cassandra.repair.messages.SyncRequest$SyncRequestSerializer
3309:             1             16  org.apache.cassandra.repair.messages.ValidationComplete$ValidationCompleteSerializer
3310:             1             16  org.apache.cassandra.repair.messages.ValidationRequest$ValidationRequestSerializer
3311:             1             16  org.apache.cassandra.scheduler.NoScheduler
3312:             1             16  org.apache.cassandra.schema.CQLTypeParser$$Lambda$207/2843617
3313:             1             16  org.apache.cassandra.schema.CompressionParams$Serializer
3314:             1             16  org.apache.cassandra.schema.Functions$$Lambda$236/1017996482
3315:             1             16  org.apache.cassandra.schema.Functions$$Lambda$237/2135117754
3316:             1             16  org.apache.cassandra.schema.Functions$$Lambda$239/854637578
3317:             1             16  org.apache.cassandra.schema.Functions$$Lambda$240/305461269
3318:             1             16  org.apache.cassandra.schema.Functions$Builder$$Lambda$36/146874094
3319:             1             16  org.apache.cassandra.schema.IndexMetadata$Serializer
3320:             1             16  org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$132/399524457
3321:             1             16  org.apache.cassandra.schema.SchemaKeyspace$$Lambda$216/2137640552
3322:             1             16  org.apache.cassandra.schema.Types$RawBuilder$$Lambda$206/1399449613
3323:             1             16  org.apache.cassandra.schema.Types$RawBuilder$RawUDT$$Lambda$210/2069170964
3324:             1             16  org.apache.cassandra.schema.Views$$Lambda$50/1348115836
3325:             1             16  org.apache.cassandra.serializers.BooleanSerializer
3326:             1             16  org.apache.cassandra.serializers.ByteSerializer
3327:             1             16  org.apache.cassandra.serializers.BytesSerializer
3328:             1             16  org.apache.cassandra.serializers.DecimalSerializer
3329:             1             16  org.apache.cassandra.serializers.DoubleSerializer
3330:             1             16  org.apache.cassandra.serializers.InetAddressSerializer
3331:             1             16  org.apache.cassandra.serializers.Int32Serializer
3332:             1             16  org.apache.cassandra.serializers.LongSerializer
3333:             1             16  org.apache.cassandra.serializers.TimeUUIDSerializer
3334:             1             16  org.apache.cassandra.serializers.TimestampSerializer
3335:             1             16  org.apache.cassandra.serializers.TimestampSerializer$1
3336:             1             16  org.apache.cassandra.serializers.TimestampSerializer$2
3337:             1             16  org.apache.cassandra.serializers.TimestampSerializer$3
3338:             1             16  org.apache.cassandra.serializers.UTF8Serializer
3339:             1             16  org.apache.cassandra.serializers.UUIDSerializer
3340:             1             16  org.apache.cassandra.service.CacheService$CounterCacheSerializer
3341:             1             16  org.apache.cassandra.service.CacheService$KeyCacheSerializer
3342:             1             16  org.apache.cassandra.service.CacheService$RowCacheSerializer
3343:             1             16  org.apache.cassandra.service.CassandraDaemon$$Lambda$273/1244026033
3344:             1             16  org.apache.cassandra.service.CassandraDaemon$1
3345:             1             16  org.apache.cassandra.service.CassandraDaemon$2
3346:             1             16  org.apache.cassandra.service.CassandraDaemon$NativeAccess
3347:             1             16  org.apache.cassandra.service.ClientState$$Lambda$97/466481125
3348:             1             16  org.apache.cassandra.service.ClientWarn
3349:             1             16  org.apache.cassandra.service.DefaultFSErrorHandler
3350:             1             16  org.apache.cassandra.service.EchoVerbHandler
3351:             1             16  org.apache.cassandra.service.LoadBroadcaster
3352:             1             16  org.apache.cassandra.service.LoadBroadcaster$1
3353:             1             16  org.apache.cassandra.service.MigrationManager
3354:             1             16  org.apache.cassandra.service.MigrationManager$MigrationsSerializer
3355:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$277/794251840
3356:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$279/1246696592
3357:             1             16  org.apache.cassandra.service.PendingRangeCalculatorService$1
3358:             1             16  org.apache.cassandra.service.SnapshotVerbHandler
3359:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$1/1204167249
3360:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$114/1819989346
3361:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$2/1615780336
3362:             1             16  org.apache.cassandra.service.StartupChecks$1
3363:             1             16  org.apache.cassandra.service.StartupChecks$10
3364:             1             16  org.apache.cassandra.service.StartupChecks$11
3365:             1             16  org.apache.cassandra.service.StartupChecks$12
3366:             1             16  org.apache.cassandra.service.StartupChecks$2
3367:             1             16  org.apache.cassandra.service.StartupChecks$3
3368:             1             16  org.apache.cassandra.service.StartupChecks$4
3369:             1             16  org.apache.cassandra.service.StartupChecks$5
3370:             1             16  org.apache.cassandra.service.StartupChecks$6
3371:             1             16  org.apache.cassandra.service.StartupChecks$7
3372:             1             16  org.apache.cassandra.service.StartupChecks$9
3373:             1             16  org.apache.cassandra.service.StorageProxy
3374:             1             16  org.apache.cassandra.service.StorageProxy$1
3375:             1             16  org.apache.cassandra.service.StorageProxy$2
3376:             1             16  org.apache.cassandra.service.StorageProxy$3
3377:             1             16  org.apache.cassandra.service.StorageProxy$4
3378:             1             16  org.apache.cassandra.service.StorageService$$Lambda$259/1361973748
3379:             1             16  org.apache.cassandra.service.StorageService$1
3380:             1             16  org.apache.cassandra.service.paxos.Commit$CommitSerializer
3381:             1             16  org.apache.cassandra.service.paxos.CommitVerbHandler
3382:             1             16  org.apache.cassandra.service.paxos.PrepareResponse$PrepareResponseSerializer
3383:             1             16  org.apache.cassandra.service.paxos.PrepareVerbHandler
3384:             1             16  org.apache.cassandra.service.paxos.ProposeVerbHandler
3385:             1             16  org.apache.cassandra.streaming.ReplicationFinishedVerbHandler
3386:             1             16  org.apache.cassandra.streaming.StreamHook$1
3387:             1             16  org.apache.cassandra.streaming.StreamRequest$StreamRequestSerializer
3388:             1             16  org.apache.cassandra.streaming.StreamSummary$StreamSummarySerializer
3389:             1             16  org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer
3390:             1             16  org.apache.cassandra.streaming.messages.CompleteMessage$1
3391:             1             16  org.apache.cassandra.streaming.messages.FileMessageHeader$FileMessageHeaderSerializer
3392:             1             16  org.apache.cassandra.streaming.messages.IncomingFileMessage$1
3393:             1             16  org.apache.cassandra.streaming.messages.KeepAliveMessage$1
3394:             1             16  org.apache.cassandra.streaming.messages.OutgoingFileMessage$1
3395:             1             16  org.apache.cassandra.streaming.messages.PrepareMessage$1
3396:             1             16  org.apache.cassandra.streaming.messages.ReceivedMessage$1
3397:             1             16  org.apache.cassandra.streaming.messages.RetryMessage$1
3398:             1             16  org.apache.cassandra.streaming.messages.SessionFailedMessage$1
3399:             1             16  org.apache.cassandra.streaming.messages.StreamInitMessage$StreamInitMessageSerializer
3400:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$add
3401:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$atomic_batch_mutate
3402:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate
3403:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$cas
3404:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_cluster_name
3405:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace
3406:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspaces
3407:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_local_ring
3408:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_partitioner
3409:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_ring
3410:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_schema_versions
3411:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_snitch
3412:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits
3413:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits_ex
3414:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_token_map
3415:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_version
3416:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query
3417:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query
3418:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query
3419:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql_query
3420:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get
3421:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_count
3422:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices
3423:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_multi_slice
3424:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice
3425:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices
3426:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_slice
3427:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$insert
3428:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$login
3429:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_count
3430:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_slice
3431:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql3_query
3432:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query
3433:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove
3434:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove_counter
3435:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_cql_version
3436:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_keyspace
3437:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family
3438:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace
3439:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family
3440:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace
3441:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family
3442:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_keyspace
3443:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$trace_next_query
3444:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$truncate
3445:             1             16  org.apache.cassandra.thrift.CassandraServer
3446:             1             16  org.apache.cassandra.thrift.CassandraServer$1
3447:             1             16  org.apache.cassandra.transport.CBUtil$1
3448:             1             16  org.apache.cassandra.transport.Message$ExceptionHandler
3449:             1             16  org.apache.cassandra.transport.Server$1
3450:             1             16  org.apache.cassandra.transport.messages.AuthChallenge$1
3451:             1             16  org.apache.cassandra.transport.messages.AuthResponse$1
3452:             1             16  org.apache.cassandra.transport.messages.AuthSuccess$1
3453:             1             16  org.apache.cassandra.transport.messages.AuthenticateMessage$1
3454:             1             16  org.apache.cassandra.transport.messages.BatchMessage$1
3455:             1             16  org.apache.cassandra.transport.messages.CredentialsMessage$1
3456:             1             16  org.apache.cassandra.transport.messages.ErrorMessage$1
3457:             1             16  org.apache.cassandra.transport.messages.EventMessage$1
3458:             1             16  org.apache.cassandra.transport.messages.ExecuteMessage$1
3459:             1             16  org.apache.cassandra.transport.messages.OptionsMessage$1
3460:             1             16  org.apache.cassandra.transport.messages.PrepareMessage$1
3461:             1             16  org.apache.cassandra.transport.messages.QueryMessage$1
3462:             1             16  org.apache.cassandra.transport.messages.ReadyMessage$1
3463:             1             16  org.apache.cassandra.transport.messages.RegisterMessage$1
3464:             1             16  org.apache.cassandra.transport.messages.ResultMessage$1
3465:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Prepared$1
3466:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Rows$1
3467:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SchemaChange$1
3468:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SetKeyspace$1
3469:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Void$1
3470:             1             16  org.apache.cassandra.transport.messages.StartupMessage$1
3471:             1             16  org.apache.cassandra.transport.messages.SupportedMessage$1
3472:             1             16  org.apache.cassandra.utils.AlwaysPresentFilter
3473:             1             16  org.apache.cassandra.utils.AsymmetricOrdering$Reversed
3474:             1             16  org.apache.cassandra.utils.BloomFilter$1
3475:             1             16  org.apache.cassandra.utils.BooleanSerializer
3476:             1             16  org.apache.cassandra.utils.Clock
3477:             1             16  org.apache.cassandra.utils.CoalescingStrategies$1
3478:             1             16  org.apache.cassandra.utils.CoalescingStrategies$2
3479:             1             16  org.apache.cassandra.utils.EstimatedHistogram$EstimatedHistogramSerializer
3480:             1             16  org.apache.cassandra.utils.FBUtilities$1
3481:             1             16  org.apache.cassandra.utils.FastByteOperations$UnsafeOperations
3482:             1             16  org.apache.cassandra.utils.Interval$1
3483:             1             16  org.apache.cassandra.utils.Interval$2
3484:             1             16  org.apache.cassandra.utils.JMXServerUtils$Exporter
3485:             1             16  org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper
3486:             1             16  org.apache.cassandra.utils.JVMStabilityInspector$Killer
3487:             1             16  org.apache.cassandra.utils.MerkleTree$Hashable$HashableSerializer
3488:             1             16  org.apache.cassandra.utils.MerkleTree$Inner$InnerSerializer
3489:             1             16  org.apache.cassandra.utils.MerkleTree$Leaf$LeafSerializer
3490:             1             16  org.apache.cassandra.utils.MerkleTree$MerkleTreeSerializer
3491:             1             16  org.apache.cassandra.utils.MerkleTrees$MerkleTreesSerializer
3492:             1             16  org.apache.cassandra.utils.NanoTimeToCurrentTimeMillis$$Lambda$255/703776031
3493:             1             16  org.apache.cassandra.utils.NativeLibraryLinux
3494:             1             16  org.apache.cassandra.utils.NoSpamLogger$1
3495:             1             16  org.apache.cassandra.utils.StreamingHistogram$$Lambda$76/244613162
3496:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$136/1321552491
3497:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$137/732447846
3498:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramSerializer
3499:             1             16  org.apache.cassandra.utils.SystemTimeSource
3500:             1             16  org.apache.cassandra.utils.UUIDGen
3501:             1             16  org.apache.cassandra.utils.UUIDSerializer
3502:             1             16  org.apache.cassandra.utils.btree.BTree$$Lambda$193/1448037571
3503:             1             16  org.apache.cassandra.utils.btree.UpdateFunction$$Lambda$29/24650043
3504:             1             16  org.apache.cassandra.utils.concurrent.Ref$ReferenceReaper
3505:             1             16  org.apache.cassandra.utils.memory.BufferPool$1
3506:             1             16  org.apache.cassandra.utils.memory.BufferPool$2
3507:             1             16  org.apache.cassandra.utils.memory.HeapAllocator
3508:             1             16  org.apache.cassandra.utils.vint.VIntCoding$1
3509:             1             16  org.apache.thrift.TProcessorFactory
3510:             1             16  org.apache.thrift.transport.TFramedTransport$Factory
3511:             1             16  org.cliffc.high_scale_lib.NonBlockingHashMap$Prime
3512:             1             16  org.cliffc.high_scale_lib.NonBlockingHashSet
3513:             1             16  org.codehaus.jackson.map.deser.std.AtomicBooleanDeserializer
3514:             1             16  org.codehaus.jackson.map.deser.std.ClassDeserializer
3515:             1             16  org.codehaus.jackson.map.deser.std.DateDeserializer
3516:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$CurrencyDeserializer
3517:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$InetAddressDeserializer
3518:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$LocaleDeserializer
3519:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$PatternDeserializer
3520:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$TimeZoneDeserializer
3521:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URIDeserializer
3522:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URLDeserializer
3523:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$UUIDDeserializer
3524:             1             16  org.codehaus.jackson.map.deser.std.JavaTypeDeserializer
3525:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers
3526:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$BooleanDeser
3527:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ByteDeser
3528:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$CharDeser
3529:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$DoubleDeser
3530:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$FloatDeser
3531:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$IntDeser
3532:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$LongDeser
3533:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ShortDeser
3534:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$StringDeser
3535:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigDecimalDeserializer
3536:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigIntegerDeserializer
3537:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$NumberDeserializer
3538:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$SqlDateDeserializer
3539:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$StackTraceElementDeserializer
3540:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$BoolKD
3541:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$ByteKD
3542:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$CharKD
3543:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$DoubleKD
3544:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$FloatKD
3545:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$IntKD
3546:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$LongKD
3547:             1             16  org.codehaus.jackson.map.deser.std.StringDeserializer
3548:             1             16  org.codehaus.jackson.map.deser.std.TimestampDeserializer
3549:             1             16  org.codehaus.jackson.map.deser.std.TokenBufferDeserializer
3550:             1             16  org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer
3551:             1             16  org.codehaus.jackson.map.ext.OptionalHandlerFactory
3552:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector
3553:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$GetterMethodFilter
3554:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$MinimalMethodFilter
3555:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterAndGetterMethodFilter
3556:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterMethodFilter
3557:             1             16  org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector
3558:             1             16  org.codehaus.jackson.map.ser.StdSerializers$DoubleSerializer
3559:             1             16  org.codehaus.jackson.map.ser.StdSerializers$FloatSerializer
3560:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntLikeSerializer
3561:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntegerSerializer
3562:             1             16  org.codehaus.jackson.map.ser.StdSerializers$LongSerializer
3563:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlDateSerializer
3564:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlTimeSerializer
3565:             1             16  org.codehaus.jackson.map.ser.impl.UnknownSerializer
3566:             1             16  org.codehaus.jackson.map.ser.std.CalendarSerializer
3567:             1             16  org.codehaus.jackson.map.ser.std.DateSerializer
3568:             1             16  org.codehaus.jackson.map.ser.std.NullSerializer
3569:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$ByteArraySerializer
3570:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$CharArraySerializer
3571:             1             16  org.codehaus.jackson.map.ser.std.StringSerializer
3572:             1             16  org.codehaus.jackson.map.ser.std.ToStringSerializer
3573:             1             16  org.codehaus.jackson.map.type.TypeParser
3574:             1             16  org.codehaus.jackson.node.JsonNodeFactory
3575:             1             16  org.github.jamm.MemoryLayoutSpecification$2
3576:             1             16  org.github.jamm.MemoryMeter$1
3577:             1             16  org.github.jamm.NoopMemoryMeterListener
3578:             1             16  org.github.jamm.NoopMemoryMeterListener$1
3579:             1             16  org.slf4j.helpers.NOPLoggerFactory
3580:             1             16  org.slf4j.helpers.SubstituteLoggerFactory
3581:             1             16  org.slf4j.impl.StaticMDCBinder
3582:             1             16  org.xerial.snappy.SnappyNative
3583:             1             16  org.yaml.snakeyaml.constructor.SafeConstructor$ConstructUndefined
3584:             1             16  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.UnicodeEscaper$2
3585:             1             16  sun.management.ClassLoadingImpl
3586:             1             16  sun.management.HotSpotDiagnostic
3587:             1             16  sun.management.ManagementFactoryHelper$PlatformLoggingImpl
3588:             1             16  sun.misc.ASCIICaseInsensitiveComparator
3589:             1             16  sun.misc.FloatingDecimal$1
3590:             1             16  sun.misc.FormattedFloatingDecimal$1
3591:             1             16  sun.misc.Launcher
3592:             1             16  sun.misc.Launcher$Factory
3593:             1             16  sun.misc.ObjectInputFilter$Config$$Lambda$294/1344368391
3594:             1             16  sun.misc.Perf
3595:             1             16  sun.misc.Unsafe
3596:             1             16  sun.net.DefaultProgressMeteringPolicy
3597:             1             16  sun.net.ExtendedOptionsImpl$$Lambda$253/1943122657
3598:             1             16  sun.net.www.protocol.file.Handler
3599:             1             16  sun.net.www.protocol.jar.JarFileFactory
3600:             1             16  sun.nio.ch.EPollSelectorProvider
3601:             1             16  sun.nio.ch.ExtendedSocketOption$1
3602:             1             16  sun.nio.ch.FileChannelImpl$1
3603:             1             16  sun.nio.ch.Net$1
3604:             1             16  sun.nio.ch.Util$1
3605:             1             16  sun.nio.fs.LinuxFileSystemProvider
3606:             1             16  sun.reflect.GeneratedConstructorAccessor12
3607:             1             16  sun.reflect.GeneratedConstructorAccessor18
3608:             1             16  sun.reflect.GeneratedMethodAccessor10
3609:             1             16  sun.reflect.GeneratedMethodAccessor11
3610:             1             16  sun.reflect.GeneratedMethodAccessor12
3611:             1             16  sun.reflect.GeneratedMethodAccessor13
3612:             1             16  sun.reflect.GeneratedMethodAccessor14
3613:             1             16  sun.reflect.GeneratedMethodAccessor15
3614:             1             16  sun.reflect.GeneratedMethodAccessor6
3615:             1             16  sun.reflect.GeneratedMethodAccessor7
3616:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor36
3617:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor37
3618:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor38
3619:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor39
3620:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor40
3621:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor41
3622:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor42
3623:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor43
3624:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor44
3625:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor45
3626:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor46
3627:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor47
3628:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor49
3629:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor50
3630:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor51
3631:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor52
3632:             1             16  sun.reflect.ReflectionFactory
3633:             1             16  sun.reflect.generics.tree.BooleanSignature
3634:             1             16  sun.reflect.generics.tree.BottomSignature
3635:             1             16  sun.reflect.generics.tree.VoidDescriptor
3636:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$8/817299424
3637:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$9/2031951755
3638:             1             16  sun.rmi.registry.RegistryImpl_Skel
3639:             1             16  sun.rmi.registry.RegistryImpl_Stub
3640:             1             16  sun.rmi.runtime.Log$LoggerLogFactory
3641:             1             16  sun.rmi.runtime.RuntimeUtil
3642:             1             16  sun.rmi.server.LoaderHandler$2
3643:             1             16  sun.rmi.server.UnicastServerRef$HashToMethod_Maps
3644:             1             16  sun.rmi.transport.DGCImpl$$Lambda$6/516537656
3645:             1             16  sun.rmi.transport.DGCImpl$2$$Lambda$7/1023268896
3646:             1             16  sun.rmi.transport.DGCImpl_Skel
3647:             1             16  sun.rmi.transport.DGCImpl_Stub
3648:             1             16  sun.rmi.transport.Target$$Lambda$339/2000963151
3649:             1             16  sun.rmi.transport.proxy.RMIDirectSocketFactory
3650:             1             16  sun.rmi.transport.tcp.TCPTransport$1
3651:             1             16  sun.security.rsa.RSAKeyFactory
3652:             1             16  sun.security.ssl.EphemeralKeyManager
3653:             1             16  sun.security.util.ByteArrayLexOrder
3654:             1             16  sun.security.util.ByteArrayTagOrder
3655:             1             16  sun.text.normalizer.NormalizerBase$Mode
3656:             1             16  sun.text.normalizer.NormalizerBase$NFCMode
3657:             1             16  sun.text.normalizer.NormalizerBase$NFDMode
3658:             1             16  sun.text.normalizer.NormalizerBase$NFKCMode
3659:             1             16  sun.text.normalizer.NormalizerBase$NFKDMode
3660:             1             16  sun.util.calendar.Gregorian
3661:             1             16  sun.util.locale.provider.AuxLocaleProviderAdapter$NullProvider
3662:             1             16  sun.util.locale.provider.CalendarDataUtility$CalendarWeekParameterGetter
3663:             1             16  sun.util.locale.provider.SPILocaleProviderAdapter
3664:             1             16  sun.util.resources.LocaleData
3665:             1             16  sun.util.resources.LocaleData$LocaleDataResourceBundleControl
Total     119374210     4034601936
{code}

"
CASSANDRA-14092,Max ttl of 20 years will overflow localDeletionTime,"CASSANDRA-4771 added a max value of 20 years for ttl to protect against [year 2038 overflow bug|https://en.wikipedia.org/wiki/Year_2038_problem] for {{localDeletionTime}}.

It turns out that next year the {{localDeletionTime}} will start overflowing with the maximum ttl of 20 years ({{System.currentTimeMillis() + ttl(20 years) > Integer.MAX_VALUE}}), so we should remove this limitation."
CASSANDRA-14087,NPE when CAS encounters empty frozen collection,"When a compare-and-set operation specifying an equality criterion with a non-{{null}} value encounters an empty collection ({{null}} cell), the server throws a {{NullPointerException}} and the query fails.

This does not happen for non-frozen collections.

There's a self-contained test case at [github|https://github.com/incub8/cassandra-npe-in-cas].

The stack trace for 3.11.0 is:

{code}
ERROR [Native-Transport-Requests-1] 2017-11-27 12:59:26,924 QueryMessage.java:129 - Unexpected error during query
java.lang.NullPointerException: null
        at org.apache.cassandra.cql3.ColumnCondition$CollectionBound.appliesTo(ColumnCondition.java:546) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest$ColumnsConditions.appliesTo(CQL3CasRequest.java:324) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest.appliesTo(CQL3CasRequest.java:210) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.service.StorageProxy.cas(StorageProxy.java:265) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithCondition(ModificationStatement.java:441) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:416) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
{code}
"
CASSANDRA-14052,test_low_cardinality_indexes almost always deadlocks nosetests,test_low_cardinality_indexes currently almost always causes nosetests to deadlock on test cleanup after it runs. This causes us to have to hit timeouts and the test never actually completes successfully. We should disable this test from running until we root cause it and make it a useful test. I've yet to get this test to pass once successfully.
CASSANDRA-14048,test_ttl_deletions - paging_test.TestPagingWithDeletions fails: [Unavailable exception] ,"test_ttl_deletions - paging_test.TestPagingWithDeletions fails: [Unavailable exception] 

Error from server: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={'required_replicas': 2, 'alive_replicas': 1, 'consistency': 'ALL'}
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-lCOixs
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.9 dc3> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.7 dc3> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.4 dc2> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.9:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2452, in cassandra.cluster.Session.add_or_renew_pool.run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""cassandra/pool.py"", line 332, in cassandra.pool.HostConnection.__init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.9', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.4:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2452, in cassandra.cluster.Session.add_or_renew_pool.run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""cassandra/pool.py"", line 332, in cassandra.pool.HostConnection.__init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.4', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Host 127.0.0.9 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.4 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.9, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.9', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.4', 9042)]. Last error: Connection refused
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #0
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #3
dtest: DEBUG: Retrying request after UE. Attempt #4
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #1
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
dtest: DEBUG: Retrying request after UE. Attempt #2
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/paging_test.py"", line 3425, in test_ttl_deletions
    data = self.setup_data()
  File ""/home/cassandra/cassandra-dtest/paging_test.py"", line 3191, in setup_data
    'col3': int
  File ""/home/cassandra/cassandra-dtest/tools/datahelp.py"", line 158, in create_rows
    query_results = execute_concurrent_with_args(session, prepared, [d.values() for d in dicts])
  File ""cassandra/concurrent.py"", line 236, in cassandra.concurrent.execute_concurrent_with_args
    return execute_concurrent(session, zip(cycle((statement,)), parameters), *args, **kwargs)
  File ""cassandra/concurrent.py"", line 92, in cassandra.concurrent.execute_concurrent
    return executor.execute(concurrency, raise_on_first_error)
  File ""cassandra/concurrent.py"", line 199, in cassandra.concurrent.ConcurrentExecutorListResults.execute
    return super(ConcurrentExecutorListResults, self).execute(concurrency, fail_fast)
  File ""cassandra/concurrent.py"", line 118, in cassandra.concurrent._ConcurrentExecutor.execute
    return self._results()
  File ""cassandra/concurrent.py"", line 213, in cassandra.concurrent.ConcurrentExecutorListResults._results
    with self._condition:
  File ""cassandra/concurrent.py"", line 217, in cassandra.concurrent.ConcurrentExecutorListResults._results
    self._raise(self._exception)
  File ""cassandra/concurrent.py"", line 165, in cassandra.concurrent._ConcurrentExecutor._raise
    raise exc
'Error from server: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={\'required_replicas\': 2, \'alive_replicas\': 1, \'consistency\': \'ALL\'}\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-lCOixs\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.9 dc3> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.7 dc3> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.4 dc2> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.9:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2452, in cassandra.cluster.Session.add_or_renew_pool.run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""cassandra/pool.py"", line 332, in cassandra.pool.HostConnection.__init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.9\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.4:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2452, in cassandra.cluster.Session.add_or_renew_pool.run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""cassandra/pool.py"", line 332, in cassandra.pool.HostConnection.__init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.4\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Host 127.0.0.9 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.4 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.9, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.9\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.4\', 9042)]. Last error: Connection refused\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #0\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #3\ndtest: DEBUG: Retrying request after UE. Attempt #4\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #1\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\ndtest: DEBUG: Retrying request after UE. Attempt #2\n--------------------- >> end captured logging << ---------------------'"
CASSANDRA-14044,test_paging_with_filtering_on_partition_key - paging_test.TestPagingData fails,"test_paging_with_filtering_on_partition_key - paging_test.TestPagingData fails

Inet address 127.0.0.3:9042 is not available: [Errno 98] Address already in use; a cluster may already be running or you may need to add the loopback alias
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-c8abwC
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/cassandra/cassandra-dtest/paging_test.py"", line 2154, in test_paging_with_filtering_on_partition_key
    session = self.prepare(row_factory=tuple_factory)
  File ""/home/cassandra/cassandra-dtest/paging_test.py"", line 26, in prepare
    cluster.populate(3).start(wait_for_binary_proto=True)
  File ""/home/cassandra/env/local/lib/python2.7/site-packages/ccmlib/cluster.py"", line 389, in start
    common.assert_socket_available(itf)
  File ""/home/cassandra/env/local/lib/python2.7/site-packages/ccmlib/common.py"", line 519, in assert_socket_available
    raise UnavailableSocketError(""Inet address %s:%s is not available: %s; a cluster may already be running or you may need to add the loopback alias"" % (addr, port, msg))
""Inet address 127.0.0.3:9042 is not available: [Errno 98] Address already in use; a cluster may already be running or you may need to add the loopback alias\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-c8abwC\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\n--------------------- >> end captured logging << ---------------------"""
CASSANDRA-14018,All dtests in bootstrap_test that call _base_bootstrap_test will cause nose to deadlock,"Any of the dtests that eventually call BaseBootstrapTest#_base_bootstrap_test will hang forever

this includes:
bootstrap_test: simple_bootstrap_test_with_ssl
bootstrap_test: simple_bootstrap_test
bootstrap_test: bootstrap_on_write_survey_test

I've added debug logging after the tests finish their last assert and that reliably is logged.. debugging with gdb has been futile so far. The hang is on a futext inside nose during test teardown. I believe it might have something to do with the threads we spin up for the cleanup (compaction cleanup) that prevents the test cleanup from finishing because we hang waiting for the threads to exit and they never do.

Until we get these tests fixed we should disable them as they will always fail and cause dtest runs to be very long and painful to run.

simple_bootstrap_test_with_ssl-novnodes (bootstrap_test.TestBootstrap) ... cluster ccm directory: /tmp/dtest-ors2iY
Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
***using internode ssl***
generating keystore.jks in [/tmp/dtest-ors2iY]
exporting cert from keystore.jks in [/tmp/dtest-ors2iY]
Certificate stored in file </tmp/dtest-ors2iY/ccm_node.cer>
importing cert into truststore.jks in [/tmp/dtest-ors2iY]
Certificate was added to keystore
[node1, node2] tokens: [-9223372036854775808, 0]
node1 empty size : 130.73
node1 size before bootstrapping node2: 747.61
node1 size after cleanup: 473.43
node1 size after compacting: 447.91
node2 size after compacting: 434.44"
CASSANDRA-13999,Segfault during memtable flush,"We are getting segfaults on a production Cassandra cluster, apparently caused by Memtable flushes to disk.
{code}
Current thread (0x000000000cd77920):  JavaThread ""PerDiskMemtableFlushWriter_0:140"" daemon [_thread_in_Java, id=28952, stack(0x00007f8b7aa53000,0x00007f8b7aa94000)]
{code}

Stack
{code}
Stack: [0x00007f8b7aa53000,0x00007f8b7aa94000],  sp=0x00007f8b7aa924a0,  free space=253k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 21889 C2 org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(Lorg/apache/cassandra/db/rows/UnfilteredRowIterator;)Lorg/apache/cassandra/db/RowIndexEntry; (361 bytes) @ 0x00007f8e9fcf75ac [0x00007f8e9fcf42c0+0x32ec]
J 22464 C2 org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents()V (383 bytes) @ 0x00007f8e9f17b988 [0x00007f8e9f17b5c0+0x3c8]
j  org.apache.cassandra.db.Memtable$FlushRunnable.call()Lorg/apache/cassandra/io/sstable/SSTableMultiWriter;+1
j  org.apache.cassandra.db.Memtable$FlushRunnable.call()Ljava/lang/Object;+1
J 18865 C2 java.util.concurrent.FutureTask.run()V (126 bytes) @ 0x00007f8e9d3c9540 [0x00007f8e9d3c93a0+0x1a0]
J 21832 C2 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x00007f8e9f16856c [0x00007f8e9f168400+0x16c]
J 6720 C1 java.util.concurrent.ThreadPoolExecutor$Worker.run()V (9 bytes) @ 0x00007f8e9def73c4 [0x00007f8e9def72c0+0x104]
J 22079 C2 java.lang.Thread.run()V (17 bytes) @ 0x00007f8e9e67c4ac [0x00007f8e9e67c460+0x4c]
v  ~StubRoutines::call_stub
V  [libjvm.so+0x691d16]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x1056
V  [libjvm.so+0x692221]  JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)+0x321
V  [libjvm.so+0x6926c7]  JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Thread*)+0x47
V  [libjvm.so+0x72da50]  thread_entry(JavaThread*, Thread*)+0xa0
V  [libjvm.so+0xa76833]  JavaThread::thread_main_inner()+0x103
V  [libjvm.so+0xa7697c]  JavaThread::run()+0x11c
V  [libjvm.so+0x927568]  java_start(Thread*)+0x108
C  [libpthread.so.0+0x7de5]  start_thread+0xc5
{code}

For further details, we attached:
* JVM error file with all details
* cassandra config file (we are using offheap_buffers as memtable_allocation_method)
* some lines printed in debug.log when the JVM error file was created and process died

h5. Reproducing the issue
So far we have been unable to reproduce it. It happens once/twice a week on single nodes. It happens either during high load or low load times. We have seen that when we replace EC2 instances and bootstrap new ones, due to compactions happening on source nodes before stream starts, sometimes more than a single node was affected by this, letting us with 2 out of 3 replicas out and UnavailableExceptions in the cluster.

This issue might have relation with CASSANDRA-12590 (Segfault reading secondary index) even this is the write path. Can someone confirm if both issues could be related? 

h5. Specifics of our scenario:
* Cassandra 3.9 on Amazon Linux (previous to this, we were running Cassandra 2.0.9 and there are no records of this also happening, even I was not working on Cassandra)
* 12 x i3.2xlarge EC2 instances (8 core, 64GB RAM)
* a total of 176 keyspaces (there is a per-customer pattern)
** Some keyspaces have a single table, while others have 2 or 5 tables
** There is a table that uses standard Secondary Indexes (""emailindex"" on ""user_info"" table)
* It happens on both Oracle JDK 1.8.0_112 and 1.8.0_131
* It happens in both kernel 4.9.43-17.38.amzn1.x86_64 and 3.14.35-28.38.amzn1.x86_64


h5. Possible workarounds/solutions that we have in mind (to be validated yet)
* switching to heap_buffers (in case offheap_buffers triggers the bug), even we are still pending to measure performance degradation under that scenario.
* removing secondary indexes in favour of Materialized Views for this specific case, even we are concerned too about the fact that using MVs introduces new issues that may be present in our current Cassandra 3.9
* Upgrading to 3.11.1 is an option, but we are trying to keep it as last resort given that the cost of migrating is big and we don't have any guarantee that new bugs that affects nodes availability are not introduced."
CASSANDRA-13981,Enable Cassandra for Persistent Memory ,"Currently, Cassandra relies on disks for data storage and hence it needs data serialization, compaction, bloom filters and partition summary/index for speedy access of the data. However, with persistent memory, data can be stored directly in the form of Java objects and collections, which can greatly simplify the retrieval mechanism of the data. What we are proposing is to make use of faster and scalable B+ tree-based data collections built for persistent memory in Java (PCJ: https://github.com/pmem/pcj) and enable a complete in-memory version of Cassandra, while still keeping the data persistent."
CASSANDRA-13980,Compaction deadlock,"While upgrading the cluster from 2.1.16 from 3.11.2, after a few hours most of the upgraded nodes started to go in a compaction infinite loop and showing many events like the one below (always for the same SSTable):

{code:java}
INFO  [CompactionExecutor:4] 2017-10-29 00:28:31,480 LeveledManifest.java:474 - Adding high-level (L5) BigTableReader(path='/var/lib/cassandra/data/datadisk4/blobstore/block-1d63273065b911e49cd7ef0972cffde6/blobstore-block-ka-201694-Data.db') to candidates
{code}

Since the log get spammed at a huge rate, I'm unable to get any previous events.

Tried restarts and sstablescrub -m without success. The only workaround that seems to work (so far) was sstablelevelreset.

I've attached the dump.

"
CASSANDRA-13929,BTree$Builder / io.netty.util.Recycler$Stack leaking memory,"Different to CASSANDRA-13754, there seems to be another memory leak in 3.11.0+ in BTree$Builder / io.netty.util.Recycler$Stack.

* heap utilization increase after upgrading to 3.11.0 => cassandra_3.11.0_min_memory_utilization.jpg
* No difference after upgrading to 3.11.1 (snapshot build) => cassandra_3.11.1_snapshot_heaputilization.png; thus most likely after fixing CASSANDRA-13754, more visible now
* MAT shows io.netty.util.Recycler$Stack as top contributing class => cassandra_3.11.1_mat_dominator_classes.png
* With -Xmx8G (CMS) and our load pattern, we have to do a rolling restart after ~ 72 hours

Verified the following fix, namely explicitly unreferencing the _recycleHandle_ member (making it non-final). In _org.apache.cassandra.utils.btree.BTree.Builder.recycle()_
{code}
        public void recycle()
        {
            if (recycleHandle != null)
            {
                this.cleanup();
                builderRecycler.recycle(this, recycleHandle);
                recycleHandle = null; // ADDED
            }
        }
{code}

Patched a single node in our loadtest cluster with this change and after ~ 10 hours uptime, no sign of the previously offending class in MAT anymore => cassandra_3.11.1_mat_dominator_classes_FIXED.png

Can' say if this has any other side effects etc., but I doubt."
CASSANDRA-13923,Flushers blocked due to many SSTables,"This started on the mailing list and I'm not 100% sure of the root cause, feel free to re-title if needed.

I just upgraded Cassandra from 2.2.6 to 3.11.0. Within a few hours of serving traffic, thread pools begin to back up and grow pending tasks indefinitely. This happens to multiple different stages (Read, Mutation) and consistently builds pending tasks for MemtablePostFlush and MemtableFlushWriter.

Using jstack shows that there is blocking going on when trying to call getCompactionCandidates, which seems to happen on flush. We have fairly large nodes that have ~15,000 SSTables per node, all LCS.

I seems like this can cause reads to get blocked because they try to acquire a read lock when calling shouldDefragment.

And writes, of course, block once we can't allocate anymore memtables, because flushes are backed up.

We did not have this problem in 2.2.6, so it seems like there is some regression causing it to be incredibly slow trying to do calls like getCompactionCandidates that list out the SSTables.

In our case this causes nodes to build up pending tasks and simply stop responding to requests."
CASSANDRA-13902,document memtable_flush_period_in_ms,
CASSANDRA-13897,"nodetool compact and flush fail with ""error: null""","{{nodetool flush}} and {{nodetool compact}} return an error message that is not clear. This could probably be improved. Both of my two nodes return this error.

{{nodetool flush}} Will return this error the first 2-3 times you invoke it, then the error temporarily disappears. {{nodetool compress}} always returns this error message no matter how many times you invoke it.

I have tried deleting saved_caches, commit logs, doing nodetool compact/rebuild/scrub, and nothing seems to remove the error. 

{noformat}
cass@s5:~/apache-cassandra-3.11.0$ nodetool compact
error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.<init>(ChunkCache.java:222)
	at org.apache.cassandra.cache.ChunkCache.wrap(ChunkCache.java:175)
	at org.apache.cassandra.io.util.FileHandle$Builder.maybeCached(FileHandle.java:412)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:381)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:331)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinal(BigTableWriter.java:333)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinalEarly(BigTableWriter.java:318)
	at org.apache.cassandra.io.sstable.SSTableRewriter.switchWriter(SSTableRewriter.java:322)
	at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:370)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:111)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:121)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:733)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:748)

{noformat}
"
CASSANDRA-13869,AbstractTokenTreeBuilder#serializedSize returns wrong value when there is a single leaf and overflow collisions,In the extremely rare case where a small token tree (< 248 values) has overflow collisions the size returned by AbstractTokenTreeBuilder#serializedSize is incorrect because it fails to account for the overflow collisions. 
CASSANDRA-13866,Clock-dependent integer overflow in tests CellTest and RowsTest,"These tests create timestamps from Unix time, but this is done as int math with the result stored in a long. This means that if the test is run at certain times, like 1505177731, corresponding to Tuesday, September 12, 2017, 12:55:31, the test can have two timestamps separated by a single second that reverse their ordering when multiplied by 1000000, such as 1505177731 -> 2147149504 and 1505177732 -> -2146817792. This causes a variety of test failures, since it changes the reconciliation order of these cells.

Note that I've tagged this as trivial because the problem is in the manual construction of timestamps in the test; I know of nowhere  that we make this mistake with real data."
CASSANDRA-13852,Race condition when closing stream sessions (4.0),"bootstrap_test:TestBootstrap.manual_bootstrap_test is hanging due to a race condition that can occur when waiting for a streaming COMPLETE message and the remote side has already closed the connection. We do not check to see if we still have remaining bytes buffered for consumption in {{RebufferringByteBufDataInputPlus}}, but always go ahead and throw an {{EOFException}}. We should consume the bytes, then throw an {{EOFException}} on subsequent calls."
CASSANDRA-13849,GossipStage blocks because of race in ActiveRepairService,"Bad luck caused a kernel panic in a cluster, and that took another node with it because GossipStage stopped responding.

I think it's pretty obvious what's happening, here are the relevant excerpts from the stack traces :

{noformat}
""Thread-24004"" #393781 daemon prio=5 os_prio=0 tid=0x00007efca9647400 nid=0xe75c waiting on condition [0x00007efaa47fe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x000000052b63a7e8> (a java.util.concurrent.CountDownLatch$Sync)
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
    at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
    at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:332)
    - locked <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:211)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)                                                                                                           at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipTasks:1"" #367 daemon prio=5 os_prio=0 tid=0x00007efc5e971000 nid=0x700b waiting for monitor entry [0x00007dfb839fe000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:306)
    at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:775)                                                                                                                at org.apache.cassandra.gms.Gossiper.access$800(Gossiper.java:67)
    at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:187)
    at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipStage:1"" #320 daemon prio=5 os_prio=0 tid=0x00007efc5b9f2c00 nid=0x6fcd waiting for monitor entry [0x00007e260186a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)                                                                                          at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.service.ActiveRepairService.onRestart(ActiveRepairService.java:744)
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1049)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1143)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)                                                                                       at java.lang.Thread.run(Thread.java:748)
{noformat}

iow, org.apache.cassandra.service.ActiveRepairService.prepareForRepair holds a lock until the repair is prepared, which means waiting for other nodes to respond, which may die at exactly that moment, so they won't complete. Gossip will at the same time try to mark the node as down, but it requires that same lock :)"
CASSANDRA-13840,GraphiteReporter stops reporting from Memory concurrent access,"Gauges in the GraphiteReporter can, when getting compression offheap size, trip over the assertion in {{Memory.size()}}. As the GraphiteReporter only handles Exceptions this crashes the reporter and it sends no more metrics.

[~benedict] described it [here|https://issues.apache.org/jira/browse/CASSANDRA-9625?focusedCommentId=14711914&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14711914] and [~jbellis] confirmed that it's now ok to remove the assertion.

{noformat}
java.lang.AssertionError: null
        at org.apache.cassandra.io.util.Memory.size(Memory.java:359) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.io.compress.CompressionMetadata.offHeapSize(CompressionMetadata.java:167) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.io.sstable.SSTableReader.getCompressionMetadataOffHeapSize(SSTableReader.java:1315) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.metrics.ColumnFamilyMetrics$30.value(ColumnFamilyMetrics.java:573) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.metrics.ColumnFamilyMetrics$30.value(ColumnFamilyMetrics.java:568) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at com.yammer.metrics.reporting.GraphiteReporter.processGauge(GraphiteReporter.java:309) ~[metrics-graphite-2.2.0.jar:na]
        at com.yammer.metrics.reporting.GraphiteReporter.processGauge(GraphiteReporter.java:26) ~[metrics-graphite-2.2.0.jar:na]
        at com.yammer.metrics.core.Gauge.processWith(Gauge.java:28) ~[metrics-core-2.2.0.jar:na]
        at com.yammer.metrics.reporting.GraphiteReporter.printRegularMetrics(GraphiteReporter.java:251) ~[metrics-graphite-2.2.0.jar:na]
        at com.yammer.metrics.reporting.GraphiteReporter.run(GraphiteReporter.java:216) ~[metrics-graphite-2.2.0.jar:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_101]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_101]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{noformat}"
CASSANDRA-13808,Integer overflows with Amazon Elastic File System (EFS),"Integer overflow issue was fixed for cassandra 2.2, but in 3.8 new property was introduced in config that also derives from disk size  {{cdc_total_space_in_mb}}, see CASSANDRA-8844

It should be updated too https://github.com/apache/cassandra/blob/6b7d73a49695c0ceb78bc7a003ace606a806c13a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L484"
CASSANDRA-13804,dtest failure: paging_test.TestPagingWithModifiers.test_with_allow_filtering,http://cassci.datastax.com/job/cassandra-3.11_dtest/151/testReport/paging_test/TestPagingWithModifiers/test_with_allow_filtering
CASSANDRA-13789,Reduce memory copies and object creations when acting on  ByteBufs,"There are multiple ""low-hanging-fruits"" when it comes to reduce memory copies and object allocations when acting on ByteBufs."
CASSANDRA-13768,auth_test.py dtest fails intermittently due to thread race condition,"*Root cause:* dtest {{auth_test.TestAuth.system_auth_ks_is_alterable_test}} is failing intermittently and it is due to race condition between Python driver continuously sending cql query (by thread t1) to know status of cluster, and {{ccm}} is stopping the nodes (by thread t2)

*Failure details:* http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/lastCompletedBuild/testReport/auth_test/TestAuth/system_auth_ks_is_alterable_test/
*Solution:* Ignore {{UnavailableException}} for this particular test as it is not going to harm test case in anyway.
Please find snippet of C* log when test fails:
{{INFO [InternalResponseStage:8] 2017-08-15 10:44:13,213 RepairRunnable.java:337 - Repair command #2 finished in 0 seconds INFO [GossipStage:1] 2017-08-15 10:44:13,601 Gossiper.java:1019 - InetAddress /127.0.0.1 is now DOWN INFO [HANDSHAKE-/127.0.0.1] 2017-08-15 10:44:13,715 OutboundTcpConnection.java:564 - Handshaking version with /127.0.0.1 INFO [SharedPool-Worker-2] 2017-08-15 10:44:16,712 Message.java:512 - Received: OPTIONS, v=4 INFO [SharedPool-Worker-2] 2017-08-15 10:44:16,713 Message.java:512 - Received: STARTUP {CQL_VERSION=3.4.0, COMPRESSION=snappy}, v=4 INFO [SharedPool-Worker-2] 2017-08-15 10:44:16,719 Message.java:512 - Received: org.apache.cassandra.transport.messages.AuthResponse@4474c719, v=4 INFO [GossipStage:1] 2017-08-15 10:44:16,723 Gossiper.java:1019 - InetAddress /127.0.0.3 is now DOWN ERROR [SharedPool-Worker-2] 2017-08-15 10:44:16,822 Message.java:621 - Unexpected exception during request; channel = [id: 0xffc2fc9a, L:/127.0.0.2:9042 - R:/127.0.0.2:51369] java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:516) ~[main/:na] at org.apache.cassandra.auth.CassandraRoleManager.canLogin(CassandraRoleManager.java:310) ~[main/:na] at org.apache.cassandra.service.ClientState.login(ClientState.java:270) ~[main/:na] at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:79) ~[main/:na] at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [main/:na] at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [main/:na] at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final] at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final] at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_144] at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [main/:na] at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_144] Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[main/:na] at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[main/:na] at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1733) ~[main/:na] at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1696) ~[main/:na] at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1640) ~[main/:na] at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1544) ~[main/:na] at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:965) ~[main/:na] at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:263) ~[main/:na] at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:224) ~[main/:na] at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:524) ~[main/:na] at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:506) ~[main/:na] ... 13 common frames omitted INFO [HANDSHAKE-/127.0.0.3] 2017-08-15 10:44:17,678 OutboundTcpConnection.java:564 - Handshaking version with /127.0.0.3 INFO [SharedPool-Worker-1] 2017-08-15 10:44:17,927 Message.java:512 - Received: OPTIONS, v=4 INFO [SharedPool-Worker-1] 2017-08-15 10:44:17,928 Message.java:512 - Received: STARTUP {CQL_VERSION=3.4.0, COMPRESSION=snappy}, v=4 INFO [SharedPool-Worker-2] 2017-08-15 10:44:17,932 Message.java:512 - Received: org.apache.cassandra.transport.messages.AuthResponse@3cc2a95c, v=4 INFO [StorageServiceShutdownHook] 2017-08-15 10:44:19,814 HintsService.java:212 - Paused hints dispatch INFO [StorageServiceShutdownHook] 2017-08-15 10:44:19,818 Server.java:180 - Stop listening for CQL clients INFO [StorageServiceShutdownHook] 2017-08-15 10:44:19,818 Gossiper.java:1490 - Announcing shutdown INFO [StorageServiceShutdownHook] 2017-08-15 10:44:19,819 StorageService.java:2011 - Node /127.0.0.2 state jump to shutdown}}"
CASSANDRA-13754,BTree.Builder memory leak,"After a chronic bout of {{OutOfMemoryError}} in our development environment, a heap analysis is showing that more than 10G of our 12G heaps are consumed by the {{threadLocals}} members (instances of {{java.lang.ThreadLocalMap}}) of various {{io.netty.util.concurrent.FastThreadLocalThread}} instances.  Reverting [cecbe17|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=cecbe17e3eafc052acc13950494f7dddf026aa54] fixes the issue."
CASSANDRA-13751,Race / ref leak in PendingRepairManager,"PendingRepairManager#getScanners has an assertion that confirms an sstable is, in fact, marked as pending repair. Since validation compactions don't use the same concurrency controls as proper compactions, they can race with promotion/demotion compactions and end up getting assertion errors when the pending repair id is changed while the scanners are being acquired. Also, error handling in PendingRepairManager and CompactionStrategyManager leaks refs when this happens."
CASSANDRA-13743,CAPTURE not easilly usable with PAGING,See https://github.com/iksaif/cassandra/commit/7ed56966a7150ced44c375af307685517d7e09a3 for a patch fixing that.
CASSANDRA-13715,Allow TRACE logging on upgrade dtests,
CASSANDRA-13688,Anticompaction race can leak sstables/txn,"At the top of {{CompactionManager#performAntiCompaction}}, the parent repair session is loaded, if the session can't be found, a RuntimeException is thrown. This can happen if a participant is evicted after the IR prepare message is received, but before the anticompaction starts. This exception is thrown outside of the try/finally block that guards the sstable and lifecycle transaction, causing them to leak, and preventing the sstables from ever being removed from View.compacting."
CASSANDRA-13659,PendingRepairManager race can cause NPE during validation,"{{getScanners}} assumes that a compaction strategy exists for the given repair session, which may not always be the case"
CASSANDRA-13652,Deadlock in AbstractCommitLogSegmentManager,"AbstractCommitLogManager uses LockSupport.(un)park incorreclty. It invokes unpark without checking if manager thread was parked in approriate place. 
For example, logging frameworks uses queues and queues uses ReadWriteLock's that uses LockSupport. Therefore AbstractCommitLogManager.wakeManager can wake thread inside Lock and manager thread will sleep forever at park() method (because unpark permit was already consumed inside lock).

For examle stack traces:
{code}
""MigrationStage:1"" id=412 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.awaitAvailableSegment(AbstractCommitLogSegmentManager.java:263)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.advanceAllocatingFrom(AbstractCommitLogSegmentManager.java:237)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.forceRecycleAll(AbstractCommitLogSegmentManager.java:279)
    at org.apache.cassandra.db.commitlog.CommitLog.forceRecycleAllSegments(CommitLog.java:210)
    at org.apache.cassandra.config.Schema.dropView(Schema.java:708)
    at org.apache.cassandra.schema.SchemaKeyspace.lambda$updateKeyspace$23(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$382/1123232162.accept(Unknown Source)
    at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608)
    at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
    at org.apache.cassandra.schema.SchemaKeyspace.updateKeyspace(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1332)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1282)
      - locked java.lang.Class@cc38904
    at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:51)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper.run(DebuggableThreadPoolExecutor.java:322)
    at com.ringcentral.concurrent.executors.MonitoredRunnable.run(MonitoredRunnable.java:36)
    at MON_R_MigrationStage.run(NamedRunnableFactory.java:67)
    at com.ringcentral.concurrent.executors.MonitoredThreadPoolExecutor$MdcAwareRunnable.run(MonitoredThreadPoolExecutor.java:114)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)

""COMMIT-LOG-ALLOCATOR:1"" id=80 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1.runMayThrow(AbstractCommitLogSegmentManager.java:128)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)
{code}

Solution is to use Semaphore instead of low-level LockSupport."
CASSANDRA-13635,update dtests to support netty-based internode messaging/streaming,some dtests need to be updated to work correctly with CASSANDRA-13628
CASSANDRA-13631,add header parameter size to internode messaging protocol,"as netty is not a streaming/blocking protocol and works best when buffer sizes are known, I had to do a bunch of contortions in {{MessageInHandler}} (as part of CASSANDRA-8457) to safely read the message header parameters. If we add a header parameters size field to the internode messaging protocol, the header parsing code would be dramatically simpler (note: we'll still need the existing parsing to support the cluster upgrade use case). An alternative to adding a new field is to hijack the existing header parameter count field; that field is an {{int}}, so field width is not an issue."
CASSANDRA-13619,java.nio.BufferOverflowException: null while flushing hints,"I'm seeing the following exception running Cassandra 3.0.11 on 21 node cluster in two AWS regions when half of the nodes in one region go down, and the load is high on the rest of the nodes:

{code}
WARN  [SharedPool-Worker-10] 2017-06-14 12:57:15,017 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-10,5,main]: {}
java.lang.RuntimeException: java.nio.BufferOverflowException
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2549) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0-zing_17.03.1.0]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.11.jar:3.0.11]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0-zing_17.03.1.0]
Caused by: java.nio.BufferOverflowException: null
        at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:195) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.writeUnsignedVInt(BufferedDataOutputStreamPlus.java:258) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithVIntLength(ByteBufferUtil.java:296) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Columns$Serializer.serialize(Columns.java:405) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.SerializationHeader$Serializer.serializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:120) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.serialize(PartitionUpdate.java:625) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Mutation$MutationSerializer.serialize(Mutation.java:305) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.Hint$Serializer.serialize(Hint.java:141) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:251) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:230) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBufferPool.write(HintsBufferPool.java:61) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsService.write(HintsService.java:154) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$11.runMayThrow(StorageProxy.java:2627) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2545) ~[apache-cassandra-3.0.11.jar:3.0.11]
        ... 5 common frames omitted
{code}

Relevant configurations from cassandra.yaml:

{code}
-cassandra_hinted_handoff_throttle_in_kb: 1024
 cassandra_max_hints_delivery_threads: 4
-cassandra_hints_flush_period_in_ms: 10000
-cassandra_max_hints_file_size_in_mb: 512
{code}

When I reduce -cassandra_hints_flush_period_in_ms: 10000 to 5000, the number of exceptions lowers significantly, but they are still present."
CASSANDRA-13592,Null Pointer exception at SELECT JSON statement,"A Nulll pointer exception appears when the command

{code}
SELECT JSON * FROM examples.basic;

---MORE---
<Error from server: code=0000 [Server error] message=""java.lang.NullPointerException"">

Examples.basic has the following description (DESC examples.basic;):
CREATE TABLE examples.basic (
    key frozen<tuple<uuid, int>> PRIMARY KEY,
    wert text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

The error appears after the ---MORE--- line.

The field ""wert"" has a JSON formatted string."
CASSANDRA-13587,Deadlock during CommitLog replay when Cassandra restarts,"Possible deadlock found when Cassandra is replaying commit log and at the same time Mutation gets triggered by SSTableReader(SystemKeyspace.persistSSTableReadMeter). As a result Cassandra restart hangs forever

Please find details of stack trace here:

*Frame#1* This thread is trying to apply {{persistSSTableReadMeter}} mutation and as a result it has called {{writeOrder.start()}} in {{Keyspace.java:533}}
but there are no Commitlog Segments available because {{createReserveSegments (CommitLogSegmentManager.java)}} is not yet {{true}} 

Hence this thread is blocked on {{createReserveSegments}} to become {{true}}, please note this thread has already started {{writeOrder}}

{quote}
""pool-11-thread-1"" #251 prio=5 os_prio=0 tid=0x00007fe128478400 nid=0x1b274 waiting on condition [0x00007fe1389a0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.advanceAllocatingFrom(CommitLogSegmentManager.java:277)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.allocate(CommitLogSegmentManager.java:196)
        at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:260)
        at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:540)
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:421)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:210)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:224)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:566)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:556)
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:295)
        at org.apache.cassandra.db.SystemKeyspace.persistSSTableReadMeter(SystemKeyspace.java:1181)
        at org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1.run(SSTableReader.java:2202)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Frame#2* This thread is trying to recover commit logs and as a result it tries to flush Memtable by calling following code:
{{futures.add(Keyspace.open(SystemKeyspace.NAME).getColumnFamilyStore(SystemKeyspace.BATCHES).forceFlush());}}
As a result Frame#3 (below) gets created

{quote}
""main"" #1 prio=5 os_prio=0 tid=0x00007fe1c64ec400 nid=0x1af29 waiting on condition [0x00007fe1c94a1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
parking to wait for  <0x00000006370da0c0> (a com.google.common.util.concurrent.ListenableFutureTask)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
        at java.util.concurrent.FutureTask.get(FutureTask.java:191)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:383)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:207)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:182)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:161)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:295)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{quote}

*Frame#3* This thread is waiting at {{writeBarrier.await();}} in {{ColumnFamilyStore.java:1027}} 
but {{writeBarrier}} is locked by thread in Frame#1, and Frame#1 thread is waiting for more CommitlogSegements to be available. 
Frame#1 thread will not get new segment because variable {{createReserveSegments(CommitLogSegmentManager.java)}} is not yet {{true}}. 
This variable gets set to {{true}} after successful execution of Frame#2.

Here we can see Frame#3 and Frame#1 are in deadlock state and Cassandra restart hangs forever.
 
{quote}
""MemtableFlushWriter:5"" #433 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0400 nid=0x1dea8 waiting on condition [0x00007e753c2ca000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.utils.concurrent.OpOrder$Barrier.await(OpOrder.java:419)
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1027)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)


""MemtablePostFlush:3"" #432 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0000 nid=0x1dea7 waiting on condition [0x00007e753c30b000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
 parking to wait for  <0x00000006370d9cd0> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:941)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:924)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Reproducible steps*: Reproducing this problem is tricky as it involves multiple conditions to happen at the same time and is timing bases, so I have done some small code change to reproduce this:
1. Create a Keyspace and table
2. Inject data until there are few SSTables generated and CommitLog available
3. Kill Cassandra process
4. Use the custom code (in the attached file ""Reproduce_CASSANDRA-13587.txt"") on top of 3.0.14 branch 
5. Build Cassandra jar and use this custom jar
6. Restart Cassandra
    Here you will see Cassandra is hanging forever
7. Now apply this fix on top of ""Reproduce_CASSANDRA-13587.txt"", and repeat step-6
    Here you should see Cassandra is starting normally

*Solution*: I am proposing that we should enable variable {{createReserveSegments(CommitLogSegmentManager.java)}} before recovering any CommitLogs in CommitLog.java file
so this will not block Frame#1 from acquiring new segment as a result Frame#1 will finish and then Frame#2 will also finish.
Please note, this variable {{createReserveSegments}} has been removed from the trunk branch as part of (https://issues.apache.org/jira/browse/CASSANDRA-10202), also in the trunk branch CommitLog segments gets created when needed. So as per my understanding enabling this variable before CommitLog recovery should not create any other side effect, please let me know your comments.
"
CASSANDRA-13585,NPE in IR cleanup when columnfamily has no sstables,"On {{PendingAntiCompaction::abort()}} , we try to release refs to sstables and a lifecycle transaction that can be null if there are no sstables in the table.
"
CASSANDRA-13555,Thread leak during repair,"The symptom is similar to what happened in [CASSANDRA-13204 | https://issues.apache.org/jira/browse/CASSANDRA-13204] that the thread waiting forever doing nothing. This one happened during ""nodetool repair -pr -seq -j 1"" in production but I can easily simulate the problem with just ""nodetool repair"" in dev environment (CCM). I'm trying to explain what happened with 3.0.13 code base.

1. One node is down while doing repair. This is the error I saw in production:

{code}
ERROR [GossipTasks:1] 2017-05-19 15:00:10,545 RepairSession.java:334 - [repair #bc9a3cd1-3ca3-11e7-a44a-e30923ac9336] session completed with the following error
java.io.IOException: Endpoint /10.185.43.15 died
    at org.apache.cassandra.repair.RepairSession.convict(RepairSession.java:333) ~[apache-cassandra-3.0.11.jar:3.0.11]
    at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:306) [apache-cassandra-3.0.11.jar:3.0.11]
    at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:766) [apache-cassandra-3.0.11.jar:3.0.11]
    at org.apache.cassandra.gms.Gossiper.access$800(Gossiper.java:66) [apache-cassandra-3.0.11.jar:3.0.11]
    at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:181) [apache-cassandra-3.0.11.jar:3.0.11]
    at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118) [apache-cassandra-3.0.11.jar:3.0.11]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_121]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_121]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_121]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [apache-cassandra-3.0.11.jar:3.0.11]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{code}

2. At this moment the repair coordinator hasn't received the response (MerkleTrees) for the node that was marked down. This means, RepairJob#run will never return because it waits for validations to finish:

{code}
        // Wait for validation to complete
        Futures.getUnchecked(validations);
{code}

Be noted that all RepairJob's (as Runnable) run on a shared executor created in RepairRunnable#runMayThrow, while all snapshot, validation and sync'ing happen on a per-RepairSession ""taskExecutor"". The RepairJob#run will only return when it receives MerkleTrees (or null) from all endpoints for a given column family and token range.

As evidence of the thread leak, below is from the thread dump. I can also get the same stack trace when simulating the same issue in dev environment.

{code}
""Repair#129:56"" #406373 daemon prio=5 os_prio=0 tid=0x00007fc495028400 nid=0x1a77d waiting on condition [0x00007fc021530000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00000002d7c00198> (a com.google.common.util.concurrent.AbstractFuture$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
    at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:285)
    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
    at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:137)
    at com.google.common.util.concurrent.Futures.getUnchecked(Futures.java:1509)
    at org.apache.cassandra.repair.RepairJob.run(RepairJob.java:160)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/725832346.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
    - <0x00000002d7c00230> (a java.util.concurrent.ThreadPoolExecutor$Worker)
{code}

So here are two things:
1. For the thread leak itself, either we do something like below in RepairSession#terminate, or we use timed wait at the end of RepairJob#run.
{code}
        for (ValidationTask validationTask : validating.values()) {
            validationTask.treesReceived(null);
        }
        validating.clear();
{code}

2. Another question is, instead of waiting for synchronization (SyncTask) to finish, why we just wait for validation? Is it because we want to speed things up and anyway we have throttling on streaming?

[~yukim] I'd love to get your comment. I'll check if this issue exists in other versions."
CASSANDRA-13533,ColumnIdentifier object size wrong when tables are not flushed,"It turns out that the object size of {{ColumnIdentifier}} is wrong when *cassandra.test.flush_local_schema_changes: false*. This looks like stuff is being wrongly reused when no flush is happening.

We only noticed this because we were using the prepared stmt cache and noticed that prepared statements would account for *1-6mb* when *cassandra.test.flush_local_schema_changes: false*. With *cassandra.test.flush_local_schema_changes: true* (which is the default) those would be around *5000 bytes*.

Attached is a test that reproduces the problem and also a fix.

Also after talking to [~jkni] / [~blerer] we shouldn't probably take {{ColumnDefinition}} into account when measuring object sizes with {{MemoryMeter}}
"
CASSANDRA-13507,dtest failure in paging_test.TestPagingWithDeletions.test_ttl_deletions ,"{noformat}
Failed 7 times in the last 30 runs. Flakiness: 34%, Stability: 76%
Error Message

4 != 8
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-z1xodw
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
{noformat}
Most output omitted. It's attached."
CASSANDRA-13482,NPE on non-existing row read when row cache is enabled,"The problem is reproducible on 3.0 with:

{code}
-# row_cache_class_name: org.apache.cassandra.cache.OHCProvider
+row_cache_class_name: org.apache.cassandra.cache.OHCProvider

-row_cache_size_in_mb: 0
+row_cache_size_in_mb: 100
{code}

Table setup:

{code}
CREATE TABLE cache_tables (pk int, v1 int, v2 int, v3 int, primary key (pk, v1)) WITH CACHING = { 'keys': 'ALL', 'rows_per_partition': '1' } ;
{code}

No data is required, only a head query (or any pk/ck query but with full partitions cached). 

{code}
select * from cross_page_queries where pk = 10000 ;
{code}

{code}
java.lang.AssertionError: null
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.concat(UnfilteredRowIterators.java:193) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.getThroughCache(SinglePartitionReadCommand.java:461) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:358) ~[main/:na]
        at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:395) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1794) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2472) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}"
CASSANDRA-13432,MemtableReclaimMemory can get stuck because of lack of timeout in getTopLevelColumns(),"This might affect 3.x too, I'm not sure.

{code}
$ nodetool tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
MutationStage                     0         0       32135875         0                 0
ReadStage                       114         0       29492940         0                 0
RequestResponseStage              0         0       86090931         0                 0
ReadRepairStage                   0         0         166645         0                 0
CounterMutationStage              0         0              0         0                 0
MiscStage                         0         0              0         0                 0
HintedHandoff                     0         0             47         0                 0
GossipStage                       0         0         188769         0                 0
CacheCleanupExecutor              0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
CompactionExecutor                0         0          86835         0                 0
ValidationExecutor                0         0              0         0                 0
MigrationStage                    0         0              0         0                 0                                    
AntiEntropyStage                  0         0              0         0                 0                                    
PendingRangeCalculator            0         0             92         0                 0                                    
Sampler                           0         0              0         0                 0                                    
MemtableFlushWriter               0         0            563         0                 0                                    
MemtablePostFlush                 0         0           1500         0                 0                                    
MemtableReclaimMemory             1        29            534         0                 0                                    
Native-Transport-Requests        41         0       54819182         0              1896                            
{code}

{code}
""MemtableReclaimMemory:195"" - Thread t@6268
   java.lang.Thread.State: WAITING
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
	at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:283)
	at org.apache.cassandra.utils.concurrent.OpOrder$Barrier.await(OpOrder.java:417)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush$1.runMayThrow(ColumnFamilyStore.java:1151)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- locked <6e7b1160> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""SharedPool-Worker-195"" - Thread t@989
   java.lang.Thread.State: RUNNABLE
	at org.apache.cassandra.db.RangeTombstoneList.addInternal(RangeTombstoneList.java:690)
	at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:650)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:171)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:143)
	at org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:240)
	at org.apache.cassandra.db.ArrayBackedSortedColumns.delete(ArrayBackedSortedColumns.java:483)
	at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:153)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:184)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:156)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:146)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:125)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:263)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:108)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:82)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:69)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:316)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:2015)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1858)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:353)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:85)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- None

""SharedPool-Worker-206"" - Thread t@1014
   java.lang.Thread.State: RUNNABLE
	at org.apache.cassandra.db.RangeTombstoneList.addInternal(RangeTombstoneList.java:690)
	at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:650)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:171)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:143)
	at org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:240)
	at org.apache.cassandra.db.ArrayBackedSortedColumns.delete(ArrayBackedSortedColumns.java:483)
	at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:153)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:184)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:156)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:146)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:89)
	at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:48)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:105)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:82)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:69)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:316)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:2015)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1858)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:353)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:85)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- None
{code}

As you can see MemtableReclaimMemory is waiting on the read barrier to be released, but there are two queries currently being executed which are locking this.

Since most of the time is spent pretty low in the stack, these read operations will never timeout (they are reading rows with tons of tombstones).

We also can easily detect or purge the offending line because there is no easy way to find out which partition is currently being read.

The TombstoneFailureThreshold should also protect us, but it is probably being checked too high in the call stack.

Looks like RangeTombstoneList or DeletionInfo should also check for DatabaseDescriptor.getTombstoneFailureThreshold()"
CASSANDRA-13416,Multiple test failures with NPE in org.apache.cassandra.streaming.StreamSession,"example failures:

http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.dht/StreamStateStoreTest/testUpdateAndQueryAvailableRanges
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.dht/StreamStateStoreTest/testUpdateAndQueryAvailableRanges_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/LocalSyncTaskTest/testDifference
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/LocalSyncTaskTest/testDifference_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/incrementalStreamPlan
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/fullStreamPlan
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/incrementalStreamPlan_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/fullStreamPlan_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testScheduleTimeout
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testFailSessionDuringTransferShouldNotReleaseReferences
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testScheduleTimeout_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testFailSessionDuringTransferShouldNotReleaseReferences_compression

one example stack trace:
{code}
Stacktrace

java.lang.NullPointerException
	at org.apache.cassandra.streaming.StreamSession.isKeepAliveSupported(StreamSession.java:244)
	at org.apache.cassandra.streaming.StreamSession.<init>(StreamSession.java:196)
	at org.apache.cassandra.streaming.StreamCoordinator$HostStreamingData.getOrCreateNextSession(StreamCoordinator.java:293)
	at org.apache.cassandra.streaming.StreamCoordinator.getOrCreateNextSession(StreamCoordinator.java:158)
	at org.apache.cassandra.streaming.StreamPlan.requestRanges(StreamPlan.java:94)
	at org.apache.cassandra.repair.LocalSyncTask.startSync(LocalSyncTask.java:85)
	at org.apache.cassandra.repair.SyncTask.run(SyncTask.java:75)
	at org.apache.cassandra.repair.LocalSyncTaskTest.testDifference(LocalSyncTaskTest.java:117)
{code}"
CASSANDRA-13389,Possible NPE on upgrade to 3.0/3.X in case of IO errors,"There is a NPE on upgrade to 3.0/3.X if a data directory contains directories that generate IO errors, for example if the cassandra process does not have permission to read them.

Here is the exception:

{code}
ERROR [main] 2017-03-06 16:41:30,678  CassandraDaemon.java:710 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.io.util.FileUtils.delete(FileUtils.java:372) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.db.SystemKeyspace.migrateDataDirs(SystemKeyspace.java:1359) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:190) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
{code}

This is caused by {{File.listFiles()}}, which returns null in case of an IO error."
CASSANDRA-13362,Cassandra 2.1.15 main thread stuck in logback stack trace upon joining existing cluster,"Switching from Cassandra 2.0.17 to Cassandra 2.1.15 (DSC edition: dsc-cassandra-2.1.15-bin.tar.gz) in a local VM based Linux environment for installer verification tests.
{noformat}
[root@localhost jdk1.8.0_102]# lsb_release -d
Description:	CentOS release 6.7 (Final)
You have new mail in /var/spool/mail/root
[root@localhost jdk1.8.0_102]# uname -a
Linux localhost 2.6.32-573.el6.x86_64 #1 SMP Thu Jul 23 15:44:03 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
{noformat}

The test environment is started from scratch, thus in the following scenario not an upgrade from 2.0 to 2.1, but a fresh 2.1 installation.

The first node started up fine, but when extending the cluster with a second node, the second node hangs in the following Cassandra log output while starting up, joining the existing node:
{noformat}
INFO  [InternalResponseStage:1] 2017-03-21 21:10:43,864 DefsTables.java:373 - Loading org.apache.cassandra.config.CFMetaData@1c3daf27[cfId=a8cb1eb0-0e61-11e7-9a56-b999920ca863,ksName=ruxitdb,cfName=EventQueue,cf$
INFO  [main] 2017-03-21 21:11:11,404 StorageService.java:1138 - JOINING: schema complete, ready to bootstrap
...
INFO  [main] 2017-03-22 03:13:36,148 StorageService.java:1138 - JOINING: waiting for pending range calculation
INFO  [main] 2017-03-22 03:13:36,149 StorageService.java:1138 - JOINING: calculation complete, ready to bootstrap
INFO  [main] 2017-03-22 03:13:36,156 StorageService.java:1138 - JOINING: getting bootstrap token
...
{noformat}

So, basically it was stuck on 2017-03-21 21:11:11,404 and the main thread somehow continued on  2017-03-22 03:13:36,148, ~ 6 hours later.

I have two thread dumps. The first from 21:30:
[^td___2017-03-21-21-30-09.tdump]

and a second one ~ 100min later:
[^td___2017-03-21-23-09-59.tdump]

Both thread dumps have in common, that the main thread is stuck in some logback code:
{noformat}
""main"" #1 prio=5 os_prio=0 tid=0x00007fe93821a800 nid=0x4d4e waiting on condition [0x00007fe93c813000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c861bb88> (a java.util.concurrent.locks.ReentrantLock$FairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantLock$FairSync.lock(ReentrantLock.java:224)
	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
	at ch.qos.logback.core.OutputStreamAppender.subAppend(OutputStreamAppender.java:217)
	at ch.qos.logback.core.OutputStreamAppender.append(OutputStreamAppender.java:103)
	at ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:88)
	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:48)
	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:273)
	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:260)
	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:442)
	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:396)
	at ch.qos.logback.classic.Logger.info(Logger.java:600)
	at org.apache.cassandra.service.StorageService.setMode(StorageService.java:1138)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:870)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:740)
	- locked <0x00000000c85d37d8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:617)
	- locked <0x00000000c85d37d8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:391)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:566)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:655)
{noformat}"
CASSANDRA-13357,A possible NPE in nodetool getendpoints,"The GetEndpoints.execute method has the following code:

{code:title=GetEndpoints.java|borderStyle=solid}
       List<InetAddress> endpoints = probe.getEndpoints(ks, table, key);
        for (InetAddress endpoint : endpoints)
        {
            System.out.println(endpoint.getHostAddress());
        }
{code}

This code can throw NPE. A similar bug is fixed in CASSANDRA-8950. The buggy code  is 
{code:title=NodeCmd.java|borderStyle=solid}
  List<InetAddress> endpoints = this.probe.getEndpoints(keySpace, cf, key);

        for (InetAddress anEndpoint : endpoints)
        {
           output.println(anEndpoint.getHostAddress());
        }
{code}

The fixed code is:

{code:title=NodeCmd.java|borderStyle=solid}
try
        {
            List<InetAddress> endpoints = probe.getEndpoints(keySpace, cf, key);
            for (InetAddress anEndpoint : endpoints)
               output.println(anEndpoint.getHostAddress());
        }
        catch (IllegalArgumentException ex)
        {
            output.println(ex.getMessage());
            probe.failed();
        }
{code}

The GetEndpoints.execute method shall be modified as CASSANDRA-8950 does."
CASSANDRA-13339,java.nio.BufferOverflowException: null,"I'm seeing the following exception running Cassandra 3.9 (with Netty updated to 4.1.8.Final) running on a 2 node cluster.  It would have been processing around 50 queries/second at the time (mixture of inserts/updates/selects/deletes) : there's a collection of tables (some with counters some without) and a single materialized view.

{code}
ERROR [MutationStage-4] 2017-03-15 22:50:33,052 StorageProxy.java:1353 - Failed to apply mutation locally : {}
java.nio.BufferOverflowException: null
	at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.writeUnsignedVInt(BufferedDataOutputStreamPlus.java:262) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.EncodingStats$Serializer.serialize(EncodingStats.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.SerializationHeader$Serializer.serializeForMessaging(SerializationHeader.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:122) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:89) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.serialize(PartitionUpdate.java:790) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation$MutationSerializer.serialize(Mutation.java:393) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:279) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:396) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.applyFuture(Mutation.java:215) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1347) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2539) [apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}

and then again shortly afterwards

{code}
ERROR [MutationStage-3] 2017-03-15 23:27:36,198 StorageProxy.java:1353 - Failed to apply mutation locally : {}
java.nio.BufferOverflowException: null
	at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.writeUnsignedVInt(BufferedDataOutputStreamPlus.java:262) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.EncodingStats$Serializer.serialize(EncodingStats.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.SerializationHeader$Serializer.serializeForMessaging(SerializationHeader.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:122) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:89) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.serialize(PartitionUpdate.java:790) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation$MutationSerializer.serialize(Mutation.java:393) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:279) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:396) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.applyFuture(Mutation.java:215) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1347) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2539) [apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}"
CASSANDRA-13326,Support unaligned memory access for AArch64,ARMv8 (AArch64)  supports unaligned memory access. The patch will enable it and will improve performance on AArch64
CASSANDRA-13283,Make it possible to override MessagingService.Verb ids,"We need to add a few custom Verbs in MessagingService, it would be nice to be able to give them id's that wont clash with any future Verbs"
CASSANDRA-13242,Null Pointer Exception when upgrading from 2.1.13 to 3.9,"We get an error during startup, when upgrade from 2.1.13 to 3.9 similar with CASSANDRA-11008.  help plz.

INFO  09:44:54 Migrating legacy hints to new storage
INFO  09:44:54 Forcing a major compaction of system.hints table
INFO  09:44:54 Writing legacy hints to the new storage
Exception (java.lang.NullPointerException) encountered during startup: null
java.lang.NullPointerException
	at org.apache.cassandra.serializers.Int32Serializer.deserialize(Int32Serializer.java:31)
	at org.apache.cassandra.serializers.Int32Serializer.deserialize(Int32Serializer.java:25)
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:115)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getInt(UntypedResultSet.java:288)
	at org.apache.cassandra.hints.LegacyHintsMigrator.convertLegacyHint(LegacyHintsMigrator.java:197)
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHintsInternal(LegacyHintsMigrator.java:175)
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:158)
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:151)
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:142)
	at org.apache.cassandra.hints.LegacyHintsMigrator.lambda$migrateLegacyHints$1(LegacyHintsMigrator.java:128)
	at java.lang.Iterable.forEach(Iterable.java:75)
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:128)
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrate(LegacyHintsMigrator.java:96)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:334)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730)
ERROR 09:44:54 Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.serializers.Int32Serializer.deserialize(Int32Serializer.java:31) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.serializers.Int32Serializer.deserialize(Int32Serializer.java:25) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:115) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getInt(UntypedResultSet.java:288) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.hints.LegacyHintsMigrator.convertLegacyHint(LegacyHintsMigrator.java:197) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHintsInternal(LegacyHintsMigrator.java:175) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:158) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:151) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:142) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.hints.LegacyHintsMigrator.lambda$migrateLegacyHints$1(LegacyHintsMigrator.java:128) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:128) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrate(LegacyHintsMigrator.java:96) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:334) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730) [apache-cassandra-3.9.jar:3.9]"
CASSANDRA-13226,StreamPlan for incremental repairs flushing memtables unnecessarily,"Since incremental repairs are run against a fixed dataset, there's no need to flush memtables when streaming for them."
CASSANDRA-13222,Paging with reverse queries and static columns may return incorrectly sized pages,"There are 2 specialisations of {{ColumnCounter}} that deal with static columns differently depending on the order of iteration through the column family and which impl is used generally depends on whether or not the {{ColumnFilter}} in use is reversed. However, the base method {{ColumnCounter::countAll}} always uses forward iteration, which can result in overcounting when the query is reversed and there are statics involved. In turn, this leads to incorrectly sized pages being returned to the client."
CASSANDRA-13218,Duration validation error is unclear in case of overflow.,"If a user try to insert a {{duration}} with a number of months or days that cannot fit in an {{int}} (for example: {{9223372036854775807mo1d}}), the error message is confusing."
CASSANDRA-13216,testall failure in org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages,"example failure:

http://cassci.datastax.com/job/cassandra-3.11_testall/81/testReport/org.apache.cassandra.net/MessagingServiceTest/testDroppedMessages

{code}
Error Message

expected:<... dropped latency: 27[30 ms and Mean cross-node dropped latency: 2731] ms> but was:<... dropped latency: 27[28 ms and Mean cross-node dropped latency: 2730] ms>
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: expected:<... dropped latency: 27[30 ms and Mean cross-node dropped latency: 2731] ms> but was:<... dropped latency: 27[28 ms and Mean cross-node dropped latency: 2730] ms>
	at org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages(MessagingServiceTest.java:83)
{code}"
CASSANDRA-13208,test failure in paging_test.TestPagingWithDeletions.test_failure_threshold_deletions,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/528/testReport/paging_test/TestPagingWithDeletions/test_failure_threshold_deletions

{noformat}
Error Message

errors={'127.0.0.2': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.2
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-iXUhoT
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 3404, in test_failure_threshold_deletions
    self.session.execute(SimpleStatement(""select * from paging_test"", fetch_size=1000, consistency_level=CL.ALL, retry_policy=FallthroughRetryPolicy()))
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
""errors={'127.0.0.2': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.2\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-iXUhoT\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\n--------------------- >> end captured logging << ---------------------""
{noformat}"
CASSANDRA-13204,Thread Leak in OutboundTcpConnection,"We found threads leaking from OutboundTcpConnection to machines which are not part of the cluster and still in Gossip for some reason. There are two issues here, this JIRA will cover the second one which is most important. 



1) First issue is that Gossip has information about machines not in the ring which has been replaced out. It causes Cassandra to connect to those machines but due to internode auth, it wont be able to connect to them at the socket level.  

2) Second issue is a race between creating a connection and closing a connections which is triggered by the gossip bug explained above. Let me try to explain it using the code

In OutboundTcpConnection, we are calling closeSocket(true) which will set isStopped=true and also put a close sentinel into the queue to exit the thread. On the ack connection, Gossip tries to send a message which calls connect() which will block for 10 seconds which is RPC timeout. The reason we will block is because Cassandra might not be running there or internode auth will not let it connect. During this 10 seconds, if Gossip calls closeSocket, it will put close sentinel into the queue. When we return from the connect method after 10 seconds, we will clear the backlog queue causing this thread to leak. 

Proofs from the heap dump of the affected machine which is leaking threads 
1. Only ack connection is leaking and not the command connection which is not used by Gossip. 
2. We see thread blocked on the backlog queue, isStopped=true and backlog queue is empty. This is happening on the threads which have already leaked. 
3. A running thread was blocked on the connect waiting for timeout(10 seconds) and we see backlog queue to contain the close sentinel. Once the connect will return false, we will clear the backlog and this thread will have leaked.  


Interesting bits from j stack 
1282 number of threads for ""MessagingService-Outgoing-/<IP-Address>""

Thread which is about to leak:
""MessagingService-Outgoing-/<IP Address>"" 
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:454)
	at sun.nio.ch.Net.connect(Net.java:446)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:137)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:119)
	at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:381)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:217)

Thread already leaked:
""MessagingService-Outgoing-/<IP Address>""
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy.coalesceInternal(CoalescingStrategies.java:482)
	at org.apache.cassandra.utils.CoalescingStrategies$CoalescingStrategy.coalesce(CoalescingStrategies.java:213)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:190)
"
CASSANDRA-13193,PK indices in 'Prepared' response can overflow,"CASSANDRA-7660 added PK indices to the {{Prepared}} response. They are encoded as shorts.

It's possible to prepare a query with more than 32768 placeholders (the hard limit is 64K). For example, we sometimes see users running IN queries with thousands of elements (a bad practice of course, but still possible).

When a PK component is present after the 32768th position, the PK index overflows and a negative value is returned. This can throw off clients if they're not prepared to handle it. For example, the Java driver currently accepts the response, but will fail much later if you try to compute a bound statement's routing key.

Failing fast would be safer here, the prepare query should error out if we detect a PK index overflow."
CASSANDRA-13181,test failure in paging_test.TestPagingData.test_select_in_clause_with_duplicate_keys,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/352/testReport/paging_test/TestPagingData/test_select_in_clause_with_duplicate_keys

{noformat}
Error Message

'TestPagingData' object has no attribute 'create_ks'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-I7zCAw
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 1721, in test_select_in_clause_with_duplicate_keys
    self.create_ks(session, 'test_paging_static_cols', 2)
""'TestPagingData' object has no attribute 'create_ks'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-I7zCAw\ndtest: DEBUG: Done setting configuration options:\n{   'num_tokens': None,\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\n--------------------- >> end captured logging << ---------------------""
{noformat}"
CASSANDRA-13163,NPE in StorageService.excise,"{code}
    private void excise(Collection<Token> tokens, InetAddress endpoint)
    {
        logger.info(""Removing tokens {} for {}"", tokens, endpoint);

        if (tokenMetadata.isMember(endpoint))
            HintsService.instance.excise(tokenMetadata.getHostId(endpoint));

{code}

The check for TMD.isMember() is not enough to guarantee that TMD.getHostId() will not return null. If HintsService.excise() is called with null you get an NPE in a map lookup."
CASSANDRA-13137,nodetool disablethrift deadlocks if THsHaDisruptorServer is stopped while a request is being processed,"We are using Thrift with {{rpc_server_type}} set to {{hsha}}. This creates a {{THsHaDisruptorServer}} which is a subclass of [{{TDisruptorServer}}|https://github.com/xedin/disruptor_thrift_server/blob/master/src/main/java/com/thinkaurelius/thrift/TDisruptorServer.java].

Internally, this spawns {{number_of_cores}} number of selector threads. Each gets a {{RingBuffer}} and {{rpc_max_threads / cores}} number of worker threads (the {{RPC-Thread}} threads). As the server starts receiving requests, each selector thread adds events to its {{RingBuffer}} and the worker threads process them. 

The _events_ are [{{Message}}|https://github.com/xedin/disruptor_thrift_server/blob/master/src/main/java/com/thinkaurelius/thrift/Message.java] instances, which have preallocated buffers for eventual IO.

When the thrift server starts up, the corresponding {{ThriftServerThread}} joins on the selector threads, waiting for them to die. It then iterates through all the {{SelectorThread}} objects and calls their {{shutdown}} method which attempts to drain their corresponding {{RingBuffer}}. The [drain ({{drainAndHalt}})|https://github.com/LMAX-Exchange/disruptor/blob/master/src/main/java/com/lmax/disruptor/WorkerPool.java#L147] works by letting the worker pool ""consumer"" threads catch up to the ""producer"" index, ie. the selector thread.

When we execute a {{nodetool disablethrift}}, it attempts to {{stop}} the {{THsHaDisruptorServer}}. That works by setting a {{stopped}} flag to {{true}}. When the selector threads see that, they break from their {{select()}} loop, and clean up their resources, ie. the {{Message}} objects they've created and their buffers. *However*, if one of those {{Message}} objects is currently being used by a worker pool thread to process a request, if it calls [this piece of code|https://github.com/xedin/disruptor_thrift_server/blob/master/src/main/java/com/thinkaurelius/thrift/Message.java#L317], you'll get the following {{NullPointerException}}

{noformat}
Jan 18, 2017 6:28:50 PM com.lmax.disruptor.FatalExceptionHandler handleEventException
SEVERE: Exception processing: 633124 com.thinkaurelius.thrift.Message$Invocation@25c9fbeb
java.lang.NullPointerException
        at com.thinkaurelius.thrift.Message.getInputTransport(Message.java:338)
        at com.thinkaurelius.thrift.Message.invoke(Message.java:308)
        at com.thinkaurelius.thrift.Message$Invocation.execute(Message.java:90)
        at com.thinkaurelius.thrift.TDisruptorServer$InvocationHandler.onEvent(TDisruptorServer.java:695)
        at com.thinkaurelius.thrift.TDisruptorServer$InvocationHandler.onEvent(TDisruptorServer.java:689)
        at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:112)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

That fails because it tries to dereference one of the {{Message}} ""cleaned up"", ie. {{null}}, buffers.

Because that call is outside the {{try}} block, the exception escapes and basically kills the worker pool thread. This has the side effect of ""discarding"" one of the consumers of a selector's {{RingBuffer}}. 

*That* has the side effect of preventing the {{ThriftServerThread}} from draining the {{RingBuffer}} (and dying) since the consumers never catch up to the stopped producer. And that finally has the effect of preventing the {{nodetool disablethrift}} from proceeding since it's trying to {{join}} the {{ThriftServerThread}}. Deadlock!

The {{ThriftServerThread}} thread looks like

{noformat}
""Thread-1"" #2234 prio=5 os_prio=0 tid=0x00007f4ae6ff1000 nid=0x2eb6 runnable [0x00007f4729174000]
   java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.yield(Native Method)
        at com.lmax.disruptor.WorkerPool.drainAndHalt(WorkerPool.java:147)
        at com.thinkaurelius.thrift.TDisruptorServer$SelectorThread.shutdown(TDisruptorServer.java:633)
        at com.thinkaurelius.thrift.TDisruptorServer.gracefullyShutdownInvokerPool(TDisruptorServer.java:301)
        at com.thinkaurelius.thrift.TDisruptorServer.waitForShutdown(TDisruptorServer.java:280)
        at org.apache.thrift.server.AbstractNonblockingServer.serve(AbstractNonblockingServer.java:95)
        at org.apache.cassandra.thrift.ThriftServer$ThriftServerThread.run(ThriftServer.java:137)
{noformat}

The {{nodetool disablethrift}} thread looks like

{noformat}
""RMI TCP Connection(18183)-127.0.0.1"" #12121 daemon prio=5 os_prio=0 tid=0x00007f4ac2c61000 nid=0x5805 in Object.wait() [0x00007f4aab7ec000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Thread.join(Thread.java:1249)
        - locked <0x000000055d3cb010> (a org.apache.cassandra.thrift.ThriftServer$ThriftServerThread)
        at java.lang.Thread.join(Thread.java:1323)
        at org.apache.cassandra.thrift.ThriftServer.stop(ThriftServer.java:70)
        - locked <0x000000055bffb5e0> (a org.apache.cassandra.thrift.ThriftServer)
        at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:337)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
        at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:324)
        at sun.rmi.transport.Transport$1.run(Transport.java:200)
        at sun.rmi.transport.Transport$1.run(Transport.java:197)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:683)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$1/1038375559.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Most of the code involved isn't part of Cassandra source, but it's an external dependency that should be fixed."
CASSANDRA-13120,Trace and Histogram output misleading,"If we look at the following output:

{noformat}
[centos@cassandra-c-3]$ nodetool getsstables -- keyspace table 60ea4399-6b9f-4419-9ccb-ff2e6742de10
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647146-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647147-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647145-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647152-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-647157-big-Data.db
/mnt/cassandra/data/data/keyspace/table-62f30431acf411e69a4ed7dd11246f8a/mc-648137-big-Data.db
{noformat}

We can see that this key value appears in just 6 sstables.  However, when we run a select against the table and key we get:

{noformat}
Tracing session: a6c81330-d670-11e6-b00b-c1d403fd6e84

 activity                                                                                                          | timestamp                  | source         | source_elapsed
-------------------------------------------------------------------------------------------------------------------+----------------------------+----------------+----------------
                                                                                                Execute CQL3 query | 2017-01-09 13:36:40.419000 | 10.200.254.141 |              0
 Parsing SELECT * FROM keyspace.table WHERE id = 60ea4399-6b9f-4419-9ccb-ff2e6742de10; [SharedPool-Worker-2]       | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            104
                                                                         Preparing statement [SharedPool-Worker-2] | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            220
                                        Executing single-partition query on table [SharedPool-Worker-1]            | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            450
                                                                Acquiring sstable references [SharedPool-Worker-1] | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            477
                                                 Bloom filter allows skipping sstable 648146 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419000 | 10.200.254.141 |            496
                                                 Bloom filter allows skipping sstable 648145 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            503
                                                            Key cache hit for sstable 648140 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            513
                                                 Bloom filter allows skipping sstable 648135 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            520
                                                 Bloom filter allows skipping sstable 648130 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            526
                                                 Bloom filter allows skipping sstable 648048 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            530
                                                 Bloom filter allows skipping sstable 647749 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            535
                                                 Bloom filter allows skipping sstable 647404 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            540
                                                            Key cache hit for sstable 647145 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            548
                                                            Key cache hit for sstable 647146 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419001 | 10.200.254.141 |            556
                                                            Key cache hit for sstable 647147 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            564
                                                 Bloom filter allows skipping sstable 647148 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            570
                                                 Bloom filter allows skipping sstable 647149 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            575
                                                 Bloom filter allows skipping sstable 647150 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            580
                                                 Bloom filter allows skipping sstable 647151 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            585
                                                            Key cache hit for sstable 647152 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            591
                                                 Bloom filter allows skipping sstable 647153 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            597
                                                 Bloom filter allows skipping sstable 647154 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            601
                                                 Bloom filter allows skipping sstable 647155 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            606
                                                 Bloom filter allows skipping sstable 647156 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            611
                                                            Key cache hit for sstable 647157 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419002 | 10.200.254.141 |            617
                                                 Bloom filter allows skipping sstable 647158 [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            623
                  Skipped 0/22 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            644
                                                 Merging data from memtables and 22 sstables [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            654
                                                           Read 9 live and 0 tombstone cells [SharedPool-Worker-1] | 2017-01-09 13:36:40.419003 | 10.200.254.141 |            732
                                                                                                  Request complete | 2017-01-09 13:36:40.419808 | 10.200.254.141 |            808
{noformat}

You'll note we claim not to have skipped any files due to bloom filters - even though we know the data is only in 6 files.

CFHistograms also report that we're hitting every sstables:

{noformat}
Percentile SSTables Write Latency Read Latency Partition Size Cell Count 
(micros) (micros) (bytes) 
50% 24.00 14.24 182.79 103 1 
75% 24.00 17.08 315.85 149 2 
95% 24.00 20.50 7007.51 372 7 
98% 24.00 24.60 10090.81 642 12 
99% 24.00 29.52 12108.97 770 14 
Min 21.00 3.31 29.52 43 0 
Max 29.00 1358.10 62479.63 1597 35
{noformat}

Code for the read is here:

https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L561

We seem to iterate over all the sstables and increment the metric as part of that iteration.

Either the reporting is incorrect - or we should maybe check the bloom filters first and then iterate the tombstones after?  

In this particular case we were using TWCS which makes the problem more apparent.  TWCS guarantees that we'll keep more sstables in an un-merged state.  With STCS we have to search them all, but most of them should be merged together if Compaction is keeping up.  LCS the read path is restricted which will disguise the impact.
"
CASSANDRA-13114,Upgrade netty to 4.0.44 to fix memory leak with client encryption,"https://issues.apache.org/jira/browse/CASSANDRA-12032 updated netty for Cassandra 3.8, but this wasn't backported. Netty 4.0.23, which ships with Cassandra 3.0.x, has some serious bugs around memory handling for SSL connections.

It would be nice if both were updated to 4.0.42, a version released this year.

4.0.23 makes it impossible for me to run SSL, because nodes run out of memory every ~30 minutes. This was fixed in 4.0.27."
CASSANDRA-13108,Uncaught exeption stack traces not logged,"In a refactoring to parameterized logging we lost the stack traces of uncaught exceptions. This means, apart from the thread, I have no idea where they come from e.g.

{code}
ERROR [OptionalTasks:1] 2017-01-06 12:53:14,204 CassandraDaemon.java:231 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException: null
{code}"
CASSANDRA-13091,SASIIndexTest.testTableRebuild throws unsafe memory access exception,"While running SASI tests, the following issue keeps popping up on trunk:

{code}
    [junit] DEBUG [main] 2017-01-03 15:30:38,708 ?:? - Failed search an index /Users/oleksandrpetrov/foss/java/backup/cassandra/build/test/cassandra/data:0/sasi/clustering_test_cf_1-28004b20d1c111e6a96b29c83c9e6808/mc-3-big-SI_age.db, skipping.
    [junit] java.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code
    [junit]     at org.apache.cassandra.index.sasi.utils.MappedBuffer.position(MappedBuffer.java:105) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.disk.OnDiskIndex$Level.getBlock(OnDiskIndex.java:556) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.disk.OnDiskIndex.searchIndex(OnDiskIndex.java:464) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.disk.OnDiskIndex.getTerm(OnDiskIndex.java:458) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.disk.OnDiskIndex.search(OnDiskIndex.java:231) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.SSTableIndex.search(SSTableIndex.java:103) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.TermIterator.lambda$build$0(TermIterator.java:130) ~[main/:na]
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_91]
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_91]
    [junit]     at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299) ~[guava-18.0.jar:na]
    [junit]     at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_91]
    [junit]     at com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:50) ~[guava-18.0.jar:na]
    [junit]     at com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:37) ~[guava-18.0.jar:na]
    [junit]     at org.apache.cassandra.index.sasi.TermIterator.build(TermIterator.java:125) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.plan.QueryController.getIndexes(QueryController.java:139) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.plan.Operation$Builder.complete(Operation.java:433) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.plan.QueryPlan.analyze(QueryPlan.java:57) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.plan.QueryPlan.execute(QueryPlan.java:68) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.SASIIndex.lambda$searcherFor$2(SASIIndex.java:287) ~[main/:na]
    [junit]     at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:357) ~[main/:na]
    [junit]     at org.apache.cassandra.db.ReadCommand.executeInternal(ReadCommand.java:387) ~[main/:na]
    [junit]     at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:425) ~[main/:na]
    [junit]     at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:411) ~[main/:na]
    [junit]     at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:78) ~[main/:na]
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.executeOnceInternal(QueryProcessor.java:332) ~[main/:na]
    [junit]     at org.apache.cassandra.index.sasi.SASIIndexTest.executeCQL(SASIIndexTest.java:2438) ~[classes/:na]
    [junit]     at org.apache.cassandra.index.sasi.SASIIndexTest.testTableRebuild(SASIIndexTest.java:1895) ~[classes/:na]
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_91]
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_91]
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_91]
    [junit]     at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_91]
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) ~[junit-4.6.jar:na]
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) ~[junit-4.6.jar:na]
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) ~[junit-4.6.jar:na]
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) ~[junit-4.6.jar:na]
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) ~[junit-4.6.jar:na]
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) ~[junit-4.6.jar:na]
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) ~[junit-4.6.jar:na]
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44) ~[junit-4.6.jar:na]
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180) ~[junit-4.6.jar:na]
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41) ~[junit-4.6.jar:na]
    [junit]     at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173) ~[junit-4.6.jar:na]
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) ~[junit-4.6.jar:na]
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) ~[junit-4.6.jar:na]
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:220) ~[junit-4.6.jar:na]
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39) ~[junit-4.6.jar:na]
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535) ~[ant-junit.jar:na]
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182) ~[ant-junit.jar:na]
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033) ~[ant-junit.jar:na]
{code}

It doesn't cause any test failures, although still doesn't look right."
CASSANDRA-13067,Integer overflows with file system size reported by Amazon Elastic File System (EFS),"When not explicitly configured Cassandra uses [{{nio.FileStore.getTotalSpace}}|https://docs.oracle.com/javase/7/docs/api/java/nio/file/FileStore.html] to determine the total amount of available space in order to [calculate the preferred commit log size|https://github.com/apache/cassandra/blob/cassandra-3.9/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L553]. [Amazon EFS|https://aws.amazon.com/efs/] instances report a filesystem size of 8 EiB when empty. [{{getTotalSpace}} causes an integer overflow (JDK-8162520)|https://bugs.openjdk.java.net/browse/JDK-8162520] and returns a negative number, resulting in a negative preferred size and causing the checked integer to throw.

Overriding {{commitlog_total_space_in_mb}} is not sufficient as [{{DataDirectory.getAvailableSpace}}|https://github.com/apache/cassandra/blob/cassandra-3.9/src/java/org/apache/cassandra/db/Directories.java#L550] makes use of {{nio.FileStore.getUsableSpace}}.

[AMQ-6441] is a comparable issue in ActiveMQ."
CASSANDRA-13060,NPE in StorageService.java while bootstrapping,"Lots of NPE happens when bootstrapping new node:
{code}
WARN  [SharedPool-Worker-1] 2016-12-19 23:09:09,034 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-1,5,main]: {}
java.lang.NullPointerException: null
        at org.apache.cassandra.service.StorageService.isRpcReady(StorageService.java:1829) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.service.StorageService.notifyUp(StorageService.java:1787) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.service.StorageService.onAlive(StorageService.java:2424) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.gms.Gossiper.realMarkAlive(Gossiper.java:999) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.gms.Gossiper$3.response(Gossiper.java:979) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_111]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.10.jar:3.0.10]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_111]
{code}"
CASSANDRA-13031,Speed-up start-up sequence by avoiding un-needed flushes,"Similar to CASSANDRA-12969, do a conditional update for all functions
    with a forced blocking flush to avoid slowed-down boot sequences. The
    small performance hit of doing a read is always smaller than the one
    associated with a fsync().
"
CASSANDRA-13028,Log message size in trace message in OutboundTcpConnection,It would be nice to know message sizes when tracing a query.
CASSANDRA-13026,dtest failure in upgrade_tests.paging_test.TestPagingDataNodes3RF3_Upgrade_current_3_x_To_indev_trunk.static_columns_paging_test,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/83/testReport/upgrade_tests.paging_test/TestPagingDataNodes3RF3_Upgrade_current_3_x_To_indev_trunk/static_columns_paging_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 214, in tearDown
    super(UpgradeTester, self).tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 583, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
{code}{code}
ERROR [Native-Transport-Requests-2] 2016-12-08 22:47:22,944 Message.java:623 - Unexpected exception during request; channel = [id: 0x9f44f9af, L:/127.0.0.1:9042 - R:/127.0.0.1:53962]
java.io.IOError: java.io.EOFException: EOF after 3 bytes out of 4
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:227) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:215) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:374) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:186) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:155) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:492) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:352) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133) ~[main/:na]
	at org.apache.cassandra.service.pager.AbstractQueryPager$Pager.applyToPartition(AbstractQueryPager.java:108) ~[main/:na]
	at org.apache.cassandra.service.pager.AbstractQueryPager$Pager.applyToPartition(AbstractQueryPager.java:81) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:761) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:406) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:384) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:257) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:79) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[main/:na]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_51]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
Caused by: java.io.EOFException: EOF after 3 bytes out of 4
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:68) ~[main/:na]
	at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:60) ~[main/:na]
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:402) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.readValue(AbstractType.java:425) ~[main/:na]
	at org.apache.cassandra.db.rows.Cell$Serializer.deserialize(Cell.java:245) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.readSimpleColumn(UnfilteredSerializer.java:610) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.lambda$deserializeRowBody$317(UnfilteredSerializer.java:575) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$83/33119331.accept(Unknown Source) ~[na:na]
	at org.apache.cassandra.utils.btree.BTree.applyForwards(BTree.java:1222) ~[main/:na]
	at org.apache.cassandra.utils.btree.BTree.apply(BTree.java:1177) ~[main/:na]
	at org.apache.cassandra.db.Columns.apply(Columns.java:377) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserializeRowBody(UnfilteredSerializer.java:571) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:440) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:222) ~[main/:na]
	... 33 common frames omitted
{code}"
CASSANDRA-12920,Possible messaging latency caused by not flushing in time,"Not sure if this can be improved. We're using Cassandra 2.2.5. Cassandra considers messages whose payload size <= 64KB as small messages. For response for such as digest request, it's very likely a small message. Even quite a few such responses cannot fill in the output buffer and thus trigger a flush.

One possible issue is that, we use conditions count == 1 && backlog.isEmpty() to decide whether to flush the output stream. That means if the backlog is empty, we won't flush the output stream even we drained, say, 10 messages whose sizes sum < 64K. This may cause delay for small messages. Shouldn't we flush after writing the last drained message if backlog is empty? Of course if backlog is not empty, we can continue draining more messages and very likely trigger a flush very soon.

Here are some tracing events that show the latency. Be noted that the involved nodes are in the same data center whose round trip network latency is well below 1ms. Alao we have disabled message coalescing due to CASSANDRA-12676.

{noformat}
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127c7c28-aaa6-11e6-941f-f3a2e2759bd2 |                                       Enqueuing response to /*******32.28 |  *******29.3 |           1199 |                     SharedPool-Worker-4
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127cca45-aaa6-11e6-941f-f3a2e2759bd2 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******29.3 |           2667 | MessagingService-Outgoing-/*******32.28
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127cca40-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******29.3 | *******32.28 |         152635 |  MessagingService-Incoming-/*******29.3
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127cca41-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******29.3 | *******32.28 |         152665 |                    SharedPool-Worker-45


 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b08e68a-aaa6-11e6-8dce-951d39203586 |                                       Enqueuing response to /*******32.28 |  *******29.5 |            532 |                   SharedPool-Worker-178
 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b090d90-aaa6-11e6-8dce-951d39203586 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******29.5 |            579 | MessagingService-Outgoing-/*******32.28
 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b090d90-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******29.5 | *******32.28 |         143865 |  MessagingService-Incoming-/*******29.5
 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b090d91-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******29.5 | *******32.28 |         143908 |                 
   SharedPool-Worker-41
 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b0934a8-aaa6-11e6-8dce-951d39203586 |                                       Enqueuing response to /*******32.28 |  *******29.5 |           1632 |                 
  SharedPool-Worker-159 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b0934a9-aaa6-11e6-8dce-951d39203586 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******29.5 |           1660 | MessagingService
-Outgoing-/*******32.28 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b095bb0-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******29.5 | *******32.28 |         129813 |  MessagingServic
e-Incoming-/*******29.5 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b095bb1-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******29.5 | *******32.28 |         129835 |                 
    SharedPool-Worker-3

 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 3029fd17-aaa6-11e6-a5cc-8b24438aa927 |                                       Enqueuing response to /*******32.28 |  *******30.8 |           1686 |                   SharedPool-Worker-271
 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 302ae770-aaa6-11e6-a5cc-8b24438aa927 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******30.8 |           6938 | MessagingService-Outgoing-/*******32.28
 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 302ae770-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******30.8 | *******32.28 |         171380 |  MessagingService-Incoming-/*******30.8
 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 302ae771-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******30.8 | *******32.28 |         171472 |                    SharedPool-Worker-55
{noformat}
"
CASSANDRA-12913,System tables should have gc_grace of zero,"System tables are not replicated and some have deletes (views tables, local, peers, index info)."
CASSANDRA-12899,stand alone sstableupgrade prints LEAK DETECTED warnings some times,"the stand alone sstableupgrade prints LEAK DETECTED warnings some times.

{code}
sstableupgrade keyspace1 standard1
WARN  04:22:41,942 Small commitlog volume detected at /var/lib/cassandra/commitlog; setting commitlog_total_space_in_mb to 8047.  You can override this in cassandra.yaml
WARN  04:22:41,948 Small cdc volume detected at /var/lib/cassandra/cdc_raw; setting cdc_total_space_in_mb to 4023.  You can override this in cassandra.yaml
WARN  04:22:41,950 Only 29.016GiB free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots
Found 0 sstables that need upgrading.
ERROR 04:22:45,472 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7b135bd7) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@2080155253:/var/lib/cassandra/data/keyspace1/standard1-24e490e1a6fb11e68ebe232ff1b97ca7/mc-17-big was not released before the reference was garbage collected
ERROR 04:22:45,472 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@7b135bd7:
Thread[main,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:245)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:175)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:97)
	at org.apache.cassandra.io.sstable.format.SSTableReader.<init>(SSTableReader.java:237)
	at org.apache.cassandra.io.sstable.format.big.BigTableReader.<init>(BigTableReader.java:57)
	at org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory.open(BigFormat.java:101)
	at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:638)
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:506)
	at org.apache.cassandra.io.sstable.format.SSTableReader.openNoValidation(SSTableReader.java:401)
	at org.apache.cassandra.tools.StandaloneUpgrader.main(StandaloneUpgrader.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.datastax.bdp.tools.ShellToolWrapper.main(ShellToolWrapper.java:34)
{code}"
CASSANDRA-12877,SASI index throwing AssertionError on creation/flush,"Possibly a 3.10 regression?  The exact test shown below does not error in 3.9.

I built and installed a 3.10 snapshot (built 04-Nov-2016) to get around CASSANDRA-11670, CASSANDRA-12689, and CASSANDRA-12223 which are holding me back when using 3.9.

Now I'm able to make nodetool flush (or a scheduled flush) produce an unhandled error easily with a SASI:

{code}
CREATE KEYSPACE vjtest WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};
use vjtest ;
create table tester(id1 text, id2 text, id3 text, val1 text, primary key((id1, id2), id3));
create custom index tester_idx_val1 on tester(val1) using 'org.apache.cassandra.index.sasi.SASIIndex';
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','1-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','2-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','3-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','4-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','5-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','6-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','7-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','8-3','asdf');
insert into tester(id1,id2,id3, val1) values ('1-1','1-2','9-3','asdf');
{code}

Not enough going on here to trigger a flush, so following a manual {{nodetool flush vjtest}} I get the following in {{system.log}}:

{code}
INFO  [MemtableFlushWriter:3] 2016-11-04 22:19:35,412 PerSSTableIndexWriter.java:284 - Scheduling index flush to /mydir/apache-cassandra-3.10-SNAPSHOT/data/data/vjtest/tester-6f1fdff0a30611e692c087673c5ef8d4/mc-1-big-SI_tester_idx_val1.db
INFO  [SASI-Memtable:1] 2016-11-04 22:19:35,447 PerSSTableIndexWriter.java:335 - Index flush to /mydir/apache-cassandra-3.10-SNAPSHOT/data/data/vjtest/tester-6f1fdff0a30611e692c087673c5ef8d4/mc-1-big-SI_tester_idx_val1.db took 16 ms.
ERROR [SASI-Memtable:1] 2016-11-04 22:19:35,449 CassandraDaemon.java:229 - Exception in thread Thread[SASI-Memtable:1,5,RMI Runtime]
java.lang.AssertionError: cannot have more than 8 overflow collisions per leaf, but had: 9
    at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder$Leaf.createOverflowEntry(AbstractTokenTreeBuilder.java:357) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder$Leaf.createEntry(AbstractTokenTreeBuilder.java:346) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.DynamicTokenTreeBuilder$DynamicLeaf.serializeData(DynamicTokenTreeBuilder.java:180) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder$Leaf.serialize(AbstractTokenTreeBuilder.java:306) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder.write(AbstractTokenTreeBuilder.java:90) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder$MutableDataBlock.flushAndClear(OnDiskIndexBuilder.java:629) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder$MutableLevel.flush(OnDiskIndexBuilder.java:446) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder$MutableLevel.finalFlush(OnDiskIndexBuilder.java:451) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder.finish(OnDiskIndexBuilder.java:296) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder.finish(OnDiskIndexBuilder.java:258) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder.finish(OnDiskIndexBuilder.java:241) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.lambda$scheduleSegmentFlush$0(PerSSTableIndexWriter.java:267) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.lambda$complete$1(PerSSTableIndexWriter.java:296) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_101]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_101]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
ERROR [MemtableFlushWriter:3] 2016-11-04 22:19:35,453 DataTracker.java:168 - Can't open index file at /mydir/apache-cassandra-3.10-SNAPSHOT/data/data/vjtest/tester-6f1fdff0a30611e692c087673c5ef8d4/mc-1-big-SI_tester_idx_val1.db, skipping.
java.lang.IllegalArgumentException: position: 8314882962218811392, limit: 4113
    at org.apache.cassandra.index.sasi.utils.MappedBuffer.position(MappedBuffer.java:106) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.disk.OnDiskIndex.<init>(OnDiskIndex.java:147) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.SSTableIndex.<init>(SSTableIndex.java:62) ~[apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.conf.DataTracker.getIndexes(DataTracker.java:150) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.conf.DataTracker.update(DataTracker.java:69) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.conf.ColumnIndex.update(ColumnIndex.java:147) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.index.sasi.SASIIndex.handleNotification(SASIIndex.java:320) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.db.lifecycle.Tracker.notifyAdded(Tracker.java:421) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.db.lifecycle.Tracker.replaceFlushed(Tracker.java:356) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.replaceFlushed(CompactionStrategyManager.java:317) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1574) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1197) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1100) [apache-cassandra-3.10-SNAPSHOT.jar:3.10-SNAPSHOT]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
ERROR [Reference-Reaper:1] 2016-11-04 22:21:59,610 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1fb888b0) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@293565751:/mydir/apache-cassandra-3.10-SNAPSHOT/data/data/vjtest/tester-6f1fdff0a30611e692c087673c5ef8d4/mc-1-big was not released before the reference was garbage collected
{code}

Subsequently:
{code}select * from tester where val1='asdf';{code} produces 0 rows."
CASSANDRA-12875,testall failure in org.apache.cassandra.net.MessagingServiceTest.testDCLatency-compression,"example failure:

http://cassci.datastax.com/job/cassandra-3.X_testall/54/testReport/org.apache.cassandra.net/MessagingServiceTest/testDCLatency_compression/

{code}
Error Message

expected:<107964792> but was:<129557750>
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: expected:<107964792> but was:<129557750>
	at org.apache.cassandra.net.MessagingServiceTest.testDCLatency(MessagingServiceTest.java:115)
{code}"
CASSANDRA-12832,SASI index corruption on too many overflow items,"When SASI index has too many overflow items, it currently writes a corrupted index file:

{code}
java.lang.AssertionError: cannot have more than 8 overflow collisions per leaf, but had: 15
        at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder$Leaf.createOverflowEntry(AbstractTokenTreeBuilder.java:357) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder$Leaf.createEntry(AbstractTokenTreeBuilder.java:346) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.DynamicTokenTreeBuilder$DynamicLeaf.serializeData(DynamicTokenTreeBuilder.java:180) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder$Leaf.serialize(AbstractTokenTreeBuilder.java:306) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.AbstractTokenTreeBuilder.write(AbstractTokenTreeBuilder.java:90) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder$MutableDataBlock.flushAndClear(OnDiskIndexBuilder.java:629) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder$MutableLevel.flush(OnDiskIndexBuilder.java:446) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder$MutableLevel.finalFlush(OnDiskIndexBuilder.java:451) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder.finish(OnDiskIndexBuilder.java:296) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder.finish(OnDiskIndexBuilder.java:258) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.OnDiskIndexBuilder.finish(OnDiskIndexBuilder.java:241) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.lambda$scheduleSegmentFlush$0(PerSSTableIndexWriter.java:267) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.lambda$complete$1(PerSSTableIndexWriter.java:296) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_91]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
ERROR [MemtableFlushWriter:4] 2016-10-23 23:17:19,920 DataTracker.java:168 - Can't open index file at ...., skipping.
java.lang.IllegalArgumentException: position: -524200, limit: 12288
        at org.apache.cassandra.index.sasi.utils.MappedBuffer.position(MappedBuffer.java:106) ~[main/:na]
        at org.apache.cassandra.index.sasi.disk.OnDiskIndex.<init>(OnDiskIndex.java:155) ~[main/:na]
        at org.apache.cassandra.index.sasi.SSTableIndex.<init>(SSTableIndex.java:62) ~[main/:na]
        at org.apache.cassandra.index.sasi.conf.DataTracker.getIndexes(DataTracker.java:150) [main/:na]
        at org.apache.cassandra.index.sasi.conf.DataTracker.update(DataTracker.java:69) [main/:na]
        at org.apache.cassandra.index.sasi.conf.ColumnIndex.update(ColumnIndex.java:147) [main/:na]
        at org.apache.cassandra.index.sasi.SASIIndex.handleNotification(SASIIndex.java:320) [main/:na]
        at org.apache.cassandra.db.lifecycle.Tracker.notifyAdded(Tracker.java:421) [main/:na]
        at org.apache.cassandra.db.lifecycle.Tracker.replaceFlushed(Tracker.java:356) [main/:na]
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.replaceFlushed(CompactionStrategyManager.java:317) [main/:na]
        at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1569) [main/:na]
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1197) [main/:na]
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1100) [main/:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
{code}"
CASSANDRA-12831,OutOfMemoryError with Cassandra 3.0.9,"I have running some tests on a monitoring system I work on and Cassandra is consistently crashing with OOMEs, and the JVM exists. This is happening in a dev environment with a single node created with ccm. 

The monitoring server is ingesting 4,000 data points every 10 seconds. Every two hours a job runs which fetches all raw data from the past two hours. The raw data is compressed, written to another table, and then deleted. After 3 or 4 runs of the job Cassandra crashes. Initially I thought that the problem was in my application code, but I am no longer of that opinion because I set up the same test environment with Cassandra 3.9, and it has been running for almost 48 hours without error. And I actually increased the load on the 3.9 environment.

The schema for the raw data which is queried looks like:

{noformat}
CREATE TABLE hawkular_metrics.data (
    tenant_id text,
    type tinyint,
    metric text,
    dpart bigint,
    time timeuuid,
    n_value double,
    tags map<text, text>,
    PRIMARY KEY ((tenant_id, type, metric, dpart), time)
) WITH CLUSTERING ORDER BY (time DESC)
{noformat}

And the schema for the table that is written to:

{noformat}
CREATE TABLE hawkular_metrics.data_compressed (
    tenant_id text,
    type tinyint,
    metric text,
    dpart bigint,
    time timestamp,
    c_value blob,
    tags blob,
    ts_value blob,
    PRIMARY KEY ((tenant_id, type, metric, dpart), time)
) WITH CLUSTERING ORDER BY (time DESC)
{noformat}

I am using version 3.0.1 of the DataStax Java driver. Last night I changed the driver's page size from the default to 1000, and so far I have not yet seen any errors.

I have attached the log file. I was going to attach one of the heap dumps, but it looks like they are too big."
CASSANDRA-12813,NPE in auth for bootstrapping node,"{code}
ERROR [SharedPool-Worker-1] 2016-10-19 21:40:25,991 Message.java:617 - Unexpected exception during request; channel = [id: 0x15eb017f, /<public IP omitted>:40869 => /10.0.0.254:9042]
java.lang.NullPointerException: null
	at org.apache.cassandra.auth.PasswordAuthenticator.doAuthenticate(PasswordAuthenticator.java:144) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.auth.PasswordAuthenticator.authenticate(PasswordAuthenticator.java:86) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.auth.PasswordAuthenticator.access$100(PasswordAuthenticator.java:54) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.auth.PasswordAuthenticator$PlainTextSaslAuthenticator.getAuthenticatedUser(PasswordAuthenticator.java:182) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:78) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.9.jar:3.0.9]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.9.jar:3.0.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{code}

I have a node that has been joining for around 24 hours.  My application is configured with the IP address of the joining node in the list of nodes to connect to (ruby driver), and I have been getting around 200 events of this NPE per hour.  I removed the IP of the joining node from the list of nodes for my app to connect to and the errors stopped."
CASSANDRA-12776,when memtable flush Statistics thisOffHeap error,"{code}
if (largest != null)
            {
                float usedOnHeap = Memtable.MEMORY_POOL.onHeap.usedRatio();
                float usedOffHeap = Memtable.MEMORY_POOL.offHeap.usedRatio();
                float flushingOnHeap = Memtable.MEMORY_POOL.onHeap.reclaimingRatio();
                float flushingOffHeap = Memtable.MEMORY_POOL.offHeap.reclaimingRatio();
                float thisOnHeap = largest.getAllocator().onHeap().ownershipRatio();
                float thisOffHeap = largest.getAllocator().onHeap().ownershipRatio();
                logger.info(""Flushing largest {} to free up room. Used total: {}, live: {}, flushing: {}, this: {}"",
                            largest.cfs, ratio(usedOnHeap, usedOffHeap), ratio(liveOnHeap, liveOffHeap),
                            ratio(flushingOnHeap, flushingOffHeap), ratio(thisOnHeap, thisOffHeap));
                largest.cfs.switchMemtableIfCurrent(largest);
            }
{code}

Should:

{code}
float thisOffHeap = largest.getAllocator().onHeap().ownershipRatio();
{code}

Be:

{{offHeap().ownershipRatio();}}"
CASSANDRA-12754,Change cassandra.wait_for_tracing_events_timeout_secs default to -1 so C* doesn't wait on trace events to be written before responding to request by default,"[CASSANDRA-11465] introduces a new system property {{cassandra.wait_for_tracing_events_timeout_secs}} that controls whether or not C* waits for events to be written before responding to client.   The current default behavior is to wait up to 1 second and then respond and timeout.  

If using probabilistic tracing this can cause queries to be randomly delayed up to 1 second.

Changing the default to -1 (disabled and enabling it explicitly in {{cql_tracing_test.TestCqlTracing.tracing_unknown_impl_test}}.

Ideally it would be nice to be able to control this behavior on a per request basis (which would require native protocol changes)."
CASSANDRA-12726,dtest failure in upgrade_tests.paging_test.TestPagingWithModifiersNodes2RF1_Upgrade_current_3_x_To_indev_3_x.test_with_allow_filtering,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/58/testReport/upgrade_tests.paging_test/TestPagingWithModifiersNodes2RF1_Upgrade_current_3_x_To_indev_3_x/test_with_allow_filtering

This is happening on many trunk upgrade tests. See : http://cassci.datastax.com/job/trunk_dtest_upgrade/58/testReport/

{code}
Standard Output
http://git-wip-us.apache.org/repos/asf/cassandra.git git:d45f323eb972c6fec146e5cfa84fdc47eb8aa5eb
Unexpected error in node2 log, error: 
ERROR [MessagingService-Incoming-/127.0.0.1] 2016-09-28 04:30:11,223 CassandraDaemon.java:217 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.RuntimeException: Unknown column cdc during deserialization
	at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.SerializationHeader$Serializer.deserializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.deserializeHeader(UnfilteredRowIteratorSerializer.java:192) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:668) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:656) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:341) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:350) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:610) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:593) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:114) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:190) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.7.jar:3.7]
Unexpected error in node2 log, error: 
ERROR [MessagingService-Incoming-/127.0.0.1] 2016-09-28 04:30:11,270 CassandraDaemon.java:217 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.RuntimeException: Unknown column cdc during deserialization
	at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.SerializationHeader$Serializer.deserializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.deserializeHeader(UnfilteredRowIteratorSerializer.java:192) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:668) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:656) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:341) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:350) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:610) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:593) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:114) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:190) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.7.jar:3.7]
{code}"
CASSANDRA-12666,dtest failure in paging_test.TestPagingData.test_paging_with_filtering_on_partition_key,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/480/testReport/paging_test/TestPagingData/test_paging_with_filtering_on_partition_key

{code}
Standard Output

Unexpected error in node3 log, error: 
ERROR [Native-Transport-Requests-3] 2016-09-17 00:50:11,543 Message.java:622 - Unexpected exception during request; channel = [id: 0x467a4afe, L:/127.0.0.3:9042 - R:/127.0.0.1:59115]
java.lang.AssertionError: null
	at org.apache.cassandra.dht.IncludingExcludingBounds.split(IncludingExcludingBounds.java:45) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.getRestrictedRanges(StorageProxy.java:2368) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$RangeIterator.<init>(StorageProxy.java:1951) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:2235) ~[main/:na]
	at org.apache.cassandra.db.PartitionRangeReadCommand.execute(PartitionRangeReadCommand.java:184) ~[main/:na]
	at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:66) ~[main/:na]
	at org.apache.cassandra.service.pager.PartitionRangeQueryPager.fetchPage(PartitionRangeQueryPager.java:36) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement$Pager$NormalPager.fetchPage(SelectStatement.java:328) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:375) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:250) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:78) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:216) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:247) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:232) ~[main/:na]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:516) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:409) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{code}

Related failures:

http://cassci.datastax.com/job/trunk_novnode_dtest/480/testReport/paging_test/TestPagingData/test_paging_with_filtering_on_partition_key_on_clustering_columns/
http://cassci.datastax.com/job/trunk_novnode_dtest/480/testReport/paging_test/TestPagingData/test_paging_with_filtering_on_partition_key_on_clustering_columns_with_contains/
http://cassci.datastax.com/job/trunk_novnode_dtest/480/testReport/paging_test/TestPagingData/test_paging_with_filtering_on_partition_key_on_counter_columns/"
CASSANDRA-12658,dtest failure in paging_test.py:TestPagingData.test_paging_with_filtering_on_partition_key,"Looks like a paging issue so far.

Stably reproducible with 

{code}
./run_dtests.py --vnodes false paging_test.py:TestPagingData.test_paging_with_filtering_on_partition_key
{code}"
CASSANDRA-12643,Improve reporting of overflowing histograms,
CASSANDRA-12638,dtest failure in upgrade_tests.paging_test.TestPagingWithDeletionsNodes2RF1_Upgrade_current_2_1_x_To_indev_3_x.test_failure_threshold_deletions,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest_upgrade/31/testReport/upgrade_tests.paging_test/TestPagingWithDeletionsNodes2RF1_Upgrade_current_2_1_x_To_indev_3_x/test_failure_threshold_deletions

{code}
Error Message

Some kind of default pattern must be specified!

Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/paging_test.py"", line 1548, in test_failure_threshold_deletions
    timeout_seconds=50)
  File ""/home/automaton/src/ccm/ccmlib/cluster.py"", line 680, in timed_grep_nodes_for_patterns
    pattern = versions_to_patterns(node.get_cassandra_version())
  File ""/home/automaton/src/ccm/ccmlib/common.py"", line 92, in __call__
    raise ValueError(""Some kind of default pattern must be specified!"")
{code}"
CASSANDRA-12618,Out of memory bug with one insert,"When executing an INSERT built by QueryBuilder in java driver produces an OutOfMemory in the server.

Having a table with a List<String> field like this:

CREATE TABLE keyspace_name.table_name( 
    pk uuid, 
    mylist list<text>, 
    PRIMARY KEY (pk)
);


Anyone can build an INSERT with QueryBuilder like this:

{code}
*Statement insert = QueryBuilder.insertInto(keyspace, table).value(""pk"", UUID.randomUUID()).value(""mylist"",""blabla"");*
*session.execute(insert);*
{code}

just trying to set a String where should be a List<String>. 

I have set tracing on in the cassandra node.

{code}
TRACE [SharedPool-Worker-2] 2016-09-06 19:24:37,964 Message.java:506 - Received: QUERY INSERT INTO test_evil.table_aa (pk,mylist) VALUES (?,?);[pageSize = 5000], v=4
DEBUG [SharedPool-Worker-2] 2016-09-06 19:24:37,967 ListSerializer.java:92 - deseiralizign a ByteBuffer in List lenght: 1651269986
TRACE [EXPIRING-MAP-REAPER:1] 2016-09-06 19:24:38,319 ExpiringMap.java:102 - Expired 0 entries
TRACE [SharedPool-Worker-2] 2016-09-06 19:24:38,320 Tracing.java:182 - request complete
TRACE [Service Thread] 2016-09-06 19:24:38,321 GCInspector.java:286 - ParNew GC in 41ms.  CMS Old Gen: 0 -> 9337048; Par Eden Space: 525391216 -> 0; Par Survivor Space: 18860168 -> 30843544
TRACE [Service Thread] 2016-09-06 19:24:38,322 GCInspector.java:286 - ConcurrentMarkSweep GC in 148ms.  CMS Old Gen: 9337048 -> 37098008; Par Survivor Space: 30843544 -> 0
TRACE [Service Thread] 2016-09-06 19:24:38,322 GCInspector.java:286 - ConcurrentMarkSweep GC in 0ms.  CMS Old Gen: 37098008 -> 36063280; 
ERROR [SharedPool-Worker-2] 2016-09-06 19:24:38,323 JVMStabilityInspector.java:140 - JVM state determined to be unstable.  Exiting forcefully due to:
java.lang.OutOfMemoryError: Java heap space
	at java.util.ArrayList.<init>(ArrayList.java:152) ~[na:1.8.0_101]
	at org.apache.cassandra.serializers.ListSerializer.deserializeForNativeProtocol(ListSerializer.java:93) ~[main/:na]
	at org.apache.cassandra.cql3.Lists$Value.fromSerialized(Lists.java:137) ~[main/:na]
	at org.apache.cassandra.cql3.Lists$Marker.bind(Lists.java:242) ~[main/:na]
	at org.apache.cassandra.cql3.Lists$Setter.execute(Lists.java:295) ~[main/:na]
	at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:94) ~[main/:na]
	at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:676) ~[main/:na]
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:616) ~[main/:na]
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:429) ~[main/:na]
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:417) ~[main/:na]
	at com.stratio.cassandra.lucene.IndexQueryHandler.execute(IndexQueryHandler.java:181) ~[cassandra-lucene-index-plugin-3.7.2-RC1-SNAPSHOT.jar:na]
	at com.stratio.cassandra.lucene.IndexQueryHandler.processStatement(IndexQueryHandler.java:155) ~[cassandra-lucene-index-plugin-3.7.2-RC1-SNAPSHOT.jar:na]
	at com.stratio.cassandra.lucene.IndexQueryHandler.process(IndexQueryHandler.java:129) ~[cassandra-lucene-index-plugin-3.7.2-RC1-SNAPSHOT.jar:na]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:292) [netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:32) [netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:283) [netty-all-4.0.36.Final.jar:4.0.36.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{code}

I think that QueryBuilder builds the query with prepared statement: INSERT INTO test_evil.table_aa (pk,mylist) VALUES (?,?); and in the server side it does not validate the correct types. 

I have tested this with normal CQL in cqlsh and it works well.
So, i think it is only in the prepared statement validation. 

I attached a maven project in order to help you to reproduce it. 

To generate the jar: 'mvn clean compile assembly:single'
To execute it: 'java -jar target/EvilQuery-1.0-SNAPSHOT-jar-with-dependencies.jar -host localhost -keyspace keyspace_name -table table_name'







"
CASSANDRA-12609,sstableloader NPE when partial directory path is given,"If a user is running sstableloader for example in the directory where the sstables are the parameter given is not the full <keyspace>/<table> directory. That causes a NPE:

{code}
hkroger@home:~/data/temperatures/clients-f3ae39a0854411e5927a9f25d4de7071
$ ~/work/cassandra/bin/sstableloader  -d 127.0.0.1  .
Exception in thread ""main"" java.lang.NullPointerException
       	at org.apache.cassandra.io.sstable.SSTableLoader.<init>(SSTableLoader.java:65)
       	at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:65)
       	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:49)
{code}
"
CASSANDRA-12556,dtest failure in paging_test.TestPagingDatasetChanges.test_cell_TTL_expiry_during_paging,"example failure:

http://cassci.datastax.com/job/cassandra-3.9_dtest/49/testReport/paging_test/TestPagingDatasetChanges/test_cell_TTL_expiry_during_paging

{code}
Error Message

Error from server: code=2200 [Invalid query] message=""unconfigured table paging_test""
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-V_YoOr
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
--------------------- >> end captured logging << ---------------------

{code}

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 2660, in test_cell_TTL_expiry_during_paging
    session, 'paging_test', cl=CL.ALL, format_funcs={'id': int, 'mytext': random_txt}
  File ""/home/automaton/cassandra-dtest/datahelp.py"", line 130, in create_rows
    vals=', '.join('?' for k in dicts[0].keys()), postfix=postfix)
  File ""cassandra/cluster.py"", line 2162, in cassandra.cluster.Session.prepare (cassandra/cluster.c:37231)
    raise
  File ""cassandra/cluster.py"", line 2159, in cassandra.cluster.Session.prepare (cassandra/cluster.c:37087)
    query_id, bind_metadata, pk_indexes, result_metadata = future.result()
  File ""cassandra/cluster.py"", line 3665, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:70216)
    raise self._final_exception
'Error from server: code=2200 [Invalid query] message=""unconfigured table paging_test""\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-V_YoOr\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\n--------------------- >> end captured logging << ---------------------'
{code}"
CASSANDRA-12530,NPE in MutationStage after WriteTimeoutException,"I observed a situation where nodes seem to be overloaded by MV updates (probably). This produces tons of exceptions like the following.
I guess the WriteTimeoutExceptions come from updates on remote hosts during the MV-write-path. So far so good - shit happens in an overloaded system. But the following NPE is probably not really intended and a follow-up exception of the timed out write.

2016-08-24T12:22:45+00:00 cas5 [MutationStage-25] org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService Uncaught exception on thread Thread[MutationStage-25,5,main]: {}
2016-08-24T12:22:45+00:00 cas5 #011org.apache.cassandra.exceptions.WriteTimeoutException: Operation timed out - received only 0 responses.
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:486)
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.db.Keyspace.lambda$apply$0(Keyspace.java:495)
2016-08-24T12:22:45+00:00 cas5 #011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109)
2016-08-24T12:22:45+00:00 cas5 #011at java.lang.Thread.run(Thread.java:745)
2016-08-24T12:22:45+00:00 cas5 [MutationStage-31] org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService Uncaught exception on thread Thread[MutationStage-31,5,main]: {}
2016-08-24T12:22:45+00:00 cas5 #011org.apache.cassandra.exceptions.WriteTimeoutException: Operation timed out - received only 0 responses.
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:486)
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.db.Keyspace.lambda$apply$0(Keyspace.java:495)
2016-08-24T12:22:45+00:00 cas5 #011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
2016-08-24T12:22:45+00:00 cas5 #011at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109)
2016-08-24T12:22:45+00:00 cas5 #011at java.lang.Thread.run(Thread.java:745)
2016-08-24T12:22:45+00:00 cas5 [MutationStage-7] org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService Uncaught exception on thread Thread[MutationStage-7,5,main]: {}
2016-08-24T12:22:45+00:00 cas5 #011java.lang.NullPointerException: null
2016-08-24T12:22:45+00:00 cas5 [MutationStage-32] org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService Uncaught exception on thread Thread[MutationStage-32,5,main]: {}
2016-08-24T12:22:45+00:00 cas5 #011java.lang.NullPointerException: null
2016-08-24T12:22:45+00:00 cas5 [MutationStage-9] org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService Uncaught exception on thread Thread[MutationStage-9,5,main]: {}
2016-08-24T12:22:45+00:00 cas5 #011java.lang.NullPointerException: null"
CASSANDRA-12527,Stack Overflow returned to queries while upgrading,"I am currently upgrading our cluster from 2.2.5 to 3.0.8.

Some queries (not sure which) appear to be triggering a stack overflow:

ERROR [SharedPool-Worker-2] 2016-08-24 04:34:52,464 Message.java:611 - Unexpected exception during request; channel = [id: 0x5ccb2627, /10.0.2.5:42925 => /10.0.2.10:9042]
java.lang.StackOverflowError: null
        at org.apache.cassandra.db.ClusteringComparator.compare(ClusteringComparator.java:131) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$LegacyBoundComparator.compare(LegacyLayout.java:1761) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.add(LegacyLayout.java:1835) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.addAll(LegacyLayout.java:1900) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:709) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.LegacyLayout$3.computeNext(LegacyLayout.java:711) ~[apache-cassandra-3.0.8.jar:3.0.8]

And it goes on like that. I don't know if this is a real issue or if it will subside once I am done the upgrade. Happy to provide more information where I can."
CASSANDRA-12524,dtest failure in upgrade_tests.paging_test.TestPagingWithDeletionsNodes2RF1_Upgrade_current_3_x_To_indev_3_x.test_failure_threshold_deletions,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/34/testReport/upgrade_tests.paging_test/TestPagingWithDeletionsNodes2RF1_Upgrade_current_3_x_To_indev_3_x/test_failure_threshold_deletions

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/paging_test.py"", line 1733, in test_failure_threshold_deletions
    self.assertTrue(failed, ""Cannot find tombstone failure threshold error in log for {} node"".format((""upgraded"" if is_upgraded else ""old"")))
  File ""/usr/lib/python2.7/unittest/case.py"", line 422, in assertTrue
    raise self.failureException(msg)
""Cannot find tombstone failure threshold error in log for upgraded node
{code}

Seems to be a repeat of CASSANDRA-10869."
CASSANDRA-12513,IOException (No such file or directory) closing MessagingService's server socket (locally),"_Sometimes_ the {{RemoveTest}} fails with the following exception. It's not related to the test itself.

The exception is risen in {{ServerSocketChannelImpl.implCloseSelectableChannel}} where it checks that a thread ID is non-zero. The {{thread}} instance field is set inside its accept and poll methods. It looks like this is caused by some race condition - i.e. stopping in debugger at certain points prevents it from being triggered.
I could not find any misuse in the code base - but want to document this issue.

No difference between 8u92 and 8u102

{code}
INFO  [ACCEPT-/127.0.0.1] 2016-08-22 08:35:16,606 ?:? - MessagingService has terminated the accept() thread

java.io.IOError: java.io.IOException: No such file or directory

	at org.apache.cassandra.net.MessagingService.shutdown(MessagingService.java:914)
	at org.apache.cassandra.service.RemoveTest.tearDown(RemoveTest.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:37)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:159)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:117)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:42)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:262)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:84)
Caused by: java.io.IOException: No such file or directory
	at sun.nio.ch.NativeThread.signal(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.implCloseSelectableChannel(ServerSocketChannelImpl.java:292)
	at java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:234)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:115)
	at sun.nio.ch.ServerSocketAdaptor.close(ServerSocketAdaptor.java:137)
	at org.apache.cassandra.net.MessagingService$SocketThread.close(MessagingService.java:1249)
	at org.apache.cassandra.net.MessagingService.shutdown(MessagingService.java:904)
	... 22 more
{code}
"
CASSANDRA-12477,BackgroundCompaction causes Node crash (OutOfMemoryError),"After ingesting data, certain nodes of my cluster (2 out of 5) are not able to restart because Compaction fails with the following exception.

I was running a write-heavy ingestion before things started to break. The data size I was only 20GB but the ingestion speed was rather fast I guess. I ingested with the datastax C* java driver and used writeAsync to pump my BoundStatements to the Cluster. The ingestion client was running on a different node connected via GBit LAN. 

The nodes were unable to restart Cassandra.
I am using Cassandra 3.0.8. 

I was using untouched parameters for the heap size in cassandra-env.sh. 
After the nodes started failing to restart, I tried increasing MAX_JAVA_HEAP to 36gb and NEW_SIZE to 12gb but the memory will completely be consumed and then the exception will be thrown. 

{code}
java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:693) ~[na:1.8.0_91]
        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) ~[na:1.8.0_91]
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[na:1.8.0_91]
        at org.apache.cassandra.utils.memory.BufferPool.allocate(BufferPool.java:108) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.memory.BufferPool.access$1000(BufferPool.java:45) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.memory.BufferPool$LocalPool.allocate(BufferPool.java:387) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.memory.BufferPool$LocalPool.access$000(BufferPool.java:314) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.memory.BufferPool.takeFromPool(BufferPool.java:120) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.memory.BufferPool.get(BufferPool.java:92) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.util.RandomAccessReader.allocateBuffer(RandomAccessReader.java:87) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.access$100(CompressedRandomAccessReader.java:38) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader$Builder.createBuffer(CompressedRandomAccessReader.java:275) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:74) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:59) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader$Builder.build(CompressedRandomAccessReader.java:283) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.util.CompressedSegmentedFile.createReader(CompressedSegmentedFile.java:145) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.util.SegmentedFile.createReader(SegmentedFile.java:133) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getFileDataInput(SSTableReader.java:1711) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:93) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:46) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:36) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:62) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:580) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:492) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at com.stratio.cassandra.lucene.IndexService.read(IndexService.java:618) ~[cassandra-lucene-index-plugin-3.0.8.0.jar:na]
        at com.stratio.cassandra.lucene.IndexWriterWide.finish(IndexWriterWide.java:89) ~[cassandra-lucene-index-plugin-3.0.8.0.jar:na]
        at org.apache.cassandra.index.SecondaryIndexManager$IndexGCTransaction.commit(SecondaryIndexManager.java:958) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.compaction.CompactionIterator$1$1.onMergedRows(CompactionIterator.java:197) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer.getReduced(UnfilteredRowIterators.java:484) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer.getReduced(UnfilteredRowIterators.java:446) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:220) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:159) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:428) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:288) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:128) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.ColumnIndex$Builder.build(ColumnIndex.java:111) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.ColumnIndex.writeAndBuildIndex(ColumnIndex.java:52) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:149) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:57) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:182) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:78) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:263) ~[apache-cassandra-3.0.8.jar:3.0.8]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_91]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
{code}

I attached both debug.log and system.log to this issue. Both logs were created while heap parameters in cassandra-env.sh were unchanged. "
CASSANDRA-12436,Under some races commit log may incorrectly think it has unflushed data,"This can mainfest itself as a ""Failed to force-recycle all segments; at least one segment is still in use with dirty CFs."" message after CASSANDRA-11828."
CASSANDRA-12412,dtest failure in upgrade_tests.paging_test.TestPagingDataNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x.test_paging_using_secondary_indexes,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/22/testReport/upgrade_tests.paging_test/TestPagingDataNodes3RF3_Upgrade_current_3_0_x_To_indev_3_0_x/test_paging_using_secondary_indexes

{code}
ERROR [MessagingService-Incoming-/127.0.0.1] 2016-08-06 02:34:06,595 CassandraDaemon.java:201 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer.deserialize(ReadCommand.java:1042) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer.deserialize(ReadCommand.java:964) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.8.jar:3.0.8]

{code}"
CASSANDRA-12400,dtest failure in upgrade_tests.paging_test.TestPagingDatasetChangesNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x.test_row_TTL_expiry_during_paging,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/17/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x/test_row_TTL_expiry_during_paging/

{code}
Error Message

Lists differ: [300, 300, 131] != [300, 300, 200]

First differing element 2:
131
200

- [300, 300, 131]
?            ^^^

+ [300, 300, 200]
?      
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/paging_test.py"", line 1221, in test_row_TTL_expiry_during_paging
    self.assertEqual(pf.num_results_all(), [300, 300, 200])
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 742, in assertListEqual
    self.assertSequenceEqual(list1, list2, msg, seq_type=list)
  File ""/usr/lib/python2.7/unittest/case.py"", line 724, in assertSequenceEqual
    self.fail(msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
{code}"
CASSANDRA-12364,dtest failure in upgrade_tests.paging_test.TestPagingDatasetChangesNodes3RF3_Upgrade_current_3_x_To_indev_3_x.test_cell_TTL_expiry_during_paging,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/10/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes3RF3_Upgrade_current_3_x_To_indev_3_x/test_cell_TTL_expiry_during_paging

{code}
Error Message

Unexpected error in log, see stdout
{code}

{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:5051c0f6eb3f984600600c9577d6b5ece9038c74
Unexpected error in node1 log, error: 
ERROR [InternalResponseStage:4] 2016-07-28 03:23:02,097 CassandraDaemon.java:217 - Exception in thread Thread[InternalResponseStage:4,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) ~[na:1.8.0_51]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.ColumnFamilyStore.waitForFlushes(ColumnFamilyStore.java:930) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:892) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$1(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$200/2113926365.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_51]
	at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1271) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1253) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:92) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
{code}

Related failures:
http://cassci.datastax.com/job/trunk_dtest_upgrade/11/testReport/upgrade_tests.paging_test/TestPagingWithDeletionsNodes3RF3_Upgrade_current_3_x_To_indev_3_x/test_ttl_deletions/
http://cassci.datastax.com/job/trunk_dtest_upgrade/12/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/static_columns_with_distinct_test/
http://cassci.datastax.com/job/trunk_dtest_upgrade/12/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/refuse_in_with_indexes_test/"
CASSANDRA-12362,dtest failure in upgrade_tests.paging_test.TestPagingDatasetChangesNodes2RF1_Upgrade_current_3_x_To_indev_3_x.test_row_TTL_expiry_during_paging,"example failure:

http://cassci.datastax.com/job/trunk_dtest_upgrade/5/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes2RF1_Upgrade_current_3_x_To_indev_3_x/test_row_TTL_expiry_during_paging

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/paging_test.py"", line 1217, in test_row_TTL_expiry_during_paging
    self.assertEqual(pf.pagecount(), 3)
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""2 != 3
{code}"
CASSANDRA-12358,Slow PostFlush execution due to 2i flushing can cause near OOM to OOM,"2i can be slow to flush for a variety of reasons. Potentially slower than the rate at which Memtables can ingest and flush data. If this occurs the heap fills up with Memtables that are waiting for PostFlush to run.

This occurs because reclaiming the memory is done before PostFlush runs.

I will post a branch that has the reclaim memory task run after PostFlush has completed. As far as I can tell this is safe and correct since the memory is committed up until that point.

It's not clear to me if PostFlush has to bind the Memtables or not. I suspect it does, but I'm not sure if that is a route I should go down."
CASSANDRA-12251,"Move migration tasks to non-periodic queue, assure flush executor shutdown after non-periodic executor","example failure:

http://cassci.datastax.com/job/cassandra-3.8_dtest_upgrade/1/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/whole_list_conditional_test

Failed on CassCI build cassandra-3.8_dtest_upgrade #1

Relevant error in logs is
{code}
Unexpected error in node1 log, error: 
ERROR [InternalResponseStage:2] 2016-07-20 04:58:45,876 CassandraDaemon.java:217 - Exception in thread Thread[InternalResponseStage:2,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) ~[na:1.8.0_51]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_51]
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:842) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtableIfCurrent(ColumnFamilyStore.java:822) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:891) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$1(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$200/1129213153.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_51]
	at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1271) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1253) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:92) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
{code}
This is on a mixed 3.0.8, 3.8-tentative cluster"
CASSANDRA-12249,dtest failure in upgrade_tests.paging_test.TestPagingDataNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x.basic_paging_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.8_dtest_upgrade/1/testReport/upgrade_tests.paging_test/TestPagingDataNodes3RF3_Upgrade_current_3_0_x_To_indev_3_x/basic_paging_test

Failed on CassCI build cassandra-3.8_dtest_upgrade #1

This is on a mixed version cluster, one node is 3.0.8 and the other is 3.8-tentative.

Stack trace looks like:
{code}
ERROR [MessagingService-Incoming-/127.0.0.1] 2016-07-20 04:51:02,836 CassandraDaemon.java:201 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer.deserialize(ReadCommand.java:1042) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer.deserialize(ReadCommand.java:964) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.8.jar:3.0.8]
{code}
This trace is from the 3.0.8 node."
CASSANDRA-12221,Maximum Memory Usage Reached - NoSpamLogger,"I see some logs which look like :-

INFO  [SharedPool-Worker-22] 2016-07-17 22:03:02,725 NoSpamLogger.java:91 - Maximum memory usage reached (536870912 bytes), cannot allocate chunk of 1048576 bytes
INFO  [SharedPool-Worker-33] 2016-07-17 22:18:02,747 NoSpamLogger.java:91 - Maximum memory usage reached (536870912 bytes), cannot allocate chunk of 1048576 bytes
INFO  [SharedPool-Worker-35] 2016-07-17 22:33:02,829 NoSpamLogger.java:91 - Maximum memory usage reached (536870912 bytes), cannot allocate chunk of 1048576 bytes
INFO  [SharedPool-Worker-31] 2016-07-17 22:48:02,834 NoSpamLogger.java:91 - Maximum memory usage reached (536870912 bytes), cannot allocate chunk of 1048576 bytes

When i get these logs, CPU usage is quite high on the system.
Are they expected? Log message also seems confusing and i am not sure what memory we are talking about here.."
CASSANDRA-12198,Deadlock in CDC during segment flush,"In the patch for CASSANDRA-8844, we added a {{synchronized(this)}} block inside CommitLogSegment.setCDCState. This introduces the possibility of deadlock in the following scenario:
# A {{CommitLogSegment.sync()}} call is made (synchronized method)
# A {{CommitLogSegment.allocate}} call from a cdc-enabled write is in flight and acquires a reference to the Group on appendOrder (the OpOrder in the Segment)
# {{CommmitLogSegment.sync}} hits {{waitForModifications}} which calls {{appendOrder.awaitNewBarrier}}
# The in-flight write, if changing the state of the segment from CDCState.PERMITTED to CDCState.CONTAINS, enters {{setCDCState}} and blocks on synchronized(this)

And neither of them ever come back. This came up while doing some further work on CASSANDRA-12148.
"
CASSANDRA-12143,NPE when trying to remove purgable tombstones from result,"A cluster running 2.2.6 started throwing NPEs.
(500K exceptions on a node was seen.)

{noformat}WARN  … AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-5,5,main]: {}
java.lang.NullPointerException: null{noformat}

Bisecting this highlighted commit d3db33c008542c7044f3ed8c19f3a45679fcf52e as the culprit, which was a fix for CASSANDRA-11427.

This commit added a line to ""remove purgable tombstones from result"" but failed to null check the {{data}} variable first. This variable comes from {{Row.cf}} which is permitted to be null where the CFS has no data."
CASSANDRA-12083,Race condition during system.roles column family creation,"There is an issue where Cassandra fails with the following exception on startup:

{noformat}
DEBUG [InternalResponseStage:2] 2016-06-20 09:43:00,651 Schema.java:465 - Adding org.apache.cassandra.config.CFMetaData@2882b66d[cfId=5bc52802-de25-35ed-aeab-188eecebb090,ksName=system_auth,cfName=roles,flags=[COMPOUND],params=TableParams{comment=role definitions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={max_threshold=32, min_threshold=4}}, compression=org.apache.cassandra.schema.CompressionParams@e6e0212, extensions={}},comparator=comparator(),partitionColumns=[[] | [can_login is_superuser salted_hash member_of]],partitionKeyColumns=[ColumnDefinition{name=role, type=org.apache.cassandra.db.marshal.UTF8Type, kind=PARTITION_KEY, position=0}],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[ColumnDefinition{name=role, type=org.apache.cassandra.db.marshal.UTF8Type, kind=PARTITION_KEY, position=0}, ColumnDefinition{name=salted_hash, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, position=-1}, ColumnDefinition{name=member_of, type=org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, position=-1}, ColumnDefinition{name=can_login, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, position=-1}, ColumnDefinition{name=is_superuser, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, position=-1}],droppedColumns={},triggers=[],indexes=[]] to cfIdMap
INFO  [InternalResponseStage:2] 2016-06-20 09:43:00,653 ColumnFamilyStore.java:381 - Initializing system_auth.roles
DEBUG [InternalResponseStage:2] 2016-06-20 09:43:00,664 MigrationManager.java:556 - Gossiping my schema version c2a2bb4f-7d31-3fb8-a216-00b41a643650
DEBUG [InternalResponseStage:1] 2016-06-20 09:43:00,669 ColumnFamilyStore.java:831 - Enqueuing flush of keyspaces: 1566 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:2] 2016-06-20 09:43:00,669 Memtable.java:372 - Writing Memtable-keyspaces@650010305(0.437KiB serialized bytes, 3 ops, 0%/0% of on/off-heap limit)
ERROR [main] 2016-06-20 09:43:00,670 CassandraDaemon.java:692 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 5bc52802-de25-35ed-aeab-188eecebb090
        at org.apache.cassandra.db.Keyspace.getColumnFamilyStore(Keyspace.java:206) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.db.Keyspace.getColumnFamilyStore(Keyspace.java:199) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:168) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepareRestrictions(SelectStatement.java:874) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:821) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:809) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.auth.CassandraRoleManager.prepare(CassandraRoleManager.java:446) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.auth.CassandraRoleManager.setup(CassandraRoleManager.java:144) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.service.StorageService.doAuthSetup(StorageService.java:1084) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1032) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:755) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:620) ~[apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:333) [apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:551) [apache-cassandra-3.0.6.jar:3.0.6]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:679) [apache-cassandra-3.0.6.jar:3.0.6]
{noformat}

A quick glance at code suggests that this race is possible: 
{noformat}
In Keyspace.java
    public ColumnFamilyStore getColumnFamilyStore(String cfName)
    {
        UUID id = Schema.instance.getId(getName(), cfName);
        if (id == null)
            throw new IllegalArgumentException(String.format(""Unknown keyspace/cf pair (%s.%s)"", getName(), cfName));
        return getColumnFamilyStore(id);
    }

    public ColumnFamilyStore getColumnFamilyStore(UUID id)
    {
        ColumnFamilyStore cfs = columnFamilyStores.get(id);
        if (cfs == null)
            throw new IllegalArgumentException(""Unknown CF "" + id);
        return cfs;
    }
{noformat}

{noformat}
In Schema.java:
 public void addTable(CFMetaData cfm)
    {
        assert getCFMetaData(cfm.ksName, cfm.cfName) == null;

        // Make sure the keyspace is initialized
        Keyspace.open(cfm.ksName);
        // Update the keyspaces map with the updated metadata
        update(cfm.ksName, ks -> ks.withSwapped(ks.tables.with(cfm)));
        // Update the table ID <-> table name map (cfIdMap)
        load(cfm);

        // init the new CF before switching the KSM to the new one
        // to avoid races as in CASSANDRA-10761
        Keyspace.open(cfm.ksName).initCf(cfm, true);
        MigrationManager.instance.notifyCreateColumnFamily(cfm);
    }

    public void load(CFMetaData cfm)
    {
        Pair<String, String> key = Pair.create(cfm.ksName, cfm.cfName);

        if (cfIdMap.containsKey(key))
            throw new RuntimeException(String.format(""Attempting to load already loaded table %s.%s"", cfm.ksName, cfm.cfName));

        logger.debug(""Adding {} to cfIdMap"", cfm);
        cfIdMap.put(key, cfm.cfId);
    }
{noformat}

So something can be added to the cfIdMap and not be present in the columnFamilyStores (which is added in Keyspace.open(cfm.ksName).initCf(cfm, true); step). "
CASSANDRA-12077,NPE when trying to get sstables for anticompaction,This was introduced in CASSANDRA-11739 - we need to avoid trying to get sstables for tables we are not repairing
CASSANDRA-12071,Regression in flushing throughput under load after CASSANDRA-6696,"The way flushing used to work is that a ColumnFamilyStore could have multiple Memtables flushing at once and multiple ColumnFamilyStores could flush at the same time. The way it works now there can be only a single flush of any ColumnFamilyStore & Memtable running in the C* process, and the number of threads applied to that flush is bounded by the number of disks in JBOD.

This works ok most of the time but occasionally flushing will be a little slower and ingest will outstrip it and then block on available memory. At this point you see several second stalls that cause timeouts.

This is a problem for reasonable configurations that don't use JBOD but have access to a fast disk that can handle some IO queuing (RAID, SSD).

You can reproduce on beefy hardware (12 cores 24 threads, 64 gigs of RAM, SSD) if you unthrottle compaction or set it to something like 64 megabytes/second and run with 8 compaction threads and stress with the default write workload and a reasonable number of threads. I tested with 96.

It started happening after about 60 gigabytes of data was loaded."
CASSANDRA-12068,dtest failure in paging_test.TestPagingData.static_columns_paging_test,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/261/testReport/paging_test/TestPagingData/static_columns_paging_test

Failed on CassCI build trunk_offheap_dtest #261

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 715, in static_columns_paging_test
    self.assertEqual(16, len(results))
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 506, in _baseAssertEqual
    raise self.failureException(msg)
""16 != 6
{code}"
CASSANDRA-12034,Special handling for Netty's direct memory allocation failure,"With CASSANDRA-12032, Netty throws a {{io.netty.util.internal.OutOfDirectMemoryError}} if there's not enough off-heap memory for the response buffer. We can easily handle this situation and return an error. This is not a condition that destabilizes the system and should therefore not passed to {{JVMStabilityInspector}}."
CASSANDRA-12031,"""LEAK DETECTED"" during incremental repairs","I encountered some errors during an incremental repair session which look like  :-

ERROR [Reference-Reaper:1] 2016-06-19 03:28:35,884 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2ce0fab3) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1513857473:Memory@[7f2d462191f0..7f2d46219510) was not released before the reference was garbage collected

Should i be worried about these? "
CASSANDRA-12025,dtest failure in paging_test.TestPagingData.test_paging_with_filtering_on_counter_columns,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1276/testReport/paging_test/TestPagingData/test_paging_with_filtering_on_counter_columns

Failed on CassCI build trunk_dtest #1276

{code}
Error Message

Lists differ: [[4, 7, 8, 9], [4, 9, 10, 11]] != [[4, 7, 8, 9], [4, 8, 9, 10], ...

First differing element 1:
[4, 9, 10, 11]
[4, 8, 9, 10]

Second list contains 1 additional elements.
First extra element 2:
[4, 9, 10, 11]

- [[4, 7, 8, 9], [4, 9, 10, 11]]
+ [[4, 7, 8, 9], [4, 8, 9, 10], [4, 9, 10, 11]]
?                    +++  ++++++++++++
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools.py"", line 288, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 1148, in test_paging_with_filtering_on_counter_columns
    self._test_paging_with_filtering_on_counter_columns(session, True)
  File ""/home/automaton/cassandra-dtest/paging_test.py"", line 1107, in _test_paging_with_filtering_on_counter_columns
    [4, 9, 10, 11]])
  File ""/usr/lib/python2.7/unittest/case.py"", line 513, in assertEqual
    assertion_func(first, second, msg=msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 742, in assertListEqual
    self.assertSequenceEqual(list1, list2, msg, seq_type=list)
  File ""/usr/lib/python2.7/unittest/case.py"", line 724, in assertSequenceEqual
    self.fail(msg)
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
{code}
Logs are attached."
CASSANDRA-12016,Create MessagingService mocking classes,"Interactions between clients and nodes in the cluster are taking place by exchanging messages through the {{MessagingService}}. Black box testing for message based systems is usually pretty easy, as we're just dealing with messages in/out. My suggestion would be to add tests that make use of this fact by mocking message exchanges via MessagingService. Given the right use case, this would turn out to be a much simpler and more efficient alternative for dtests."
CASSANDRA-12012,CQLSSTableWriter and composite clustering keys trigger NPE,"It triggers when using multiple clustering keys in the primary keys

{code}
package tests;

import java.io.File;
import org.apache.cassandra.io.sstable.CQLSSTableWriter;
import org.apache.cassandra.config.Config;

public class DefaultWriter {
    
    public static void main(String[] args) throws Exception {
        Config.setClientMode(true);
        
        String createTableQuery = ""CREATE TABLE ks_test.table_test (""
        + ""    pk1 int,""
        + ""    ck1 int,""
        + ""    ck2 int,""
        + ""    PRIMARY KEY ((pk1), ck1, ck2)""
        + "");"";
        String insertQuery = ""INSERT INTO ks_test.table_test(pk1, ck1, ck2) VALUES(?,?,?)"";
        
        CQLSSTableWriter writer = CQLSSTableWriter.builder()
            .inDirectory(Files.createTempDirectory(""sst"").toFile())
            .forTable(createTableQuery)
            .using(insertQuery)
            .build();
        writer.close();
    }
}
{code}

Exception : 

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:368)
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:305)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:129)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:106)
	at org.apache.cassandra.db.Keyspace.openAndGetStore(Keyspace.java:159)
	at org.apache.cassandra.cql3.restrictions.PrimaryKeyRestrictionSet.hasSupportingIndex(PrimaryKeyRestrictionSet.java:156)
	at org.apache.cassandra.cql3.restrictions.PrimaryKeyRestrictionSet.<init>(PrimaryKeyRestrictionSet.java:118)
	at org.apache.cassandra.cql3.restrictions.PrimaryKeyRestrictionSet.mergeWith(PrimaryKeyRestrictionSet.java:213)
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.addSingleColumnRestriction(StatementRestrictions.java:266)
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.addRestriction(StatementRestrictions.java:250)
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:159)
	at org.apache.cassandra.cql3.statements.UpdateStatement$ParsedInsert.prepareInternal(UpdateStatement.java:183)
	at org.apache.cassandra.cql3.statements.ModificationStatement$Parsed.prepare(ModificationStatement.java:782)
	at org.apache.cassandra.cql3.statements.ModificationStatement$Parsed.prepare(ModificationStatement.java:768)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:505)
	at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.getStatement(CQLSSTableWriter.java:508)
	at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.using(CQLSSTableWriter.java:439)
	at tests.DefaultWriter.main(DefaultWriter.java:29)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.getFlushWriters(DatabaseDescriptor.java:1188)
	at org.apache.cassandra.db.ColumnFamilyStore.<clinit>(ColumnFamilyStore.java:127)
	... 18 more
{code}"
CASSANDRA-12011,Logback shutdown hook races with StorageServiceShutdownHook,"In {{StorageService}}, we add a JVM shutdown hook called {{StorageServiceShutdownHook}} that shuts down gossip and flushes writes for non durable keyspaces, among other things. 

In {{logback.xml}}, we add a JVM shutdown hook that tears down logback resources.

Since JVM shutdown hooks are started concurrently, the logback shutdown hook almost always completes during the {{StorageServiceShutdownHook}}, which means that any log statements in this hook will not work since the logback resources have been torn down.

This makes debugging the {{StorageServiceShutdownHook}} challenging. We should do our own logback teardown that does not run until after the completion of any Cassandra-oriented JVM shutdown hooks such as the {{StorageServiceShutdownHook}}.

"
CASSANDRA-11985,2.0.x leaks file handles (Again),"We are running Cassandra 2.0.14 in production environment and disk usage is very high. On investigating it further we found that there are around 4-5 files(~ 150 GB) in stuck mode.

Command Fired : lsof /var/lib/cassandra | grep -i deleted 

Output : 

java    12158 cassandra  308r   REG   8,16  34396638044 12727268 /var/lib/cassandra/data/mykeyspace/mycolumnfamily/mykeyspace-mycolumnfamily-jb-16481-Data.db (deleted)
java    12158 cassandra  327r   REG   8,16 101982374806 12715102 /var/lib/cassandra/data/mykeyspace/mycolumnfamily/mykeyspace-mycolumnfamily-jb-126861-Data.db (deleted)
java    12158 cassandra  339r   REG   8,16  12966304784 12714010 /var/lib/cassandra/data/mykeyspace/mycolumnfamily/mykeyspace-mycolumnfamily-jb-213548-Data.db (deleted)
java    12158 cassandra  379r   REG   8,16  15323318036 12714957 /var/lib/cassandra/data/mykeyspace/mycolumnfamily/mykeyspace-mycolumnfamily-jb-182936-Data.db (deleted)

we are not able to see these files in any directory. This is somewhat similar to CASSANDRA-6275 which is fixed but still issue is there on higher version. Also in logs no error related to compaction is reported.

so could any one of you please provide any suggestion how to counter this. Restarting Cassandra is one solution but this issue keeps on occurring so we cannot restart production machine is not recommended so frequently.

Also we know that this version is not supported but there is high probability that it can occur in higher version too."
CASSANDRA-11973,Is MemoryUtil.getShort() supposed to return a sign-extended or non-sign-extended value?,"In org.apache.cassandra.utils.memory.MemoryUtil.getShort(), the returned value of unsafe.getShort(address) is bit-wise-AND'ed with 0xffff, while that of getShortByByte(address) is not. This inconsistency results in different returned values when the short integer is negative. Which is preferred behavior? Looking at NativeClustering and NativeCellTest, it seems like non-sign-extension is assumed.

By the way, is there any reason MemoryUtil.getShort() and MemoryUtil.getShortByByte() return ""int"", not ""short""?"
CASSANDRA-11962,Non-fatal NPE during concurrent repair check,"""Usual"" checks for multiple repairs result in this exception in the log file:
{code}
ERROR [ValidationExecutor:6] 2016-06-06 07:56:23,530 CassandraDaemon.java:222 - Exception in thread Thread[ValidationExecutor:6,1,main]
java.lang.RuntimeException: Cannot start multiple repair sessions over the same sstables
        at org.apache.cassandra.db.compaction.CompactionManager.getSSTablesToValidate(CompactionManager.java:1325) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1215) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:81) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager$11.call(CompactionManager.java:844) ~[main/:na]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_91]
{code}

However, I saw this one:

{code}
ERROR [ValidationExecutor:6] 2016-06-06 07:56:25,002 CassandraDaemon.java:222 - Exception in thread Thread[ValidationExecutor:6,1,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.service.ActiveRepairService$ParentRepairSession.getActiveSSTables(ActiveRepairService.java:495) ~[main/:na]
        at org.apache.cassandra.service.ActiveRepairService$ParentRepairSession.access$300(ActiveRepairService.java:451) ~[main/:na]
        at org.apache.cassandra.service.ActiveRepairService.currentlyRepairing(ActiveRepairService.java:338) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager.getSSTablesToValidate(CompactionManager.java:1320) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1215) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:81) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager$11.call(CompactionManager.java:844) ~[main/:na]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_91]
{code}

Looks like there is no entry for {{cfId}} in {{getActiveSStables}} at {{for (SSTableReader sstable : columnFamilyStores.get(cfId).getSSTables())}}.

(against trunk)"
CASSANDRA-11961,Nonfatal NPE in CompactionMetrics,"Just saw the following NPE on trunk. Means, that {{metaData}} from {{CFMetaData metaData = compaction.getCompactionInfo().getCFMetaData();}} is {{null}}. A simple {{if (metaData == null) continue;}} should fix this.

{code}
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.metrics.CompactionMetrics$2.getValue(CompactionMetrics.java:103) ~[main/:na]
	at org.apache.cassandra.metrics.CompactionMetrics$2.getValue(CompactionMetrics.java:78) ~[main/:na]
{code}"
CASSANDRA-11904,"Exception in thread Thread[CompactionExecutor:13358,1,main] java.lang.AssertionError: Memory was freed","We have Cassandra cluster 2.2.5 with two datacenters(3 nodes each).
We observe ERRORs below on all nodes. The ERROR is repeated every minute. 

No any complains from customers. Do we have any chance to fix it without restart?

{code}
ERROR [CompactionExecutor:13996] 2016-05-26 21:20:46,700 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:13996,1,main]
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:103) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:292) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.IndexSummary.getPositionInSummary(IndexSummary.java:148) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.IndexSummary.fillTemporaryKey(IndexSummary.java:162) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:121) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:1398) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.estimatedKeysForRanges(SSTableReader.java:1354) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:403) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:412) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:101) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getNextBackgroundTask(WrappingCompactionStrategy.java:88) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:250) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_74]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_74]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_74]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_74]
ERROR [CompactionExecutor:13996] 2016-05-26 21:21:46,702 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:13996,1,main]
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:103) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:292) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.IndexSummary.getPositionInSummary(IndexSummary.java:148) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.IndexSummary.fillTemporaryKey(IndexSummary.java:162) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:121) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:1398) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.SSTableReader.estimatedKeysForRanges(SSTableReader.java:1354) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:403) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:412) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:101) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getNextBackgroundTask(WrappingCompactionStrategy.java:88) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:250) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_74]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_74]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_74]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_74]
{code}"
CASSANDRA-11889,LogRecord: file system race condition may cause verify() to fail,"The following exception was reported in CASSANDRA-11470. It occurred whilst listing files with compaction in progress:

{code}
WARN  [CompactionExecutor:2006] 2016-05-23 18:23:31,694 BigTableWriter.java:171 - Writing large partition test_keyspace/test_columnfamily:eda6b9c36f8df6fe596492c3438d7a38e9b109a6 (123663388 bytes)
INFO  [IndexSummaryManager:1] 2016-05-23 18:24:23,731 IndexSummaryRedistribution.java:74 - Redistributing index summaries
WARN  [CompactionExecutor:2006] 2016-05-23 18:24:56,669 BigTableWriter.java:171 - Writing large partition test_keyspace/test_columnfamily:05b6b424194dd19ab7cfbcd53c4979768cd859e9 (256286063 bytes)
WARN  [CompactionExecutor:2006] 2016-05-23 18:26:23,575 BigTableWriter.java:171 - Writing large partition test_keyspace/test_columnfamily:04e9fac15552b9ae77c27a6cb8d0fd11fdcc24d7 (212445557 bytes)
INFO  [CompactionExecutor:2005] 2016-05-23 18:29:26,839 LeveledManifest.java:437 - Adding high-level (L3) BigTableReader(path='/data/cassandra/data/test_keyspace/test_columnfamily_2-d29dd71045a811e59aff6776bf484396/ma-61041-big-Data.db') to candidates
WARN  [CompactionExecutor:2006] 2016-05-23 18:30:34,154 BigTableWriter.java:171 - Writing large partition test_keyspace/test_columnfamily:edbe6f178503be90911dbf29a55b97a4b095a9ec (183852539 bytes)
INFO  [CompactionExecutor:2006] 2016-05-23 18:31:21,080 LeveledManifest.java:437 - Adding high-level (L3) BigTableReader(path='/data/cassandra/data/test_keyspace/test_columnfamily_2-d29dd71045a811e59aff6776bf484396/ma-61042-big-Data.db') to candidates
ERROR [metrics-graphite-reporter-1-thread-1] 2016-05-23 18:31:21,207 LogFile.java:173 - Unexpected files detected for sstable [ma-91034-big], record [REMOVE:[/data/cassandra/data/test_keyspace/test_columnfamily-3996ce80b7ac11e48a9b6776bf484396/ma-91034-big,1463992176000,8][457420186]]: last update time [00:00:00] should have been [08:29:36]
ERROR [metrics-graphite-reporter-1-thread-1] 2016-05-23 18:31:21,208 ScheduledReporter.java:119 - RuntimeException thrown from GraphiteReporter#report. Exception was suppressed.
java.lang.RuntimeException: Failed to list files in /data/cassandra/data/test_keyspace/test_columnfamily-3996ce80b7ac11e48a9b6776bf484396
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:57) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:547) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:691) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.Directories$SSTableLister.listFiles(Directories.java:662) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.Directories$TrueFilesSizeVisitor.<init>(Directories.java:981) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.Directories.getTrueAllocatedSizeIn(Directories.java:893) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.Directories.trueSnapshotsSize(Directories.java:883) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.ColumnFamilyStore.trueSnapshotsSize(ColumnFamilyStore.java:2332) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.metrics.TableMetrics$32.getValue(TableMetrics.java:637) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.metrics.TableMetrics$32.getValue(TableMetrics.java:634) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.metrics.TableMetrics$33.getValue(TableMetrics.java:692) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.metrics.TableMetrics$33.getValue(TableMetrics.java:686) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at com.codahale.metrics.graphite.GraphiteReporter.reportGauge(GraphiteReporter.java:281) ~[metrics-graphite-3.1.0.jar:3.1.0]
	at com.codahale.metrics.graphite.GraphiteReporter.report(GraphiteReporter.java:158) ~[metrics-graphite-3.1.0.jar:3.1.0]
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162) ~[metrics-core-3.1.0.jar:3.1.0]
	at com.codahale.metrics.ScheduledReporter$1.run(ScheduledReporter.java:117) ~[metrics-core-3.1.0.jar:3.1.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_91]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: org.apache.cassandra.db.lifecycle.LogTransaction$CorruptTransactionLogException: Some records failed verification. See earlier in log for details.
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.readTxnLog(LogAwareFileLister.java:114) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.classifyFiles(LogAwareFileLister.java:106) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184) ~[na:1.8.0_91]
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[na:1.8.0_91]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) ~[na:1.8.0_91]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[na:1.8.0_91]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[na:1.8.0_91]
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151) ~[na:1.8.0_91]
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174) ~[na:1.8.0_91]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_91]
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) ~[na:1.8.0_91]
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:75) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:53) ~[apache-cassandra-3.0.6.jar:3.0.6]
	... 22 common frames omitted
INFO  [CompactionExecutor:2005] 2016-05-23 18:32:27,284 LeveledManifest.java:437 - Adding high-level (L3) BigTableReader(path='/data/cassandra/data/test_keyspace/test_columnfamily_3-37a9cb90b7ac11e48a9b6776bf484396/ma-98189-big-Data.db') to candidates
INFO  [CompactionExecutor:2005] 2016-05-23 18:34:42,562 LeveledManifest.java:437 - Adding high-level (L3) BigTableReader(path='/data/cassandra/data/test_keyspace/test_columnfamily_3-37a9cb90b7ac11e48a9b6776bf484396/ma-98210-big-Data.db') to candidates
{code}

The error of interest is:

{code}
ERROR [metrics-graphite-reporter-1-thread-1] 2016-05-23 18:31:21,207 LogFile.java:173 - Unexpected files detected for sstable [ma-91034-big], record [REMOVE:[/data/cassandra/data/test_keyspace/test_columnfamily-3996ce80b7ac11e48a9b6776bf484396/ma-91034-big,1463992176000,8][457420186]]: last update time [00:00:00] should have been [08:29:36]
{code}

The problem is this code here in LogRecord:

{code}
    public LogRecord withExistingFiles()
    {
        return make(type, getExistingFiles(), 0, absolutePath.get());
    }

    public static LogRecord make(Type type, List<File> files, int minFiles, String absolutePath)
    {
        long lastModified = files.stream().map(File::lastModified).reduce(0L, Long::max);
        return new LogRecord(type, absolutePath, lastModified, Math.max(minFiles, files.size()));
    }
{code}

If the sstable files are completely deleted after getExistingFiles() has returned them and before lastModified is calculated, then we may report last update time as zero because the files no longer exist and still indicate that there are files on disk so that this code in LogFile.verifyRecord fails:

{code}
        record.status.onDiskRecord = record.withExistingFiles();
        if (record.updateTime != record.status.onDiskRecord.updateTime && record.status.onDiskRecord.numFiles > 0)
        {
            record.setError(String.format(""Unexpected files detected for sstable [%s], "" +
                                          ""record [%s]: last update time [%tT] should have been [%tT]"",
                                          record.fileName(),
                                          record,
                                          record.status.onDiskRecord.updateTime,
                                          record.updateTime));

        }
{code}

As a consequence, we throw an exception when listing files. In this case we were calculating the true snapshot size for the metrics.

Note that we are careful to delete files from oldest to newest so that the last update time should always match what is stored in the txn log, however if all files are deleted then we report last update time as zero. So, if my analysis is correct, this problem should not be very frequent."
CASSANDRA-11883,dtest failure in paging_test.TestPagingWithDeletions.test_single_cell_deletions,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest_jdk8/223/testReport/paging_test/TestPagingWithDeletions/test_single_cell_deletions

Failed on CassCI build cassandra-2.1_dtest_jdk8 #223

Logs are attached."
CASSANDRA-11867,Possible memory leak in NIODataInputStream,{{NIODataInputStream}} allocates direct memory but does not clean it on close. Proposed patch adds a call to {{FileUtils.clean()}} for this direct memory buffer.
CASSANDRA-11855,MessagingService#getCommandDroppedTasks should be displayed in netstats,MessagingService#getCommandDroppedTasks should be displayed in netstats along with the pending and completed.
CASSANDRA-11854,Remove finished streaming connections from MessagingService,"When a new {{IncomingStreamingConnection}} is created, [we register it in the connections map|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/MessagingService.java#L1109] of {{MessagingService}}, but we [only remove it if there is an exception|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/IncomingStreamingConnection.java#L83] while attaching the socket to the stream session.

On nodes with SSL and large number of vnodes, after many repair sessions these old connections can accumulate and cause OOM (heap dump attached).

The connection should be removed from the connections map after if it's finished in order to be garbage collected."
CASSANDRA-11830,"""nodetool flush"" not flushing system keyspace","I'm regularly splitting off maintenance systems from our QA cluster by adding a new node to a new ""datacenter"", joining it, then stopping and removing it (adapting the schema before and after accordingly).

In order to not mix up the systems I rename the cluster in the newly created maintenance system by updating cluster_name in system.local

In the past, when running ""nodetool flush"" then restarting Cassandra, this worked as expected.

However, this time it did not. After restarting Cassandra the old cluster name was in place every time. However, explicitly flushing the system keyspace using ""nodetool flush system"" did work as expected.

Is this a bug or did the default behaviour of ""nodetool flush"" change at some point?"
CASSANDRA-11828,Commit log needs to track unflushed intervals rather than positions,"In CASSANDRA-11448 in an effort to give a more thorough handling of flush errors I have introduced a possible correctness bug with disk failure policy ignore if a flush fails with an error:
- we report the error but continue
- we correctly do not update the commit log with the flush position
- but we allow the post-flush executor to resume
- a successful later flush can thus move the log's clear position beyond the data from the failed flush
- the log will then delete segment(s) that contain unflushed data.

After CASSANDRA-9669 it is relatively easy to fix this problem by making the commit log track sets of intervals of unflushed data (as described in CASSANDRA-8496)."
CASSANDRA-11823,Creating a table leads to a race with GraphiteReporter,"Happened only on 3/4 nodes out of 13.

{code:xml}
INFO  [MigrationStage:1] 2016-05-18 00:34:11,566 ColumnFamilyStore.java:381 - Initializing schema.table
ERROR [metrics-graphite-reporter-1-thread-1] 2016-05-18 00:34:11,569 ScheduledReporter.java:119 - RuntimeException thrown from GraphiteReporter#report. Exception was suppressed.
java.util.ConcurrentModificationException: null
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1429) ~[na:1.8.0_91]
	at java.util.HashMap$KeyIterator.next(HashMap.java:1453) ~[na:1.8.0_91]
	at org.apache.cassandra.metrics.TableMetrics$33.getValue(TableMetrics.java:690) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at org.apache.cassandra.metrics.TableMetrics$33.getValue(TableMetrics.java:686) ~[apache-cassandra-3.0.6.jar:3.0.6]
	at com.codahale.metrics.graphite.GraphiteReporter.reportGauge(GraphiteReporter.java:281) ~[metrics-graphite-3.1.0.jar:3.1.0]
	at com.codahale.metrics.graphite.GraphiteReporter.report(GraphiteReporter.java:158) ~[metrics-graphite-3.1.0.jar:3.1.0]
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162) ~[metrics-core-3.1.0.jar:3.1.0]
	at com.codahale.metrics.ScheduledReporter$1.run(ScheduledReporter.java:117) ~[metrics-core-3.1.0.jar:3.1.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_91]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
{code}"
CASSANDRA-11818,C* does neither recover nor trigger stability inspector on direct memory OOM,"The following stack trace is not caught by {{JVMStabilityInspector}}.
Situation was caused by a load test with a lot of parallel writes and reads against a single node.

{code}
ERROR [SharedPool-Worker-1] 2016-05-17 18:38:44,187 Message.java:611 - Unexpected exception during request; channel = [id: 0x1e02351b, L:/127.0.0.1:9042 - R:/127.0.0.1:51087]
java.lang.OutOfMemoryError: Direct buffer memory
	at java.nio.Bits.reserveMemory(Bits.java:693) ~[na:1.8.0_92]
	at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) ~[na:1.8.0_92]
	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[na:1.8.0_92]
	at io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:672) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:234) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.buffer.PoolArena.allocate(PoolArena.java:218) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.buffer.PoolArena.allocate(PoolArena.java:138) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:270) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:177) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:168) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.buffer.AbstractByteBufAllocator.buffer(AbstractByteBufAllocator.java:105) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at org.apache.cassandra.transport.Message$ProtocolEncoder.encode(Message.java:349) ~[main/:na]
	at org.apache.cassandra.transport.Message$ProtocolEncoder.encode(Message.java:314) ~[main/:na]
	at io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:89) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:619) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:676) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:612) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at org.apache.cassandra.transport.Message$Dispatcher$Flusher.run(Message.java:445) ~[main/:na]
	at io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:120) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:358) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:374) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) ~[netty-all-4.0.36.Final.jar:4.0.36.Final]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_92]
{code}

The situation does not get better when the load driver is stopped.

I can reproduce this scenario at will. Managed to get histogram, stack traces and heap dump. Already increased {{-XX:MaxDirectMemorySize}} to {{2g}}.

A {{nodetool flush}} causes the daemon to exit (as that direct-memory OOM is caught by {{JVMStabilityInspector}})."
CASSANDRA-11751,Histogram overflow in metrics,"One particular histogram in the cassandra metrics seems to overflow preventing the calculation of the mean on the dropwizard ""Snapshot"". Here is the exception that comes from the metrics library:

{code}
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
        at org.apache.cassandra.utils.EstimatedHistogram.rawMean(EstimatedHistogram.java:232) ~[apache-cassandra-2.2.6.jar:2.2.6-SNAPSHOT]
        at org.apache.cassandra.metrics.EstimatedHistogramReservoir$HistogramSnapshot.getMean(EstimatedHistogramReservoir.java:103) ~[apache-cassandra-2.2.6.jar:2.2.6-SNAPSHOT]
        at com.addthis.metrics3.reporter.config.SplunkReporter.reportHistogram(SplunkReporter.java:155) ~[reporter-config3-3.0.0.jar:3.0.0]
        at com.addthis.metrics3.reporter.config.SplunkReporter.report(SplunkReporter.java:101) ~[reporter-config3-3.0.0.jar:3.0.0]
        at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162) ~[metrics-core-3.1.0.jar:3.1.0]
        at com.codahale.metrics.ScheduledReporter$1.run(ScheduledReporter.java:117) ~[metrics-core-3.1.0.jar:3.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_72]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_72]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_72]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_72]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_72]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72]
{code}

On deeper analysis, it seems like this is happening specifically on this metric:
{code}
ColUpdateTimeDeltaHistogram
{code}

I think this is where it is updated in ColumnFamilyStore.java
{code}
    public void apply(DecoratedKey key, ColumnFamily columnFamily, SecondaryIndexManager.Updater indexer, OpOrder.Group opGroup, ReplayPosition replayPosition)
    {
        long start = System.nanoTime();
        Memtable mt = data.getMemtableFor(opGroup, replayPosition);
        final long timeDelta = mt.put(key, columnFamily, indexer, opGroup);
        maybeUpdateRowCache(key);
        metric.samplers.get(Sampler.WRITES).addSample(key.getKey(), key.hashCode(), 1);
        metric.writeLatency.addNano(System.nanoTime() - start);
        if(timeDelta < Long.MAX_VALUE)
            metric.colUpdateTimeDeltaHistogram.update(timeDelta);
    }
{code}

Considering it's calculating a mean, i don't know if perhaps a large sum might be overflowing? But that ""if (timeDelta < Long.MAX_VALUE)"" looks suspect, doesn't it?
"
CASSANDRA-11743,Race condition in CommitLog.recover can prevent startup,"In {{CommitLog::recover}} the list of segments to recover from is determined by removing the files managed by the {{CommitLogSegmentManager}} from the list of files present in the commit log directory. Unfortunatly, due to the way the creation of segments is done there is a time window where a segment file has been created but has not been added yet to the list of segments managed by the {{CommitLogSegmentManager}}. If the filtering ocurs during that time window the Commit log might try to recover from that new segment and crash.   "
CASSANDRA-11719,Add bind variables to trace,"{{org.apache.cassandra.transport.messages.ExecuteMessage#execute}} mentions a _TODO_ saying ""we don't have [typed] access to CQL bind variables here"".
In fact, we now have access typed access to CQL bind variables there. So, it is now possible to show the bind variables in the trace."
CASSANDRA-11680,Inconsistent data while paging through a table,"We have the following table structure:
CREATE TABLE keyspace.book_properties (
book_id text,
group_id bigint,
property_display_name text,
created timestamp,
property_name text,
property_uuid uuid,
property_value text,
updated timestamp,
PRIMARY KEY (book_id, group_id, property_display_name)
) WITH CLUSTERING ORDER BY (group_id ASC, property_display_name ASC);

We have lucene indexes on group_id, property_display_name, created, property_name, property_uuid, updated

When we run a full table scan. Below is the sample code snippet

boundStatement = new BoundStatement(session.prepare(""select * from keyspace.book_properties"");
boundStatement.setConsistencyLevel(ConsistencyLevel.ALL);
boundStatement.setFetchSize(fetchSize);
PagingState currentPageInfo = null;
do {
try {
if (currentPageInfo != null)
{ boundStatement.setPagingState(currentPageInfo); }

ResultSet rs = session.execute(boundStatement);
processResultSet(rs);
currentPageInfo = rs.getExecutionInfo().getPagingState();
} catch (NoHostAvailableException e) {
}
} while (currentPageInfo != null);
......
processResultSet(ResultSet rs){
int remaining = rs.getAvailableWithoutFetching();
if (remaining != 0) {
for (Row row : rs) {
processCassandraRow(row);
if (--remaining == 0)
{ break; }
}
}
}

Many a time, we got corrupted data in this process.
1. property_uuid was returned as null in many cases, when actual data had a value for it.
2. returned value for property_uuid in table scan was different from property_uuid as seen from cqlsh
3. returned value for group_id in table scan was different from group_id as seen from cqlsh

book_properties has around 140 million records.

book_properties has heavy read, write and update requests while paging is in process

Cassandra version dsc3.0.3

Side Note:
For one of the inconsistent column, we specifically checked the writetime(..) to make sure data hadn't been changed while the job was in process. This was not the case
checked for case 2 : select property_uuid, writetime(property_uuid) from book_properties where book_id = 'BOOK31263786';

Edit1:
->when we do ""select * from book_properties where book_id = 'BOOK31263786';"" we get two records
->when while pagination job, I match and print Row where book_id = 'BOOK31263786', and we got 4 records.
It is a speculation from our side, that other two might have been deleted some time back(definitely not during the job). Again, it is a speculation, not sure.
"
CASSANDRA-11658,java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed,"On our 8 node Cassandra cluster ( 2.2.4v ) i am seeing the below error on multiple nodes.



ERROR [CompactionExecutor:3] 2016-04-26 01:24:06,784 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:3,1,main]
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
        at org.apache.cassandra.utils.EstimatedHistogram.mean(EstimatedHistogram.java:203) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at org.apache.cassandra.io.sstable.metadata.StatsMetadata.getEstimatedDroppableTombstoneRatio(StatsMetadata.java:98) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getEstimatedDroppableTombstoneRatio(SSTableReader.java:1840) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:372) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:96) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:180) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getNextBackgroundTask(WrappingCompactionStrategy.java:85) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:241) ~[apache-cassandra-2.2.4.jar:2.2.4]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_65]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_65]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_65]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_65]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_65]

. "
CASSANDRA-11621,Stack Overflow inserting value with many columns,"I am using CQL to insert into a table that has ~4000 columns

{code}
  TABLE_DEFINITION = ""
      id uuid,
      ""dimension_n"" for n in _.range(N_DIMENSIONS)
      ETAG timeuuid,
      PRIMARY KEY (id)
    ""
{code}

I am using the node.js library from Datastax to execute CQL. This creates a prepared statement and then uses it to perform an insert. It works fine on C* 2.1 but after upgrading to C* 2.2.5 I get the stack overflow below.

I know enough Java to think that recursing an iterator is bad form and should be easy to fix.

{code}
ERROR 14:59:01 Unexpected exception during request; channel = [id: 0xaac42a5d, /10.0.7.182:58736 => /10.0.0.87:9042]
java.lang.StackOverflowError: null
	at com.google.common.base.Preconditions.checkPositionIndex(Preconditions.java:339) ~[guava-16.0.jar:na]
	at com.google.common.collect.AbstractIndexedListIterator.<init>(AbstractIndexedListIterator.java:69) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$11.<init>(Iterators.java:1048) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators.forArray(Iterators.java:1048) ~[guava-16.0.jar:na]
	at com.google.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.listIterator(ImmutableList.java:344) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.iterator(ImmutableList.java:340) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.iterator(ImmutableList.java:61) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables.iterators(Iterables.java:504) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables.access$100(Iterables.java:60) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$2.iterator(Iterables.java:494) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$3.transform(Iterables.java:508) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$3.transform(Iterables.java:505) ~[guava-16.0.jar:na]
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:543) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
...
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
        at org.apache.cassandra.cql3.statements.ModificationStatement.checkAccess(ModificationStatement.java:168) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:257) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:242) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:123) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_77]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_77]
{code}"
CASSANDRA-11554,Allow trace of cql3.QueryProcessor,{{org.apache.cassandra.cql3.QueryProcessor}} has no logging at all. This makes it for example difficult to debug applications that unnecessarily re-prepare statements and cause prepared statements to be evicted from the cache.
CASSANDRA-11545,integer overflow in AbstractQueryPager and friends,"While bootstrapping a new node into an existing cluster we seem to have encountered an integer overflow (note the ""remaining rows"" counters) :

{noformat}
DEBUG [CompactionExecutor:30] 2016-04-11 12:11:58,810 SliceQueryPager.java:92 - Querying next page of slice query; new filter: SliceQueryFilter [reversed=false, slices=[[, ]], count=10000, toGroup = -1]
DEBUG [CompactionExecutor:29] 2016-04-11 12:11:58,810 AbstractQueryPager.java:112 - Got result (15) smaller than page size (10000), considering pager exhausted
DEBUG [CompactionExecutor:29] 2016-04-11 12:11:58,810 AbstractQueryPager.java:133 - Remaining rows to page: 2147483632
DEBUG [CompactionExecutor:29] 2016-04-11 12:11:58,810 SliceQueryPager.java:92 - Querying next page of slice query; new filter: SliceQueryFilter [reversed=false, slices=[[, ]], count=10000, toGroup = -1]
DEBUG [CompactionExecutor:30] 2016-04-11 12:11:58,810 AbstractQueryPager.java:95 - Fetched 14 live rows
DEBUG [CompactionExecutor:30] 2016-04-11 12:11:58,810 AbstractQueryPager.java:112 - Got result (14) smaller than page size (10000), considering pager exhausted
DEBUG [CompactionExecutor:30] 2016-04-11 12:11:58,810 AbstractQueryPager.java:133 - Remaining rows to page: 2147483633
DEBUG [CompactionExecutor:28] 2016-04-11 12:11:58,796 AbstractQueryPager.java:133 - Remaining rows to page: 2147483634
DEBUG [CompactionExecutor:27] 2016-04-11 12:11:58,810 AbstractQueryPager.java:95 - Fetched 14 live rows
{noformat}

It seems to be related to index builds, and this only seems to happen when doing a very large index build (in this case the index is ~10GB per node)."
CASSANDRA-11535,Add dtests for PER PARTITION LIMIT queries with paging,"[#7017|https://issues.apache.org/jira/browse/CASSANDRA-7017] introduces {{PER PARTITION LIMIT}} queries. In order to ensure they work with paging, with partitions containing only static columns, we need to add {{dtests}} to it."
CASSANDRA-11479,BatchlogManager unit tests failing on truncate race condition,"Example on CI [here|http://cassci.datastax.com/job/trunk_testall/818/testReport/junit/org.apache.cassandra.batchlog/BatchlogManagerTest/testLegacyReplay_compression/]. This seems to have only started happening relatively recently (within the last month or two).

As far as I can tell, this is only showing up on BatchlogManagerTests purely because it is an aggressive user of truncate. The assertion is hit in the setUp method, so it can happen before any of the test methods. The assertion occurs because a compaction is happening when truncate wants to discard SSTables; trace level logs suggest that this compaction is submitted after the pause on the CompactionStrategyManager.

This should be reproducible by running BatchlogManagerTest in a loop - it takes up to half an hour in my experience. A trace-level log from such a run is attached - grep for my added log message ""SSTABLES COMPACTING WHEN DISCARDING"" to find when the assert is hit."
CASSANDRA-11467,Paging loses rows in certain conditions,"The bug occurs under the following conditions:
  - RandomPartitioner
  - a compact storage CF
  - querying across rows
  - a tombstone in the first column of a row on the pagesize boundary
  - you need to be querying at least 2*pagesize + 1 records


Attached is a go program using gocql which reproduces the bug fairly minimally."
CASSANDRA-11460,memory leak,"env:
cassandra3.3
jdk8
8G Ram
so set
MAX_HEAP_SIZE=""2G""
HEAP_NEWSIZE=""400M""

1.met same problem about this:
https://issues.apache.org/jira/browse/CASSANDRA-9549

I confuse about that this was fixed in release 3.3 according this page:
https://github.com/apache/cassandra/blob/trunk/CHANGES.txt

so I change to 3.4,and also have  found this problem again 
I think this fix should be included in 3.3.3.4
can you explain about this?

2.our write rate exceed the value that our cassandra env can support,
but i think it should descrese the write rate,or block.consumer the writed data,keep the memory down,then go on writing,not cause out-of-memory instead."
CASSANDRA-11447,Flush writer deadlock in Cassandra 2.2.5,"When writing heavily to one of my Cassandra tables, I got a deadlock similar to CASSANDRA-9882:

{code}
""MemtableFlushWriter:4589"" #34721 daemon prio=5 os_prio=0 tid=0x0000000005fc11d0 nid=0x7664 waiting for monitor entry [0x00007fb83f0e5000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:266)
        - waiting to lock <0x0000000400956258> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
        at org.apache.cassandra.db.lifecycle.Tracker.notifyAdded(Tracker.java:400)
        at org.apache.cassandra.db.lifecycle.Tracker.replaceFlushed(Tracker.java:332)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1580)
        at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:362)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1139)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

The compaction strategies in this keyspace are mixed with one table using LCS and the rest using DTCS.  None of the tables here save for the LCS one seem to have large SSTable counts:

{code}
		Table: active_counters
		SSTable count: 2
--

		Table: aggregation_job_entries
		SSTable count: 2
--

		Table: dsp_metrics_log
		SSTable count: 207
--

		Table: dsp_metrics_ts_5min
		SSTable count: 3
--

		Table: dsp_metrics_ts_day
		SSTable count: 2
--

		Table: dsp_metrics_ts_hour
		SSTable count: 2
{code}

Yet the symptoms are similar. 

The ""dsp_metrics_ts_5min"" table had had a major compaction shortly before all this to get rid of the 400+ SStable files before this system went into use, but they should have been eliminated.

Have other people seen this?  I am attaching a strack trace.

Thanks!"
CASSANDRA-11425,"Add prepared query parameter to trace for ""Execute CQL3 prepared query"" session","For now, the system_traces.sessions rows for ""Execute CQL3 prepared query"" do not show us any information about the prepared query which is executed on the session. So we can't see what query is the session executing.
I think this makes performance tuning difficult on Cassandra.

So, In this ticket, I'd like to add the prepared query parameter on Execute session trace like this.

{noformat}
cqlsh:system_traces> select * from sessions ;

 session_id                           | client    | command | coordinator | duration | parameters                                                                                                                                           | request                     | started_at
--------------------------------------+-----------+---------+-------------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+---------------------------------
 a001ec00-f1c5-11e5-b14a-6fe1292cf9f1 | 127.0.0.1 |   QUERY |   127.0.0.1 |      666 |      \{'consistency_level': 'ONE', 'page_size': '5000', 'query': 'SELECT * FROM test.test2 WHERE id=? LIMIT 1', 'serial_consistency_level': 'SERIAL'\} | Execute CQL3 prepared query | 2016-03-24 13:38:00.000000+0000
 a0019de0-f1c5-11e5-b14a-6fe1292cf9f1 | 127.0.0.1 |   QUERY |   127.0.0.1 |      109 |                                                                                             {'query': 'SELECT * FROM test.test2 WHERE id=? LIMIT 1'} |        Preparing CQL3 query | 2016-03-24 13:37:59.998000+0000
 a0014fc0-f1c5-11e5-b14a-6fe1292cf9f1 | 127.0.0.1 |   QUERY |   127.0.0.1 |      126 |                                                                                           {'query': 'INSERT INTO test.test2(id,value) VALUES (?,?)'} |        Preparing CQL3 query | 2016-03-24 13:37:59.996000+0000
 a0019de1-f1c5-11e5-b14a-6fe1292cf9f1 | 127.0.0.1 |   QUERY |   127.0.0.1 |      764 |      {'consistency_level': 'ONE', 'page_size': '5000', 'query': 'SELECT * FROM test.test2 WHERE id=? LIMIT 1', 'serial_consistency_level': 'SERIAL'} | Execute CQL3 prepared query | 2016-03-24 13:37:59.998000+0000
 a00176d0-f1c5-11e5-b14a-6fe1292cf9f1 | 127.0.0.1 |   QUERY |   127.0.0.1 |      857 | {'consistency_level': 'QUORUM', 'page_size': '5000', 'query': 'INSERT INTO test.test2(id,value) VALUES (?,?)', 'serial_consistency_level': 'SERIAL'} | Execute CQL3 prepared query | 2016-03-24 13:37:59.997000+0000
{noformat}

Now, ""Execute CQL3 prepared query"" session displays its query.
I believe that this additional information would help operators a lot."
CASSANDRA-11402,Alignment wrong in tpstats output for PerDiskMemtableFlushWriter and format to YAML is broken,"With the accompanying designation of which memtableflushwriter it is, this threadpool name is too long for the hardcoded padding in tpstats output.

We should dynamically calculate padding so that we don't need to check this every time we add a threadpool."
CASSANDRA-11396,dtest failure in upgrade_tests.cql_tests.TestCQLNodes3RF3_2_2_UpTo_3_0_HEAD.collection_flush_test,"example failure:

{code}
<ErrorMessage code=1003 [Error during truncate] message=""Error during truncate: Cannot achieve consistency level ALL"">
{code}

http://cassci.datastax.com/job/upgrade_tests-all/25/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_2_2_UpTo_3_0_HEAD/collection_flush_test

Failed on CassCI build upgrade_tests-all #25

It's an inconsistent failure that happens across a number of tests. I'll include all the ones I find here.

http://cassci.datastax.com/job/upgrade_tests-all/26/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_2_1_UpTo_2_2_HEAD/static_with_limit_test/

http://cassci.datastax.com/job/upgrade_tests-all/28/testReport/upgrade_tests.paging_test/TestPagingDatasetChangesNodes3RF3_2_2_HEAD_UpTo_Trunk/test_data_change_impacting_later_page/

http://cassci.datastax.com/job/upgrade_tests-all/29/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_2_1_UpTo_Trunk/range_key_ordered_test/"
CASSANDRA-11374,LEAK DETECTED during repair,"When running a range repair we are seeing the following LEAK DETECTED errors:

{noformat}
ERROR [Reference-Reaper:1] 2016-03-17 06:58:52,261 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5ee90b43) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@367168611:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-03-17 06:58:52,262 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4ea9d4a7) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1875396681:Memory@[7f34b905fd10..7f34b9060b7a) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-03-17 06:58:52,262 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@27a6b614) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@838594402:Memory@[7f34bae11ce0..7f34bae11d84) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-03-17 06:58:52,263 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@64e7b566) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@674656075:Memory@[7f342deab4e0..7f342deb7ce0) was not released before the reference was garbage collected
{noformat}
"
CASSANDRA-11345,"Assertion Errors ""Memory was freed"" during streaming","We encountered the following AssertionError (twice on the same node) during a repair :

On node /172.16.63.41

{noformat}
INFO  [STREAM-IN-/10.174.216.160] 2016-03-09 02:38:13,900 StreamResultFuture.java:180 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Session with /10.174.216.160 is complete                                                                                                        
WARN  [STREAM-IN-/10.174.216.160] 2016-03-09 02:38:13,900 StreamResultFuture.java:207 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Stream failed                           
ERROR [STREAM-OUT-/10.174.216.160] 2016-03-09 02:38:13,906 StreamSession.java:505 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Streaming error occurred                    
java.lang.AssertionError: Memory was freed                                                                                                                                     
        at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:97) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                   
        at org.apache.cassandra.io.util.Memory.getLong(Memory.java:249) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                              
        at org.apache.cassandra.io.compress.CompressionMetadata.getTotalSizeForSections(CompressionMetadata.java:247) ~[apache-cassandra-2.1.13.jar:2.1.13]                
        at org.apache.cassandra.streaming.messages.FileMessageHeader.size(FileMessageHeader.java:112) ~[apache-cassandra-2.1.13.jar:2.1.13]                                
        at org.apache.cassandra.streaming.StreamSession.fileSent(StreamSession.java:546) ~[apache-cassandra-2.1.13.jar:2.1.13]                                             
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:50) ~[apache-cassandra-2.1.13.jar:2.1.13]                      
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:41) ~[apache-cassandra-2.1.13.jar:2.1.13]                      
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45) ~[apache-cassandra-2.1.13.jar:2.1.13]                                    
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:351) ~[apache-cassandra-2.1.13.jar:2.1.13]           
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:331) ~[apache-cassandra-2.1.13.jar:2.1.13]                   
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]                                                                                                                 
{noformat}     

On node /10.174.216.160
 
{noformat}       
ERROR [STREAM-OUT-/172.16.63.41] 2016-03-09 02:38:14,140 StreamSession.java:505 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Streaming error occurred                          
java.io.IOException: Connection reset by peer                                                                                                                              
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.7.0_65]                                                                                              
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.7.0_65]                                                                                      
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.7.0_65]                                                                                          
        at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.7.0_65]                                                                                                          
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487) ~[na:1.7.0_65]                                                                                   
        at org.apache.cassandra.io.util.DataOutputStreamAndChannel.write(DataOutputStreamAndChannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                     
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]        
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:323) [apache-cassandra-2.1.13.jar:2.1.13]                
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]                                                                                                             
INFO  [STREAM-IN-/172.16.63.41] 2016-03-09 02:38:14,142 StreamResultFuture.java:180 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Session with /172.16.63.41 is complete
WARN  [STREAM-IN-/172.16.63.41] 2016-03-09 02:38:14,142 StreamResultFuture.java:207 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Stream failed                         
ERROR [STREAM-OUT-/172.16.63.41] 2016-03-09 02:38:14,143 StreamSession.java:505 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Streaming error occurred                  
java.io.IOException: Broken pipe                                                                                                                                           
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.7.0_65]                                                                                              
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.7.0_65]                                                                                      
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.7.0_65]                                                                                          
        at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.7.0_65]                                                                                                          
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487) ~[na:1.7.0_65]                                                                                   
        at org.apache.cassandra.io.util.DataOutputStreamAndChannel.write(DataOutputStreamAndChannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                     
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]        
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:331) [apache-cassandra-2.1.13.jar:2.1.13]                
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]     
{noformat}"
CASSANDRA-11327,Maintain a histogram of times when writes are blocked due to no available memory,"I have a theory that part of the reason C* is so sensitive to timeouts during saturating write load is that throughput is basically a sawtooth with valleys at zero. This is something I have observed and it gets worse as you add 2i to a table or do anything that decreases the throughput of flushing.

I think the fix for this is to incrementally release memory pinned by memtables and 2i during flushing instead of releasing it all at once. I know that's not really possible, but we can fake it with memory accounting that tracks how close to completion flushing is and releases permits for additional memory. This will lead to a bit of a sawtooth in real memory usage, but we can account for that so the peak footprint is the same.

I think the end result of this change will be a sawtooth, but the valley of the sawtooth will not be zero it will be the rate at which flushing progresses. Optimizing the rate at which flushing progresses and it's fairness with other work can then be tackled separately.

Before we do this I think we should demonstrate that pinned memory due to flushing is actually the issue by getting better visibility into the distribution of instances of not having any memory by maintaining a histogram of spans of time where no memory is available and a thread is blocked.

[MemtableAllocatr$SubPool.allocate(long)|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/utils/memory/MemtableAllocator.java#L186] should be a relatively straightforward entry point for this. The first thread to block can mark the start of memory starvation and the last thread out can mark the end. Have a periodic task that tracks the amount of time spent blocked per interval of time and if it is greater than some threshold log with more details, possibly at debug."
CASSANDRA-11312,DatabaseDescriptor#applyConfig should log stacktrace in case of Eception during seed provider creation,"The comment says that: ""See log for stacktrace."", but with the the flag ""false"" stacktrace is not logged. Logging stacktrace will save some time when someone (like me) mess up with the configuration."
CASSANDRA-11304,Stack overflow when querying 2ndary index,"When reading data through a secondary index _select * from tableName where secIndexField = 'foo'_  (from a Java application) I get the following stacktrace on all nodes; after the query read fails. It happens repeatably when  I rerun the same query:

{quote}
WARN  [SharedPool-Worker-8] 2016-03-04 13:26:28,041 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-8,5,main]: {}
java.lang.StackOverflowError: null
        at org.apache.cassandra.db.rows.BTreeRow$Builder.build(BTreeRow.java:653) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.rows.UnfilteredSerializer.deserializeRowBody(UnfilteredSerializer.java:436) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.UnfilteredDeserializer$CurrentDeserializer.readNext(UnfilteredDeserializer.java:211) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardIndexedReader.computeNext(SSTableIterator.java:266) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.hasNextInternal(SSTableIterator.java:153) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:340) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.hasNext(AbstractSSTableIterator.java:219) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.columniterator.SSTableIterator.hasNext(SSTableIterator.java:32) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:369) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:189) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:428) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:288) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:108) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.index.internal.composites.CompositesSearcher$1.prepareNext(CompositesSearcher.java:128) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.index.internal.composites.CompositesSearcher$1.prepareNext(CompositesSearcher.java:133) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.index.internal.composites.CompositesSearcher$1.prepareNext(CompositesSearcher.java:133) ~[apache-cassandra-3.0.3.jar:3.0.3]
{quote}"
CASSANDRA-11293,NPE when using CQLSSTableWriter,"Hi all!

I'm trying to using CQLSSTableWriter to load a bunch of historical data into my cluster and I'm getting NullPointerExceptions consistently after having written a few million rows (anywhere from 0.5 to 1.5 GB of data).

{code}
java.lang.NullPointerException
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:422) at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:598) at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677) at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735) at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160) at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418) at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
 at com.atlassian.engagementengine.segmentation.helenus.Daemon.main(Daemon.java:24)

Caused by: java.lang.NullPointerException at org.apache.cassandra.db.ClusteringComparator.compare(ClusteringComparator.java:126) at org.apache.cassandra.db.ClusteringComparator.compare(ClusteringComparator.java:44) at
java.util.TimSort.binarySort(TimSort.java:296) at
java.util.TimSort.sort(TimSort.java:239) at
java.util.Arrays.sort(Arrays.java:1512) at
org.apache.cassandra.utils.btree.BTree$Builder.sort(BTree.java:1027) at org.apache.cassandra.utils.btree.BTree$Builder.autoEnforce(BTree.java:1036) at org.apache.cassandra.utils.btree.BTree$Builder.build(BTree.java:1075) at org.apache.cassandra.db.partitions.PartitionUpdate.build(PartitionUpdate.java:572) at org.apache.cassandra.db.partitions.PartitionUpdate.maybeBuild(PartitionUpdate.java:562) at org.apache.cassandra.db.partitions.PartitionUpdate.holder(PartitionUpdate.java:370) at org.apache.cassandra.db.partitions.AbstractBTreePartition.unfilteredIterator(AbstractBTreePartition.java:177) at org.apache.cassandra.db.partitions.AbstractBTreePartition.unfilteredIterator(AbstractBTreePartition.java:172) at org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter$DiskWriter.run(SSTableSimpleUnsortedWriter.java:209)
{code}

This may be a red herring, but I started encountering this when I parallelized writes. (I wasn't aware that doing so was safe until I saw CASSANDRA-7463; I Googled in vain for a while before that.) I'm also definitely not passing any nulls in my {{addRow}} calls."
CASSANDRA-11272,NullPointerException (NPE) during bootstrap startup in StorageService.java,"After bootstrapping fails due to stream closed error, the following error results:

{code}
Feb 27, 2016 8:06:38 PM com.google.common.util.concurrent.ExecutionList executeListener
SEVERE: RuntimeException while executing runnable com.google.common.util.concurrent.Futures$6@3d61813b with executor INSTANCE
java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService$2.onFailure(StorageService.java:1284)
        at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457)
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202)
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:210)
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:186)
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:430)
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:525)
        at org.apache.cassandra.streaming.StreamSession.doRetry(StreamSession.java:645)
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:70)
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:39)
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:59)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:261)
        at java.lang.Thread.run(Thread.java:745)
{code}
"
CASSANDRA-11239,Deprecated repair methods cause NPE,The deprecated repair methods cause an NPE if you aren't doing local repairs. Attaching patch to fix.
CASSANDRA-11215,Reference leak with parallel repairs on the same table,"When starting multiple repairs on the same table Cassandra starts to log about reference leak as:
{noformat}
ERROR [Reference-Reaper:1] 2016-02-23 15:02:05,516 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5213f926) to class org.apache.cassandra.io.sstable.format.SSTableReader
$InstanceTidier@605893242:.../testrepair/standard1-dcf311a0da3411e5a5c0c1a39c091431/la-30-big was not released before the reference was garbage collected
{noformat}

Reproducible with:
{noformat}
ccm create repairtest -v 2.2.5 -n 3
ccm start
ccm stress write n=1000000 -schema replication(strategy=SimpleStrategy,factor=3) keyspace=testrepair
# And then perform two repairs concurrently with:
ccm node1 nodetool repair testrepair
{noformat}

I know that starting multiple repairs in parallel on the same table isn't very wise, but this shouldn't result in reference leaks."
CASSANDRA-11209,SSTable ancestor leaked reference,"We're running a fork of 2.1.13 that adds the TimeWindowCompactionStrategy from [~jjirsa]. We've been running 4 clusters without any issues for many months until a few weeks ago we started scheduling incremental repairs every 24 hours (previously we didn't run any repairs at all).

Since then we started noticing big discrepancies in the LiveDiskSpaceUsed, TotalDiskSpaceUsed, and actual size of files on disk. The numbers are brought back in sync by restarting the node. We also noticed that when this bug happens there are several ancestors that don't get cleaned up. A restart will queue up a lot of compactions that slowly eat away the ancestors.

I looked at the code and noticed that we only decrease the LiveTotalDiskUsed metric in the SSTableDeletingTask. Since we have no errors being logged, I'm assuming that for some reason this task is not getting queued up. If I understand correctly this only happens when the reference count for the SStable reaches 0. So this is leading us to believe that something during repairs and/or compactions is causing a reference leak to the ancestor table."
CASSANDRA-11208,Paging is broken for IN queries,"If the number of selected row is greater than the page size, C* will return some duplicates.

The problem can be reproduced with the java driver using the following code:
{code}
       session = cluster.connect();
       session.execute(""CREATE KEYSPACE IF NOT EXISTS test WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor' : '1'}"");
       session.execute(""USE test"");
       session.execute(""DROP TABLE IF EXISTS test"");
       session.execute(""CREATE TABLE test (rc int, pk int, PRIMARY KEY (pk))"");

       for (int i = 0; i < 5; i++)
           session.execute(""INSERT INTO test (pk, rc) VALUES (?, ?);"", i, i);

       ResultSet rs = session.execute(session.newSimpleStatement(""SELECT * FROM test WHERE  pk IN (1, 2, 3)"").setFetchSize(2));
{code}"
CASSANDRA-11201,Compaction memory fault in 3.0.3,"I have been encountering the following errors periodically on the system:

ERROR [CompactionExecutor:6] 2016-02-20 16:54:09,069 CassandraDaemon.java:195 - Exception in thread Thread[CompactionExecutor:6,1,main]
java.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:366) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:376) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.io.sstable.format.big.BigTableScanner.seekToCurrentRangeStart(BigTableScanner.java:175) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.io.sstable.format.big.BigTableScanner.access$200(BigTableScanner.java:51) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:280) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator.computeNext(BigTableScanner.java:260) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.io.sstable.format.big.BigTableScanner.hasNext(BigTableScanner.java:240) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:369) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:189) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:150) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:72) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:226) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:177) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:78) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:263) ~[apache-cassandra-3.0.3.jar:3.0.3]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_65]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_65]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_65]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_65]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_65]

This problem persisted after several reboots and even when most other applications on the system were terminated to provide more memory availability.

The problem also occurs when running 'nodetool compact'."
CASSANDRA-11195,paging may returns incomplete results on small page size,"This was found through a flapping test, and running that test is still the easiest way to repro the issue. On CI we're seeing a 40-50% failure rate, but locally this test fails much less frequently.

If I attach a python debugger and re-query the ""bad"" query, it continues to return incomplete data indefinitely. If I go directly to cqlsh I can see all rows just fine.
"
CASSANDRA-11167,NPE when creating serializing ErrorMessage for Exception with null message,"In {{ErrorMessage.encode()}} and {{encodedSize()}}, we do not handle the exception having a {{null}} message.  This can result in an error like the following:

{noformat}
ERROR [SharedPool-Worker-1] 2016-02-10 17:41:29,793  Message.java:611 - Unexpected exception during request; channel = [id: 0xc2c6499a, /127.0.0.1:53299 => /127.0.0.1:9042]
java.lang.NullPointerException: null
        at org.apache.cassandra.db.TypeSizes.encodedUTF8Length(TypeSizes.java:46) ~[cassandra-all-3.0.3.874.jar:3.0.3.874]
        at org.apache.cassandra.transport.CBUtil.sizeOfString(CBUtil.java:132) ~[cassandra-all-3.0.3.874.jar:3.0.3.874]
        at org.apache.cassandra.transport.messages.ErrorMessage$1.encodedSize(ErrorMessage.java:215) ~[cassandra-all-3.0.3.874.jar:3.0.3.874]
        at org.apache.cassandra.transport.messages.ErrorMessage$1.encodedSize(ErrorMessage.java:44) ~[cassandra-all-3.0.3.874.jar:3.0.3.874]
        at org.apache.cassandra.transport.Message$ProtocolEncoder.encode(Message.java:328) ~[cassandra-all-3.0.3.874.jar:3.0.3.874]
        at org.apache.cassandra.transport.Message$ProtocolEncoder.encode(Message.java:314) ~[cassandra-all-3.0.3.874.jar:3.0.3.874]
        at io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:89) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:629) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:686) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:622) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at org.apache.cassandra.transport.Message$Dispatcher$Flusher.run(Message.java:445) ~[cassandra-all-3.0.3.874.jar:3.0.3.874]
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:358) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) ~[netty-all-4.0.34.Final.jar:4.0.34.Final]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_71]
{noformat}"
CASSANDRA-11159,SASI indexes don't switch memtable on flush,"SASI maintains its own in-memory structures for indexing the contents of a base Memtable. On flush, these are simply discarded & replaced with an new instance, whilst the on disk index is built as the base memtable is flushed to SSTables. 

SASIIndex implements INotificationHandler and this switching of the index memtable is triggered by receipt of a MemtableRenewedNotification. In the original SASI implementation, one of the necessary modifications to C* was to emit this notification from DataTracker::switchMemtable, but this was overlooked when porting to 3.0. The net result is that the index memtable is never switched out, which eventually leads to OOME. 

Simply applying the original modification isn't entirely appropriate though, as it creates a window where it's possible for the index memtable to have been switched, but the flushwriter is yet to finish writing the new index sstables. During this window, index entries will be missing and query results inaccurate. 

I propose leaving Tracker::switchMemtable as is, so that INotificationConsumers are only notified from there when truncating, but adding similar notifications in Tracker::replaceFlushed, to fire after the View is updated. 

I'm leaning toward re-using MemtableRenewedNotification for this as semantically I don't believe there's any meaningful difference between the flush and truncation cases here. If anyone has a compelling argument for a new notification type though to distinguish the two events, I'm open to hear it.
"
CASSANDRA-11120,Leak Detected while bringing nodes up and down when under stress.,"So after CASSANDRA-10688 has been fixed I'm able to reproduce some leaks consistently with my stress test suite that is doing repairs/stress/bringing nodes up and down.
One such example:

{noformat}
ERROR [Strong-Reference-Leak-Detector:1] 2016-02-03 23:32:38,827  NoSpamLogger.java:97 - Strong self-ref loop detected [/var/lib/cassandra/data/keyspace1/standard1-7cddd4c1cacc11e5aa69f375b464842d/ma-14-big, private java.util.concurrent.ScheduledFuture org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy.readMeterSyncFuture-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask, final java.util.concurrent.ScheduledThreadPoolExecutor java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.this$0-java.util.concurrent.ScheduledThreadPoolExecutor, private final java.util.HashSet java.util.concurrent.ThreadPoolExecutor.workers-java.util.HashSet, private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, final java.lang.Object java.util.HashMap$Node.key-java.util.concurrent.ThreadPoolExecutor$Worker, final java.lang.Thread java.util.concurrent.ThreadPoolExecutor$Worker.thread-java.lang.Thread, private java.lang.ThreadGroup java.lang.Thread.group-java.lang.ThreadGroup, private final java.lang.ThreadGroup java.lang.ThreadGroup.parent-java.lang.ThreadGroup, java.lang.Thread[] java.lang.ThreadGroup.threads-[Ljava.lang.Thread;, java.lang.Thread[] java.lang.ThreadGroup.threads-java.lang.Thread, private java.lang.Runnable java.lang.Thread.target-java.util.concurrent.ThreadPoolExecutor$Worker, final java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ThreadPoolExecutor$Worker.this$0-java.util.concurrent.ScheduledThreadPoolExecutor, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask, private java.util.concurrent.Callable java.util.concurrent.FutureTask.callable-java.util.concurrent.Executors$RunnableAdapter, final java.lang.Runnable java.util.concurrent.Executors$RunnableAdapter.task-sun.rmi.transport.DGCImpl$1, final sun.rmi.transport.DGCImpl sun.rmi.transport.DGCImpl$1.this$0-sun.rmi.transport.DGCImpl, private java.util.Map sun.rmi.transport.DGCImpl.leaseTable-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, java.lang.Object java.util.HashMap$Node.value-sun.rmi.transport.DGCImpl$LeaseInfo, java.util.Set sun.rmi.transport.DGCImpl$LeaseInfo.notifySet-java.util.HashSet, private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, final java.lang.Object java.util.HashMap$Node.key-sun.rmi.transport.Target, private final sun.rmi.transport.WeakRef sun.rmi.transport.Target.weakImpl-sun.rmi.transport.WeakRef, private java.lang.Object sun.rmi.transport.WeakRef.strongRef-javax.management.remote.rmi.RMIJRMPServerImpl, private final java.util.List javax.management.remote.rmi.RMIServerImpl.clientList-java.util.ArrayList, transient java.lang.Object[] java.util.ArrayList.elementData-[Ljava.lang.Object;, transient java.lang.Object[] java.util.ArrayList.elementData-java.lang.ref.WeakReference, private java.lang.Object java.lang.ref.Reference.referent-javax.management.remote.rmi.RMIConnectionImpl, private final javax.management.MBeanServer javax.management.remote.rmi.RMIConnectionImpl.mbeanServer-com.sun.jmx.mbeanserver.JmxMBeanServer, private volatile javax.management.MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer.mbsInterceptor-com.sun.jmx.interceptor.DefaultMBeanServerInterceptor, private final transient com.sun.jmx.mbeanserver.Repository com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.repository-com.sun.jmx.mbeanserver.Repository, private final java.util.Map com.sun.jmx.mbeanserver.Repository.domainTb-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, java.lang.Object java.util.HashMap$Node.value-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, java.lang.Object java.util.HashMap$Node.value-com.sun.jmx.mbeanserver.NamedObject, private final javax.management.DynamicMBean com.sun.jmx.mbeanserver.NamedObject.object-com.sun.jmx.mbeanserver.StandardMBeanSupport, private final java.lang.Object com.sun.jmx.mbeanserver.MBeanSupport.resource-org.apache.cassandra.db.ColumnFamilyStore, public final org.apache.cassandra.db.Keyspace org.apache.cassandra.db.ColumnFamilyStore.keyspace-org.apache.cassandra.db.Keyspace, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-java.util.concurrent.ConcurrentHashMap, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-org.apache.cassandra.db.ColumnFamilyStore, private final java.util.concurrent.ScheduledFuture org.apache.cassandra.db.ColumnFamilyStore.latencyCalculator-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask, final java.util.concurrent.ScheduledThreadPoolExecutor java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.this$0-org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask, private java.util.concurrent.Callable java.util.concurrent.FutureTask.callable-java.util.concurrent.Executors$RunnableAdapter, final java.lang.Runnable java.util.concurrent.Executors$RunnableAdapter.task-org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable, private final java.lang.Runnable org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.runnable-org.apache.cassandra.db.ColumnFamilyStore$3, final org.apache.cassandra.db.ColumnFamilyStore org.apache.cassandra.db.ColumnFamilyStore$3.this$0-org.apache.cassandra.db.ColumnFamilyStore, public final org.apache.cassandra.db.Keyspace org.apache.cassandra.db.ColumnFamilyStore.keyspace-org.apache.cassandra.db.Keyspace, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-java.util.concurrent.ConcurrentHashMap, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-org.apache.cassandra.db.ColumnFamilyStore, private final org.apache.cassandra.db.lifecycle.Tracker org.apache.cassandra.db.ColumnFamilyStore.data-org.apache.cassandra.db.lifecycle.Tracker, public final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-java.util.concurrent.CopyOnWriteArrayList, public final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-org.apache.cassandra.db.compaction.CompactionStrategyManager, private volatile org.apache.cassandra.db.compaction.AbstractCompactionStrategy org.apache.cassandra.db.compaction.CompactionStrategyManager.repaired-org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, private final java.util.Set org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.sstables-java.util.HashSet, private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, final java.lang.Object java.util.HashMap$Node.key-org.apache.cassandra.io.sstable.format.big.BigTableReader, private final org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier org.apache.cassandra.io.sstable.format.SSTableReader.tidy-org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier, private org.apache.cassandra.utils.concurrent.Ref org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier.globalRef-org.apache.cassandra.utils.concurrent.Ref]
{noformat}

CC [~benedict]"
CASSANDRA-11117,ColUpdateTimeDeltaHistogram histogram overflow,"{code}
getting attribute Mean of org.apache.cassandra.metrics:type=ColumnFamily,name=ColUpdateTimeDeltaHistogram threw an exceptionjavax.management.RuntimeMBeanException: java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
{code}

Although the fact that this histogram has 164 buckets already, I wonder if there is something weird with the computation thats causing this to be so large? It appears to be coming from updates to system.local

{code}
org.apache.cassandra.metrics:type=Table,keyspace=system,scope=local,name=ColUpdateTimeDeltaHistogram
{code}"
CASSANDRA-11109,Cassandra process killed by OS due to out of memory issue,"After we upgraded Cassandra from 2.1.12 to 2.2.4 on one of our nodes (A three nodes Cassandra cluster), we've been experiencing an og-going issue with cassandra process running with continuously increasing memory util killed by OOM. 

{quote}
Feb  1 23:53:10 kernel: [24135455.025185] [19862]   494 19862 133728623  7379077  139068        0             0 java
Feb  1 23:53:10 kernel: [24135455.029678] Out of memory: Kill process 19862 (java) score 973 or sacrifice child
Feb  1 23:53:10 kernel: [24135455.035434] Killed process 19862 (java) total-vm:534918588kB, anon-rss:29413728kB, file-rss:102940kB
{quote}"
CASSANDRA-11076,LEAK detected after bootstrapping a new node,"Sequence of events:
* start up a 2 node cluster
* bootstrap one additional node so that the cluster consists of 3 nodes in total
* the bootstrapped node will contain the LEAK error in the log file

{code}
INFO  [main] 2016-01-26 10:59:06,206  Server.java:162 - Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...
INFO  [main] 2016-01-26 10:59:06,269  ThriftServer.java:119 - Binding thrift service to /0.0.0.0:9160
INFO  [Thread-6] 2016-01-26 10:59:06,280  ThriftServer.java:136 - Listening for thrift clients...
INFO  [HANDSHAKE-/10.200.178.183] 2016-01-26 10:59:06,703  OutboundTcpConnection.java:503 - Handshaking version with /10.200.178.183
INFO  [RMI TCP Connection(4)-10.200.178.183] 2016-01-26 10:59:20,079  StorageService.java:1099 - rebuild from dc: (any dc)
INFO  [RMI TCP Connection(4)-10.200.178.183] 2016-01-26 10:59:20,090  RangeStreamer.java:339 - Some ranges of [(-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]] are already available. Skipping streaming those ranges.
INFO  [RMI TCP Connection(4)-10.200.178.183] 2016-01-26 10:59:20,091  RangeStreamer.java:339 - Some ranges of [(-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]] are already available. Skipping streaming those ranges.
INFO  [RMI TCP Connection(4)-10.200.178.183] 2016-01-26 10:59:20,092  RangeStreamer.java:339 - Some ranges of [(3074457345618258602,-9223372036854775808], (-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]] are already available. Skipping streaming those ranges.
INFO  [RMI TCP Connection(4)-10.200.178.183] 2016-01-26 10:59:20,093  RangeStreamer.java:339 - Some ranges of [(3074457345618258602,-9223372036854775808], (-9223372036854775808,-3074457345618258603], (-3074457345618258603,3074457345618258602]] are already available. Skipping streaming those ranges.
INFO  [RMI TCP Connection(4)-10.200.178.183] 2016-01-26 10:59:20,094  StreamResultFuture.java:86 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03] Executing streaming plan for Rebuild
INFO  [StreamConnectionEstablisher:3] 2016-01-26 10:59:20,095  StreamSession.java:238 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03] Starting streaming to /10.200.178.185
INFO  [StreamConnectionEstablisher:4] 2016-01-26 10:59:20,096  StreamSession.java:238 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03] Starting streaming to /10.200.178.193
INFO  [StreamConnectionEstablisher:4] 2016-01-26 10:59:20,097  StreamCoordinator.java:213 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03, ID#0] Beginning stream session with /10.200.178.193
INFO  [StreamConnectionEstablisher:3] 2016-01-26 10:59:20,098  StreamCoordinator.java:213 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03, ID#0] Beginning stream session with /10.200.178.185
INFO  [STREAM-IN-/10.200.178.185] 2016-01-26 10:59:20,102  StreamResultFuture.java:182 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03] Session with /10.200.178.185 is complete
INFO  [STREAM-IN-/10.200.178.193] 2016-01-26 10:59:20,359  StreamResultFuture.java:182 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03] Session with /10.200.178.193 is complete
INFO  [STREAM-IN-/10.200.178.193] 2016-01-26 10:59:20,362  StreamResultFuture.java:214 - [Stream #d9bbe900-c41b-11e5-8540-d1e65b596c03] All sessions completed
ERROR [Reference-Reaper:1] 2016-01-26 11:00:39,410  Ref.java:197 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1c7d1dcf) to @2011417651 was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2016-01-26 11:00:39,411  Ref.java:228 - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@1c7d1dcf:
Thread[SharedPool-Worker-6,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:218)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:148)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:70)
	at org.apache.cassandra.utils.memory.BufferPool$Chunk.setAttachment(BufferPool.java:646)
	at org.apache.cassandra.utils.memory.BufferPool$Chunk.get(BufferPool.java:786)
	at org.apache.cassandra.utils.memory.BufferPool$Chunk.get(BufferPool.java:776)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.get(BufferPool.java:379)
	at org.apache.cassandra.utils.memory.BufferPool.maybeTakeFromPool(BufferPool.java:139)
	at org.apache.cassandra.utils.memory.BufferPool.takeFromPool(BufferPool.java:113)
	at org.apache.cassandra.utils.memory.BufferPool.get(BufferPool.java:92)
	at org.apache.cassandra.io.util.RandomAccessReader.allocateBuffer(RandomAccessReader.java:87)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.access$100(CompressedRandomAccessReader.java:38)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader$Builder.createBuffer(CompressedRandomAccessReader.java:275)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:74)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:59)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader$Builder.build(CompressedRandomAccessReader.java:283)
	at org.apache.cassandra.io.util.CompressedSegmentedFile.createReader(CompressedSegmentedFile.java:145)
	at org.apache.cassandra.io.util.SegmentedFile.createReader(SegmentedFile.java:133)
	at org.apache.cassandra.io.sstable.format.SSTableReader.getFileDataInput(SSTableReader.java:1762)
	at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:93)
	at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:46)
	at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:36)
	at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:62)
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:547)
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:459)
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:325)
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:350)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1719)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2375)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105)
	at java.lang.Thread.run(Thread.java:745)

INFO  [CompactionExecutor:5] 2016-01-26 11:05:49,416  CompactionManager.java:1456 - Compaction interrupted: Compaction@a17c09d1-c41b-11e5-8d26-7dd65babdc03(keyspace1, standard1, 0/229706280)bytes
INFO  [CompactionExecutor:5] 2016-01-26 11:06:07,332  CompactionManager.java:1456 - Compaction interrupted: Compaction@a17c09d1-c41b-11e5-8d26-7dd65babdc03(keyspace1, standard1, 86299/211434370)bytes
INFO  [CompactionExecutor:4] 2016-01-26 11:07:31,662  CompactionManager.java:1456 - Compaction interrupted: Compaction@a17c09d1-c41b-11e5-8d26-7dd65babdc03(keyspace1, standard1, 118545029/211434370)bytes
{code}

This happened with *Cassandra version: 3.0.1.816*"
CASSANDRA-11070,Dispatcher.Flusher's control has duplicated/conflict control,"org.apache.cassandra.transport.Message.Dispatcher.Flusher

remove duplicated control for flush message control …

Motivation:

the !doneWork's control is duplicated and confused with runsSinceFlush > 2

if on the first run:the queue size is 20
donework will be set to true and not do flush due to the size<50 and runsSinceFlush<2.

if on the second run. the queue size is 0,
donework will be reset to false and not set to true due to no new items in queue, but the flush will be triggered due to:
  if (!doneWork || runsSinceFlush > 2 || flushed.size() > 50)
now the runsSinceFlush is 2. so in actual, its function is similar with runsSinceFlush>1.
so it is no need to keep it so that the code is confused and duplicated.
              

Modifications:

remove it

Result:

after remove it, it will more clear and no confused.

 "
CASSANDRA-11068,Entire row is compacted away if remaining cells are tombstones expiring after gc_grace_seconds,"Assuming the following schema:

{code}
CREATE TABLE simple.data (
    k text PRIMARY KEY,
    v int
) WITH gc_grace_seconds = 300;
{code}

And the following queries:

{code}
insert into simple.data (k, v) values ('blah', 1);
delete v from simple.data where k='blah';
{code}

Performing a {{select *}} from this table will return 1 row with a null value:

{code}
cqlsh> select * from simple.data;

         k | v
-----------+---------
      blah |    null
{code}

Prior the 3.0, if I were to do a flush, the sstable representation of this table would include an empty cell and a tombstone:

{code}
[
{""key"": ""blah"",
 ""cells"": [["""","""",1453747038457027],
           [""v"",1453747112,1453747112383096,""d""]]}
]
{code}

As my gc_grace_seconds value is 300, if I wait 5 minutes and perform a compaction, the new sstable would omit the tombstone, but the empty cell would still be present:

{code}
[
{""key"": ""blah"",
 ""cells"": [["""","""",1453747038457027]]}
]
{code}

Performing the {{select *}} query would still yield the same result because of this.

However, in 3.2.1 this does not seem to be the behavior, after deleting the 'v' cell, performing a flush and then waiting 5 minutes and doing a compact, what ends up happening is that the sstable completely disappears (presumably because there is no remaining data) and the select query emits 0 rows:

{code}
cqlsh> select * from simple.data;

         k | v
-----------+---------

(0 rows)
{code}

I'm unsure if this is by design or a bug, but it does represent a change between C* versions.

-I have not tried this for a table with clustering columns yet, but I assume that the behavior will be the same.- (The problem only manifests for tables with no clustering columns)."
CASSANDRA-11065,null pointer exception in CassandraDaemon.java:195,"Running Cassandra 3.0.1 installed from apt-get on debian.

I had a keyspace called 'tests'. I dropped it. Then I checked some nodes and one of them still had that keyspace 'tests'. On a node that still has the dropped keyspace I ran:
nodetools repair tests;

In the system logs of another node that did not have keyspace 'tests' I am seeing a null pointer exception:

{code:java}
ERROR [AntiEntropyStage:2] 2016-01-25 15:02:46,323 RepairMessageVerbHandler.java:161 - Got error, removing parent repair session
ERROR [AntiEntropyStage:2] 2016-01-25 15:02:46,324 CassandraDaemon.java:195 - Exception in thread Thread[AntiEntropyStage:2,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:164) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_66-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_66-internal]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_66-internal]
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:69) ~[apache-cassandra-3.0.1.jar:3.0.1]
	... 4 common frames omitted
{code}

The error appears every time I run:
nodetools repair tests;

I can see it in the logs of all nodes, including the node on which I run the repair."
CASSANDRA-11063,Unable to compute ceiling for max when histogram overflowed,"Issue https://issues.apache.org/jira/browse/CASSANDRA-8028 seems related with error we are getting. But we are getting this with Cassandra 2.1.9 when autocompaction is running it keeps throwing following errors, we are unsure if its a bug or can be resolved, please suggest.

{code}
WARN  [CompactionExecutor:3] 2016-01-23 13:30:40,907 SSTableWriter.java:240 - Compacting large partition gccatlgsvcks/category_name_dedup:66611300 (138152195 bytes)
ERROR [CompactionExecutor:1] 2016-01-23 13:30:50,267 CassandraDaemon.java:223 - Exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
        at org.apache.cassandra.utils.EstimatedHistogram.mean(EstimatedHistogram.java:203) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at org.apache.cassandra.io.sstable.metadata.StatsMetadata.getEstimatedDroppableTombstoneRatio(StatsMetadata.java:98) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at org.apache.cassandra.io.sstable.SSTableReader.getEstimatedDroppableTombstoneRatio(SSTableReader.java:1987) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:370) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:96) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:179) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getNextBackgroundTask(WrappingCompactionStrategy.java:84) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:230) ~[apache-cassandra-2.1.9.jar:2.1.9]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
{code}

h3. Additional info:

*cfstats is running fine for that table...*

{code}
~ $ nodetool cfstats gccatlgsvcks.category_name_dedup
Keyspace: gccatlgsvcks
        Read Count: 0
        Read Latency: NaN ms.
        Write Count: 0
        Write Latency: NaN ms.
        Pending Flushes: 0
                Table: category_name_dedup
                SSTable count: 6
                Space used (live): 836314727
                Space used (total): 836314727
                Space used by snapshots (total): 3621519
                Off heap memory used (total): 6930368
                SSTable Compression Ratio: 0.03725358753117693
                Number of keys (estimate): 3004
                Memtable cell count: 0
                Memtable data size: 0
                Memtable off heap memory used: 0
                Memtable switch count: 0
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 0
                Local write latency: NaN ms
                Pending flushes: 0
                Bloom filter false positives: 0
                Bloom filter false ratio: 0.00000
                Bloom filter space used: 5240
                Bloom filter off heap memory used: 5192
                Index summary off heap memory used: 1200
                Compression metadata off heap memory used: 6923976
                Compacted partition minimum bytes: 125
                Compacted partition maximum bytes: 30753941057
                Compacted partition mean bytes: 8352388
                Average live cells per slice (last five minutes): 0.0
                Maximum live cells per slice (last five minutes): 0.0
                Average tombstones per slice (last five minutes): 0.0
                Maximum tombstones per slice (last five minutes): 0.0

{code}

*cfhistograms is also running fine...*

{code}
~ $ nodetool cfhistograms gccatlgsvcks category_name_dedup
gccatlgsvcks/category_name_dedup histograms
Percentile  SSTables     Write Latency      Read Latency    Partition Size        Cell Count
                              (micros)          (micros)           (bytes)
50%             0.00              0.00              0.00              1109                20
75%             0.00              0.00              0.00              2299                42
95%             0.00              0.00              0.00             11864               215
98%             0.00              0.00              0.00             35425               642
99%             0.00              0.00              0.00             51012               924
Min             0.00              0.00              0.00               125                 4
Max             0.00              0.00              0.00       30753941057         268650950
{code}

"
CASSANDRA-11062,BatchlogManager May Attempt to Flush Hints After HintsService is Shutdown,"{{ScheduledThreadPoolExecutor}}'s default behavior is to keep running delayed tasks after shutdown, so I think that means {{BatchlogManager}} is trying to call {{replayFailedBatches()}} after drain has instructed both it and the {{HintsService}} to shut down. When this happens, we get an exception when that tries to submit a task to the executor in {{HintsWriteExecutor}}:

{noformat}
ERROR [BatchlogTasks:1] 2016-01-22 17:01:38,936  CassandraDaemon.java:195 - Exception in thread Thread[BatchlogTasks:1,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[cassandra-all-3.0.1.816.jar:3.0.1.816]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) [na:1.8.0_65]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) [na:1.8.0_65]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[cassandra-all-3.0.1.816.jar:3.0.1.816]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_65]
	at org.apache.cassandra.hints.HintsWriteExecutor.flushBufferPool(HintsWriteExecutor.java:89) ~[cassandra-all-3.0.1.816.jar:3.0.1.816]
	at org.apache.cassandra.hints.HintsService.flushAndFsyncBlockingly(HintsService.java:177) ~[cassandra-all-3.0.1.816.jar:3.0.1.816]
	at org.apache.cassandra.batchlog.BatchlogManager.processBatchlogEntries(BatchlogManager.java:259) ~[cassandra-all-3.0.1.816.jar:3.0.1.816]
	at org.apache.cassandra.batchlog.BatchlogManager.replayFailedBatches(BatchlogManager.java:200) ~[cassandra-all-3.0.1.816.jar:3.0.1.816]
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118) ~[cassandra-all-3.0.1.816.jar:3.0.1.816]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_65]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_65]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_65]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_65]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_65]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_65]
{noformat}"
CASSANDRA-11032,Full trace returned on ReadFailure by cqlsh,"I noticed that the full traceback is returned on a read failure where I expected this to be a one line exception with the ReadFailure message. It is minor, but would it be better to only return the ReadFailure details?

{code}
cqlsh> SELECT * FROM test_encryption_ks.test_bad_table;
Traceback (most recent call last):
  File ""/usr/local/lib/dse/bin/../resources/cassandra/bin/cqlsh.py"", line 1276, in perform_simple_statement
    result = future.result()
  File ""/usr/local/lib/dse/resources/cassandra/bin/../lib/cassandra-driver-internal-only-3.0.0-6af642d.zip/cassandra-driver-3.0.0-6af642d/cassandra/cluster.py"", line 3122, in result
    raise self._final_exception
ReadFailure: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}"
CASSANDRA-11024,Unexpected exception during request; java.lang.StackOverflowError: null,"This happened when I run a ""SELECT *"" query on a very wide table. The table has over 1000 columns and a lot of nulls. If I run ""SELECT * ... LIMIT 10"" or ""SELECT a,b,c FROM ..."", then it's fine. The data is being actively inserted when I run the query. Will try later when compaction (LCS) catches up.

{noformat}
ERROR [SharedPool-Worker-5] 2016-01-15 20:49:08,212 Message.java:611 - Unexpected exception during request; channel = [id: 0x8e11d570, /192.168.0.3:50332 => /192.168.0.11:9042]
java.lang.StackOverflowError: null
	at com.google.common.base.Preconditions.checkPositionIndex(Preconditions.java:339) ~[guava-16.0.jar:na]
	at com.google.common.collect.AbstractIndexedListIterator.<init>(AbstractIndexedListIterator.java:69) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$11.<init>(Iterators.java:1048) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators.forArray(Iterators.java:1048) ~[guava-16.0.jar:na]
	at com.google.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.listIterator(ImmutableList.java:344) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.iterator(ImmutableList.java:340) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.iterator(ImmutableList.java:61) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables.iterators(Iterables.java:504) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables.access$100(Iterables.java:60) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$2.iterator(Iterables.java:494) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$3.transform(Iterables.java:508) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$3.transform(Iterables.java:505) ~[guava-16.0.jar:na]
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:543) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
{noformat}"
CASSANDRA-11008,Null Pointer Exception when upgrading from 2.1.12 to 3.0.2,"When upgrading a node from 2.1.12 to 3.0.2, I get a NPE during startup.

{code}
INFO  [main] 2016-01-13 19:25:52,566 LegacyHintsMigrator.java:88 - Migrating legacy hints to new storage
INFO  [main] 2016-01-13 19:25:52,566 LegacyHintsMigrator.java:91 - Forcing a major compaction of system.hints table
WARN  [CompactionExecutor:2] 2016-01-13 19:26:05,372 BigTableWriter.java:171 - Writing large partition system/hints:4dbdaab0-f52d-46db-9367-b350567c89b8 (2548854721 bytes)
INFO  [main] 2016-01-13 19:26:05,528 LegacyHintsMigrator.java:95 - Writing legacy hints to the new storage
ERROR [main] 2016-01-13 19:26:08,777 CassandraDaemon.java:690 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.serializers.Int32Serializer.deserialize(Int32Serializer.java:31) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.serializers.Int32Serializer.deserialize(Int32Serializer.java:25) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:114) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getInt(UntypedResultSet.java:287) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator.convertLegacyHint(LegacyHintsMigrator.java:197) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHintsInternal(LegacyHintsMigrator.java:175) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:158) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:151) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:142) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator.lambda$migrateLegacyHints$201(LegacyHintsMigrator.java:128) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator$$Lambda$140/892745204.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_45]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrateLegacyHints(LegacyHintsMigrator.java:128) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.hints.LegacyHintsMigrator.migrate(LegacyHintsMigrator.java:96) ~[apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:294) [apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:549) [apache-cassandra-3.0.2.jar:3.0.2]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:677) [apache-cassandra-3.0.2.jar:3.0.2]
INFO  [main] 2016-01-13 19:29:10,940 YamlConfigurationLoader.java:92 - Loading settings from file:/etc/cassandra/cassandra.yaml
{code}"
CASSANDRA-10980,nodetool scrub NPEs when keyspace isn't specified,I've attached logs of what I saw. Running nodetool scrub without anything else specified resulted in the NPE. Running with the keyspace specified saw successful termination.
CASSANDRA-10955,Multi-partitions queries with ORDER BY can result in a NPE,"In the case of a table with static columns, if only the static columns have been set for some partitions, a multi-partitions query with an {{ORDER BY}} can cause a {{NPE}}.

The following unit test can be used to reproduce the problem:
{code}
    @Test
    public void testOrderByForInClauseWithNullValue() throws Throwable
    {
        createTable(""CREATE TABLE %s (a int, b int, c int, s int static, d int, PRIMARY KEY (a, b, c))"");

        execute(""INSERT INTO %s (a, b, c, d) VALUES (1, 1, 1, 1)"");
        execute(""INSERT INTO %s (a, b, c, d) VALUES (1, 1, 2, 1)"");
        execute(""INSERT INTO %s (a, b, c, d) VALUES (2, 2, 1, 1)"");
        execute(""INSERT INTO %s (a, b, c, d) VALUES (2, 2, 2, 1)"");

        execute(""UPDATE %s SET s = 1 WHERE a = 1"");
        execute(""UPDATE %s SET s = 2 WHERE a = 2"");
        execute(""UPDATE %s SET s = 3 WHERE a = 3"");

        assertRows(execute(""SELECT a, b, c, d, s FROM %s WHERE a IN (1, 2, 3) ORDER BY b DESC""),
                   row(2, 2, 2, 1, 2),
                   row(2, 2, 1, 1, 2),
                   row(1, 1, 2, 1, 1),
                   row(1, 1, 1, 1, 1),
                   row(3, null, null, null, 3));
    }
{code} "
CASSANDRA-10942,guard against npe if no thrift supercolumn data,
CASSANDRA-10909,NPE in ActiveRepairService,"NPE after one started multiple incremental repairs

{code}
INFO  [Thread-62] 2015-12-21 11:40:53,742  RepairRunnable.java:125 - Starting repair command #1, repairing keyspace keyspace1 with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 2)
INFO  [Thread-62] 2015-12-21 11:40:53,813  RepairSession.java:237 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] new session: will sync /10.200.177.32, /10.200.177.33 on range [(10,-9223372036854775808]] for keyspace1.[counter1, standard1]
INFO  [Repair#1:1] 2015-12-21 11:40:53,853  RepairJob.java:100 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] requesting merkle trees for counter1 (to [/10.200.177.33, /10.200.177.32])
INFO  [Repair#1:1] 2015-12-21 11:40:53,853  RepairJob.java:174 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Requesting merkle trees for counter1 (to [/10.200.177.33, /10.200.177.32])
INFO  [Thread-62] 2015-12-21 11:40:53,854  RepairSession.java:237 - [repair #b1449fe0-a7d7-11e5-b568-f565b837eb0d] new session: will sync /10.200.177.32, /10.200.177.31 on range [(0,10]] for keyspace1.[counter1, standard1]
INFO  [AntiEntropyStage:1] 2015-12-21 11:40:53,896  RepairSession.java:181 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Received merkle tree for counter1 from /10.200.177.32
INFO  [AntiEntropyStage:1] 2015-12-21 11:40:53,906  RepairSession.java:181 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Received merkle tree for counter1 from /10.200.177.33
INFO  [Repair#1:1] 2015-12-21 11:40:53,906  RepairJob.java:100 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] requesting merkle trees for standard1 (to [/10.200.177.33, /10.200.177.32])
INFO  [Repair#1:1] 2015-12-21 11:40:53,906  RepairJob.java:174 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Requesting merkle trees for standard1 (to [/10.200.177.33, /10.200.177.32])
INFO  [RepairJobTask:2] 2015-12-21 11:40:53,910  SyncTask.java:66 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Endpoints /10.200.177.33 and /10.200.177.32 are consistent for counter1
INFO  [RepairJobTask:1] 2015-12-21 11:40:53,910  RepairJob.java:145 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] counter1 is fully synced
INFO  [AntiEntropyStage:1] 2015-12-21 11:40:54,823  Validator.java:272 - [repair #b17a2ed0-a7d7-11e5-ada8-8304f5629908] Sending completed merkle tree to /10.200.177.33 for keyspace1.counter1
ERROR [ValidationExecutor:3] 2015-12-21 11:40:55,104  CompactionManager.java:1065 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:3] 2015-12-21 11:40:55,105  Validator.java:259 - Failed creating a merkle tree for [repair #b17a2ed0-a7d7-11e5-ada8-8304f5629908 on keyspace1/standard1, [(10,-9223372036854775808]]], /10.200.177.33 (see log for details)
ERROR [ValidationExecutor:3] 2015-12-21 11:40:55,110  CassandraDaemon.java:195 - Exception in thread Thread[ValidationExecutor:3,1,main]
java.lang.RuntimeException: Cannot start multiple repair sessions over the same sstables
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1066) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:80) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.db.compaction.CompactionManager$10.call(CompactionManager.java:679) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_40]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_40]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]
ERROR [AntiEntropyStage:1] 2015-12-21 11:40:55,174  RepairMessageVerbHandler.java:161 - Got error, removing parent repair session
INFO  [CompactionExecutor:3] 2015-12-21 11:40:55,175  CompactionManager.java:489 - Starting anticompaction for keyspace1.counter1 on 0/[] sstables
INFO  [CompactionExecutor:3] 2015-12-21 11:40:55,176  CompactionManager.java:547 - Completed anticompaction successfully
ERROR [AntiEntropyStage:1] 2015-12-21 11:40:55,179  CassandraDaemon.java:195 - Exception in thread Thread[AntiEntropyStage:1,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:164) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_40]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_40]
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.service.ActiveRepairService$ParentRepairSession.getAndReferenceSSTables(ActiveRepairService.java:452) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.service.ActiveRepairService.doAntiCompaction(ActiveRepairService.java:379) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:136) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	... 4 common frames omitted
{code}"
CASSANDRA-10907,Nodetool snapshot should provide an option to skip flushing,"For some practical scenarios, it doesn't matter if the data is flushed to disk before taking a snapshot. However, it's better to save some flushing time to make snapshot process quick.

As such, it will be a good idea to provide this option to snapshot command. The wiring from nodetool to MBean to VerbHandler should be easy. 

I can provide a patch if this makes sense."
CASSANDRA-10880,Paging state between 2.2 and 3.0 are incompatible on protocol v4,"In CASSANDRA-10254, the paging states generated by 3.0 for the native protocol v4 were made 3.0 specific. This was done because the paging state in pre-3.0 versions contains a serialized cell name, but 3.0 doesn't talk in term of cells internally (at least not the pre-3.0 ones) and so using an old-format cell name when we only have 3.0 nodes is inefficient and inelegant.

Unfortunately that change was made on the assumption than the protocol v4 was 3.0 only but it's not, it ended up being released with 2.2 and that completely slipped my mind. So in practice, you can't properly have a mixed 2.2/3.0 cluster if your driver is using the protocol v4.

And unfortunately, I don't think there is an easy way to fix that without breaking something. Concretely, I can see 3 choices:
# we change 3.0 so that it generates old-format paging states on the v4 protocol. The 2 main downsides are that 1) this breaks 3.0 upgrades if the driver is using the v4 protocol, and at least on the java side the only driver versions that support 3.0 will use v4 by default and 2) we're signing off on having sub-optimal paging state until the protocol v5 ships (probably not too soon).
# we remove the v4 protocol from 2.2. This means 2.2 will have to use v3 before upgrade at the risk of breaking upgrade. This is also bad, but I'm not sure the driver version using the v4 protocol are quite ready yet (at least the java driver is not GA yet) so if we work with the drivers teams to make sure the v3 protocol gets prefered by default on 2.2 in the GA versions of these driver, this might be somewhat transparent to users.
# we don't change anything code-wise, but we document clearly that you can't upgrade from 2.2 to 3.0 if your clients use protocol v4 (so we leave upgrade broken if the v4 protocol is used as it is currently). This is not great, but we can work with the drivers teams here again to make sure drivers prefer the v3 version for 2.2 nodes so most people don't notice in practice.

I think I'm leaning towards solution 3). It's not great but at least we break no minor upgrades (neither on 2.2, nor on 3.0) which is probably the most important. We'd basically be just adding a new condition on 2.2->3.0 upgrades.  We could additionally make 3.0 node completely refuse v4 connections if they know a 2.2 nodes is in the cluster for extra safety.

Ping [~omichallat], [~adutra] and [~aholmber] as you might want to be aware of that ticket."
CASSANDRA-10871,MemtableFlushWriter blocks and no flushing happens,"After some time MemtableFlushWriter thread blocks, resulting first full filling of the FlushWriterQueue, than full filling of MutationStage queue. After this 2 things might happen - Cassandra might drop the queued mutations and everything becomes normal or it shuts down with insufficient HeapSpace.
Here is the thread dump.
{noformat}

""MemtableFlushWriter:3"" - Thread t@2610
   java.lang.Thread.State: BLOCKED
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:250)
	- waiting to lock <f9dab27> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy) owned by ""CompactionExecutor:51"" t@2638
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1502)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- locked <7ef8cd1b> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""MemtableFlushWriter:4"" - Thread t@2616
   java.lang.Thread.State: BLOCKED
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:250)
	- waiting to lock <f9dab27> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy) owned by ""CompactionExecutor:51"" t@2638
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1502)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- locked <2f842d9b> (a java.util.concurrent.ThreadPoolExecutor$Worker)
{noformat}

and here are the tpsats
{noformat}
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
CounterMutationStage              0         0              0         0                 0
ReadStage                         0         0             28         0                 0
RequestResponseStage              0         0        2020253         0                 0
MutationStage                    32     63221       27858588         0                 0
ReadRepairStage                   0         0              0         0                 0
GossipStage                       0         0          16430         0                 0
CacheCleanupExecutor              0         0              0         0                 0
AntiEntropyStage                  0         0           3008         0                 0
MigrationStage                    0         0              0         0                 0
Sampler                           0         0              0         0                 0
ValidationExecutor                0         0           1500         0                 0
CommitLogArchiver                 0         0              0         0                 0
MiscStage                         0         0              0         0                 0
MemtableFlushWriter               2       220           3531         0                 0
MemtableReclaimMemory             0         0           4277         0                 0
PendingRangeCalculator            0         0             22         0                 0
MemtablePostFlush                 1       306           5186         0                 0
CompactionExecutor               36       142           5326         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     0         0             13         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
PAGED_RANGE                  0
BINARY                       0
READ                         0
MUTATION                220352
_TRACE                       0
REQUEST_RESPONSE             0
COUNTER_MUTATION             0
{noformat}

cfstats reports 12k++ sstables."
CASSANDRA-10869,paging_test.py:TestPagingWithDeletions.test_failure_threshold_deletions dtest fails on 2.1,"This test is failing hard on 2.1. Here is its history on the JDK8 job for cassandra-2.1:

http://cassci.datastax.com/job/cassandra-2.1_dtest_jdk8/lastCompletedBuild/testReport/paging_test/TestPagingWithDeletions/test_failure_threshold_deletions/history/

and on the JDK7 job:

http://cassci.datastax.com/job/cassandra-2.1_dtest/lastCompletedBuild/testReport/paging_test/TestPagingWithDeletions/test_failure_threshold_deletions/history/

It fails because a read times out after ~1.5 minutes. If this is a test error, it's specific to 2.1, because the test passes consistently on newer versions."
CASSANDRA-10861,Memory leak with Cassadra java driver 3.0.0-beta1 and Cassandra 3.0.1,"Same dev environment with same application on Tomcat 8.0.23. However the dev nodes have been upgraded to 3.0.0 and later to 3.0.1. The Cassandra driver is version 3.0.0-beta1.

It seems that connections crash, do not get cleared and it leads to a memory leak stack overflow condition.

Attached is an error log file from tomcat."
CASSANDRA-10848,Upgrade paging dtests involving deletion flap on CassCI,"A number of dtests in the {{upgrade_tests.paging_tests}} that involve deletion flap with the following error:

{code}
Requested pages were not delivered before timeout.
{code}

This may just be an effect of CASSANDRA-10730, but it's worth having a look at separately. Here are some examples of tests flapping in this way:"
CASSANDRA-10823,LEAK DETECTED (org.apache.cassandra.utils.concurrent.Ref$State@),"{code}
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,455 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@66909a93) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@529816960:/var/lib/cassandra/data2/sync/user_quota-fe54df20770e11e4a0a975bb514ae072/sync-user_quota-ka-61776-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,456 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@45868eb2) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@84044743:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,456 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@61f1d862) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1286945834:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,456 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@e8110be) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@997339490:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,456 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4608376b) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1164867000:[[OffHeapBitSet]] was not released before the reference was garbage collectedERROR [Reference-Reaper:1] 2015-12-07 14:09:30,456 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@56f2a6a4) to class org.apache.cassandra
.utils.concurrent.WrappedSharedCloseable$1@1419412884:[[OffHeapBitSet]] was not released before the reference was garbage collectedERROR [Reference-Reaper:1] 2015-12-07 14:09:30,456 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@6cb7e2f0) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@479474259:[Memory@[0..4), Memory@[0..11)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,457 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4573f5cd) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1074694490:[[OffHeapBitSet]] was not released before the reference was garbage collectedERROR [Reference-Reaper:1] 2015-12-07 14:09:30,457 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7a5b9490) to class org.apache.cassandra
.utils.concurrent.WrappedSharedCloseable$1@309770418:[[OffHeapBitSet]] was not released before the reference was garbage collectedERROR [Reference-Reaper:1] 2015-12-07 14:09:30,457 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3057b796) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1322643877:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,498 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3febb012) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@175410823:/var/lib/cassandra/data2/sync/entity2-e24b5040199b11e5a30f75bb514ae072/sync-entity2-tmplink-ka-1175811 was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,498 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@6a39466d) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1446958230:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,499 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@36f6f016) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@235688075:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,499 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4a7bdce1) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@165830139:[Memory@[0..4), Memory@[0..11)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-12-07 14:09:30,499 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@6c880ece) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1485057548:/var/lib/cassandra/data/sync/user_quota-fe54df20770e11e4a0a975bb514ae072/sync-user_quota-ka-64122-Index.db was not released before the reference was garbage collected
{code}"
CASSANDRA-10821,OOM Killer terminates Cassandra when Compactions use too much memory then won't restart," 

We were writing to the DB from EC2 instances in us-east-1 at a rate of about 3000 per second, replication us-east:2 us-west:2, LeveledCompaction and DeflateCompressor.

After about 48 hours some nodes had over 800 pending compactions and a few of them started getting killed for Linux OOM. Priam attempts to restart the nodes, but they fail because of corrupted saved_cahce files.

Loading has finished, and the cluster is mostly idle, but 6 of the nodes were killed again last night by OOM.

This is the log message where the node won't restart:

ERROR [main] 2015-12-05 13:59:13,754 CassandraDaemon.java:635 - Detected unreadable sstables /media/ephemeral0/cassandra/saved_caches/KeyCache-ca.db, please check NEWS.txt and ensure that you have upgraded through all required intermediate versions, running upgradesstables

This is the dmesg where the node is terminated:

[360803.234422] Out of memory: Kill process 10809 (java) score 949 or sacrifice child
[360803.237544] Killed process 10809 (java) total-vm:438484092kB, anon-rss:29228012kB, file-rss:107576kB

This is what Compaction Stats look like currently:

pending tasks: 1096
id compaction type keyspace table completed total unit progress
93eb3200-9b58-11e5-b9f1-ffef1041ec45 Compaction overlordpreprod document 8670748796 839129219651 bytes 1.03%
Compaction system hints 30 1921326518 bytes 0.00%
Active compaction remaining time : 27h33m47s

Only 6 of the 32 nodes have compactions pending, and all on the order of 1000."
CASSANDRA-10787,OutOfMemoryError after few hours from node restart,"Cassandra Cluster was operating flawessly for around 3 months. Lately I've got a critical problem with it - after few hours of running clients are disconnected permanently (that may be Datastax C# Driver problem, though), however few more hours later (with smaller load), on all 2 nodes there is thrown an exception (details in files):

bq. java.lang.OutOfMemoryError: Java heap space


Cases description:

    Case 2 (heavy load):

        - 2015-11-26 16:09:40,834 Restarted all nodes in cassandra cluster
		- 2015-11-26 17:03:46,774 First client disconnected permanently
		- 2015-11-26 22:17:02,327 Node shutdown

	Case 3 (unknown load, different node):
		- 2015-11-26 02:19:49,585 Node shutdown (visible only in systemlog, I don't know why not in debug log)

	Case 4 (low load):
		- 2015-11-27 13:00:24,994 Node restart
		- 2015-11-27 22:26:56,131 Node shutdown

Is that a software issue or I am using too weak Amazon instances? If so, how can the required amount of memory be calculated?"
CASSANDRA-10697,Leak detected while running offline scrub,"I got couple of those:
{code}
ERROR 05:09:15 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3b60e162) to class org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier@1433208674:/var/lib/cassandra/data/sync/entity2-e24b5040199b11e5a30f75bb514ae072/sync-entity2-ka-405434 was not released before the reference was garbage collected
{code}

and then:
{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space

        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.decompressChunk(CompressedRandomAccessReader.java:99)

        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:81)

        at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:353)

        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:444)

        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:424)

        at org.apache.cassandra.io.util.RandomAccessReader.readBytes(RandomAccessReader.java:378)

        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:348)

        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:327)

        at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:397)

        at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:381)

        at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:75)

        at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52)

        at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46)

        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)

        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)

        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:120)

        at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:202)

        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)

        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)

        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:645)

        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)

        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)

        at org.apache.cassandra.db.ColumnIndex$Builder.buildForCompaction(ColumnIndex.java:165)

        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:121)

        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:192)

        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:127)

        at org.apache.cassandra.io.sstable.SSTableRewriter.tryAppend(SSTableRewriter.java:158)

        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:220)

        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:116)
{code}"
CASSANDRA-10688,Stack overflow from SSTableReader$InstanceTidier.runOnClose in Leak Detector,"Running some tests against cassandra-3.0 9fc957cf3097e54ccd72e51b2d0650dc3e83eae0

The tests are just running cassandra-stress write and read while adding and removing nodes from the cluster.  After the test runs when I go back through logs I find the following Stackoverflow fairly often:

ERROR [Strong-Reference-Leak-Detector:1] 2015-11-11 00:04:10,638  Ref.java:413 - Stackoverflow [private java.lang.Runnable org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier.runOnClose, final java.lang.Runnable org.apache.cassandra.io.sstable.format.SSTableReader$DropPageCache.andThen, final org.apache.cassandra.cache.InstrumentingCache org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys.cache, private final org.apache.cassandra.cache.ICache org.apache.cassandra.cache.InstrumentingCache.map, private final com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap org.apache.cassandra.cache.ConcurrentLinkedHashCache.map, final com.googlecode.concurrentlinkedhashmap.LinkedDeque com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.evictionDeque, com.googlecode.concurrentlinkedhashmap.Linked com.googlecode.concurrentlinkedhashmap.LinkedDeque.first, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node 
....... (repeated a whole bunch more) .... 
com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.next, final java.lang.Object com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node.key, public final byte[] org.apache.cassandra.cache.KeyCacheKey.key
"
CASSANDRA-10678,add SSTable flush observer,"Add general interface which can intercept per SSTable flush events e.g. - start of the key, columns etc. Make Index interface return such observer on request, which couples index with corresponding SSTable file if needed."
CASSANDRA-10674,Materialized View SSTable streaming/leaving status race on decommission,"On decommission of a node in a cluster with materialized views, it is possible for the decommissioning node to begin streaming sstables for an MV base table before the receiving node is aware of the leaving status.

The materialized view base/view replica pairing checks pending endpoints to handle the case when an sstable is received from a leaving node; without the leaving message, this check breaks and an exception is thrown. The streamed sstable is never applied.

Logs from a decommissioning node and a node receiving such a stream are attached."
CASSANDRA-10615,Large partition plus row cache causes JVM OutOfMemory Error,"Found on multiple nodes of cluster, restarting C* resulted in hung startup process, nodes never came up to Up Joining, but would hang until OOM. The work-around is to disable row cache. 

{code}
ERROR [SharedPool-Worker-22] 2015-10-27 17:22:52,753  JVMStabilityInspector.java:94 - JVM state determined to be unstable.  Exiting forcefully due to:
java.lang.OutOfMemoryError: Java heap space
        at org.apache.cassandra.io.util.RandomAccessReader.readBytes(RandomAccessReader.java:374) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:348) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:327) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:397) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:381) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:75) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) ~[guava-16.0.1.jar:na]
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138) ~[guava-16.0.1.jar:na]
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:83) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:37) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) ~[guava-16.0.1.jar:na]
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138) ~[guava-16.0.1.jar:na]
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:82) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:173) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:156) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:146) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:125) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) ~[guava-16.0.1.jar:na]
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138) ~[guava-16.0.1.jar:na]
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:264) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:108) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:82) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:69) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:314) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1967) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.service.CacheService$RowCacheSerializer$1.call(CacheService.java:456) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.service.CacheService$RowCacheSerializer$1.call(CacheService.java:451) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
{code}"
CASSANDRA-10614,AssertionError while flushing memtables,"While running mvbench against a single local node, I noticed this stacktrace showing up multiple times in the logs:

{noformat}
ERROR 16:40:01 Exception in thread Thread[MemtableFlushWriter:1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:197) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:116) ~[main/:na]
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:38) ~[main/:na]
	at org.apache.cassandra.db.ColumnIndex.writeAndBuildIndex(ColumnIndex.java:49) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:149) ~[main/:na]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:45) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableTxnWriter.append(SSTableTxnWriter.java:52) ~[main/:na]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:389) ~[main/:na]
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:352) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299) ~[guava-18.0.jar:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1037) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
{noformat}

To reproduce, run mvbench with the regular schema and the following arguments:

{noformat}
mvn exec:java -Dexec.args=""--num-users 100000 --num-songs 1000000 --num-artists 10000 -n 500000 --endpoint 127.0.0.1""
{noformat}"
CASSANDRA-10613,Upgrade test on 2.1->3.0 path fails with NPE in getExistingFiles (likely known bug),"In this job:

http://cassci.datastax.com/view/Upgrades/job/cassandra_upgrade_2.1_to_3.0_proto_v3/10/

The following tests fail due to an NPE in {{org.apache.cassandra.db.lifecycle.LogRecord.getExistingFiles}}:

upgrade_through_versions_test.py:TestUpgrade_from_3_0_latest_tag_to_3_0_HEAD.bootstrap_test
upgrade_through_versions_test.py:TestUpgrade_from_3_0_latest_tag_to_3_0_HEAD.rolling_upgrade_test
upgrade_through_versions_test.py:TestUpgrade_from_3_0_latest_tag_to_3_0_HEAD.parallel_upgrade_with_internode_ssl_test
upgrade_through_versions_test.py:TestUpgrade_from_3_0_latest_tag_to_3_0_HEAD.rolling_upgrade_with_internode_ssl_test
upgrade_through_versions_test.py:TestUpgrade_from_cassandra_2_1_HEAD_to_cassandra_3_0_HEAD.rolling_upgrade_with_internode_ssl_test
upgrade_through_versions_test.py:TestUpgrade_from_3_0_latest_tag_to_3_0_HEAD.parallel_upgrade_test

I believe this is likely happening because of CASSANDRA-10602, so let's hold off on messing with this until that's merged."
CASSANDRA-10608,Adding a dynamic column to a compact storage table with the same name as the partition key causes a memtable flush deadlock,"The reproduction steps for this are as follows: 
# Create the following schema:
{noformat}
CREATE KEYSPACE ks WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': '1'};
CREATE TABLE ks.cf (k int, v int, PRIMARY KEY(k)) WITH COMPACT STORAGE;
{noformat}
# Using the thrift client execute the following:
{noformat}
        Column column = new Column(ByteBufferUtil.bytes(""k""));
        column.setValue(ByteBufferUtil.bytes(1));
        column.setTimestamp(1);
        client.insert(key, new ColumnParent(compactCf), column, ConsistencyLevel.ONE);
{noformat}

This causes an invalid {{PartitionUpdate}} to be added to {{Memtable}} containing a {{PartitionColumns}} containing the partition key {{ColumnDefinition}}.

This happens because {{LegacyLayout.decodeCellName}} does not check whether the {{ColumnDefinition}} is a partition key "
CASSANDRA-10602,2 upgrade test failures: static_columns_paging_test and multi_list_set_test,"The two following test throws a NPE:
* http://cassci.datastax.com/job/cassandra-3.0_dtest/293/testReport/junit/upgrade_tests.paging_test/TestPagingDataNodes2RF1/static_columns_paging_test/
* http://cassci.datastax.com/job/cassandra-3.0_dtest/293/testReport/junit/upgrade_tests.cql_tests/TestCQLNodes3RF3/multi_list_set_test/
"
CASSANDRA-10587,sstablemetadata NPE on cassandra 2.2,"I have recently upgraded my cassandra cluster to 2.2, currently running 2.2.3. After running the first repair, cassandra renames the sstables to the new naming schema that does not contain the keyspace name.

 This causes sstablemetadata to fail with the following stack trace:


{noformat}
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:275)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:172)
        at org.apache.cassandra.tools.SSTableMetadataViewer.main(SSTableMetadataViewer.java:52)
{noformat}"
CASSANDRA-10581,"Update cassandra.yaml comments to reflect memory_allocator deprecation, remove in 3.0","Looks like in 2.2+ changing the {{memory_allocator}} field in cassandra.yaml has no effect:

https://github.com/apache/cassandra/commit/0d2ec11c7e0abfb84d872289af6d3ac386cf381f#diff-b66584c9ce7b64019b5db5a531deeda1R207

The instructions in comments on how to use jemalloc haven't been updated to reflect this change. [~snazy], is that an accurate assessment?

[~iamaleksey] How do we want to handle the deprecation more generally? Warn on 2.2, remove in 3.0?

EDIT: I described this in a way that isn't very clear to those unfamiliar with Robert's change. To be clear: You can still use JEMAlloc with Cassandra. The {{memory_allocator}} option was deprecated in favor of automatically detecting if JEMAlloc is installed, and, if so, using it. "
CASSANDRA-10579,IndexOutOfBoundsException during memtable flushing at startup (with offheap_objects),"Sometimes we have problems at startup where memtable flushes with an index out of bounds exception as seen below. Cassandra is then dead in the water until we track down the corresponding commit log via the segment ID and remove it:

{code}
INFO  [main] 2015-10-23 14:43:36,440 CommitLogReplayer.java:267 - Replaying /home/y/var/cassandra/commitlog/CommitLog-4-1445474832692.log
INFO  [main] 2015-10-23 14:43:36,440 CommitLogReplayer.java:270 - Replaying /home/y/var/cassandra/commitlog/CommitLog-4-1445474832692.log (CL version 4, messaging version 8)
INFO  [main] 2015-10-23 14:43:36,594 CommitLogReplayer.java:478 - Finished reading /home/y/var/cassandra/commitlog/CommitLog-4-1445474832692.log
INFO  [main] 2015-10-23 14:43:36,594 CommitLogReplayer.java:267 - Replaying /home/y/var/cassandra/commitlog/CommitLog-4-1445474832693.log
INFO  [main] 2015-10-23 14:43:36,595 CommitLogReplayer.java:270 - Replaying /home/y/var/cassandra/commitlog/CommitLog-4-1445474832693.log (CL version 4, messaging version 8)
INFO  [main] 2015-10-23 14:43:36,699 CommitLogReplayer.java:478 - Finished reading /home/y/var/cassandra/commitlog/CommitLog-4-1445474832693.log
INFO  [main] 2015-10-23 14:43:36,699 CommitLogReplayer.java:267 - Replaying /home/y/var/cassandra/commitlog/CommitLog-4-1445474832694.log
INFO  [main] 2015-10-23 14:43:36,699 CommitLogReplayer.java:270 - Replaying /home/y/var/cassandra/commitlog/CommitLog-4-1445474832694.log (CL version 4, messaging version 8)
WARN  [SharedPool-Worker-5] 2015-10-23 14:43:36,747 AbstractTracingAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-5,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: 6
        at org.apache.cassandra.db.AbstractNativeCell.nametype(AbstractNativeCell.java:204) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.AbstractNativeCell.isStatic(AbstractNativeCell.java:199) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.composites.AbstractCType.compare(AbstractCType.java:166) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:61) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:58) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.utils.btree.BTree.find(BTree.java:277) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.utils.btree.NodeBuilder.update(NodeBuilder.java:154) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.utils.btree.Builder.update(Builder.java:74) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.utils.btree.BTree.update(BTree.java:186) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.AtomicBTreeColumns.addAllWithSizeDelta(AtomicBTreeColumns.java:225) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.Memtable.put(Memtable.java:210) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1225) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:396) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:359) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer$1.runMayThrow(CommitLogReplayer.java:455) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_31]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) ~[apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-2.1.10.jar:2.1.10-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
{code}"
CASSANDRA-10550,NPE on null 'highestSelectivityIndex()',{{org.apache.cassandra.db.index.SecondaryIndexSearcher.highestSelectivityIndex()}} might return 'null' which makes {{org.apache.cassandra.service.StorageProxy.estimateResultRowsPerRange()}} NPE on some custom index implementations.
CASSANDRA-10543,Self-reference leak in SegmentedFile,"CASSANDRA-9839, which moved {{crc_check_chance}} out of compression params and made it a top level table property introduced a reference leak in {{SegmentedFile}}. See [this comment|https://issues.apache.org/jira/browse/CASSANDRA-9839?focusedCommentId=14960528&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14960528 ] from [~benedict]"
CASSANDRA-10510,"Compacted SSTables failing to get removed, overflowing disk","Short version: it appears that if the resulting SSTable of a compaction enters another compaction soon after, the SSTables participating in the former compaction don't get deleted from disk until Cassandra is restarted.

We have run into a big problem after applying CASSANDRA-10276 and CASSANDRA-10280, backported to 2.0.14. But the bug we're seeing is not introduced by these patches, it has just made itself very apparent and harmful.

Here's what has happened. We had repair running on our table that is a time series and uses DTCS. The ring was split into 5016 small ranges being repaired one after the other (using parallel repair, i.e. not snapshot repair). This causes a flood of tiny SSTables to get streamed into all nodes (we don't use vnodes), with timestamp ranges similar to existing SSTables on disk. The problem with that is the sheer number of SSTables, disk usage is not affected. This has been reported before, see CASSANDRA-9644. These SSTables are streamed continuously for up to a couple of days.

The patches were applied to fix the problem of ending up with tens of thousands of SSTables that would never get touched by DTCS. But now that DTCS does touch them, we have run into a new problem instead. While disk usage was in the 25-30% neighborhood before repairs began, disk usage started growing fast when these continuous streams started coming in. Eventually, a couple of nodes ran out of disk, which led us to stop all the repairing on the cluster.

This didn't reduce the disk usage. Compactions were of course very active. More than doubling disk usage should not be possible, regardless of the choices your compaction strategy makes. And we were not getting magnitudes of data streamed in. Large quantities of SSTables, yes, but this was the nodes creating more data out of thin air.

We have a tool to show timestamp and size metadata of SSTables. What we found, looking at all non-tmp data files, was something akin to duplicates of almost all the largest SSTables. Not quite exact replicas, but there were these multi-gigabyte SSTables covering exactly the same range of timestamps and differing in size by mere kilobytes. There were typically 3 of each of the largest SSTables, sometimes even more.

Here's what I suspect: DTCS is the only compaction strategy that would commonly finish compacting a really large SSTable and on the very next run of the compaction strategy nominate the result for yet another compaction. Even together with tiny SSTables, which certainly happens in our scenario. Potentially, the large SSTable that participated in the first compaction might even get nominated again by DTCS, if for some reason it can be returned by getUncompactingSSTables.

Whatever the reason, I have collected evidence showing that these large ""duplicate"" SSTables are of the same ""lineage"". Only one should remain on disk: the latest one. The older ones have already been compacted, resulting in the newer ones. But for some reason, they never got deleted from disk. And this was really harmful when combining DTCS with continuously streaming in tiny SSTables. The same but worse would happen without the patches and uncapped max_sstable_age_days.

Attached is one occurrence of 3 duplicated SSTables, their metadata and log lines about their compactions. You can see how similar they were to each other. SSTable generations 374277, 374249, 373702 (the large one), 374305, 374231 and 374333 completed compaction at 04:05:26,878, yet they were all still on disk over 6 hours later. At 04:05:26,898 the result, 374373, entered another compaction with 375174. They also stayed around after that compaction finished. Literally all SSTables named in these log lines were still on disk when I checked! Only one should have remained: 375189.

Now this was just one random example from the data I collected. This happened everywhere. Some SSTables should probably have been deleted a day before.

However, once we restarted the nodes, all of the duplicates were suddenly gone!"
CASSANDRA-10503,NPE in MVs on update,"I've stumbled upon an NPE in MVs on update. This script will reproduce 100% on trunk from {{Date:   Sat Oct 10 09:23:15 2015 +0100}}

{code}
ERROR [SharedPool-Worker-3] 2015-10-10 21:35:01,867 Keyspace.java:487 - Unknown exception caught while attempting to update MaterializedView! test.test_with_cluster
java.lang.NullPointerException: null
        at org.apache.cassandra.db.view.TemporalRow.clusteringValue(TemporalRow.java:381) ~[main/:na]
        at org.apache.cassandra.db.view.View.createUpdatesForInserts(View.java:355) ~[main/:na]
        at org.apache.cassandra.db.view.View.createMutations(View.java:664) ~[main/:na]
        at org.apache.cassandra.db.view.ViewManager.pushViewReplicaUpdates(ViewManager.java:130) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:482) [main/:na]
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:387) [main/:na]
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:205) [main/:na]
        at org.apache.cassandra.service.StorageProxy$$Lambda$149/1333013217.run(Unknown Source) [main/:na]
        at org.apache.cassandra.service.StorageProxy$7.runMayThrow(StorageProxy.java:1247) [main/:na]
        at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2399) [main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{code}

And the script to trigger:

{code}
ccm remove test; 
ccm create test --install-dir=/Users/jeff/Desktop/Dev/cassandra/ -s -n 1 ; 
echo ""create keyspace test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; use test; create table test ( id text primary key, last text, first text, high int, low int); insert into test(id,last,first,high,low) values ('a', 'a', 'a', 1, 1); insert into test(id,last,first,high,low) values ('a', 'b', 'b', 2, 2); insert into test(id,last,first,high,low) values ('a', 'c', 'c', 3, 3); insert into test(id,last,first,high,low) values ('a', 'e', 'e', 5, 5); insert into test(id,last,first,high,low) values ('a', 'd', 'd', 4, 4); select * from test where id='a';"" | ccm node1 cqlsh

echo ""Creating MV test_by_high on test""
echo ""use test; create materialized view test_by_high as select id, high from test where high is not null primary key(high, id);"" | ccm node1 cqlsh

echo ""Insert high score 6, this will succeed""
echo ""use test; insert into test(id,last,first,high,low) values ('a', 'f', 'f', 6, 6); "" | ccm node1 cqlsh 

sleep 1

echo ""Select from MV where score = 6, this will succeed""
echo ""use test; select * from test_by_high where high=6; "" | ccm node1 cqlsh

echo ""Create a larger table with clustering key""
echo ""use test; create table test_with_cluster(part text, clus text, last text, first text, high int, low int, primary key (part, clus));"" | ccm node1 cqlsh

echo ""use test; create materialized view high_view as select part, clus, high from test_with_cluster where part is not null and clus is not null and high is not null primary key(high, part, clus);"" | ccm node1 cqlsh

echo ""Populate test_with_cluster, this will break""
echo ""use test; insert into test_with_cluster(part, clus,last,first,high,low) values ('a', 'a', 'a', 'a', 1, 1); "" | ccm node1 cqlsh
{code}

Logs from my previous tests (which I've deleted, unfortunately) suggest that the NPE is due to using the wrong {{ColumnIdentifier}} - it's using {{id}} (from test.test?) which causes the NPE in {{clusteringValue()}}, since it's in the wrong base table. "
CASSANDRA-10497,NPE on removeUnfinishedCompactionLeftovers after sstablesplit,"After stopping the node and running {{sstablesplit}} on a single table, restarting Cassandra results in an NPE:

{noformat}
INFO  [SSTableBatchOpen:2] 2015-10-09 13:15:38,745 SSTableReader.java:471 - Opening /var/lib/cassandra/xvdd/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-514 (175 bytes)
INFO  [main] 2015-10-09 13:15:38,747 AutoSavingCache.java:146 - reading saved cache /var/lib/cassandra/xvdb/cache/system-schema_keyspaces-b0f2235744583cdb9631c43e59ce3676-KeyCache-b.db
ERROR [main] 2015-10-09 13:15:39,114 CassandraDaemon.java:541 - Exception encountered during startup
org.apache.cassandra.io.FSReadError: java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:641) ~[apache-cassandra-2.1.7-SNAPSHOT.jar:2.1.7-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:302) [apache-cassandra-2.1.7-SNAPSHOT.jar:2.1.7-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:524) [apache-cassandra-2.1.7-SNAPSHOT.jar:2.1.7-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:613) [apache-cassandra-2.1.7-SNAPSHOT.jar:2.1.7-SNAPSHOT]
Caused by: java.lang.NullPointerException: null
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:633) ~[apache-cassandra-2.1.7-SNAPSHOT.jar:2.1.7-SNAPSHOT]
        ... 3 common frames omitted
{noformat}

The node would only come back up after deleting all the files in compactions_in_progress."
CASSANDRA-10476,Fix upgrade paging dtest failures on 2.2->3.0 path,"EDIT: this list of failures is no longer current; see comments for current failures.

The following upgrade tests for paging features fail or flap on the upgrade path from 2.2 to 3.0:

- {{upgrade_tests/paging_test.py:TestPagingData.static_columns_paging_test}}
- {{upgrade_tests/paging_test.py:TestPagingSize.test_undefined_page_size_default}}
- {{upgrade_tests/paging_test.py:TestPagingSize.test_with_more_results_than_page_size}}
- {{upgrade_tests/paging_test.py:TestPagingWithDeletions.test_failure_threshold_deletions}}
- {{upgrade_tests/paging_test.py:TestPagingWithDeletions.test_multiple_cell_deletions}}
- {{upgrade_tests/paging_test.py:TestPagingWithDeletions.test_single_cell_deletions}}
- {{upgrade_tests/paging_test.py:TestPagingWithDeletions.test_single_row_deletions}}
- {{upgrade_tests/paging_test.py:TestPagingDatasetChanges.test_cell_TTL_expiry_during_paging/}}

I've grouped them all together because I don't know how to tell if they're related; once someone triages them, it may be appropriate to break this out into multiple tickets.

The failures can be found here:

http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/44/testReport/upgrade_tests.paging_test/TestPagingData/static_columns_paging_test/history/
http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/44/testReport/upgrade_tests.paging_test/TestPagingSize/test_undefined_page_size_default/history/
http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/42/testReport/upgrade_tests.paging_test/TestPagingSize/test_with_more_results_than_page_size/history/
http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/44/testReport/upgrade_tests.paging_test/TestPagingWithDeletions/test_failure_threshold_deletions/history/
http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/44/testReport/upgrade_tests.paging_test/TestPagingWithDeletions/test_multiple_cell_deletions/history/
http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/44/testReport/upgrade_tests.paging_test/TestPagingWithDeletions/test_single_cell_deletions/history/
http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/44/testReport/upgrade_tests.paging_test/TestPagingWithDeletions/test_single_row_deletions/history/
http://cassci.datastax.com/view/Upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_HEAD/44/testReport/upgrade_tests.paging_test/TestPagingDatasetChanges/test_cell_TTL_expiry_during_paging/

Once [this dtest PR|https://github.com/riptano/cassandra-dtest/pull/586] is merged, these tests should also run with this upgrade path on normal 3.0 jobs. Until then, you can run them with the following command:

{code}
SKIP=false CASSANDRA_VERSION=binary:2.2.0 UPGRADE_TO=git:cassandra-3.0 nosetests upgrade_tests/paging_test.py:TestPagingData.static_columns_paging_test upgrade_tests/paging_test.py:TestPagingSize.test_undefined_page_size_default upgrade_tests/paging_test.py:TestPagingSize.test_with_more_results_than_page_size upgrade_tests/paging_test.py:TestPagingWithDeletions.test_failure_threshold_deletions upgrade_tests/paging_test.py:TestPagingWithDeletions.test_multiple_cell_deletions upgrade_tests/paging_test.py:TestPagingWithDeletions.test_single_cell_deletions upgrade_tests/paging_test.py:TestPagingWithDeletions.test_single_row_deletions
upgrade_tests/paging_test.py:TestPagingDatasetChanges.test_cell_TTL_expiry_during_paging
{code}"
CASSANDRA-10450,upgrade_tests/paging_test.py:TestPagingWithDeletions.test_single_cell_deletions flapped on CassCI,"This test has failed with a single flap:

http://cassci.datastax.com/view/trunk/job/trunk_dtest/lastCompletedBuild/testReport/upgrade_tests.paging_test/TestPagingWithDeletions/test_single_cell_deletions/history/

This may not be worth blocking release on; I haven't been able to reproduce on openstack, and it's only flapped once."
CASSANDRA-10442,Paging repeats records,"Paging repeats records every fetchSize records. The following sample easily reproduces the problem on Cassandra 2.0.16 with Java Driver 2.0.11.

{noformat}
public class TestPagingBug
{
	public static void main(String[] args)
	{
		Cluster.Builder builder = Cluster.builder();
		Cluster c = builder.addContactPoints(""192.168.98.190"").build();		
		Session s = c.connect();
		
		s.execute(""CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 }"");
		s.execute(""CREATE TABLE IF NOT EXISTS test.test_page(id INT, sec BIGINT, data VARCHAR static, PRIMARY KEY ((id), sec))"");

		s.execute(""INSERT INTO test.test_page (id, data) VALUES (1, 'asdfasdfasdfasdfasdfasdf')"");
		
		PreparedStatement insert = s.prepare(""INSERT INTO test.test_page (id, sec) VALUES (1, ?)"");		
		for (int i = 0; i < 1000; i++)
			s.execute(insert.bind((long) i));
		
		PreparedStatement select = s.prepare(""SELECT sec FROM test.test_page WHERE id = 1"");
		
		long lastSec = -1;		
		for (Row row : s.execute(select.bind().setFetchSize(300)))
		{
			long sec = row.getLong(""sec"");
			if (sec == lastSec)
				System.out.println(String.format(""Duplicated id %d"", sec));
			
			lastSec = sec;
		}
		System.exit(0);
	}
}
{noformat}

The program outputs the following:

Duplicated id 299
Duplicated id 598
Duplicated id 897

Note that the static column is required. This bug doesn't occur if you remove the column from the schema.

I realize that this may be a driver bug, but I don't really know, so I'm logging it here until that can be determined."
CASSANDRA-10429,Flush schema tables after local schema change,"In CASSANDRA-7327, we disabled the normal flush of system schema tables after ""local"" schema changes to improve the runtime of unit tests.  However, there are some cases where this flush is necessary for schema durability.  For example, if a custom secondary index needs to make schema changes as part of it's creation, this is desirable."
CASSANDRA-10425,Autoselect GC settings depending on system memory,"1) Make GC modular within cassandra-env
2) For systems with 32GB or less of ram, use the classic CMS with the established default settings.
3) For systems with 48GB or more of ram, use 1/2 or up to 32GB of heap with G1, whichever is lower.
"
CASSANDRA-10419,Make JBOD compaction and flushing more robust,"With JBOD and several smaller disks, like SSDs at 1.2 TB or lower, it is possible to run out of space prematurely. With a sufficient ingestion rate, disk selection logic seems to overselect on certain JBOD targets. This causes a premature C* shutdown when there is a significant amount of space left. With DTCS, for example, it should be possible to utilize over 90% of the available space with certain settings. However in the scenario I tested, only about 50% was utilized, before a filesystem full error. (see below). It is likely that this is a scheduling challenge between high rates of ingest and smaller data directories. It would be good to use an anticipatory model if possible to more carefully select compaction targets according to fill rates. As well, if the largest sstable that can be supported is constrained by the largest JBOD extent, we should make that visible to the compaction logic where possible.

The attached image shows a test with 12 1.2TB JBOD data directories. At the end, the utilizations are:
59GiB, 83GiB, 83GiB, 97GiB, 330GiB, 589GiB, 604GiB, 630GiB, 697GiB, 1.055TiB, 1.083TB, 1.092TiB,  "
CASSANDRA-10402,reduce the memory usage of Metadata.tokenMap.tokenToHost,"My application uses 2000+ keyspaces, and will dynamically create keyspaces and tables. And then in java client, the Metadata.tokenMap.tokenToHost would use about 1g memory. so this will cause a lot of  full gc.
   As I see, the key of the tokenToHost is keyspace, and the value is a tokenId_to_replicateNodes map.

   When I try to solve this problem, I find something not sure: * all keyspaces have same 'tokenId_to_replicateNodes' map *.
    My replication strategy of all keyspaces is : simpleStrategy and replicationFactor is 3

    So would it be possible if keyspaces use same strategy, the value of tokenToHost map use a same map. So it would extremely reduce the memory usage

     ps： the following is effected code:
for (KeyspaceMetadata keyspace : keyspaces)
{
    ReplicationStrategy strategy = keyspace.replicationStrategy();
    Map<Token, Set<Host>> ksTokens = (strategy == null)
        ? makeNonReplicatedMap(tokenToPrimary)
        : strategy.computeTokenToReplicaMap(tokenToPrimary, ring);

    tokenToHosts.put(keyspace.getName(), ksTokens);
}

tokenToPrimary is all same, ring is all same, and if strategy is all same , strategy.computeTokenToReplicaMap would return 'same' map but different object( cause every calling returns a new HashMap)"
CASSANDRA-10393,LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref),"When trying to repair full on a table with the following schema my nodes stall 
and end up with spamming this 

I've recently changed the table from SizeTieredCompactionStrategy to LeveledCompactionStrategy.

Coming from 2.1.9 -> 2.2.0 -> 2.2.1 i ran upgradesstable without issue as well

When trying to full repair post compaction change, I got ""out of order"" errors. A few google searches later, I was told to ""scrub"" the keyspace - did that during the night (no problems logged, and no data lost)

Now a repair just stalls and output memory leaks all over the place 

{code}
CREATE KEYSPACE sessions WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}  AND durable_writes = true;

CREATE TABLE sessions.sessions (
    id text PRIMARY KEY,
    client_ip text,
    controller text,
    controller_action text,
    created timestamp,
    data text,
    expires timestamp,
    http_host text,
    modified timestamp,
    request_agent text,
    request_agent_bot boolean,
    request_path text,
    site_id int,
    user_id int
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""NONE"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
{code}

{code}
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4428a373) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1811114765:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104037-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@368dd97) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1811114765:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104037-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@66fb78be) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1811114765:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104037-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@9fdd2e6) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1460906269:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104788-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@84fcb91) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1460906269:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104788-big was not released before the reference was garbage collected
{code}"
CASSANDRA-10392,Allow Cassandra to trace to custom tracing implementations,"It can be possible to use an external tracing solution in Cassandra by abstracting out the writing of tracing to system_traces tables in the tracing package to separate implementation classes and leaving abstract classes in place that define the interface and behaviour otherwise of C* tracing.

Then via a system property ""cassandra.custom_tracing_class"" the Tracing class implementation could be swapped out with something third party.

An example of this is adding Zipkin tracing into Cassandra in the Summit [presentation|http://thelastpickle.com/files/2015-09-24-using-zipkin-for-full-stack-tracing-including-cassandra/presentation/tlp-reveal.js/tlp-cassandra-zipkin.html]. Code for the implemented Zipkin plugin can be found at https://github.com/thelastpickle/cassandra-zipkin-tracing/

In addition this patch passes the custom payload through into the tracing session allowing a third party tracing solution like Zipkin to do full-stack tracing from clients through and into Cassandra."
CASSANDRA-10381,NullPointerException in cqlsh paging through CF with static columns,"When running select count( * ) from cqlsh with limit, the following NPE occurs:

select count( * ) from tbl1 limit 50000 ; 
{code}
ERROR [SharedPool-Worker-4] 2015-09-16 14:49:43,480 QueryMessage.java:132 - Unexpected error during query
java.lang.NullPointerException: null
at org.apache.cassandra.service.pager.RangeSliceQueryPager.containsPreviousLast(RangeSliceQueryPager.java:99) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:119) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.service.pager.RangeSliceQueryPager.fetchPage(RangeSliceQueryPager.java:37) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.statements.SelectStatement.pageCountQuery(SelectStatement.java:286) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:230) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:67) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler$StatementExecution.execute(DseQueryHandler.java:291) ~[dse-4.7.2.jar:4.7.2]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithTiming(DseQueryHandler.java:223) ~[dse-4.7.2.jar:4.7.2]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithAuditLogging(DseQueryHandler.java:259) ~[dse-4.7.2.jar:4.7.2]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler.process(DseQueryHandler.java:94) ~[dse-4.7.2.jar:4.7.2]
at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:119) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_75]
at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
{code}

Table definition looks something like:
{code}
CREATE TABLE tbl1 (
    field1 bigint,
    field2 int,
    field3 timestamp,
    field4 map<int, float>,
    field5 text static,
    field6 text static,
    field7 text static
    PRIMARY KEY (field1, field2, field3)
) WITH CLUSTERING ORDER BY (field2 ASC, field3 ASC)
    AND bloom_filter_fp_chance = 0.1
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
   ...
{code}
Following appears in debug log leading up to the error:
{code}
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,484  AbstractQueryPager.java:95 - Fetched 101 live rows
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,484  AbstractQueryPager.java:133 - Remaining rows to page: 1
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,485  SelectStatement.java:285 - New maxLimit for paged count query is 1
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,486  StorageProxy.java:1646 - Estimated result rows per range: 2586.375; requested rows: 2, ranges.size(): 762; concurrent range requests: 1
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,487  AbstractQueryPager.java:95 - Fetched 2 live rows
ERROR [SharedPool-Worker-1] 2015-09-17 15:32:06,487  QueryMessage.java:132 - Unexpected error during query
java.lang.NullPointerException: null
{code}

I'm working on recreating to have a workable dataset.  When running cqlsh from remote node version 2.0.14, query returns successfully"
CASSANDRA-10376,Fix upgrade_tests/paging_test.py:TestPagingData.static_columns_paging_test,"Follow-up to CASSANDRA-10354 to fix the related upgrade issue.

To quote [~bdeggleston]:
{quote}
So the failure is caused by an edge case where a names filter is used in a paging query against a table that needs SinglePartitionNamesCommand instances converted to SinglePartitionSliceCommand instances in order to be converted to legacy read commands.

If the previous read returned all requested clusterings and a number of rows equal to the page size, the subsequent read would have an empty clustering names filter. When an empty clustering names filter is converted to a slice filter, the slice filter is created with Slices.ALL.
{quote}"
CASSANDRA-10363,"NullPointerException returned with select ttl(value), IN, ORDER BY and paging off","Running this query with paging off returns a NullPointerException:

cqlsh:test> SELECT value, ttl(value), last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; 
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.NullPointerException"">

Here's the stack trace from the system.log:

ERROR [SharedPool-Worker-1] 2015-09-17 13:11:03,937  ErrorMessage.java:251 - Unexpected exception during request
java.lang.NullPointerException: null
        at org.apache.cassandra.db.marshal.LongType.compareLongs(LongType.java:41) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.db.marshal.TimestampType.compare(TimestampType.java:48) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.db.marshal.TimestampType.compare(TimestampType.java:38) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:2419) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:2406) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at java.util.TimSort.countRunAndMakeAscending(TimSort.java:351) ~[na:1.8.0_40]
        at java.util.TimSort.sort(TimSort.java:216) ~[na:1.8.0_40]
        at java.util.Arrays.sort(Arrays.java:1512) ~[na:1.8.0_40]
        at java.util.ArrayList.sort(ArrayList.java:1454) ~[na:1.8.0_40]
        at java.util.Collections.sort(Collections.java:175) ~[na:1.8.0_40]
        at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:1400) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1255) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:299) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:276) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:224) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:67) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at com.datastax.bdp.cassandra.cql3.DseQueryHandler$StatementExecution.execute(DseQueryHandler.java:291) ~[dse.jar:4.7.3]
        at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithTiming(DseQueryHandler.java:223) ~[dse.jar:4.7.3]
        at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithAuditLogging(DseQueryHandler.java:259) ~[dse.jar:4.7.3]
        at com.datastax.bdp.cassandra.cql3.DseQueryHandler.process(DseQueryHandler.java:94) ~[dse.jar:4.7.3]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:122) ~[cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [cassandra-all-2.1.8.689.jar:2.1.8.689]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [cassandra-all-2.1.8.689.jar:2.1.8.689]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [cassandra-all-2.1.8.689.jar:2.1.8.689]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]

Here's the full reproduction:

CREATE KEYSPACE TEST 
WITH replication = {'class': 'SimpleStrategy', 'replication_factor':3} 
AND durable_writes = true;

USE test;

CREATE TABLE test ( 
useruid varchar, 
direction varchar, 
last_modified timestamp, 
value varchar, 
PRIMARY KEY ((useruid, direction), last_modified) 
);

//insert 4 entries in the table 
INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'out', '2013-05-13 15:18:51', 'a value1'); 
INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'out', '2013-05-13 15:12:51', 'a value2'); 
INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'none', '2013-05-13 15:20:51', 'a value3'); 
INSERT INTO test (useruid,direction,last_modified,value) VALUES ('userid1', 'in', '2013-05-13 15:34:51', 'a value4');

First query to check the value in the table, and its results : 

SELECT value, ttl(value), last_modified FROM test; 
value | ttl(value) | last_modified
----------+------------+--------------------------
a value4 | null | 2013-05-13 15:34:51+0000
a value2 | null | 2013-05-13 15:12:51+0000
a value1 | null | 2013-05-13 15:18:51+0000
a value3 | null | 2013-05-13 15:20:51+0000

(4 rows)

Run this query using the IN clause and the ORDER BY clause, but it fails with an error:

SELECT value, ttl(value), last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; 
InvalidRequest: code=2200 [Invalid query] message=""Cannot page queries with both ORDER BY and a IN restriction on the partition key; you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query""

If you run the same query without the ttl(value) in the SELECT part, it also shows the same error:
 
SELECT value, last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; 
InvalidRequest: code=2200 [Invalid query] message=""Cannot page queries with both ORDER BY and a IN restriction on the partition key; you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query""

This message suggests these JIRAs are the reason for this message:

https://issues.apache.org/jira/browse/CASSANDRA-7853 select . . . in . . . order by regression
Resolution: Duplicate of CASSANDRA-7514
Fix Version/s: None

https://issues.apache.org/jira/browse/CASSANDRA-7514 Support paging in cqlsh
Resolution: Fixed
Fix Version/s: 2.1.1

https://issues.apache.org/jira/browse/CASSANDRA-6722 cross-partition ordering should have warning or be disallowed when paging
Resolution: Fixed
Fix Version/s: 2.0.6

If I turn off paging:

cqlsh:test> paging off;
Disabled Query paging.

Then re-run the query without the ttl(value) I see the results:

cqlsh:test> SELECT value, last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified;

value | last_modified
----------+--------------------------
a value2 | 2013-05-13 15:12:51+0000
a value1 | 2013-05-13 15:18:51+0000
a value4 | 2013-05-13 15:34:51+0000

(3 rows)

However, if you now re-run this query with the ttl(value) you get a NullPointerException:

cqlsh:test> SELECT value, ttl(value), last_modified FROM test where useruid='userid1' AND direction IN ('out','in') ORDER BY last_modified; 
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.NullPointerException"">"
CASSANDRA-10354,Invalid paging with DISTINCT on static and IN,"Basically, the test from CASSANDRA-10352 happens to fail on 3.0, but not for the reason in CASSANDRA-10352 but rather because it incorrectly return duplicate results. This is also the reason for the failure of {{upgrade_tests/paging_test.py:TestPagingData.static_columns_paging_test}} once that test doesn't run into CASSANDRA-10352 (which it currently does)."
CASSANDRA-10352,Paging with DISTINCT and IN can throw ClassCastException,"The ctor of {{SliceQueryPager}} has the following code:
{noformat}
        if (state != null)
        {
            // The only case where this could be a non-CellName Composite is if it's Composites.EMPTY, but that's not
            // valid for PagingState.cellName, so we can safely cast to CellName.
            lastReturned = (CellName) cfm.comparator.fromByteBuffer(state.cellName);
            restoreState(state.remaining, true);
        }
{noformat}
The assumption that we can only get a {{CellName}} is invalid however. When using a {{MultiPartitionPager}} (so when the query has a {{IN}} on the partition key), it's possible for a page to stop at the end of one of the underlying pager, and for that pager to be exhausted. If that's the case, the {{PagingState}} returned will have the partition key of the next key to retrieve, but a {{null}} cellname (since we haven't started to query that next key). When the next page is queried, that {{PagingState}} will trigger a {{ClassCastException}}.

The [following dtest|https://github.com/pcmanus/cassandra-dtest/commit/b98a112fc22f92aa965d2068345c708ae43b8a85] reproduces this problem."
CASSANDRA-10317,some paging dtests still flap on trunk,"This is follow-up to CASSANDRA-9775. It appears that two of the paging dtests are still flapping or have started flapping again:

http://cassci.datastax.com/job/trunk_dtest-skipped-with-require/lastCompletedBuild/testReport/upgrade_tests.paging_test/TestPagingData/static_columns_paging_test/history/

http://cassci.datastax.com/job/trunk_dtest-skipped-with-require/lastCompletedBuild/testReport/upgrade_tests.paging_test/TestPagingWithDeletions/history/

[~blerer] Since you worked on 9775, may I assign this to you? I also can add the tests back to the regular dtest job if you like."
CASSANDRA-10267,Failing tests in upgrade_trests.paging_test,This is a continuation of CASSANDRA-9893 to deal with the failure of the {{upgrade_trests.paging_test}} tests.
CASSANDRA-10260,NPE in SSTableReader.invalidateCacheKey,"{code}
ERROR [NonPeriodicTasks:1] 2015-09-03 05:24:51,056 CassandraDaemon.java:223 - Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.io.sstable.SSTableReader.invalidateCacheKey(SSTableReader.java:1445) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.SSTableRewriter$1.run(SSTableRewriter.java:323) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.SSTableReader$5.run(SSTableReader.java:1024) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier$1.run(SSTableReader.java:2146) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[na:1.8.0_51]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[na:1.8.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
{code}

Seen in 2.1.7 in a node running cleanup. May be fixed already, logging to be safe. 
"
CASSANDRA-10254,3.0 paging states are incompatible with pre-3.0 nodes,"When running the upgrade dtest {{TestCQL.static_columns_with_distinct_test}}, the client appears to be passing in an invalid paging state (no cell name). Instead of catching the problem in validation and returning an error to the client, the select statement tries to run it, causing this exception:

{noformat}
ERROR QueryMessage.java:135 - Unexpected error during query
java.lang.ClassCastException: org.apache.cassandra.db.composites.Composites$EmptyComposite cannot be cast to org.apache.cassandra.db.composites.CellName
	at org.apache.cassandra.service.pager.SliceQueryPager.<init>(SliceQueryPager.java:64) ~[main/:na]
	at org.apache.cassandra.service.pager.MultiPartitionPager.makePager(MultiPartitionPager.java:93) ~[main/:na]
	at org.apache.cassandra.service.pager.MultiPartitionPager.<init>(MultiPartitionPager.java:75) ~[main/:na]
	at org.apache.cassandra.service.pager.QueryPagers.pager(QueryPagers.java:102) ~[main/:na]
	at org.apache.cassandra.service.pager.QueryPagers.pager(QueryPagers.java:126) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:228) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:67) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:260) ~[main/:na]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:122) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]
{noformat}

Running {code} UPGRADE_MODE=none nosetests upgrade_tests/cql_tests.py:TestCQL.static_columns_with_distinct_test{code} should cause the error."
CASSANDRA-10234,Add nodetool gettraceprobability command,nodetool has a settraceprobability command but there is no way to get the value from the nodetool command. Attaching patch that adds the gettraceprobability command.
CASSANDRA-10194,Deadlock on startup,"Possible deadlock on startup. Seen on 2.1.5 and 2.1.7. Thread dump attached. 
"
CASSANDRA-10183,logger.trace statements in hot code path in CodecRegistry,"There are a number of logger.trace statements in CodecRegistry that should be updated to log conditionally based on whether or not trace is enabled, i.e.:

{code:java}
        logger.trace(""Querying cache for codec [{} <-> {}]"", cqlType, javaType);
{code}

should be:

{code:java}
        if(logger.isTraceEnabled())
            logger.trace(""Querying cache for codec [{} <-> {}]"", cqlType, javaType);
{code}
"
CASSANDRA-10181,Deadlock flushing tables with CUSTOM indexes,"In 3.0, if a table with a CUSTOM secondary index is force flushed, Cassandra will deadlock while attempting to perform a blocking flush on the tables backing the secondary indexes.

The basic problem is that the base table's post-flush task ends up waiting on the post-flush task for the secondary index to complete.  However, since the post-flush executor is single-threaded, this results in a deadlock.

Here's the partial stacktrace for the base table part of this (line numbers may not be 100% accurate):
{noformat}
org.apache.cassandra.db.ColumnFamilyStore.forceBlockingFlush(ColumnFamilyStore.java:927)
	at org.apache.cassandra.index.internal.CustomIndex.lambda$getBlockingFlushTask$0(VertexCentricIndex.java:114)
	at org.apache.cassandra.index.internal.CustomIndex$$Lambda$95/057902870.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134)
	at com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58)
	at com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:37)
	at org.apache.cassandra.index.SecondaryIndexManager.lambda$executeAllBlocking$39(SecondaryIndexManager.java:896)
	at org.apache.cassandra.index.SecondaryIndexManager$$Lambda$94/25774682.accept(Unknown Source)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
	at org.apache.cassandra.index.SecondaryIndexManager.executeAllBlocking(SecondaryIndexManager.java:893)
	at org.apache.cassandra.index.SecondaryIndexManager.flushIndexesBlocking(SecondaryIndexManager.java:346)
	at org.apache.cassandra.index.SecondaryIndexManager.flushAllCustomIndexesBlocking(SecondaryIndexManager.java:358)
	at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.run(ColumnFamilyStore.java:960)
{noformat}

First, note that the base of this stacktrace is in CFS$PostFlush.run(), which means it's running on the post-flush executor.  When {{CFS.forceBlockingFlush()}} is called on the secondary index table, we end up blocking on another task that's submitted to the post-flush executor.  Since that executor is single-threaded and is already running the base table task, this results in deadlock.

The attached patch includes a unit test and custom secondary index class (basically just KeysIndex) to reproduce the issue."
CASSANDRA-10180,Secondary index tables are flushed twice,"In {{SecondaryIndexManager.flushIndexesBlocking()}}, we accidentally add all indexes to the {{nonCfsIndexes}} list, resulting in them being flushed twice."
CASSANDRA-10150,Cassandra read latency potentially caused by memory leak,"  We are currently migrating to a new cassandra cluster which is multi-region on ec2.  Our previous cluster was also on ec2 but only in the east region.  In addition we have upgraded to cassandra 2.0.12 from 2.0.4 and from ubuntu 12 to 14.

  We are investigating a cassandra latency problem on our new cluster.  The symptom is that over a long period of time (12-16 hours) the TP90-95 read latency degrades to the point of being well above our SLA's.  During normal operation our TP95 for a 50key lookup is 75ms, when fully degraded, we are facing 300ms TP95 latencies.  Doing a rolling restart resolves the problem.

We are noticing a high correlation between the Old Gen heap usage (and how much is freed up) and the high latencies.  We are running with a max heap size of 12GB and a max new-gen size of 2GB.

Below is a chart of the heap usage over a 24 hour period.  Right below it is a chart of TP95 latencies (was a mixed workload of 50 and single key lookups), the third image is a look at CMS Old Gen memory usage:
Overall heap usage over 24 hrs:
!https://dl.dropboxusercontent.com/u/303980955/1.png|height=300,width=500!
TP95 latencies over 24 hours:
!https://dl.dropboxusercontent.com/u/303980955/2.png|height=300,width=500!
OldGen memory usage over 24 hours:
!https://dl.dropboxusercontent.com/u/303980955/3.png|height=300,width=500!

 You can see from this that the old gen section of our heap is what is using up the majority of the heap space.  We cannot figure out why the memory is not being collected during a full GC.  For reference, in our old cassandra cluster, the behavior is that the full GC will clear up the majority of the heap space.  See image below from an old production node operating normally:

!https://dl.dropboxusercontent.com/u/303980955/4.png|height=300,width=500!

From heap dump file we found that most memory is consumed by unreachable objects. With further analysis we were able to see those objects are RMIConnectionImpl$CombinedClassLoader$ClassLoaderWrapper (holding 4GB of memory) and java.security.ProtectionDomain (holding 2GB) . The only place we know Cassandra is using RMI is in JMX, but
does anyone has any clue on where else those objects are used? And Why do they take so much memory?
Or It would be great if someone could offer any further debugging tips on the latency or GC issue.
"
CASSANDRA-10117,FD Leak with DTCS,"Using 2.1-HEAD, specifically commit 972ae147247a, I am experiencing issues in a one node test with DTCS. These seem separate from CASSANDRA-9882. 

Using an ec2 i2.2xlarge node with all default settings and the following schema:

{code}
CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

  CREATE TABLE tab (
      key uuid,
      year int,
      month int,
      day int,
      c0 blob,
      c1 blob,
      c2 blob,
      c3 blob,
      c4 blob,
      c5 blob,
      c6 blob,
      c7 blob,
      c8 blob,
      PRIMARY KEY ((year, month, day), key)
  ) WITH compaction = {'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy'};
{code}

I loaded 4500M rows via stress, which totaled ~1.2TB. I then performed a few mixed workloads via stress, which were 50% insert, 50% the following read: {{Select * from tab where year = ? and month = ? and day = ? limit 1000}}.

This was done to reproduce a separate issue for a user. I then received reports from the user that they were experiencing open FD counts per sstable in the thousands. With absolutely no load on my cluster, I was finding that any sstable with open FDs had between 243 and 245 open FDs. I then started a stress process performing the same read/write workload as before. I was then immediately seeing FD counts as high as 6615 for a single sstable.

I was determining FD counts per sstable with the following [example] call:
{{lsof | grep '16119-Data.db' | wc -l}}.

I still have this cluster running, for you to examine. System.log is attached."
CASSANDRA-10079,"LEAK DETECTED, after nodetool drain","6 node cluster running 2.1.8

Sequence of events:

{quote}
2015-08-14 13:37:07,049 - Drain the node
2015-08-14 13:37:11,943 - Drained
2015-08-14 13:37:37,055 Ref.java:179 - LEAK DETECTED:
{quote}

{code}
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,055 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5534701) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@fab2c71) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@555d8efb) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7b29bfea) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2d37dc5a) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@713444527:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@13153552) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@713444527:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@25f51e35) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@713444527:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3633d3dd) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2ec81280) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,058 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@144d1dae) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,058 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1944bda4) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,058 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@31c1386a) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1601396928:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-8229-Index.db was not released before the reference was garbage collected
{code}

See full log here:
https://dl.dropboxusercontent.com/u/4179566/cassandra-system.log"
CASSANDRA-10012,Deadlock when session streaming is retried after exception,"This patch ensures that the CompressedInputStream thread is cleaned up properly (for example, if an Exception occurs and a session stream is retried)."
CASSANDRA-10010,Paging on DISTINCT queries repeats result when first row in partition changes,"When paging, we always check new pages to see if they start with the same row that the previous page ended with, and if so, we trim that row to avoid duplicates.  With {{DISTINCT}} queries, we only fetch the first row in each partition.  If that row happens to change (it's deleted, or another row is inserted at the front of the partition) in between fetching the two pages, our check for a matching row will fail, resulting in a duplicate row being returned.

It seems like the correct fix is to handle {{DISTINCT}} queries specially and only check to see if the partition key matches the last returned one instead checking that the rows match."
CASSANDRA-9998,LEAK DETECTED with snapshot/sequential repairs,"http://cassci.datastax.com/job/cassandra-2.1_dtest/lastCompletedBuild/testReport/repair_test/TestRepair/simple_sequential_repair_test/

does not happen if I add -par to the test"
CASSANDRA-9973,java.lang.IllegalStateException: Unable to compute when histogram overflowed,"I recently, and probably mistakenly, upgraded one of my production C* clusters to 2.2.0.  I am seeing these errors in the logs, followed by an intense period of garbage collection until the node, then the ring, becomes crippled:

{code}
ERROR [OptionalTasks:1] 2015-08-04 03:24:56,057 CassandraDaemon.java:182 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.IllegalStateException: Unable to compute when histogram overflowed
        at org.apache.cassandra.utils.EstimatedHistogram.percentile(EstimatedHistogram.java:179) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at org.apache.cassandra.metrics.EstimatedHistogramReservoir$HistogramSnapshot.getValue(EstimatedHistogramReservoir.java:84) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at org.apache.cassandra.db.ColumnFamilyStore$3.run(ColumnFamilyStore.java:405) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_45]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_45]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{code}

I am not sure if the GC instability is this or something else, but I though this histogram overflow issue was fixed in 2.1.3?  Anyway, reporting now as a possible regression.  Please let me know what I can provide in terms of information to help with this.  Thanks!"
CASSANDRA-9917,MVs should validate gc grace seconds on the tables involved,"For correctness reasons (potential resurrection of dropped values), batchlog entries are TTLs with the lowest gc grace second of all the tables involved in a batch.

It means that if gc gs is set to 0 in one of the tables, the batchlog entry will be dead on arrival, and never replayed.

We should probably warn against such LOGGED writes taking place, in general, but for MVs, we must validate that gc gs on the base table (and on the MV table, if we should allow altering gc gs there at all), is never set too low, or else."
CASSANDRA-9908,Potential race caused by async cleanup of transaction log files,"There seems to be a potential race in the cleanup of transaction log files, introduced in CASSANDRA-7066
It's pretty hard to trigger on trunk, but it's possible to hit it via {{o.a.c.db.SecondaryIndexTest#testCreateIndex}} 

That test creates an index, then removes it to check that the removal is correctly recorded, then adds the index again to assert that it gets rebuilt from the existing data. 
The removal causes the SSTables of the index CFS to be dropped, which is a transactional operation and so writes a transaction log. When the drop is completed and the last reference to an SSTable is released, the cleanup of the transaction log is scheduled on the periodic tasks executor. The issue is that re-creating the index re-creates the index CFS. When this happens, it's possible for the cleanup of the txn log to have not yet happened. If so, the initialization of the CFS attempts to read the log to identify any orphaned temporary files. The cleanup can happen between the finding the log file and reading it's contents, which results in a {{NoSuchFileException}}

{noformat}
[junit] java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit] java.lang.RuntimeException: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:620)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionFile.getTrackedFiles(TransactionLogs.java:190)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionData.getTemporaryFiles(TransactionLogs.java:338)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs.getTemporaryFiles(TransactionLogs.java:739)
[junit]     at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getTemporaryFiles(LifecycleTransaction.java:541)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.getFilter(Directories.java:652)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:641)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:606)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:351)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:313)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:511)
[junit]     at org.apache.cassandra.index.internal.CassandraIndexer.addIndexedColumn(CassandraIndexer.java:115)
[junit]     at org.apache.cassandra.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:265)
[junit]     at org.apache.cassandra.db.SecondaryIndexTest.testIndexCreate(SecondaryIndexTest.java:467)
[junit] Caused by: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
[junit]     at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:361)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:407)
[junit]     at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
[junit]     at java.nio.file.Files.newInputStream(Files.java:152)
[junit]     at java.nio.file.Files.newBufferedReader(Files.java:2784)
[junit]     at java.nio.file.Files.readAllLines(Files.java:3202)
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:616)
[junit] 
[junit] 
[junit] Test org.apache.cassandra.db.SecondaryIndexTest FAILED
{noformat}
"
CASSANDRA-9882,DTCS (maybe other strategies) can block flushing when there are lots of sstables,"MemtableFlushWriter tasks can get blocked by Compaction getNextBackgroundTask.  This is in a wonky cluster with 200k sstables in the CF, but seems bad for flushing to be blocked by getNextBackgroundTask when we are trying to make these new ""smart"" strategies that may take some time to calculate what to do.

{noformat}
""MemtableFlushWriter:21"" daemon prio=10 tid=0x00007ff7ad965000 nid=0x6693 waiting for monitor entry [0x00007ff78a667000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:237)
	- waiting to lock <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1475)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1127)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- <0x0000000743b3ac38> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""MemtableFlushWriter:19"" daemon prio=10 tid=0x00007ff7ac57a000 nid=0x649b waiting for monitor entry [0x00007ff78b8ee000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:237)
	- waiting to lock <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1475)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1127)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""CompactionExecutor:14"" daemon prio=10 tid=0x00007ff7ad359800 nid=0x4d59 runnable [0x00007fecce3ea000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.cassandra.io.sstable.SSTableReader.equals(SSTableReader.java:628)
	at com.google.common.collect.ImmutableSet.construct(ImmutableSet.java:206)
	at com.google.common.collect.ImmutableSet.construct(ImmutableSet.java:220)
	at com.google.common.collect.ImmutableSet.access$000(ImmutableSet.java:74)
	at com.google.common.collect.ImmutableSet$Builder.build(ImmutableSet.java:531)
	at com.google.common.collect.Sets$1.immutableCopy(Sets.java:606)
	at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:1352)
	at org.apache.cassandra.db.compaction.DateTieredCompactionStrategy.getNextBackgroundSSTables(DateTieredCompactionStrategy.java:88)
	at org.apache.cassandra.db.compaction.DateTieredCompactionStrategy.getNextBackgroundTask(DateTieredCompactionStrategy.java:65)
	- locked <0x00000006fcdbbf00> (a org.apache.cassandra.db.compaction.DateTieredCompactionStrategy)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getNextBackgroundTask(WrappingCompactionStrategy.java:72)
	- locked <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

"
CASSANDRA-9849,Paging changes to 2.1/2.2 for backward compatibilty with 3.0,"Quoting [~thobbs] from CASSANDRA-9704:
{quote}
I forgot to attach a patch for the required 2.1/2.2 changes as well. Basically, when paging in 2.x, our check to see if a new page contains the same row that the previous page ended on looks for an exact cell name match. This is fine in 2.x because we will return partial rows at the end of the page (just the row marker cell). However, in 3.0, we always return full rows. While we could make some very hacky changes to 3.0 to enable returning a partial row at the end of the page, this seems like the cleanest solution.
{quote}

Tyler's patch is [there|https://github.com/thobbs/cassandra/tree/CASSANDRA-9704-2.1-forward-compat]"
CASSANDRA-9837,"Fix broken logging for ""empty"" flushes in Memtable","Notice the following non-interpolated log entry showing up in our logs ""Completed flushing %s"". Looking at the source it appears to be a mix between logback style ""{}"" substitution vs String.format ""%s""  style.

Attaching a trivial patch."
CASSANDRA-9798,Cassandra seems to have deadlocks during flush operations,"Hi,
We noticed some problem with dropped mutationstages. Usually on one random node there is a situation that:
MutationStage ""active"" is full, ""pending"" is increasing  ""completed"" is stalled.
MemtableFlushWriter ""active"" 6, pending: 25 completed: stalled 
MemtablePostFlush ""active"" is 1, pending 29 completed: stalled

after a some time (30s-10min) pending mutations are dropped and everything is working.
When it happened:
1. Cpu idle is ~95%
2. no gc long pauses or more activity.
3. memory usage 3.5GB form 8GB
4. only writes is processed by cassandra
5. when LOAD > 400GB/node problems appeared 
6. cassandra 2.1.6

There is gap in logs:
{code}
INFO  08:47:01 Timed out replaying hints to /192.168.100.83; aborting (0 delivered)
INFO  08:47:01 Enqueuing flush of hints: 7870567 (0%) on-heap, 0 (0%) off-heap
INFO  08:47:30 Enqueuing flush of table1: 95301807 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:31 Enqueuing flush of table1: 60462632 (3%) on-heap, 0 (0%) off-heap
INFO  08:47:31 Enqueuing flush of table2: 76973746 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:31 Enqueuing flush of table1: 84290135 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:32 Enqueuing flush of table3: 56926652 (3%) on-heap, 0 (0%) off-heap
INFO  08:47:32 Enqueuing flush of table1: 85124218 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:33 Enqueuing flush of table2: 95663415 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:58 CompactionManager                 2        39
INFO  08:47:58 Writing Memtable-table2@1767938721(13843064 serialized bytes, 162359 ops, 4%/0% of on/off-heap l
imit)
INFO  08:47:58 Writing Memtable-hints@1433125911(478703 serialized bytes, 424 ops, 0%/0% of on/off-heap limit)
INFO  08:47:58 Writing Memtable-table2@1318583275(11783615 serialized bytes, 137378 ops, 4%/0% of on/off-heap l
imit)
INFO  08:47:58 Enqueuing flush of compactions_in_progress: 969 (0%) on-heap, 0 (0%) off-heap
INFO  08:47:58 Writing Memtable-table1@541175113(17221327 serialized bytes, 180792 ops, 4%/0% of on/off-heap
 limit)
INFO  08:47:58 Writing Memtable-table1@1361154669(27138519 serialized bytes, 273472 ops, 6%/0% of on/off-hea
p limit)

INFO  08:48:03 2176 MUTATION messages dropped in last 5000ms
{code}

use case:
100% write - 100Mb/s, couples of CF ~10column each. max cell size 100B
CMS and G1GC tested - no difference
"
CASSANDRA-9785,dtests-offheap: leak detected,"Following dtests fail with ""LEAK DETECTED"" with {{OFFHEAP_MEMTABLES=yes}}:
* repair_test.py:TestRepair.dc_repair_test
* repair_test.TestRepair.simple_sequential_repair_test
"
CASSANDRA-9775,some paging dtests fail/flap on trunk,"Several paging dtests fail on trunk:

[static_columns_paging_test|http://cassci.datastax.com/view/trunk/job/trunk_dtest/lastSuccessfulBuild/testReport/junit/paging_test/TestPagingData/static_columns_paging_test/history/]

[test_undefined_page_size_default|http://cassci.datastax.com/view/trunk/job/trunk_dtest/lastSuccessfulBuild/testReport/junit/paging_test/TestPagingSize/test_undefined_page_size_default/history/]

[test_failure_threshold_deletions|http://cassci.datastax.com/view/trunk/job/trunk_dtest/lastSuccessfulBuild/testReport/junit/paging_test/TestPagingWithDeletions/test_failure_threshold_deletions/history/]

I'm not sure if these are all rooted in the same underlying problem, so I defer to whoever takes this ticket on.

[~thobbs] I'm assigning you because this is about paging, but reassign as you see fit. Thanks!"
CASSANDRA-9758,nodetool compactionhistory NPE,"nodetool compactionhistory may trigger NPE : 

{code}
admin@localhost:~$ nodetool compactionhistory
Compaction History: 
error: null
-- StackTrace --
java.lang.NullPointerException
	at com.google.common.base.Joiner$MapJoiner.join(Joiner.java:330)
	at org.apache.cassandra.utils.FBUtilities.toString(FBUtilities.java:515)
	at org.apache.cassandra.db.compaction.CompactionHistoryTabularData.from(CompactionHistoryTabularData.java:78)
	at org.apache.cassandra.db.SystemKeyspace.getCompactionHistory(SystemKeyspace.java:422)
	at org.apache.cassandra.db.compaction.CompactionManager.getCompactionHistory(CompactionManager.java:1490)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$2.run(Transport.java:202)
	at sun.rmi.transport.Transport$2.run(Transport.java:199)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:198)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:567)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.access$400(TCPTransport.java:619)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$1.run(TCPTransport.java:684)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$1.run(TCPTransport.java:681)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:681)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

{code}
admin@localhost:~$ cqlsh -e ""select * from system.compaction_history"" | grep -F null
 ede434c0-2306-11e5-8a1a-85b300e09146 |  120 | 0 |     peers | 2015-07-05 13:13:57+0200 |system | null
 ae33fb90-23a0-11e5-9245-85b300e09146 |  120 | 0 |     peers | 2015-07-06 07:34:32+0200 |system | null
 085cb1f0-2542-11e5-9291-dfb803ff9672 |  120 | 0 |     peers | 2015-07-08 09:22:04+0200 |system | null
 0dbd4240-2349-11e5-a72b-85b300e09146 |  120 | 0 |     peers | 2015-07-05 21:07:17+0200 |system | null
 51e56b70-2261-11e5-8df2-85b300e09146 |  120 | 0 |     peers | 2015-07-04 17:28:28+0200 |system | null
 8152f810-2229-11e5-9218-dfb803ff9672 |   30 | 0 |     peers | 2015-07-04 10:48:56+0200 |system | null
 659c0840-222f-11e5-b9c9-dfb803ff9672 |  120 | 0 |     peers | 2015-07-04 11:31:06+0200 |system | null
 f6538e30-249b-11e5-9bdf-85b300e09146 |  120 | 0 |     peers | 2015-07-07 13:33:17+0200 |system | null
 20817760-2499-11e5-a613-85b300e09146 |  120 | 0 |     peers | 2015-07-07 13:12:59+0200 |system | null
 f70364c0-2470-11e5-9408-dfb803ff9672 |  120 | 0 |     peers | 2015-07-07 08:25:30+0200 |system | null
 fe44d050-2567-11e5-9441-85b300e09146 |  120 | 0 |     peers | 2015-07-08 13:53:48+0200 |system | null
 9fb3fde1-2413-11e5-8b06-dfb803ff9672 |  120 | 0 |     peers | 2015-07-06 21:17:20+0200 |system | null
 084dbdd0-2542-11e5-9291-dfb803ff9672 |   32 | 0 | IndexInfo | 2015-07-08 09:22:04+0200 |system | null
{code}"
CASSANDRA-9751,Ability to change gc_grace period for system tables,"For an admin user, or via JMX, give the possibility to change default  gc_grace period for system keyspace tables."
CASSANDRA-9730,Flappy NPE when starting C* on trunk,"The following exception occurs approx. 50% of the time when C* is restarted after a clean install.

{noformat}
ERROR 09:02:19 Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.NullPointerException: null
	at org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys.run(SSTableRewriter.java:264) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier$1.run(SSTableReader.java:2084) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[na:1.8.0_45]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{noformat}

Steps to reproduce:
{noformat}
git checkout trunk
ant ...
bin/cassandra
(kill cassandra daemon)
bin/cassandra
{noformat}
"
CASSANDRA-9729,CQLSH exception - OverflowError: normalized days too large to fit in a C int,"Running a select command using CQLSH 2.1.5, 2.1.7 throws exception. This works nicely in 2.0.14 version.

Environment:
============
JAVA - 1.8
Python - 2.7.6
Cassandra Server - 2.1.7
CQLSH - 5.0.1

Logs:
======

CQLSH - cassandra 2.0.14 - working with no issues
-------------------------------------
{noformat}
NCHAN-M-D0LZ:apache nchan$ cd apache-cassandra-2.0.14/
NCHAN-M-D0LZ:apache-cassandra-2.0.14 nchan$ bin/cqlsh
Connected to CCC Multi-Region Cassandra Cluster at <myip>:9160.
[cqlsh 4.1.1 | Cassandra 2.1.7 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> use ccc;
cqlsh:ccc> select count(*) from task_result where submissionid='40f89a3d1f4711e5ac2b005056bb0e8b';

 count
-------
    25

(1 rows)

cqlsh:ccc> select * from task_result where submissionid='40f89a3d1f4711e5ac2b005056bb0e8b';
< i get all the 25 values>
{noformat}

CQLSH - cassandra 2.1.5  - python exception
-------------------------------------
{noformat}
NCHAN-M-D0LZ:apache-cassandra-2.1.5 nchan$ bin/cqlsh
Connected to CCC Multi-Region Cassandra Cluster at <ip-address>:9042.
[cqlsh 5.0.1 | Cassandra 2.1.7 | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
cqlsh> use ccc;
cqlsh:ccc> select count(*) from task_result where submissionid='40f89a3d1f4711e5ac2b005056bb0e8b';

 count
-------
    25

(1 rows)
cqlsh:ccc> select * from task_result where submissionid='40f89a3d1f4711e5ac2b005056bb0e8b';
Traceback (most recent call last):
  File ""bin/cqlsh"", line 1001, in perform_simple_statement
    rows = self.session.execute(statement, trace=self.tracing_enabled)
  File ""/Users/nchan/Programs/apache/apache-cassandra-2.1.5/bin/../lib/cassandra-driver-internal-only-2.5.0.zip/cassandra-driver-2.5.0/cassandra/cluster.py"", line 1404, in execute
    result = future.result(timeout)
  File ""/Users/nchan/Programs/apache/apache-cassandra-2.1.5/bin/../lib/cassandra-driver-internal-only-2.5.0.zip/cassandra-driver-2.5.0/cassandra/cluster.py"", line 2974, in result
    raise self._final_exception
OverflowError: normalized days too large to fit in a C int

cqlsh:ccc> 
{noformat}

CQLSH - cassandra 2.1.7 - python exception
-------------------------------------
{noformat}
NCHAN-M-D0LZ:apache-cassandra-2.1.7 nchan$ bin/cqlsh
Connected to CCC Multi-Region Cassandra Cluster at 171.71.189.11:9042.
[cqlsh 5.0.1 | Cassandra 2.1.7 | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
cqlsh> use ccc;
cqlsh:ccc> select count(*) from task_result where submissionid='40f89a3d1f4711e5ac2b005056bb0e8b';

 count
-------
    25

(1 rows)
cqlsh:ccc> select * from task_result where submissionid='40f89a3d1f4711e5ac2b005056bb0e8b';
Traceback (most recent call last):
  File ""bin/cqlsh"", line 1041, in perform_simple_statement
    rows = self.session.execute(statement, trace=self.tracing_enabled)
  File ""/Users/nchan/Programs/apache/apache-cassandra-2.1.7/bin/../lib/cassandra-driver-internal-only-2.5.1.zip/cassandra-driver-2.5.1/cassandra/cluster.py"", line 1405, in execute
    result = future.result(timeout)
  File ""/Users/nchan/Programs/apache/apache-cassandra-2.1.7/bin/../lib/cassandra-driver-internal-only-2.5.1.zip/cassandra-driver-2.5.1/cassandra/cluster.py"", line 2976, in result
    raise self._final_exception
OverflowError: normalized days too large to fit in a C int

cqlsh:ccc> 
{noformat}"
CASSANDRA-9727,AuthSuccess NPE,"Triggered while playing with org.apache.cassandra.transport.Client with PasswordAuthenticator  : 
 
{code}
>> startup
11:48:42.522 [main] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.maxCapacity.default: 262144
11:48:42.530 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 8
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 8
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 8192
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 11
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 16777216
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.tinyCacheSize: 512
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 256
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 64
11:48:42.531 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
11:48:42.532 [nioEventLoopGroup-2-1] DEBUG i.n.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 8192
11:48:42.552 [nioEventLoopGroup-2-1] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetectionLevel: simple
-> AUTHENTICATE org.apache.cassandra.auth.PasswordAuthenticator
>> authenticate username=cassandra password=WRONGPASSWORD
ERROR: org.apache.cassandra.exceptions.AuthenticationException: Username and/or password are incorrect
>> authenticate username=cassandra password=cassandra
11:50:00.095 [nioEventLoopGroup-2-1] DEBUG io.netty.util.internal.Cleaner0 - java.nio.ByteBuffer.cleaner(): available
11:50:00.113 [nioEventLoopGroup-2-1] ERROR o.a.cassandra.transport.SimpleClient - Exception in response
io.netty.handler.codec.DecoderException: org.apache.cassandra.transport.messages.ErrorMessage$WrappedException: java.lang.NullPointerException
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:99) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:163) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: org.apache.cassandra.transport.messages.ErrorMessage$WrappedException: java.lang.NullPointerException
	at org.apache.cassandra.transport.messages.ErrorMessage.wrap(ErrorMessage.java:349) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.Message$ProtocolDecoder.decode(Message.java:309) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.Message$ProtocolDecoder.decode(Message.java:261) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	... 17 common frames omitted
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.transport.messages.AuthSuccess$1.decode(AuthSuccess.java:39) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.messages.AuthSuccess$1.decode(AuthSuccess.java:35) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.Message$ProtocolDecoder.decode(Message.java:280) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	... 19 common frames omitted
11:50:00.115 [nioEventLoopGroup-2-1] WARN  i.n.channel.DefaultChannelPipeline - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.
io.netty.handler.codec.DecoderException: org.apache.cassandra.transport.messages.ErrorMessage$WrappedException: java.lang.NullPointerException
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:99) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:163) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: org.apache.cassandra.transport.messages.ErrorMessage$WrappedException: java.lang.NullPointerException
	at org.apache.cassandra.transport.messages.ErrorMessage.wrap(ErrorMessage.java:349) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.Message$ProtocolDecoder.decode(Message.java:309) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.Message$ProtocolDecoder.decode(Message.java:261) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:89) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	... 17 common frames omitted
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.transport.messages.AuthSuccess$1.decode(AuthSuccess.java:39) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.messages.AuthSuccess$1.decode(AuthSuccess.java:35) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	at org.apache.cassandra.transport.Message$ProtocolDecoder.decode(Message.java:280) ~[apache-cassandra-2.2.0-rc2-SNAPSHOT.jar:2.2.0-rc2-SNAPSHOT]
	... 19 common frames omitted
{code}"
CASSANDRA-9723,UDF / UDA execution time in trace,I'd like to see how long my UDF/As take in the trace. Checked in 2.2rc1 and doesn't appear to be mentioned.
CASSANDRA-9686,FSReadError and LEAK DETECTED after upgrading,"After upgrading one of 15 nodes from 2.1.7 to 2.2.0-rc1 I get FSReadError and LEAK DETECTED on start. Deleting the listed files, the failure goes away.
{code:title=system.log}
ERROR [SSTableBatchOpen:1] 2015-06-29 14:38:34,554 DebuggableThreadPoolExecutor.java:242 - Error in ThreadPoolExecutor
org.apache.cassandra.io.FSReadError: java.io.IOException: Compressed file with 0 chunks encountered: java.io.DataInputStream@1c42271
	at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:117) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.compress.CompressionMetadata.create(CompressionMetadata.java:86) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.util.CompressedSegmentedFile$Builder.metadata(CompressedSegmentedFile.java:142) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Builder.complete(CompressedPoolingSegmentedFile.java:101) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.util.SegmentedFile$Builder.complete(SegmentedFile.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:681) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:644) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:443) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:350) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader$4.run(SSTableReader.java:480) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_55]
	at java.lang.Thread.run(Unknown Source) [na:1.7.0_55]
Caused by: java.io.IOException: Compressed file with 0 chunks encountered: java.io.DataInputStream@1c42271
	at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:174) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	... 15 common frames omitted
ERROR [Reference-Reaper:1] 2015-06-29 14:38:34,734 Ref.java:189 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3e547f) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1926439:D:\Programme\Cassandra\data\data\system\compactions_in_progress\system-compactions_in_progress-ka-6866 was not released before the reference was garbage collected
{code}"
CASSANDRA-9669,"If sstable flushes complete out of order, on restart we can fail to replay necessary commit log records","While {{postFlushExecutor}} ensures it never expires CL entries out-of-order, on restart we simply take the maximum replay position of any sstable on disk, and ignore anything prior. 

It is quite possible for there to be two flushes triggered for a given table, and for the second to finish first by virtue of containing a much smaller quantity of live data (or perhaps the disk is just under less pressure). If we crash before the first sstable has been written, then on restart the data it would have represented will disappear, since we will not replay the CL records.

This looks to be a bug present since time immemorial, and also seems pretty serious."
CASSANDRA-9659,Better messages/protection against bad paging state,"An issue exists where a java.lang.AssertionError occurs for a select number of read queries from Cassandra within our application.

It was suggested that a ticket be created to see if the error below is the same as CASSANDRA-8949 which was fixed in version 2.1.5.

Here is a portion of the Cassandra log file where the exception occurs:
{code}
INFO  [MemtableFlushWriter:50153] 2015-06-23 13:11:17,517 Memtable.java:385 - Completed flushing; nothing needed to be retained.  Commitlog position was ReplayPosition(segmentId=1425054853780, position=8886361)
ERROR [SharedPool-Worker-1] 2015-06-23 13:11:29,047 Message.java:538 - Unexpected exception during request; channel = [id: 0x8f1ca59e, /10.30.43.68:33717 => /10.30.43.146:9042]javaa.lang.AssertionError: [DecoratedKey(5747358200379796162, 64623465383566662d653235382d343130352d616131612d346230396635353965666364),DecoratedKey(3303996443194009861, 34623632646562322d626234332d346661642d613263312d356334613233633037353932)]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:41) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:34) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.pager.RangeSliceQueryPager.makeIncludingKeyBounds(RangeSliceQueryPager.java:123) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.pager.RangeSliceQueryPager.queryNextPage(RangeSliceQueryPager.java:74) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:87) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.pager.RangeSliceQueryPager.fetchPage(RangeSliceQueryPager.java:37) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:219) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:493) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:134) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [apache-cassandra-2.1.3.jar:2.1.3]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [na:1.7.0_76]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-2.1.3.jar:2.1.3]
        at java.lang.Thread.run(Unknown Source) [na:1.7.0_76]
INFO  [BatchlogTasks:1] 2015-06-23 13:12:17,521 ColumnFamilyStore.java:877 - Enqueuing flush of batchlog: 27641 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:50154] 2015-06-23 13:12:17,522 Memtable.java:339 - Writing Memtable-batchlog@297832842(22529 serialized bytes, 40 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:50154] 2015-06-23 13:12:17,523 Memtable.java:385 - Completed flushing; nothing needed to be retained.  Commitlog position was ReplayPosition(segmentId=1425054853780, position=8948299)
{code}"
CASSANDRA-9658,Re-enable memory-mapped index file reads on Windows,"It appears that the impact of buffered vs. memory-mapped index file reads has changed dramatically since last I tested. [Here's some results on various platforms we pulled together yesterday w/2.2-HEAD|https://docs.google.com/spreadsheets/d/1JaO2x7NsK4SSg_ZBqlfH0AwspGgIgFZ9wZ12fC4VZb0/edit#gid=0].

TL;DR: On linux we see a 40% hit in performance from 108k ops/sec on reads to 64.8k ops/sec. While surprising in itself, the really unexpected result (to me) is on Windows - with standard access we're getting 16.8k ops/second on our bare-metal perf boxes vs. 184.7k ops/sec with memory-mapped index files, an over 10-fold increase in throughput. While testing w/standard access, CPU's on the stress machine and C* node are both sitting < 4%, network doesn't appear bottlenecked, resource monitor doesn't show anything interesting, and performance counters in the kernel show very little. Changes in thread count simply serve to increase median latency w/out impacting any other visible metric that we're measuring, so I'm at a loss as to why the disparity is so huge on the platform.

The combination of my changes to get the 2.1 branch to behave on Windows along with [~benedict] and [~Stefania]'s changes in lifecycle and cleanup patterns on 2.2 should hopefully have us in a state where transitioning back to using memory-mapped I/O on Windows will only cause trouble on snapshot deletion. Fairly simple runs of stress w/compaction aren't popping up any obvious errors on file access or renaming - I'm going to do some much heavier testing (ccm multi-node clusters, long stress w/repair and compaction, etc) and see if there's any outstanding issues that need to be stamped out to call mmap'ed index files on Windows safe. The one thing we'll never be able to support is deletion of snapshots while a node is running and sstables are mapped, but for a > 10x throughput increase I think users would be willing to make that sacrifice.

The combination of the powercfg profile change, the kernel timer resolution, and memory-mapped index files are giving some pretty interesting performance numbers on EC2."
CASSANDRA-9656,Strong circular-reference leaks,"As discussed in CASSANDRA-9423, we are leaking references to the ref-counted object into the Ref.Tidy, so that they remain strongly reachable, significantly limiting the value of the leak detection.

"
CASSANDRA-9628,"""Unknown keyspace system_traces"" exception when using nodetool on a new cluster","When creating a new cluster from scratch,  nodetool status fails on system_traces as follow

{code}
$ nodetool status

error: Unknown keyspace system_traces
-- StackTrace --
java.lang.AssertionError: Unknown keyspace system_traces
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:270)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:119)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:96)
...
{code}

the problem disappear when creating an empty keyspace
{code}
cqlsh> create keyspace temp WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 2 };
{code}
My guess is system_traces initialization complete only after any data insertion.
Before it does, any attempt to read  from it either from nodetool, cqlsh or streaming to a new node will fail."
CASSANDRA-9621,Repair of the SystemDistributed keyspace creates a non-trivial amount of memory pressure,"When a repair without any particular option is triggered, the {{SystemDistributed}} keyspace is repaired for all range, and in particular the {{repair_history}} table. For every range, that table is written and flushed (as part of normal repair), meaning that every range triggers the creation of a new 1MB slab region (this also triggers quite a few compactions that also write and flush {{compaction_progress}} at every start and end).

I don't know how much of a big deal this will be in practice, but I wonder if it's really useful to repair the {{repair_*}} tables by default so maybe we could exclude the SystemDistributed keyspace from default repairs and only repair it if asked explicitly?"
CASSANDRA-9604,"Upon JVM OOM (Out Of Memory), print out a summary of heap histograms in addition to heap dump","It appears that jmap has an option to allow dumping heap histograms after a heap dump is generated (http://www.oracle.com/technetwork/java/javase/memleaks-137499.html#gbywi). This could be useful to provide a very quick overview of the biggest objects in heap without having to download the massive hprof file off the Cassandra nodes. We can potentially hook it up using this JVM option: -XX:OnOutOfMemoryError=""<cmd args>; <cmd args>"" in the cassandra-env.sh file."
CASSANDRA-9576,Connection leak in CQLRecordWriter,"Ran into connection leaks when using CQLCassandra apache-cassandra-2.2.0-beta1-src + CQLOutputFormat (via CqlNativeStorage). 

It seems like the order blocks of code starting at https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/hadoop/cql3/CqlRecordWriter.java#L298 were reversed in 2.2 which leads to the connection leaks.

"
CASSANDRA-9575,LeveledCompactionStrategyTest.testCompactionRace and testMutateLevel sometimes fail,"testCompactionRace
http://cassci.datastax.com/view/trunk/job/trunk_testall/136/testReport/junit/org.apache.cassandra.db.compaction/LeveledCompactionStrategyTest/testCompactionProgress/

testMutateLevel (only seen on my desktop)
{noformat}
    [junit] Testcase: testMutateLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):        FAILED
    [junit] expected:<6> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<6> but was:<0>
    [junit]     at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testMutateLevel(LeveledCompactionStrateg
yTest.java:281)
{noformat}"
CASSANDRA-9571,Set HAS_MORE_PAGES flag when PagingState is null,"If {{ResultSet.Metadata#setHasMorePages}} is called with a null {{PagingState}}, indicating that there are no more available results, we should explicitly unset the {{HAS_MORE_PAGES}} flag. This can be necessary where a custom {{QueryHandler}} implements its own {{LIMIT}} logic and wishes to send the signal to the client that the results are exhausted before the storage engine would normally do so.
"
CASSANDRA-9565,'WITH WITH' in alter keyspace statements causes NPE,"Running any of these statements:

{code}
ALTER KEYSPACE WITH WITH DURABLE_WRITES = true;
ALTER KEYSPACE ks WITH WITH DURABLE_WRITES = true;
CREATE KEYSPACE WITH WITH DURABLE_WRITES = true;
CREATE KEYSPACE ks WITH WITH DURABLE_WRITES = true;
{code}

Fails, raising a {{SyntaxException}} and giving a {{NullPointerException}} as the reason for failure. This happens in all versions I tried, including 2.0.15, 2.1.5, and HEAD on cassandra-2.0, cassandra-2.1, cassandra-2.2, and trunk.

EDIT: A dtest is [here|https://github.com/mambocab/cassandra-dtest/commit/da3785e25cce505183e0ebc8dd21340f3a3ea3a4#diff-dcb0fc3aff201fd7eeea6cbf1181f921R5300], but it would probably be more appropriate to test with unit tests."
CASSANDRA-9549,Memory leak in Ref.GlobalState due to pathological ConcurrentLinkedQueue.remove behaviour,"We have been experiencing a severe memory leak with Cassandra 2.1.5 that, over the period of a couple of days, eventually consumes all of the available JVM heap space, putting the JVM into GC hell where it keeps trying CMS collection but can't free up any heap space. This pattern happens for every node in our cluster and is requiring rolling cassandra restarts just to keep the cluster running. We have upgraded the cluster per Datastax docs from the 2.0 branch a couple of months ago and have been using the data from this cluster for more than a year without problem.

As the heap fills up with non-GC-able objects, the CPU/OS load average grows along with it. Heap dumps reveal an increasing number of java.util.concurrent.ConcurrentLinkedQueue$Node objects. We took heap dumps over a 2 day period, and watched the number of Node objects go from 4M, to 19M, to 36M, and eventually about 65M objects before the node stops responding. The screen capture of our heap dump is from the 19M measurement.

Load on the cluster is minimal. We can see this effect even with only a handful of writes per second. (See attachments for Opscenter snapshots during very light loads and heavier loads). Even with only 5 reads a sec we see this behavior.

Log files show repeated errors in Ref.java:181 and Ref.java:279 and ""LEAK detected"" messages:
{code}
ERROR [CompactionExecutor:557] 2015-06-01 18:27:36,978 Ref.java:279 - Error when closing class org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier@1302301946:/data1/data/ourtablegoeshere-ka-1150
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@32680b31 rejected from org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor@573464d6[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1644]
{code}
{code}
ERROR [Reference-Reaper:1] 2015-06-01 18:27:37,083 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@74b5df92) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@2054303604:/data2/data/ourtablegoeshere-ka-1151 was not released before the reference was garbage collected
{code}
This might be related to [CASSANDRA-8723]?
"
CASSANDRA-9510,assassinating an unknown endpoint could npe,"If the code assissinates an unknown endpoint, it doesn't generate a 'tokens' collection, which then does

epState.addApplicationState(ApplicationState.STATUS, StorageService.instance.valueFactory.left(tokens, computeExpireTime()));

and left(null, time); will npe"
CASSANDRA-9488,CrcCheckChanceTest.testChangingCrcCheckChance fails with stack overflow,"http://cassci.datastax.com/job/trunk_utest/212/testReport/junit/org.apache.cassandra.cql3/CrcCheckChanceTest/testChangingCrcCheckChance/
{noformat}
java.lang.StackOverflowError
	at org.apache.cassandra.io.compress.CompressionParameters.validateCrcCheckChance(CompressionParameters.java:138)
	at org.apache.cassandra.io.compress.CompressionParameters.setCrcCheckChance(CompressionParameters.java:110)
	at org.apache.cassandra.io.compress.CompressionParameters.setCrcCheckChance(CompressionParameters.java:114)
	at org.apache.cassandra.io.compress.CompressionParameters.setCrcCheckChance(CompressionParameters.java:114)
	at org.apache.cassandra.io.compress.CompressionParameters.setCrcCheckChance(CompressionParameters.java:114)
{noformat}

The code is
{noformat}
    public void setCrcCheckChance(double crcCheckChance) throws ConfigurationException
    {
        validateCrcCheckChance(crcCheckChance);
        this.crcCheckChance = crcCheckChance;

        if (liveMetadata != null)
            liveMetadata.compressionParameters.setCrcCheckChance(crcCheckChance);
    }
{noformat}

Looks like it is just following itself circularly. Should definitely add an assertion that they are not the same compression parameters. Will run it in a loop for a while to see if it reproduces."
CASSANDRA-9482,SSTable leak after stress and repair,"I have a dtest that fails intermittently because of SSTable leaks. The test logic leading to the error is:

- create a 5-node cluster
- insert 5000 records with {{stress}}, RF=3 at CL=ONE
- run {{flush}} on all nodes 
- run {{repair}} on a single node.

The leak is detected on a different node than {{repair}} was run on.

The failing test is [here|https://github.com/mambocab/cassandra-dtest/blob/CASSANDRA-5839-squash/repair_test.py#L317]. The relevant error his [here|https://gist.github.com/mambocab/8aab7b03496e0b279bd3#file-node2-log-L256], along with the errors from the entire 5-node cluster. In these logs, the {{repair}} was run on {{node1}} and the leak was found on {{node2}}.

I can bisect, but I thought I'd get the ball rolling in case someone knows where to look."
CASSANDRA-9479,Improve trace messages,"Currently, tracing only records lines like
{{Enqueuing response to}} / {{Processing response from}} or
{{Sending message to}} / {{Message received from}}.

It would help if these messages also contain some information about the verb and (if easily accessible) about kind of content.
"
CASSANDRA-9458,Race condition causing StreamSession to get stuck in WAIT_COMPLETE,"I think there is a race condition in StreamSession where one side of the stream could get stuck in WAIT_COMPLETE although both have sent COMPLETE messages. Consider a scenario that node B is being bootstrapped and it only receives files during the session:

1- During a stream session A sends some files to B and B sends no files to A.
2- Once B completes the last task (receiving), StreamSession::maybeComplete is invoked.
3- While B is sending the COMPLETE message via StreamSession::maybeComplete, it also receives the COMPLETE message from A and therefore StreamSession::complete() is invoked.
4- Therefore both maybeComplete() and complete() functions have branched into the state != State.WAIT_COMPLETE case and both set the state to WAIT_COMPLETE.
5- Now B is waiting to receive COMPLETE although it's already received it and nothing triggers checking the state again, until it times out after streaming_socket_timeout_in_ms.

In the log below:

https://gist.github.com/omidaladini/003de259958ad8dfb07e

although the node has received COMPLETE, ""SocketTimeoutException"" is thrown after streaming_socket_timeout_in_ms (30 minutes here)."
CASSANDRA-9456,"while starting cassandra using cassandra -f ; encountered an errror ERROR 11:46:42 Exception in thread Thread[MemtableFlushWriter:1,5,main]","I am using openjdk7 and build cassandra successfully using source code from github(https://github.com/apache/cassandra.git)
 
afer successfuly I set the path for cassandra 
and tried to start using cassandra -f 

below is the errror encountered while startup
{code}
INFO  11:46:42 Token metadata:
INFO  11:46:42 Enqueuing flush of local: 653 (0%) on-heap, 0 (0%) off-heap
INFO  11:46:42 Writing Memtable-local@1257824677(110 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)
ERROR 11:46:42 Exception in thread Thread[MemtableFlushWriter:1,5,main]
java.lang.NoClassDefFoundError: Could not initialize class com.sun.jna.Native
        at org.apache.cassandra.utils.memory.MemoryUtil.allocate(MemoryUtil.java:82) ~[main/:na]
        at org.apache.cassandra.io.util.Memory.<init>(Memory.java:74) ~[main/:na]
        at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:32) ~[main/:na]
        at org.apache.cassandra.io.compress.CompressionMetadata$Writer.<init>(CompressionMetadata.java:274) ~[main/:na]
        at org.apache.cassandra.io.compress.CompressionMetadata$Writer.open(CompressionMetadata.java:288) ~[main/:na]
        at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:75) ~[main/:na]
        at org.apache.cassandra.io.util.SequentialWriter.open(SequentialWriter.java:168) ~[main/:na]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:74) ~[main/:na]
        at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107) ~[main/:na]
        at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:84) ~[main/:na]
        at org.apache.cassandra.db.Memtable$FlushRunnable.createFlushWriter(Memtable.java:396) ~[main/:na]
        at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:343) ~[main/:na]
        at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:328) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.jar:na]
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1085) ~[main/:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_79]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_79]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_79]
{code}"
CASSANDRA-9433,TraceCompleteTest.testTraceComplete is failing infrequently,"I have only seen this fail once, but haven't been looking hard.
http://cassci.datastax.com/view/trunk/job/trunk_testall/98/testReport/junit/org.apache.cassandra.tracing/TraceCompleteTest/testTraceComplete/

This is a weird one. It looks straightforward because the test is waiting for an event, but for only 250 milliseconds which is within the amount of time a node can just up and pause because life is hard.

However if you look at the test run it failed twice! Once in test and again in test-compression. Those odds aren't great.

I am tempted to increase the timeout and keep an eye open. It returns as soon as the event is available so it shouldn't have an impact on how long the test runs on average."
CASSANDRA-9423,Improve Leak Detection to cover strong reference leaks,"Currently we detect resources that we don't cleanup that become unreachable. We could also detect references that appear to have leaked without becoming unreachable, by periodically scanning the set of extant refs, and checking if they are reachable via their normal means (if any); if their lifetime is unexpectedly long this likely indicates a problem, and we can log a warning/error.

Assigning to myself to not forget it, since this may well help especially with [~tjake]'s concerns highlighted on 8099 for 3.0."
CASSANDRA-9397,Wrong gc_grace_seconds used in anticompaction,"looks like we use CFMetaData.DEFAULT_GC_GRACE_SECONDS instead of the configured one during anticompaction

attached patch fixes"
CASSANDRA-9382,Snapshot file descriptors not getting purged (possible fd leak),"OpsCenter has the repair service which does a lot of small range repairs. Each repair would generate a snapshot as per normal. The cluster was showing a steady increase in disk space over the course of a couple of days and the only way to workaround the issue was to restart the node.

Upon some further inspection it was seen that a lsof output of the cassandra process was still showing file descriptors for snapshots that no longer existed on the file system. For example:

{code}
ava    5822 cassandra  DEL    REG             202,32                 7359833 /media/ephemeral1/cassandra/data/somekeyspace/table1/snapshots/669a3a30-f3d3-11e4-bec6-3f6c4fb06498/somekeyspace-table1-jb-897689-Data.db
{code}

We also took a heapdump which basically showed the same thing, lots of references to these file handles. We checked the logs for any errors especially relating to repairs that might have failed but there was nothing observed

The repair service logs in OpsCenter showed also that all repairs (1000s of them) had completed successfully, again showing that there was no repair issue.

I have not yet been able to reproduce the issue locally on a test box. The cluster that this original issue appeared on was a production cluster with the following spec:

cassandra_versions: 2.0.14.352
cluster_cores : 8, 
cluster_instance_types : i2.2xlarge
cluster_os : Amazon linux amd64 
node count: 4
node java version: Oracle Java 1.7.0_51
"
CASSANDRA-9368,Remove MessagingService.allNodesAtLeast21 and related digest logic,"In 2.2, we should not have any 2.0 nodes in the ring, so conditional digest update is not required. We should clean this up."
CASSANDRA-9332,NPE when creating column family via thrift,"When triggering unit test ""testAddDropColumnFamily()"" in https://github.com/hector-client/hector/blob/master/core/src/test/java/me/prettyprint/cassandra/service/CassandraClusterTest.java 

It occurs NPE when using *Cassandra 2.0.6* or later version.
{noformat}
11:42:39,173 [Thrift:1] ERROR CustomTThreadPoolServer:212 - Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.db.RowMutation.add(RowMutation.java:112)
	at org.apache.cassandra.service.MigrationManager.addSerializedKeyspace(MigrationManager.java:265)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:213)
	at org.apache.cassandra.thrift.CassandraServer.system_add_column_family(CassandraServer.java:1521)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:4300)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:4284)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

It seems that was introduced by fix of CASSANDRA-5631."
CASSANDRA-9290,Fix possible NPE in Scrubber,"In scrubber there is a possible NullPointerException introduced by CASSANDRA-9140, as discovered by the Coverity static analyzer (CID 109861):

{code}
throw new IOError(new IOException(String.format(""Key from data file (%s) does not match key from index file (%s)"",
        ByteBufferUtil.bytesToHex(key.getKey()), ByteBufferUtil.bytesToHex(currentIndexKey))));
{code}

{{currentIndexKey}} may be null and {{bytesToHex}} expects non null."
CASSANDRA-9285,LEAK DETECTED in sstwriter,"reproduce bug : 

{code}
    public static void main(String[] args) throws Exception {
        System.setProperty(""cassandra.debugrefcount"",""true"");
        
        String ks = ""ks1"";
        String table = ""t1"";
        
        String schema = ""CREATE TABLE "" + ks + ""."" + table + ""(a1 INT, PRIMARY KEY (a1));"";
        String insert = ""INSERT INTO ""+ ks + ""."" + table + ""(a1) VALUES(?);"";
        
        File dir = new File(""/var/tmp/"" + ks + ""/"" + table);
        dir.mkdirs();
        
        CQLSSTableWriter writer = CQLSSTableWriter.builder().forTable(schema).using(insert).inDirectory(dir).build();
        
        writer.addRow(1);
        writer.close();
        writer = null;
        
        Thread.sleep(1000);System.gc();
        Thread.sleep(1000);System.gc();
    }
{code}

{quote}
[2015-05-01 16:09:59,139] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@79fa9da9) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@2053866990:Memory@[7f87f8043b20..7f87f8043b48) was not released before the reference was garbage collected
[2015-05-01 16:09:59,143] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@79fa9da9:
Thread[Thread-2,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:32)
	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:33)
	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:111)
	at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.<init>(SSTableWriter.java:576)
	at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:140)
	at org.apache.cassandra.io.sstable.AbstractSSTableSimpleWriter.getWriter(AbstractSSTableSimpleWriter.java:58)
	at org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter$DiskWriter.run(SSTableSimpleUnsortedWriter.java:227)

[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@664382e3) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@899100784:Memory@[7f87f8043990..7f87f8043994) was not released before the reference was garbage collected
[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@664382e3:
Thread[Thread-2,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:32)
	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:33)
	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:110)
	at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.<init>(SSTableWriter.java:576)
	at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:140)
	at org.apache.cassandra.io.sstable.AbstractSSTableSimpleWriter.getWriter(AbstractSSTableSimpleWriter.java:58)
	at org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter$DiskWriter.run(SSTableSimpleUnsortedWriter.java:227)

[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3cca0ac2) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@499043670:Memory@[7f87f8039940..7f87f8039c60) was not released before the reference was garbage collected
[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@3cca0ac2:
Thread[Thread-2,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:32)
	at org.apache.cassandra.io.compress.CompressionMetadata$Writer.<init>(CompressionMetadata.java:274)
	at org.apache.cassandra.io.compress.CompressionMetadata$Writer.open(CompressionMetadata.java:285)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:74)
	at org.apache.cassandra.io.util.SequentialWriter.open(SequentialWriter.java:124)
	at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:129)
	at org.apache.cassandra.io.sstable.AbstractSSTableSimpleWriter.getWriter(AbstractSSTableSimpleWriter.java:58)
	at org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter$DiskWriter.run(SSTableSimpleUnsortedWriter.java:227)
{quote}
"
CASSANDRA-9271,IndexSummaryManagerTest.testCompactionRace times out periodically,"The issue is that the amount of time the test takes is highly variable to it being biased towards creating a condition where the test has to retry the compaction it is attempting.

Solution is to decrease the bias by having https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L2522 check every millisecond instead of every 100 milliseconds."
CASSANDRA-9269,Huge commitlog not flushed.,"I wrote at once a lot of data in several column families of keyspaceName on a single node cluster. I only read from this keyspace afterward. My issue is that size of commitlog is huge and does not shrink:
$ du -sh data/*
7.8G    data/commitlog
7.0G    data/data
36M     data/saved_caches

When I try to flush using nodetool, if i run ""./bin/nodetool flush"" no error occurs and no change happens in size of commit log.  even when I specify keyspaceName no change happens."
CASSANDRA-9256,Refactor MessagingService to support pluggable transports,"CASSANDRA-7029 and CASSANDRA-9237 would both benefit greatly from a pluggable MessagingService.

Ideally, we would refactor the native transport to use the same abstractions, so that we could have a single implementation of each viable transport mechanism for both, and we can easily test out the impact of any transport on the whole cluster, not just one half. This is especially important for establishing if there is a benefit to approaches that permit us to isolate networking to a single thread/core, as the characteristics would be quite different if we still needed many networking threads for the other half of the equation."
CASSANDRA-9255,"If a memtable flush fails, possible deadlock for future flushes","If a memtable flush throws and exception, it won't signal the countdown latch that the post flush task is waiting on. Since the post flush executor is single threaded, it will continue to wait on its countdown latch and will not process any additional post flush tasks. After that, any blocking flush will deadlock waiting for the post flush task to finish.

I hit this while I was adding tests for CASSANDRA-9057 where the memtable failed to write because my validation was wrong and then it deadlocked creating a new table, but it should be safeguarded against the possibility that a memtable flush fails for any reason.

Attaching the jstack showing the deadlock. "
CASSANDRA-9238,Race condition after shutdown gossip message,"CASSANDRA-8336 introduced a race condition causing gossip messages to be sent to shutdown nodes even if they have been already marked dead.

That's because CASSANDRA-8336 changed (among other things) the way the SHUTDOWN gossip message is sent by moving it before the gossip task (the one sending SYN messages), and by putting a few secs wait between the two; this opens a race window by the receiving side between the time the SHUTDOWN message is received, causing the outbound sockets to be closed, and the moment the other side listening socket is actually closed, meaning that any SYN gossip message exchanged in such window will reopen the sockets and never close them again, as the node is already marked dead. "
CASSANDRA-9235,"Max sstable size in leveled manifest is an int, creating large sstables overflows this and breaks LCS","nodetool compactionstats
pending tasks: -222222228

I can see negative numbers in 'pending tasks' on all 8 nodes
it looks like -222222228 + real number of pending tasks
for example -222222128 for 100 real pending tasks

"
CASSANDRA-9226,Log streamid at the trace level on sending request and receiving response,"Multiple requests are sent on the same connection and responses arrive in any order. It is hard to track single request/response pair even when the trace logs are turned on today because the trace logs only provide information up to connection as shown below. It will be super useful to also provide the streamid in the trace logs. 

2015/04/17 14:02:43.334  TRACE [New I/O worker #447] com.datastax.driver.core.Connection - Connection[host1.domain.com/10.0.1.1:9042-1, inFlight=1, closed=false] request sent successfully
2015/04/17 14:02:43.337  TRACE [New I/O worker #447] com.datastax.driver.core.Connection - Connection[host1.domain.com/10.0.1.1:9042-1, inFlight=1, closed=false] received: ROWS [column1 (timeuuid)][value (blob)][writetime(value) (bigint)]

streamid will be useful at-least in the 2 logs above.  But if you know of other cases where it will be useful, please add it there as well.  "
CASSANDRA-9216,NullPointerException (NPE) during Compaction Cache Serialization,"In case this hasn't been reported (I looked but did not see it), a null pointer exception is occurring during compaction. The stack track is as follows:
{code}
ERROR [CompactionExecutor:50] 2015-04-20 13:42:43,827 CassandraDaemon.java:223 - Exception in thread Thread[CompactionExecutor:50,1,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:475) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:463) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:274) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1152) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_75]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_75]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_75]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]{code}"
CASSANDRA-9208,Setting rpc_interface in cassandra.yaml causes NPE during startup,"In the cassandra.yaml file when ""rpc_interface"" option is set it causes a NPE (stack trace at the end).
Upon further investigation it turns out that there is a serious problem is in the way this logic is handled in the code DatabaseDescriptor.java (#374).

Following is the code snippet 
 else if (conf.rpc_interface != null)
        {
            listenAddress = getNetworkInterfaceAddress(conf.rpc_interface, ""rpc_interface"");
        }
        else
        {
            rpcAddress = FBUtilities.getLocalAddress();
        }

If you notice, 

1) The code above sets the ""listenAddress"" instead of ""rpcAddress"".  
2) The function getNetworkInterfaceAddress() blindly assumes that this is called to set the ""listenAddress"" (see line 171). The ""configName"" variable passed to the function is royally ignored and only used for printing out exception (which again is misleading)

I am also attaching a suggested patch (NOTE: the patch tries to address this issue, the function getNetworkInterfaceAddress() needs revision ).


INFO  15:36:56 Windows environment detected.  DiskAccessMode set to standard, indexAccessMode standard
INFO  15:36:56 Global memtable on-heap threshold is enabled at 503MB
INFO  15:36:56 Global memtable off-heap threshold is enabled at 503MB
ERROR 15:37:50 Fatal error during configuration loading
java.lang.NullPointerException: null
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:411) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:164) [apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:533) [apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:622) [apache-cassandra-2.1.4.jar:2.1.4]
null
Fatal error during configuration loading; unable to start. See log for stacktrace."
CASSANDRA-9201,Race condition on creating system_auth.roles and using it on startup,"It looks like it's possible for {{system_auth.roles}} to have a statement prepared against it before the table exists on startup:

{noformat}
ERROR [main] 2015-04-15 15:12:35,626 CassandraDaemon.java: Exception encountered during startup
java.lang.AssertionError: org.apache.cassandra.exceptions.InvalidRequestException: unconfigured table roles
    at org.apache.cassandra.auth.CassandraRoleManager.prepare(CassandraRoleManager.java:427) ~[main/:na]
    at org.apache.cassandra.auth.CassandraRoleManager.setup(CassandraRoleManager.java:139) ~[main/:na]
    at org.apache.cassandra.service.StorageService.doAuthSetup(StorageService.java:1009) ~[main/:na]
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:936) ~[main/:na]
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:670) ~[main/:na]
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:557) ~[main/:na]
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:412) [main/:na]
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:561) [main/:na]
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:668) [main/:na]
Caused by: org.apache.cassandra.exceptions.InvalidRequestException: unconfigured table roles
    at org.apache.cassandra.thrift.ThriftValidation.validateColumnFamily(ThriftValidation.java:115) ~[main/:na]
    at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:739) ~[main/:na]
    at org.apache.cassandra.auth.CassandraRoleManager.prepare(CassandraRoleManager.java:423) ~[main/:na]
    ... 8 common frames omitted
INFO  [StorageServiceShutdownHook] 2015-04-15 15:12:35,636 Gossiper.java: Announcing shutdown
INFO  [MigrationStage:1] 2015-04-15 15:12:35,639 Schema.java: Loading org.apache.cassandra.config.CFMetaData@57a4b158[cfId=5bc52802-de25-35ed-aeab-188eecebb090,ksName=system_auth,cfName=roles,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(6d656d6265725f6f66:org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type))),comment=role definitions,readRepairChance=0.0,dcLocalReadRepairChance=0.0,gcGraceSeconds=7776000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UTF8Type,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata=[ColumnDefinition{name=member_of, type=org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, ColumnDefinition{name=is_superuser, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, ColumnDefinition{name=role, type=org.apache.cassandra.db.marshal.UTF8Type, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, ColumnDefinition{name=salted_hash, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, ColumnDefinition{name=can_login, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}],compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,droppedColumns={},triggers=[],isDense=false]
INFO  [MigrationStage:1] 2015-04-15 15:12:35,654 ColumnFamilyStore.java: Initializing system_auth.roles
{noformat}"
CASSANDRA-9193,Facility to write dynamic code to selectively trigger trace or log for queries,"I want the equivalent of dtrace for Cassandra. I want the ability to intercept a query with a dynamic script (assume JS) and based on logic in that script trigger the statement for trace or logging. 

Examples 
- Trace only INSERT statements to a particular CF. 
- Trace statements for a particular partition or consistency level.
- Log statements that fail to reach the desired consistency for read or write.
- Log If the request size for read or write exceeds some threshold

At some point in the future it would be helpful to also do things such as log partitions greater than X bytes or Z cells when performing compaction. Essentially be able to inject custom code dynamically without a reboot to the different stages of C*. 

The code should be executed synchronously as part of the monitored task, but we should provide the ability to log or execute CQL asynchronously from the provided API.

Further down the line we could use this functionality to modify/rewrite requests or tasks dynamically."
CASSANDRA-9182,NPE during startup,"Environment:
* cassandra 2.1.3

Got NPE during startup. Here is steps to reproduce (however not sure if that will be enough):
* start single node cluster. fill it with data (replication factor 1)
* start second node.
* in second node's logs:

{code}
ERROR [Thread-3] 2015-04-13 07:22:58,558 CassandraDaemon.java - Exception in thread Thread[Thread-3,5,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.db.PagedRangeCommand$Serializer.deserialize(PagedRangeCommand.java:165) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.db.PagedRangeCommand$Serializer.deserialize(PagedRangeCommand.java:124) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:168) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:150) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82) ~[apache-cassandra-2.1.3.jar:2.1.3]
INFO  [GossipStage:1] 2015-04-13 07:23:00,149 Gossiper.java - Node /172.30.0.86 is now part of the cluster
ERROR [MigrationStage:1] 2015-04-13 07:23:00,176 MigrationTask.java - Can't send migration request: node /172.30.0.86 is down.
ERROR [MigrationStage:1] 2015-04-13 07:23:00,178 MigrationTask.java - Can't send migration request: node /172.30.0.86 is down.
INFO  [HANDSHAKE-/172.30.0.86] 2015-04-13 07:23:00,184 OutboundTcpConnection.java - Handshaking version with /172.30.0.86
INFO  [SharedPool-Worker-1] 2015-04-13 07:23:00,347 Gossiper.java - InetAddress /172.30.0.86 is now UP
INFO  [HANDSHAKE-/172.30.0.86] 2015-04-13 07:23:00,351 OutboundTcpConnection.java - Handshaking version with /172.30.0.86
INFO  [SharedPool-Worker-1] 2015-04-13 07:23:00,509 Gossiper.java - InetAddress /172.30.0.86 is now UP
{code}"
CASSANDRA-9171,Add deletions to paging dtests,"Deletions can easily impact paging (see CASSANDRA-8490 for an example).  We should improve the paging dtests to include various kinds of deletions: partition-level, row-level, and cell-level.  The tests should also exercise deleting enough contiguous partitions/rows/levels to fill an entire page.

Assigning to [~rhatch] for now since he created the original paging tests, but I can reassign this if needed."
CASSANDRA-9134,Fix leak detected errors in unit tests,"There are several of these errors when running unit tests on trunk:

{code}
    [junit] ERROR 01:09:36 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@317c884a) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@943674927:Memory@[7f1bcc0078e0..7f1bcc007908) was not released before the reference was garbage collected
    [junit] ERROR 01:09:36 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@317c884a) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@943674927:Memory@[7f1bcc0078e0..7f1bcc007908) was not released before the reference was garbage collected
    [junit] ERROR 01:09:36 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@317c884a:
    [junit] Thread[CompactionExecutor:1,1,main]
    [junit] 	at java.lang.Thread.getStackTrace(Thread.java:1589)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
    [junit] 	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:33)
    [junit] 	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:31)
    [junit] 	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:112)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.<init>(BigTableWriter.java:491)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:83)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107)
    [junit] 	at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:89)
    [junit] 	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:53)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:253)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:153)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73)
    [junit] 	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:58)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:239)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    [junit] 	at java.lang.Thread.run(Thread.java:745)
    [junit] 
    [junit] ERROR 01:09:36 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@317c884a:
    [junit] Thread[CompactionExecutor:1,1,main]
    [junit] 	at java.lang.Thread.getStackTrace(Thread.java:1589)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
    [junit] 	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:33)
    [junit] 	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:31)
    [junit] 	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:112)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.<init>(BigTableWriter.java:491)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:83)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107)
    [junit] 	at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:89)
    [junit] 	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:53)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:253)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:153)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73)
    [junit] 	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:58)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:239)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    [junit] 	at java.lang.Thread.run(Thread.java:745)
    [junit] 
{code}"
CASSANDRA-9128,Flush system.IndexInfo after index state changed,"We don't force a flush of {{system.IndexInfo}} after updating it by marking an index as built. This may lead to indexes being unnecessarily rebuilt following a disorderly shutdown.

We also don't update it after an index is removed, but that's probably less of an issue as we do flush {{system.schema_columns}} after removing the index, so those won't get rebuilt."
CASSANDRA-9127,Out of memory failure: ~2Gb retained,See the snapshot analysis.
CASSANDRA-9120,OutOfMemoryError when read auto-saved cache (probably broken),"Found during tests on a 100 nodes cluster. After restart I found that one node constantly crashes with OutOfMemory Exception. I guess that auto-saved cache was corrupted and Cassandra can't recognize it. I see that similar issues was already fixed (when negative size of some structure was read). Does auto-saved cache have checksum? it'd help to reject corrupted cache at the very beginning.

As far as I can see current code still have that problem. Stack trace is:
{code}
INFO [main] 2015-03-28 01:04:13,503 AutoSavingCache.java (line 114) reading saved cache /storage/core/loginsight/cidata/cassandra/saved_caches/system-sstable_activity-KeyCache-b.db
ERROR [main] 2015-03-28 01:04:14,718 CassandraDaemon.java (line 513) Exception encountered during startup
java.lang.OutOfMemoryError: Java heap space
        at java.util.ArrayList.<init>(Unknown Source)
        at org.apache.cassandra.db.RowIndexEntry$Serializer.deserialize(RowIndexEntry.java:120)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:365)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:262)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:421)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:315)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:272)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:114)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:92)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:536)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:261)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:496)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:585)
{code}
I looked at source code of Cassandra and see:
http://grepcode.com/file/repo1.maven.org/maven2/org.apache.cassandra/cassandra-all/2.0.10/org/apache/cassandra/db/RowIndexEntry.java

119 int entries = in.readInt();
120 List<IndexHelper.IndexInfo> columnsIndex = new ArrayList<IndexHelper.IndexInfo>(entries);

It seems that value entries is invalid (negative) and it tries too allocate an array with huge initial capacity and hits OOM. I have deleted saved_cache directory and was able to start node correctly. We should expect that it may happen in real world. Cassandra should be able to skip incorrect cached data and run."
CASSANDRA-9117,"LEAK DETECTED during repair, startup","When running the {{incremental_repair_test.TestIncRepair.multiple_repair_test}} dtest, the following error logs show up:

{noformat}
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,491 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@83f047e) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1631580268:Memory@[7f354800bdc0..7f354800bde8) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,493 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@50bc8f67) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@191552666:Memory@[7f354800ba90..7f354800bdb0) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,493 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7fd10877) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1954741807:Memory@[7f3548101190..7f3548101194) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,494 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@578550ac) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1903393047:[[OffHeapBitSet]] was not released before the reference was garbage collected
{noformat}

The test is being run against trunk (commit {{1dff098e}}).  I've attached a DEBUG-level log from the test run."
CASSANDRA-9077,Deleting an element from a List which is null throws a NPE,"I am seeing an NPE on the latest 2.1 branch with this sequence of deletes from a list - first delete the entire list, then attempt to delete one element.

I expected to see {{List index 0 out of bound, list has size 0}} but instead got an NPE.

{noformat}
./bin/cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.1.3-SNAPSHOT | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
cqlsh> use frozen_collections ;
cqlsh:frozen_collections> DROP TABLE IF EXISTS t;
cqlsh:frozen_collections> CREATE TABLE t (id text PRIMARY KEY, l list<text>, s set<text>);
cqlsh:frozen_collections> INSERT INTO t (id, l, s) VALUES ('user', ['1'], {'1'});
cqlsh:frozen_collections>
cqlsh:frozen_collections> DELETE l FROM t WHERE id ='user';
cqlsh:frozen_collections> //INSERT INTO t (id, l) VALUES ('user', ['1']);
cqlsh:frozen_collections> DELETE l[0] FROM t WHERE id = 'user';
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.NullPointerException"">
cqlsh:frozen_collections>
cqlsh:frozen_collections> DELETE s FROM t WHERE id ='user';
cqlsh:frozen_collections> DELETE s['1'] FROM t WHERE id = 'user';
{noformat}

It appears the {{DELETE emails...}} directly followed by {{DELETE emails[0]...}} is the offending sequence. Either one alone works fine, as does adding an intervening insert/update.

The same sequence performed on a Set rather than List works (as shown above).
"
CASSANDRA-9070,Race in cancelling compactions,"seems we might have a race situation when cancelling compactions

currently we do the following to ensure that we don't start any new compactions when we try to do markAllCompacting()

# pause compactions - this makes sure we don't create any new compaction tasks from the compaction strategies
# cancel any ongoing compactions - compactions register themselves with the CompactionMetrics and then, when cancelling we get all compactions here, and tell them to stop

Problem is that there is a window between when the CompactionTask is created and when it is registered in CompactionMetrics meaning with a bit of bad luck, we could have a situation like this:
# we finish a compaction and create a new CompactionTask from the compaction strategy
# we pause the compaction strategies to not create any new CompactionTasks
# we cancel all ongoing compactions
# The CompactionTask created in #1 above registers itself in CompactionMetrics and misses that it should be cancelled"
CASSANDRA-9021,AssertionError and Leak detected during sstable compaction,"After ~3 hours of data ingestion we see assertion errors and 'LEAK DETECTED' errors during what looks like sstable compaction.


system.log snippets (full log attached):
{code}
...
INFO  [CompactionExecutor:12] 2015-03-23 02:45:51,770  CompactionTask.java:267 - Compacted 4 sstables to [/mnt/cass_data_disks/data1/requests_ks/timeline-       9500fe40d0f611e495675d5ea01541b5/requests_ks-timeline-ka-185,].  65,916,594 bytes to 66,159,512 (~100% of original) in 26,554ms = 2.376087MB/s.  983 total       partitions merged to 805.  Partition merge counts were {1:627, 2:178, }
INFO  [CompactionExecutor:11] 2015-03-23 02:45:51,837  CompactionTask.java:267 - Compacted 4 sstables to [/mnt/cass_data_disks/data1/system/                     compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-119,].  426 bytes to 42 (~9% of original) in 82ms = 0.000488MB/s.  5  total partitions merged to 1.  Partition merge counts were {1:1, 2:2, }
ERROR [NonPeriodicTasks:1] 2015-03-23 02:45:52,251  CassandraDaemon.java:167 - Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.AssertionError: null
 at org.apache.cassandra.io.compress.CompressionMetadata$Chunk.<init>(CompressionMetadata.java:438) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:228) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.dropPageCache(CompressedPoolingSegmentedFile.java:80) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.sstable.SSTableReader$6.run(SSTableReader.java:923) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier$1.run(SSTableReader.java:2036) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_45]
 at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_45]
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) ~[na:1.7.0_45]
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) ~[na:1.7.0_45]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_45]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
 at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
...
INFO  [MemtableFlushWriter:50] 2015-03-23 02:47:29,465  Memtable.java:378 - Completed flushing /mnt/cass_data_disks/data1/requests_ks/timeline-                  9500fe40d0f611e495675d5ea01541b5/requests_ks-timeline-ka-188-Data.db (16311981 bytes) for commitlog position ReplayPosition(segmentId=1427071574495,             position=4523631)
ERROR [Reference-Reaper:1] 2015-03-23 02:47:33,987  Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2f59b10) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@1251424500:/mnt/cass_data_disks/data1/requests_ks/timeline-9500fe40d0f611e495675d5ea01541b5/    requests_ks-timeline-ka-149 was not released before the reference was garbage collected
INFO  [Service Thread] 2015-03-23 02:47:40,158  GCInspector.java:142 - ConcurrentMarkSweep GC in 12247ms.  CMS Old Gen: 5318987136 -> 457655168; CMS Perm Gen:   44731264 -> 44699160; Par Eden Space: 8597912 -> 418006664; Par Survivor Space: 71865728 -> 59679584
...
{code}
"
CASSANDRA-8981,IndexSummaryManagerTest.testCompactionsRace intermittently timing out on trunk,"Keep running it repeatedly w/showoutput=""yes"" in build.xml on junit and you'll see it time out with:

{noformat}
    [junit] WARN  17:02:56 Unable to cancel in-progress compactions for StandardRace.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.
    [junit] WARN  17:02:56 Unable to cancel in-progress compactions for StandardRace.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.
    [junit] WARN  17:02:57 Unable to cancel in-progress compactions for StandardRace.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.
    [junit] WARN  17:02:57 Unable to cancel in-progress compactions for StandardRace.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.
{noformat}

I originally thought this was a Windows specific problem (CASSANDRA-8962) but can reproduce on linux by just repeatedly running the test."
CASSANDRA-8969,Add indication in cassandra.yaml that rpc timeouts going too high will cause memory build up,It would be helpful to communicate that setting the rpc timeouts too high may cause memory problems on the server as it can become overloaded and has to retain the in flight requests in memory.  I'll get this done but just adding the ticket as a placeholder for memory.
CASSANDRA-8939,Stack overflow when reading data ingested through SSTableLoader,"I created an empty table:

{noformat}
CREATE TABLE test.kv (
    key int PRIMARY KEY,
    value text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
{noformat}

Then I loaded some rows into it using CqlSSTableWriter and SSTableLoader (programmatically, doing it the same way as BulkLoader is doing it). The streaming finished with no errors. 

I can even read all the data back with cqlsh:

{noformat}
cqlsh> SELECT key, value FROM test.kv;
....
 3405 | foo3405
 5504 | foo5504
 3476 | foo3476
 2542 | foo2542
 6931 | foo6931

---MORE---
(10000 rows)
{noformat}

However, filtering by token fails:
{noformat}
cqlsh> SELECT key, value FROM test.kv WHERE token(key) > 854443789258213092;
OperationTimedOut: errors={}, last_host=127.0.0.1
cqlsh> 
{noformat}

Server log repors a StackOverflowException:
{noformat}
WARN  15:10:05  Uncaught exception on thread Thread[SharedPool-Worker-2,5,main]: {}
java.lang.StackOverflowError: null
	at java.nio.charset.CharsetDecoder.implReplaceWith(CharsetDecoder.java:302) ~[na:1.7.0_75]
	at java.nio.charset.CharsetDecoder.replaceWith(CharsetDecoder.java:288) ~[na:1.7.0_75]
	at java.nio.charset.CharsetDecoder.<init>(CharsetDecoder.java:203) ~[na:1.7.0_75]
	at java.nio.charset.CharsetDecoder.<init>(CharsetDecoder.java:226) ~[na:1.7.0_75]
	at sun.nio.cs.UTF_8$Decoder.<init>(UTF_8.java:84) ~[na:1.7.0_75]
	at sun.nio.cs.UTF_8$Decoder.<init>(UTF_8.java:81) ~[na:1.7.0_75]
	at sun.nio.cs.UTF_8.newDecoder(UTF_8.java:68) ~[na:1.7.0_75]
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:152) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:39) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:26) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.marshal.AbstractType.getString(AbstractType.java:82) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:54) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.composites.CompoundSparseCellNameType.idFor(CompoundSparseCellNameType.java:169) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.composites.CompoundSparseCellNameType.makeWith(CompoundSparseCellNameType.java:177) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.composites.AbstractCompoundCellNameType.fromByteBuffer(AbstractCompoundCellNameType.java:106) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:397) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:381) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:75) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) ~[guava-16.0.1.jar:na]
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138) ~[guava-16.0.1.jar:na]
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:116) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:172) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:155) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:203) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:107) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:81) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:99) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:71) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:117) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:100) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) ~[guava-16.0.1.jar:na]
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138) ~[guava-16.0.1.jar:na]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:1996) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
	at org.apache.cassandra.db.ColumnFamilyStore$8.computeNext(ColumnFamilyStore.java:2007) ~[cassandra-all-2.1.3.248.jar:2.1.3.248]
{noformat}

This problem doesn't happen if the table data was inserted using the standard write path, i.e. with INSERTs/UPDATEs. 

Cassandra version:
branch: 2.1
commit: 3f6ad3c9886c01c2cdaed6cad10c6f0672004473"
CASSANDRA-8925,broadcast_rpc_address NPEs while using rpc_interface ,"Somewhat amusingly, it looks like my NPE on startup is the result of a copy-paste error in:
{code}
[clockfort@clockfort cassandra]$ git log --stat 3e5edb82
commit 3e5edb82c73b7b7c6e1d1e970fb764c3e3158da6
Author: Ariel Weisberg <ariel.weisberg@datastax.com>
Date:   Tue Jan 27 13:30:47 2015 +0100

    rpc_interface and listen_interface generate NPE on startup when specified interface doesn't exist
    
    Patch by Ariel Weisberg; reviewed by Robert Stupp for CASSANDRA-8677

 src/java/org/apache/cassandra/config/DatabaseDescriptor.java | 46 +++++++++++++++++++++++-----------------------
 1 file changed, 23 insertions(+), 23 deletions(-)
{code}

The log looks like:
{code}
INFO  18:51:13 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  18:51:13 Global memtable on-heap threshold is enabled at 2008MB
INFO  18:51:13 Global memtable off-heap threshold is enabled at 2008MB
ERROR 18:51:13 Fatal error during configuration loading
java.lang.NullPointerException: null
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:411) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:110) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:465) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:554) [apache-cassandra-2.1.3.jar:2.1.3]
null
Fatal error during configuration loading; unable to start. See log for stacktrace.
{code}

This is with a cassandra.yaml config snippet of:
{code}
start_rpc: true
# rpc_address: localhost
rpc_interface: eth0
# RPC address to broadcast to drivers and other Cassandra nodes. This cannot
# be set to 0.0.0.0. If left blank, this will be set to the value of
# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must
# be set.
# broadcast_rpc_address: 1.2.3.4
{code}

"
CASSANDRA-8884,Opening a non-system keyspace before first accessing the system keyspace results in deadlock,"I created a writer like this:
{code}
val writer = CQLSSTableWriter.builder()
      .forTable(tableDef.cql)
      .using(insertStatement)
      .withPartitioner(partitioner)
      .inDirectory(outputDirectory)    
      .withBufferSizeInMB(bufferSizeInMB)
      .build()
{code}

Then I'm trying to write a row with {{addRow}} and it blocks forever.
Everything related to {{CQLSSTableWriter}}, including its creation, is happening in only one thread.

{noformat}
""SSTableBatchOpen:3"" daemon prio=10 tid=0x00007f4b399d7000 nid=0x4778 waiting for monitor entry [0x00007f4b240a7000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:118)
	- waiting to lock <0x00000000e35fd6d0> (a java.lang.Class for org.apache.cassandra.db.Keyspace)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:99)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1464)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:517)
	at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:265)
	at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:306)
	at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:316)
	at org.apache.cassandra.db.SystemKeyspace.getSSTableReadMeter(SystemKeyspace.java:910)
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:561)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:433)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:343)
	at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:480)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""SSTableBatchOpen:2"" daemon prio=10 tid=0x00007f4b399e7800 nid=0x4777 waiting for monitor entry [0x00007f4b23ca3000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:118)
	- waiting to lock <0x00000000e35fd6d0> (a java.lang.Class for org.apache.cassandra.db.Keyspace)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:99)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1464)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:517)
	at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:265)
	at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:306)
	at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:316)
	at org.apache.cassandra.db.SystemKeyspace.getSSTableReadMeter(SystemKeyspace.java:910)
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:561)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:433)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:343)
	at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:480)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""SSTableBatchOpen:1"" daemon prio=10 tid=0x00007f4b399e7000 nid=0x4776 waiting for monitor entry [0x00007f4b2359d000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:118)
	- waiting to lock <0x00000000e35fd6d0> (a java.lang.Class for org.apache.cassandra.db.Keyspace)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:99)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1464)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:517)
	at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:265)
	at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:306)
	at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:316)
	at org.apache.cassandra.db.SystemKeyspace.getSSTableReadMeter(SystemKeyspace.java:910)
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:561)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:433)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:343)
	at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:480)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""Executor task launch worker-3"" daemon prio=10 tid=0x00007f4b38ce6000 nid=0x472c waiting on condition [0x00007f4b26deb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000fac03b40> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1468)
	at org.apache.cassandra.io.sstable.SSTableReader.openAll(SSTableReader.java:496)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:321)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:478)
	- locked <0x00000000e374a2b0> (a java.lang.Class for org.apache.cassandra.db.ColumnFamilyStore)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:449)
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:327)
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:280)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:122)
	- locked <0x00000000e35fd6d0> (a java.lang.Class for org.apache.cassandra.db.Keyspace)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:99)
	at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:101)
	at org.apache.cassandra.io.sstable.CQLSSTableWriter.rawAddRow(CQLSSTableWriter.java:225)
	at org.apache.cassandra.io.sstable.CQLSSTableWriter.addRow(CQLSSTableWriter.java:144)
	at org.apache.cassandra.io.sstable.CQLSSTableWriter.addRow(CQLSSTableWriter.java:119)
	at com.datastax.bdp.spark.writer.BulkTableWriter$$anonfun$write$1.apply(BulkTableWriter.scala:101)
	at com.datastax.bdp.spark.writer.BulkTableWriter$$anonfun$write$1.apply(BulkTableWriter.scala:97)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at com.datastax.bdp.spark.writer.BulkTableWriter.write(BulkTableWriter.scala:97)
	- locked <0x00000000e0f00b78> (a com.datastax.bdp.spark.writer.BulkTableWriter$)
	at com.datastax.bdp.spark.writer.BulkTableWriter$$anonfun$saveRddToCassandra$1.apply(BulkTableWriter.scala:144)
	at com.datastax.bdp.spark.writer.BulkTableWriter$$anonfun$saveRddToCassandra$1.apply(BulkTableWriter.scala:144)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

"
CASSANDRA-8871,Non-null paging state returned if last page,"When retrieving the next page from the result of a simple statement, the result will return a non-null paging state even if it's the last page of the query. This only happens if it's the last page, and the results of the last page exactly matches the paging size.

Schema:
{noformat}
      CREATE KEYSPACE simplex WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
      USE simplex;
      CREATE TABLE test (k text, v int, PRIMARY KEY (k, v));
      INSERT INTO test (k, v) VALUES ('a', 0);
      INSERT INTO test (k, v) VALUES ('b', 1);
      INSERT INTO test (k, v) VALUES ('c', 2);
      INSERT INTO test (k, v) VALUES ('d', 3);
      INSERT INTO test (k, v) VALUES ('e', 4);
{noformat}

Query:
{noformat}
      result  = session.execute(""SELECT * FROM test"", page_size: 5)

      loop do
        puts ""last page? #{result.last_page?}""
        puts ""page size: #{result.size}""

        result.each do |row|
          puts row
        end
        puts """"

        break if result.last_page?
        result = result.next_page
      end
{noformat}

Result:
{noformat}
      last page? false
      page size: 5
      {""k""=>""a"", ""v""=>0}
      {""k""=>""c"", ""v""=>2}
      {""k""=>""m"", ""v""=>12}
      {""k""=>""f"", ""v""=>5}
      {""k""=>""o"", ""v""=>14}
      
      last page? true
      page size: 0
{noformat}
"
CASSANDRA-8864,NPE in CQLSSTableWriter when bulk loading data from Hadoop,"CASSANDRA-8280 introduced a bug in CQLSSTableWriter when used by CqlBulkRecordWriter. It assumes fields in the DatabaseDescriptor is set when they are not, causing NPE.

Relevant stack trace below.

Error: java.lang.NullPointerException at org.apache.cassandra.config.DatabaseDescriptor.createAllDirectories(DatabaseDescriptor.java:605)
  at org.apache.cassandra.db.Keyspace.<clinit>(Keyspace.java:73)
  at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:109)
  at org.apache.cassandra.io.sstable.CQLSSTableWriter.rawAddRow(CQLSSTableWriter.java:218)
  at com.spotify.hdfs2cass.cassandra.cql.CrunchCqlBulkRecordWriter.write(CrunchCqlBulkRecordWriter.java:114)
  at com.spotify.hdfs2cass.cassandra.cql.CrunchCqlBulkRecordWriter.write(CrunchCqlBulkRecordWriter.java:51)
  at org.apache.crunch.io.CrunchOutputs.write(CrunchOutputs.java:133)
  at org.apache.crunch.impl.mr.emit.MultipleOutputEmitter.emit(MultipleOutputEmitter.java:41)
  at org.apache.crunch.MapFn.process(MapFn.java:34)
  "
CASSANDRA-8839,DatabaseDescriptor throws NPE when rpc_interface is used,"Copy from mail to dev mailinglist. 

When using

- listen_interface instead of listen_address
- rpc_interface instead of rpc_address

starting 2.1.3 throws an NPE:

{code}
ERROR [main] 2015-02-20 07:50:09,661 DatabaseDescriptor.java:144 - Fatal error during configuration loading
java.lang.NullPointerException: null
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:411) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:110) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:465) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:554) [apache-cassandra-2.1.3.jar:2.1.3]
{code}

Occurs on debian package as well as in tar.gz distribution. 

{code}
/* Local IP, hostname or interface to bind RPC server to */
if(conf.rpc_address !=null&& conf.rpc_interface !=null)
{
    throw newConfigurationException(""Set rpc_address OR rpc_interface, not both"");
}
else if(conf.rpc_address !=null)
{
    try
    {
        rpcAddress = InetAddress.getByName(conf.rpc_address);
    }
    catch(UnknownHostException e)
    {
        throw newConfigurationException(""Unknown host in rpc_address ""+ conf.rpc_address);
    }
}
else if(conf.rpc_interface !=null)
{
    listenAddress = getNetworkInterfaceAddress(conf.rpc_interface,""rpc_interface"");
}
else
{
    rpcAddress = FBUtilities.getLocalAddress();
}
{code}

I think that listenAddress in the second else block is an error. In my case rpc_interface is eth0, so listenAddress gets set, and rpcAddress remains unset. The result is NPE in line 411:

{code}
if(rpcAddress.isAnyLocalAddress())
{code}

After changing rpc_interface to rpc_address everything works as expected.

"
CASSANDRA-8815,Race in sstable ref counting during streaming failures,"We have a seen a machine in Prod whose all read threads are blocked(spinning) on trying to acquire the reference lock on stables. There are also some stream sessions which are doing the same. 
On looking at the heap dump, we could see that a live sstable which is part of the View has a ref count = 0. This sstable is also not compacting or is part of any failed compaction. 

On looking through the code, we could see that if ref goes to zero and the stable is part of the View, all reader threads will spin forever. 

On further looking through the code of streaming, we could see that if StreamTransferTask.complete is called after closeSession has been called due to error in OutgoingMessageHandler, it will double decrement the ref count of an sstable. 

This race can happen and we see through exception in logs that closeSession was triggered by OutgoingMessageHandler. 

The fix for this is very simple i think. In StreamTransferTask.abort, we can remove a file from ""files” before decrementing the ref count. This will avoid this race. "
CASSANDRA-8802,Leaked reference on windows,"The dtest counter_tests.py:TestCounters.upgrade_test is failing on Windows with the following error:
{code}
ERROR [Reference-Reaper:1] 2015-02-13 11:06:17,802 Ref.java:167 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2d0bdc9) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@669450811:[OffHeapBitSet] was not released before the reference was garbage collected
{code}

This exception is not occurring on Linux or OSX. The test is also erroring from CASSANDRA-8535.

/cc [~benedict] [~JoshuaMcKenzie]"
CASSANDRA-8793,Avoid memory allocation when searching index summary,"Currently we build a byte[] for each comparison, when we could just fill the details into a DirectByteBuffer"
CASSANDRA-8792,Improve Memory assertions,Null pointers are valid returns if a size of zero is returned. We assume a null pointer implies resource mismanagement in a few places. We also don't properly check the bounds of all of our accesses; this patch attempts to tidy up both of these things.
CASSANDRA-8758,CompressionMetadata.Writer should use a safer version of RefCountedMemory,Another small follow-on from CASSANDRA-8707 to finish off the memory and resource safety improvements.
CASSANDRA-8757,"IndexSummaryBuilder should construct itself offheap, and share memory between the result of each build() invocation",
CASSANDRA-8748,Backport memory leak fix from CASSANDRA-8707 to 2.0,There are multiple elements in CASSANDRA-8707 but the memory leak is common to Cassandra 2.0.  This ticket is to fix the memory leak specifically for 2.0.
CASSANDRA-8726,throw OOM in Memory if we fail to allocate,
CASSANDRA-8723,Cassandra 2.1.2 Memory issue - java process memory usage continuously increases until process is killed by OOM killer,"Issue:
We have an on-going issue with cassandra nodes running with continuously increasing memory until killed by OOM.


{noformat}
Jan 29 10:15:41 cass-chisel19 kernel: [24533109.783481] Out of memory: Kill process 13919 (java) score 911 or sacrifice child
Jan 29 10:15:41 cass-chisel19 kernel: [24533109.783557] Killed process 13919 (java) total-vm:18366340kB, anon-rss:6461472kB, file-rss:6684kB
{noformat}

System Profile:
cassandra version 2.1.2
system: aws c1.xlarge instance with 8 cores, 7.1G memory.

cassandra jvm:
-Xms1792M -Xmx1792M -Xmn400M -Xss256k


{noformat}
java -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.8.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1792M -Xmx1792M -Xmn400M -XX:+HeapDumpOnOutOfMemoryError -Xss256k -XX:StringTableSize=1000003 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseTLAB -XX:+CMSClassUnloadingEnabled -XX:+UseCondCardMark -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintPromotionFailure -Xloggc:/var/log/cassandra/gc-1421511249.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=48M -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote.port=7199 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -javaagent:/usr/share/java/graphite-reporter-agent-1.0-SNAPSHOT.jar=graphiteServer=metrics-a.hq.nest.com;graphitePort=2003;graphitePollInt=60 -Dlogback.configurationFile=logback.xml -Dcassandra.logdir=/var/log/cassandra -Dcassandra.storagedir= -Dcassandra-pidfile=/var/run/cassandra/cassandra.pid -cp /etc/cassandra:/usr/share/cassandra/lib/airline-0.6.jar:/usr/share/cassandra/lib/antlr-runtime-3.5.2.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang3-3.1.jar:/usr/share/cassandra/lib/commons-math3-3.2.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/usr/share/cassandra/lib/disruptor-3.0.1.jar:/usr/share/cassandra/lib/guava-16.0.jar:/usr/share/cassandra/lib/high-scale-lib-1.0.6.jar:/usr/share/cassandra/lib/jackson-core-asl-1.9.2.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/usr/share/cassandra/lib/jamm-0.2.8.jar:/usr/share/cassandra/lib/javax.inject.jar:/usr/share/cassandra/lib/jbcrypt-0.3m.jar:/usr/share/cassandra/lib/jline-1.0.jar:/usr/share/cassandra/lib/jna-4.0.0.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.9.1.jar:/usr/share/cassandra/lib/logback-classic-1.1.2.jar:/usr/share/cassandra/lib/logback-core-1.1.2.jar:/usr/share/cassandra/lib/lz4-1.2.0.jar:/usr/share/cassandra/lib/metrics-core-2.2.0.jar:/usr/share/cassandra/lib/metrics-graphite-2.2.0.jar:/usr/share/cassandra/lib/mx4j-tools.jar:/usr/share/cassandra/lib/netty-all-4.0.23.Final.jar:/usr/share/cassandra/lib/reporter-config-2.1.0.jar:/usr/share/cassandra/lib/slf4j-api-1.7.2.jar:/usr/share/cassandra/lib/snakeyaml-1.11.jar:/usr/share/cassandra/lib/snappy-java-1.0.5.2.jar:/usr/share/cassandra/lib/stream-2.5.2.jar:/usr/share/cassandra/lib/stringtemplate-4.0.2.jar:/usr/share/cassandra/lib/super-csv-2.1.0.jar:/usr/share/cassandra/lib/thrift-server-0.3.7.jar:/usr/share/cassandra/apache-cassandra-2.1.2.jar:/usr/share/cassandra/apache-cassandra-thrift-2.1.2.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/cassandra/cassandra-driver-core-2.0.5.jar:/usr/share/cassandra/netty-3.9.0.Final.jar:/usr/share/cassandra/stress.jar: -XX:HeapDumpPath=/var/lib/cassandra/java_1421511248.hprof -XX:ErrorFile=/var/lib/cassandra/hs_err_1421511248.log org.apache.cassandra.service.CassandraDaemon
{noformat}
"
CASSANDRA-8716,"""java.util.concurrent.ExecutionException: java.lang.AssertionError: Memory was freed"" when running cleanup","{code}Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.AssertionError: Memory was freed
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:188)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:234)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:272)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:1115)
        at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:2177)
        at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:259)
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:211)
        at org.apache.cassandra.io.sstable.IndexSummary.getIndex(IndexSummary.java:79)
        at org.apache.cassandra.io.sstable.IndexSummary.getKey(IndexSummary.java:84)
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:58)
        at org.apache.cassandra.io.sstable.SSTableReader.getIndexScanPosition(SSTableReader.java:602)
        at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:947)
        at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:910)
        at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:819)
        at org.apache.cassandra.db.ColumnFamilyStore.getExpectedCompactedFileSize(ColumnFamilyStore.java:1088)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:564)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:63)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:281)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:225)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        ... 3 more{code}


The error continue to happen even after scrub.
If I retry a few time, it may run successfully.
I am attaching the server log.
"
CASSANDRA-8715,"Possible Deadlock in Cqlsh in a Kerberos-enabled environment when using ""COPY ... FROM ...""","When running a COPY ... FROM ... command in a Kerberos environment, I see the number of rows processed, but eventually, Cqlsh never returns. I can verify, that all the data was copied, but the progress bar shows me the last shown info and cqlsh hangs there and never returns.

Please note that this issue did *not* occur in the exact same environment with *Cassandra 2.0.12.156*.

With the help of Tyler Hobbs, I investigated the problem a little bit further and added some debug statements at specific points. For example, in the CountdownLatch class at https://github.com/apache/cassandra/blob/a323a1a6d5f28ced1a51ba559055283f3eb356ff/pylib/cqlshlib/async_insert.py#L35-L36 I can see that the counter always stays above zero and therefore never returns (even when the data to be copied is already copied).

I've also seen that somehow when I type in one cqlsh command, there will be actually two commands. Let me give you an example:

I added a debug statement just before https://github.com/apache/cassandra/blob/d76450c7986202141f3a917b3623a4c3138c1094/bin/cqlsh#L920
{code}
cqlsh> use libdata ; 
2015-01-30 18:54:56,113 [DEBUG] root: STATEMENT: [('K_USE', 'use', (0, 3)), ('identifier', 'libdata', (4, 11)), ('endtoken', ';', (12, 13))] 
2015-01-30 18:54:56,113 [DEBUG] root: STATEMENT: [('K_USE', 'use', (0, 3)), ('identifier', 'libdata', (4, 11)), ('endtoken', ';', (12, 13))]
{code}

and saw that all commands I enter, they end up being executed twice (same goes for the COPY command).

If I can provide any other input for debugging purposes, please let me know."
CASSANDRA-8677,rpc_interface and listen_interface generate NPE on startup when specified interface doesn't exist,"This is just a buggy UI bit.

Initially the error I got was this which is redundant and not well formatted.
{noformat}
ERROR 20:12:55 Exception encountered during startup
java.lang.ExceptionInInitializerError: null
Fatal configuration error; unable to start. See log for stacktrace.
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:108) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:122) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:479) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:571) [main/:na]
java.lang.ExceptionInInitializerError: null
Fatal configuration error; unable to start. See log for stacktrace.
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:108)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:122)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:479)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:571)
Exception encountered during startup: null
Fatal configuration error; unable to start. See log for stacktrace.
ERROR 20:12:55 Exception encountered during startup
java.lang.ExceptionInInitializerError: null
Fatal configuration error; unable to start. See log for stacktrace.
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:108) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:122) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:479) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:571) [main/:na]
{noformat}

This has no description of the error that occurred. After logging the exception.

{noformat}
java.lang.NullPointerException: null
	at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:347) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:102) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:122) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:479) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:571) [main/:na]
{noformat}

Exceptions thrown in the DatabaseDescriptor should log in a useful way.

This particular error should generate a message without a stack trace since it is easily recognized."
CASSANDRA-8671,"Give compaction strategy more control over where sstables are created, including for flushing and streaming.","This would enable routing different partitions to different disks based on some user defined parameters.

My initial take on how to do this would be to make an interface from SSTableWriter, and have a table's compaction strategy do all SSTableWriter instantiation. Compaction strategies could then implement their own SSTableWriter implementations (which basically wrap one or more normal sstablewriters) for compaction, flushing, and streaming. 
"
CASSANDRA-8670,Large columns + NIO memory pooling causes excessive direct memory usage,"If you provide a large byte array to NIO and ask it to populate the byte array from a socket it will allocate a thread local byte buffer that is the size of the requested read no matter how large it is. Old IO wraps new IO for sockets (but not files) so old IO is effected as well.

Even If you are using Buffered{Input | Output}Stream you can end up passing a large byte array to NIO. The byte array read method will pass the array to NIO directly if it is larger than the internal buffer.  

Passing large cells between nodes as part of intra-cluster messaging can cause the NIO pooled buffers to quickly reach a high watermark and stay there. This ends up costing 2x the largest cell size because there is a buffer for input and output since they are different threads. This is further multiplied by the number of nodes in the cluster - 1 since each has a dedicated thread pair with separate thread locals.

Anecdotally it appears that the cost is doubled beyond that although it isn't clear why. Possibly the control connections or possibly there is some way in which multiple 

Need a workload in CI that tests the advertised limits of cells on a cluster. It would be reasonable to ratchet down the max direct memory for the test to trigger failures if a memory pooling issue is introduced. I don't think we need to test concurrently pulling in a lot of them, but it should at least work serially.

The obvious fix to address this issue would be to read in smaller chunks when dealing with large values. I think small should still be relatively large (4 megabytes) so that code that is reading from a disk can amortize the cost of a seek. It can be hard to tell what the underlying thing being read from is going to be in some of the contexts where we might choose to implement switching to reading chunks."
CASSANDRA-8668,We don't enforce offheap memory constraints; regression introduced by 7882,"Very simple mistake, not sure how it was introduced (looks like accidental delete, or possibly a half-rolled back change). Introducing a unit test to ensure basic functionality here is covered to catch such mistakes in future."
CASSANDRA-8665,Cassandra does not start with NPE in ColumnFamilyStore.removeUnfinishedCompactionLeftovers,"During a ruby driver endurance/duration test, the following error occurred:

{noformat}
/mnt/systemlogs/system.log:ERROR [main] 2015-01-17 21:18:25,780 CassandraDaemon.java:482 - Exception encountered during startup
/mnt/systemlogs/system.log-java.lang.NullPointerException: null
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:573) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:249) [main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:465) [main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:554) [main/:na]
{noformat}

Here is the system.log leading up to the error:
{noformat}
INFO  [main] 2015-01-17 21:18:24,581 ColumnFamilyStore.java:278 - Initializing system.peers
INFO  [SSTableBatchOpen:1] 2015-01-17 21:18:24,593 SSTableReader.java:392 - Opening /srv/performance/cass/data/system/peers-37f71aca7dc2383ba70672528af04d4f/system-peers-ka-169 (10533 bytes)
INFO  [SSTableBatchOpen:1] 2015-01-17 21:18:24,597 SSTableReader.java:392 - Opening /srv/performance/cass/data/system/peers-37f71aca7dc2383ba70672528af04d4f/system-peers-ka-171 (10572 bytes)
INFO  [SSTableBatchOpen:1] 2015-01-17 21:18:24,598 SSTableReader.java:392 - Opening /srv/performance/cass/data/system/peers-37f71aca7dc2383ba70672528af04d4f/system-peers-ka-170 (10581 bytes)
INFO  [main] 2015-01-17 21:18:24,609 ColumnFamilyStore.java:278 - Initializing system.local
INFO  [SSTableBatchOpen:1] 2015-01-17 21:18:24,613 SSTableReader.java:392 - Opening /srv/performance/cass/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-679 (5257 bytes)
INFO  [SSTableBatchOpen:1] 2015-01-17 21:18:24,616 SSTableReader.java:392 - Opening /srv/performance/cass/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-678 (5679 bytes)
{noformat}

Cassandra attempted to restart twice unsuccessfully (this error occurred twice) and then gave up; it seems like a corrupt Data.db file? The endurance test consists of a chaos rhino which randomly rolling restarts a node.

The only other significant error is TombstoneOverwhelmingException and is probably unrelated:
{noformat}
/mnt/systemlogs/system.log:ERROR [HintedHandoff:2] 2015-01-17 12:46:32,378 SliceQueryFilter.java:218 - Scanned over 100000 tombstones in system.hints; query aborted (see tombstone_failure_threshold)
/mnt/systemlogs/system.log:ERROR [HintedHandoff:2] 2015-01-17 12:46:32,416 CassandraDaemon.java:170 - Exception in thread Thread[HintedHandoff:2,1,main]
/mnt/systemlogs/system.log-org.apache.cassandra.db.filter.TombstoneOverwhelmingException: null
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:220) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:107) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:81) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:69) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:320) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1885) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1693) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:378) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:337) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:88) ~[main/:na]
/mnt/systemlogs/system.log-	at org.apache.cassandra.db.HintedHandOffManager$5.run(HintedHandOffManager.java:548) ~[main/:na]
/mnt/systemlogs/system.log-	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_25]
/mnt/systemlogs/system.log-	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) ~[na:1.7.0_25]
/mnt/systemlogs/system.log-	at java.util.concurrent.FutureTask.run(FutureTask.java:166) ~[na:1.7.0_25]
/mnt/systemlogs/system.log-	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) ~[na:1.7.0_25]
/mnt/systemlogs/system.log-	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) ~[na:1.7.0_25]
/mnt/systemlogs/system.log-	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]
/mnt/systemlogs/system.log-	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]
/mnt/systemlogs/system.log-	at java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]
{noformat}"
CASSANDRA-8661,We don't detect OOM when allocating a Memory object,"This affects OffHeapBitSet, and is a likely explanation for the SIGSEGV that was reported to IRC last night.

We also don't check this in NativeAllocator, so I've made a change there as well."
CASSANDRA-8631,long-test MeteredFlusherTest fails with heap OOM,"I attempted to find a spot where this test actually passes, and I was unable to do so. I tried back to 1.1 HEAD and various release tags under jdk7 and jdk6 for <= 1.2 tags. In 1.2.0 with jdk6, the test times out with java.lang.OutOfMemoryError: GC overhead limit exceeded, so it's similar.

{noformat}
    [junit] Testcase: testManyMemtables(org.apache.cassandra.db.MeteredFlusherTest):    Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
    [junit]     at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
    [junit]     at org.apache.cassandra.utils.SlabAllocator$Region.<init>(SlabAllocator.java:157)
    [junit]     at org.apache.cassandra.utils.SlabAllocator$Region.<init>(SlabAllocator.java:131)
    [junit]     at org.apache.cassandra.utils.SlabAllocator.getRegion(SlabAllocator.java:101)
    [junit]     at org.apache.cassandra.utils.SlabAllocator.allocate(SlabAllocator.java:73)
    [junit]     at org.apache.cassandra.utils.Allocator.clone(Allocator.java:30)
    [junit]     at org.apache.cassandra.db.Column.localCopy(Column.java:277)
    [junit]     at org.apache.cassandra.db.Memtable$1.apply(Memtable.java:114)
    [junit]     at org.apache.cassandra.db.Memtable$1.apply(Memtable.java:111)
    [junit]     at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:194)
    [junit]     at org.apache.cassandra.db.Memtable.resolve(Memtable.java:218)
    [junit]     at org.apache.cassandra.db.Memtable.put(Memtable.java:165)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:901)
    [junit]     at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:374)
    [junit]     at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:339)
    [junit]     at org.apache.cassandra.db.RowMutation.applyUnsafe(RowMutation.java:216)
    [junit]     at org.apache.cassandra.db.MeteredFlusherTest.testManyMemtables(MeteredFlusherTest.java:59)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.MeteredFlusherTest FAILED
{noformat}"
CASSANDRA-8603,Cut tombstone memory footprint in half for cql deletes,"As CQL does not yet support range deletes every delete from CQL results in a ""Semi-RangeTombstone"" which actually has the same start and end values - but until today they are copies. Effectively doubling the required heap memory to store the RangeTombstone.

"
CASSANDRA-8581,Null pointer in cassandra.hadoop.ColumnFamilyRecoderWriter,"When I run examples/hadoop_word_count. I find that ReducerToFilesystem is correct but when I use ReducerToCassandra, the program will call loadYaml().

The reason is that the program catch a exception at line 196 of ColumnFamilyRecoderWriter.java. 

Then it check why the exception occur, then it loadYaml to check if the disk is broken...

However, the exception is NullPointerException. because the client is not initialized.
 
So we need a check to judge whether the client is null. 
(
The exception, original code and fixed code are in the attachments.
)"
CASSANDRA-8544,Cassandra could not start with NPE in ColumnFamilyStore.removeUnfinishedCompactionLeftovers,"It happens sometimes after restarts caused by undeletable files under Windows.

{quote}
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:579)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:232)
    at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:377)
    at com.jetbrains.cassandra.service.CassandraServiceMain.start(CassandraServiceMain.java:81)
    ... 6 more
{quote}"
CASSANDRA-8541,References to non-existent/deprecated CqlPagingInputFormat in code,"On Mac 10.9.5, Java 1.7, latest cassandra trunk -
References to non-existent/deprecated CqlPagingInputFormat in code.
As per Changes.txt/7570 both CqlPagingInputFormat and CqlPagingRecordReader are removed, but lingering references in WordCount,CqlStorage.."
CASSANDRA-8530,Query on a secondary index creates huge CPU spike + unable to trace,"After upgrading cassandra from 2.0.10 to 2.1.2 we are having all kinds of issues, especially with performance.

java version ""1.7.0_65""

Table creation:
{noformat}
tweets> desc table tweets;

CREATE TABLE tweets.tweets (
    uname text,
    tweet_id bigint,
    tweet text,
    tweet_date timestamp,
    tweet_date_only text,
    uid bigint,
    PRIMARY KEY (uname, tweet_id)
) WITH CLUSTERING ORDER BY (tweet_id ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'min_threshold': '10', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.0
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.1
    AND speculative_retry = '99.0PERCENTILE';
CREATE INDEX tweets_tweet_date_only_idx ON tweets.tweets (tweet_date_only);
CREATE INDEX tweets_uid ON tweets.tweets (uid);
{noformat}

With Cassandra 2.0.10 this query:
{noformat}
select uname from tweets where uid = 636732672 limit 1;
{noformat}
did not have any issues. After upgrade, I can see the cpu spikes and load avg goes from ~1 to ~13, especially if I execute the query over and over again.

Doing ""tracing on"" does not work and just returns: 
""Statement trace did not complete within 10 seconds""

I've done:
nodetool upgradesstables
recreated indexes
"
CASSANDRA-8510,CompactionManager.submitMaximal may leak resources,"Once an AbstractCompactionTask is created it MUST be executed for its resources to be cleaned up, however here a collection of AbstractCompactionTask are constructed; if any of them yield an exception the remainder will not be executed and their resources leaked. Somebody with experience of compaction should decide how best to deal with this: either release the resources manually on an exception or always execute each task."
CASSANDRA-8504,Stack trace is erroneously logged twice,"The dtest {{replace_address_test.TestReplaceAddress.replace_active_node_test}} is failing on 3.0. The following can be seen in the log:{code}ERROR [main] 2014-12-17 15:12:33,871 CassandraDaemon.java:496 - Exception encountered during startup
java.lang.UnsupportedOperationException: Cannot replace a live node...
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:773) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:593) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:483) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:356) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:479) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:571) [main/:na]
ERROR [main] 2014-12-17 15:12:33,872 CassandraDaemon.java:584 - Exception encountered during startup
java.lang.UnsupportedOperationException: Cannot replace a live node...
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:773) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:593) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:483) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:356) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:479) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:571) [main/:na]
INFO  [StorageServiceShutdownHook] 2014-12-17 15:12:33,873 Gossiper.java:1349 - Announcing shutdown
INFO  [StorageServiceShutdownHook] 2014-12-17 15:12:35,876 MessagingService.java:708 - Waiting for messaging service to quiesce{code}

The test starts up a three node cluster, loads some data, then attempts to start a fourth node with replace_address against the IP of a live node. This is expected to fail, with one ERROR message in the log. In 3.0, we are seeing two messages. 2.1-HEAD is working as expected. Attached is the full log of the fourth node."
CASSANDRA-8490,DISTINCT queries with LIMITs or paging are incorrect when partitions are deleted,"Using paging demo code from https://github.com/PatrickCallaghan/datastax-paging-demo

The code creates and populates a table with 1000 entries and pages through them with setFetchSize set to 100. If we then delete one entry with 'cqlsh':

{noformat}
cqlsh:datastax_paging_demo> delete from datastax_paging_demo.products  where productId = 'P142'; (The specified productid is number 6 in the resultset.)
{noformat}

and run the same query (""Select * from"") again we get:

{noformat}
[com.datastax.paging.Main.main()] INFO  com.datastax.paging.Main - Paging demo took 0 secs. Total Products : 999
{noformat}

which is what we would expect.


If we then change the ""select"" statement in dao/ProductDao.java (line 70) from ""Select * from "" to ""Select DISTINCT productid from "" we get this result:

{noformat}
[com.datastax.paging.Main.main()] INFO  com.datastax.paging.Main - Paging demo took 0 secs. Total Products : 99
{noformat}

So it looks like the tombstone stops the paging behaviour. Is this a bug?

{noformat}
DEBUG [Native-Transport-Requests:788] 2014-12-16 10:09:13,431 Message.java (line 319) Received: QUERY Select DISTINCT productid from datastax_paging_demo.products, v=2
DEBUG [Native-Transport-Requests:788] 2014-12-16 10:09:13,434 AbstractQueryPager.java (line 98) Fetched 99 live rows
DEBUG [Native-Transport-Requests:788] 2014-12-16 10:09:13,434 AbstractQueryPager.java (line 115) Got result (99) smaller than page size (100), considering pager exhausted
{noformat}"
CASSANDRA-8485,Move 2.0 metered flusher to its own thread,"We are using SS.optionalTasks for the MF right now - something we most definitely should not be doing, given just how important running MF regularly is to the stability of a node. Currently a bunch of other tasks are also using SS.optionalTasks (like serializing caches).

See also: CASSANDRA-8285."
CASSANDRA-8476,RE in writeSortedContents or replaceFlushed blocks compaction threads indefinitely.,"Encountered this problem while generating some test data, incremental backup failed to create hard-links to some of the of the system files (which is done at the end of each compaction):

Example of the RE stacktrace:
{noformat}
14/12/12 15:47:47 ERROR cassandra.SchemaLoader: Fatal exception in thread Thread[FlushWriter:5,5,main]
java.lang.RuntimeException: Tried to create duplicate hard link to <path>/cassandra/data/system/IndexInfo/backups/system-IndexInfo-jb-1-Index.db
	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:75)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1222)
	at org.apache.cassandra.db.DataTracker.maybeIncrementallyBackup(DataTracker.java:189)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:166)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:231)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1141)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:343)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14
{noformat}

jstack shows that CompactionExecutor threads are now blocked waiting on the flush future which will actually never decrement a latch.

{noformat}
""CompactionExecutor:125"" daemon prio=5 tid=0x00007fb3a10da800 nid=0x13c43 waiting on condition [0x000000012a900000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000071b669088> (a java.util.concurrent.FutureTask)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:425)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:409)
	at org.apache.cassandra.db.SystemKeyspace.forceBlockingFlush(SystemKeyspace.java:457)
	at org.apache.cassandra.db.SystemKeyspace.finishCompaction(SystemKeyspace.java:203)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:225)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""CompactionExecutor:124"" daemon prio=5 tid=0x00007fb35cc09800 nid=0x13a2b waiting on condition [0x000000012934f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007ce4bf918> (a java.util.concurrent.FutureTask)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:425)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:409)
	at org.apache.cassandra.db.SystemKeyspace.forceBlockingFlush(SystemKeyspace.java:457)
	at org.apache.cassandra.db.SystemKeyspace.finishCompaction(SystemKeyspace.java:203)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:225)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}"
CASSANDRA-8457,nio MessagingService,"Thread-per-peer (actually two each incoming and outbound) is a big contributor to context switching, especially for larger clusters.  Let's look at switching to nio, possibly via Netty."
CASSANDRA-8451,NPE when writetime() or ttl() are nested inside function call,"When the {{writetime()}} or {{ttl()}} functions are nested inside another function, the query will result in a NullPointerException:

{noformat}
cqlsh> select bigintAsBlob(writetime(durable_writes)) from system.schema_keyspaces;
<ErrorMessage code=0000 [Server error] message=""java.lang.NullPointerException"">
{noformat}

Here's the stacktrace:

{noformat}
ERROR 00:14:56 Unexpected exception during request
java.lang.NullPointerException: null
	at org.apache.cassandra.cql3.statements.Selection$WritetimeOrTTLSelector.compute(Selection.java:538) ~[main/:na]
	at org.apache.cassandra.cql3.statements.Selection$FunctionSelector.compute(Selection.java:462) ~[main/:na]
	at org.apache.cassandra.cql3.statements.Selection$SelectionWithProcessing.handleRow(Selection.java:434) ~[main/:na]
	at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.newRow(Selection.java:331) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.processColumnFamily(SelectStatement.java:1243) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1177) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:274) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:213) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:260) ~[main/:na]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:119) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_40]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
{noformat}

Basically, it's failing to collect timestamps (or TTLs) when the function is wrapped.  A similar query that additionally selects {{writetime()}} directly will succeed (because the timestamps will get collected):

{noformat}
cqlsh> select bigintAsBlob(writetime(durable_writes)), writetime(durable_writes) from system.schema_keyspaces ;

 bigintAsBlob(writetime(durable_writes)) | writetime(durable_writes)
-----------------------------------------+---------------------------
                      0x000509d1865c67f9 |          1418170390571001
                      0x0000000000000000 |                         0
{noformat}"
CASSANDRA-8419,NPE in SelectStatement,"The dtest {{cql_tests.py:TestCQL.empty_in_test}} is failing in trunk with a Null Pointer Exception. The stack trace is:
{code}ERROR [SharedPool-Worker-1] 2014-12-03 16:24:16,274 ErrorMessage.java:243 - Unexpected exception
during request
java.lang.NullPointerException: null
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:213) ~[guava-16.0
.jar:na]
        at com.google.common.collect.Lists$TransformingSequentialList.<init>(Lists.java:525) ~[gu
ava-16.0.jar:na]
        at com.google.common.collect.Lists.transform(Lists.java:508) ~[guava-16.0.jar:na]
        at org.apache.cassandra.db.composites.Composites.toByteBuffers(Composites.java:45) ~[main
/:na]
        at org.apache.cassandra.cql3.restrictions.SingleColumnPrimaryKeyRestrictions.values(Singl
eColumnPrimaryKeyRestrictions.java:257) ~[main/:na]
        at org.apache.cassandra.cql3.restrictions.StatementRestrictions.getPartitionKeys(StatementRestrictions.java:362) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:296) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.getPageableCommand(SelectStatement.java:205) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:165) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:72) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:239) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:261) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:118) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_67]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]{code}

The error occurred while executing {{SELECT v FROM test_compact WHERE k1 IN ()}}."
CASSANDRA-8410,Select with many IN values on clustering columns can result in a StackOverflowError,"When executing a SELECT statement with an IN clause on the clustering columns, a StackOverflowError can occur if the memtable doesn't contain any of the requested slices.  In 2.0, this happens with the following stack trace:

{noformat}

ERROR [ReadStage:23] 2014-12-02 14:53:11,077 CassandraDaemon.java (line 199) Exception in thread Thread[ReadStage:23,5,main]
java.lang.StackOverflowError
at org.apache.cassandra.db.marshal.Int32Type.compare(Int32Type.java:52)
at org.apache.cassandra.db.marshal.Int32Type.compare(Int32Type.java:28)
at org.apache.cassandra.db.marshal.AbstractType.compareCollectionMembers(AbstractType.java:279)
at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:64)
at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:36)
at edu.stanford.ppl.concurrent.SnapTreeMap$1.compareTo(SnapTreeMap.java:538)
at edu.stanford.ppl.concurrent.SnapTreeMap.boundedMax(SnapTreeMap.java:905)
at edu.stanford.ppl.concurrent.SnapTreeMap.boundedExtreme(SnapTreeMap.java:833)
at edu.stanford.ppl.concurrent.SnapTreeMap.access$1000(SnapTreeMap.java:90)
at edu.stanford.ppl.concurrent.SnapTreeMap$AbstractIter.<init>(SnapTreeMap.java:2028)
at edu.stanford.ppl.concurrent.SnapTreeMap$EntryIter.<init>(SnapTreeMap.java:1951)
at edu.stanford.ppl.concurrent.SnapTreeMap$EntryIter.<init>(SnapTreeMap.java:1940)
at edu.stanford.ppl.concurrent.SnapTreeMap$SubMap$EntrySubSet.iterator(SnapTreeMap.java:2462)
at java.util.AbstractMap$2$1.<init>(AbstractMap.java:378)
at java.util.AbstractMap$2.iterator(AbstractMap.java:377)
at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:154)
at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:162)
at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:162)
{noformat}

In 2.1, there's a similar error, but it occurs in AtomicBTreeColumns."
CASSANDRA-8409,Node generating a huge number of tiny sstable_activity flushes,"On one of my nodes, I’m seeing hundreds per second of “INFO  21:28:05 Enqueuing flush of sstable_activity: 0 (0%) on-heap, 33 (0%) off-heap”. tpstats shows a steadily climbing # of pending MemtableFlushWriter/MemtablePostFlush until the node OOMs. When the flushes actually happen the sstable written is invariably 121 bytes. I’m writing pretty aggressively to one of my user tables (sev.mdb_group_pit), but that table's flushing behavior seems reasonable.

tpstats:
{quote}
frew@hostname:~/s_dist/apache-cassandra-2.1.0$ bin/nodetool -h hostname tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
MutationStage                   128      4429          36810         0                 0
ReadStage                         0         0           1205         0                 0
RequestResponseStage              0         0          24910         0                 0
ReadRepairStage                   0         0             26         0                 0
CounterMutationStage              0         0              0         0                 0
MiscStage                         0         0              0         0                 0
HintedHandoff                     2         2              9         0                 0
GossipStage                       0         0           5157         0                 0
CacheCleanupExecutor              0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
CompactionExecutor                4        28            429         0                 0
ValidationExecutor                0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
AntiEntropyStage                  0         0              0         0                 0
PendingRangeCalculator            0         0             11         0                 0
MemtableFlushWriter               8     38644           8987         0                 0
MemtablePostFlush                 1     38940           8735         0                 0
MemtableReclaimMemory             0         0           8987         0                 0

Message type           Dropped
READ                         0
RANGE_SLICE                  0
_TRACE                       0
MUTATION                 10457
COUNTER_MUTATION             0
BINARY                       0
REQUEST_RESPONSE             0
PAGED_RANGE                  0
READ_REPAIR                208
{quote}

I've attached one of the produced sstables."
CASSANDRA-8403,limit disregarded when paging with IN clause under certain conditions,"This issue was originally reported on the python-driver userlist and confirmed by [~aholmber]

When:

page_size < limit < data size,

the limit value is disregarded and all rows are paged back.

to repro:
create a table and populate it with two partitions

CREATE TABLE paging_test ( id int, value text, PRIMARY KEY (id, value) )

Add data: in one partition create 10 rows, an in a second partition create 20 rows

perform a query with page_size of 10 and a LIMIT of 20, like so:

SELECT * FROM paging_test where id in (1,2) LIMIT 20;

The limit is disregarded and three pages of 10 records each will be returned."
CASSANDRA-8383,Memtable flush may expire records from the commit log that are in a later memtable,"This is a pretty obvious bug with any care of thought, so not sure how I managed to introduce it. We use OpOrder to ensure all writes to a memtable have finished before flushing, however we also use this OpOrder to direct writes to the correct memtable. However this is insufficient, since the OpOrder is only a partial order; an operation from the ""future"" (i.e. for the next memtable) could still interleave with the ""past"" operations in such a way that they grab a CL entry inbetween the ""past"" operations. Since we simply take the max ReplayPosition of those in the past, this would mean any interleaved future operations would be expired even though they haven't been persisted to disk.
"
CASSANDRA-8380,Only record trace if query exceeds latency threshold.,"Probabilistic trace isn't very useful because you're typically trying to only find the badly performing queries which may only be .01% of requests. I would like to enable probabilistic trace on a sample of queries, but then only record the trace if the request exceeded a time threshold. This would allow us to more easily isolate performance problems."
CASSANDRA-8355,NPE when passing wrong argument in ALTER TABLE statement,"When I tried to change the caching strategy of a table, I provided a wrong argument {{'rows_per_partition' : ALL}} with unquoted ALL. Cassandra returned a SyntaxError, which is good, but it seems it was because of a NullPointerException.

*Howto*
{code}
CREATE TABLE foo (k int primary key);
ALTER TABLE foo WITH caching = {'keys' : 'all', 'rows_per_partition' : ALL};
{code}

*Output*
{code}
<ErrorMessage code=2000 [Syntax error in CQL query] message=""Failed parsing statement: [ALTER TABLE foo WITH caching = {'keys' : 'all', 'rows_per_partition' : ALL};] reason: NullPointerException null"">
{code}

"
CASSANDRA-8332,Null pointer after droping keyspace,"After dropping keyspace, sometimes I see this in logs:
{code}
ERROR 03:40:29 Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError: null
	at org.apache.cassandra.io.compress.CompressionParameters.setLiveMetadata(CompressionParameters.java:108) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableReader.getCompressionMetadata(SSTableReader.java:1142) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1896) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:68) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1681) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1693) ~[main/:na]
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners(LeveledCompactionStrategy.java:181) ~[main/:na]
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getScanners(WrappingCompactionStrategy.java:320) ~[main/:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:340) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:151) ~[main/:na]
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:75) ~[main/:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:233) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_71]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_71]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_71]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_71]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_71]
{code}
Minor issue since doesn't really affect anything, but the error makes it look like somethings wrong.  Seen on 2.1 branch (1b21aef8152d96a180e75f2fcc5afad9ded6c595), not sure how far back (may be post 2.1.2)."
CASSANDRA-8276,Duplicate values in an IN restriction on the partition key column can break paging,"We had an issue ([JAVA-515|https://datastax-oss.atlassian.net/browse/JAVA-515]) in the java-driver when the number of parameters in a statement is greater than the supported limit (65k).

I added a limit-test to verify that prepared statements with 65535 parameters were accepted by the driver, but ran into an issue on the Cassandra side.

Basically, the test runs forever, because the driver receives an inconsistent answer from Cassandra.  When we prepare the statement, C* answers that it is correctly prepared, however when we try to execute it, we receive a {{UNPREPARED}} answer.

[Here is the code|https://github.com/datastax/java-driver/blob/JAVA-515/driver-core/src/test/java/com/datastax/driver/core/PreparedStatementTest.java#L448] to reproduce the issue.
"
CASSANDRA-8259,Add column family name when reporting OutOfMemory errors,"When we get a Thrift error like this which causes a server crash:
{noformat}
ERROR [Thrift:33] 2014-11-05 17:36:07,486 CassandraDaemon.java (line 196)
Exception in thread Thread[Thrift:33,5,main]
java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2271)
        at java.io.ByteArrayOutputStream.grow
(ByteArrayOutputStream.java:113)
        at java.io.ByteArrayOutputStream.ensureCapacity
(ByteArrayOutputStream.java:93)
        at java.io.ByteArrayOutputStream.write
(ByteArrayOutputStream.java:140)
        at org.apache.thrift.transport.TFramedTransport.write
(TFramedTransport.java:146)
        at org.apache.thrift.protocol.TBinaryProtocol.writeBinary
(TBinaryProtocol.java:183)
        at org.apache.cassandra.thrift.Column$ColumnStandardScheme.write
(Column.java:678)
        at org.apache.cassandra.thrift.Column$ColumnStandardScheme.write
(Column.java:611)
        at org.apache.cassandra.thrift.Column.write(Column.java:538)
        at org.apache.cassandra.thrift.ColumnOrSuperColumn
$ColumnOrSuperColumnStandardScheme.write(ColumnOrSuperColumn.java:673)
        at org.apache.cassandra.thrift.ColumnOrSuperColumn
$ColumnOrSuperColumnStandardScheme.write(ColumnOrSuperColumn.java:607)
        at org.apache.cassandra.thrift.ColumnOrSuperColumn.write
(ColumnOrSuperColumn.java:517)
        at org.apache.cassandra.thrift.Cassandra$multiget_slice_result
$multiget_slice_resultStandardScheme.write(Cassandra.java:14559)
        at org.apache.cassandra.thrift.Cassandra$multiget_slice_result
$multiget_slice_resultStandardScheme.write(Cassandra.java:14463)
        at org.apache.cassandra.thrift.Cassandra
$multiget_slice_result.write(Cassandra.java:14393)
        at org.apache.thrift.ProcessFunction.process
(ProcessFunction.java:53)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer
$WorkerProcess.run(CustomTThreadPoolServer.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker
(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run
(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
 INFO [StorageServiceShutdownHook] 2014-11-05 17:36:07,488
ThriftServer.java (line 141) Stop listening to thrift clients
{noformat}

we have no clue as to which column family was being queried. That makes it extremely difficult to troubleshoot which query in a complex code base caused this error.

We have multiple servers and they all throw a NoAvailableHostException in Astyanax at the same time, all in different parts of the code...so figuring out the root cause is an exercise in frustration that takes many hours.

At least listing the column family in this message would save us COUNTLESS hours of troubleshooting.

We're on 2.0.8, JDK 1.7, RHEL 6"
CASSANDRA-8248,Possible memory leak,"Sometimes during repair cassandra starts to consume more memory than expected.

Total amount of data on node is about 20GB.
Size of the data directory is 66GC because of snapshots.

Top reports: 
{noformat}
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
15724 loadbase  20   0  493g  55g  44g S   28 44.2   4043:24 java
{noformat}

At the /proc/15724/maps there are a lot of deleted file maps
{quote}
7f63a6102000-7f63a6332000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a6332000-7f63a6562000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a6562000-7f63a6792000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a6792000-7f63a69c2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a69c2000-7f63a6bf2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a6bf2000-7f63a6e22000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a6e22000-7f63a7052000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a7052000-7f63a7282000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a7282000-7f63a74b2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a74b2000-7f63a76e2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a76e2000-7f63a7912000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a7912000-7f63a7b42000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a7b42000-7f63a7d72000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a7d72000-7f63a7fa2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a7fa2000-7f63a81d2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a81d2000-7f63a8402000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a8402000-7f63a8622000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a8622000-7f63a8842000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a8842000-7f63a8a62000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a8a62000-7f63a8c82000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a8c82000-7f63a8ea2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a8ea2000-7f63a90c2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
7f63a90c2000-7f63a92e2000 r--s 00000000 08:21 9442763                    /ssd/cassandra/data/iss/feedback_history-d32bc7e048c011e49b989bc3e8a5a440/iss-feedback_history-tmplink-ka-328671-Index.db (deleted)
{quote}

{quote}
$ sudo grep deleted /proc/15724/maps | wc -l
640118
{quote}

{quote}
$ sudo grep -v deleted /proc/15724/maps | wc -l
303340
{quote}"
CASSANDRA-8238,NPE in SizeTieredCompactionStrategy.filterColdSSTables,"{noformat}
ERROR [CompactionExecutor:15] 2014-10-31 15:28:32,318 CassandraDaemon.java:153 - Exception in thread Thread[CompactionExecutor:15,1,main]
    java.lang.NullPointerException: null
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.filterColdSSTables(SizeTieredCompactionStrategy.java:181) ~[apache-cassandra-2.1.1.jar:2.1.1]
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:83) ~[apache-cassandra-2.1.1.jar:2.1.1]
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:267) ~[apache-cassandra-2.1.1.jar:2.1.1]
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:226) ~[apache-cassandra-2.1.1.jar:2.1.1]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_72]
    at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_72]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_72]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_72]
    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_72]
{noformat}"
CASSANDRA-8210,"""java.lang.AssertionError: Memory was freed"" exception in CompactionExecutor","I have just got this problem on multiple nodes. Cassandra 2.0.10 (DSE 4.5.2). After looking through the history I have found that it was actually happening on all nodes since the start of large compaction process (I've loaded tons of data in the system and then turned off all load to let it compact the data).

{code}
ERROR [CompactionExecutor:1196] 2014-10-28 17:14:50,124 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:1196,1,main]
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:259)
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:211)
        at org.apache.cassandra.io.sstable.IndexSummary.getIndex(IndexSummary.java:79)
        at org.apache.cassandra.io.sstable.IndexSummary.getKey(IndexSummary.java:84)
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:58)
        at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:692)
        at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:663)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:328)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:354)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getMaximalTask(LeveledCompactionStrategy.java:125)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:113)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:192)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}"
CASSANDRA-8192,Better error logging on corrupt compressed SSTables: currently AssertionError in Memory.java,"Since update of 1 of 12 nodes from 2.1.0-rel to 2.1.1-rel Exception during start up.
{panel:title=system.log}
ERROR [SSTableBatchOpen:1] 2014-10-27 09:44:00,079 CassandraDaemon.java:153 - Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.io.util.Memory.size(Memory.java:307) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:135) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.compress.CompressionMetadata.create(CompressionMetadata.java:83) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.util.CompressedSegmentedFile$Builder.metadata(CompressedSegmentedFile.java:50) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Builder.complete(CompressedPoolingSegmentedFile.java:48) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:766) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:725) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:402) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:302) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:438) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_55]
	at java.lang.Thread.run(Unknown Source) [na:1.7.0_55]
{panel}
In the attached log you can still see as well CASSANDRA-8069 and CASSANDRA-6283."
CASSANDRA-8188,don't block SocketThread for MessagingService,"We have two datacenters A and B.
The node in A cannot handshake version with nodes in B, logs in A as follow:
{noformat}	
	INFO [HANDSHAKE-/B] 2014-10-24 04:29:49,075 OutboundTcpConnection.java (line 395) Cannot handshake version with B
    TRACE [WRITE-/B] 2014-10-24 11:02:49,044 OutboundTcpConnection.java (line 368) unable to connect to /B
		java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method)
        at sun.nio.ch.Net.connect(Net.java:364)
        at sun.nio.ch.Net.connect(Net.java:356)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:623)
        at java.nio.channels.SocketChannel.open(SocketChannel.java:184)
        at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:134)
        at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:119)
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:299)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:150)
{noformat}
    
The jstack output of nodes in B shows it blocks in inputStream.readInt resulting in SocketThread not accept socket any more, logs as follow:
{noformat}
	   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        - locked <0x00000007963747e8> (a java.lang.Object)
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:203)
        - locked <0x0000000796374848> (a java.lang.Object)
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
        - locked <0x00000007a5c7ca88> (a sun.nio.ch.SocketAdaptor$SocketInputStream)
        at java.io.InputStream.read(InputStream.java:101)
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:81)
        - locked <0x00000007a5c7ca88> (a sun.nio.ch.SocketAdaptor$SocketInputStream)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:879)
{noformat}
       
In nodes of B tcpdump shows retransmission of SYN,ACK during the tcp three-way handshake phase because tcp implementation drops the last ack when the backlog queue is full.

In nodes of B ss -tl shows ""Recv-Q 51 Send-Q 50"".
        
In nodes of B netstat -s shows “SYNs to LISTEN sockets dropped” and “times the listen queue of a socket overflowed” are both increasing.

This patch sets read timeout to 2 * OutboundTcpConnection.WAIT_FOR_VERSION_MAX_TIME for the accepted socket."
CASSANDRA-8176,Intermittent NPE from RecoveryManagerTest RecoverPIT unit test,"{noformat}
    [junit] Testsuite: org.apache.cassandra.db.RecoveryManagerTest
    [junit] Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.654 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] WARN  16:40:38 No host ID found, created 2cbd54a8-79a5-40e0-a8e6-c8bf2c575877 (Note: This should happen exactly once per node).
    [junit] WARN  16:40:38 No host ID found, created 2cbd54a8-79a5-40e0-a8e6-c8bf2c575877 (Note: This should happen exactly once per node).
    [junit] WARN  16:40:38 Encountered bad header at position 16 of commit log /home/mshuler/git/cassandra/build/test/cassandra/commitlog:0/CommitLog-4-1414082433807.log, with invalid CRC. The end of segment marker should be zero.
    [junit] WARN  16:40:38 Encountered bad header at position 16 of commit log /home/mshuler/git/cassandra/build/test/cassandra/commitlog:0/CommitLog-4-1414082433807.log, with invalid CRC. The end of segment marker should be zero.
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRecoverPIT(org.apache.cassandra.db.RecoveryManagerTest):      Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at org.apache.cassandra.db.RecoveryManagerTest.testRecoverPIT(RecoveryManagerTest.java:129)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RecoveryManagerTest FAILED
{noformat}

Test fails roughly 20-25% of CI runs. Several 10x and 25x bisections for 2.1 {{git bisect start cassandra-2.1 f03e505}} resulted in {noformat}first bad commit: [1394b128c65ef1ad59f765e9c9c5058cac04ca69]{noformat} which is CASSANDRA-6904.

That patch went to 2.0 and I still need to dig there to see if we're getting the same error, but I've attached the unit test failure system.log from 2.1."
CASSANDRA-8164,OOM due to slow memory meter,"Memory meter holds strong reference to memtable while it iterates over memtable cells. Because meter is not fast, it prevents memtable from being GCed after it has been flushed for some time.

If write rate is fast enough, this makes node OOM.

Fixed this by aborting metering if table becomes not active in datatracker, i.e. flushing or flushed."
CASSANDRA-8162,Log client address in query trace,"With probabilistic tracing, it can be helpful to log the source IP for queries."
CASSANDRA-8159,NPE in SSTableReader,"Log file contained a lot of following exceptions:

{quote}
WARN  [CompactionExecutor:15674] 2014-10-21 20:57:56,838 OutputHandler.java:52 - Out of order row detected (DecoratedKey(8937955371032053430, 39352e3130382e3234322e32302d6765744d656d427566666572734d62) found
 after DecoratedKey(9186481584950194146, 800100010000000c62617463685f6d7574617465000056640d00010b0d0000000100))
{quote}

I tried to scrub sstables by nodetool and got:

{quote}
ERROR [CompactionExecutor:15674] 2014-10-21 20:57:57,229 CassandraDaemon.java:166 - Exception in thread Thread[CompactionExecutor:15674,1,RMI Runtime]
java.lang.NullPointerException: null
        at org.apache.cassandra.io.sstable.SSTableReader.cloneWithNewStart(SSTableReader.java:942) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.moveStarts(SSTableRewriter.java:238) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewriter.java:318) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:257) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:592) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:100) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$3.execute(CompactionManager.java:315) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:270) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
{quote}

sstablescrub successfully fixed sstables."
CASSANDRA-8152,Cassandra crashes with Native memory allocation failure,"On a 6 node Cassandra (datastax-community-2.1) cluster running on EC2 (i2.xlarge) instances, Jvm hosting the cassandra service randomly crashes with following error.

{code}
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 12288 bytes for committing reserved memory.
# Possible reasons:
#   The system is out of physical RAM or swap space
#   In 32 bit mode, the process size limit was hit
# Possible solutions:
#   Reduce memory load on the system
#   Increase physical memory or swap space
#   Check if swap backing store is full
#   Use 64 bit Java on a 64 bit OS
#   Decrease Java heap size (-Xmx/-Xms)
#   Decrease number of Java threads
#   Decrease Java thread stack sizes (-Xss)
#   Set larger code cache with -XX:ReservedCodeCacheSize=
# This output file may be truncated or incomplete.
#
#  Out of Memory Error (os_linux.cpp:2747), pid=26159, tid=140305605682944
#
# JRE version: Java(TM) SE Runtime Environment (7.0_60-b19) (build 1.7.0_60-b19)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.60-b09 mixed mode linux-amd64 compressed oops)
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#

---------------  T H R E A D  ---------------

Current thread (0x0000000008341000):  JavaThread ""MemtableFlushWriter:2055"" daemon [_thread_new, id=23336, stack(0x00007f9b71c56000,0x00007f9b71c97000)]

Stack: [0x00007f9b71c56000,0x00007f9b71c97000],  sp=0x00007f9b71c95820,  free space=254k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0x99e7ca]  VMError::report_and_die()+0x2ea
V  [libjvm.so+0x496fbb]  report_vm_out_of_memory(char const*, int, unsigned long, char const*)+0x9b
V  [libjvm.so+0x81d81e]  os::Linux::commit_memory_impl(char*, unsigned long, bool)+0xfe
V  [libjvm.so+0x81d8dc]  os::pd_commit_memory(char*, unsigned long, bool)+0xc
V  [libjvm.so+0x81565a]  os::commit_memory(char*, unsigned long, bool)+0x2a
V  [libjvm.so+0x81bdcd]  os::pd_create_stack_guard_pages(char*, unsigned long)+0x6d
V  [libjvm.so+0x9522de]  JavaThread::create_stack_guard_pages()+0x5e
V  [libjvm.so+0x958c24]  JavaThread::run()+0x34
V  [libjvm.so+0x81f7f8]  java_start(Thread*)+0x108
{code}

Changes in cassandra-env.sh settings

{code}
MAX_HEAP_SIZE=""8G""
HEAP_NEWSIZE=""800M""
JVM_OPTS=""$JVM_OPTS -XX:TargetSurvivorRatio=50""
JVM_OPTS=""$JVM_OPTS -XX:+AggressiveOpts""
JVM_OPTS=""$JVM_OPTS -XX:+UseLargePages""
{code}

Writes are about 10K-15K/sec and there are very few reads. Cassandra 2.0.9 with same settings never crashed. JVM crash logs are attached from two machines."
CASSANDRA-8137,Prepared statement size overflow error,"When using C* 2.1.0 and Ruby-driver master, I get the following error when running the Ruby duration test (which prepares a lot of statements, in many threads):

{noformat}
Prepared statement of size 4451848 bytes is larger than allowed maximum of 2027520 bytes.
Prepared statement of size 4434568 bytes is larger than allowed maximum of 2027520 bytes.
{noformat}

They usually occur in batches of 1, but sometimes in multiples as seen above.  It happens occasionally, around 20% of the time when running the code.  Unfortunately I don't have a stacktrace as the error isn't recorded in the system log. 
This is my schema, and the offending prepare statement:

{noformat}
@session.execute(""CREATE TABLE duration_test.ints (
                        key INT,
                        copy INT,
                        value INT,
                        PRIMARY KEY (key, copy))""
)
{noformat}

{noformat}
select = @session.prepare(""SELECT * FROM ints WHERE key=?"")
{noformat}

Now, I notice that if I explicitly specify the keyspace in the prepare, I don't get the error."
CASSANDRA-8108,Errors paging DISTINCT queries on static columns,"{noformat}
java.lang.ClassCastException: org.apache.cassandra.db.composites.Composites$EmptyComposite cannot be cast to org.apache.cassandra.db.composites.CellName
	at org.apache.cassandra.db.composites.AbstractCellNameType.cellFromByteBuffer(AbstractCellNameType.java:170) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.service.pager.SliceQueryPager.<init>(SliceQueryPager.java:57) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.service.pager.MultiPartitionPager.makePager(MultiPartitionPager.java:84) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.service.pager.MultiPartitionPager.<init>(MultiPartitionPager.java:68) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.service.pager.QueryPagers.pager(QueryPagers.java:101) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.service.pager.QueryPagers.pager(QueryPagers.java:125) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:215) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:60) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:187) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:413) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:133) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:422) [apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:318) [apache-cassandra-2.1.0.jar:2.1.0]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:103) [netty-all-4.0.20.Final.jar:4.0.20.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:332) [netty-all-4.0.20.Final.jar:4.0.20.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:31) [netty-all-4.0.20.Final.jar:4.0.20.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:323) [netty-all-4.0.20.Final.jar:4.0.20.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_51]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:163) [apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:103) [apache-cassandra-2.1.0.jar:2.1.0]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
{noformat}"
CASSANDRA-8105,NPE for null embedded UDT inside set,"An NPE is thrown parsing an INSERT statement when a embedded UDT inside another UDT is set to null inside a set.
This sounds very convoluted, but the examples below will hopefully make it clear...

With the following definitions:
CREATE TYPE ut1 (a int, b int);
CREATE TYPE ut2 (j frozen<ut1>, k int);
CREATE TYPE ut3 (i int, j frozen<ut1>);
CREATE TABLE tab1 (x int PRIMARY KEY, y set<frozen<ut2>>);
CREATE TABLE tab2 (x int PRIMARY KEY, y list<frozen<ut2>>);
CREATE TABLE tab3 (x int PRIMARY KEY, y set<frozen<ut3>>);

This query throws a NullPointerException:
INSERT INTO tab1 (x, y) VALUES (1, { { k: 1 } });

These however doesn't:
INSERT INTO tab2 (x, y) VALUES (1, [ { k: 1 } ]);
INSERT INTO tab3 (x, y) VALUES (1, { { i: 1 } });

So, the bug seems to be triggered only when the UDT is in a set, lists are fine. If the embedded UDT is after the value specified in the query, the bug  doesn't seem to trigger."
CASSANDRA-8071,OutOfMemoryError in CompactionExecutor,"Since update of 2 of 12 nodes from 2.0.10 to 2.1-release Exception during operation.
{panel:title=system.log}
ERROR [CompactionExecutor:18] 2014-10-02 09:26:44,431 CassandraDaemon.java:166 - Exception in thread Thread[CompactionExecutor:18,1,main]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:79) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1874) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1660) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1672) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:272) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:278) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:148) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:468) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_67]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_67]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_67]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_67]
	at java.lang.Thread.run(Unknown Source) [na:1.7.0_67]
{panel}"
CASSANDRA-8070,OutOfMemoryError in OptionalTasks,"Since update of 2 of 12 nodes from 2.0.10 to 2.1-release Exception during operation.
{panel:title=system.log}
ERROR [OptionalTasks:1] 2014-10-02 09:26:42,821 CassandraDaemon.java:166 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.OutOfMemoryError: Java heap space
	at com.yammer.metrics.stats.Snapshot.<init>(Snapshot.java:30) ~[metrics-core-2.2.0.jar:na]
	at com.yammer.metrics.stats.ExponentiallyDecayingSample.getSnapshot(ExponentiallyDecayingSample.java:131) ~[metrics-core-2.2.0.jar:na]
	at com.yammer.metrics.core.Histogram.getSnapshot(Histogram.java:180) ~[metrics-core-2.2.0.jar:na]
	at com.yammer.metrics.core.Timer.getSnapshot(Timer.java:183) ~[metrics-core-2.2.0.jar:na]
	at org.apache.cassandra.db.ColumnFamilyStore$2.run(ColumnFamilyStore.java:340) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75) ~[apache-cassandra-2.1.0.jar:2.1.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [na:1.7.0_67]
	at java.util.concurrent.FutureTask.runAndReset(Unknown Source) [na:1.7.0_67]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source) [na:1.7.0_67]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) [na:1.7.0_67]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_67]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_67]
	at java.lang.Thread.run(Unknown Source) [na:1.7.0_67]
{panel}"
CASSANDRA-8057,Record the real messaging version in all cases in OutboundTcpConnection,"Piotr's quote, from CASSANDRA-6700 description:

bq. IncomingTcpConnection#handleModernVersion sets version to min(my version, version of the peer). This messes up schema pull/push.

Seems like we've missed a similar, yet related, case in OutboundTcpConnection#connect():

{code}
if (targetVersion < maxTargetVersion && targetVersion < MessagingService.current_version)
{
    logger.trace(""Detected higher max version {} (using {}); will reconnect when queued messages are done"",
            maxTargetVersion, targetVersion);
    MessagingService.instance().setVersion(poolReference.endPoint(), Math.min(MessagingService.current_version, maxTargetVersion));
    softCloseSocket();
}
{code}

Should really set the true version (maxTargetVersion), since MessagingService#getVersion() will return the min for us, anyway:

{code}
public int getVersion(InetAddress endpoint)
{
    Integer v = versions.get(endpoint);
    if (v == null)
    {
        // we don't know the version. assume current. we'll know soon enough if that was incorrect.
        logger.trace(""Assuming current protocol version for {}"", endpoint);
        return MessagingService.current_version;
    }
    else
        return Math.min(v, MessagingService.current_version);
}
{code}

But we need the true version for schema exchange decisions."
CASSANDRA-8039,dh_pysupport -> dh_python2 debian packaging,"Noticed that dh_pysupport is deprecated in Jessie when I was building the deb:
{noformat}
dh_pysupport: This program is deprecated, you should use dh_python2 instead. Migration guide: http://deb.li/dhs2p
{noformat}"
CASSANDRA-8028,Unable to compute when histogram overflowed,"It seems like with 2.1.0 histograms can't be computed most of the times:

$ nodetool cfhistograms draios top_files_by_agent1
nodetool: Unable to compute when histogram overflowed
See 'nodetool help' or 'nodetool help <command>'.

I can probably find a way to attach a .cql script to reproduce it, but I suspect it must be obvious to replicate it as it happens on more than 50% of my column families."
CASSANDRA-8014,NPE in Message.java line 324,"We received this when a server was rebooting and attempted to shut Cassandra down while it was still quite busy. While it's normal for us to have a handful of the RejectedExecution exceptions on a sudden shutdown like this these NPEs in Message.java are new.

The attached file include the logs from ""StorageServiceShutdownHook"" to the ""Logging initialized"" after the server restarts and Cassandra comes back up.


{code}ERROR [pool-10-thread-2] 2014-09-29 08:33:44,055 Message.java (line 324) Unexpected throwable while invoking!
java.lang.NullPointerException
        at com.thinkaurelius.thrift.util.mem.Buffer.size(Buffer.java:83)
        at com.thinkaurelius.thrift.util.mem.FastMemoryOutputTransport.expand(FastMemoryOutputTransport.java:84)
        at com.thinkaurelius.thrift.util.mem.FastMemoryOutputTransport.write(FastMemoryOutputTransport.java:167)
        at org.apache.thrift.transport.TFramedTransport.flush(TFramedTransport.java:156)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:55)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at com.thinkaurelius.thrift.Message.invoke(Message.java:314)
        at com.thinkaurelius.thrift.Message$Invocation.execute(Message.java:90)
        at com.thinkaurelius.thrift.TDisruptorServer$InvocationHandler.onEvent(TDisruptorServer.java:638)
        at com.thinkaurelius.thrift.TDisruptorServer$InvocationHandler.onEvent(TDisruptorServer.java:632)
        at com.lmax.disruptor.WorkProcessor.run(WorkProcessor.java:112)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}"
CASSANDRA-7992,Arithmetic overflow sorting commit log segments on replay,"When replaying a lot of commit logs aged several days commit log segments are sorted incorrectly due to arith overflow in CommitLogSegmentFileComparator
"
CASSANDRA-7952,DataStax Agent Null Pointer Exception,"I've got a Null Pointer Exception in my DataStax OpsCenter Agent log, and it's not reporting in to the OpsCenter. Here is the log
{code}
 INFO [StompConnection receiver] 2014-09-17 13:01:15,992 New JMX connection (127.0.0.1:7199)
 INFO [Jetty] 2014-09-17 13:01:16,019 Jetty server started
 INFO [Initialization] 2014-09-17 13:01:16,031 Using x.x.x.x as the cassandra broadcast address
 INFO [StompConnection receiver] 2014-09-17 13:01:16,032 Starting up agent collection.
 INFO [Initialization] 2014-09-17 13:01:16,162 agent RPC address is  x.x.x.x
 INFO [StompConnection receiver] 2014-09-17 13:01:16,162 agent RPC address is  x.x.x.x
 INFO [Initialization] 2014-09-17 13:01:16,162 agent RPC broadcast address is  x.x.x.x
 INFO [StompConnection receiver] 2014-09-17 13:01:16,162 agent RPC broadcast address is  x.x.x.x
 INFO [StompConnection receiver] 2014-09-17 13:01:16,163 Starting OS metric collectors (Linux)
 INFO [Initialization] 2014-09-17 13:01:16,166 Clearing ssl.truststore
 INFO [Initialization] 2014-09-17 13:01:16,166 Clearing ssl.truststore.password
 INFO [Initialization] 2014-09-17 13:01:16,167 Setting ssl.store.type to JKS
 INFO [Initialization] 2014-09-17 13:01:16,167 Clearing kerberos.service.principal.name
 INFO [Initialization] 2014-09-17 13:01:16,167 Clearing kerberos.principal
 INFO [Initialization] 2014-09-17 13:01:16,167 Setting kerberos.useTicketCache to true
 INFO [Initialization] 2014-09-17 13:01:16,167 Clearing kerberos.ticketCache
 INFO [Initialization] 2014-09-17 13:01:16,168 Setting kerberos.useKeyTab to true
 INFO [Initialization] 2014-09-17 13:01:16,168 Clearing kerberos.keyTab
 INFO [Initialization] 2014-09-17 13:01:16,168 Setting kerberos.renewTGT to true
 INFO [Initialization] 2014-09-17 13:01:16,168 Setting kerberos.debug to false
 INFO [StompConnection receiver] 2014-09-17 13:01:16,171 Starting Cassandra JMX metric collectors
 INFO [thrift-init] 2014-09-17 13:01:16,171 Connecting to Cassandra cluster: x.x.x.x (port 9160)
 INFO [StompConnection receiver] 2014-09-17 13:01:16,187 New JMX connection (127.0.0.1:7199)
 INFO [thrift-init] 2014-09-17 13:01:16,189 Downed Host Retry service started with queue size -1 and retry delay 10s
 INFO [thrift-init] 2014-09-17 13:01:16,192 Registering JMX me.prettyprint.cassandra.service_Agent Cluster:ServiceType=hector,MonitorType=hector
 INFO [pdp-loader] 2014-09-17 13:01:16,231 in execute with client org.apache.cassandra.thrift.Cassandra$Client@7a22c094
 INFO [pdp-loader] 2014-09-17 13:01:16,237 Attempting to load stored metric values.
 INFO [thrift-init] 2014-09-17 13:01:16,240 Connected to Cassandra cluster: PoC
 INFO [thrift-init] 2014-09-17 13:01:16,240 in execute with client org.apache.cassandra.thrift.Cassandra$Client@7a22c094
 INFO [thrift-init] 2014-09-17 13:01:16,240 Using partitioner: org.apache.cassandra.dht.Murmur3Partitioner
 INFO [jmx-metrics-1] 2014-09-17 13:01:21,181 New JMX connection (127.0.0.1:7199)
ERROR [StompConnection receiver] 2014-09-17 13:01:24,376 Failed to collect machine info
java.lang.NullPointerException
        at clojure.lang.Numbers.ops(Numbers.java:942)
        at clojure.lang.Numbers.divide(Numbers.java:157)
        at opsagent.nodedetails.machine_info$get_machine_info.invoke(machine_info.clj:76)
        at opsagent.nodedetails$get_static_properties$fn__4313.invoke(nodedetails.clj:161)
        at opsagent.nodedetails$get_static_properties.invoke(nodedetails.clj:160)
        at opsagent.nodedetails$get_longtime_values$fn__4426.invoke(nodedetails.clj:227)
        at opsagent.nodedetails$get_longtime_values.invoke(nodedetails.clj:226)
        at opsagent.nodedetails$send_all_nodedetails$fn__4444.invoke(nodedetails.clj:245)
        at opsagent.jmx$jmx_wrap.doInvoke(jmx.clj:111)
        at clojure.lang.RestFn.invoke(RestFn.java:410)
        at opsagent.nodedetails$send_all_nodedetails.invoke(nodedetails.clj:241)
        at opsagent.opsagent$post_interface_startup.doInvoke(opsagent.clj:125)
        at clojure.lang.RestFn.invoke(RestFn.java:421)
        at opsagent.conf$handle_new_conf.invoke(conf.clj:179)
        at opsagent.messaging$message_callback$fn__6059.invoke(messaging.clj:30)
        at opsagent.messaging.proxy$java.lang.Object$StompConnection$Listener$7f16bc72.onMessage(Unknown Source)
        at org.jgroups.client.StompConnection.notifyListeners(StompConnection.java:319)
        at org.jgroups.client.StompConnection.run(StompConnection.java:269)
        at java.lang.Thread.run(Thread.java:745)
{code}"
CASSANDRA-7946,NPE when streaming data to a joining node and dropping table in cluster,"Summary

 The cluster has 3 nodes with Vnodes. We were adding a 4th one (in auto bootstrap mode). The 4 node has stream sessions, receiving data from the others.

 While the streaming was going on (it takes quite a while because there are 200Gb of data of worth), an existing table is dropped in the cluster and a new one is created.

 A few time after, we caught an NPE in the log file of the joining node (still not finished joining):

 The NPE is located here: https://github.com/apache/cassandra/blob/cassandra-2.0/src/java/org/apache/cassandra/streaming/compress/CompressedStreamReader.java#L63

 It can be a race condition where schema agreement has not reached the joining node."
CASSANDRA-7940,Gossip only node removal can race with FD.convict,"{noformat}
INFO  00:04:48 FatClient /10.208.8.63 has been silent for 30000ms, removing from gossip
INFO  00:04:49 Handshaking version with /10.208.8.63
ERROR 00:04:53 Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.gms.Gossiper.convict(Gossiper.java:301) ~[main/:na]
        at org.apache.cassandra.gms.FailureDetector.forceConviction(FailureDetector.java:251) ~[main/:na]
        at org.apache.cassandra.gms.GossipShutdownVerbHandler.doVerb(GossipShutdownVerbHandler.java:37) ~[main/:na]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:62) ~[main/:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_60]
{noformat}"
CASSANDRA-7932,Corrupt SSTable Cleanup Leak Resources,"CASSANDRA-7705, during the BlackingListingCompactionsTest, detected leaks. I've tracked this down to DataTracker.removeUnreadableSSTables() , which does not release the reference to any sstables from the tracker."
CASSANDRA-7897,NodeTool command to display OffHeap memory usage,"Most of the highest memory consuming data structure in Cassandra is now off-heap. It will be nice to display the memory used by BF's, Index Summaries, FS Buffers, Caches and Memtables (when enabled)

This ticket is to track and display off heap memory allocation/used by running Cassandra process, this will help users to further tune the memory used by these data structures per CF."
CASSANDRA-7896,Running sstableupgrade in Cassandra 2.0.10  causes java.lang.OutOfMemoryError: Java heap space,"Trying to upgrade Cassandra data from version 1.2.2 to 2.0.10.

I took a snapshot from 1.2.2 version. I created the keyspace schema in 2.0.10 version and copied the 1.2.2 snapshot under it. I then run 

bin/sstableupgrade <keyspace> <cf> snapshot

in version 2.0.10 and I get following error:

Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at java.util.ArrayList.<init>(ArrayList.java:132)
        at org.apache.cassandra.db.RowIndexEntry$Serializer.deserialize(RowIndexEntry.java:120)
        at org.apache.cassandra.io.sstable.SSTableReader.buildSummary(SSTableReader.java:487)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:455)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:422)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:203)
        at org.apache.cassandra.io.sstable.SSTableReader.openNoValidation(SSTableReader.java:155)
        at org.apache.cassandra.tools.StandaloneUpgrader.main(StandaloneUpgrader.java:82)

I have tried increasing memory to 2 G but I get the same error. The largest data file is of size 1275248."
CASSANDRA-7869,NPE while freezing a tuple containing a list,"Detected in 2.1.0-rc7 (not released yet):
{code}
cqlsh:test> CREATE TABLE mytable (k int PRIMARY KEY, v_0 frozen<tuple<list<int>>>);
<ErrorMessage code=2000 [Syntax error in CQL query] message=""Failed parsing statement: [CREATE TABLE mytable (k int PRIMARY KEY, v_0 frozen<tuple<list<int>>>);] reason: NullPointerException null"">
{code}"
CASSANDRA-7805,Performance regression in multi-get (in clause) due to automatic paging,"Comparative benchmarking of 1.2 vs. 2.0 shows a regression in multi-get (in clause) queries due to automatic paging.  Take the following example:

select myId, col1, col2, col3 from myTable where col1 = 'xyz' and myId IN (id1, id1, ..., id100); // primary key is (myId, col1)

We were suprised to see that in 2.0, the above query was giving an order of magnitude worse performance than in 1.2. Digging in, I believe it is due to the issue described in the comment at the top of MultiPartitionPager.java (v2.0.9): ""Note that this is not easy to make efficient. Indeed, we need to page the first command fully before returning results from the next one, but if the result returned by each command is small (compared to pageSize), paging the commands one at a time under-performs compared to parallelizing.""

The perf regression is due to the new paging feature in 2.0. The server is executing the read for each id in the IN clause *sequentially* in order to implement the paging semantics.

The wisdom of using multi-get like this has been debated in other forums, but the thing that's unfortunate from a user point of view, is if they had a bunch of code working against 1.2 and then they upgrade their cluster to 2.0 and all of a sudden start to see an order of magnitude or worse perf regression. That will be perceived as a problem. I think it would surprise anyone not familiar with the code that the separate reads for the IN clause would be done sequentially rather than in parallel.

As a workaround, disable paging in the Java driver by setting fetchSize to Integer.MAX_VALUE on your QueryOptions"
CASSANDRA-7787,2i index indexing the cql3 row marker throws NPE,"If you have a secondary index implementation that 'indexes()' the cql3 row marker you get a NPE in SecondaryIndexManager/deleteFromIndexes() as the call to getColumnDefinitionFromColumnName() returns null which is not checked for.

This has been detected in the context of inserting PK only rows, where the row marker is expected to be present. When 'indexes()' returned 'false', the row would mistakenly get deleted as the row marker didn't go through.

If 'indexes()' returns 'true' the row marker goes through but you get a NPE."
CASSANDRA-7782,NPE autosaving cache,"With the machine just sitting idle for a while:

{noformat}
INFO  18:33:35 Writing Memtable-sstable_activity@1889719059(162 serialized bytes, 72 ops, 0%/0% of on/off-heap limit)
INFO  18:33:35 Completed flushing /srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-3-Data.db (176 bytes) for commitlog position ReplayPosition(segmentId=1408116815479, position=129971)
ERROR 19:33:34 Exception in thread Thread[CompactionExecutor:12,1,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:464) ~[main/:na]
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:452) ~[main/:na]
        at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:225) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1053) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_65]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_65]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_65]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
INFO  19:33:35 Enqueuing flush of sstable_activity: 1561 (0%) on-heap, 0 (0%) off-heap
INFO  19:33:35 Writing Memtable-sstable_activity@1705670040(162 serialized bytes, 72 ops, 0%/0% of on/off-heap limit)
INFO  19:33:35 Completed flushing /srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-4-Data.db (177 bytes) for commitlog position ReplayPosition(segmentId=1408116815479, position=134711)
INFO  19:33:35 Compacting [SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-4-Data.db'), SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-2-Data.db'), SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-3-Data.db'), SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-1-Data.db')]
{noformat}

Looks similar to CASSANDRA-7632"
CASSANDRA-7775,Cassandra attempts to flush an empty memtable into disk and fails,"I'm not sure what triggers this flush, but when it happens the following appears in our logs:
{code}
 INFO [OptionalTasks:1] 2014-08-15 02:24:20,115 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-app_recs_best_in_expr_prefix2@1219170646(0/0 serialized/live bytes, 0 ops)
 INFO [FlushWriter:34] 2014-08-15 02:24:20,116 Memtable.java (line 331) Writing Memtable-app_recs_best_in_expr_prefix2@1219170646(0/0 serialized/live bytes, 0 ops)
ERROR [FlushWriter:34] 2014-08-15 02:24:20,127 CassandraDaemon.java (line 196) Exception in thread Thread[FlushWriter:34,5,main]
java.lang.RuntimeException: Cannot get comparator 1 in org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type). This might due to a mismatch between the schema and the data read
        at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:133)
        at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:140)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:96)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:35)
        at org.apache.cassandra.db.RangeTombstone$Tracker$1.compare(RangeTombstone.java:125)
        at org.apache.cassandra.db.RangeTombstone$Tracker$1.compare(RangeTombstone.java:122)
        at java.util.TreeMap.compare(TreeMap.java:1188)
        at java.util.TreeMap$NavigableSubMap.<init>(TreeMap.java:1264)
        at java.util.TreeMap$AscendingSubMap.<init>(TreeMap.java:1699)
        at java.util.TreeMap.tailMap(TreeMap.java:905)
        at java.util.TreeSet.tailSet(TreeSet.java:350)
        at java.util.TreeSet.tailSet(TreeSet.java:383)
        at org.apache.cassandra.db.RangeTombstone$Tracker.update(RangeTombstone.java:203)
        at org.apache.cassandra.db.ColumnIndex$Builder.add(ColumnIndex.java:192)
        at org.apache.cassandra.db.ColumnIndex$Builder.build(ColumnIndex.java:138)
        at org.apache.cassandra.io.sstable.SSTableWriter.rawAppend(SSTableWriter.java:202)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:187)
        at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:365)
        at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:318)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)
        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:306)
        at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:285)
        at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45)
        at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:124)
        ... 23 more
{code}

After this happens, the MemtablePostFlusher thread pool starts piling up.
When trying to restart the cluster, a similar exception occurs when trying to replay the commit log.
Our way of recovering from this is to delete all commit logs in the faulty node, start it and issue a repair."
CASSANDRA-7772,"Windows - fsync-analog, flush data to disk",We currently use CLibrary fsync linux-native calls to flush to disk.  Given the role this plays in our SequentialWriter and data integrity in general we need some analog to this function on Windows.
CASSANDRA-7770,NPE when clean up compaction leftover if table is already dropped,"As reported in CASSANDRA-7759, Directories throws NPE when trying to remove compaction leftovers on already dropped table.

Attaching patch to check if table exists in schema before clean up."
CASSANDRA-7754,FileNotFoundException in MemtableFlushWriter,"Exception in cassandra logs, after upgrade to 2.1:

[MemtableFlushWriter:91] ERROR o.a.c.service.CassandraDaemon - Exception in thread Thread[MemtableFlushWriter:91,5,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /xxx/cassandra/data/system/batchlog-0290003c977e397cac3efdfdc01d626b/system-batchlog-tmp-ka-186-Index.db (No such file or directory)
	at org.apache.cassandra.io.util.SequentialWriter.<init>(SequentialWriter.java:75) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.io.util.SequentialWriter.open(SequentialWriter.java:104) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.io.util.SequentialWriter.open(SequentialWriter.java:99) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.<init>(SSTableWriter.java:550) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:134) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.db.Memtable$FlushRunnable.createFlushWriter(Memtable.java:383) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:330) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:314) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at 
org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.jar:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1054) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_65]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_65]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_65]
Caused by: java.io.FileNotFoundException: /xxx/cassandra/data/system/batchlog-0290003c977e397cac3efdfdc01d626b/system-batchlog-tmp-ka-186-Index.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method) ~[na:1.7.0_65]
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:241) ~[na:1.7.0_65]
	at org.apache.cassandra.io.util.SequentialWriter.<init>(SequentialWriter.java:71) ~[cassandra-all-2.1.0-rc5.jar:2.1.0-rc5]
	... 14 common frames omitted"
CASSANDRA-7750,"Do not flush on truncate if ""durable_writes"" is false.","CASSANDRA-7511 changed truncate so it will always flush to fix commit log issues.  If durable_writes is false, then there will not be able data in the commit log for the table, so we can safely just drop the memtables and not flush."
CASSANDRA-7709,sstable2json has resource leaks,"Coverity found 3 resource leak within  SSTable2JsonExporter:
** CID 71971:  Resource leak  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 300 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, java.util.Collection, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()

** CID 71972:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 332 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.SSTableReader,
java.io.PrintStream, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()

** CID 71975:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 236 in
org.apache.cassandra.tools.SSTableExport.enumeratekeys(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, org.apache.cassandra.config.CFMetaData)()


________________________________________________________________________________________________________
*** CID 71971:  Resource leak  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 300 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, java.util.Collection, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()
294                 i++;
295                 serializeRow(deletionInfo, atomIterator,
sstable.metadata, decoratedKey, outs);
296             }
297
298             outs.println(""\n]"");
299             outs.flush();
>>>     CID 71971:  Resource leak  (RESOURCE_LEAK)
>>>     Variable ""dfile"" going out of scope leaks the resource it refers to.
300         }
301
302         // This is necessary to accommodate the test suite since
you cannot open a Reader more
303         // than once from within the same process.
304         static void export(SSTableReader reader, PrintStream outs,
String[] excludes, CFMetaData metadata) throws IOException
305         {

________________________________________________________________________________________________________
*** CID 71972:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 332 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.SSTableReader,
java.io.PrintStream, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()
326                 if (excludeSet.contains(currentKey))
327                     continue;
328                 else if (i != 0)
329                     outs.println("","");
330
331                 serializeRow(row, row.getKey(), outs);
>>>     CID 71972:  Resource leak on an exceptional path  (RESOURCE_LEAK)
>>>     Variable ""scanner"" going out of scope leaks the resource it refers to.
332                 checkStream(outs);
333
334                 i++;
335             }
336
337             outs.println(""\n]"");

________________________________________________________________________________________________________
*** CID 71975:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 236 in
org.apache.cassandra.tools.SSTableExport.enumeratekeys(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, org.apache.cassandra.config.CFMetaData)()
230             while (iter.hasNext())
231             {
232                 DecoratedKey key = iter.next();
233
234                 // validate order of the keys in the sstable
235                 if (lastKey != null && lastKey.compareTo(key) > 0)
>>>     CID 71975:  Resource leak on an exceptional path  (RESOURCE_LEAK)
>>>     Variable ""iter"" going out of scope leaks the resource it refers to.
236                     throw new IOException(""Key out of order! "" +
lastKey + "" > "" + key);
237                 lastKey = key;
238
239
outs.println(metadata.getKeyValidator().getString(key.getKey()));
240                 checkStream(outs); // flushes
241             }

"
CASSANDRA-7701,Infinite flush loop,"If you run the repro from CASSANDRA-7695, you can get an infinite flush loop:

{noformat}
NFO  20:46:34 Compacting [SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1442-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1443-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1440-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1441-Data.db')]
INFO  20:46:34 Enqueuing flush of compactions_in_progress: 180 (0%) on-heap, 0 (0%) off-heap
INFO  20:46:34 Writing Memtable-compactions_in_progress@842537816(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  20:46:34 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-954-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1407269935245, position=12803233)
INFO  20:46:34 Compacted 4 sstables to [/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1444,].  9,664 bytes to 2,416 (~25% of original) in 37ms = 0.062272MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
INFO  20:46:35 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:35 Writing Memtable-put_test@418061108(512021 serialized bytes, 356 ops, 33%/0% of on/off-heap limit)
INFO  20:46:35 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1445-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935247, position=30214640)
INFO  20:46:37 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:37 Writing Memtable-put_test@1715686331(512021 serialized bytes, 348 ops, 33%/0% of on/off-heap limit)
INFO  20:46:37 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1446-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935250, position=19460288)
INFO  20:46:39 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:39 Writing Memtable-put_test@295716698(512021 serialized bytes, 344 ops, 33%/0% of on/off-heap limit)
INFO  20:46:39 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1447-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935253, position=7681712)
INFO  20:46:39 Enqueuing flush of compactions_in_progress: 1333 (0%) on-heap, 0 (0%) off-heap
INFO  20:46:39 Writing Memtable-compactions_in_progress@833384128(143 serialized bytes, 9 ops, 0%/0% of on/off-heap limit)
INFO  20:46:39 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-955-Data.db (166 bytes) for commitlog position ReplayPosition(segmentId=1407269935253, position=8194154)
INFO  20:46:39 Compacting [SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1447-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1446-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1445-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1444-Data.db')]
INFO  20:46:39 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-952-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-955-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-953-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-954-Data.db')]
INFO  20:46:39 Enqueuing flush of compactions_in_progress: 180 (0%) on-heap, 0 (0%) off-heap
INFO  20:46:39 Writing Memtable-compactions_in_progress@1180400490(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  20:46:39 Compacted 4 sstables to [/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-956,].  462 bytes to 166 (~35% of original) in 44ms = 0.003598MB/s.  5 total partitions merged to 1.  Partition merge counts were {1:3, 2:1, }
INFO  20:46:39 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-957-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1407269935253, position=12803233)
INFO  20:46:39 Compacted 4 sstables to [/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1448,].  9,664 bytes to 2,416 (~25% of original) in 68ms = 0.033883MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
INFO  20:46:40 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:40 Writing Memtable-put_test@553655581(512021 serialized bytes, 358 ops, 33%/0% of on/off-heap limit)
INFO  20:46:40 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1449-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935255, position=32775200)
INFO  20:46:42 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:42 Writing Memtable-put_test@1630754028(512021 serialized bytes, 354 ops, 34%/0% of on/off-heap limit)
INFO  20:46:42 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1450-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935258, position=23557184)
{noformat}

This keeps going long after the run is complete.  Restarting the node stops the loop."
CASSANDRA-7684,flush makes rows invisible to cluster key equality query,"{noformat}
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };
USE test;
CREATE TYPE point_t (x double, y double);
CREATE TABLE points (partitionkey int, b boolean static, clusteringkey point_t, PRIMARY KEY (partitionkey, clusteringkey) );
INSERT INTO points (partitionkey, clusteringkey) VALUES (1, {x:-104.9925100000,y:39.7476520000});
select * from points WHERE partitionkey=1 AND clusteringkey = {x:-104.9925100000,y:39.7476520000};
 partitionkey | clusteringkey           | b
--------------+-------------------------+------
            1 | {x: -104.99, y: 39.748} | null
(1 rows)
cqlsh:test> update points set b = true where partitionkey=1;
cqlsh:test> select * from points WHERE partitionkey=1 AND clusteringkey = {x:-104.9925100000,y:39.7476520000};
 partitionkey | clusteringkey           | b
--------------+-------------------------+------
            1 | {x: -104.99, y: 39.748} | True
(1 rows)
{noformat}

// run bin/nodetool flush here

{noformat}
cqlsh:test> select * from points WHERE partitionkey=1 AND clusteringkey = {x:-104.9925100000,y:39.7476520000};
(0 rows)
cqlsh:test> select * from points WHERE partitionkey=1;
 partitionkey | clusteringkey           | b
--------------+-------------------------+------
            1 | {x: -104.99, y: 39.748} | True
(1 rows)
{noformat}

i.e. the data is not lost, it's just invisible when read from sstable, but visible when read from memtable.


"
CASSANDRA-7668,Make gc_grace_seconds 7 days for system tables,"The system tables have had a {{gc_grace_seconds}} of 8640 since CASSANDRA-4018.  This was probably a typo and was intended to be 10 days.  In CASSANDRA-6717 we will set gc_grace to seven days, so that would be a reasonable value to use here."
CASSANDRA-7645,cqlsh: show partial trace if incomplete after max_trace_wait,"If a trace hasn't completed within {{max_trace_wait}}, cqlsh will say the trace is unavailable and not show anything.  It (and the underlying python driver) determines when the trace is complete by checking if the {{duration}} column in {{system_traces.sessions}} is non-null.  If {{duration}} is null but we still have some trace events when the timeout is hit, cqlsh should print whatever trace events we have along with a warning about it being incomplete."
CASSANDRA-7641,cqlsh should automatically disable tracing when selecting from system_traces,Nobody needs to trace their traces while they're tracing.
CASSANDRA-7632,NPE in AutoSavingCache$Writer.deleteOldCacheFiles,"Observed this NPE in one of our production cluster (2.0.9). Does not seem to be causing harm but good to resolve.

ERROR [CompactionExecutor:1188] 2014-07-27 21:57:08,225 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:1188,1,main] 
clusterName=clouddb_p03 
java.lang.NullPointerException 
at org.apache.cassandra.cache.AutoSavingCache$Writer.deleteOldCacheFiles(AutoSavingCache.java:265) 
at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:195) 
at org.apache.cassandra.db.compaction.CompactionManager$10.run(CompactionManager.java:862) 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) 
at java.util.concurrent.FutureTask.run(FutureTask.java:166) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) 
at java.lang.Thread.run(Thread.java:722)"
CASSANDRA-7625,cqlsh not paging results,"If you try to make a selection of more data than can fit in a page, you'll get this error:

{code}
cqlsh> select * from ""Keyspace1"".""Standard1"";

'PagedResult' object does not support indexing
{code}

With debugging on, I get this traceback:

{code}
Traceback (most recent call last):
  File ""/home/automaton/fab/cassandra/bin/cqlsh"", line 822, in onecmd
    self.handle_statement(st, statementtext)
  File ""/home/automaton/fab/cassandra/bin/cqlsh"", line 860, in handle_statement
    return custom_handler(parsed)
  File ""/home/automaton/fab/cassandra/bin/cqlsh"", line 890, in do_select
    self.perform_statement(statement, with_default_limit=with_default_limit)
  File ""/home/automaton/fab/cassandra/bin/cqlsh"", line 895, in perform_statement
    with_default_limit=with_default_limit)
  File ""/home/automaton/fab/cassandra/bin/cqlsh"", line 923, in perform_simple_statement
    self.print_result(rows, with_default_limit, cfMetaData)
  File ""/home/automaton/fab/cassandra/bin/cqlsh"", line 935, in print_result
    self.print_static_result(rows, cfMetaData)
  File ""/home/automaton/fab/cassandra/bin/cqlsh"", line 961, in print_static_result
    colnames = rows[0]._fields
TypeError: 'PagedResult' object does not support indexing
{code}

Placing a limit on the query still works."
CASSANDRA-7570,CqlPagingRecordReader is broken,"As mentioned on CASSANDRA-7059, it broke CPRR.  It's not quite as simple as changing the greater than to a greater than equal, either, since that makes the task run forever."
CASSANDRA-7551,improve 2.1 flush defaults,Default of mfw=2 and mct=0.4 does a poor job saturating multi-disk systems.
CASSANDRA-7546,AtomicSortedColumns.addAllWithSizeDelta has a spin loop that allocates memory,"In order to preserve atomicity, this code attempts to read, clone/update, then CAS the state of the partition.

Under heavy contention for updating a single partition this can cause some fairly staggering memory growth (the more cores on your machine the worst it gets).

Whilst many usage patterns don't do highly concurrent updates to the same partition, hinting today, does, and in this case wild (order(s) of magnitude more than expected) memory allocation rates can be seen (especially when the updates being hinted are small updates to different partitions which can happen very fast on their own) - see CASSANDRA-7545

It would be best to eliminate/reduce/limit the spinning memory allocation whilst not slowing down the very common un-contended case."
CASSANDRA-7518,The In-Memory option,"There is an In-Memory option introduced in the commercial version of Cassandra by DataStax Enterprise 4.0:
http://www.datastax.com/documentation/datastax_enterprise/4.0/datastax_enterprise/inMemory.html
But with 1GB size limited for an in-memory table.

It would be great if the In-Memory option can be available to the community version of Cassandra, and extend to a large size of in-memory table, such as 64GB.
"
CASSANDRA-7514,Support paging in cqlsh,"Once we've switch cqlsh to use the python driver 2.x (CASSANDRA-7506), we should also make it use paging. Currently cqlsh adds an implicit limit which is kind of ugly. Instead we should use some reasonably small page size (100 is probably fine) and display one page at a time, adding some ""NEXT"" command to query/display following pages."
CASSANDRA-7511,Always flush on TRUNCATE,"Commit log grows infinitely after CF truncate operation via cassandra-cli, regardless CF receives writes or not thereafter.
CF's could be non-CQL Standard and Super column type. Creation of snapshots after truncate is turned off.
Commit log may start grow promptly, may start grow later, on a few only or on all nodes at once.
Nothing special in the system log. No idea how to reproduce.
After rolling restart commit logs are cleared and back to normal. Just annoying to do rolling restart after each truncate."
CASSANDRA-7505,cassandra fails to start if WMI memory query fails on Windows,"{code:title=failure}
C:\src\cassandra>bin\cassandra.bat -f
Detected powershell execution permissions.  Running with enhanced startup scripts.
Setting up Cassandra environment
Starting cassandra server
Invalid initial heap size: -Xms0M
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{code}"
CASSANDRA-7496,ClassCastException in MessagingService,"Got the following exception when running repair on a 3 node ccm cluster

{code}
ERROR [EXPIRING-MAP-REAPER:1] 2014-07-03 21:24:33,063 CassandraDaemon.java:166 - Exception in thread Thread[EXPIRING-MAP-REAPER:1,5,main]
java.lang.ClassCastException: org.apache.cassandra.net.CallbackInfo cannot be cast to org.apache.cassandra.net.WriteCallbackInfo
	at org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:352) ~[main/:na]
	at org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:335) ~[main/:na]
	at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:98) ~[main/:na]
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_60]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) [na:1.7.0_60]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_60]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.7.0_60]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_60]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_60]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
{code}

Looks like that block (MessagingService, li. 352), was changed with CASSANDRA-7245.

While I produced this on trunk, I compared the MS code on trunk with 2.1.0 and it is  the same. This is the change that 7245 introduced:

pre-7245
{code}
                if (expiredCallbackInfo.shouldHint())
                {
                    Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;
                    return StorageProxy.submitHint(mutation, expiredCallbackInfo.target, null);
                }
{code}

7245:
{code}
                Mutation mutation = (Mutation) ((WriteCallbackInfo) expiredCallbackInfo).sentMessage.payload;

                try
                {
                    if (expiredCallbackInfo.shouldHint())
                    {
                        return StorageProxy.submitHint(mutation, expiredCallbackInfo.target, null);
                    }
                }
                finally
                {
                    //We serialized a hint so we don't need this mutation anymore
                    mutation.release();
                }
{code}
"
CASSANDRA-7495,Remove traces of populate_io_cache_on_flush and replicate_on_write,Following up on CASSANDRA-7493. Apparently we still have a few traces of replicate_on_write and populate_io_cache_on_write in C* (neither used in 2.1).
CASSANDRA-7493,Remove traces of rows_per_partition_to_cache,"CASSANDRA-6745 changed the way we configure how many rows per partition we cache, some old stuff was left in, clear it out"
CASSANDRA-7480,CommitLogTest.testCommitFailurePolicy_stop Fails w/ NPE,"1.) Check out trunk at commit b4a354f3e23b8a23acce70eb911dc58e7f57298c
2.) run {{ant test}} from the project root
3.) Observe...
{noformat}
Testsuite: org.apache.cassandra.db.CommitLogTest
Tests run: 14, Failures: 0, Errors: 1, Time elapsed: 1.891 sec

------------- Standard Output ---------------
WARN  00:44:47 Couldn't open /proc/stats
WARN  00:44:47 Couldn't open /proc/stats
WARN  00:44:47 JNA link failure, one or more native method will be unavailable.
WARN  00:44:47 JNA link failure, one or more native method will be unavailable.
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-6714392903486597656.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-6714392903486597656.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-3130605652588023504.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-3130605652588023504.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-7416815543415721780.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-7416815543415721780.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-2864073117299213590.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-2864073117299213590.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-908098755907065048.log, with invalid CRC. The end of segment marker should be zero.
WARN  00:44:48 Encountered bad header at position 16 of commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-908098755907065048.log, with invalid CRC. The end of segment marker should be zero.
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-2921422245890190390.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-2921422245890190390.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-2938149009734594039.log; not enough room for a header
WARN  00:44:48 Encountered bad header at position 16 of Commit log /var/folders/bg/r0bzhsj50rj2gxntn7ddmgt80000gn/T/CommitLog-4-2938149009734594039.log; not enough room for a header
------------- ---------------- ---------------
Testcase: testCommitFailurePolicy_stop(org.apache.cassandra.db.CommitLogTest):	Caused an ERROR
null
java.lang.NullPointerException
	at org.apache.cassandra.db.Mutation.addOrGet(Mutation.java:160)
	at org.apache.cassandra.db.Mutation.addOrGet(Mutation.java:155)
	at org.apache.cassandra.db.Mutation.add(Mutation.java:176)
	at org.apache.cassandra.db.Mutation.add(Mutation.java:186)
	at org.apache.cassandra.db.CommitLogTest.testCommitFailurePolicy_stop(CommitLogTest.java:301)


Test org.apache.cassandra.db.CommitLogTest FAILED
{noformat}"
CASSANDRA-7454,NPE When Prepared Statement ID is not Found,"CASSANDRA-6855 introduced a NullPointerException when an unknown prepared statement ID is used.

You'll see a stack trace like this:
{noformat}
ERROR [SharedPool-Worker-4] 2014-06-26 15:02:04,911 ErrorMessage.java:218 - Unexpected exception during request
java.lang.NullPointerException: null
    at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:105) ~[main/:na]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:412) [main/:na]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:309) [main/:na]
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:103) [netty-all-4.0.20.Final.jar:4.0.20.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:332) [netty-all-4.0.20.Final.jar:4.0.20.Final]
    at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:31) [netty-all-4.0.20.Final.jar:4.0.20.Final]
    at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:323) [netty-all-4.0.20.Final.jar:4.0.20.Final]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_40]
    at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:162) [main/:na]
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:103) [main/:na]
    at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
{noformat}"
CASSANDRA-7451,Launch scripts on Windows don't handle spaces gracefully,"There's also some .ps1 problems after we get past just the .bat.  Should be some trivial escaping to fix.

C:\src - Copy\cassandra\bin>cassandra.bat
Detected powershell execution permissions.  Running with enhanced startup scripts.
Processing -File 'C:\src' failed because the file does not have a '.ps1' extension. Specify a valid PowerShell script file name, and then try again."
CASSANDRA-7445,Support Thrift tables clustering columns on CqlPagingInputFormat,"CASSANDRA-5752 introduced support to thrift tables on CQLPagingInputFormat via the retrieveKeysForThriftTables() method.

However, this method only retrieves partition keys from CFMetaData, so clustering columns are ignored.

So, when the RowIterator tries to fetch the next page of a wide-row thrift CF, it ignores the clustering column altogether, going to the next partition token. So, only cassandra.input.page.row.size CQL rows are retrieved for each partition key.

CqlRecordWriter had the same issue and was fixed on bcfe352ea6ed3786f274b90191f988210360689d (CASSANDRA-5718)."
CASSANDRA-7428,clean out DD.inMemoryCompactionLimit,still used in a couple places
CASSANDRA-7385,sstableloader OutOfMemoryError: Java heap space,"We hit the following exception with sstableloader while attempting to bulk load about 100GB of SSTs. We are now employing this workaround before starting an sstableloader run:

sed -i -e 's/-Xmx256M/-Xmx8G/g' /usr/bin/sstableloader

{code}
ERROR 19:25:45,060 Error in ThreadPoolExecutor
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.io.util.FastByteArrayOutputStream.expand(FastByteArrayOutputStream.java:104)
	at org.apache.cassandra.io.util.FastByteArrayOutputStream.write(FastByteArrayOutputStream.java:235)
	at java.io.DataOutputStream.writeInt(DataOutputStream.java:199)
	at org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer.serialize(CompressionMetadata.java:412)
	at org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer.serialize(CompressionMetadata.java:407)
	at org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer.serialize(CompressionInfo.java:59)
	at org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer.serialize(CompressionInfo.java:46)
	at org.apache.cassandra.streaming.PendingFile$PendingFileSerializer.serialize(PendingFile.java:142)
	at org.apache.cassandra.streaming.StreamHeader$StreamHeaderSerializer.serialize(StreamHeader.java:67)
	at org.apache.cassandra.streaming.StreamHeader$StreamHeaderSerializer.serialize(StreamHeader.java:58)
	at org.apache.cassandra.net.MessagingService.constructStreamHeader(MessagingService.java:782)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:65)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Exception in thread ""Streaming to /10.167.a.b:1"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.io.util.FastByteArrayOutputStream.expand(FastByteArrayOutputStream.java:104)
	at org.apache.cassandra.io.util.FastByteArrayOutputStream.write(FastByteArrayOutputStream.java:235)
	at java.io.DataOutputStream.writeInt(DataOutputStream.java:199)
	at org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer.serialize(CompressionMetadata.java:412)
	at org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer.serialize(CompressionMetadata.java:407)
	at org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer.serialize(CompressionInfo.java:59)
	at org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer.serialize(CompressionInfo.java:46)
	at org.apache.cassandra.streaming.PendingFile$PendingFileSerializer.serialize(PendingFile.java:142)
	at org.apache.cassandra.streaming.StreamHeader$StreamHeaderSerializer.serialize(StreamHeader.java:67)
	at org.apache.cassandra.streaming.StreamHeader$StreamHeaderSerializer.serialize(StreamHeader.java:58)
	at org.apache.cassandra.net.MessagingService.constructStreamHeader(MessagingService.java:782)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:65)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.cassandra.io.compress.CompressionMetadata.getChunksForSections(CompressionMetadata.java:210)
	at org.apache.cassandra.streaming.StreamOut.createPendingFiles(StreamOut.java:182)
	at org.apache.cassandra.streaming.StreamOut.transferSSTables(StreamOut.java:157)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:145)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:67)
{code}"
CASSANDRA-7373,Commit logs no longer deleting and MemtablePostFlusher pending growing,"We have this issue where once in a while, we get into a situation where the MemtablePostFlusher is not executing and the space used by the commit logs on disks keeps on increasing and increasing.

We can observe the problem by invoking nodetool tpstats:
{code}
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
ReadStage                         6         6       46650213         0                 0
RequestResponseStage              0         0      130547421         0                 0
MutationStage                     2         2      116813206         0                 0
ReadRepairStage                   0         0        2322201         0                 0
ReplicateOnWriteStage             0         0              0         0                 0
GossipStage                       0         0         120780         0                 0
AntiEntropyStage                  0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
MemoryMeter                       0         0            456         0                 0
MemtablePostFlusher               1       447           6344         0                 0
FlushWriter                       0         0           6132         0                62
MiscStage                         0         0              0         0                 0
PendingRangeCalculator            0         0              6         0                 0
commitlog_archiver                0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     2         2              4         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
BINARY                       0
READ                         0
MUTATION                     0
_TRACE                       0
REQUEST_RESPONSE             0
COUNTER_MUTATION             0
{code}

Here is a potential error in the logs that can explain this:
{code}
ERROR [FlushWriter:2693] 2014-06-09 22:05:38,452 CassandraDaemon.java (line 191) Exception in thread Thread[FlushWriter:2693,5,main]
java.lang.NegativeArraySizeException
	at org.apache.cassandra.io.util.FastByteArrayOutputStream.expand(FastByteArrayOutputStream.java:104)
	at org.apache.cassandra.io.util.FastByteArrayOutputStream.write(FastByteArrayOutputStream.java:220)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.cassandra.io.util.DataOutputBuffer.write(DataOutputBuffer.java:60)
	at org.apache.cassandra.utils.ByteBufferUtil.write(ByteBufferUtil.java:328)
	at org.apache.cassandra.utils.ByteBufferUtil.writeWithLength(ByteBufferUtil.java:315)
	at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:55)
	at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:30)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.serializeForSSTable(OnDiskAtom.java:62)
	at org.apache.cassandra.db.ColumnIndex$Builder.add(ColumnIndex.java:181)
	at org.apache.cassandra.db.ColumnIndex$Builder.build(ColumnIndex.java:133)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:185)
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:430)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:385)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)

{code}"
CASSANDRA-7360,CQLSSTableWriter consumes all memory for table with compound primary key,"When using CQLSSTableWriter to write a table with compound primary key, if the partition key is identical for a huge amount of records, the sync() method is never called, and the memory usage keeps growing until the memory is exhausted. 

Could the code be improved to do sync() even when there is no new row  created? The relevant code is in SSTableSimpleUnsortedWriter.java and AbstractSSTableSimpleWriter.java. I am new to the code and cannot produce a reasonable patch for now.

The problem can be reproduced by the following test case:
{code}
import org.apache.cassandra.io.sstable.CQLSSTableWriter;
import org.apache.cassandra.exceptions.InvalidRequestException;

import java.io.IOException;
import java.util.UUID;

class SS {
    public static void main(String[] args) {
        String schema = ""create table test.t (x uuid, y uuid, primary key (x, y))"";


        String insert = ""insert into test.t (x, y) values (?, ?)"";
        CQLSSTableWriter writer = CQLSSTableWriter.builder()
            .inDirectory(""/tmp/test/t"")
            .forTable(schema).withBufferSizeInMB(32)
            .using(insert).build();

        UUID id = UUID.randomUUID();
        try {
            for (int i = 0; i < 50000000; i++) {
                UUID id2 = UUID.randomUUID();
                writer.addRow(id, id2);
            }

            writer.close();
        } catch (Exception e) {
            System.err.println(""hell"");
        }
    }
}
{code}"
CASSANDRA-7332,NPE in SecondaryIndexManager.deleteFromIndexes,"When trying to run ""nodetool cleanup"", I get this exception on the node:

ERROR [CompactionExecutor:1363] 2014-05-31 11:55:45,708 CassandraDaemon.java (line 198) Exception in thread Thread[CompactionExecutor:1363,1,RMI Runtime]
java.lang.NullPointerException
        at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:460)
        at org.apache.cassandra.db.compaction.CompactionManager$CleanupStrategy$Full.cleanup(CompactionManager.java:719)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:579)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:274)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:222)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)


Nodetool itself got this:
Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:188)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:227)
	at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:265)
	at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:1105)
	at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:2215)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:460)
	at org.apache.cassandra.db.compaction.CompactionManager$CleanupStrategy$Full.cleanup(CompactionManager.java:719)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:579)
	at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:274)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:222)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	... 3 more


I suspect it is related to this table which used to have another index, which I dropped:

CREATE TABLE positions (
  syd int,
  broker uuid,
  engine uuid,
  confirmed bigint,
  open_buy bigint,
  open_sell bigint,
  PRIMARY KEY (syd, broker, engine)
) WITH
  bloom_filter_fp_chance=0.000100 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.010000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX engine_positions ON positions (engine);

The other index was: CREATE INDEX broker_positions ON positions (broker);

Curiously, a day or so after dropping it, it had come back!  I noticed this when trying to understand why I got this cleanup exception.  I then dropped it again, and it appears to have gone but the exception still occurs."
CASSANDRA-7323,NPE in StreamTransferTask.createMessageForRetry,"seeing this NPE during repair:
at org.apache.cassandra.streaming.StreamTransferTask.createMessageForRetry(StreamTransferTask.java:106)
at org.apache.cassandra.streaming.StreamSession.retry(StreamSession.java:525)
at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:401)
at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:289)
at java.lang.Thread.run(Thread.java:744)"
CASSANDRA-7322,CqlPagingRecordReader returns partial or no data,"In trying to solve the last problem with CASSANDRA-7241, this bisected to CASSANDRA-6379.  On the surface of course this doesn't seem right, so I repeated the bisect (both of which were completely manual and tested by hand) and landed at the same place, and since it did touch getSplits I think it's the right candidate.

What I see is that CPRR gets all the splits, which with vnodes are mostly empty since there's very little data in the test.  Before CASSANDRA-6379 it iterates them all and succeeds by eventually finding it.  When it fails, however, it only checks the first split and gives up, even if it does happen to get lucky and find one row.

If I use data generated before CASSANDRA-6379, it works fine, but not with data generated afterward.  I suspect this isn't specific to CPRR but hadoop in general, it's just been easier for me to spot with CPRR, perhaps due to the nature of the test."
CASSANDRA-7321,retire jython paging tests and port over to python driver,"The latest version of the python driver now supports paging, so we have no need to test paging with jython anymore.

We should either create another paging test project to supplant https://github.com/riptano/cassandra-dtest-jython, or better yet get these tests incorporated into cassandra-dtest since we are soon going to port dtest over to the new python driver (away from dbapi2)."
CASSANDRA-7303,OutOfMemoryError during prolonged batch processing,"We have a prolonged batch processing job. 
It writes a lot of records, every batch mutation creates probably on average 300-500 columns per row key (with many disparate row keys).

It works fine but within a few hours we get error like this:

ERROR [Thrift:15] 2014-05-24 14:16:20,192 CassandraDaemon.java (line |
|196) Except                                                          |
|ion in thread Thread[Thrift:15,5,main]                               |
|java.lang.OutOfMemoryError: Requested array size exceeds VM limit    |
|at java.util.Arrays.copyOf(Arrays.java:2271)                         |
|at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)|
|at java.io.ByteArrayOutputStream.ensureCapacity                      |
|(ByteArrayOutputStream.ja                                            |
|va:93)                                                               |
|at java.io.ByteArrayOutputStream.write                               |
|(ByteArrayOutputStream.java:140)                                     |
|at org.apache.thrift.transport.TFramedTransport.write                |
|(TFramedTransport.j                                                  |
|ava:146)                                                             |
|at org.apache.thrift.protocol.TBinaryProtocol.writeBinary            |
|(TBinaryProtoco                                                      |
|l.java:183)                                                          |
|at org.apache.cassandra.thrift.Column$ColumnStandardScheme.write     |
|(Column.                                                             |
|java:678)                                                            |
|at org.apache.cassandra.thrift.Column$ColumnStandardScheme.write     |
|(Column.                                                             |
|java:611)                                                            |
|at org.apache.cassandra.thrift.Column.write(Column.java:538)         |
|at org.apache.cassandra.thrift.ColumnOrSuperColumn                   |
|$ColumnOrSuperColumnSt                                               |
|andardScheme.write(ColumnOrSuperColumn.java:673)                     |
|at org.apache.cassandra.thrift.ColumnOrSuperColumn                   |
|$ColumnOrSuperColumnSt                                               |
|andardScheme.write(ColumnOrSuperColumn.java:607)                     |
|at org.apache.cassandra.thrift.ColumnOrSuperColumn.write             |
|(ColumnOrSuperCo                                                     |
|lumn.java:517)                                                       |
|at org.apache.cassandra.thrift.Cassandra$get_slice_result            |
|$get_slice_resu                                                      |
|ltStandardScheme.write(Cassandra.java:11682)                         |
|at org.apache.cassandra.thrift.Cassandra$get_slice_result            |
|$get_slice_resu                                                      |
|ltStandardScheme.write(Cassandra.java:11603)                         |
|at org.apache.cassandra.thrift.Cassandra

The server already has 16 GB heap, which we hear is the max Cassandra can run with. The writes are heavily multi-threaded from a single server.

The jist of the issue is that Cassandra should not crash with OOM when under heavy load. It is  OK  to slow down, even maybe start throwing operation timeout exceptions, etc.

But to just crash in the middle of the processing should not be allowed.

is there any internal monitoring of heap usage in Cassandra where it could detect that it is getting close to the heap limit and start throttling the incoming requests to avoid this type of error?

Thanks
"
CASSANDRA-7286,Exception: NPE ,"Sometimes Cassandra nodes (in a multi datacenter deployment)  are throwing NPE (see attached stack trace)

Let me know what additional information I could provide.

Thank you."
CASSANDRA-7284,ClassCastException in HintedHandoffManager.pagingFinished,"During a long running stress test on bdplab, Ryan encountered the following interesting exception, which I think is an as yet unfiled bug. Looks to be a pretty simple issue, introduced by CASSANDRA-5417

{code}
java.lang.ClassCastException: org.apache.cassandra.db.composites.Composites$EmptyComposite cannot be cast to org.apache.cassandra.db.composites.CellName
	at org.apache.cassandra.db.HintedHandOffManager.pagingFinished(HintedHandOffManager.java:266) ~[main/:na]
	at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:376) ~[main/:na]
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331) ~[main/:na]
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:93) ~[main/:na]
	at org.apache.cassandra.db.HintedHandOffManager$5.run(HintedHandOffManager.java:547) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_51]
	at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
{code}"
CASSANDRA-7278,NPE in StorageProxy.java:1920,"Got this this morning under heavy load:

ERROR [ReadStage:128] 2014-05-21 07:59:03,274 CassandraDaemon.java (line 198) Exception in thread Thread[ReadStage:128,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1920)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.util.RandomAccessReader.getTotalBufferSize(RandomAccessReader.java:157)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.getTotalBufferSize(CompressedRandomAccessReader.java:159)
        at org.apache.cassandra.service.FileCacheService.get(FileCacheService.java:96)
        at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:36)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1540)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1369)
        at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:327)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1352)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1916)
        ... 3 more


There had just been a 20 second GC pause, and the system was dropping messages like mad, see attached log snippet."
CASSANDRA-7275,Errors in FlushRunnable may leave threads hung,"In Memtable.FlushRunnable, the CountDownLatch will never be counted down if there are errors, which results in hanging any threads that are waiting for the flush to complete.  For example, an error like this causes the problem:

{noformat}
ERROR [FlushWriter:474] 2014-05-20 12:10:31,137 CassandraDaemon.java (line 198) Exception in thread Thread[FlushWriter:474,5,main]
java.lang.IllegalArgumentException
    at java.nio.Buffer.position(Unknown Source)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:64)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:72)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.split(AbstractCompositeType.java:138)
    at org.apache.cassandra.io.sstable.ColumnNameHelper.minComponents(ColumnNameHelper.java:103)
    at org.apache.cassandra.db.ColumnFamily.getColumnStats(ColumnFamily.java:439)
    at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:194)
    at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:397)
    at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:350)
    at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
{noformat}"
CASSANDRA-7260,Stress overflows past 2^31 keys,"Writing more than 2^31 keys with stress causes an overflow making stress start to count negative key numbers. It doesn't stop the write, but it does foul up the aggregate statistics:

{code}
2147395249,   69404,   69404,    11.7,     5.1,    35.4,   135.7,   169.9,   250.8,27093.0,  0.00211
-2147481870,   89925,   89925,     9.0,     4.8,    27.1,   110.3,   120.3,   128.5,27094.0,  0.00211

.....
-1894967296,    7318,    7318,     0.7,     0.6,     0.8,     1.9,    60.4,    60.4,30546.6,  0.00197


Results:
real op rate              : -62035
adjusted op rate stderr   : 0
key rate                  : -62035
latency mean              : -13.0
latency median            : 3.9
latency 95th percentile   : 16.2
latency 99th percentile   : 35.1
latency 99.9th percentile : 170.1
latency max               : 4376.7
Total operation time      : 08:29:06
END
{code}"
CASSANDRA-7254,NPE on startup if another Cassandra instance is already running,"After CASSANDRA-7087, if you try to start cassandra while another instance is already running, you'll see something like this:

{noformat}
$ bin/cassandra -f
Error: Exception thrown by the agent : java.lang.NullPointerException
{noformat}

This is probably a JVM bug, but we should confirm that, open a JVM ticket, and see if we can give a more useful error message on the C* side.
"
CASSANDRA-7246,Gossip Null Pointer Exception when a cassandra instance in ring is restarted,"12 Cassandra instances, one per node.
11 of the Cassandra instances are 1.2.15.
1 of the Cassandra instances is 1.2.16.

One of the eleven 1.2.15 Cassandra instances is restarted (disable thrift, gossip, then flush, drain, stop, start).

The 1.2.16 Cassandra instance noted this by throwing a Null Pointer Exception. None of the 1.2.15 instances threw an exception and this is new behavior that hasn't been observed before.


ERROR 02:18:06,009 Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.gms.Gossiper.convict(Gossiper.java:264)
        at org.apache.cassandra.gms.FailureDetector.forceConviction(FailureDetector.java:246)
        at org.apache.cassandra.gms.GossipShutdownVerbHandler.doVerb(GossipShutdownVerbHandler.java:37)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
 INFO 02:18:23,402 Node /10.x.y.x is now part of the cluster
 INFO 02:18:23,403 InetAddress /10.x.y.z is now UP
 INFO 02:18:53,494 FatClient /10.x.y.z has been silent for 30000ms, removing from gossip
 INFO 02:19:00,031 Handshaking version with /10.x.y.z

"
CASSANDRA-7203,Flush (and Compact) High Traffic Partitions Separately,"An idea possibly worth exploring is the use of streaming count-min sketches to collect data over the up-time of a server to estimating the velocity of different partitions, so that high-volume partitions can be flushed separately on the assumption that they will be much smaller in number, thus reducing write amplification by permitting compaction independently of any low-velocity data.

Whilst the idea is reasonably straight forward, it seems that the biggest problem here will be defining any success metric. Obviously any workload following an exponential/zipf/extreme distribution is likely to benefit from such an approach, but whether or not that would translate in real terms is another matter."
CASSANDRA-7198,CqlPagingRecordReader throws IllegalStateException,"Getting the following exception when running a Spark job that does *not* specify cassandra.input.page.row.size:

{code}
14/05/08 14:30:43 ERROR executor.Executor: Exception in task ID 12
java.lang.IllegalStateException: Optional.get() cannot be called on an absent value
        at com.google.common.base.Absent.get(Absent.java:47)
        at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader.initialize(CqlPagingRecordReader.java:120)
        at com.tuplejump.calliope.cql3.Cql3CassandraRDD$$anon$1.<init>(Cql3CassandraRDD.scala:65)
        at com.tuplejump.calliope.cql3.Cql3CassandraRDD.compute(Cql3CassandraRDD.scala:53)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
14/05/08 14:30:43 ERROR executor.Executor: Exception in task ID 21
java.lang.IllegalStateException: Optional.get() cannot be called on an absent value
        at com.google.common.base.Absent.get(Absent.java:47)
        at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader.initialize(CqlPagingRecordReader.java:120)
        at com.tuplejump.calliope.cql3.Cql3CassandraRDD$$anon$1.<init>(Cql3CassandraRDD.scala:65)
        at com.tuplejump.calliope.cql3.Cql3CassandraRDD.compute(Cql3CassandraRDD.scala:53)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

Reason why is CqlPagingRecordReader catching the wrong exception type. Patch attached."
CASSANDRA-7192,QueryTrace for a paginated query exists only for the first element of the list returned by getAllExecutionInfo(),"Within the Java Driver, with tracing enabled, I execute a large query that benefits from automatic pagination (with fetchSize=10).

I make sure to go through all of the ResultSet, and by the end of the query I call getAllExecutionInfo() on the ResultSet. This returns an ArrayList of 9 ExecutionInfo elements (the number of pages it requested from Cassandra).

When accessing the QueryTrace in the ExecutionInfo from the ArrayList at index 0, I can retrieve the information without issues. However, the first is the only one that has QueryTrace information, every other ExecutionInfo of the array returns a NULL QueryTrace object."
CASSANDRA-7177,Starting threads in the OutboundTcpConnectionPool constructor causes race conditions,"The OutboundTcpConnectionPool starts connection threads in its constructor, causing race conditions when MessagingService#getConnectionPool is concurrently called for the first time for a given address.

I.e., here's one of the races:
{noformat}
 WARN 12:49:03,182 Error processing org.apache.cassandra.metrics:type=Connection,scope=127.0.0.1,name=CommandPendingTasks
javax.management.InstanceAlreadyExistsException: org.apache.cassandra.metrics:type=Connection,scope=127.0.0.1,name=CommandPendingTasks
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at com.yammer.metrics.reporting.JmxReporter.registerBean(JmxReporter.java:464)
	at com.yammer.metrics.reporting.JmxReporter.processGauge(JmxReporter.java:438)
	at com.yammer.metrics.reporting.JmxReporter.processGauge(JmxReporter.java:16)
	at com.yammer.metrics.core.Gauge.processWith(Gauge.java:28)
	at com.yammer.metrics.reporting.JmxReporter.onMetricAdded(JmxReporter.java:395)
	at com.yammer.metrics.core.MetricsRegistry.notifyMetricAdded(MetricsRegistry.java:516)
	at com.yammer.metrics.core.MetricsRegistry.getOrAdd(MetricsRegistry.java:491)
	at com.yammer.metrics.core.MetricsRegistry.newGauge(MetricsRegistry.java:79)
	at com.yammer.metrics.Metrics.newGauge(Metrics.java:70)
	at org.apache.cassandra.metrics.ConnectionMetrics.<init>(ConnectionMetrics.java:71)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.<init>(OutboundTcpConnectionPool.java:55)
	at org.apache.cassandra.net.MessagingService.getConnectionPool(MessagingService.java:498)
{noformat}"
CASSANDRA-7171,Startup Cassandra and encountered java.lang.OutOfMemoryError: Java heap space,"canssdra.log:
INFO 15:06:44,665 Data files directories: [../../../../cassandra_data]
INFO 15:06:44,665 Commit log directory: ../../../../cassandra_data/commitlog
INFO 15:06:44,665 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO 15:06:44,665 disk_failure_policy is stop
INFO 15:06:44,758 Global memtable threshold is enabled at 247MB
INFO 15:06:45,039 Not using multi-threaded compaction
INFO 15:06:45,507 Initializing key cache with capacity of 37 MBs.
INFO 15:06:45,538 Scheduling key cache save to each 14400 seconds (going to save all keys).
INFO 15:06:45,538 Initializing row cache with capacity of 0 MBs
INFO 15:06:45,554 Scheduling row cache save to each 0 seconds (going to save all keys).
INFO 15:06:45,819 Initializing system.schema_triggers
INFO 15:06:45,913 Initializing system.batchlog
INFO 15:06:45,913 Initializing system.peer_events
INFO 15:06:45,928 Initializing system.compactions_in_progress
INFO 15:06:45,928 Opening ..\..\..\..\cassandra_data\system\compactions_in_progress\system-compactions_in_progress-ja-1772 (42 bytes)
INFO 15:06:45,928 Opening ..\..\..\..\cassandra_data\system\compactions_in_progress\system-compactions_in_progress-ja-1773 (42 bytes)
INFO 15:06:45,928 Opening ..\..\..\..\cassandra_data\system\compactions_in_progress\system-compactions_in_progress-ja-1771 (252 bytes)
INFO 15:06:46,022 Initializing system.hints
INFO 15:06:46,037 Initializing system.schema_keyspaces
INFO 15:06:46,037 Opening ..\..\..\..\cassandra_data\system\schema_keyspaces\system-schema_keyspaces-ja-37 (266 bytes)
INFO 15:06:46,069 Initializing system.range_xfers
INFO 15:06:46,069 Initializing system.schema_columnfamilies
INFO 15:06:46,069 Opening ..\..\..\..\cassandra_data\system\schema_columnfamilies\system-schema_columnfamilies-ja-37 (5979 bytes)
INFO 15:06:46,115 Initializing system.NodeIdInfo
INFO 15:06:46,115 Initializing system.paxos
INFO 15:06:46,115 Initializing system.schema_columns
INFO 15:06:46,115 Opening ..\..\..\..\cassandra_data\system\schema_columns\system-schema_columns-ja-42 (9423 bytes)
INFO 15:06:46,162 reading saved cache ..\..\..\..\cassandra_data\saved_caches\system-schema_columns-KeyCache-b.db
INFO 15:06:46,162 Initializing system.IndexInfo
INFO 15:06:46,178 Initializing system.peers
INFO 15:06:46,178 Initializing system.local
INFO 15:06:46,178 Opening ..\..\..\..\cassandra_data\system\local\system-local-ja-149 (5752 bytes)
INFO 15:06:46,225 reading saved cache ..\..\..\..\cassandra_data\saved_caches\system-local-KeyCache-b.db
INFO 15:06:47,224 Enqueuing flush of Memtable-local@714233355(114/114 serialized/live bytes, 4 ops)
INFO 15:06:47,224 Writing Memtable-local@714233355(114/114 serialized/live bytes, 4 ops)
INFO 15:06:47,318 Completed flushing ..\..\..\..\cassandra_data\system\local\system-local-ja-150-Data.db (148 bytes) for commitlog position ReplayPosition(segmentId=1399273607006, position=289)
INFO 15:06:47,333 Initializing kairosdb.row_key_index
INFO 15:06:47,333 Opening ..\..\..\..\cassandra_data\kairosdb\row_key_index\kairosdb-row_key_index-ja-3663 (1107026 bytes)
INFO 15:06:47,333 Opening ..\..\..\..\cassandra_data\kairosdb\row_key_index\kairosdb-row_key_index-ja-3662 (9834029 bytes)
INFO 15:06:47,411 reading saved cache ..\..\..\..\cassandra_data\saved_caches\kairosdb-row_key_index-KeyCache-b.db
INFO 15:06:47,442 Initializing kairosdb.string_index
INFO 15:06:47,442 Opening ..\..\..\..\cassandra_data\kairosdb\string_index\kairosdb-string_index-ja-3602 (96700 bytes)
INFO 15:06:47,442 Opening ..\..\..\..\cassandra_data\kairosdb\string_index\kairosdb-string_index-ja-3601 (123957 bytes)
INFO 15:06:47,458 reading saved cache ..\..\..\..\cassandra_data\saved_caches\kairosdb-string_index-KeyCache-b.db
INFO 15:06:47,458 Initializing kairosdb.data_points
INFO 15:06:47,474 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-2608 (813190770 bytes)
INFO 15:06:47,489 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-2251 (3227646469 bytes)
INFO 15:06:47,489 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3587 (58079087 bytes)
INFO 15:06:47,489 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3302 (826655679 bytes)
INFO 15:06:47,489 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-2977 (815231395 bytes)
INFO 15:06:47,489 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3601 (3986479 bytes)
INFO 15:06:47,489 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3545 (209800102 bytes)
INFO 15:06:47,489 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-701 (414906383 bytes)
INFO 15:06:48,098 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3464 (211306283 bytes)
INFO 15:06:48,098 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-1115 (3547590516 bytes)
INFO 15:06:48,269 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3383 (212410575 bytes)
INFO 15:06:48,269 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3566 (57746128 bytes)
INFO 15:06:48,332 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-722 (523682069 bytes)
INFO 15:06:48,394 Opening ..\..\..\..\cassandra_data\kairosdb\data_points\kairosdb-data_points-ja-3600 (37086136 bytes)
INFO 15:06:49,080 reading saved cache ..\..\..\..\cassandra_data\saved_caches\kairosdb-data_points-KeyCache-b.db
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid25432.hprof ...
Heap dump file created [8712741 bytes in 0.099 secs]
java.lang.OutOfMemoryError: Java heap space
ERROR 15:06:49,392 Exception encountered during startup
at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
java.lang.OutOfMemoryError: Java heap space
at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:337)
at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:147)
at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:337)
at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:267)
at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:147)
at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:411)
at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:267)
at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:383)
at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:411)
at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:314)
at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:383)
at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:268)
at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:314)
at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:268)
at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:273)
at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:443)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:273)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:486)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:443)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:486)
Exception encountered during startup: Java heap space
Out enabled"
CASSANDRA-7170,socket should enable keepalive in MessagingService.SocketThread,"The number of socket fds increase over time in some nodes of one of our cassandra clusters .
Finally we find that the socket should enable keepalive in MessagingService.SocketThread .

the 2.x versions have fix this.
but the 1.x versions still have this problem."
CASSANDRA-7129,Consider allowing clients to make the Paging State available to users,"This is a follow up to a ticket that has been opened on the DataStax Java Driver JIRA (https://datastax-oss.atlassian.net/browse/JAVA-323).

Currently the Paging State is described as an internal data structure that might change in any upcoming version. As a consequence it isn't safe to make it available to users of the Cassandra Drivers.

It would be and interesting feature to work on making Cassandra safe against all the situation that might happen after unleashing paging states in the wild on the client side: they could end up being included in some web cookies, allowing malicious users to forge some, we might also have some compatibility issues as some paging states might come back to Cassandra after an upgrade of the cluster,...

If the discussion in this ticket turns out to conclude that the paging state SHOULD NOT be made available to users, at least it will be a clarification of something that was mostly implicit (AFAIK) so far."
CASSANDRA-7120,Bad paging state returned for prepared statements for last page,"When executing a paged query with a prepared statement, a non-null paging state is sometimes being returned for the final page, causing an endless paging loop.

Specifically, this is the schema being used:
{noformat}
    CREATE KEYSPACE test3rf WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}';
    USE test3rf;
    CREATE TABLE test3rf.test (
                k int PRIMARY KEY,
                v int
    )
{noformat}

The inserts are like so:
{noformat}
INSERT INTO test3rf.test (k, v) VALUES (?, 0)
{noformat}

With values from [0, 99] used for k.

The query is {{SELECT * FROM test3rf.test}} with a fetch size of 3.

The final page returns the row with k=3, and the paging state is {{0004000000420004000176007fffffa2}}.  This matches the paging state from three pages earlier.  When executing this with a non-prepared statement, no paging state is returned for this page.

This problem doesn't happen with the 2.0 branch."
CASSANDRA-7114,Paging dtests,[Test plan is here|https://github.com/riptano/cassandra-test-plans/wiki/Paging-Tests]
CASSANDRA-7064,snapshot creation for non-system keyspaces snapshots to the flush and data dirs,"{noformat}
root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot -t testsnapshot
Requested creating snapshot(s) for [all keyspaces] with snapshot name [testsnapshot]
Snapshot directory: testsnapshot
root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/
foo  system  system_traces
root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/
bar-c5e32c10c96b11e39f8d3b546d897db7
root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot
Requested creating snapshot(s) for [all keyspaces] with snapshot name [1398095603827]
Snapshot directory: 1398095603827
root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/
foo/           system/        system_traces/ 
root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/
bar-c5e32c10c96b11e39f8d3b546d897db7
root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo
Requested creating snapshot(s) for [foo] with snapshot name [1398095800752]
Snapshot directory: 1398095800752
root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/
bar-c5e32c10c96b11e39f8d3b546d897db7
root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo -cf bar -t testsnapshot
Requested creating snapshot(s) for [foo] with snapshot name [testsnapshot]
error: Snapshot testsnapshot already exists.
-- StackTrace --
java.io.IOException: Snapshot testsnapshot already exists.
 at org.apache.cassandra.service.StorageService.takeColumnFamilySnapshot(StorageService.java:2315)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
 at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
 at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
 at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
 at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
 at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
 at sun.rmi.transport.Transport$1.run(Transport.java:177)
 at sun.rmi.transport.Transport$1.run(Transport.java:174)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
 at java.lang.Thread.run(Thread.java:744)

root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/
bar-c5e32c10c96b11e39f8d3b546d897db7
root@bw-3:/srv/cassandra-dtest# ls -R /var/lib/cassandra/data/foo/|grep testsnap
root@bw-3:/srv/cassandra-dtest# ls -R /var/lib/cassandra/data/|grep testsnap
testsnapshot
/var/lib/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/snapshots/testsnapshot:
root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo -cf bar -t testsnapshot2
Requested creating snapshot(s) for [foo] with snapshot name [testsnapshot2]
Snapshot directory: testsnapshot2
root@bw-3:/srv/cassandra-dtest# ls -R /var/lib/cassandra/data/|grep testsnap
testsnapshot
/var/lib/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/snapshots/testsnapshot:
root@bw-3:/srv/cassandra-dtest# 
{noformat}

As shown above, snapshots for the 'foo' keyspace are never created, but it does seem to think it created them since I can't use the same name twice."
CASSANDRA-7050,AbstractColumnFamilyInputFormat & AbstractColumnFamilyOutputFormat throw NPE if username is provided but password is null,If a username is provided to either of these classes but the password is null the thrift layer throws an NPE because it can't handle null values for the login.
CASSANDRA-7015,sstableloader NPE,"The basic snapshot dtest is failing:

{code}
PRINT_DEBUG=true nosetests2 -x -s -v snapshot_test.py:TestSnapshot
{code}

This is due to this error from sstableloader:

{code}
Opening sstables and calculating sections to stream
null
java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableReader.getFilename(SSTableReader.java:627)
	at org.apache.cassandra.io.sstable.SSTable.toString(SSTable.java:243)
	at org.apache.cassandra.io.sstable.SSTableReader.loadSummary(SSTableReader.java:802)
	at org.apache.cassandra.io.sstable.SSTableReader.openForBatch(SSTableReader.java:343)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:113)
	at java.io.File.list(File.java:1155)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:74)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:156)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:84)
FAIL
{code}

This was working as of CASSANDRA-6965 being fixed, but it's broken again. I think it's due to the changes in 5ebadc11e36749e, so I'm assigning to you Bendedict."
CASSANDRA-6932,StorageServiceMbean needs to expose flush directory.,"Storage service currently exposes data dirs, commitlog dir, and saved caches dir. Should add the flush dir now that we have that as well."
CASSANDRA-6867,MeteredFlusher should ignore memtables not affected by it,"Before metered flusher runs, count up the number of bytes used by memtables unaffected by metered flusher and subtract that from the maximum allowed bytes."
CASSANDRA-6860,Race condition in Batch CLE,"CASSANDRA-6759 exposed a bug in CLE, where we are signalling the waiting segments before setting the value they're waiting on. This is occurring much more readily now as we perform a slow call in the middle of the race. Attaching a quick and simple patch."
CASSANDRA-6859,Can't gracefull shutdown cassandra,"Sometimes can't gracefully shutdown cassandra server. No exceptions in log. HSHA thrift server is used and it seems cassandra is stuck on stopping it. From jstack (full version attached):

""StorageServiceShutdownHook"" prio=10 tid=0x00007f7a1c004000 nid=0x2422 in Object.wait() [0x00007f7a2fb2b000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Thread.join(Thread.java:1280)
        - locked <0x00000006141c67a8> (a org.apache.cassandra.thrift.ThriftServer$ThriftServerThread)
        at java.lang.Thread.join(Thread.java:1354)
        at org.apache.cassandra.thrift.ThriftServer.stop(ThriftServer.java:68)
        at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:312)
        at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:381)
        at org.apache.cassandra.service.StorageService.access$000(StorageService.java:97)
        at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:567)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:744)

"
CASSANDRA-6857,SELECT DISTINCT with a LIMIT is broken by paging,"The paging for RangeSliceCommand only support the case where we count CQL3 rows . However, in the case of SELECT DISTINCT, we do actually want to use the ""count partitions, not CQL3 row"" path and that's currently broken when the paging commands are used (this was first reported on the [Java driver JIRA|https://datastax-oss.atlassian.net/browse/JAVA-288] and there is a reproduction script there)."
CASSANDRA-6838,FileCacheService overcounting its memoryUsage,"On investigating why I was seeing dramatically worse performance for counter updates over prepared CQL3 statements compred to unprepared CQL2 statements, I stumbled upon a bug in FileCacheService wherein, on returning a cached reader back to the pool, its memory is counted again towards the total memory usage, but is not matched by a decrement when checked out. So we effectively are probably not caching readers most of the time.
"
CASSANDRA-6832,File handle leak in StreamWriter.java,"Reference CASSANDRA-6283 where this first came up.  nodetool.bat repair -par on 2.0.5 pops up the following stack:

ERROR [Finalizer] 2014-02-17 09:21:52,922 RandomAccessReader.java (line 399) LEAK finalizer had to clean up
java.lang.Exception: RAR for C:\var\lib\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-jb-41-CRC.db allocated
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:106)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:98)
        at org.apache.cassandra.io.util.DataIntegrityMetadata$ChecksumValidator.<init>(DataIntegrityMetadata.java:53)
        at org.apache.cassandra.io.util.DataIntegrityMetadata.checksumValidator(DataIntegrityMetadata.java:40)
        at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:76)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:59)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42)
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:383)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:355)
        at java.lang.Thread.run(Thread.java:744)

This leak doesn't look like it's breaking anything but is still worth fixing."
CASSANDRA-6820,NPE in MeteredFlusher.run,"Hello,

I've been seeing this exception with Cassandra 2.0.5:

{code}
ERROR 15:41:46,754 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:40)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}

Could it be that {{Memtable.activelyMeasuring}} becomes null right after the test?"
CASSANDRA-6817,Update gc_grace not taken into account,"1 - connect cassandra using cassandra-cli  [OK]
2 - select DB to use [OK]
3 - update column family with;

[default@BNPPFortis] update column family CallSafety with gc_grace=0;
5aa7f6fc-968d-3fa5-9e3a-6ece15fe9c15
Waiting for schema agreement...
... schemas agree across the cluster

4 - check modificatios with:
[default@BNPPFortis] show schema;
create column family CallSafety
  with column_type = 'Super'
  and comparator = 'UTF8Type'
  and subcomparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};

5 - value is not taken into account [KO] 

6 - inspect system.log:
 INFO [MigrationStage:1] 2014-03-07 11:40:20,558 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columnfamilies@24479498(1339/1673 serialized/live bytes, 21 ops)
 INFO [FlushWriter:4] 2014-03-07 11:40:20,559 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@24479498(1339/1673 serialized/live bytes, 21 ops)
 INFO [FlushWriter:4] 2014-03-07 11:40:20,571 Memtable.java (line 307) Completed flushing /opt/Alcatel/database/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-38-Data.db (1407 bytes)


"
CASSANDRA-6806,"In AtomicBTreeColumns, construct list of unwinds after a race lazily","Currently we store these in a List, but this is wasteful. We can construct them lazily from a diff between the original and partially constructed replacement BTree. The UpdaterFunction could define a method to be passed such a collection in the event of an early abort."
CASSANDRA-6805,When something goes wrong the `nodetool` command prints the whole Java stacktrace instead of a simple error message to stderr,"{noformat}
$ nodetool snapshot XXX -t YYY
Requested creating snapshot for: XXX
Exception in thread ""main"" java.io.IOException: Table XXX does not exist
        at org.apache.cassandra.service.StorageService.getValidTable(StorageService.java:2267)
        at org.apache.cassandra.service.StorageService.takeSnapshot(StorageService.java:2222)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}

Please change the `nodetool` command so that it does not print the stacktrace by default, it makes using it from other scripts a PITA. You can possibly add a `--debug` parameter that can be used to print the stacktrace if the user really wants it.
"
CASSANDRA-6804,Consolidate on-disk and NativeCell layouts so that reads from disk require less memory,"If the on-disk Cell representation were the same as we use for NativeCell, we could easily allocate a NativeCell instead of a BufferCell, immediately reducing the amount of garbage generated on reads. With further work we may also be able to reach a zero-copy allocation as well, reducing further the read costs."
CASSANDRA-6797,compaction and scrub data directories race on startup," 
Hi,  

On doing a rolling restarting of a 2.0.5 cluster in several environments I'm seeing the following error:
{code}

 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,549 CompactionTask.java (line 115) Compacting [SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compactio
n_race/node1/data/system/local/system-local-jb-15-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-16-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/syst
em-local-jb-14-Data.db')]
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,557 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,560 ColumnFamilyStore.java (line 254) Initializing system_traces.events
 WARN [main] 2014-03-03 17:11:07,608 ColumnFamilyStore.java (line 473) Removing orphans for /Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13: [CompressionInfo.db, Filter.db, Index.db, TOC.txt, Summary.db, Data.db, Statistics.
db]
ERROR [main] 2014-03-03 17:11:07,609 CassandraDaemon.java (line 479) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file system-local-jb-13-CompressionInfo.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:111)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:106)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:476)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,612 CompactionTask.java (line 275) Compacted 4 sstables to [/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-17,].  10,963 bytes to 5,572 (~50% of original) in 57ms = 0.093226MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }

{code}
Seems like a potential race, since compactions are occurring whilst the existing data directories are being scrubbed.
Probably an in progress compaction looks like an incomplete one and results in it being attempted to be scrubbed whilst in progress. 
On the attempt to delete in the scrubDataDirectories we discover that it no longer exists, presumably because it has now been compacted away. 
This then causes an assertion error and the node fails to start up. 

Here is a ccm script which just stops and starts a 3 node 2.0.5 cluster repeatedly. 
It seems to fairly reliably reproduce the problem, in less than ten iterations: 

{code}
#!/bin/bash

ccm create compaction_race -v 2.0.5
ccm populate -n 3
ccm start

for i in $(seq 0 1000); do 
    echo $i;
    ccm stop
    ccm start
    grep ERR ~/.ccm/compaction_race/*/logs/system.log;
done

{code}
 
Someone else should probably confirm that this is what is going wrong,  
however if it is, the solution might be as simple as to disable autocompactions slightly earlier in CassandraDaemon.setup. 
 
Or alternatively if there isn't a good reason why we are first scrubbing the system tables and then scrubbing all keyspaces (including the system keyspace), you could perhaps just scrub solely the non system keyspaces on the second scrub.

Please let me know if there is anything else I can provide.
Thanks,
Matt
"
CASSANDRA-6793,NPE in Hadoop Word count example,"The partition keys requested in WordCount.java do not match the primary key set up in the table output_words. It looks this patch was not merged properly from [CASSANDRA-5622|https://issues.apache.org/jira/browse/CASSANDRA-5622].The attached patch addresses the NPE and uses the correct keys defined in #5622.

I am assuming there is no need to fix the actual NPE like throwing an InvalidRequestException back to user to fix the partition keys, as it would be trivial to get the same from the TableMetadata using the driver API.

java.lang.NullPointerException
	at org.apache.cassandra.dht.Murmur3Partitioner.getToken(Murmur3Partitioner.java:92)
	at org.apache.cassandra.dht.Murmur3Partitioner.getToken(Murmur3Partitioner.java:40)
	at org.apache.cassandra.client.RingCache.getRange(RingCache.java:117)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:163)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:63)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:587)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at WordCount$ReducerToCassandra.reduce(Unknown Source)
	at WordCount$ReducerToCassandra.reduce(Unknown Source)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)"
CASSANDRA-6788,Race condition silently kills thrift server,"There's a race condition in CustomTThreadPoolServer that can cause the thrift server to silently stop listening for connections. 

It happens when the executor service throws a RejectedExecutionException, which is not caught.
 
Silent in the sense that OpsCenter doesn't notice any problem since JMX is still running fine."
CASSANDRA-6785,clean out populate_io_cache_on_flush option,populate_io_cache_on_flush is a per-table option as of 1.2.2 (CASSANDRA-4694) but the old global option remains in Config where it is ignored.
CASSANDRA-6751,Setting -Dcassandra.fd_initial_value_ms Results in NPE,"Start Cassandra with {{-Dcassandra.fd_initial_value_ms=1000}} and you'll get the following stacktrace:

{noformat}
 INFO [main] 2014-02-21 14:45:57,731 StorageService.java (line 617) Starting up server gossip
ERROR [main] 2014-02-21 14:45:57,736 CassandraDaemon.java (line 464) Exception encountered during startup
java.lang.ExceptionInInitializerError
    at org.apache.cassandra.gms.Gossiper.<init>(Gossiper.java:178)
    at org.apache.cassandra.gms.Gossiper.<clinit>(Gossiper.java:71)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:618)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:583)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:480)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.gms.FailureDetector.getInitialValue(FailureDetector.java:81)
    at org.apache.cassandra.gms.FailureDetector.<clinit>(FailureDetector.java:48)
    ... 8 more
ERROR [StorageServiceShutdownHook] 2014-02-21 14:45:57,754 CassandraDaemon.java (line 191) Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.gms.Gossiper
    at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:550)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.lang.Thread.run(Thread.java:724)
{noformat}

Glancing at the code, this is because the FailureDetector logger isn't initialized when the static initialization of {{INITIAL_VALUE}} happens."
CASSANDRA-6748,"If null is explicitly set to a column, paging_state will not work","If null is explicitly set to a column, paging_state will not work. My test procedure is as follows:

------
Create a table and insert 10 records using cqlsh. The query is as follows:

{code}
CREATE TABLE mytable (id int, range int, value text, PRIMARY KEY (id, range));
INSERT INTO mytable (id, range) VALUES (0, 0);
INSERT INTO mytable (id, range) VALUES (0, 1);
INSERT INTO mytable (id, range) VALUES (0, 2);
INSERT INTO mytable (id, range) VALUES (0, 3);
INSERT INTO mytable (id, range) VALUES (0, 4);
INSERT INTO mytable (id, range, value) VALUES (0, 5, null);
INSERT INTO mytable (id, range, value) VALUES (0, 6, null);
INSERT INTO mytable (id, range, value) VALUES (0, 7, null);
INSERT INTO mytable (id, range, value) VALUES (0, 8, null);
INSERT INTO mytable (id, range, value) VALUES (0, 9, null);
{code}

Select 10 records using datastax driver. The pseudocode is as follows:

{code}
Statement statement = QueryBuilder.select().from(""mytable"").setFetchSize(1);
ResultSet rs = session.execute(statement);
for(Row row : rs){
    System.out.println(String.format(""id=%d, range=%d, value=%s"",
        row.getInt(""id""), row.getInt(""range""), row.getString(""value"")));
}
{code}

The result is as follows:

{code}
id=0, range=0, value=null
id=0, range=1, value=null
id=0, range=2, value=null
id=0, range=3, value=null
id=0, range=4, value=null
id=0, range=5, value=null
id=0, range=7, value=null
id=0, range=9, value=null
{code}
------

Result is 8 records although 10 records were expected. I originally raised this issue in the mailing lists: http://www.mail-archive.com/user@cassandra.apache.org/msg34752.html"
CASSANDRA-6747,MessagingService should handle failures on remote nodes.,"While going through the code of MessagingService, I discovered that we don't handle callbacks on failure very well. If a Verb Handler on the remote machine throws an exception, it goes right through uncaught exception handler. The machine which triggered the message will keep waiting and will timeout. On timeout, it will so some stuff hard coded in the MS like hints and add to Latency. There is no way in IAsyncCallback to specify that to do on timeouts and also on failures. 

Here are some examples which I found will help if we enhance this system to also propagate failures back.  So IAsyncCallback will have methods like onFailure.

1) From ActiveRepairService.prepareForRepair

   IAsyncCallback callback = new IAsyncCallback()
       {
           @Override
           public void response(MessageIn msg)
           {
               prepareLatch.countDown();
           }

           @Override
           public boolean isLatencyForSnitch()
           {
               return false;
           }
       };

       List<UUID> cfIds = new ArrayList<>(columnFamilyStores.size());
       for (ColumnFamilyStore cfs : columnFamilyStores)
           cfIds.add(cfs.metadata.cfId);

       for(InetAddress neighbour : endpoints)
       {
           PrepareMessage message = new PrepareMessage(parentRepairSession, cfIds, ranges);
           MessageOut<RepairMessage> msg = message.createMessage();
           MessagingService.instance().sendRR(msg, neighbour, callback);
       }
       try
       {
           prepareLatch.await(1, TimeUnit.HOURS);
       }
       catch (InterruptedException e)
       {
           parentRepairSessions.remove(parentRepairSession);
           throw new RuntimeException(""Did not get replies from all endpoints."", e);
       }

2) During snapshot phase in repair, if SnapshotVerbHandler throws an exception, we will wait forever. "
CASSANDRA-6740,Non-empty flush directory causes NPE on startup,"With the changes in CASSANDRA-6357, there is now the case where someone may want to start a fresh cluster on a machine that previously hosted one, clean out the directories they are familiar with (eg, data_file_directories, commitlog_directories) but fail to clean out the flush_directory. If they don't clean that out, they will see this in the logs:

{code}
INFO  [main] 2014-02-19 12:20:13,530 ColumnFamilyStore.java:281 - Initializing system.IndexInfo
INFO  [main] 2014-02-19 12:20:13,534 ColumnFamilyStore.java:281 - Initializing system.peers
INFO  [main] 2014-02-19 12:20:13,539 ColumnFamilyStore.java:281 - Initializing system.local
ERROR [main] 2014-02-19 12:20:13,703 CassandraDaemon.java:458 - Exception encountered during startup
java.lang.NullPointerException: null
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167) ~[main/:na]
        at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:39) ~[main/:na]
        at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:26) ~[main/:na]
        at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:66) ~[main/:na]
        at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:150) ~[main/:na]
        at org.apache.cassandra.config.CFMetaData.fromSchemaNoTriggers(CFMetaData.java:1761) ~[main/:na]
        at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1883) ~[main/:na]
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:320) ~[main/:na]
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:301) ~[main/:na]
        at org.apache.cassandra.db.DefsTables.loadFromKeyspace(DefsTables.java:131) ~[main/:na]
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:539) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:230) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:441) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:530) [main/:na]
{code}

I suggest a better warning message telling them it's due to the unclean flush dir, instead of an NPE."
CASSANDRA-6735,Exceptions during memtable flushes on shutdown hook prevent process shutdown,"If an exception occurs while flushing memtables during the shutdown hook, the process is left hanging due to non-daemon threads still running."
CASSANDRA-6722,cross-partition ordering should have warning or be disallowed when paging,"consider this schema/data/query:
{noformat}
CREATE TABLE paging_test (
    id int,
    value text,
    PRIMARY KEY (id, value)
) WITH CLUSTERING ORDER BY (value ASC)

            |id|value|
            |1 |a    |
            |2 |b    |
            |1 |c    |
            |2 |d    | 
            |1 |e    | 
            |2 |f    | 
            |1 |g    | 
            |2 |h    |
            |1 |i    |
            |2 |j    |

select * from paging_test where id in (1,2) order by value asc;
{noformat}
When paging the above query I get the sorted results from id=1 first, then the sorted results from id=2 after that. I was testing this because I was curious if the paging system could somehow globally sort the results but it makes sense that we can't do that, since that would require all results to be collated up front."
CASSANDRA-6700,Use real node messaging versions for schema exchange decisions,"IncomingTcpConnection#handleModernVersion sets version to min(my version, version of the peer). This messes up schema pull/push."
CASSANDRA-6699,NPE in migration stage on trunk,"Simple to reproduce, start a cluster and run legacy stress against it:

{noformat}
ERROR 12:56:12 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
 at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411) ~[main/:na]
 at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:281) ~[main/:na]
 at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:211) ~[main/:na]
 at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:105) ~[main/:na]
 at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:71) ~[main/:na]
 at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:180) ~[main/:na]
 at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:214) ~[main/:na]
 at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:204) ~[main/:na]
 at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1973) ~[main/:na]
 at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4486) ~[thrift/:na]
 at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4470) ~[thrift/:na]
 at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
 at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
 at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194) ~[main/:na]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
 at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
 at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.7.0_51]
 at java.util.concurrent.FutureTask.get(FutureTask.java:188) ~[na:1.7.0_51]
 at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407) ~[main/:na]
 ... 16 common frames omitted
Caused by: java.lang.NullPointerException: null
 at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167) ~[main/:na]
 at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:39) ~[main/:na]
 at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:26) ~[main/:na]
 at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:66) ~[main/:na]
 at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:150) ~[main/:na]
 at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:373) ~[main/:na]
 at org.apache.cassandra.config.CFMetaData.fromSchemaNoTriggers(CFMetaData.java:1712) ~[main/:na]
 at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1832) ~[main/:na]
 at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:320) ~[main/:na]
 at org.apache.cassandra.db.DefsTables.mergeColumnFamilies(DefsTables.java:306) ~[main/:na]
 at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:181) ~[main/:na]
 at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299) ~[main/:na]
 at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
 at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
 ... 3 common frames omitted
{noformat}"
CASSANDRA-6675,Debian packaging: Recognize oracle java7 packages,debian/control could be updated to allow building/running with the oracle java7 packages.
CASSANDRA-6655,Writing mostly deletes to a Memtable results in undercounting the table's occupancy so it may not flush,"In the extreme case of only deletes the memtable will never flush, and we will OOM."
CASSANDRA-6648,Race condition during node bootstrapping,"When bootstrapping a new node, data is ""missing"" as if the new node didn't actually bootstrap, which I tracked down to the following scenario:

1) New node joins token ring and waits for schema to be settled before actually bootstrapping.
2) The schema scheck somewhat passes and it starts bootstrapping.
3) Bootstrapping doesn't find the ks/cf that should have received from the other node.
4) Queries at this point cause NPEs, until when later they ""recover"" but data is missed.

The problem seems to be caused by a race condition between the migration manager and the bootstrapper, with the former running after the latter.
I think this is supposed to protect against such scenarios:
{noformat}
            while (!MigrationManager.isReadyForBootstrap())
            {
                setMode(Mode.JOINING, ""waiting for schema information to complete"", true);
                Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);
            }
{noformat}

But MigrationManager.isReadyForBootstrap() implementation is quite fragile and doesn't take into account ""slow"" schema propagation."
CASSANDRA-6645,upgradesstables causes NPE for secondary indexes without an underlying column family,"SecondaryIndex#getIndexCfs is allowed to return null by contract, if the index is not backed by a column family, but this causes an NPE as StorageService#getValidColumnFamilies and StorageService#upgradeSSTables do not check for null values."
CASSANDRA-6635,Rebuilding secondary indexes leaks SSTable references,"SecondaryIndex#buildIndexBlocking doesn't release SSTable references, which is generally no good, and most notably leaves sstable files hanging on the file system even after the column family is dropped."
CASSANDRA-6619,Race condition issue during upgrading 1.1 to 1.2,"There is a race condition during upgrading a C* 1.1x cluster to C* 1.2.
One issue is that OutboundTCPConnection can't establish from a 1.2 node to some 1.1x nodes.  Because of this, a live cluster during the upgrading will suffer in high read latency and be unable to fulfill some write requests.  It won't be a problem if there is a small cluster but it is a problem in a large cluster (100+ nodes) because the upgrading process takes 10+ hours to 1+ day(s) to complete.


Acknowledging about CASSANDRA-5692, however, it does not fully fix the issue.  We already have a patch for this and will attach shortly for feedback.


"
CASSANDRA-6614,"2 hours loop flushing+compacting system/{schema_keyspaces,schema_columnfamilies,schema_columns} when upgrading","It happens when we upgrade one node to 1.2.13 on a 1.2.2 cluster 
see http://pastebin.com/YZKUQLXz
If I grep for only InternalResponseStage logs I get http://pastebin.com/htnXZCiT which always displays same account of ops and serialized/live bytes per column family.
When I upgrade one node from 1.2.2 to 1.2.13, for 2h I get the previous messages with a raise of CPU (as it flushes and compacts continually) on all nodes http://picpaste.com/pics/Screen_Shot_2014-01-24_at_09.18.50-ggcCDVqd.1390587562.png
After that, everything is fine and I can upgrade other nodes without any raise of cpus load. when I start the upgrade, the more nodes I upgrade at the same time (at the beginning), the higher the cpu load is http://picpaste.com/pics/Screen_Shot_2014-01-23_at_17.45.56-I3fdEQ2T.1390587597.png"
CASSANDRA-6567,StackOverflowError with big IN list,"Cassandra throws StackOverflowError when binding big list in prepared query  in IN parameter

Stack trace:
java.lang.StackOverflowError
        at org.apache.cassandra.utils.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer.compareTo(FastByteComparisons.java:110)
        at org.apache.cassandra.utils.FastByteComparisons.compareTo(FastByteComparisons.java:41)
        at org.apache.cassandra.utils.FBUtilities.compareUnsigned(FBUtilities.java:216)
        at org.apache.cassandra.utils.ByteBufferUtil.compareUnsigned(ByteBufferUtil.java:89)
        at org.apache.cassandra.db.marshal.LongType.compareLongs(LongType.java:54)
        at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:36)
        at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:28)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.binarySearch(ArrayBackedSortedColumns.java:170)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.binarySearch(ArrayBackedSortedColumns.java:152)
        at org.apache.cassandra.db.ArrayBackedSortedColumns.getColumn(ArrayBackedSortedColumns.java:89)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:825)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:826)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:826)
        at org.apache.cassandra.cql3.statements.SelectStatement$1$1.computeNext(SelectStatement.java:826)
        at .... many more same line stack elements

Would be nice to change the logic to exclude manual paging in such cases"
CASSANDRA-6557,CommitLogSegment may be duplicated in unlikely race scenario,"In the unlikely event that the thread that switched to a new CLS has not finished executing the cleanup of its switch by the time the CLS has finished being used, it is possible for the same segment to be 'switched' in again. This would be benign except that it is added to the activeSegments queue a second time also, which would permit it to be recycled twice, creating two different CLS objects in memory pointing to the same CLS on disk, after which all bets are off.

The issue is highly unlikely to occur, but highly unlikely means it will probably happen eventually. I've fixed this based on my patch for CASSANDRA-5549, using the NonBlockingQueue I introduce there to simplify the logic and make it more obviously correct."
CASSANDRA-6530,Fix logback configuration in scripts and debian packaging for trunk/2.1,
CASSANDRA-6513,CqlPaging for hadoop fails when writetime(column_name) is set as one of the input columns,"InvalidRequestException thrown when running a hadoop job with CqlPaging with writetime(column_name) set as one of the input columns.

java.lang.RuntimeException
        at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader$RowIterator.executeQuery(CqlPagingRecordReader.java:665)
        at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader$RowIterator.<init>(CqlPagingRecordReader.java:301)
        at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader.initialize(CqlPagingRecordReader.java:167)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:522)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: InvalidRequestException(why:Undefined name WRITETIME(foo) in selection clause)
        at org.apache.cassandra.thrift.Cassandra$prepare_cql3_query_result.read(Cassandra.java:40395)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_prepare_cql3_query(Cassandra.java:1660)
        at org.apache.cassandra.thrift.Cassandra$Client.prepare_cql3_query(Cassandra.java:1646)
        at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader$RowIterator.prepareQuery(CqlPagingRecordReader.java:605)
        at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader$RowIterator.executeQuery(CqlPagingRecordReader.java:635)
        ... 10 more

The hadoop config lines look as such:
//read input from cassandra column family using CQL Input Format
        job.setInputFormatClass(CqlPagingInputFormat.class);
        ConfigHelper.setInputColumnFamily(job.getConfiguration(), KEYSPACE, INPUT_COLUMN_FAMILY);
        ConfigHelper.setInputRpcPort(job.getConfiguration(), ""9160"");
        ConfigHelper.setInputInitialAddress(job.getConfiguration(), HOST);
        ConfigHelper.setInputPartitioner(job.getConfiguration(), ""RandomPartitioner"");
        CqlConfigHelper.setInputColumns(job.getConfiguration(), ""foo,WRITETIME(foo)"");
        job.getConfiguration().set(""cassandra.consistencylevel.read"", READ_CONSISTENCY);
        CqlConfigHelper.setInputCQLPageRowSize(job.getConfiguration(), ""3000"");
"
CASSANDRA-6498,Null pointer exception in custom secondary indexes,"StorageProxy#estimateResultRowsPerRange raises a null pointer exception when using a custom 2i implementation that not uses a column family as underlying storage:
{code}
resultRowsPerRange = highestSelectivityIndex.getIndexCfs().getMeanColumns();
{code}
According to the documentation, the method SecondaryIndex#getIndexCfs should return null when no column family is used."
CASSANDRA-6497,Iterable CqlPagingRecordReader,The current CqlPagingRecordReader implementation provides a non-standard way of iterating over the underlying {{rowIterator}}. It would be nice to have an Iterable CqlPagingRecordReader like the one proposed in the attached diff.
CASSANDRA-6485,NPE in calculateNaturalEndpoints,"I was running a test where I added a new data center to an existing cluster. 

Test outline:
Start 25 Node DC1
Keyspace Setup Replication 3
Begin insert against DC1 Using Stress
While the inserts are occuring
Start up 25 Node DC2
Alter Keyspace to include Replication in 2nd DC
Run rebuild on DC2
Wait for stress to finish
Run repair on Cluster
... Some other operations

Although there are no issues with smaller clusters or clusters without vnodes, Larger setups with vnodes seem to consistently see the following exception in the logs as well as a write operation failing for each exception. Usually this happens between 1-8 times during an experiment. 

The exceptions/failures are Occurring when DC2 is brought online but *before* any alteration of the Keyspace. All of the exceptions are happening on DC1 nodes. One of the exceptions occurred on a seed node though this doesn't seem to be the case most of the time. 

While the test was running, nodetool was run every second to get cluster status. At no time did any nodes report themselves as down. 


{code}
ystem_logs-107.21.186.208/system.log-ERROR [Thrift:1] 2013-12-13 06:19:52,647 CustomTThreadPoolServer.java (line 217) Error occurred during processing of message.
system_logs-107.21.186.208/system.log:java.lang.NullPointerException
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:128)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2624)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:375)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:190)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:866)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:849)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:749)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3690)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3678)
system_logs-107.21.186.208/system.log-	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
system_logs-107.21.186.208/system.log-	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
system_logs-107.21.186.208/system.log-	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
system_logs-107.21.186.208/system.log-	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
system_logs-107.21.186.208/system.log-	at java.lang.Thread.run(Thread.java:724)
{code}"
CASSANDRA-6476,Assertion error in MessagingService.addCallback,"Two of the three Cassandra nodes in one of our clusters just started behaving very strange about an hour ago. Within a minute of each other they started logging AssertionErrors (see stack traces here: https://gist.github.com/iconara/7917438) over and over again. The client lost connection with the nodes at roughly the same time. The nodes were still up, and even if no clients were connected to them they continued logging the same errors over and over.

The errors are in the native transport (specifically MessagingService.addCallback) which makes me suspect that it has something to do with a test that we started running this afternoon. I've just implemented support for frame compression in my CQL driver cql-rb. About two hours before this happened I deployed a version of the application which enabled Snappy compression on all frames larger than 64 bytes. It's not impossible that there is a bug somewhere in the driver or compression library that caused this -- but at the same time, it feels like it shouldn't be possible to make C* a zombie with a bad frame.

Restarting seems to have got them back running again, but I suspect they will go down again sooner or later."
CASSANDRA-6464,Paging queries with IN on the partition key is broken,"Feels like MultiPartitionPager (which handles paging queries when there is a IN on the partition key) has completely missed CASSANDRA-5714's train. As a result, it completely broken and will typically loop infinitely.

Attaching patch to fix."
CASSANDRA-6461,CQLSSTableWriter throws NPE on addRow(Map values),"On 2.0.3 version CQLSSTableWriter throws NPE on method addRow ( Map<String, Object> values ) on line  159 :
{code:java}
rawValues.add(((AbstractType)spec.type).decompose(values.get(spec.name.toString())));
{code}

I think the solution is to copy the code of method   addRow(List<Object> values) because it has a control over null values :

{code:java}
133: rawValues.add(values.get(i) == null ? null :  (AbstractType)boundNames.get(i).type).decompose(values.get(i)));
{code}
"
CASSANDRA-6434,Repair-aware gc grace period,"Since the reason for gcgs is to ensure that we don't purge tombstones until every replica has been notified, it's redundant in a world where we're tracking repair times per sstable (and repairing frequentily), i.e., a world where we default to incremental repair a la CASSANDRA-5351.
"
CASSANDRA-6433,snapshot race with compaction causes missing link error,"Cassandra 1.2.11

When trying to snapshot, I encountered this error. It appears that snapshot doesn't lock the sstable list in a keyspace which can cause a race condition with compaction. (I think it's compaction, at least)

[cassandra@dev-cass00 ~]$ cas cluster snap pre-1.2.12
*** dev-cass01 (1) ***
 
Nodetool command ""snapshot -t pre-1.2.12"" failed!
 
Output:
 
Requested creating snapshot for: all keyspaces
Exception in thread ""main"" java.lang.RuntimeException: Tried to hard link to file that does not exist /data2/data-cassandra/csprocessor/csprocessor/csprocessor-csprocessor-ic-4-Summary.db
        at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
        at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1095)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1567)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1612)
        at org.apache.cassandra.db.Table.snapshot(Table.java:194)
        at org.apache.cassandra.service.StorageService.takeSnapshot(StorageService.java:2233)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
        at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)"
CASSANDRA-6431,Prevent same CF from being enqueued to flush more than once,"As things stand we can, in certain circumstances, fill up the flush queue with multiple requests to flush the same CF, which will lead to all writes blocking until the CF is flushed. Ideally we would only enqueue each CF/Memtable once and, if requested to be flushed whilst already enqueued, mark it to be requeued once the outstanding flush completes.

On a related note, a single table can already block writes if it has <flush queue size> or more secondary indexes. At the same time it might be worth deciding if this is also a problem and address it."
CASSANDRA-6417,Make flush single-pass when partition tombstones are present,"CASSANDRA-2589 added some code to make sure we don't write cell tombstones that are shadowed by a partition tombstone, by iterating over the CF object and removing them, before iterating again to write the remaining cells to disk.

We can simplify this at the same time as we generalize this to all shadowed cells."
CASSANDRA-6410,gossip memory usage improvement,"It looks to me that any given node will need ~2 MB of Java VM heap for each other node in the ring.  This was observed with num_tokens=512 but still seems excessive.
"
CASSANDRA-6405,"When making heavy use of counters, neighbor nodes occasionally enter spiral of constant memory consumpion","We're randomly running into an interesting issue on our ring. When making use of counters, we'll occasionally have 3 nodes (always neighbors) suddenly start immediately filling up memory, CMSing, fill up again, repeat. This pattern goes on for 5-20 minutes. Nearly all requests to the nodes time out during this period. Restarting one, two, or all three of the nodes does not resolve the spiral; after a restart the three nodes immediately start hogging up memory again and CMSing constantly.

When the issue resolves itself, all 3 nodes immediately get better. Sometimes it reoccurs in bursts, where it will be trashed for 20 minutes, fine for 5, trashed for 20, and repeat that cycle a few times.

There are no unusual logs provided by cassandra during this period of time, other than recording of the constant dropped read requests and the constant CMS runs. I have analyzed the log files prior to multiple distinct instances of this issue and have found no preceding events which are associated with this issue.

I have verified that our apps are not performing any unusual number or type of requests during this time.

This behaviour occurred on 1.0.12, 1.1.7, and now on 1.2.11.

The way I've narrowed this down to counters is a bit naive. It started happening when we started making use of counter columns, went away after we rolled back use of counter columns. I've repeated this attempted rollout on each version now, and it consistently rears its head every time. I should note this incident does _seem_ to happen more rarely on 1.2.11 compared to the previous versions.

This incident has been consistent across multiple different types of hardware, as well as major kernel version changes (2.6 all the way to 3.2). The OS is operating normally during the event.


I managed to get an hprof dump when the issue was happening in the wild. Something notable in the class instance counts as reported by jhat. Here are the top 5 counts for this one node:

{code}
5967846 instances of class org.apache.cassandra.db.CounterColumn 
1247525 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue 
1247310 instances of class org.apache.cassandra.cache.KeyCacheKey 
1246648 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node 
1237526 instances of class org.apache.cassandra.db.RowIndexEntry 
{code}

Is it normal or expected for CounterColumn to have that number of instances?

The data model for how we use counters is as follows: between 50-20000 counter columns per key. We currently have around 3 million keys total, but this issue also replicated when we only had a few thousand keys total. Average column count is around 1k, and 90th is 18k. New columns are added regularly, and columns are incremented regularly. No column or key deletions occur. We probably have 1-5k ""hot"" keys at any given time, spread across the entire ring. R:W ratio is typically around 50:1. This is the only CF we're using counters on, at this time. CF details are as follows:

{code}
    ColumnFamily: CommentTree
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.CounterColumnType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.01
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.LeveledCompactionStrategy
      Compaction Strategy Options:
        sstable_size_in_mb: 160



                Column Family: CommentTree
                SSTable count: 30
                SSTables in each level: [1, 10, 19, 0, 0, 0, 0, 0, 0]
                Space used (live): 4656930594
                Space used (total): 4677221791
                SSTable Compression Ratio: 0.0
                Number of Keys (estimate): 679680
                Memtable Columns Count: 8289
                Memtable Data Size: 2639908
                Memtable Switch Count: 5769
                Read Count: 185479324
                Read Latency: 1.786 ms.
                Write Count: 5377562
                Write Latency: 0.026 ms.
                Pending Tasks: 0
                Bloom Filter False Positives: 2914204
                Bloom Filter False Ratio: 0.56403
                Bloom Filter Space Used: 523952
                Compacted row minimum size: 30
                Compacted row maximum size: 4866323
                Compacted row mean size: 7742
                Average live cells per slice (last five minutes): 39.0
                Average tombstones per slice (last five minutes): 0.0

{code}


Please let me know if I can provide any further information. I can provide the hprof if desired, however it is 3GB so I'll need to provide it outside of JIRA."
CASSANDRA-6392,Cassandra 2.0.1 leaking open file handles for deleted system hints,"The system was running for a few days with a heavy write workload. The system crashed with too many open file handles. It seems the deleted system hint file handles are still open even though the file has been deleted on disk. We use a 3-node cluster with replication factor 3 on ec2 us-east-1 using NetworkTopologyStrategy. Default consistency level for write is ANY.

This is blocking our production deployment and causing instability in production. 

Appreciate your quick response.

Many thanks,
Prateek

See logs below:

[(bloomreach-ami) ubuntu@ip-10-12-33-23 :/mnt/cassandra/log]# lsof -p 13568 |wc -l
201499
[(bloomreach-ami) ubuntu@ip-10-12-33-23 :/mnt/cassandra/log]# lsof -p 13568 |grep deleted|wc -l
186261

java    13568 ubuntu *676r   REG                9,0         356 1610775839 /mnt/cassandra/data/system/hints/system-hints-jb-848-Data.db (deleted)
java    13568 ubuntu *677r   REG                9,0         309 1610770742 /mnt/cassandra/data/system/hints/system-hints-jb-845-Data.db (deleted)
java    13568 ubuntu *678r   REG                9,0         506 1610775703 /mnt/cassandra/data/system/hints/system-hints-jb-800-Data.db (deleted)


[(bloomreach-ami) ubuntu@ip-10-12-33-23 :/mnt/cassandra/log]# ls -l /mnt/cassandra/data/system/hints/system-hints-jb-800-Data.db
ls: cannot access /mnt/cassandra/data/system/hints/system-hints-jb-800-Data.db: No such file or directory

"
CASSANDRA-6359,sstableloader does not free off-heap memory for index summary,"Although sstableloader tells {{SSTableReaders}} to release their references to the {{IndexSummary}} objects, the summary's {{Memory}} is never {{free()}}'d, causing an off-heap memory leak."
CASSANDRA-6357,Flush memtables to separate directory,"Flush writers are a critical element for keeping a node healthy. When several compactions run on systems with low performing data directories, IO becomes a premium. Once the disk subsystem is saturated, write IO is blocked which will cause flush writer threads to backup. Since memtables are large blocks of memory in the JVM, too much blocking can cause excessive GC over time degrading performance. In the worst case causing an OOM.

Since compaction is running on the data directories. My proposal is to create a separate directory for flushing memtables. Potentially we can use the same methodology of keeping the commit log separate and minimize disk contention against the critical function of the flushwriter. "
CASSANDRA-6349,IOException in MessagingService.run() causes orphaned storage server socket,"The refactoring of reading the message header in MessagingService.run() vs IncomingTcpConnection seems to mishandle IOException as the loop is broken and MessagingService.SocketThread never seems to get reinitialized.

To reproduce: telnet to port 7000 and send random data. This then prevents any new or restarting node in the cluster from handshaking with this defunct storage port."
CASSANDRA-6346,Cassandra 2.0 server node runs out of memory during writes/replications,"Currently we are running 18 node cassandra cluster with NetworkTopologyReplication Strategy (d1 = 3 and d2=3).  

Our severs seem to crash with OOM exceptions. Our heap size is 8Gb. However while crashing i got hold of the hprof file and ran it through an eclipse MAT analyzer

After analyzing the hprof (please see attachment for top offenders), i find that there is a linked blocking queue (from mutation stage) that seems to hold about 7.3 Gb of the total 8Gb of ram. 

After deep diving into the cassandra2.0 code, i see that every update/write/replication goes through stages and mutation stage  and the no of threads that flush this queue (I am assuming memtable to sstable write) is controlled by concurrent writes. Ours is set to 32 concurrent writes

However we observe node crashes even when there are 0 writes to the node but replication requests are floating around the cluster. 

Any ideas what are the knobs to throttle the size of these queues/max no of write and replication requests a node can get? What are the recommended settings to operate cassandra node in a mode where it rejects requests beyond certain queue threshold?


"
CASSANDRA-6343,C* throws AssertionError when using paging and reverse ordering,"We have a table with CLUSTERING ORDER BY (date DESC). We try to do a query C* with paging and ORDER BY date ASC. 
This leads to the following exception in C* when pager goes to the last page:
{quote}
ERROR [Native-Transport-Requests:1287744] 2013-10-30 01:53:14,720 ErrorMessage.java (line 210) Unexpected exception during request
java.lang.AssertionError: Added column does not sort as the first column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:116)
        at org.apache.cassandra.service.pager.AbstractQueryPager.discardLast(AbstractQueryPager.java:238)
        at org.apache.cassandra.service.pager.AbstractQueryPager.discardLast(AbstractQueryPager.java:182)
        at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:100)
        at org.apache.cassandra.service.pager.SliceQueryPager.fetchPage(SliceQueryPager.java:33)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:179)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:235)
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:139)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:296)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{quote}

"
CASSANDRA-6308,Thread leak caused in creating OutboundTcpConnectionPool,"We have seen in one of our large clusters that there are many OutboundTcpConnection threads having the same names.  From a thread dump, OutboundTcpConnection threads have accounted for the largest shares of the total threads (65%+) and kept growing.

Here is a portion of a grep output for threads in which names start with ""WRITE-"":

""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00002aaac4022000 nid=0x2cb5 waiting on condition [0x00002acfbacda000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00002aaac42fe000 nid=0x2cb4 waiting on condition [0x00002acfbacad000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x0000000040840000 nid=0x2cb1 waiting on condition [0x00002acfbac80000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004083e000 nid=0x2cb0 waiting on condition [0x00002acfbac53000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004083b800 nid=0x2caf waiting on condition [0x00002acfbac26000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040839800 nid=0x2cae waiting on condition [0x00002acfbabf9000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040837800 nid=0x2cad waiting on condition [0x00002acfbabcc000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000404a3800 nid=0x2cac waiting on condition [0x00002acfbab9f000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x00000000404a1800 nid=0x2cab waiting on condition [0x00002acfbab72000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049f800 nid=0x2caa waiting on condition [0x00002acfbab45000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049e000 nid=0x2ca9 waiting on condition [0x00002acfbab18000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049c800 nid=0x2ca8 waiting on condition [0x00002acfbaaeb000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x000000004049a800 nid=0x2ca7 waiting on condition [0x00002acfbaabe000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040498800 nid=0x2ca6 waiting on condition [0x00002acfbaa91000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040496800 nid=0x2ca5 waiting on condition [0x00002acfbaa64000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040717800 nid=0x2ca4 waiting on condition [0x00002acfbaa37000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040716000 nid=0x2ca3 waiting on condition [0x00002acfbaa0a000]
""WRITE-/10.30.146.195"" daemon prio=10 tid=0x0000000040714800 nid=0x2ca2 waiting on condition [0x00002acfba9dd000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040712800 nid=0x2ca1 waiting on condition [0x00002acfba9b0000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040710800 nid=0x2ca0 waiting on condition [0x00002acfba983000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070e800 nid=0x2c9f waiting on condition [0x00002acfba956000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070d000 nid=0x2c9e waiting on condition [0x00002acfba929000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070b800 nid=0x2c9d waiting on condition [0x00002acfba8fc000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070a000 nid=0x2c9c waiting on condition [0x00002acfba8cf000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040827000 nid=0x2c9b waiting on condition [0x00002acfba8a2000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040825000 nid=0x2c9a waiting on condition [0x00002acfba875000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac488e000 nid=0x2c99 waiting on condition [0x00002acfba848000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040823000 nid=0x2c98 waiting on condition [0x00002acfba81b000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040821800 nid=0x2c97 waiting on condition [0x00002acfba7ee000]
""WRITE-/10.30.146.195"" daemon prio=10 tid=0x000000004081f000 nid=0x2c96 waiting on condition [0x00002acfba7c1000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004081d000 nid=0x2c95 waiting on condition [0x00002acfba794000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004081b000 nid=0x2c94 waiting on condition [0x00002acfba767000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac488b000 nid=0x2c93 waiting on condition [0x00002acfba73a000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040819000 nid=0x2c92 waiting on condition [0x00002acfba70d000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f9000 nid=0x2c91 waiting on condition [0x00002acfba6e0000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f7000 nid=0x2c90 waiting on condition [0x00002acfba6b3000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f5000 nid=0x2c8f waiting on condition [0x00002acfba686000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f3000 nid=0x2c8d waiting on condition [0x00002acfba659000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f1800 nid=0x2c8c waiting on condition [0x00002acfba62c000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000407ef000 nid=0x2c8b waiting on condition [0x00002acfba5ff000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000407ed800 nid=0x2c8a waiting on condition [0x00002acfba5d2000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407ec000 nid=0x2c89 waiting on condition [0x00002acfba5a5000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x00000000407e9800 nid=0x2c88 waiting on condition [0x00002acfba578000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f5000 nid=0x2c87 waiting on condition [0x00002acfba54b000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f3000 nid=0x2c86 waiting on condition [0x00002acfba51e000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f1000 nid=0x2c85 waiting on condition [0x00002acfba4f1000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ef000 nid=0x2c83 waiting on condition [0x00002acfba4c4000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ed800 nid=0x2c82 waiting on condition [0x00002acfba497000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405eb800 nid=0x2c81 waiting on condition [0x00002acfba46a000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ea000 nid=0x2c80 waiting on condition [0x00002acfba43d000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405e8800 nid=0x2c7f waiting on condition [0x00002acfba40f000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405e7800 nid=0x2c7e waiting on condition [0x00002acfba3e2000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040607000 nid=0x2c7d waiting on condition [0x00002acfba3b5000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040605800 nid=0x2c7c waiting on condition [0x00002acfba388000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x0000000040604000 nid=0x2c7b waiting on condition [0x00002acfba35b000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040602000 nid=0x2c7a waiting on condition [0x00002acfba32e000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ff800 nid=0x2c79 waiting on condition [0x00002acfba301000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fe000 nid=0x2c78 waiting on condition [0x00002acfba2d4000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fc000 nid=0x2c77 waiting on condition [0x00002acfba2a7000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fa800 nid=0x2c75 waiting on condition [0x00002acfba27a000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af9800 nid=0x2c74 waiting on condition [0x00002acfba24d000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af8000 nid=0x2c73 waiting on condition [0x00002acfba220000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040af6000 nid=0x2c72 waiting on condition [0x00002acfba1f3000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x0000000040af4000 nid=0x2c71 waiting on condition [0x00002acfba1c6000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af2000 nid=0x2c70 waiting on condition [0x00002acfba199000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040af0800 nid=0x2c6f waiting on condition [0x00002acfba16c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aef000 nid=0x2c6e waiting on condition [0x00002acfba13f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aed000 nid=0x2c6d waiting on condition [0x00002acfba112000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aeb800 nid=0x2c6b waiting on condition [0x00002acfba0b8000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac46b9000 nid=0x2c6a waiting on condition [0x00002acfba08b000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407b3000 nid=0x2c69 waiting on condition [0x00002acfba05e000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407b1800 nid=0x2c68 waiting on condition [0x00002acfba031000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407af800 nid=0x2c66 waiting on condition [0x00002acfba004000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407ae000 nid=0x2c65 waiting on condition [0x00002acfb9fd7000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407ab800 nid=0x2c64 waiting on condition [0x00002acfb9faa000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a9800 nid=0x2c63 waiting on condition [0x00002acfb9f7d000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a8000 nid=0x2c62 waiting on condition [0x00002acfb9f50000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a6800 nid=0x2c61 waiting on condition [0x00002acfb9f23000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408d2800 nid=0x2c60 waiting on condition [0x00002acfb9ef6000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408d1000 nid=0x2c5f waiting on condition [0x00002acfb9ec9000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cf800 nid=0x2c5d waiting on condition [0x00002acfb9e9c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cd800 nid=0x2c5c waiting on condition [0x00002acfb9e6f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cc000 nid=0x2c5b waiting on condition [0x00002acfb9e42000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004088d800 nid=0x2c5a waiting on condition [0x00002acfb9e15000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004088b000 nid=0x2c59 waiting on condition [0x00002acfb9de8000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040889000 nid=0x2c58 waiting on condition [0x00002acfb9dbb000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040887800 nid=0x2c57 waiting on condition [0x00002acfb9d8e000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040720000 nid=0x2c56 waiting on condition [0x00002acfb9d61000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004071f000 nid=0x2c55 waiting on condition [0x00002acfb9d34000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407c3000 nid=0x2c54 waiting on condition [0x00002acfb9d07000]
""WRITE-/10.78.95.30"" daemon prio=10 tid=0x00000000407c1800 nid=0x2c53 waiting on condition [0x00002acfb9cda000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407c0000 nid=0x2c52 waiting on condition [0x00002acfb9cac000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407be000 nid=0x2c51 waiting on condition [0x00002acfb9c7f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405cc000 nid=0x2c50 waiting on condition [0x00002acfb9c52000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405ca800 nid=0x2c4f waiting on condition [0x00002acfb9c24000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405c8800 nid=0x2c4e waiting on condition [0x00002acfb9bf7000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405c6800 nid=0x2c4d waiting on condition [0x00002acfb9bca000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac5010800 nid=0x2c4c waiting on condition [0x00002acfb9b9c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac4cd9800 nid=0x2c4b waiting on condition [0x00002acfb9b6f000]
""WRITE-/10.11.15.209"" daemon prio=10 tid=0x0000000040756800 nid=0x2c4a waiting on condition [0x00002acfb9b42000]
""WRITE-/10.11.15.209"" daemon prio=10 tid=0x0000000040754800 nid=0x2c49 waiting on condition [0x00002acfb9b15000]

 

We have patched this https://issues.apache.org/jira/browse/CASSANDRA-5175 but I don't this fix solves the issue totally.  I will attach  a patch soon. 

"
CASSANDRA-6302,make CqlPagingRecordReader more robust to failures,"CPPR currently bails if the first location fails for any reason, and generates invalid CQL if only the row key is specified in the column list."
CASSANDRA-6287,Avoid flushing compaction_history after each operation,
CASSANDRA-6281,Use Atomic*FieldUpdater to save memory,"Followup to CASSANDRA-6278, use Atomic*FieldUpdater in;
AtomicSortedColumns
ReadCallback
WriteResponseHandler
"
CASSANDRA-6278,Use AtomicIntegerFieldUpdater to save memory in row cache,"from this:
http://normanmaurer.me/blog/2013/10/28/Lesser-known-concurrent-classes-Part-1/

instead of having an AtomicInteger, we can keep a volatile int and use AtomicIntegerFieldUpdater to atomically update the value"
CASSANDRA-6275,2.0.x leaks file handles,"Looks like C* is leaking file descriptors when doing lots of CAS operations.

{noformat}
$ sudo cat /proc/15455/limits
Limit                     Soft Limit           Hard Limit           Units    
Max cpu time              unlimited            unlimited            seconds  
Max file size             unlimited            unlimited            bytes    
Max data size             unlimited            unlimited            bytes    
Max stack size            10485760             unlimited            bytes    
Max core file size        0                    0                    bytes    
Max resident set          unlimited            unlimited            bytes    
Max processes             1024                 unlimited            processes
Max open files            4096                 4096                 files    
Max locked memory         unlimited            unlimited            bytes    
Max address space         unlimited            unlimited            bytes    
Max file locks            unlimited            unlimited            locks    
Max pending signals       14633                14633                signals  
Max msgqueue size         819200               819200               bytes    
Max nice priority         0                    0                   
Max realtime priority     0                    0                   
Max realtime timeout      unlimited            unlimited            us 
{noformat}

Looks like the problem is not in limits.

Before load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
166

cassandra-test1 ~]$ lsof -n | grep java | wc -l
164

cassandra-test2 ~]$ lsof -n | grep java | wc -l
180
{noformat}

After load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
967

cassandra-test1 ~]$ lsof -n | grep java | wc -l
1766

cassandra-test2 ~]$ lsof -n | grep java | wc -l
2578
{noformat}

Most opened files have names like:
{noformat}
java      16890 cassandra 1636r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1637r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1638r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1639r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1640r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1641r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1642r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1643r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1644r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1645r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1646r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1647r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1648r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1649r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1650r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1651r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1652r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1653r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1654r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1655r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1656r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
{noformat}

Also, when that happens it's not always possible to shutdown server process via SIGTERM. Have to use SIGKILL.

p.s. See mailing thread for more context information https://www.mail-archive.com/user@cassandra.apache.org/msg33035.html"
CASSANDRA-6274,fixes for compacting larger-than-memory rows,
CASSANDRA-6266,Keyspace definition is leaked to users without SELECT permissions,"From CQLSH, a user without permissions on keyspaces can see a list of all keyspaces and get the keyspace definition.

{code}
$ ./cqlsh -u bob -p restricted
Connected to Test Cluster at localhost:9160.
[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> DESC KEYSPACES;

stress  system  schema1  customer_a  test  system_auth  system_traces

cqlsh> DESC KEYSPACE test;

CREATE KEYSPACE test WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE test;

CREATE TABLE data (
  assetid int,
  year int,
  field text,
  time bigint,
  value double,
  PRIMARY KEY ((assetid, year, field), time)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

cqlsh> USE test;
cqlsh:test> SELECT * FROM data LIMIT 10;
Bad Request: User bob has no SELECT permission on <table test.data> or any of its parents
cqlsh:test>
{code}"
CASSANDRA-6260,Cassandra 2.0.1 OutOfMemoryError:  Requested array size exceeds VM limit,"I am running cassandra 2.0.1 server with cascading client https://github.com/ifesdjeen/cascading-cassandra/ (v 1.0.0-rc6). I am running a problem on restarting one of the nodes in the cassandra cluster. All other nodes in the cluster started properly without any issues. I had originally assigned 8G of RAM to heap space. I tried starting the node with 12G of RAM but it still fails with the following error. This is currently blocking a production release so appreciate your quick response.

[(bloomreach-ami) ubuntu@ip-10-179-26-169 :/mnt/cassandra_latest]# ERROR 20:55:58,738 Exception encountered during startup
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:411)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:383)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:314)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:268)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:474)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:226)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:442)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:485)
 INFO 20:55:58,739 Initializing system.schema_triggers
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)

"
CASSANDRA-6257,Safety check on node joining cluster (based on last seen vs GC grace period) to avoid zombie data,"When a node is rejoining a cluster, it would be nice to have some form of safety check that the cluster recognises the last time the node was part of the cluster is greater that the GC grace period and therefore should not be able to rejoin the cluster unless the administrator specifically requests it.

The goal of this is to help avoid the potential issues with deleted data coming back from the rejoining nodes dataset.
"
CASSANDRA-6256,Gossip race condition can be missing HOST_ID,"A very rare and tight race of some sort can cause:

{noformat}
ERROR [GossipStage:1] 2013-10-26 00:48:32,071 CassandraDaemon.java (line 191) Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:696)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1388)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1257)
        at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1876)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:861)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:939)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}

It isn't immediately clear how this happens since we set HOST_ID before the gossiper even starts."
CASSANDRA-6255,Exception count not incremented on OutOfMemoryError (HSHA),"One of our nodes decided to stop listening on 9160 (netstat -l was showing nothing and telnet was reporting connection refused). Nodetool status showed no hosts down and on the offending node nodetool info gave the following:

{noformat}
nodetool info
Token            : (invoke with -T/--tokens to see all 256 tokens)
ID               : (removed)
Gossip active    : true
Thrift active    : true
Native Transport active: false
Load             : 2.05 TB
Generation No    : 1382536528
Uptime (seconds) : 432970
Heap Memory (MB) : 8098.05 / 14131.25
Data Center      : DC1
Rack             : RAC2
Exceptions       : 0
Key Cache        : size 536854996 (bytes), capacity 536870912 (bytes), 41383646 hits, 1710831591 requests, 0.024 recent hit rate, 0 save period in seconds
Row Cache        : size 0 (bytes), capacity 0 (bytes), 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
{noformat}

After looking at the cassandra log, I saw a bunch of the following:

{noformat}
ERROR [Selector-Thread-16] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
ERROR [Selector-Thread-7] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
{noformat}

There wasn't anything else overtly suspicious in the logs except for the occasional 
{noformat}
ERROR [Selector-Thread-0] 2013-10-27 17:35:58,662 TNonblockingServer.java (line 468) Read an invalid frame size of 0. Are you using TFramedTransport on the client side?
{noformat}
but  that periodically comes up - I have looked into it before but it has never seemed to have any serious impact.

This ticket is not about *why* an OutOfMemoryError occurred - which is bad but I don't think I have enough information to reproduce or speculate on a cause. This ticket is about the fact that an OutOfMemoryError occurred and nodetool info was reporting Thrift active : true and Exceptions : 0. 

Our monitoring systems and investigation processes are both starting to rely on on the exception count. The fact that it was not accurate here is disconcerting."
CASSANDRA-6249,Keep memtable data size updated even during flush; add a method to calculate total memtables size (incl pending flush),Keep memtable data size updated even during flush. Add a method to calculate total memtables size (including pending flush ones).
CASSANDRA-6228,"Add ""view trace session"" to cqlsh","It would be nice if cqlsh had a command to pass a tracing session id in, and have it print out the trace the same way it does when tracing is on."
CASSANDRA-6225,GCInspector should not wait after ConcurrentMarkSweep GC to flush memtables and reduce cache size,"In GCInspector.logGCResults, cassandra won't flush memtables and reduce Cache Sizes until there is a ConcurrentMarkSweep GC. It caused a long pause on the service. And other nodes could mark it as DEAD.

In our stress test, we were using 64 concurrent threads to write data to cassandra. The heap usage grew up quickly and reach to maximum.
We saw several ConcurrentMarkSweep GCs which only freed very few rams until a memtable flush was called. The other nodes marked the node as DOWN when GC took more than 20 seconds.
{code}
INFO [ScheduledTasks:1] 2013-10-18 15:42:36,176 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 27481 ms for 1 collections, 5229917848 used; max is 6358564864
 INFO [ScheduledTasks:1] 2013-10-18 15:43:14,013 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 27729 ms for 1 collections, 5381504752 used; max is 6358564864
 INFO [ScheduledTasks:1] 2013-10-18 15:43:50,565 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 29867 ms for 1 collections, 5479631256 used; max is 6358564864
 INFO [ScheduledTasks:1] 2013-10-18 15:44:23,457 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 28166 ms for 1 collections, 5545752344 used; max is 6358564864
 INFO [ScheduledTasks:1] 2013-10-18 15:44:58,290 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 29377 ms for 2 collections, 5343255456 used; max is 6358564864
{code}
{code}
INFO [GossipTasks:1] 2013-10-18 15:42:29,004 Gossiper.java (line 803) InetAddress /1.2.3.4 is now DOWN
 INFO [GossipTasks:1] 2013-10-18 15:43:06,901 Gossiper.java (line 803) InetAddress /1.2.3.4 is now DOWN
 INFO [GossipTasks:1] 2013-10-18 15:44:18,254 Gossiper.java (line 803) InetAddress /1.2.3.4 is now DOWN
 INFO [GossipTasks:1] 2013-10-18 15:44:48,507 Gossiper.java (line 803) InetAddress /1.2.3.4 is now DOWN
 INFO [GossipTasks:1] 2013-10-18 15:45:32,375 Gossiper.java (line 803) InetAddress /1.2.3.4 is now DOWN
{code}

We found two solutions to fix the long pause which result in a DOWN status.
1. We reduced the maximum ram to 3G. The behavior is the same, but gc was faster(under 20 seconds), so no nodes were marked as DOWN

2. Running a cronjob on the cassandra server which period call nodetool -h localhost flush.

Flush after a full gc just make thing worse and waste time spent on GC. In a heavily load system, you would have several full GCs before a flush can finish. (a flush may take more than 30 seconds)

Ideally, GCInspector should has a better logic on when to flush memtable. 
1. Flush memtable/reduce cache size when it reached the threshold(smaller than full gc threshold).
2. prevent frequently flush by remembering the last running time.

If we call flush before a full gc, then the full gc will release those rams occupied by memtable. Thus reduce the heap usage a lot. Otherwise, full gc will be called again and again until a flush was finished.

"
CASSANDRA-6223,SELECT timeouts due to stackoverflow on cassandra node,"I seems that I regularly experience a timeout when querying for a specific partition key in one of my tables. I can repro from datastax-java-driver and cqlsh. There isn't anything particularly interesting about this partition key; it has less than 100,000 rows with fixed size columns.

{code}
cqlsh> use my_keyspace ;
cqlsh:my_keyspace> CONSISTENCY LOCAL_QUORUM ;
Consistency level set to LOCAL_QUORUM.
cqlsh:my_keyspace> select * from my_table where s = 2047653 limit 100000000;
Request did not complete within rpc_timeout.

Tracing session: efd49ae0-3a41-11e3-9dcf-c74364da0bbf

 activity                                                          | timestamp    | source      | source_elapsed
-------------------------------------------------------------------+--------------+-------------+----------------
                                                execute_cql3_query | 04:14:22,228 | 10.12.34.48 |              0
 Parsing select * from my_table where s = 2047653 limit 100000000; | 04:14:22,229 | 10.12.34.48 |           1548
                                               Preparing statement | 04:14:22,230 | 10.12.34.48 |           2663
                          Enqueuing digest request to /10.12.34.40 | 04:14:22,231 | 10.12.34.48 |           3206
                      Executing single-partition query on my_table | 04:14:22,231 | 10.12.34.48 |           3336
                                      Acquiring sstable references | 04:14:22,231 | 10.12.34.48 |           3411
                                       Merging memtable tombstones | 04:14:22,231 | 10.12.34.48 |           3503
                                   Sending message to /10.12.34.40 | 04:14:22,231 | 10.12.34.48 |           3566
                        Bloom filter allows skipping sstable 38138 | 04:14:22,231 | 10.12.34.48 |           3598
                                   Key cache hit for sstable 38102 | 04:14:22,231 | 10.12.34.48 |           3692
                       Seeking to partition beginning in data file | 04:14:22,231 | 10.12.34.48 |           3710
                                   Key cache hit for sstable 37767 | 04:14:22,232 | 10.12.34.48 |           4265
                       Seeking to partition beginning in data file | 04:14:22,232 | 10.12.34.48 |           4285
                        Bloom filter allows skipping sstable 37220 | 04:14:22,232 | 10.12.34.48 |           4696
                        Merging data from memtables and 2 sstables | 04:14:22,232 | 10.12.34.48 |           4719
                                Message received from /10.12.34.48 | 04:14:22,234 | 10.12.34.40 |            101
                      Executing single-partition query on my_table | 04:14:22,236 | 10.12.34.40 |           2258
                                      Acquiring sstable references | 04:14:22,236 | 10.12.34.40 |           2410
                                       Merging memtable tombstones | 04:14:22,236 | 10.12.34.40 |           2528
                                   Key cache hit for sstable 39973 | 04:14:22,236 | 10.12.34.40 |           2622
                       Seeking to partition beginning in data file | 04:14:22,236 | 10.12.34.40 |           2686
                        Bloom filter allows skipping sstable 34597 | 04:14:22,237 | 10.12.34.40 |           3200
                        Bloom filter allows skipping sstable 26753 | 04:14:22,237 | 10.12.34.40 |           3259
                        Merging data from memtables and 1 sstables | 04:14:22,237 | 10.12.34.40 |           3281
                            Read 83652 live and 0 tombstoned cells | 04:14:23,204 | 10.12.34.40 |         970145
                                Enqueuing response to /10.12.34.48 | 04:14:23,324 | 10.12.34.40 |        1089946
                                   Sending message to /10.12.34.48 | 04:14:23,324 | 10.12.34.40 |        1090355
                                Message received from /10.12.34.40 | 04:14:23,325 | 10.12.34.48 |        1097153
                             Processing response from /10.12.34.40 | 04:14:23,325 | 10.12.34.48 |        1097500
               Timed out; received 1 of 2 responses (only digests) | 04:14:32,232 | 10.12.34.48 |       10003795
                                                  Request complete | 04:14:32,231 | 10.12.34.48 |       10003974
{code}

Stacktrace from 10.12.34.48:
{code}
ERROR [ReadStage:366027] 2013-10-21 04:14:22,468 (org.apache.cassandra.service.CassandraDaemon) Exception in thread Thread[ReadStage:366027,5,main]
java.lang.StackOverflowError
    at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:78)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:439)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
    at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
    at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
    at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
{code}"
CASSANDRA-6215,Possible space leak in datastax.driver.core,"I wrote a java benchmark app that uses CQL cassandra-driver-core:1.0.3  and repeatedly saves to column families using code like:
{noformat}
   final Insert writeReportInfo = QueryBuilder.insertInto(KEYSPACE_NAME, REPORT_INFO_TABLE_NAME).value(""type"",report.type.toString()).value(...) ...

    m_session.execute(writeReportInfo);
{noformat}
After running for about an hour, with -Xmx2000m, and writing about 20,000 reports (each with about 10000 rows), it got: java.lang.OutOfMemoryError: Java heap space.

Using jmap and jhat I can see that the objects taking up space are 
{noformat}
 Instance Counts for All Classes (excluding platform)
1657280 instances of class com.datastax.driver.core.ColumnDefinitions$Definition
31628 instances of class com.datastax.driver.core.ColumnDefinitions
31628 instances of class [Lcom.datastax.driver.core.ColumnDefinitions$Definition;
31627 instances of class com.datastax.driver.core.PreparedStatement
31627 instances of class org.apache.cassandra.utils.MD5Digest 
...
{noformat}"
CASSANDRA-6211,NPE in system.log,"I wrote a stresstest to test C* and my code that uses CAS heavily. I see strange exception messages in logs:
{noformat}
ERROR [MutationStage:320] 2013-10-17 13:59:10,710 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:320,5,main]
java.lang.NullPointerException
ERROR [MutationStage:328] 2013-10-17 13:59:10,718 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:328,5,main]
java.lang.NullPointerException
ERROR [MutationStage:327] 2013-10-17 13:59:10,732 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:327,5,main]
java.lang.NullPointerException
ERROR [MutationStage:325] 2013-10-17 13:59:10,750 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:325,5,main]
java.lang.NullPointerException
ERROR [MutationStage:326] 2013-10-17 13:59:10,762 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:326,5,main]
java.lang.NullPointerException
ERROR [MutationStage:330] 2013-10-17 13:59:10,768 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:330,5,main]
java.lang.NullPointerException
ERROR [MutationStage:331] 2013-10-17 13:59:10,775 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:331,5,main]
java.lang.NullPointerException
ERROR [MutationStage:334] 2013-10-17 13:59:10,789 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:334,5,main]
java.lang.NullPointerException
ERROR [MutationStage:329] 2013-10-17 13:59:10,803 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:329,5,main]
java.lang.NullPointerException
ERROR [MutationStage:335] 2013-10-17 13:59:10,812 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:335,5,main]
java.lang.NullPointerException
ERROR [MutationStage:333] 2013-10-17 13:59:10,826 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:333,5,main]
java.lang.NullPointerException
ERROR [MutationStage:332] 2013-10-17 13:59:10,834 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:332,5,main]
java.lang.NullPointerException
ERROR [MutationStage:337] 2013-10-17 13:59:10,842 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:337,5,main]
java.lang.NullPointerException
ERROR [MutationStage:336] 2013-10-17 13:59:10,859 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:336,5,main]
java.lang.NullPointerException
ERROR [MutationStage:338] 2013-10-17 13:59:10,870 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:338,5,main]
java.lang.NullPointerException
ERROR [MutationStage:339] 2013-10-17 13:59:10,884 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:339,5,main]
java.lang.NullPointerException
ERROR [MutationStage:341] 2013-10-17 13:59:10,894 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:341,5,main]
java.lang.NullPointerException
ERROR [MutationStage:340] 2013-10-17 13:59:10,910 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:340,5,main]
java.lang.NullPointerException
ERROR [MutationStage:344] 2013-10-17 13:59:10,920 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:344,5,main]
java.lang.NullPointerException
{noformat}"
CASSANDRA-6192,Ignore gc_grace when all replicas have ACKed the delete,"When a client issues a delete with a consistency level >= QUORUM, all replicas are contacted, even though the coordinator may return the answer before all responses arrive if QUORUM. Therefore, in the usual case when all replicas are alive, the coordinator will know when a delete has been ACKed by all replicas responsible for that data. In this situation I think it would be beneficial if the coordinator could notify the replicas that that tombstone is safe to purge on the next compaction, regardless of the gc_grace value.

This would make tombstones disappear much faster than they normally would.

"
CASSANDRA-6181,Replaying a commit led to java.lang.StackOverflowError and node crash,"2 of our nodes died after attempting to replay a commit.  I can attach the commit log file if that helps.
It was occurring on 1.2.8, after several failed attempts to start, we attempted startup with 1.2.10.  This also yielded the same issue (below).  The only resolution was to physically move the commit log file out of the way and then the nodes were able to start...  

The replication factor was 3 so I'm hoping there was no data loss...

{code}
 INFO [main] 2013-10-11 14:50:35,891 CommitLogReplayer.java (line 119) Replaying /ebs/cassandra/commitlog/CommitLog-2-1377542389560.log
ERROR [MutationStage:18] 2013-10-11 14:50:37,387 CassandraDaemon.java (line 191) Exception in thread Thread[MutationStage:18,5,main]
java.lang.StackOverflowError
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:68)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:57)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
        at org.apache.cassandra.db.marshal.AbstractType.compareCollectionMembers(AbstractType.java:229)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:81)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:439)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)

.... etc.... over and over until ....

        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:144)
        at org.apache.cassandra.db.RangeTombstoneList.addAll(RangeTombstoneList.java:186)
        at org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:180)
        at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:197)
        at org.apache.cassandra.db.AbstractColumnContainer.addAllWithSizeDelta(AbstractColumnContainer.java:99)
        at org.apache.cassandra.db.Memtable.resolve(Memtable.java:207)
        at org.apache.cassandra.db.Memtable.put(Memtable.java:170)
        at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:745)
        at org.apache.cassandra.db.Table.apply(Table.java:388)
        at org.apache.cassandra.db.Table.apply(Table.java:353)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer$1.runMayThrow(CommitLogReplayer.java:258)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}

"
CASSANDRA-6180,NPE in CqlRecordWriter: Related to AbstractCassandraStorage handling null values,"I encountered an issue with the {{CqlStorage}} and it's handling of null values. The {{CqlRecordWriter}} throws an NPE when a value is null. I found a related ticket CASSANDRA-5885 and applied the there stated fix to the {{AbstractCassandraStorage}}.
Instead of converting {{null}} values to {{ByteBuffer.wrap(new byte[0])}} {{AbstractCassandraStorage}} returns {{(ByteBuffer)null}}

This issue can be reproduced with the attached files: {{test_null.cql}}, {{test_null_data}}, {{null_test.pig}}

A fix can be found in the attached patch.

{code}
java.io.IOException: java.lang.NullPointerException
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:248)
Caused by: java.lang.NullPointerException
	at org.apache.thrift.protocol.TBinaryProtocol.writeBinary(TBinaryProtocol.java:194)
	at org.apache.cassandra.thrift.Cassandra$execute_prepared_cql3_query_args.write(Cassandra.java:41253)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)
	at org.apache.cassandra.thrift.Cassandra$Client.send_execute_prepared_cql3_query(Cassandra.java:1683)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_prepared_cql3_query(Cassandra.java:1673)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:232)
{code}"
CASSANDRA-6169,"Too many splits causes a ""OutOfMemoryError: unable to create new native thread"" in AbstractColumnFamilyInputFormat","The problem is caused by having 2300+ tokens due to vnodes.

In the client side I get this exception

{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:691)
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:943)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1336)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)
	at org.apache.cassandra.hadoop.AbstractColumnFamilyInputFormat.getSplits(AbstractColumnFamilyInputFormat.java:187)
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:1054)
	at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1071)
	at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:550)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:580)
	at com.relateiq.hadoop.cassandra.etl.CassandraETLJob.run(CassandraETLJob.java:58)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at com.relateiq.hadoop.cassandra.etl.CassandraETLJob.main(CassandraETLJob.java:149)
{code}

The problem seem to be in AbstractColumnFamilyInputFormat line ~180 which has an unbounded upper limit (actually it is Integer.MAX_INT)
{code}
ExecutorService executor = Executors.newCachedThreadPool();
{code}

Followed by:
{code}
            for (TokenRange range : masterRangeNodes)
            {
                if (jobRange == null)
                {
                    // for each range, pick a live owner and ask it to compute bite-sized splits
                    splitfutures.add(executor.submit(new SplitCallable(range, conf)));
                }
                else
                .....
{code}

which gets called one time per token and creates one thread just as many times.

The easy fix unless there is a longer term fix I'm unaware of would be to set an upper limit to the thread pool.

Something like this:
{code}
ExecutorService executor = new ThreadPoolExecutor(0, ConfigHelper.getMaxConcurrentSplitsResolution(), 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
{code}


Shall I proceed with a patch ?"
CASSANDRA-6151,CqlPagingRecorderReader Used when Partition Key Is Explicitly Stated,"From http://stackoverflow.com/questions/19189649/composite-key-in-cassandra-with-pig/19211546#19211546

The user was attempting to load a single partition using a where clause in a pig load statement. 

CQL Table
{code}
CREATE table data (
  occurday  text,
  seqnumber int,
  occurtimems bigint,
  unique bigint,

  fields map<text, text>,

  primary key ((occurday, seqnumber), occurtimems, unique)
)
{code}

Pig Load statement Query
{code}
data = LOAD 'cql://ks/data?where_clause=seqnumber%3D10%20AND%20occurday%3D%272013-10-01%27' USING CqlStorage();    
{code}

This results in an exception when processed by the the CqlPagingRecordReader which attempts to page this query even though it contains at most one partition key. This leads to an invalid CQL statement. 

CqlPagingRecordReader Query
{code}
SELECT * FROM ""data"" WHERE token(""occurday"",""seqnumber"") > ? AND
token(""occurday"",""seqnumber"") <= ? AND occurday='A Great Day' 
AND seqnumber=1 LIMIT 1000 ALLOW FILTERING
{code}
Exception
{code}
 InvalidRequestException(why:occurday cannot be restricted by more than one relation if it includes an Equal)
{code}
I'm not sure it is worth the special case but, a modification to not use the paging record reader when the entire partition key is specified would solve this issue. 

h3. Solution
 If it have EQUAL clauses for all the partitioning keys, we use Query 

{code}
  SELECT * FROM ""data"" 
  WHERE occurday='A Great Day' 
       AND seqnumber=1 LIMIT 1000 ALLOW FILTERING
{code}

instead of 
{code}
  SELECT * FROM ""data"" 
  WHERE token(""occurday"",""seqnumber"") > ? 
   AND token(""occurday"",""seqnumber"") <= ? 
   AND occurday='A Great Day' 
   AND seqnumber=1 LIMIT 1000 ALLOW FILTERING
{code}

The base line implementation is to retrieve all data of all rows around the ring. This new feature is to retrieve all data of a wide row. It's a one level lower than the base line. It helps for the use case where user is only interested in a specific wide row, so the user doesn't spend whole job to retrieve all the rows around the ring.



"
CASSANDRA-6125,Race condition in Gossip propagation,"Gossip propagation has a race when concurrent VersionedValues are created and submitted/propagated, causing some updates to be lost, even if happening on different ApplicationStatuses.
That's what happens basically:
1) A new VersionedValue V1 is created with version X.
2) A new VersionedValue V2 is created with version Y = X + 1.
3) V2 is added to the endpoint state map and propagated.
4) Nodes register Y as max version seen.
5) At this point, V1 is added to the endpoint state map and propagated too.
6) V1 version is X < Y, so nodes do not ask for his value after digests.

A possible solution would be to propagate/track per-ApplicationStatus versions, possibly encoding them to avoid network overhead."
CASSANDRA-6112,Memtable flushing should write any columns shadowed by partition/range tombstones if any 2i are present,"We shouldn't be dropping any columns obsoleted by partition and/or range tombstones in case the table has secondary indexes, or else the stale entries wouldn't be cleaned up during compaction, and will only be dropped during 2i query read-repair, if that happens."
CASSANDRA-6107,CQL3 Batch statement memory leak,"We are doing large volume insert/update tests on a CASS via CQL3. 


Using 4GB heap, after roughly 750,000 updates create/update 75,000 row keys, we run out of heap, and it never dissipates, and we begin getting this infamous error which many people seem to be encountering:

WARN [ScheduledTasks:1] 2013-09-26 16:17:10,752 GCInspector.java (line 142) Heap is 0.9383457210434385 full.  You may need to reduce memtable and/or cache sizes.  Cassandra will now flush up to the two largest memtables to free up memory.  Adjust flush_largest_memtables_at threshold in cassandra.yaml if you don't want Cassandra to do this automatically
 INFO [ScheduledTasks:1] 2013-09-26 16:17:10,753 StorageService.java (line 3614) Unable to reduce heap usage since there are no dirty column families


8 and 12 GB heaps appear to delay the problem by roughly proportionate amounts of 75,000 - 100,000 rowkeys per 4GB. Each run of 50,000 row key creations sees the heap grow and never shrink again. 

We have attempted to no effect:
- removing all secondary indexes to see if that alleviates overuse of bloom filters 
- adjusted parameters for compaction throughput
- adjusted memtable flush thresholds and other parameters 

By examining heapdumps, it seems apparent that the problem is perpetual retention of CQL3 BATCH statements. We have even tried dropping the keyspaces after the updates and the CQL3 statement are still visible in the heapdump, and after many many many CMS GC runs. G1 also showed this issue.

The 750,000 statements are broken into batches of roughly 200 statements."
CASSANDRA-6079,"Memtables flush is delayed when having a lot of batchlog activity, making node OOM","Both MeteredFlusher and BatchlogManager share the same OptionalTasks thread. So, when batchlog manager processes its tasks no flushes can occur. Even more, batchlog manager waits for batchlog CF compaction to finish.

On a lot of batchlog activity this prevents memtables from flush for a long time, making the node OOM.

Fixed this by moving batchlog to its own thread and not waiting for batchlog compaction to finish."
CASSANDRA-6077,SSTableMetadata.min(max)ColumnNames keep whole SlabAllocatior region referenced; wasting memory,".. which could be a problem when there is a lot of sstables, when using LCS for example.

SSTableWriter calls SSTableMetadata.Collector.updateMin(Max)ColumnNames passing List of ByteByffers which reference a small byte array in slab region.
ColumnNameHelper.mergeMin(Max) just returns a reference of column name back to SSTableMetadata. So the latter keeps whole slab region referenced, preventing it from being GCed. 

Fixed it by making copies of column name bytebuffer, if its size more than column name itself."
CASSANDRA-6072,NPE in Pig CassandraStorage,"key_alias can be null for tables created from thrift.

Which causes an NPE here:
https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/hadoop/pig/AbstractCassandraStorage.java#L633"
CASSANDRA-6059,Improve memory-use defaults,"Anecdotally, it's still too easy to OOM Cassandra even after moving sstable internals off heap."
CASSANDRA-6052,memory exhaustion,"Issuing queries such as ""select * from huge_table limit 1000000000"" or ""copy hugetable to ..."" reliably exhausts cassandra's heap space. (In cqlsh, at least.)

The JVM then get stuck in a Full GC loop, GC fails to free anything, cassandra is unresponsive and never recovers."
CASSANDRA-6047,Memory leak when using snapshot repairs,"Running nodetool repair repeatedly with the -snapshot parameter results in a native memory leak. The JVM process will take up more and more physical memory until it is killed by the Linux OOM killer.

The command used was as follows:

nodetool repair keyspace -local -snapshot -pr -st start_token -et end_token

Removing the -snapshot flag prevented the memory leak.  The subrange repair necessitated multiple repairs, so it made the problem noticeable, but I believe the problem would be reproducible even if you ran repair repeatedly without specifying a start and end token.

Notes from [~yukim]:

Probably the cause is too many snapshots. Snapshot sstables are opened during validation, but memories used are freed when releaseReferences called. But since snapshots never get marked compacted, memories never freed.

We only cleanup mmap'd memories when sstable is mark compacted. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/SSTableReader.java#L974

Validation compaction never marks snapshots compacted.
"
CASSANDRA-6040,Paging filter empty rows a bit too agressively,See the attached patch.
CASSANDRA-6029,Lightweight transactions race render primary key useless,"When multiple clients/threads do an UPDATE with a failed IF clause, then retry with a good IF clause, eventually updates with that primary key stop functioning.  This acts like a race condition and will not reproduce for me unless I have IF clauses that fail with an incorrect previous value, such as in an update race.  In my specific case, I hard coded my LWT retry logic's first UPDATE IF attempt to have always an incorrect previous value so that it would iterate and retry (see attached code.)  The second update (retry) would try with the ""returned current value"" and would generally win.  If this pattern was executed under load (jmeter to a test servlet with a lot of parallel requests), eventually I could not update the row with the PK I was using.  This was even the case in cqlsh.

The java driver complains about the following which I'm assuming is a red herring:
javax.ejb.EJBException: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1 ([/127.0.0.1] Unexpected exception triggered (org.apache.cassandra.transport.messages.ErrorMessage$WrappedException: org.apache.cassandra.transport.ProtocolException: Unknown code 8 for a consistency level)))

There is nothing printed to the cassandra console except for this:
 INFO 16:45:50,645 GC for ParNew: 224 ms for 1 collections, 66767104 used; max is 1046937600

And cqlsh ends up behaving like this in my 1 node 1 keyspace 1 replication_factor environment:

cqlsh:formula11> select last_value from id_pools where name='jae';

 last_value
------------
        261

(1 rows)

cqlsh:formula11> update id_pools set last_value=262 where name='jae' if last_value=261;
Request did not complete within rpc_timeout.
cqlsh:formula11> 

It is worth noting that other PKS continue to function in this id_pools table.  Please note that we only use these ""id pools"" for low volume required ascending ids and use UUIDs for other unique ids.

"
CASSANDRA-6026,NPE when running sstablesplit on valid sstable,"#create cluster
ccm create --cassandra-version git:cassandra-1.2 test
ccm populate -n 1
ccm start

#run stress
ccm node1 stress -n 10000000 -o insert
ccm node1 compact

cd ~/.ccm/test/node1/data
../bin/sstablesplit -n 100 ./Keyspace1/Standard1/Keyspace1-Standard1-ic-16-Data.db

#Expected
single large sstable should be split into multiple sstables with max size 100 MB

#Actual
ERROR 10:14:06,992 Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableDeletingTask.run(SSTableDeletingTask.java:70)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)


Notes: It seems like the split occurs and can be recompacted.
Last known commit where split was working on 1.2 branch: 47b2cd6620894bf0c4c4584036eab49a2e14a50e
Have not bisected further.

sstablesplit is also broken on 2.0 branch; however, it fails differently.  Filing separate bug on that.
"
CASSANDRA-6025,Deleted row resurrects if was not compacted in GC Grace Timeout due to thombstone read optimization in CollactionController,"How to reproduce:
1. Insert column
2. Flush, so you'll have sstable-1
3. Delete just inserted column
4. Flush, now you have sstable-2 as well
5. Left it uncompacted for more then gc grace time or just use 0, so you dont have to wait
6. Read data form column. You'll read just deleted column


{code}
            /* add the SSTables on disk */
// This sorts sstables in the order sstable-2, sstable-1
            Collections.sort(view.sstables, SSTable.maxTimestampComparator);
//...
            for (SSTableReader sstable : view.sstables)
            {
//...
                if (iter.getColumnFamily() != null)
//...
                    while (iter.hasNext())
                    {
                        OnDiskAtom atom = iter.next();
// the problem is here. reading atom after gc grace time
// makes this condition false. so tombstone from sstable-2
// is not placed to temp container and is just thrown away.
// On next iteration of outer for statement an original
// data inserted in step 1 from sstable-1 will be read and
// placed to temp.
                        if (atom.getLocalDeletionTime() >= gcBefore)
                            temp.addAtom(atom);
//
                    }

// .. so at the end of the for statemet we resolve data from temp. which
// do not have tombstone at all -> data are resurrected.
               container.addAll(temp, HeapAllocator.instance);
 
}
{code}
"
CASSANDRA-6016,Ability to change replication factor for the trace keyspace,"They trace keyspace is currently RF=1, and can't be changed.  I want to be able to trace stuff when nodes are down/being stupid."
CASSANDRA-6011,Race condition in snapshot repair,"When we do a snapshot/sequential repair, we use the repair session id as the snapshot name. Unfortunately in Directories.java when we delete a snapshot, we delete it for all column families, even when called on a specific cf store.

So what can happen is this:

Node B finishes validation compaction for CF1 and Notifies Node A
Node B *starts* to delete snapshot for CF1
Node A finishes repair of CF1 and starts repair of CF2
Node B takes snapshot of CF2 and starts validation compaction, but the previous validation compaction is still deleting snapshots, so the snapshot it wants to run a validation on gets deleted out from under it.

I've only reproduced on 1.2.6, but looking at the code this definitely looks like it exists in 1.2 HEAD. Not positive about 2.0.

I think the fix is just to update Directories.java to not delete the snapshot from all column families."
CASSANDRA-5989,java.lang.OutOfMemoryError: Requested array size exceeds VM limit,"This occurred in one of our nodes today. I don't have any helpful information on what is going on beforehand yet - logs don't have anything I could see that's tied for sure to it.

A few things happened in the logs beforehand. A little bit of standard GC, a bunch of status-logger entries 10 minutes before the crash, and a few nodes going up and down on the gossip.


ERROR [Thrift:7495] 2013-09-03 11:01:12,486 CassandraDaemon.java (line 192) Exception in thread Thread[Thrift:7495,5,main]
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at java.util.Arrays.copyOf(Arrays.java:2271)
        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)
        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)
        at org.apache.thrift.transport.TFramedTransport.write(TFramedTransport.java:146)
        at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:163)
        at org.apache.cassandra.thrift.TBinaryProtocol.writeBinary(TBinaryProtocol.java:69)
        at org.apache.cassandra.thrift.Column.write(Column.java:579)
        at org.apache.cassandra.thrift.CqlRow.write(CqlRow.java:439)
        at org.apache.cassandra.thrift.CqlResult.write(CqlResult.java:602)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result.write(Cassandra.java:37895)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:34)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)"
CASSANDRA-5982,OutOfMemoryError when writing text blobs to a very large number of tables,"This test goes outside the norm for Cassandra, creating ~2000 column families, and writing large text blobs to them. 

The process goes like this:

Bring up a 6 node m2.2xlarge cluster on EC2. This instance type has enough memory (34.2GB) so that Cassandra will allocate a full 8GB heap without tuning cassandra-env.sh. However, this instance type only has a single drive, so data and commitlog are comingled. (This test has also been run m1.xlarge instances which have four drives (but lower memory) and has exhibited similar results when assigning one to commitlog and 3 to datafile_directories.)

Use the 'memtable_allocator: HeapAllocator' setting from CASSANDRA-5935.

Create 2000 CFs:
{code}
CREATE KEYSPACE cf_stress WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}
CREATE COLUMNFAMILY cf_stress.tbl_00000 (id timeuuid PRIMARY KEY, val1 text, val2 text, val3 text ) ;
# repeat for tbl_00001, tbl_00002 ... tbl_02000
{code}

This process of creating tables takes a long time, about 5 hours, but for anyone wanting to create that many tables, presumably they only need to do this once, so this may be acceptable.

Write data:

The test dataset consists of writing 100K, 1M, and 10M documents to these tables:

{code}
INSERT INTO {table_name} (id, val1, val2, val3) VALUES (?, ?, ?, ?)
{code}

With 5 threads doing these inserts across the cluster, indefinitely, randomly choosing a table number 1-2000, the cluster eventually topples over with 'OutOfMemoryError: Java heap space'.

A heap dump analysis indicates that it's mostly memtables:

!2000CF_memtable_mem_usage.png!

Best current theory is that this is commitlog bound and that the memtables cannot flush fast enough due to locking issues. But I'll let [~jbellis] comment more on that.
"
CASSANDRA-5975,"Filtering on Secondary Index Takes a Long Time Even with Limit 1, Trace Log Filled with Looping Messages","After creating a table with 300,000 keys. Attempting to filter on a column with a secondary index causes an rpc timeout. Using a limit statement does not alleviate the problem. The tracing log appears to be filled with the same set of messages repeated over and over until the query times out. 

The data was created with the attached script and the command
{code}
python create_data.py --num-keys 300000 --num-columns 50 --keyspace 'ks' --columnfamily cf_300000_keys_50_cols --create-index y -v 3
{code}

The query causing the delay is
{code}
select * from cf_300000_keys_50_cols where color = 'green' limit 1;
{code}

An excerpt of the trace log
{code}
Tracing session: cedbead0-14d7-11e3-915e-999f6c86239a

 activity                                                                          | timestamp    | source       | source_elapsed
-----------------------------------------------------------------------------------+--------------+--------------+----------------
                                                                execute_cql3_query | 20:31:27,230 | 10.196.1.106 |              0
       Parsing select * from cf_300000_keys_50_cols where color = 'green' limit 1; | 20:31:27,230 | 10.196.1.106 |             31
                                                                Peparing statement | 20:31:27,230 | 10.196.1.106 |            219
                                                     Determining replicas to query | 20:31:27,230 | 10.196.1.106 |            563
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 20:31:27,232 | 10.196.1.106 |           1816
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,232 | 10.196.1.106 |           2036
                                                      Acquiring sstable references | 20:31:27,232 | 10.196.1.106 |           2201
                                                       Merging memtable tombstones | 20:31:27,232 | 10.196.1.106 |           2345
                                                       Key cache hit for sstable 3 | 20:31:27,232 | 10.196.1.106 |           2493
                                       Seeking to partition beginning in data file | 20:31:27,232 | 10.196.1.106 |           2555
                                                       Key cache hit for sstable 1 | 20:31:27,234 | 10.196.1.106 |           3742
                                       Seeking to partition beginning in data file | 20:31:27,234 | 10.196.1.106 |           3806
                                        Merging data from memtables and 2 sstables | 20:31:27,236 | 10.196.1.106 |           5805
                                                Read 3 live and 0 tombstoned cells | 20:31:27,236 | 10.196.1.106 |           5977
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,236 | 10.196.1.106 |           6166
                                                      Acquiring sstable references | 20:31:27,236 | 10.196.1.106 |           6319
                                                       Merging memtable tombstones | 20:31:27,236 | 10.196.1.106 |           6382
                                                       Key cache hit for sstable 3 | 20:31:27,236 | 10.196.1.106 |           6421
                                       Seeking to partition beginning in data file | 20:31:27,236 | 10.196.1.106 |           6423
                                            Bloom filter allows skipping sstable 2 | 20:31:27,237 | 10.196.1.106 |           7060
                                            Bloom filter allows skipping sstable 1 | 20:31:27,237 | 10.196.1.106 |           7218
                                        Merging data from memtables and 1 sstables | 20:31:27,237 | 10.196.1.106 |           7358
                                                Read 1 live and 0 tombstoned cells | 20:31:27,238 | 10.196.1.106 |           7644
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,238 | 10.196.1.106 |           7855
                                                      Acquiring sstable references | 20:31:27,238 | 10.196.1.106 |           8008
                                                       Merging memtable tombstones | 20:31:27,238 | 10.196.1.106 |           8072
                                            Bloom filter allows skipping sstable 3 | 20:31:27,238 | 10.196.1.106 |           8225
                                            Bloom filter allows skipping sstable 2 | 20:31:27,238 | 10.196.1.106 |           8284
                                                       Key cache hit for sstable 1 | 20:31:27,238 | 10.196.1.106 |           8367
                                       Seeking to partition beginning in data file | 20:31:27,238 | 10.196.1.106 |           8468
                                        Merging data from memtables and 1 sstables | 20:31:27,239 | 10.196.1.106 |           8968
                                                Read 1 live and 0 tombstoned cells | 20:31:27,239 | 10.196.1.106 |           9234
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,239 | 10.196.1.106 |           9405
                                                      Acquiring sstable references | 20:31:27,239 | 10.196.1.106 |           9547
                                                       Merging memtable tombstones | 20:31:27,240 | 10.196.1.106 |           9608
                                                       Key cache hit for sstable 3 | 20:31:27,240 | 10.196.1.106 |           9700
                                 Seeking to partition indexed section in data file | 20:31:27,240 | 10.196.1.106 |           9884
                                                       Key cache hit for sstable 1 | 20:31:27,240 | 10.196.1.106 |          10005
                                 Seeking to partition indexed section in data file | 20:31:27,240 | 10.196.1.106 |          10175
                                        Merging data from memtables and 2 sstables | 20:31:27,240 | 10.196.1.106 |          10323
                                                Read 3 live and 0 tombstoned cells | 20:31:27,249 | 10.196.1.106 |          19358
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,249 | 10.196.1.106 |          19516
                                                      Acquiring sstable references | 20:31:27,249 | 10.196.1.106 |          19580
                                                       Merging memtable tombstones | 20:31:27,250 | 10.196.1.106 |          19670
                                                       Key cache hit for sstable 3 | 20:31:27,250 | 10.196.1.106 |          19765
                                       Seeking to partition beginning in data file | 20:31:27,250 | 10.196.1.106 |          19884
                                            Bloom filter allows skipping sstable 2 | 20:31:27,250 | 10.196.1.106 |          20357
                                            Bloom filter allows skipping sstable 1 | 20:31:27,250 | 10.196.1.106 |          20514
                                        Merging data from memtables and 1 sstables | 20:31:27,250 | 10.196.1.106 |          20576
                                                Read 1 live and 0 tombstoned cells | 20:31:27,251 | 10.196.1.106 |          20864
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,251 | 10.196.1.106 |          21072
                                                      Acquiring sstable references | 20:31:27,251 | 10.196.1.106 |          21137
                                                       Merging memtable tombstones | 20:31:27,251 | 10.196.1.106 |          21315
                                                       Key cache hit for sstable 3 | 20:31:27,251 | 10.196.1.106 |          21461
                                 Seeking to partition indexed section in data file | 20:31:27,252 | 10.196.1.106 |          21599
                                                       Key cache hit for sstable 1 | 20:31:27,252 | 10.196.1.106 |          21761
                                 Seeking to partition indexed section in data file | 20:31:27,252 | 10.196.1.106 |          21909
                                        Merging data from memtables and 2 sstables | 20:31:27,252 | 10.196.1.106 |          21977
                                                Read 3 live and 0 tombstoned cells | 20:31:27,261 | 10.196.1.106 |          30678
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,261 | 10.196.1.106 |          31153
                                                      Acquiring sstable references | 20:31:27,261 | 10.196.1.106 |          31156
                                                       Merging memtable tombstones | 20:31:27,261 | 10.196.1.106 |          31221
                                                       Key cache hit for sstable 3 | 20:31:27,261 | 10.196.1.106 |          31259
                                       Seeking to partition beginning in data file | 20:31:27,261 | 10.196.1.106 |          31261
                                            Bloom filter allows skipping sstable 2 | 20:31:27,264 | 10.196.1.106 |          33808
                                            Bloom filter allows skipping sstable 1 | 20:31:27,264 | 10.196.1.106 |          33875
                                        Merging data from memtables and 1 sstables | 20:31:27,264 | 10.196.1.106 |          33877
                                                Read 1 live and 0 tombstoned cells | 20:31:27,264 | 10.196.1.106 |          34313
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,264 | 10.196.1.106 |          34488
                                                      Acquiring sstable references | 20:31:27,264 | 10.196.1.106 |          34552
                                                       Merging memtable tombstones | 20:31:27,265 | 10.196.1.106 |          34642
                                                       Key cache hit for sstable 3 | 20:31:27,265 | 10.196.1.106 |          34792
                                 Seeking to partition indexed section in data file | 20:31:27,265 | 10.196.1.106 |          34851
                                                       Key cache hit for sstable 1 | 20:31:27,265 | 10.196.1.106 |          35007
                                 Seeking to partition indexed section in data file | 20:31:27,265 | 10.196.1.106 |          35066
                                        Merging data from memtables and 2 sstables | 20:31:27,265 | 10.196.1.106 |          35272
                                                Read 3 live and 0 tombstoned cells | 20:31:27,274 | 10.196.1.106 |          44333
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,274 | 10.196.1.106 |          44529
                                                      Acquiring sstable references | 20:31:27,275 | 10.196.1.106 |          44686
                                                       Merging memtable tombstones | 20:31:27,275 | 10.196.1.106 |          44752
                                            Bloom filter allows skipping sstable 3 | 20:31:27,275 | 10.196.1.106 |          44766
                                            Bloom filter allows skipping sstable 2 | 20:31:27,275 | 10.196.1.106 |          45021
                                                       Key cache hit for sstable 1 | 20:31:27,275 | 10.196.1.106 |          45163
                                       Seeking to partition beginning in data file | 20:31:27,275 | 10.196.1.106 |          45241
                                        Merging data from memtables and 1 sstables | 20:31:27,276 | 10.196.1.106 |          45719
                                                Read 1 live and 0 tombstoned cells | 20:31:27,276 | 10.196.1.106 |          45985
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,276 | 10.196.1.106 |          46171
                                                      Acquiring sstable references | 20:31:27,276 | 10.196.1.106 |          46235
                                                       Merging memtable tombstones | 20:31:27,276 | 10.196.1.106 |          46325
                                                       Key cache hit for sstable 3 | 20:31:27,276 | 10.196.1.106 |          46501
                                 Seeking to partition indexed section in data file | 20:31:27,276 | 10.196.1.106 |          46561
                                                       Key cache hit for sstable 1 | 20:31:27,277 | 10.196.1.106 |          46652
                                 Seeking to partition indexed section in data file | 20:31:27,277 | 10.196.1.106 |          46853
                                        Merging data from memtables and 2 sstables | 20:31:27,277 | 10.196.1.106 |          46922
                                                Read 3 live and 0 tombstoned cells | 20:31:27,286 | 10.196.1.106 |          56025
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,286 | 10.196.1.106 |          56198
                                                      Acquiring sstable references | 20:31:27,286 | 10.196.1.106 |          56264
                                                       Merging memtable tombstones | 20:31:27,286 | 10.196.1.106 |          56352
                                            Bloom filter allows skipping sstable 3 | 20:31:27,286 | 10.196.1.106 |          56439
                                            Bloom filter allows skipping sstable 2 | 20:31:27,286 | 10.196.1.106 |          56543
                                                       Key cache hit for sstable 1 | 20:31:27,287 | 10.196.1.106 |          56631
                                       Seeking to partition beginning in data file | 20:31:27,287 | 10.196.1.106 |          56634
                                        Merging data from memtables and 1 sstables | 20:31:27,287 | 10.196.1.106 |          57194
                                                Read 1 live and 0 tombstoned cells | 20:31:27,287 | 10.196.1.106 |          57494
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,288 | 10.196.1.106 |          57672
                                                      Acquiring sstable references | 20:31:27,288 | 10.196.1.106 |          57736
                                                       Merging memtable tombstones | 20:31:27,288 | 10.196.1.106 |          57922
                                                       Key cache hit for sstable 3 | 20:31:27,288 | 10.196.1.106 |          58047
                                 Seeking to partition indexed section in data file | 20:31:27,288 | 10.196.1.106 |          58200
                                                       Key cache hit for sstable 1 | 20:31:27,288 | 10.196.1.106 |          58351
                                 Seeking to partition indexed section in data file | 20:31:27,289 | 10.196.1.106 |          58783
                                        Merging data from memtables and 2 sstables | 20:31:27,289 | 10.196.1.106 |          58790
                                                Read 3 live and 0 tombstoned cells | 20:31:27,299 | 10.196.1.106 |          69455
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,300 | 10.196.1.106 |          69641
                                                      Acquiring sstable references | 20:31:27,300 | 10.196.1.106 |          69707
                                                       Merging memtable tombstones | 20:31:27,300 | 10.196.1.106 |          69713
                                            Bloom filter allows skipping sstable 3 | 20:31:27,300 | 10.196.1.106 |          69806
                                            Bloom filter allows skipping sstable 2 | 20:31:27,300 | 10.196.1.106 |          69904
                                                       Key cache hit for sstable 1 | 20:31:27,300 | 10.196.1.106 |          69994
                                       Seeking to partition beginning in data file | 20:31:27,300 | 10.196.1.106 |          70297
                                        Merging data from memtables and 1 sstables | 20:31:27,301 | 10.196.1.106 |          70778
                                                Read 1 live and 0 tombstoned cells | 20:31:27,301 | 10.196.1.106 |          71044
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,301 | 10.196.1.106 |          71271
                                                      Acquiring sstable references | 20:31:27,301 | 10.196.1.106 |          71335
                                                       Merging memtable tombstones | 20:31:27,301 | 10.196.1.106 |          71424
                                                       Key cache hit for sstable 3 | 20:31:27,301 | 10.196.1.106 |          71583
                                 Seeking to partition indexed section in data file | 20:31:27,302 | 10.196.1.106 |          71645
                                                       Key cache hit for sstable 1 | 20:31:27,302 | 10.196.1.106 |          71882
                                 Seeking to partition indexed section in data file | 20:31:27,302 | 10.196.1.106 |          71940
                                        Merging data from memtables and 2 sstables | 20:31:27,302 | 10.196.1.106 |          72030
                                                Read 3 live and 0 tombstoned cells | 20:31:27,311 | 10.196.1.106 |          81217
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,311 | 10.196.1.106 |          81369
                                                      Acquiring sstable references | 20:31:27,311 | 10.196.1.106 |          81515
                                                       Merging memtable tombstones | 20:31:27,312 | 10.196.1.106 |          81661
                                                       Key cache hit for sstable 3 | 20:31:27,312 | 10.196.1.106 |          81812
                                       Seeking to partition beginning in data file | 20:31:27,312 | 10.196.1.106 |          81873
                                            Bloom filter allows skipping sstable 2 | 20:31:27,312 | 10.196.1.106 |          82409
                                            Bloom filter allows skipping sstable 1 | 20:31:27,312 | 10.196.1.106 |          82479
                                        Merging data from memtables and 1 sstables | 20:31:27,312 | 10.196.1.106 |          82481
                                                Read 1 live and 0 tombstoned cells | 20:31:27,313 | 10.196.1.106 |          82860
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,313 | 10.196.1.106 |          83035
                                                      Acquiring sstable references | 20:31:27,313 | 10.196.1.106 |          83099
                                                       Merging memtable tombstones | 20:31:27,313 | 10.196.1.106 |          83217
                                                       Key cache hit for sstable 3 | 20:31:27,313 | 10.196.1.106 |          83307
                                 Seeking to partition indexed section in data file | 20:31:27,313 | 10.196.1.106 |          83410
                                                       Key cache hit for sstable 1 | 20:31:27,314 | 10.196.1.106 |          83588
                                 Seeking to partition indexed section in data file | 20:31:27,314 | 10.196.1.106 |          83652
                                        Merging data from memtables and 2 sstables | 20:31:27,314 | 10.196.1.106 |          83742
                                                Read 3 live and 0 tombstoned cells | 20:31:27,338 | 10.196.1.106 |         108372
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,345 | 10.196.1.106 |         115180
                                                      Acquiring sstable references | 20:31:27,345 | 10.196.1.106 |         115183
                                                       Merging memtable tombstones | 20:31:27,345 | 10.196.1.106 |         115188
                                            Bloom filter allows skipping sstable 3 | 20:31:27,345 | 10.196.1.106 |         115194
                                            Bloom filter allows skipping sstable 2 | 20:31:27,345 | 10.196.1.106 |         115197
                                                       Key cache hit for sstable 1 | 20:31:27,345 | 10.196.1.106 |         115203
                                       Seeking to partition beginning in data file | 20:31:27,345 | 10.196.1.106 |         115205
                                        Merging data from memtables and 1 sstables | 20:31:27,347 | 10.196.1.106 |         116821
                                                Read 1 live and 0 tombstoned cells | 20:31:27,348 | 10.196.1.106 |         117631
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,348 | 10.196.1.106 |         117997
                                                      Acquiring sstable references | 20:31:27,348 | 10.196.1.106 |         118000
                                                       Merging memtable tombstones | 20:31:27,348 | 10.196.1.106 |         118007
                                                       Key cache hit for sstable 3 | 20:31:27,348 | 10.196.1.106 |         118014
                                 Seeking to partition indexed section in data file | 20:31:27,348 | 10.196.1.106 |         118017
                                                       Key cache hit for sstable 1 | 20:31:27,348 | 10.196.1.106 |         118023
                                 Seeking to partition indexed section in data file | 20:31:27,348 | 10.196.1.106 |         118025
                                        Merging data from memtables and 2 sstables | 20:31:27,348 | 10.196.1.106 |         118030
                                                Read 3 live and 0 tombstoned cells | 20:31:27,356 | 10.196.1.106 |         126375
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,356 | 10.196.1.106 |         126418
                                                      Acquiring sstable references | 20:31:27,356 | 10.196.1.106 |         126421
                                                       Merging memtable tombstones | 20:31:27,356 | 10.196.1.106 |         126427
                                            Bloom filter allows skipping sstable 3 | 20:31:27,357 | 10.196.1.106 |         126695
                                            Bloom filter allows skipping sstable 2 | 20:31:27,357 | 10.196.1.106 |         126699
                                                       Key cache hit for sstable 1 | 20:31:27,357 | 10.196.1.106 |         126705
                                       Seeking to partition beginning in data file | 20:31:27,357 | 10.196.1.106 |         126708
                                        Merging data from memtables and 1 sstables | 20:31:27,357 | 10.196.1.106 |         127265
                                                Read 1 live and 0 tombstoned cells | 20:31:27,357 | 10.196.1.106 |         127519
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,358 | 10.196.1.106 |         127686
                                                      Acquiring sstable references | 20:31:27,358 | 10.196.1.106 |         127694
                                                       Merging memtable tombstones | 20:31:27,358 | 10.196.1.106 |         127700
                                                       Key cache hit for sstable 3 | 20:31:27,358 | 10.196.1.106 |         127708
                                 Seeking to partition indexed section in data file | 20:31:27,358 | 10.196.1.106 |         127710
                                                       Key cache hit for sstable 1 | 20:31:27,358 | 10.196.1.106 |         127716
                                 Seeking to partition indexed section in data file | 20:31:27,358 | 10.196.1.106 |         127718
                                        Merging data from memtables and 2 sstables | 20:31:27,358 | 10.196.1.106 |         127722
                                                Read 3 live and 0 tombstoned cells | 20:31:27,366 | 10.196.1.106 |         135976
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,366 | 10.196.1.106 |         136018
                                                      Acquiring sstable references | 20:31:27,366 | 10.196.1.106 |         136020
                                                       Merging memtable tombstones | 20:31:27,366 | 10.196.1.106 |         136026
                                            Bloom filter allows skipping sstable 3 | 20:31:27,366 | 10.196.1.106 |         136032
                                            Bloom filter allows skipping sstable 2 | 20:31:27,366 | 10.196.1.106 |         136035
                                                       Key cache hit for sstable 1 | 20:31:27,366 | 10.196.1.106 |         136040
                                       Seeking to partition beginning in data file | 20:31:27,366 | 10.196.1.106 |         136043
                                        Merging data from memtables and 1 sstables | 20:31:27,367 | 10.196.1.106 |         136852
                                                Read 1 live and 0 tombstoned cells | 20:31:27,367 | 10.196.1.106 |         137046
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,367 | 10.196.1.106 |         137154
                                                      Acquiring sstable references | 20:31:27,367 | 10.196.1.106 |         137159
                                                       Merging memtable tombstones | 20:31:27,367 | 10.196.1.106 |         137165
                                                       Key cache hit for sstable 3 | 20:31:27,367 | 10.196.1.106 |         137172
                                 Seeking to partition indexed section in data file | 20:31:27,367 | 10.196.1.106 |         137175
                                                       Key cache hit for sstable 1 | 20:31:27,367 | 10.196.1.106 |         137181
                                 Seeking to partition indexed section in data file | 20:31:27,367 | 10.196.1.106 |         137183
                                        Merging data from memtables and 2 sstables | 20:31:27,367 | 10.196.1.106 |         137187
                                                Read 3 live and 0 tombstoned cells | 20:31:27,375 | 10.196.1.106 |         145161
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,375 | 10.196.1.106 |         145195
                                                      Acquiring sstable references | 20:31:27,375 | 10.196.1.106 |         145198
                                                       Merging memtable tombstones | 20:31:27,375 | 10.196.1.106 |         145203
                                                       Key cache hit for sstable 3 | 20:31:27,375 | 10.196.1.106 |         145212
                                       Seeking to partition beginning in data file | 20:31:27,375 | 10.196.1.106 |         145214
                                            Bloom filter allows skipping sstable 2 | 20:31:27,376 | 10.196.1.106 |         146059
                                            Bloom filter allows skipping sstable 1 | 20:31:27,376 | 10.196.1.106 |         146063
                                        Merging data from memtables and 1 sstables | 20:31:27,376 | 10.196.1.106 |         146066
                                                Read 1 live and 0 tombstoned cells | 20:31:27,376 | 10.196.1.106 |         146295
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,376 | 10.196.1.106 |         146416
                                                      Acquiring sstable references | 20:31:27,376 | 10.196.1.106 |         146419
                                                       Merging memtable tombstones | 20:31:27,376 | 10.196.1.106 |         146425
                                                       Key cache hit for sstable 3 | 20:31:27,376 | 10.196.1.106 |         146433
                                 Seeking to partition indexed section in data file | 20:31:27,376 | 10.196.1.106 |         146435
                                                       Key cache hit for sstable 1 | 20:31:27,376 | 10.196.1.106 |         146441
                                 Seeking to partition indexed section in data file | 20:31:27,376 | 10.196.1.106 |         146443
                                        Merging data from memtables and 2 sstables | 20:31:27,376 | 10.196.1.106 |         146447
                                                Read 3 live and 0 tombstoned cells | 20:31:27,384 | 10.196.1.106 |         153664
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,384 | 10.196.1.106 |         153708
                                                      Acquiring sstable references | 20:31:27,384 | 10.196.1.106 |         153711
                                                       Merging memtable tombstones | 20:31:27,384 | 10.196.1.106 |         153717
                                                       Key cache hit for sstable 3 | 20:31:27,384 | 10.196.1.106 |         153899
                                       Seeking to partition beginning in data file | 20:31:27,384 | 10.196.1.106 |         153902
                                            Bloom filter allows skipping sstable 2 | 20:31:27,384 | 10.196.1.106 |         154556
                                            Bloom filter allows skipping sstable 1 | 20:31:27,384 | 10.196.1.106 |         154566
                                        Merging data from memtables and 1 sstables | 20:31:27,384 | 10.196.1.106 |         154568
                                                Read 1 live and 0 tombstoned cells | 20:31:27,385 | 10.196.1.106 |         155041
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,385 | 10.196.1.106 |         155112
                                                      Acquiring sstable references | 20:31:27,385 | 10.196.1.106 |         155156
                                                       Merging memtable tombstones | 20:31:27,385 | 10.196.1.106 |         155163
                                                       Key cache hit for sstable 3 | 20:31:27,385 | 10.196.1.106 |         155171
                                 Seeking to partition indexed section in data file | 20:31:27,385 | 10.196.1.106 |         155173
                                                       Key cache hit for sstable 1 | 20:31:27,385 | 10.196.1.106 |         155179
                                 Seeking to partition indexed section in data file | 20:31:27,385 | 10.196.1.106 |         155181
                                        Merging data from memtables and 2 sstables | 20:31:27,385 | 10.196.1.106 |         155185
                                                Read 3 live and 0 tombstoned cells | 20:31:27,393 | 10.196.1.106 |         163321
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,393 | 10.196.1.106 |         163365
                                                      Acquiring sstable references | 20:31:27,393 | 10.196.1.106 |         163367
                                                       Merging memtable tombstones | 20:31:27,393 | 10.196.1.106 |         163373
                                            Bloom filter allows skipping sstable 3 | 20:31:27,393 | 10.196.1.106 |         163379
                                            Bloom filter allows skipping sstable 2 | 20:31:27,393 | 10.196.1.106 |         163382
                                                       Key cache hit for sstable 1 | 20:31:27,393 | 10.196.1.106 |         163387
                                       Seeking to partition beginning in data file | 20:31:27,393 | 10.196.1.106 |         163389
                                        Merging data from memtables and 1 sstables | 20:31:27,394 | 10.196.1.106 |         164206
                                                Read 1 live and 0 tombstoned cells | 20:31:27,394 | 10.196.1.106 |         164397
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,394 | 10.196.1.106 |         164562
                                                      Acquiring sstable references | 20:31:27,394 | 10.196.1.106 |         164570
                                                       Merging memtable tombstones | 20:31:27,394 | 10.196.1.106 |         164576
                                                       Key cache hit for sstable 3 | 20:31:27,394 | 10.196.1.106 |         164584
                                 Seeking to partition indexed section in data file | 20:31:27,394 | 10.196.1.106 |         164587
                                                       Key cache hit for sstable 1 | 20:31:27,395 | 10.196.1.106 |         164593
                                 Seeking to partition indexed section in data file | 20:31:27,395 | 10.196.1.106 |         164595
                                        Merging data from memtables and 2 sstables | 20:31:27,395 | 10.196.1.106 |         164599
                                                Read 3 live and 0 tombstoned cells | 20:31:27,403 | 10.196.1.106 |         172818
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,403 | 10.196.1.106 |         172860
                                                      Acquiring sstable references | 20:31:27,403 | 10.196.1.106 |         172862
                                                       Merging memtable tombstones | 20:31:27,403 | 10.196.1.106 |         172868
                                            Bloom filter allows skipping sstable 3 | 20:31:27,403 | 10.196.1.106 |         172967
                                            Bloom filter allows skipping sstable 2 | 20:31:27,403 | 10.196.1.106 |         172971
                                                       Key cache hit for sstable 1 | 20:31:27,403 | 10.196.1.106 |         172979
                                       Seeking to partition beginning in data file | 20:31:27,403 | 10.196.1.106 |         172982
                                        Merging data from memtables and 1 sstables | 20:31:27,404 | 10.196.1.106 |         173735
                                                Read 1 live and 0 tombstoned cells | 20:31:27,404 | 10.196.1.106 |         173923
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,404 | 10.196.1.106 |         174088
                                                      Acquiring sstable references | 20:31:27,404 | 10.196.1.106 |         174096
                                                       Merging memtable tombstones | 20:31:27,404 | 10.196.1.106 |         174102
                                                       Key cache hit for sstable 3 | 20:31:27,404 | 10.196.1.106 |         174110
                                 Seeking to partition indexed section in data file | 20:31:27,404 | 10.196.1.106 |         174112
                                                       Key cache hit for sstable 1 | 20:31:27,404 | 10.196.1.106 |         174119
                                 Seeking to partition indexed section in data file | 20:31:27,404 | 10.196.1.106 |         174121
                                        Merging data from memtables and 2 sstables | 20:31:27,404 | 10.196.1.106 |         174124
                                                Read 3 live and 0 tombstoned cells | 20:31:27,414 | 10.196.1.106 |         183917
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,414 | 10.196.1.106 |         183959
                                                      Acquiring sstable references | 20:31:27,414 | 10.196.1.106 |         183961
                                                       Merging memtable tombstones | 20:31:27,414 | 10.196.1.106 |         183967
                                            Bloom filter allows skipping sstable 3 | 20:31:27,414 | 10.196.1.106 |         183973
                                            Bloom filter allows skipping sstable 2 | 20:31:27,414 | 10.196.1.106 |         183976
                                                       Key cache hit for sstable 1 | 20:31:27,414 | 10.196.1.106 |         183982
                                       Seeking to partition beginning in data file | 20:31:27,414 | 10.196.1.106 |         183984
                                        Merging data from memtables and 1 sstables | 20:31:27,415 | 10.196.1.106 |         184807
                                                Read 1 live and 0 tombstoned cells | 20:31:27,415 | 10.196.1.106 |         184994
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,415 | 10.196.1.106 |         185105
                                                      Acquiring sstable references | 20:31:27,415 | 10.196.1.106 |         185108
                                                       Merging memtable tombstones | 20:31:27,415 | 10.196.1.106 |         185114
                                                       Key cache hit for sstable 3 | 20:31:27,415 | 10.196.1.106 |         185134
                                 Seeking to partition indexed section in data file | 20:31:27,415 | 10.196.1.106 |         185136
                                                       Key cache hit for sstable 1 | 20:31:27,415 | 10.196.1.106 |         185142
                                 Seeking to partition indexed section in data file | 20:31:27,415 | 10.196.1.106 |         185144
                                        Merging data from memtables and 2 sstables | 20:31:27,415 | 10.196.1.106 |         185148
                                                Read 3 live and 0 tombstoned cells | 20:31:27,423 | 10.196.1.106 |         193586
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,424 | 10.196.1.106 |         193635
                                                      Acquiring sstable references | 20:31:27,424 | 10.196.1.106 |         193638
                                                       Merging memtable tombstones | 20:31:27,424 | 10.196.1.106 |         193643
                                            Bloom filter allows skipping sstable 3 | 20:31:27,424 | 10.196.1.106 |         193649
                                            Bloom filter allows skipping sstable 2 | 20:31:27,424 | 10.196.1.106 |         193652
                                                       Key cache hit for sstable 1 | 20:31:27,424 | 10.196.1.106 |         193657
                                       Seeking to partition beginning in data file | 20:31:27,424 | 10.196.1.106 |         193660
                                        Merging data from memtables and 1 sstables | 20:31:27,424 | 10.196.1.106 |         194274
                                                Read 1 live and 0 tombstoned cells | 20:31:27,424 | 10.196.1.106 |         194483
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,425 | 10.196.1.106 |         194597
                                                      Acquiring sstable references | 20:31:27,425 | 10.196.1.106 |         194605
                                                       Merging memtable tombstones | 20:31:27,425 | 10.196.1.106 |         194611
                                                       Key cache hit for sstable 3 | 20:31:27,425 | 10.196.1.106 |         194619
                                 Seeking to partition indexed section in data file | 20:31:27,425 | 10.196.1.106 |         194621
                                                       Key cache hit for sstable 1 | 20:31:27,425 | 10.196.1.106 |         194627
                                 Seeking to partition indexed section in data file | 20:31:27,425 | 10.196.1.106 |         194629
                                        Merging data from memtables and 2 sstables | 20:31:27,425 | 10.196.1.106 |         194633
                                                Read 3 live and 0 tombstoned cells | 20:31:27,433 | 10.196.1.106 |         203042
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,433 | 10.196.1.106 |         203084
                                                      Acquiring sstable references | 20:31:27,433 | 10.196.1.106 |         203086
                                                       Merging memtable tombstones | 20:31:27,433 | 10.196.1.106 |         203092
                                            Bloom filter allows skipping sstable 3 | 20:31:27,433 | 10.196.1.106 |         203097
                                            Bloom filter allows skipping sstable 2 | 20:31:27,433 | 10.196.1.106 |         203100
                                                       Key cache hit for sstable 1 | 20:31:27,433 | 10.196.1.106 |         203106
                                       Seeking to partition beginning in data file | 20:31:27,433 | 10.196.1.106 |         203108
                                        Merging data from memtables and 1 sstables | 20:31:27,434 | 10.196.1.106 |         203598
                                                Read 1 live and 0 tombstoned cells | 20:31:27,434 | 10.196.1.106 |         203942
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,434 | 10.196.1.106 |         204112
                                                      Acquiring sstable references | 20:31:27,434 | 10.196.1.106 |         204120
                                                       Merging memtable tombstones | 20:31:27,434 | 10.196.1.106 |         204126
                                                       Key cache hit for sstable 3 | 20:31:27,434 | 10.196.1.106 |         204134
                                 Seeking to partition indexed section in data file | 20:31:27,434 | 10.196.1.106 |         204136
                                                       Key cache hit for sstable 1 | 20:31:27,434 | 10.196.1.106 |         204142
                                 Seeking to partition indexed section in data file | 20:31:27,434 | 10.196.1.106 |         204144
                                        Merging data from memtables and 2 sstables | 20:31:27,434 | 10.196.1.106 |         204148
                                                Read 3 live and 0 tombstoned cells | 20:31:27,441 | 10.196.1.106 |         211397
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,441 | 10.196.1.106 |         211439
                                                      Acquiring sstable references | 20:31:27,441 | 10.196.1.106 |         211441
                                                       Merging memtable tombstones | 20:31:27,441 | 10.196.1.106 |         211447
                                            Bloom filter allows skipping sstable 3 | 20:31:27,442 | 10.196.1.106 |         211546
                                            Bloom filter allows skipping sstable 2 | 20:31:27,442 | 10.196.1.106 |         211732
                                                       Key cache hit for sstable 1 | 20:31:27,442 | 10.196.1.106 |         211737
                                       Seeking to partition beginning in data file | 20:31:27,442 | 10.196.1.106 |         211739
                                        Merging data from memtables and 1 sstables | 20:31:27,442 | 10.196.1.106 |         212484
                                                Read 1 live and 0 tombstoned cells | 20:31:27,443 | 10.196.1.106 |         212809
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,443 | 10.196.1.106 |         212918
                                                      Acquiring sstable references | 20:31:27,443 | 10.196.1.106 |         212921
                                                       Merging memtable tombstones | 20:31:27,443 | 10.196.1.106 |         212927
                                                       Key cache hit for sstable 3 | 20:31:27,443 | 10.196.1.106 |         212934
                                 Seeking to partition indexed section in data file | 20:31:27,443 | 10.196.1.106 |         212936
                                                       Key cache hit for sstable 1 | 20:31:27,443 | 10.196.1.106 |         212943
                                 Seeking to partition indexed section in data file | 20:31:27,443 | 10.196.1.106 |         212945
                                        Merging data from memtables and 2 sstables | 20:31:27,443 | 10.196.1.106 |         212949
                                                Read 3 live and 0 tombstoned cells | 20:31:27,451 | 10.196.1.106 |         220911
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,451 | 10.196.1.106 |         220954
                                                      Acquiring sstable references | 20:31:27,451 | 10.196.1.106 |         220956
                                                       Merging memtable tombstones | 20:31:27,451 | 10.196.1.106 |         220962
                                            Bloom filter allows skipping sstable 3 | 20:31:27,451 | 10.196.1.106 |         220968
                                            Bloom filter allows skipping sstable 2 | 20:31:27,451 | 10.196.1.106 |         220971
                                                       Key cache hit for sstable 1 | 20:31:27,451 | 10.196.1.106 |         220976
                                       Seeking to partition beginning in data file | 20:31:27,451 | 10.196.1.106 |         220979
                                        Merging data from memtables and 1 sstables | 20:31:27,452 | 10.196.1.106 |         221774
                                                Read 1 live and 0 tombstoned cells | 20:31:27,452 | 10.196.1.106 |         221965
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,452 | 10.196.1.106 |         222093
                                                      Acquiring sstable references | 20:31:27,452 | 10.196.1.106 |         222101
                                                       Merging memtable tombstones | 20:31:27,452 | 10.196.1.106 |         222108
                                                       Key cache hit for sstable 3 | 20:31:27,452 | 10.196.1.106 |         222115
                                 Seeking to partition indexed section in data file | 20:31:27,452 | 10.196.1.106 |         222118
                                                       Key cache hit for sstable 1 | 20:31:27,452 | 10.196.1.106 |         222124
                                 Seeking to partition indexed section in data file | 20:31:27,452 | 10.196.1.106 |         222126
                                        Merging data from memtables and 2 sstables | 20:31:27,452 | 10.196.1.106 |         222130
                                                Read 3 live and 0 tombstoned cells | 20:31:27,460 | 10.196.1.106 |         230378
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,460 | 10.196.1.106 |         230420
                                                      Acquiring sstable references | 20:31:27,460 | 10.196.1.106 |         230422
                                                       Merging memtable tombstones | 20:31:27,460 | 10.196.1.106 |         230428
                                            Bloom filter allows skipping sstable 3 | 20:31:27,461 | 10.196.1.106 |         230527
                                            Bloom filter allows skipping sstable 2 | 20:31:27,461 | 10.196.1.106 |         230619
                                                       Key cache hit for sstable 1 | 20:31:27,461 | 10.196.1.106 |         230624
                                       Seeking to partition beginning in data file | 20:31:27,461 | 10.196.1.106 |         230627
                                        Merging data from memtables and 1 sstables | 20:31:27,461 | 10.196.1.106 |         231223
                                                Read 1 live and 0 tombstoned cells | 20:31:27,461 | 10.196.1.106 |         231419
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,461 | 10.196.1.106 |         231531
                                                      Acquiring sstable references | 20:31:27,461 | 10.196.1.106 |         231540
                                                       Merging memtable tombstones | 20:31:27,461 | 10.196.1.106 |         231545
                                                       Key cache hit for sstable 3 | 20:31:27,461 | 10.196.1.106 |         231553
                                 Seeking to partition indexed section in data file | 20:31:27,461 | 10.196.1.106 |         231555
                                                       Key cache hit for sstable 1 | 20:31:27,461 | 10.196.1.106 |         231562
                                 Seeking to partition indexed section in data file | 20:31:27,461 | 10.196.1.106 |         231564
                                        Merging data from memtables and 2 sstables | 20:31:27,461 | 10.196.1.106 |         231568
                                                Read 3 live and 0 tombstoned cells | 20:31:27,470 | 10.196.1.106 |         239976
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,470 | 10.196.1.106 |         240020
                                                      Acquiring sstable references | 20:31:27,470 | 10.196.1.106 |         240022
                                                       Merging memtable tombstones | 20:31:27,470 | 10.196.1.106 |         240028
                                            Bloom filter allows skipping sstable 3 | 20:31:27,470 | 10.196.1.106 |         240034
                                            Bloom filter allows skipping sstable 2 | 20:31:27,470 | 10.196.1.106 |         240037
                                                       Key cache hit for sstable 1 | 20:31:27,470 | 10.196.1.106 |         240054
                                       Seeking to partition beginning in data file | 20:31:27,470 | 10.196.1.106 |         240093
                                        Merging data from memtables and 1 sstables | 20:31:27,470 | 10.196.1.106 |         240585
                                                Read 1 live and 0 tombstoned cells | 20:31:27,471 | 10.196.1.106 |         240885
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,471 | 10.196.1.106 |         240993
                                                      Acquiring sstable references | 20:31:27,471 | 10.196.1.106 |         240997
                                                       Merging memtable tombstones | 20:31:27,471 | 10.196.1.106 |         241003
                                                       Key cache hit for sstable 3 | 20:31:27,471 | 10.196.1.106 |         241011
                                 Seeking to partition indexed section in data file | 20:31:27,471 | 10.196.1.106 |         241013
                                                       Key cache hit for sstable 1 | 20:31:27,471 | 10.196.1.106 |         241019
                                 Seeking to partition indexed section in data file | 20:31:27,471 | 10.196.1.106 |         241020
                                        Merging data from memtables and 2 sstables | 20:31:27,471 | 10.196.1.106 |         241028
                                                Read 3 live and 0 tombstoned cells | 20:31:27,479 | 10.196.1.106 |         249225
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,479 | 10.196.1.106 |         249265
                                                      Acquiring sstable references | 20:31:27,479 | 10.196.1.106 |         249267
                                                       Merging memtable tombstones | 20:31:27,479 | 10.196.1.106 |         249273
                                            Bloom filter allows skipping sstable 3 | 20:31:27,479 | 10.196.1.106 |         249279
                                            Bloom filter allows skipping sstable 2 | 20:31:27,479 | 10.196.1.106 |         249282
                                                       Key cache hit for sstable 1 | 20:31:27,479 | 10.196.1.106 |         249287
                                       Seeking to partition beginning in data file | 20:31:27,479 | 10.196.1.106 |         249289
                                        Merging data from memtables and 1 sstables | 20:31:27,479 | 10.196.1.106 |         249296
                                                Read 1 live and 0 tombstoned cells | 20:31:27,479 | 10.196.1.106 |         249558
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,480 | 10.196.1.106 |         249655
                                                      Acquiring sstable references | 20:31:27,480 | 10.196.1.106 |         249658
                                                       Merging memtable tombstones | 20:31:27,480 | 10.196.1.106 |         249755
                                                       Key cache hit for sstable 3 | 20:31:27,480 | 10.196.1.106 |         249763
                                 Seeking to partition indexed section in data file | 20:31:27,480 | 10.196.1.106 |         249765
                                                       Key cache hit for sstable 1 | 20:31:27,480 | 10.196.1.106 |         249772
                                 Seeking to partition indexed section in data file | 20:31:27,480 | 10.196.1.106 |         249774
                                        Merging data from memtables and 2 sstables | 20:31:27,480 | 10.196.1.106 |         249778
                                                Read 3 live and 0 tombstoned cells | 20:31:27,487 | 10.196.1.106 |         257542
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,487 | 10.196.1.106 |         257575
                                                      Acquiring sstable references | 20:31:27,488 | 10.196.1.106 |         257610
                                                       Merging memtable tombstones | 20:31:27,488 | 10.196.1.106 |         257616
                                            Bloom filter allows skipping sstable 3 | 20:31:27,488 | 10.196.1.106 |         257622
                                            Bloom filter allows skipping sstable 2 | 20:31:27,488 | 10.196.1.106 |         257625
                                                       Key cache hit for sstable 1 | 20:31:27,488 | 10.196.1.106 |         257631
                                       Seeking to partition beginning in data file | 20:31:27,488 | 10.196.1.106 |         257633
                                        Merging data from memtables and 1 sstables | 20:31:27,488 | 10.196.1.106 |         258473
                                                Read 1 live and 0 tombstoned cells | 20:31:27,489 | 10.196.1.106 |         259067
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,489 | 10.196.1.106 |         259197
                                                      Acquiring sstable references | 20:31:27,489 | 10.196.1.106 |         259200
                                                       Merging memtable tombstones | 20:31:27,489 | 10.196.1.106 |         259206
                                                       Key cache hit for sstable 3 | 20:31:27,489 | 10.196.1.106 |         259214
                                 Seeking to partition indexed section in data file | 20:31:27,489 | 10.196.1.106 |         259216
                                                       Key cache hit for sstable 1 | 20:31:27,489 | 10.196.1.106 |         259222
                                 Seeking to partition indexed section in data file | 20:31:27,489 | 10.196.1.106 |         259224
                                        Merging data from memtables and 2 sstables | 20:31:27,489 | 10.196.1.106 |         259319
                                                Read 3 live and 0 tombstoned cells | 20:31:27,497 | 10.196.1.106 |         267071
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,497 | 10.196.1.106 |         267135
                                                      Acquiring sstable references | 20:31:27,497 | 10.196.1.106 |         267137
                                                       Merging memtable tombstones | 20:31:27,497 | 10.196.1.106 |         267143
                                                       Key cache hit for sstable 3 | 20:31:27,497 | 10.196.1.106 |         267151
                                       Seeking to partition beginning in data file | 20:31:27,497 | 10.196.1.106 |         267154
                                            Bloom filter allows skipping sstable 2 | 20:31:27,498 | 10.196.1.106 |         267943
                                            Bloom filter allows skipping sstable 1 | 20:31:27,498 | 10.196.1.106 |         267957
                                        Merging data from memtables and 1 sstables | 20:31:27,498 | 10.196.1.106 |         267960
                                                Read 1 live and 0 tombstoned cells | 20:31:27,498 | 10.196.1.106 |         268176
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,498 | 10.196.1.106 |         268271
                                                      Acquiring sstable references | 20:31:27,498 | 10.196.1.106 |         268274
                                                       Merging memtable tombstones | 20:31:27,498 | 10.196.1.106 |         268280
                                                       Key cache hit for sstable 3 | 20:31:27,498 | 
...
{code}"
CASSANDRA-5968,Nodetool info throws NPE when connected to a booting instance,"When an instance is newly added to the cluster and it's still streaming stuff, trying to call nodetool info on it throws NPE. Stack trace below.

To replicate: add a new node to the cluster, run nodetool info before bootstrap is complete.

Expected behaviour: is nice and just says RPC server is not running.

{noformat}
$ nodetool info
Token            : (invoke with -T/--tokens to see all 0 tokens)
ID               : cc7bcf48-4a54-48af-97f6-99c82bce76f2
Gossip active    : true
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.isRPCServerRunning(StorageService.java:330)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}
"
CASSANDRA-5931,Fix periodic flushing when encountering an empty memtable,"CASSANDRA-5241 broke it by making forceFlush() always return a valid future, never a null, and CASSANDRA-4237 was relying on that null check to determine cleanliness."
CASSANDRA-5926,The native protocol server can deadlock,"Until CASSANDRA-5239 (i.e. since StorageProxy is blocking), the native protocol server needs to use a thread per request being processed. For that, it currently use a DebuggableThreadPoolExecutor, but with a limited queue. The rational being that we don't want to OOM if a client overwhelm the server. Rather, we prefer blocking (which DTPE gives us) on the submission of new request by the netty worker threads when all threads are busy.

However, as it happens, when netty sends back a response to a query, there is cases where some events (technically, InterestChanged and WriteComplete events) are send up the pipeline. And those event are submitted on the request executor as other requests. Long story short, a request thread can end blocking on the submission to its own executor, hence deadlocking.

The simplest solution is probably to reuse MemoryAwareThreadPoolExecutor from netty rather that our own DTPE as it also allow to block task submission when all threads are busy but knows not to block it's own internal events.
"
CASSANDRA-5925,Race condition in update lightweight transaction,"I'm building some tests for a Cassandra PoC.  One scenario I need to test is consumption of 1 time tokens.  These tokens must be consumed exactly once.  The cluster involved is a 3 node cluster.  All queries are run with ConsistencyLevel.QUORUM. I'm using the following queries:

CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

CREATE TABLE IF NOT EXISTS tkns (tkn blob, consumed boolean, PRIMARY KEY (tkn));

INSERT INTO tkns (tkn, consumed) VALUES (?,FALSE) USING TTL 30;

UPDATE tkns USING TTL 1 SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

I use the '[applied]' column in the result set of the update statement to determine whether the token has been successfully consumed or if the token is being replayed.

My test involves concurrently executing many sets of 1 insert and 2 update statements (using Session#execute on BoundStatemnts) then checking to make sure that only one of the updates was applied.

When I run this test with relatively few iterations (~100) my results are  what I expect (exactly 1 update succeeds).  At ~1000 iterations, I start seeing both updates reporting success in 1-2% of cases.  While my test is running, I see corresponding error entries in the Cassandra log:

ERROR 15:34:53,583 Exception in thread Thread[MutationStage:522,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:474,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:536,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:480,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:534,5,main]
java.lang.NullPointerException


Thanks.

Update:

I'm not sure what's going on with the logging the the dev release.  I grabbed the rc2 source and built that.  The resultant log is a bit more informative:

ERROR 11:53:38,967 Exception in thread Thread[MutationStage:114,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:32)
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:26)
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getUUID(UntypedResultSet.java:131)
	at org.apache.cassandra.db.SystemKeyspace.loadPaxosState(SystemKeyspace.java:785)
	at org.apache.cassandra.service.paxos.PaxosState.commit(PaxosState.java:118)
	at org.apache.cassandra.service.paxos.CommitVerbHandler.doVerb(CommitVerbHandler.java:34)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
"
CASSANDRA-5911,Commit logs are not removed after nodetool flush or nodetool drain,"Commit logs are not removed after nodetool flush or nodetool drain. This can lead to unnecessary commit log replay during startup.  I've reproduced this on Apache Cassandra 1.2.8.  Usually this isn't much of an issue but on a Solr-indexed column family in DSE, each replayed mutation has to be reindexed which can make startup take a long time (on the order of 20-30 min).

Reproduction follows:

{code}
jblangston:bin jblangston$ ./cassandra > /dev/null
jblangston:bin jblangston$ ../tools/bin/cassandra-stress -n 20000000 > /dev/null
jblangston:bin jblangston$ du -h ../commitlog
576M	../commitlog
jblangston:bin jblangston$ nodetool flush
jblangston:bin jblangston$ du -h ../commitlog
576M	../commitlog
jblangston:bin jblangston$ nodetool drain
jblangston:bin jblangston$ du -h ../commitlog
576M	../commitlog
jblangston:bin jblangston$ pkill java
jblangston:bin jblangston$ du -h ../commitlog
576M	../commitlog
jblangston:bin jblangston$ ./cassandra -f | grep Replaying
 INFO 10:03:42,915 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566761.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566762.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566763.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566764.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566765.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566766.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566767.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566768.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566769.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566770.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566771.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566772.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566773.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566774.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566775.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566776.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566777.log, /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566778.log
 INFO 10:03:42,922 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566761.log
 INFO 10:03:43,907 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566762.log
 INFO 10:03:43,907 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566763.log
 INFO 10:03:43,907 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566764.log
 INFO 10:03:43,908 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566765.log
 INFO 10:03:43,908 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566766.log
 INFO 10:03:43,908 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566767.log
 INFO 10:03:43,909 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566768.log
 INFO 10:03:43,909 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566769.log
 INFO 10:03:43,909 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566770.log
 INFO 10:03:43,910 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566771.log
 INFO 10:03:43,910 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566772.log
 INFO 10:03:43,911 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566773.log
 INFO 10:03:43,911 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566774.log
 INFO 10:03:43,911 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566775.log
 INFO 10:03:43,912 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566776.log
 INFO 10:03:43,912 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566777.log
 INFO 10:03:43,912 Replaying /opt/apache-cassandra-1.2.8/commitlog/CommitLog-2-1377096566778.log
{code}"
CASSANDRA-5910,Most CQL3 functions should handle null gracefully,"Currently, we don't allow null parameters for functions. So
{noformat}
UPDATE test SET d=dateOf(null) WHERE k=0
{noformat}
is basically an invalid query. Unfortunately, there's at least one case where we don't validate correctly, namely if we do:
{noformat}
SELECT k, dateOf(t) FROM test
{noformat}
In that case, if for any of the row {{t}} is null, we end up with a server side NPE. But more importantly, throwing an InvalidException in that case would be pretty inconvenient and actually somewhat wrong since the query is not invalid in itself. So, at least in that latter case, we want {{dateOf(t) == null}} when {{t == null}}. And if we do that, I suggest making it always the case (i.e. make the first query valid but assigning {{null}} to {{d}}).
"
CASSANDRA-5903,Integer overflow in OffHeapBitSet when bloomfilter > 2GB,"In org.apache.cassandra.utils.obs.OffHeapBitSet.

byteCount overflows and causes an IllegalArgument exception in Memory.allocate when bloomfilter is > 2GB.

Suggest changing byteCount to long.

{code:title=OffHeapBitSet.java}
    public OffHeapBitSet(long numBits)
    {
        // OpenBitSet.bits2words calculation is there for backward compatibility.
        int byteCount = OpenBitSet.bits2words(numBits) * 8;
        bytes = RefCountedMemory.allocate(byteCount);
        // flush/clear the existing memory.
        clear();
    }

{code}"
CASSANDRA-5893,CqlParser throws StackOverflowError on bigger batch operation,"We are seeing a problem with CQL3/Cassandra 1.2.8 where a large batch operation causes the CqlParser to throw a StackOverflowError (-Xss180k initially, then -Xss325k).

Shouldn't a batch be processed iteratively to avoid having to bump stack sizes to unreasonably large values?

Here is more info from the original problem description:


<<<
It looks like the CqlParser in 1.2.8 (probably 1.2.x, but i didn't look) is implemented recursively in such a way that large batch statements blow up the stack. We, of course on a Friday night, have a particular piece of code that's hitting a degenerate case that creates a batch of inserts with a VERY large number of collection items, and it manifests as a StackOverflow coming out the cass servers:

java.lang.StackOverflowError
       at org.apache.cassandra.cql3.CqlParser.value(CqlParser.java:5266)
       at org.apache.cassandra.cql3.CqlParser.term(CqlParser.java:5627)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4807)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
	...
	
I think in the short term I can give up the atomicity of a batch in this code and kind of suck it up, but obviously I'd prefer not to. I'm also not sure if I kept a single batch, but split this into smaller pieces in each statement, whether that would still fail. I'm guessing I could also crank the hell out of the stack size on the servers, but that feels pretty dirty.

It seems like the CqlParser should probably be implemented in a way that isn't quite so vulnerable to this, though I fully accept that this batch is koo-koo-bananas.
>>>

Thanks!

 "
CASSANDRA-5892,Support user defined where clause in cluster columns for CqlPagingRecordRead,"When using CqlPagingRecordReader, specifying a custom where clause using CqlConfigHelper.setInputWhereClauses() throws an exception when a GT (>) comparator is used.

Exception:
java.lang.RuntimeException at org.apache.cassandra.hadoop.cql3.CqlPagingRecordReader$RowIterator.executeQuery(CqlPagingRecordReader.java:646) Caused by: InvalidRequestException(why:Invalid restrictions found on ts) at org.apache.cassandra.thrift.Cassandra$prepare_cql3_query_result.read(Cassandra.java:39567)

This is due to the paging mechanism inserting a GT comparator on the same composite key as the custom where clause, resulting in an invalid CQL statement. For example (""ts > '634926385000000000'"" being the custom where clause):

SELECT * FROM ""test_cf""
    WHERE token(""key"") = token( ? )  AND ""ts"" > ?
    AND ts > '634926385000000000' LIMIT 3 ALLOW FILTERING"
CASSANDRA-5890,Exception: CorruptSSTableException after nightly 'enqueuing flush',"The 2 Cassandra nodes have worked well for 2 days. However, after a nightly 'enqueuing flush' process, a few column families (in all keyspaces) are corrupted and the following error is observed in log file while trying to access the CF:

ERROR [ReadStage:413277] 2013-08-14 10:16:13,322 CassandraDaemon.java (line 175) Exception in thread Thread[ReadStage:413277,5,main]
org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:65)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:133)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.RandomAccessReader.readBytes(RandomAccessReader.java:380)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:116)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:60)
        ... 14 more
ERROR [ReadStage:413292] 2013-08-14 10:16:53,700 CassandraDaemon.java (line 175) Exception in thread Thread[ReadStage:413292,5,main]
java.lang.AssertionError: DecoratedKey(-8619398030348476976, 796177616431406d6e75626f2e636f6d) != DecoratedKey(-8430385117828588592, 79624079622e636f6d) in /var/lib/cassandra/data/newui/user/newui-user-ic-1-Data.db
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:119)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:60)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:133)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)


Here the traces of the 'enqueuing flush' process:
 INFO [OptionalTasks:1] 2013-08-14 04:01:50,586 ColumnFamilyStore.java (line 631) Enqueuing flush of Memtable-user@848766585(3130/3130 serialized/live bytes, 96 ops)
 INFO [FlushWriter:381] 2013-08-14 04:01:50,632 Memtable.java (line 461) Writing Memtable-user@848766585(3130/3130 serialized/live bytes, 96 ops)
 INFO [FlushWriter:381] 2013-08-14 04:01:50,641 Memtable.java (line 495) Completed flushing /var/lib/cassandra/data/newui/user/newui-user-ic-1-Data.db (1513 bytes) for commitlog position ReplayPosition(segmentId=1373063890581, position=9254)

The issue had happened quite regularly and on a random CF. Please note that we don't have a lot of data (max 20 records per CF).

We've to run 'nodetool scrub' and 'nodetool repair' to get out of troubles. Please let me know if this issue is related to a configuration, a bad use of Cassandra or a real problem."
CASSANDRA-5885,NPE in CqlRecordWriter when sinking null values,"null values cause a Thrift NPE when sinking to CqlRecordWriter (replacing the null values with an appropriate non-null sentinel works) :

	at java.lang.Thread.run(Thread.java:680) ~[na:1.6.0_51]
java.io.IOException: java.lang.NullPointerException
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:245) ~[cassandra-all-1.2.8.jar:1.2.8]
java.lang.NullPointerException: null
	at org.apache.thrift.protocol.TBinaryProtocol.writeBinary(TBinaryProtocol.java:194) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.Cassandra$execute_prepared_cql3_query_args.write(Cassandra.java:41253) ~[cassandra-thrift-1.2.8.jar:1.2.8]
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.Cassandra$Client.send_execute_prepared_cql3_query(Cassandra.java:1683) ~[cassandra-thrift-1.2.8.jar:1.2.8]
	at org.apache.cassandra.thrift.Cassandra$Client.execute_prepared_cql3_query(Cassandra.java:1673) ~[cassandra-thrift-1.2.8.jar:1.2.8]
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:229) ~[cassandra-all-1.2.8.jar:1.2.8]"
CASSANDRA-5884,Improve offheap memory performance,"Currently getting data to and from offheap is slow due to individual calls to getByte() and putByte()

The following patch makes this a single call and is 10x faster

{code}
benchmark:
     [java]  0% Scenario{vm=java, trial=0, benchmark=GetBytesOld} 82266.48 ns; σ=2532.87 ns @ 10 trials
     [java] 50% Scenario{vm=java, trial=0, benchmark=GetBytesNew} 7876.95 ns; σ=489.78 ns @ 10 trials
     [java]
     [java]   benchmark    us linear runtime
     [java] GetBytesOld 82.27 ==============================
     [java] GetBytesNew  7.88 ==
     [java]
     [java] vm: java
     [java] trial: 0
{code}"
CASSANDRA-5877,Unclear FileNotFoundException stacktrace when system.log can't be created,"When you start Cassandra with default settings without the appropriate permissions to write in {{/var/log/cassandra}} you end up with the following stacktrace:

{code}
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /var/log/cassandra/system.log (No such file or directory)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:206)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:127)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)
	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:809)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:735)
	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:615)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:502)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:395)
	at org.apache.log4j.PropertyWatchdog.doOnChange(PropertyConfigurator.java:922)
	at org.apache.log4j.helpers.FileWatchdog.checkAndConfigure(FileWatchdog.java:89)
	at org.apache.log4j.helpers.FileWatchdog.<init>(FileWatchdog.java:58)
	at org.apache.log4j.PropertyWatchdog.<init>(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.configureAndWatch(PropertyConfigurator.java:461)
	at org.apache.cassandra.service.CassandraDaemon.initLog4j(CassandraDaemon.java:121)
	at org.apache.cassandra.service.CassandraDaemon.<clinit>(CassandraDaemon.java:69)
{code}

While a stacktrace at startup may not be the most elegant mean of communication with the user - though at least it's visible - in this situation it doesn't make it clear that Cassandra couldn't create a file in the specified log directory."
CASSANDRA-5865,NPE when you mistakenly set listen_address to 0.0.0.0,"It's clearly stated that setting {{listen_address}} to {{0.0.0.0}} is always wrong. But if you mistakenly do it anyway you end up with an NPE on 1.2.8 while it's not the case on 2.0.0-rc1. See bellow:

{code}
 INFO 16:34:43,598 JOINING: waiting for ring information
 INFO 16:34:44,505 Handshaking version with /127.0.0.1
 INFO 16:34:44,533 Handshaking version with /0.0.0.0
 INFO 16:35:13,626 JOINING: schema complete, ready to bootstrap
 INFO 16:35:13,631 JOINING: getting bootstrap token
ERROR 16:35:13,633 Exception encountered during startup
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
ERROR 16:35:13,668 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:519)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{code}"
CASSANDRA-5850,change gc_grace_seconds default to 28 days,"Current default for gc_grace_seconds is 10 days. Attached patch changes all instances of this 10 day default to 28 days. 

Rationale :

- 10 days is arbitrary, there is nothing special about the current value
- human societies do not operate on cycles which are a multiple of 10 days, they operate on a cycle of 7 day weeks
- operators must run repair once every gc_grace_seconds, and with typical data sizes (and compaction/streaming throttling) this might run for a significant fraction of 10 days
- repair often fails, and detecting and working around that failure might also take a significant fraction of 10 days
- repair is the heaviest operation one can run on a cassandra cluster and operators are therefore motivated to run it ~3x less frequently by default
- the worst case impact is keeping data around for 18 days longer than the previous default, and this only occurs in CFs which actually take DELETE operation
- 28 days is an even multiple of 7 days and easily comprehensible as a default time in which to schedule repair"
CASSANDRA-5824,Fix quoting in CqlPagingRecordReader and CqlRecordWriter,"To support case sensitive in CQL, we need add double quotes to the name of columns and table."
CASSANDRA-5816,[PATCH] Debian packaging: also recommend chrony and ptpd in addition to ntp,"I'm switching my Ubuntu servers running Cassandra from ntp to chrony for the reasons cited here when Fedora made the switch to have chrony be the default NTP client:

http://fedoraproject.org/wiki/Features/ChronyDefaultNTP

Currently, the debian packaging recommends only ntp so if chrony is installed it'll want to remove it and install ntp.  I also added ptpd, the Precision Time Protocol daemon, which is another time syncing server for completeness.

Please apply this to the 1.2 branch so the next 1.2.x release can deploy with chrony.

Below is the patch since it's a one-liner.

Thanks,
Blair



--- a/debian/control
+++ b/debian/control
@@ -12,7 +12,7 @@ Standards-Version: 3.8.3
 Package: cassandra
 Architecture: all
 Depends: openjdk-6-jre-headless (>= 6b11) | java6-runtime, jsvc (>= 1.0), libcommons-daemon-java (>= 1.0), adduser, libjna-java, python (>= 2.5), python-support (>= 0.90.0), ${misc:Depends}
-Recommends: ntp
+Recommends: chrony | ntp | ptpd
 Conflicts: apache-cassandra1
 Replaces: apache-cassandra1
 Description: distributed storage system for structured data
"
CASSANDRA-5815,NPE from migration manager,"In one of our production clusters we see this error often. Looking through the source, Gossiper.instance.getEndpointStateForEndpoint(endpoint) is returning null for some end point. De we need any config change on our end to resolve this? In any case, cassandra should be updated to protect against this NPE.

{noformat}
ERROR [OptionalTasks:1] 2013-07-24 13:40:38,972 AbstractCassandraDaemon.java (line 132) Exception in thread Thread[OptionalTasks:1,5,main] 
java.lang.NullPointerException 
at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:134) 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) 
at java.util.concurrent.FutureTask.run(FutureTask.java:138) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)
{noformat}

It turned out that the reason for NPE was we bootstrapped a node with the same token as another node. Cassandra should not throw an NPE here but log a meaningful error message. "
CASSANDRA-5811,NPE during upgrade,"The upgrade_through_versions dtest occasionally fails with the following error (on the 1.1 side):

{noformat}
ERROR [InternalResponseStage:2] 2013-07-26 12:51:10,145 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[InternalResponseStage:2,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
    at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
    at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:256)
    at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:397)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:373)
    at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
    at org.apache.cassandra.service.MigrationManager$MigrationTask$1.response(MigrationManager.java:453)
    at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:45)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
{noformat}"
CASSANDRA-5806,NPE during repair," INFO 01:06:00,656 Connecting to /10.0.0.3 for streaming
 INFO 01:06:00,656 Connecting to /10.0.0.3 for streaming
ERROR 01:06:05,828 Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(Connecti
onHandler.java:175)
        at org.apache.cassandra.streaming.StreamSession.maybeCompleted(StreamSes
sion.java:600)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.ja
va:446)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSe
ssion.java:357)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandl
er.run(ConnectionHandler.java:294)"
CASSANDRA-5802,NPE in HH metrics,"{noformat}
    [junit] Testcase: testCompactionOfHintsCF(org.apache.cassandra.db.HintedHandOffTest):	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)
    [junit] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3989)
    [junit] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3994)
    [junit] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4878)
    [junit] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4884)
    [junit] 	at org.apache.cassandra.metrics.HintedHandoffMetrics.incrCreatedHints(HintedHandoffMetrics.java:67)
    [junit] 	at org.apache.cassandra.db.HintedHandOffManager.hintFor(HintedHandOffManager.java:125)
    [junit] 	at org.apache.cassandra.db.HintedHandOffTest.testCompactionOfHintsCF(HintedHandOffTest.java:68)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.HintedHandOffTest FAILED
{noformat}"
CASSANDRA-5800,Support pre-1.2 release CQL3 tables in CqlPagingRecordReader,Pre-1.2 release CQL3 table stores the key in system.schema_columnfamilies key_alias column which is different from 1.2 release. We should support it in CqlPagingRecordReader as well.
CASSANDRA-5779,sudden NPE while accessing Cassandra with CQL driver,"java.lang.NullPointerException at org.apache.cassandra.db.RowMutation.addOrGet(RowMutation.java:153)
 at org.apache.cassandra.cql3.statements.UpdateStatement.mutationForKey(UpdateStatement.java:216)
 at org.apache.cassandra.cql3.statements.UpdateStatement.getMutations(UpdateStatement.java:133)
 at org.apache.cassandra.cql3.statements.BatchStatement.getMutations(BatchStatement.java:99)
at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:88)
at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:118)
at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:128)
at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:87)
at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:287)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:793)
at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:724)
"
CASSANDRA-5767,NPE under load,"I am new to Cassandra so my report may not contain enough relevant information. I am trying to use Cassandra as backend storage for Titan graph DB. I am running a simple test on a small cluster of 4 Cassandra nodes (1.2.6 all of them, running from scratch). I can consistently get this exception that makes Cassandra unusable:

{code}
 INFO [FlushWriter:38] 2013-07-16 23:28:37,246 Memtable.java (line 461) Writing Memtable-schema_columnfamilies@1756904544(1291/1291 serialized/live bytes, 21 ops)
 INFO [FlushWriter:38] 2013-07-16 23:28:37,395 Memtable.java (line 495) Completed flushing /hadoop/disk4/cassandra/data/system/schema_columnfamilies/system-schema_columnf
amilies-ic-284-Data.db (701 bytes) for commitlog position ReplayPosition(segmentId=1374015168706, position=2927400)
 INFO [CompactionExecutor:188] 2013-07-16 23:28:37,396 CompactionTask.java (line 105) Compacting [SSTableReader(path='/hadoop/disk4/cassandra/data/system/schema_columnfam
ilies/system-schema_columnfamilies-ic-281-Data.db'), SSTableReader(path='/hadoop/disk4/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-284-Dat
a.db'), SSTableReader(path='/hadoop/disk4/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-283-Data.db'), SSTableReader(path='/hadoop/disk4/cas
sandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-282-Data.db')]
ERROR [MigrationStage:1] 2013-07-16 23:28:37,405 CassandraDaemon.java (line 192) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:476)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:188] 2013-07-16 23:28:37,580 CompactionTask.java (line 262) Compacted 4 sstables to [/hadoop/disk4/cassandra/data/system/schema_columnfamilies/s
ystem-schema_columnfamilies-ic-285,].  7,059 bytes to 5,801 (~82% of original) in 184ms = 0.030067MB/s.  8 total rows, 5 unique.  Row merge counts were {1:4, 2:0, 3:0, 4:
1, }
 INFO [MigrationStage:1] 2013-07-16 23:28:42,595 ColumnFamilyStore.java (line 630) Enqueuing flush of Memtable-schema_columnfamilies@1684657235(1207/1207 serialized/live
bytes, 21 ops)
 INFO [FlushWriter:39] 2013-07-16 23:28:42,596 Memtable.java (line 461) Writing Memtable-schema_columnfamilies@1684657235(1207/1207 serialized/live bytes, 21 ops)
 INFO [FlushWriter:39] 2013-07-16 23:28:42,787 Memtable.java (line 495) Completed flushing /hadoop/disk4/cassandra/data/system/schema_columnfamilies/system-schema_columnf
amilies-ic-286-Data.db (699 bytes) for commitlog position ReplayPosition(segmentId=1374015168706, position=2929541)
ERROR [MigrationStage:1] 2013-07-16 23:28:42,796 CassandraDaemon.java (line 192) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:476)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [MemoryMeter:1] 2013-07-16 23:28:58,908 Memtable.java (line 238) CFS(Keyspace='mykeyspace', ColumnFamily='edgestore') liveRatio is 9.582214511662903 (just-counted w
as 9.582214511662903).  calculation took 136930ms for 18075721 columns
 INFO [MemoryMeter:1] 2013-07-16 23:29:14,208 Memtable.java (line 238) CFS(Keyspace='mykeyspace', ColumnFamily='vertexindex') liveRatio is 36.198425299625 (just-counted w
as 36.198425299625).  calculation took 15300ms for 720565 columns
{code}"
CASSANDRA-5763,CqlPagingRecordReader should quote table and column identifiers,"Using CPIF on table with uppercase name or with uppercase column names doesn't work (""unconfigured table"" message)."
CASSANDRA-5752,Thrift tables are not supported from CqlPagingInputFormat,"CqlPagingInputFormat inspects the system schema to generate the WHERE clauses needed to page ""wide rows,"" but for a classic Thrift table there are no entries for the ""default"" column names of key, column1, column2, ..., value so CPIF breaks."
CASSANDRA-5748,"When flushing, nodes spent almost 100% in AbstractCompositeType.compare","We're pretty heavy users of CQL3 and CQL3 collection types. Occasionally, some nodes of the cluster will become extremely sluggish and the cluster as a whole starts to become unresponsive, reads will time out, and nodes will drop mutation messages. This happens when nodes flush Memtables to disk (based on my tail of the system.log on each node).

I'm a curious guy, so I attached jvisualvm (v1.3.3) to the JVMs that were having this problem. These nodes are spending up to 98% of CPU in org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:78). I will attach a thread dump.

Thi is causing us quite a headache, because we're unable to figure what would be causing this. We tried tuning several configuration settings (column cache size, row key cache size), but the cluster exhibits the same issues even with the default configuration (except for a modified num_tokens and listen_address)."
CASSANDRA-5737,CassandraDaemon - recent unsafe memory access operation in compiled Java code,"I'm using 1.2.6 on Ubuntu AWS m1.xlarge instances with the Datastax Community package and have tried using Java versions jdk1.7.0_25  jre1.6.0_45
Also testing with and without libjna-java (ie the JNA jar)

However, something has triggered a bug in the CassandraDaemon:

ERROR [COMMIT-LOG-ALLOCATOR] 2013-07-05 15:00:51,663 CassandraDaemon.java (line 192) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java code
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:126)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.freshSegment(CommitLogSegment.java:81)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.createFreshSegment(CommitLogAllocator.java:250)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.access$500(CommitLogAllocator.java:48)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:104)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Unknown Source)

This brought two nodes down out of a three node cluster – using QUORUM write with 3 replicas.
Restarting the node replays this error, so I have the system in a 'stable' unstable state – which is probably a good place for trouble shooting.

Presumably something a client wrote triggered this situation, and the other third node was to be the final replication point – and is thus still up.


Subsequently discovered that only a reboot will allow that node to come back up.
Java Bug raised with Oracle after finding a Java dump text indicating a SIGBUS.
 http://bugs.sun.com/view_bug.do?bug_id=9004953

At this point, I'm thinking that there is potentially a Linux kernel bug being triggered?"
CASSANDRA-5736,CQL3PagingRecordReader can OOM and kill nodes,"It looks like the CQL3PagingRecordReader will end up OOMing many nodes in a cluster as the OOM/GC Storm due to ReadStage

This is the stack trace from all of the ReadStage threads:
{code}
org.apache.cassandra.db.marshal.DateType.compare(DateType.java:62)
org.apache.cassandra.db.marshal.DateType.compare(DateType.java:32)
org.apache.cassandra.db.marshal.AbstractType.compareCollectionMembers(AbstractType.java:229)
org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:81)
org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
java.util.TimSort.mergeHi(TimSort.java:806)
java.util.TimSort.mergeAt(TimSort.java:485)
java.util.TimSort.mergeForceCollapse(TimSort.java:426)
java.util.TimSort.sort(TimSort.java:223)
java.util.TimSort.sort(TimSort.java:173)
java.util.Arrays.sort(Arrays.java:659)
java.util.Collections.sort(Collections.java:217)
org.apache.cassandra.utils.IntervalTree$IntervalNode.<init>(IntervalTree.java:255)
org.apache.cassandra.utils.IntervalTree$IntervalNode.<init>(IntervalTree.java:281)
org.apache.cassandra.utils.IntervalTree$IntervalNode.<init>(IntervalTree.java:280)
org.apache.cassandra.utils.IntervalTree.<init>(IntervalTree.java:72)
org.apache.cassandra.utils.IntervalTree.build(IntervalTree.java:81)
org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:181)
org.apache.cassandra.db.AbstractThreadUnsafeSortedColumns.delete(AbstractThreadUnsafeSortedColumns.java:40)
org.apache.cassandra.db.AbstractColumnContainer.delete(AbstractColumnContainer.java:51)
org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:224)
org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:182)
org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:154)
org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:143)
org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:86)
org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:45)
org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:134)
org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)
org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:106)
org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:79)
org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
org.apache.cassandra.db.ColumnFamilyStore$6.computeNext(ColumnFamilyStore.java:1432)
org.apache.cassandra.db.ColumnFamilyStore$6.computeNext(ColumnFamilyStore.java:1428)
com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1499)
org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1476)
org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:46)
org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:58)
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:722)
{code}

As best I can tell this is related to any row with > 5ish tombstones and has something to do with DeletionInfo trying to sort the results. Only way to fix this was to rolling restart all of the nodes in the cluster as the ReadStage threads appeared to be making no progress (most likely due to GC..)"
CASSANDRA-5716,Remark on cassandra-5273 : Hanging system after OutOfMemory. Server cannot die due to uncaughtException handling,"Possible incorrect handling of an OOM as a result of modifications made for issue cassandra-5273.
I could reproduce the OOM, with the patch of Cassandra-5273 applied.
The good news is that, at least in my case, it works fine : the system did die !
 
However, due to multiple uncaughtException handling, multiple threads are calling the exitThread.start() routine, causing an IllegalStateException. There are some other exceptions also, but that seems logical. Also, after calling the start() function, the thread(s) will continue to run, and that could not be wanted.
 
Below I pasted the stack trace.
Just for your information, after all this works, and I could restart the Cassandra server and redo the OOM

[stack trace moved to http://aep.appspot.com/display/mQFNFHUh1VvQJYGcxRK0lQSM2j8/ ]"
CASSANDRA-5704,"add cassandra.unsafesystem property (Truncate flushes to disk again in 1.2, even with durable_writes=false)","I just upgraded my dev-environment to C* 1.2. Unfortunetaly 1.2 makes my JUnit tests slow again, due to a blocking-flush in saveTruncationRecord().

With Cassandra 1.1 truncate was very fast due to: CASSANDRA-4153


My proposal is to make saveTruncationRecord() only flush when durableWrites are enabled.

My assumption is that if somebody turn off durable writes then he does not mind if truncate is not guaranteed to be durable either.


I successfully tested the following patch with my testsuite. Its as fast as it was with 1.1 (maybe even faster!):
{code}
@@ -186,5 +186,8 @@ public class SystemTable
         String req = ""UPDATE system.%s SET truncated_at = truncated_at + %s WHERE key = '%s'"";
         processInternal(String.format(req, LOCAL_CF, truncationAsMapEntry(cfs, truncatedAt, position), LOCAL_KEY));
-        forceBlockingFlush(LOCAL_CF);
+        
+        KSMetaData ksm = Schema.instance.getKSMetaData(cfs.table.name);
+        if (ksm.durableWrites) // flush only when durable_writes are enabled
+            forceBlockingFlush(LOCAL_CF);
     }
{code}
"
CASSANDRA-5699,Streaming (2.0) can deadlock,"The new streaming implementation (CASSANDRA-5286) creates 2 threads per host for streaming, one for the incoming stream and one for the outgoing one. However, both currently share the same socket, but since we use synchronous I/O, a read can block a write, which can result in a deadlock if 2 nodes are both blocking on a read a the same time, thus blocking their respective writes (this is actually fairly easy to reproduce with a simple repair).

So instead attaching a patch that uses one socket per thread.

The patch also correct the stream throughput throttling calculation that was 8000 times lower than what it should be."
CASSANDRA-5692,Race condition in detecting version on a mixed 1.1/1.2 cluster,"On a mixed 1.1 / 1.2 cluster, starting 1.2 nodes fires sometimes a race condition in version detection, where the 1.2 node wrongly detects version 6 for a 1.1 node.

It works as follows:
1) The just started 1.2 node quickly opens an OutboundTcpConnection toward a 1.1 node before receiving any messages from the latter.
2) Given the version is correctly detected only when the first message is received, the version is momentarily set at 6.
3) This opens an OutboundTcpConnection from 1.2 to 1.1 at version 6, which gets stuck in the connect() method.

Later, the version is correctly fixed, but all outbound connections from 1.2 to 1.1 are stuck at this point.

Evidence from 1.2 logs:
TRACE 13:48:31,133 Assuming current protocol version for /127.0.0.2
DEBUG 13:48:37,837 Setting version 5 for /127.0.0.2"
CASSANDRA-5689,NPE shutting down Cassandra trunk (cassandra-1.2.5-989-g70dfb70),"I built Cassandra from git trunk at cassandra-1.2.5-989-g70dfb70 using the debian/ package.  I have a shell script to shut down Cassandra:

{code}
      $nodetool disablegossip
      sleep 5
      $nodetool disablebinary
      $nodetool disablethrift
      $nodetool drain
      /etc/init.d/cassandra stop
{code}

Shutting it down I get this exception on all three nodes:

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.transport.Server.close(Server.java:156)
	at org.apache.cassandra.transport.Server.stop(Server.java:107)
	at org.apache.cassandra.service.StorageService.stopNativeTransport(StorageService.java:347)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at com.sun.jmx.remote.security.MBeanServerAccessController.invoke(MBeanServerAccessController.java:468)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{code}
"
CASSANDRA-5688,Update debian packaging for 2.0,"Building trunk on an Ubuntu Precise VM fails with the following output:

{code}
$ git describe 
cassandra-1.2.5-983-g96a1bb0
$ dpkg-buildpackage
...
...
gen-cql3-grammar:
     [echo] Building Grammar /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/src/java/org/apache/cassandra/cql3/Cql.g  ...

build-project:
     [echo] apache-cassandra: /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/build.xml
    [javac] Compiling 41 source files to /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/build/classes/thrift
    [javac] javac: invalid target release: 1.7
    [javac] Usage: javac <options> <source files>
    [javac] use -help for a list of possible options

BUILD FAILED
{code}

I'm working on changes to the files in debian/ to support this."
CASSANDRA-5669,"Connection thrashing in multi-region ec2 during upgrade, due to messaging version","While debugging the upgrading scenario described in CASSANDRA-5660, I discovered the ITC.close() will reset the message protocol version of a peer node that disconnects. CASSANDRA-5660 has a full description of the upgrade path, but basically the Ec2MultiRegionSnitch will close connections on the publicIP addr to reconnect on the privateIp, and this causes ITC to drop the message protocol version of previously known nodes. I think we want to hang onto that version so that when the newer node (re-)connects to the lower node version, it passes the correct protocol version rather than the current version (too high for the older node),the connection attempt getting dropped, and going through the dance again.

To clarify, the 'thrashing' is at a rather low volume, from what I observed. Anecdotaly, perhaps one connection per second gets turned over."
CASSANDRA-5668,NPE in net.OutputTcpConnection when tracing is enabled,"I get multiple NullPointerException when trying to trace INSERT statements.

To reproduce:
{code}
$ ccm create -v git:trunk
$ ccm populate -n 3
$ ccm start
$ ccm node1 cqlsh < 5668_npe_ddl.cql
$ ccm node1 cqlsh < 5668_npe_insert.cql
{code}

And see many exceptions like this in the logs of node1:
{code}
ERROR [WRITE-/127.0.0.3] 2013-06-19 14:54:35,885 OutboundTcpConnection.java (line 197) error writing to /127.0.0.3
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:182)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:144)
{code}


This is similar to CASSANDRA-5658 and is the reason that npe_ddl and npe_insert are separate files."
CASSANDRA-5656,NPE in SSTableNamesIterator,"When adding a new node to our cluster we occasionally get the following error in the cassandra system log:

2013-06-18T07:13:18:942|ERROR|ReadStage:30|org.apache.cassandra.service.CassandraDaemon|Exception in thread Thread[ReadStage:30,5,main]
java.lang.NullPointerException
        at java.util.TreeSet.iterator(TreeSet.java:230)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:163)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:64)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:133)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:908)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:931)
        at java.lang.Thread.run(Thread.java:738)

The same exception then occurs repeatedly every few milliseconds and the node is not working. Calls via the API are timing out.
"
CASSANDRA-5655,Equals method in PermissionDetails causes StackOverflowException,"It simply delegates to Guava's Objects.equal, which itself ends up calling back to the original caller's equals after performing some basic checks.
"
CASSANDRA-5644,Exception swallowing in ..net.MessagingService,"While I was trying to setup internode encryption, I spent too much time finding out that the name of my keystore was wrong.
Main reason was the org/apache/cassandra/net/MessagingService.java swallowing the exception and just spitting out:
{noformat}
ERROR [main] 2013-06-15 12:49:43,758 CassandraDaemon.java (line 358) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to create ssl socket
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:432)
	at org.apache.cassandra.net.MessagingService.listen(MessagingService.java:412)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:564)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:529)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:428)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Unable to create ssl socket
{noformat}

I will attach a minor patch, that shows just a bit more:

{noformat}
ERROR [main] 2013-06-15 12:58:44,979 CassandraDaemon.java (line 358) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to create ssl socket
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:432)
	at org.apache.cassandra.net.MessagingService.listen(MessagingService.java:412)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:564)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:529)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:428)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.io.IOException: Error creating the initializing the SSL Context
	at org.apache.cassandra.security.SSLFactory.createSSLContext(SSLFactory.java:124)
	at org.apache.cassandra.security.SSLFactory.getServerSocket(SSLFactory.java:53)
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:428)
	... 7 more
Caused by: java.io.FileNotFoundException: conf/oeps-wrong-truststore.jks (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:97)
	at org.apache.cassandra.security.SSLFactory.createSSLContext(SSLFactory.java:105)
	... 9 more
Unable to create ssl socket
{noformat}

kind regards,
Harry
"
CASSANDRA-5631,NPE when creating column family shortly after multinode startup,"I'm testing a 2-node cluster and creating a column family right after the nodes startup.  I am using the Astyanax client.  Sometimes column family creation fails and I see NPEs on the cassandra server:

{noformat}
2013-06-12 14:55:31,773 ERROR CassandraDaemon [MigrationStage:1] - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:510)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:444)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:354)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

{noformat}

{noformat}
2013-06-12 14:55:31,880 ERROR CassandraDaemon [MigrationStage:1] - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:475)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:354)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{noformat}
"
CASSANDRA-5624,Memory leak in SerializingCache,"A customer reported a memory leak when off-heap row cache is enabled.

I gave them a patch against 1.1.9 to troubleshoot (https://github.com/jbellis/cassandra/commits/row-cache-finalizer).  This confirms that row cache is responsible.  Here is a sample of the log:

{noformat}
DEBUG [Finalizer] 2013-06-08 06:49:58,656 FreeableMemory.java (line 69) Unreachable memory still has nonzero refcount 1
DEBUG [Finalizer] 2013-06-08 06:49:58,656 FreeableMemory.java (line 71) Unreachable memory 140337996747792 has not been freed (will free now)
DEBUG [Finalizer] 2013-06-08 06:49:58,656 FreeableMemory.java (line 69) Unreachable memory still has nonzero refcount 1
DEBUG [Finalizer] 2013-06-08 06:49:58,656 FreeableMemory.java (line 71) Unreachable memory 140337989287984 has not been freed (will free now)
{noformat}

That is, memory is not being freed because we never got to zero references."
CASSANDRA-5619,CAS UPDATE for a lost race: save round trip by returning column values,"Looking at the new CAS CQL3 support examples [1], if one lost a race for an UPDATE, to save a round trip to get the current values to decide if you need to perform your work, could the columns that were used in the IF clause also be returned to the caller?  Maybe the columns values as part of the SET part could also be returned.

I don't know if this is generally useful though.

In the case of creating a new user account with a given username which is the partition key, if one lost the race to another person creating an account with the same username, it doesn't matter to the loser what the column values are, just that they lost.

I'm new to Cassandra, so maybe there's other use cases, such as doing incremental amount of work on a row.  In pure Java projects I've done while loops around AtomicReference.html#compareAndSet() until the work was done on the referenced object to handle multiple threads each making forward progress in updating the references object.

[1] https://github.com/riptano/cassandra-dtest/blob/master/cql_tests.py#L3044"
CASSANDRA-5612,NPE when upgrading a mixed version 1.1/1.2 cluster fully to 1.2,"See the attached upgrade_through_versions_test.py upgrade_test_mixed().

Conceptually this method does the following:

* Instantiates a 3 node 1.1.9 cluster
* Writes some data
* Shuts down node 1 and upgrades it to 1.2 (HEAD)
* Brings the node1 back up, making the cluster a mixed version 1.1/1.2
* Brings down node2 and node3 and does the same upgrade making it all the same version.
* At this point, I would run upgradesstables on each of the nodes, but there is already an error on node3 directly after it's upgrade:

{code}
INFO [FlushWriter:1] 2013-06-03 22:49:46,543 Memtable.java (line 461) Writing Memtable-peers@1023263314(237/237 serialized/live bytes, 14 op
s)
 INFO [FlushWriter:1] 2013-06-03 22:49:46,556 Memtable.java (line 495) Completed flushing /tmp/dtest-YqMtHN/test/node3/data/system/peers/syst
em-peers-ic-2-Data.db (291 bytes) for commitlog position ReplayPosition(segmentId=1370314185862, position=58616)
 INFO [GossipStage:1] 2013-06-03 22:49:46,568 StorageService.java (line 1330) Node /127.0.0.2 state jump to normal
ERROR [MigrationStage:1] 2013-06-03 22:49:46,655 CassandraDaemon.java (line 192) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:511)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:445)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{code}

This error is repeatable, but inconsistent. Interestingly, it is always node3 with the error."
CASSANDRA-5605,Crash caused by insufficient disk space to flush,"A few times now I have seen our Cassandra nodes crash by running themselves out of memory. It starts with the following exception:

{noformat}
ERROR [FlushWriter:13000] 2013-05-31 11:32:02,350 CassandraDaemon.java (line 164) Exception in thread Thread[FlushWriter:13000,5,main]
java.lang.RuntimeException: Insufficient disk space to write 8042730 bytes
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat} 

After which, it seems the MemtablePostFlusher stage gets stuck and no further memtables get flushed: 

{noformat} 
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,467 StatusLogger.java (line 68) MemtablePostFlusher               1        32         0
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,469 StatusLogger.java (line 73) CompactionManager                 1         2
{noformat} 

What makes this ridiculous is that, at the time, the data directory on this node had 981GB free disk space (as reported by du). We primarily use STCS and at the time the aforementioned exception occurred, at least one compaction task was executing which could have easily involved 981GB (or more) worth of input SSTables. Correct me if I am wrong but but Cassandra counts data currently being compacted against available disk space. In our case, this is a significant overestimation of the space required by compaction since a large portion of the data being compacted has expired or is an overwrite.

More to the point though, Cassandra should not crash because its out of disk space unless its really actually out of disk space (ie, dont consider 'phantom' compaction disk usage when flushing). I have seen one of our nodes die in this way before our alerts for disk space even went off."
CASSANDRA-5595,Add counters to TRACE for how much CASSANDRA-5514 helped a query,It might be an interesting stat to keep track of how many sstables got skipped by the comparisons added in CASSANDRA-5514.  Could be exposed in TRACE.
CASSANDRA-5564,fix memorySize bugs,
CASSANDRA-5559,Collection size overflow not handled in CQL3 binary protocol responses,"In the binary/native protocol collection sizes are specified with an unsigned int, but it's still possible to create collections larger than 2^16 items. When the client asks for a row with a collection that is bigger than this the collection size field overflows and the client will see an inconsistent size. If the collection size is 2^16 + 1 the client will see a size of 1.

All of the items in the collection are actually in the response, and the frame is still correct, it's just that a client that interprets the protocol strictly will not see them (I don't know how the Java CQL3 driver handles this, but my reading of the spec for the Ruby driver I'm writing means that I can't read those values without going outside of the spec).

I don't know exactly what the correct thing to do is. The way it works now leads to weird results (getting only one item when the collection is 2^16 + 1), and getting only the first 2^16 items may be less surprising."
CASSANDRA-5519,Reduce index summary memory use for cold sstables,
CASSANDRA-5508,Expose whether jna is enabled and memory is locked via JMX,"This may not be possible, but it would be very useful.  Currently the only definitive way to determine whether JNA is enabled and that it's able to lock the memory it needs is to look at the startup log.

It would be great if there was a way to store whether it is enabled so that jmx (or nodetool) could easily tell if JNA was enabled and whether it was able to lock the memory."
CASSANDRA-5506,Reduce memory consumption of IndexSummary,"I am evaluating cassandra for a use case with many tiny rows which would result in a node with 1-3TB of storage having billions of rows. Before loading that much data I am hitting GC issues and when looking at the heap dump I noticed that 70+% of the memory was used by IndexSummaries. 

The two major issues seem to be:

1) that the positions are stored as an ArrayList<Long> which results in each position taking 24 bytes (class + flags + 8 byte long). This might make sense when the file is initially written but once it has been serialized it would be a lot more memory efficient to just have an long[] (really a int[] would be fine unless 2GB sstables are allowed).

2) The DecoratedKey for a byte[16] key takes 195 bytes -- this is for the overhead of the ByteBuffer in the key and overhead in the token.

To somewhat ""work around"" the problem I have increased index_sample but will this many rows that didn't really help starts to have diminishing returns. 


NOTE: This heap dump was from linux with a 64bit oracle vm. 
"
CASSANDRA-5498,Possible NPE on EACH_QUORUM writes,"When upgrading from 1.0 to 1.1, we observed that DatacenterSyncWriteResponseHandler.assureSufficientLiveNodes() can throw an NPE if one of the writeEndpoints has a DC that is not listed in the keyspace while one of the nodes is down. We observed this while running in EC2, and using the Ec2Snitch. The exception typically was was brief, but a certain segment of writes (using EACH_QUORUM) failed during that time.

This ticket will address the NPE in DSWRH, while a followup ticket will be created once we get to the bottom of the incorrect DC being reported from Ec2Snitch.
"
CASSANDRA-5497,Use allocator information to improve memtable memory usage estimate,"A user reported that Cassandra's estimate of memtable space used was off by a factor of between 3 and 10 for his counter columnfamilies.

We may or may not be able to fix the counter estimate (counter merging is a cranky best, and unlike normal merging can involve allocating new objects), but we can definitely use the SlabAllocator information to cap the error in our estimate at a fairly low amount."
CASSANDRA-5488,CassandraStorage throws NullPointerException (NPE) when widerows is set to 'true',"CassandraStorage throws NPE when widerows is set to 'true'. 

2 problems in getNextWide:
1. Creation of tuple without specifying size
2. Calling addKeyToTuple on lastKey instead of key

java.lang.NullPointerException
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:34)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:26)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.addKeyToTuple(CassandraStorage.java:313)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide(CassandraStorage.java:196)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:224)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
    at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)
2013-04-16 12:28:03,671 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task"
CASSANDRA-5469,Race condition between index building and scrubDirectories() at startup,"From user group http://www.mail-archive.com/user@cassandra.apache.org/msg29207.html

In CassandraDaemon.setup() the call to SystemTable.checkHealth() results in the CFS's being created. As part of their creation they kick of async secondary index build if the index is not marked as built (SecondaryIndexManager.addIndexedColumn()). Later in CD.setup() the call is made to scrub the data dirs and this can race with the tmp files created by the index rebuild. The result is an error that prevents the node starting.

Should we delay rebuilding secondary indexes until after startup has completed or rebuild them synchronously ? 
"
CASSANDRA-5447,Include fatal errors in trace events,This would help tracking down which query is causing errors.
CASSANDRA-5446,Invalid delete statement causes NPE in CQL2,"A bad delete, (no WHERE), causes an NPE in CQL2

{noformat}
cqlsh> use cfs_archive;
cqlsh:cfs_archive> select * from rules;
cqlsh:cfs_archive> delete from rules;
TSocket read 0 bytes
{noformat}

system.log
{noformat}
ERROR [Thrift:6] 2013-04-09 13:18:29,167 CustomTThreadPoolServer.java (line 210) Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.cql.CqlParser.deleteStatement(CqlParser.java:2034)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:303)
	at com.datastax.bdp.server.Cql2Handler.parseQuery(Cql2Handler.java:298)
	at com.datastax.bdp.server.Cql2Handler.<init>(Cql2Handler.java:71)
	at com.datastax.bdp.server.DseServer.makeCqlHandlerForStatement(DseServer.java:934)
	at com.datastax.bdp.server.DseServer.execute_cql_query(DseServer.java:893)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3637)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3625)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at com.datastax.bdp.transport.server.ClientSocketAwareProcessor.process(ClientSocketAwareProcessor.java:25)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:192)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
{noformat}"
CASSANDRA-5410,incremental backups race,"incremental backups does not mark things referenced or compacting, so it could get compacted away before createLinks runs.  Occasionally you can see this happen during ColumnFamilyStoreTest.  (Since it runs on the background tasks stage, it does not fail the test.)

{noformat}
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ja-8-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1066)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
{noformat}"
CASSANDRA-5404,NPE during cql3 select with token(),"A query such as: select * from ""Standard1"" where token(key) > token(int(3030343330393233)) limit 1;

Produces:


{noformat}
 WARN 17:53:44,448 Inputing CLQ3 blobs as strings (like key = '') is now deprecated and will be removed in a future version. You should convert client code to use a blob constant (key = 0x) instead (see http://cassandra.apache.org/doc/cql3/CQL.html changelog section for more info).
ERROR 17:57:52,312 Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.cql3.functions.FunctionCall$Raw.isAssignableTo(FunctionCall.java:135)
        at org.apache.cassandra.cql3.functions.Functions.validateTypes(Functions.java:131)
        at org.apache.cassandra.cql3.functions.Functions.get(Functions.java:92)
        at org.apache.cassandra.cql3.functions.FunctionCall$Raw.prepare(FunctionCall.java:103)
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1246)
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:959)
        at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:271)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1726)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
{noformat}"
CASSANDRA-5384,SSTables are evicted from the page cache during compaction even if populate_io_cache_on_flush is true,"AbstractCompactionStrategy acquires direct scanners on SSTables to be compacted. These scanners are always created with skipIOCache set true. Because of this, compactions even for CFs that have populate_io_cache_on_flush set to true will evict source SSTables from the page cache after 128MB (CACHE_FLUSH_INTERVAL_IN_BYTES in RandomAccessReader) have been read from them. 

This leads to disk reads even in cases where the dataset completely fits into memory and unnecessarily limits compaction throughput on nodes that have lots of RAM.

Maybe compaction strategy should try to avoid skipping IO cache if CF has populate_io_cache_on_flush set to true?"
CASSANDRA-5377,MemoryMeter miscalculating memtable live ratio,"I've noticed the following logs in our running cluster:
WARN [MemoryMeter:1] 2013-03-17 23:15:55,876 Memtable.java (line 197) setting live ratio to minimum of 1.0 instead of 0.6378445488771007

It seems odd for the deep size calculation to be smaller than the aggregate sum of serialized columns.  Perhaps it's because we're mutating on a bunch of existing columns, or perhaps there's some miscalculation somewhere

Our column families all have regular columns (no super columns, expiring columns, etc)"
CASSANDRA-5350,Fix ColumnFamily opening race,"(Moving from CASSANDRA-5151)

Currently, MeteredFlusher is scheduled inside static block of ColumnFamilyStore,  and it accesses all ColumnFamilyStore when it runs every 1 sec. Scheduling is done when JVM first load ColumnFamilyStore class, so after that, there is always a chance to open SSTables before doing scrub directory/remove compaction left overs.
We should move the content of static block at the end of CassandraDaemon setup."
CASSANDRA-5344,Make LCR less memory-abusive,"We've seen several reports of compaction causing GC pauses.  You would think this would be the fault of PCR (which materializes the rows in memory) but LCR seems to be more of a problem.

I hypothesize that PCR mostly generates just young-gen garbage, but since LCR keeps the BF and row index in-memory for a long time (from construction, until after the row has been merged and written), it gets tenured and can cause fragmentation or promotion failures."
CASSANDRA-5296,NPE on /init.d/cassandra start,"{code}
==> /var/log/cassandra/output.log <==
INFO 18:05:18,466 Logging initialized
 INFO 18:05:18,477 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_32
 INFO 18:05:18,478 Heap size: 6358564864/6358564864
 INFO 18:05:18,478 Classpath: /usr/share/java/mx4j-tools.jar:/usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.6.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/usr/share/cassandra/lib/guava-13.0.1.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.9.2.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-1.0.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.7.0.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/metrics-core-2.0.3.jar:/usr/share/cassandra/lib/netty-3.5.9.Final.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.7.2.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.7.2.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.4.1.jar:/usr/share/cassandra/lib/snaptree-0.1.jar:/usr/share/cassandra/apache-cassandra-1.2.1.jar:/usr/share/cassandra/apache-cassandra-thrift-1.2.1.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/cassandra/stress.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 18:05:20,021 JNA mlockall successful
 INFO 18:05:20,042 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 18:05:20,352 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 18:05:20,352 disk_failure_policy is stop
 INFO 18:05:20,368 Global memtable threshold is enabled at 2048MB
 INFO 18:05:21,401 Initializing key cache with capacity of 100 MBs.
 INFO 18:05:21,417 Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO 18:05:21,417 Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO 18:05:21,420 Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO 18:05:21,640 Opening /var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-ib-3 (191 bytes)
 INFO 18:05:22,185 Opening /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ib-3 (2674 bytes)
 INFO 18:05:22,214 Opening /var/lib/cassandra/data/system/schema_columns/system-schema_columns-ib-3 (437 bytes)
 INFO 18:05:22,240 Opening /var/lib/cassandra/data/system/peers/system-peers-ib-3 (150 bytes)
 INFO 18:05:22,240 Opening /var/lib/cassandra/data/system/peers/system-peers-ib-2 (300 bytes)
 INFO 18:05:22,240 Opening /var/lib/cassandra/data/system/peers/system-peers-ib-1 (193 bytes)
 INFO 18:05:22,395 Opening /var/lib/cassandra/data/system/local/system-local-ib-2 (120 bytes)
 INFO 18:05:22,395 Opening /var/lib/cassandra/data/system/local/system-local-ib-7 (84 bytes)
 INFO 18:05:22,396 Opening /var/lib/cassandra/data/system/local/system-local-ib-8 (535 bytes)
 INFO 18:05:22,396 Opening /var/lib/cassandra/data/system/local/system-local-ib-4 (146 bytes)
 INFO 18:05:22,395 Opening /var/lib/cassandra/data/system/local/system-local-ib-3 (90 bytes)
 INFO 18:05:22,395 Opening /var/lib/cassandra/data/system/local/system-local-ib-6 (97 bytes)
 INFO 18:05:22,396 Opening /var/lib/cassandra/data/system/local/system-local-ib-1 (351 bytes)
 INFO 18:05:22,396 Opening /var/lib/cassandra/data/system/local/system-local-ib-5 (97 bytes)
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:32)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:96)
	at org.apache.cassandra.config.CFMetaData.fromSchemaNoColumns(CFMetaData.java:1347)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1404)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:312)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:293)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:156)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:569)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Cannot load daemon
Service exit with a return value of 3
{code}


i suppose that happened because of two operations confict:
first i got timeout on cf truncate
then i`ve tried drop this cf and i got NPE in cassandra-cli

so now i cant start this node, no matter how i tried

 
"
CASSANDRA-5282,nodetool compact cause StackOverFlow,"[root@ca-10-0-0-24-ip bin]# Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.StackOverflowError
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:263)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1643)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1792)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:235)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:250)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:791)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1447)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:89)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1292)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1380)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:812)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.StackOverflowError
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)"
CASSANDRA-5273,Hanging system after OutOfMemory. Server cannot die due to uncaughtException handling,"On out of memory exception, there is an uncaughtexception handler that is calling System.exit(). However, multiple threads are calling this handler causing a deadlock and the server cannot stop working. See http://www.mail-archive.com/user@cassandra.apache.org/msg27898.html. And see stack trace in attachement."
CASSANDRA-5257,Compaction race allows sstables to be in multiple compactions simultaneously,"Reported by [~cscotta] on Twitter.  Here is a log fragment showing the 2110 sstable pulled into two compactions:

{noformat}
 INFO [CompactionExecutor:41495] 2013-02-14 14:19:26,621 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2110-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-1711-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2068-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-1391-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2115-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2052-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2089-Data.db')]
 INFO [OptionalTasks:1] 2013-02-14 14:20:28,978 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399218463 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:20:28,979 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@347404907(60626496/399218463 serialized/live bytes, 2165232 ops)
 INFO [FlushWriter:1590] 2013-02-14 14:20:28,980 Memtable.java (line 447) Writing Memtable-Metrics@347404907(60626496/399218463 serialized/live bytes, 2165232 ops)
 INFO [FlushWriter:1590] 2013-02-14 14:20:30,606 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2117-Data.db (21066414 bytes) for commitlog position ReplayPosition(segmentId=1360737579813, position=4969777)
 INFO [OptionalTasks:1] 2013-02-14 14:21:51,046 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399221413 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:21:51,046 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@663159049(60626944/399221413 serialized/live bytes, 2165248 ops)
 INFO [FlushWriter:1591] 2013-02-14 14:21:51,047 Memtable.java (line 447) Writing Memtable-Metrics@663159049(60626944/399221413 serialized/live bytes, 2165248 ops)
 INFO [FlushWriter:1591] 2013-02-14 14:21:52,692 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2118-Data.db (21071657 bytes) for commitlog position ReplayPosition(segmentId=1360737579815, position=14067099)
 INFO [OptionalTasks:1] 2013-02-14 14:23:13,059 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:23:13,060 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@480704694(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1592] 2013-02-14 14:23:13,061 Memtable.java (line 447) Writing Memtable-Metrics@480704694(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1592] 2013-02-14 14:23:14,677 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2119-Data.db (21074605 bytes) for commitlog position ReplayPosition(segmentId=1360737579817, position=23128798)
 INFO [OptionalTasks:1] 2013-02-14 14:24:35,073 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:24:35,073 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@2075924408(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1593] 2013-02-14 14:24:35,074 Memtable.java (line 447) Writing Memtable-Metrics@2075924408(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1593] 2013-02-14 14:24:36,683 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2120-Data.db (21044055 bytes) for commitlog position ReplayPosition(segmentId=1360737579819, position=32199135)
 INFO [CompactionExecutor:41571] 2013-02-14 14:24:36,684 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2120-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2119-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2117-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2118-Data.db')]
 INFO [CompactionExecutor:41571] 2013-02-14 14:25:12,234 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2121,].  84,256,731 bytes to 84,334,086 (~100% of original) in 35,549ms = 2.262434MB/s.  14,432 total rows, 14,432 unique.  Row merge counts were {1:14432, 2:0, 3:0, 4:0, }
 INFO [OptionalTasks:1] 2013-02-14 14:25:57,115 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399232107 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:25:57,117 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@258318328(60628568/399232107 serialized/live bytes, 2165306 ops)
 INFO [FlushWriter:1594] 2013-02-14 14:25:57,118 Memtable.java (line 447) Writing Memtable-Metrics@258318328(60628568/399232107 serialized/live bytes, 2165306 ops)
 INFO [FlushWriter:1594] 2013-02-14 14:25:58,705 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2122-Data.db (21059731 bytes) for commitlog position ReplayPosition(segmentId=1360737579822, position=7758494)
 INFO [OptionalTasks:1] 2013-02-14 14:27:19,129 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399218832 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:27:19,130 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@2026189692(60626552/399218832 serialized/live bytes, 2165234 ops)
 INFO [FlushWriter:1595] 2013-02-14 14:27:19,131 Memtable.java (line 447) Writing Memtable-Metrics@2026189692(60626552/399218832 serialized/live bytes, 2165234 ops)
 INFO [FlushWriter:1595] 2013-02-14 14:27:20,726 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2123-Data.db (21081156 bytes) for commitlog position ReplayPosition(segmentId=1360737579824, position=16830476)
 INFO [OptionalTasks:1] 2013-02-14 14:28:41,143 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:28:41,143 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@835031438(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1596] 2013-02-14 14:28:41,144 Memtable.java (line 447) Writing Memtable-Metrics@835031438(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1596] 2013-02-14 14:28:42,775 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2124-Data.db (21050393 bytes) for commitlog position ReplayPosition(segmentId=1360737579826, position=25891158)
 INFO [OptionalTasks:1] 2013-02-14 14:30:03,156 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:30:03,157 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@1604541794(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1597] 2013-02-14 14:30:03,158 Memtable.java (line 447) Writing Memtable-Metrics@1604541794(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1597] 2013-02-14 14:30:04,869 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2125-Data.db (21050877 bytes) for commitlog position ReplayPosition(segmentId=1360737579829, position=1405790)
 INFO [CompactionExecutor:41657] 2013-02-14 14:30:04,870 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2123-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2122-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2125-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2124-Data.db')]
 INFO [CompactionExecutor:41657] 2013-02-14 14:30:40,416 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2126,].  84,242,157 bytes to 84,305,839 (~100% of original) in 35,545ms = 2.261930MB/s.  14,432 total rows, 14,432 unique.  Row merge counts were {1:14432, 2:0, 3:0, 4:0, }
 INFO [OptionalTasks:1] 2013-02-14 14:31:25,169 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399240957 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:31:25,171 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@960476318(60629912/399240957 serialized/live bytes, 2165354 ops)
 INFO [FlushWriter:1598] 2013-02-14 14:31:25,172 Memtable.java (line 447) Writing Memtable-Metrics@960476318(60629912/399240957 serialized/live bytes, 2165354 ops)
 INFO [FlushWriter:1598] 2013-02-14 14:31:26,783 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2127-Data.db (21084189 bytes) for commitlog position ReplayPosition(segmentId=1360737579831, position=10480922)
 INFO [OptionalTasks:1] 2013-02-14 14:32:47,183 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399238376 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:32:47,184 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@852152579(60629520/399238376 serialized/live bytes, 2165340 ops)
 INFO [FlushWriter:1599] 2013-02-14 14:32:47,185 Memtable.java (line 447) Writing Memtable-Metrics@852152579(60629520/399238376 serialized/live bytes, 2165340 ops)
 INFO [FlushWriter:1599] 2013-02-14 14:32:48,794 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2128-Data.db (21052579 bytes) for commitlog position ReplayPosition(segmentId=1360737579833, position=19561582)
 INFO [OptionalTasks:1] 2013-02-14 14:34:09,197 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399229710 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:34:09,197 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@600711801(60628204/399229710 serialized/live bytes, 2165293 ops)
 INFO [FlushWriter:1600] 2013-02-14 14:34:09,198 Memtable.java (line 447) Writing Memtable-Metrics@600711801(60628204/399229710 serialized/live bytes, 2165293 ops)
 INFO [FlushWriter:1600] 2013-02-14 14:34:10,818 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2129-Data.db (21067262 bytes) for commitlog position ReplayPosition(segmentId=1360737579835, position=28636059)
 INFO [OptionalTasks:1] 2013-02-14 14:35:31,238 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399221598 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:35:31,239 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@2104370804(60626972/399221598 serialized/live bytes, 2165249 ops)
 INFO [FlushWriter:1601] 2013-02-14 14:35:31,240 Memtable.java (line 447) Writing Memtable-Metrics@2104370804(60626972/399221598 serialized/live bytes, 2165249 ops)
 INFO [FlushWriter:1601] 2013-02-14 14:35:32,844 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2130-Data.db (21078893 bytes) for commitlog position ReplayPosition(segmentId=1360737579838, position=4177813)
 INFO [CompactionExecutor:41742] 2013-02-14 14:35:32,845 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2128-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2129-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2130-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2127-Data.db')]
 INFO [CompactionExecutor:41742] 2013-02-14 14:36:08,384 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2131,].  84,282,923 bytes to 84,299,440 (~100% of original) in 35,538ms = 2.262204MB/s.  14,432 total rows, 14,432 unique.  Row merge counts were {1:14432, 2:0, 3:0, 4:0, }
 INFO [CompactionExecutor:41767] 2013-02-14 14:36:08,385 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2126-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2131-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2115-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2121-Data.db')]
 INFO [OptionalTasks:1] 2013-02-14 14:36:53,252 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399229895 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:36:53,253 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@562526332(60628232/399229895 serialized/live bytes, 2165294 ops)
 INFO [FlushWriter:1602] 2013-02-14 14:36:53,254 Memtable.java (line 447) Writing Memtable-Metrics@562526332(60628232/399229895 serialized/live bytes, 2165294 ops)
 INFO [FlushWriter:1602] 2013-02-14 14:36:55,065 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2133-Data.db (21065439 bytes) for commitlog position ReplayPosition(segmentId=1360737579840, position=13247694)
 INFO [OptionalTasks:1] 2013-02-14 14:38:15,266 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:38:15,267 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@1048911247(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1603] 2013-02-14 14:38:15,268 Memtable.java (line 447) Writing Memtable-Metrics@1048911247(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1603] 2013-02-14 14:38:16,899 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2134-Data.db (21065175 bytes) for commitlog position ReplayPosition(segmentId=1360737579842, position=22318066)
 INFO [CompactionExecutor:41767] 2013-02-14 14:38:32,197 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2132,].  337,297,607 bytes to 337,468,452 (~100% of original) in 143,810ms = 2.237918MB/s.  57,728 total rows, 57,728 unique.  Row merge counts were {1:57728, 2:0, 3:0, 4:0, }
 INFO [CompactionExecutor:41811] 2013-02-14 14:38:32,197 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2110-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2132-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2068-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2089-Data.db')]
ERROR [CompactionExecutor:41495] 2013-02-14 14:38:35,720 CassandraDaemon.java (line 133) Exception in thread Thread[CompactionExecutor:41495,1,RMI Runtime]
java.lang.AssertionError: Memory was freed
  at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:146)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:116)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:176)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:88)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:327)
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:755)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:792)
	at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:114)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:101)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:235)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:109)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:93)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:162)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:158)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:71)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:342)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}"
CASSANDRA-5256,"""Memory was freed"" AssertionError During Major Compaction","When initiating a major compaction with `./nodetool -h localhost compact`, an AssertionError is thrown in the CompactionExecutor from o.a.c.io.util.Memory:

ERROR [CompactionExecutor:41495] 2013-02-14 14:38:35,720 CassandraDaemon.java (line 133) Exception in thread Thread[CompactionExecutor:41495,1,RMI Runtime]
java.lang.AssertionError: Memory was freed
  at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:146)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:116)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:176)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:88)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:327)
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:755)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:792)
	at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:114)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:101)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:235)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:109)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:93)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:162)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:158)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:71)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:342)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

---

I've invoked the `nodetool compact` three times; this occurred after each. The node has been up for a couple days accepting writes and has not been restarted.

Here's the server's log since it was started a few days ago: https://gist.github.com/cscotta/4956472/raw/95e7cbc68de1aefaeca11812cbb98d5d46f534e8/cassandra.log

Here's the code being used to issue writes to the datastore: https://gist.github.com/cscotta/20cbd36c2503c71d06e9

---

Configuration: One node, one keyspace, one column family. ~60 writes/second of data with a TTL of 86400, zero reads. Stock cassandra.yaml.

Keyspace DDL:

create keyspace jetpack;
use jetpack;
create column family Metrics with key_validation_class = 'UTF8Type' and comparator = 'IntegerType';"
CASSANDRA-5253,NPE while loading Saved KeyCache,"This bug occurred in the Beta version and was marked as fixed in this Jira: CASSANDRA-4553

However it seems to have reoccurred in the production 1.2.1 release. This is the first install I have made of Cassandra (so a clean install), which I downloaded prepackaged from http://www.apache.org/dyn/closer.cgi?path=/cassandra/1.2.1/apache-cassandra-1.2.1-bin.tar.gz

I have created a keyspace but not inserted any data, so that is not the issue either.

Here is a sample from the logs all the way from startup
{code}
 INFO [main] 2013-02-07 19:48:54,109 CassandraDaemon.java (line 101) Logging initialized
 INFO [main] 2013-02-07 19:48:54,125 CassandraDaemon.java (line 123) JVM vendor/version: Java HotSpot(TM) Client VM/1.7.0_11
 INFO [main] 2013-02-07 19:48:54,125 CassandraDaemon.java (line 124) Heap size: 1067057152/1067057152
 INFO [main] 2013-02-07 19:48:54,126 CassandraDaemon.java (line 125) Classpath: C:\Cassandra\\conf;C:\Cassandra\\lib\antlr-3.2.jar;C:\Cassandra\\lib\apache-cassandra-1.2.1.jar;C:\Cassandra\\lib\apache-cassandra-clientutil-1.2.1.jar;C:\Cassandra\\lib\apache-cassandra-thrift-1.2.1.jar;C:\Cassandra\\lib\avro-1.4.0-fixes.jar;C:\Cassandra\\lib\avro-1.4.0-sources-fixes.jar;C:\Cassandra\\lib\commons-cli-1.1.jar;C:\Cassandra\\lib\commons-codec-1.2.jar;C:\Cassandra\\lib\commons-lang-2.6.jar;C:\Cassandra\\lib\compress-lzf-0.8.4.jar;C:\Cassandra\\lib\concurrentlinkedhashmap-lru-1.3.jar;C:\Cassandra\\lib\guava-13.0.1.jar;C:\Cassandra\\lib\high-scale-lib-1.1.2.jar;C:\Cassandra\\lib\jackson-core-asl-1.9.2.jar;C:\Cassandra\\lib\jackson-mapper-asl-1.9.2.jar;C:\Cassandra\\lib\jamm-0.2.5.jar;C:\Cassandra\\lib\jline-1.0.jar;C:\Cassandra\\lib\json-simple-1.1.jar;C:\Cassandra\\lib\libthrift-0.7.0.jar;C:\Cassandra\\lib\log4j-1.2.16.jar;C:\Cassandra\\lib\metrics-core-2.0.3.jar;C:\Cassandra\\lib\netty-3.5.9.Final.jar;C:\Cassandra\\lib\servlet-api-2.5-20081211.jar;C:\Cassandra\\lib\slf4j-api-1.7.2.jar;C:\Cassandra\\lib\slf4j-log4j12-1.7.2.jar;C:\Cassandra\\lib\snakeyaml-1.6.jar;C:\Cassandra\\lib\snappy-java-1.0.4.1.jar;C:\Cassandra\\lib\snaptree-0.1.jar;C:\Cassandra\\build\classes\main;C:\Cassandra\\build\classes\thrift;C:\Cassandra\\lib\jamm-0.2.5.jar
 INFO [main] 2013-02-07 19:48:54,130 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2013-02-07 19:48:54,147 DatabaseDescriptor.java (line 131) Loading settings from file:/C:/Cassandra/conf/cassandra.yaml
 INFO [main] 2013-02-07 19:48:54,515 DatabaseDescriptor.java (line 150) 32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance.
 INFO [main] 2013-02-07 19:48:54,516 DatabaseDescriptor.java (line 190) DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
 INFO [main] 2013-02-07 19:48:54,516 DatabaseDescriptor.java (line 204) disk_failure_policy is stop
 INFO [main] 2013-02-07 19:48:54,524 DatabaseDescriptor.java (line 267) Global memtable threshold is enabled at 339MB
 INFO [main] 2013-02-07 19:48:55,099 CacheService.java (line 111) Initializing key cache with capacity of 50 MBs.
 INFO [main] 2013-02-07 19:48:55,109 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-02-07 19:48:55,110 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-02-07 19:48:55,117 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,452 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-1 (258 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,484 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-3 (262 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,489 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-2 (262 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,517 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-1 (4420 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,522 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-3 (4424 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,525 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-2 (4424 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,543 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-3 (3750 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,548 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-1 (3747 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,553 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-2 (3748 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,588 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-16 (119 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,594 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-18 (436 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,600 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-17 (109 bytes)
 INFO [main] 2013-02-07 19:48:55,610 AutoSavingCache.java (line 139) reading saved cache C:\Cassandra\saved_caches\system-local-KeyCache-b.db
 WARN [main] 2013-02-07 19:48:55,614 AutoSavingCache.java (line 160) error reading saved cache C:\Cassandra\saved_caches\system-local-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
	at org.apache.cassandra.db.Table.initCf(Table.java:337)
	at org.apache.cassandra.db.Table.<init>(Table.java:280)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.db.Table.open(Table.java:88)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:56,212 SSTableReader.java (line 164) Opening C:\Cassandra\data\system_auth\users\system_auth-users-ib-1 (72 bytes)
 INFO [main] 2013-02-07 19:48:56,242 CassandraDaemon.java (line 224) completed pre-loading (3 keys) key cache.
{code}"
CASSANDRA-5241,Fix forceBlockingFlush,"ForceBlockingFlush doesn't guarantee that after the call, every that the thread has written prior to the call will be fully flushed. At least not in the case of concurrent flushes, because if 2 threads flush roughly at the same time, one will have it's forceBlockingFlush call return immediately because the memtable will be clean (even though some of the thread writes may have not be fully flushed yet).

I think this is very fragile and make it easy to have hard to find races and so we should fix it. Typically a forceFlush that see a clean memtable could submit a dummy task in the postFlushExecutor and wait for that."
CASSANDRA-5221,NPE while upgrading from 1.1 to 1.2,"ERROR 00:29:25,208 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.db.SystemTable.upgradeSystemData(SystemTable.java:161)
	at org.apache.cassandra.db.SystemTable.finishStartup(SystemTable.java:107)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:276)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
java.lang.NullPointerException
	at org.apache.cassandra.db.SystemTable.upgradeSystemData(SystemTable.java:161)
	at org.apache.cassandra.db.SystemTable.finishStartup(SystemTable.java:107)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:276)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)"
CASSANDRA-5213,Failed to flush SSTable can cause commit log not to reply,"When flushing failed(e.g. out of disk space), that memtable was kept in memtablePendingFlush and might trigger next flush inside MeteredFlusher since it would be always calculated in countFlushingBytes.

If that next flush succeeded(e.g. this time it fit on disk), ReplayPosition was recorded at that point, and would cause commit log not to replay for the data that was not flushed."
CASSANDRA-5211,"Migrating Clusters with gossip tables that have old dead nodes causes NPE, inability to join cluster","I had done a removetoken on this cluster when it was 1.1.x, and it had a ""ghost"" entry for the removed node still in the stored ring data. When the nodes loaded the table up after conversion to 1.2 and attempting to migrate to VNodes, I got the following traceback:

ERROR [WRITE-/10.0.0.0] 2013-01-31 18:35:44,788 CassandraDaemon.java (line 133) Exception in thread Thread[WRITE-/10.0.0.0,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:32)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:96)
	at org.apache.cassandra.db.SystemTable.loadDcRackInfo(SystemTable.java:402)
	at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:117)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
	at org.apache.cassandra.net.OutboundTcpConnection.isLocalDC(OutboundTcpConnection.java:74)
	at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:270)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:142)

This is because these ghost nodes had a NULL tokens list in the system/peers table. A workaround was to delete the offending row in the system/peers table and restart the node."
CASSANDRA-5210,DB is randomly and undetectably corrupted during high traffic column family flushes ,"Writes during high traffic column family flushes corrupt the DB and make slice queries return incorrect data.

Any multi-column write on any version of Cassandra can put the DB in a state where some columns cannot be read alongside other columns.

eg.

{{
// *** for any NON-NULL column (eg. col_a=>AAA)
cqlsh> SELECT 'col_a' FROM test WHERE KEY='row_a';
   returns:     'AAA'

// *** it can disappear when queried alongside another column
cqlsh> SELECT 'col_a', 'col_b' FROM test WHERE KEY='row_a';
   returns:      null,   'BBB' // *** col_a is MISSING

// *** but it depends on the other columns
cqlsh> SELECT 'col_a', 'col_b', 'col_c' FROM test WHERE KEY='row_a';
   returns:     'AAA',   'BBB',   'CCC' // *** col_a is BACK
}}

Once in this state the database is corrupt and essentially returning random data depending on what columns you query. Single column queries always return correct results so there is no way to verify the data. No errors are logged during corruption and it is impossible to detect without querying all combinations of all columns.

To reproduce:

1. Unzip a distribution of Cassandra and create a test.test column family.
2. In a loop alternate between updating either row 'a' or a random row.
   Write a random value to four random columns (out of 10000). Keep track
   of all columns set in row 'a'.
3. Each pass through the loop query four random columns (out of 10000) from row 'a'. If a column that is known to be set is null, print out the columns that were requested during the query.
4. The DB is now corrupt and will return the column if queried by itself but will return null if queried alongside the columns that triggered the error. This is a permanent condition.


Observations: This bug only manifests directly after a high traffic column family flush occurs in the log. This is a correlation based on simply watching the log. There are no errors or warnings of any kind.

Workaround: Any multi-column read is potentially invalid and corruption is virtually undetectable. The only workaround is never writing or reading more than a single column in a query.

I have a simple groovy script that can trigger the error. I have verified the behavior on Cassandra versions as old as 0.8.1
"
CASSANDRA-5193,Memtable flushwriter can pick a blacklisted directory,"The top-level data directory will be picked by DiskAwareRunnable (directory = Directories.getLocationCapableOfSize(writeSize)), and the top-level data directory itself might not be blacklisted (most likely won't be).

For the same reason we can't just add a blacklist-check in the middle of Directories#getLocationCapableOfSize - most often it's the sstable directory that gets blacklisted.

The issue seems to be caused by/related to CASSANDRA-4292, which was committed just two days prior 2116-2118 and undid some blacklist-aware directory-picking logic.

Anyway, DiskAwareRunnable should be altered to respect directory blacklist."
CASSANDRA-5191,BufferOverflowException in CommitLogSegment,"Running mixed reads, writes and deletes on a single column family in a two node cluster. After a few minutes the following appears in the system log:

ERROR [COMMIT-LOG-WRITER] 2013-01-25 12:49:55,955 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.nio.BufferOverflowException
	at java.nio.Buffer.nextPutIndex(Buffer.java:499)
	at java.nio.DirectByteBuffer.putLong(DirectByteBuffer.java:756)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:265)
	at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:382)
	at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:50)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Thread.java:662)

Possibly related to https://issues.apache.org/jira/browse/CASSANDRA-3615"
CASSANDRA-5186,Schema NPEs in 1.1.6.  Upgrading to 1.1.9 causes IllegalArgumentException loading Schema,"We experienced the problems described in CASSANDRA-4219 in version 1.1.6.  We were told to upgrade to 1.1.9.  Now the schema fails to load with the following error:

java.lang.IllegalArgumentException: value already present: 1105
at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:112)
at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:96)
at com.google.common.collect.HashBiMap.put(HashBiMap.java:85)
at org.apache.cassandra.config.Schema.load(Schema.java:399)
at org.apache.cassandra.config.Schema.load(Schema.java:120)
at org.apache.cassandra.config.Schema.load(Schema.java:105)
at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:534)
at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:183)
at com.palantir.phoenix.datastore.cass.server.PTCassandraServer$PTCassandraDaemon.setup(PTCassandraServer.java:64)
at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:389)"
CASSANDRA-5160,CFS.allUserDefined() doesn't exclude system_auth and system_traces keysapces,Make sure CFS.allUserDefined() excludes all system-related keyspaces.
CASSANDRA-5111,StackOverflowError after upgrading to Cassandra 1.2.0,"After upgrading C* from 1.1.7 to 1.2.0 I have a java.lang.StackOverflowError message error on my one node dev cluster.

I changed the heap size from 3 GB to 2 GB (auto/default) considering bloomfilters are now out of the heap.

I think this was the only changes I have made (upgrade & reduce the heap)
"
CASSANDRA-5058,debian packaging should include shuffle,"Our debian packaging doesn't currently include shuffle, but we should add it so people have a way of upgrading to vnodes.  This might also be a good time to consider a different name for shuffle, though I don't believe it currently conflicts with anything else."
CASSANDRA-5025,Schema push/pull race,"When a schema change is made, the coordinator pushes the delta to the other nodes in the cluster.  This is more efficient than sending the entire schema.  But the coordinator also announces the new schema version, so the other nodes' reception of the new version races with processing the delta, and usually seeing the new schema wins.  So the other nodes also issue a pull to the coordinator for the entire schema.

Thus, schema changes tend to become O(n) in the number of KS and CF present."
CASSANDRA-5000,Null pointer exception in SecondaryIndexManager.getIndexKeyFor in Cassandra 1.1.x,"Cassandra 1.1.0 and following releases gets an NPE in SecondaryIndexManager writing a CF with multiple secondary keys.   Problem did not occur in 1.0.x, and can be resolved by downgrading the Cassandra server.   Stack trace is:

ERROR [MutationStage:47] 2012-11-28 11:24:30,865 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:47,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.index.SecondaryIndexManager.getIndexKeyFor(SecondaryIndexManager.java:299)
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:463)
	at org.apache.cassandra.db.Table.apply(Table.java:459)
	at org.apache.cassandra.db.Table.apply(Table.java:384)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)

Client stack trace is:

me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:35)[hector-core-1.0-5.jar:]
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:264)[hector-core-1.0-5.jar:]
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecuteOperation(ExecutingKeyspace.java:97)[hector-core-1.0-5.jar:]
	at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)[hector-core-1.0-5.jar:]
	... 3 more"
CASSANDRA-4956,exclude system_traces from repair,"When a repair is issued, the system ks is skipped but not system_traces."
CASSANDRA-4906,Avoid flushing other columnfamilies on truncate,"Currently truncate flushes *all* columnfamilies so it can get rid of the commitlog segments containing truncated data.  Otherwise, it could be replayed on restart since the replay position is contained in the sstables we're trying to delete."
CASSANDRA-4895,Debian packaging pulls jna.jar 3.4.2 but platform.jar 3.2.7,"It appears that the debian package pulls mixed versions of jna jars - 3.4.2 for the jna.jar itself and 3.2.7 for the platform.jar.  This creates problems when running operations like nodetool snapshot, which just hangs for no reason."
CASSANDRA-4880,Endless loop flushing+compacting system/schema_keyspaces and system/schema_columnfamilies,"After upgrading a node from 1.1.2 to 1.1.6, the startup sequence entered a loop as seen here:

http://mina.naguib.ca/misc/cassandra_116_startup_loop.txt

Stopping and starting the node entered the same loop.

Reverting back to 1.1.2 started successfully."
CASSANDRA-4855,Debian packaging doesn't do auto-reloading of log4j properties file,Cassandra isn't starting the log4j auto-reload thread because it requires -Dlog4j.defaultInitOverride=true on initialization. Is there a reason to not do this when installed from the Debian package?
CASSANDRA-4852,make trace output pretty,"In 069de3d0357fde037f3d9a25c7cef38b5ae5e5a8 I changed trace output to only show activity, source, and source elapsed.  This is fairly usable but we can do better."
CASSANDRA-4846,BulkLoader throws NPE at start up,"BulkLoader in trunk throws below exception at start up and exit abnormally.

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:180)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:148)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:96)
	at java.io.File.list(File.java:1010)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:117)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:63)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.CacheService.initRowCache(CacheService.java:154)
	at org.apache.cassandra.service.CacheService.<init>(CacheService.java:102)
	at org.apache.cassandra.service.CacheService.<clinit>(CacheService.java:83)
	... 8 more
{code}

This comes from CASSANDRA-4732, which moved keyCache in SSTableReader initialization at instance creation. This causes access to CacheService that did not happen for v1.1 and ends up NPE because BulkLoader does not load cassandra.yaml."
CASSANDRA-4836,NPE on PREPARE of INSERT using binary protocol,"This started happening on 297f530c. I've implemented the consistency level specification, and executing other queries works. Running a prepare with an INSERT statement results in this exception

{noformat}
ERROR 13:11:48,677 Unexpected exception during request
java.lang.NullPointerException
	at org.apache.cassandra.cql3.ResultSet$Metadata.allInSameCF(ResultSet.java:234)
	at org.apache.cassandra.cql3.ResultSet$Metadata.<init>(ResultSet.java:215)
	at org.apache.cassandra.transport.messages.ResultMessage$Prepared.<init>(ResultMessage.java:274)
	at org.apache.cassandra.cql3.QueryProcessor.storePreparedStatement(QueryProcessor.java:209)
	at org.apache.cassandra.cql3.QueryProcessor.prepare(QueryProcessor.java:185)
	at org.apache.cassandra.transport.messages.PrepareMessage.execute(PrepareMessage.java:58)
	at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:212)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:563)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
	at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
	at org.jboss.netty.handler.execution.OrderedMemoryAwareThreadPoolExecutor$ChildExecutor.run(OrderedMemoryAwareThreadPoolExecutor.java:315)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}"
CASSANDRA-4820,Add a close() method to CRAR to prevent leaking file descriptors.,"The problem is that under heavy load Finalizer daemon is unable to keep up with number of ""source"" and ""channel"" fields from CRAR to finalize (as FileInputStream has custom finalize() which calls close inside) which creates memory/cpu pressure on the machine."
CASSANDRA-4798,ValidationExecutor throws StackOverflowError during repair with LCS,"During a repair, I get StackOverflowError originating from ValidationExecutor. All CFs had been offline-scrubed after CASSANDRA-4411 fix.

{code}
2012-10-12_13:07:39.12921 ERROR 13:07:39,120 Exception in thread Thread[ValidationExecutor:2,1,main]
2012-10-12_13:07:39.12929 java.lang.StackOverflowError
2012-10-12_13:07:39.12934 	at sun.nio.cs.US_ASCII$Encoder.encodeLoop(Unknown Source)
2012-10-12_13:07:39.12942 	at java.nio.charset.CharsetEncoder.encode(Unknown Source)
2012-10-12_13:07:39.12950 	at java.lang.StringCoding$StringEncoder.encode(Unknown Source)
2012-10-12_13:07:39.12958 	at java.lang.StringCoding.encode(Unknown Source)
2012-10-12_13:07:39.12964 	at java.lang.String.getBytes(Unknown Source)
2012-10-12_13:07:39.12969 	at java.io.RandomAccessFile.open(Native Method)
2012-10-12_13:07:39.12976 	at java.io.RandomAccessFile.<init>(Unknown Source)
2012-10-12_13:07:39.12981 	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
2012-10-12_13:07:39.12990 	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:64)
2012-10-12_13:07:39.13003 	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:46)
2012-10-12_13:07:39.13014 	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1007)
2012-10-12_13:07:39.13024 	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
2012-10-12_13:07:39.13032 	at org.apache.cassandra.io.sstable.SSTableBoundedScanner.<init>(SSTableBoundedScanner.java:41)
2012-10-12_13:07:39.13043 	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:869)
2012-10-12_13:07:39.13053 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:247)
2012-10-12_13:07:39.13066 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
// More stack recursion goes here
2012-10-12_13:07:39.25061 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
{code}"
CASSANDRA-4786,NPE in migration stage after creating an index,"The dtests are generating this error after trying to create an index in cql2:

{noformat}

ERROR [MigrationStage:1] 2012-10-09 20:54:12,796 CassandraDaemon.java (line 132) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
ERROR [Thrift:1] 2012-10-09 20:54:12,797 CustomTThreadPoolServer.java (line 214) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:348)
    at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:238)
    at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:209)
    at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:714)
    at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:816)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1656)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:344)
    ... 13 more
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
{noformat}"
CASSANDRA-4765,StackOverflowError in CompactionExecutor thread,"Seeing the following error:


Exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.StackOverflowError
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)

"
CASSANDRA-4741,Cassandra stops persisting or flushing data,"Hello,
 
 we have a Cassandra instance running on a single node with Windows XP 32bit.
After some time of normal working and some restart, Cassandra seems to stop to persist data into the keyspace without raising any error.
It's possible to update/insert rows or columns and you can query and see the updated keyspace but if you stop and start Cassandra, you get a keyspace 'from the past', always from the same point in the past.

We were not able to understand how to trigger this inconsistency in the first place but once you get it, you can't get rid of it if not removing the data directories.
Here in attachment you can find a complete set of Cassandra data directories: 
{code}
   cassandra-data/
   |-- commitlog
   |-- data
   `-- saved_caches
{code}
with which it's possible test the inconsistent scenario. 
We tried this on another clean Win Xp 32 machine and we could get the same issue from the first run.
On a Win XP 64 and the same dir set, Cassandra didn't get into this issue, but from the log we saw a difference in the disk access mode:
* XP/64: {{DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap"" instead of 'standard'}}
* XP/32: {{DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard}}

Not sure this is the key factor, nevertheless in another test bed we experimented the same issue even on a WinXP 64 node but we couldn't collect the dataset.

Here a short cassandra-cli history to show the issue:
{code}
[default@NEUTRO_Keyspace] connect 192.168.1.33/9160;
Connected to: ""NEUTRO Cluster"" on 192.168.1.33/9160

[default@unknown] use neutro_keyspace;
Authenticated to keyspace: NEUTRO_Keyspace

[default@NEUTRO_Keyspace] describe;
Keyspace: NEUTRO_Keyspace:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: TaskId
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Column Metadata:
        Column Name: taskId
          Validation Class: org.apache.cassandra.db.marshal.LongType
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor

[default@NEUTRO_Keyspace] list TaskId;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: NE_MASTER_TASK_ID
=> (column=taskId, value=47, timestamp=1349090424494000)

1 Row Returned.
Elapsed time: 93 msec(s).

[default@NEUTRO_Keyspace] set  taskid['NE_MASTER_TASK_ID']['taskId']='55';
Value inserted.
Elapsed time: 31 msec(s).

[default@NEUTRO_Keyspace] list TaskId;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: NE_MASTER_TASK_ID
=> (column=taskId, value=55, timestamp=1349108488164000)

1 Row Returned.
Elapsed time: 31 msec(s).
{code}

...Stop and Start Cassandra...


{code}
[default@NEUTRO_Keyspace] connect 192.168.1.33/9160;
Connected to: ""NEUTRO Cluster"" on 192.168.1.33/9160

[default@unknown] use neutro_keyspace;
Authenticated to keyspace: NEUTRO_Keyspace

[default@NEUTRO_Keyspace] list TaskId;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: NE_MASTER_TASK_ID
=> (column=taskId, value=47, timestamp=1349090424494000)

1 Row Returned.
Elapsed time: 110 msec(s).
{code}

Thanks for your attention
"
CASSANDRA-4728,"NPE with some load of writes, but possible snitch setting issue for a cluster","The following errors are showing under height load

ERROR [MutationStage:8294] 2012-09-25 22:01:47,628 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:8294,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.locator.PropertyFileSnitch.getDatacenter(PropertyFileSnitch.java:104)
	at com.datastax.bdp.snitch.DseDelegateSnitch.getDatacenter(DseDelegateSnitch.java:69)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:93)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1984)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1972)
	at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:262)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:248)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:505)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:56)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


ERROR [MutationStage:13164] 2012-09-25 22:19:06,486 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13164,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [Thrift:12] 2012-09-25 22:19:07,433 Cassandra.java (line 3462) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [Thrift:16] 2012-09-25 22:19:07,437 Cassandra.java (line 2999) Internal error processing get


java.lang.NullPointerException
 INFO [GossipStage:280] 2012-09-26 00:15:15,371 Gossiper.java (line 818) InetAddress /172.16.233.208 is now dead.
ERROR [GossipStage:280] 2012-09-26 00:15:15,372 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:280,5,main]
j

ERROR [MutationStage:40529] 2012-09-26 00:15:21,527 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:40529,5,main]
java.lang.NullPointerException
 INFO [GossipStage:281] 2012-09-26 00:15:23,013 Gossiper.java (line 818) InetAddress /172.16.232.159 is now dead.
ERROR [GossipStage:281] 2012-09-26 00:15:23,014 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:281,5,main]
"
CASSANDRA-4708,StorageProxy slow-down and memory leak,"I am consistently observing slow-downs in StorageProxy caused by the NonBlockingHashMap used indirectly by MessagingService via the callbacks ExpiringMap.

This seems do be due to NBHM having unbounded memory usage in the face of workloads with high key churn. As monotonically increasing integers are used as callback id's by MessagingService, the backing NBHM eventually ends up growing the backing store unboundedly. This causes it to also do very large and expensive backing store reallocation and migrations, causing throughput to drop to tens of operations per second, lasting seconds or even minutes. 

This behavior is especially noticable for high throughput workloads where the dataset is completely in ram and I'm doing up to a hundred thousand reads per second.

Replacing NBHM in ExpiringMap with the java standard library ConcurrentHashMap resolved the issue and allowed me to keep a consistent high throughput.

An open issue on NBHM can be seen here: http://sourceforge.net/tracker/?func=detail&aid=3563980&group_id=194172&atid=948362"
CASSANDRA-4699,Add TRACE support to binary protocol,
CASSANDRA-4694,populate_io_cache_on_flush option should be configurable for each column family independently,"I suggest to configure populate_io_cache_on_flush option for each column family. It should be configurable from cassandra-cli and should be stored in System keyspace. 

That could be useful if you have a few column families inside single keyspace and you need to fit in memory only one of them.

Patch has been attached. I've been testing it on pseudo-cluster using ccm. So I don't have fully confidence about lack of bugs. Please carefully review that code."
CASSANDRA-4675,NPE in NTS when using LQ against a node (DC) that doesn't have replica,"in a NetworkTopologyStrategy where there are 2 DC:

{panel}
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
127.0.0.1       dc1         r1          Up     Normal  115.78 KB       50.00%  0                                           
127.0.0.2       dc2         r1          Up     Normal  129.3 KB        50.00%  85070591730234615865843651857942052864  
{panel}
I have a KS that has replica is 1 of the dc (dc1):

{panel}
[default@unknown] describe Keyspace3;                                                                                                                     
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

But if I connect to a node in dc2, using LOCAL_QUORUM, I get NPE in the Cassandra node's log:

{panel}
[default@unknown] consistencylevel as LOCAL_QUORUM;                       
Consistency level is set to 'LOCAL_QUORUM'.
[default@unknown] use Keyspace3;                                          
Authenticated to keyspace: Keyspace3
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                     
Internal error processing get
org.apache.thrift.TApplicationException: Internal error processing get
        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:511)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}

node2's log:
{panel}
ERROR [Thrift:3] 2012-09-17 18:15:16,868 Cassandra.java (line 2999) Internal error processing get
java.lang.NullPointerException
        at org.apache.cassandra.locator.NetworkTopologyStrategy.getReplicationFactor(NetworkTopologyStrategy.java:142)
        at org.apache.cassandra.service.DatacenterReadCallback.determineBlockFor(DatacenterReadCallback.java:90)
        at org.apache.cassandra.service.ReadCallback.<init>(ReadCallback.java:67)
        at org.apache.cassandra.service.DatacenterReadCallback.<init>(DatacenterReadCallback.java:63)
        at org.apache.cassandra.service.StorageProxy.getReadCallback(StorageProxy.java:775)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:609)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.internal_get(CassandraServer.java:383)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:401)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2989)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
{panel}

I could workaround it by adding dc2:0 to the option:

{panel}
[default@Keyspace3] describe Keyspace3;                                           
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc2:0, dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

Now you get UA:

{panel}
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                           
null
UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6506)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:519)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}


On a side note, is there a thought on having a CL.LOCAL_ONE? Ie if local node (wrt the dc) does not have replica, on a LOCAL_ONE, it won't try to go across DC to try to get it. It would be similar to LOCAL_QUORUM."
CASSANDRA-4672,_TRACE verb is not droppable which causes an AssertionError,When a big enough statement is traced (like select *) an assertion error is fired because the _TRACE verb is not droppable.
CASSANDRA-4657,cql version race condition with rpc_server_type: sync,"If clients connect to a cassandra cluster configured with rpc_server_type: sync with heterogeneous cql versions (2 and 3), the cql version used for execution on the server changes seemingly randomly.
It's due to the fact that CustomTThreadPoolServer.java does not set the remoteSocket anytime, or does not clear the cql version in the ThreadLocal clientState object.
When CassandraServer.java calls state() it gets the ThreadLocal object clientState, which has its cqlversion already changed by a previous socket that was using the same thread.


The easiest fix is probably to do a SocketSessionManagementService.instance.set when accepting a new client and SocketSessionManagementService.instance.remove when the client is closed, but if you really want to use the ThreadLocal clientState and not alloc/destroy a ClientState everytime, then you should clear this clientState on accept of a new client.

The problem can be reproduced with cqlsh -3 on one side and a client that does not set the cql version, expecting to get version 2 by default, but actually gettingv v2/v3 depending on which thread it connects to.

The problem does not happen with other rpc_server_types, nor with clients that set their cql version at connection."
CASSANDRA-4630,SSTable data file filedescriptor leak in mmap_index_only disk access mode,"After upgrading one node from 0.8.10 to 1.1.4 I've got java.net.SocketException: Too many open files in system.

lsof showed that one of CF is opened multiple times:
      7 /var/lib/cassandra/data/mail/mta_logs/mail-mta_logs-he-26595-Data.db
     18 /var/lib/cassandra/data/mail/mta_logs/mail-mta_logs-g-26576-Data.db
    185 /var/lib/cassandra/data/mail/mta_logs/mail-mta_logs-g-26536-Data.db
    328 /var/lib/cassandra/data/mail/mta_logs/mail-mta_logs-g-26580-Data.db
  18194 /var/lib/cassandra/data/mail/mta_logs/mail-mta_logs-g-26590-Data.db
  21339 /var/lib/cassandra/data/mail/mta_logs/mail-mta_logs-g-26512-Data.db
  24416 /var/lib/cassandra/data/mail/mta_logs/mail-mta_logs-g-26557-Data.db

nodetool cfstats says that this CF was read about 80K times
"
CASSANDRA-4619,Make non-heap memory usable for key cache,There might be use cases where row cache is not useful because of the data use case patterns ( too big a data wherein each is row is less frequently looked up). Hence a mechanism is needed to reuse the available non-heap memory on the node to use for key cache. 
CASSANDRA-4618,too small flushed sstable(less than 20M) because the wrong liveRatio,"After my cluster run some time, I find that there are too many small sstable files flushed from memtables, they are about 20M.I trace the system.log and find the record as below:
 WARN [MemoryMeter:1] 2012-09-04 16:04:50,392 Memtable.java (line 181) setting live ratio to maximum of 64 instead of Infinity

and after that, the live ratio is never changed, so after that, the memtables are flushed when they are not big enough.

and I also find this information


INFO [main] 2012-09-04 16:04:51,205 ColumnFamilyStore.java (line 705) Enqueuing flush of Memtable-UrlCrawlStatsCF@241601165(0/0 serialized/live bytes, 15212 ops) 

the serialized and live bytes are both zero, that's so swired。
"
CASSANDRA-4587,StackOverflowError in LeveledCompactionStrategy$LeveledScanner.computeNext,"while running nodetool repair, the following was logged in system.log:


ERROR [ValidationExecutor:2] 2012-08-30 10:58:19,490 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.StackOverflowError
        at sun.nio.cs.UTF_8.updatePositions(UTF_8.java:76)
        at sun.nio.cs.UTF_8$Encoder.encodeArrayLoop(UTF_8.java:411)
        at sun.nio.cs.UTF_8$Encoder.encodeLoop(UTF_8.java:466)
        at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:561)
        at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:258)
        at java.lang.StringCoding.encode(StringCoding.java:290)
        at java.lang.String.getBytes(String.java:954)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:64)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:46)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1007)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
        at org.apache.cassandra.io.sstable.SSTableBoundedScanner.<init>(SSTableBoundedScanner.java:41)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:869)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:247)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
.

(about 900 lines deleted)
.


        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:202)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:90)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:47)
        at org.apache.cassandra.db.compaction.CompactionIterable.iterator(CompactionIterable.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:703)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:442)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
"
CASSANDRA-4573,HSHA doesn't handle large messages gracefully,"HSHA doesn't seem to enforce any kind of max message length, and when messages are too large, it doesn't fail gracefully.

With debug logs enabled, you'll see this:

{{DEBUG 13:13:31,805 Unexpected state 16}}

Which seems to mean that there's a SelectionKey that's valid, but isn't ready for reading, writing, or accepting.

Client-side, you'll get this thrift error (while trying to read a frame as part of {{recv_batch_mutate}}):

{{TTransportException: TSocket read 0 bytes}}"
CASSANDRA-4565,TTL columns with older then gcgrace do not need to flush,"With memcache many people are willing to sacrifice durability for performance. Cassandra has a TimeToLive feature that can be used in caching scenarios with low values for gc_grace_seconds. However from a code dive it seems that cassandra will always write TTL to disk, even those that are beyond gc_grace_seconds. If a use case very large memtables,small ttl, and small gc_grace it is possible that flushing these columns to disk can be skipped entirely in some scenarios. "
CASSANDRA-4553,NPE while loading Saved KeyCache,"WARN [main] 2012-08-16 15:31:13,896 AutoSavingCache.java (line 146) error reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
java.lang.NullPointerException
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:140)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:251)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:354)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:326)
	at org.apache.cassandra.db.Table.initCf(Table.java:312)
	at org.apache.cassandra.db.Table.<init>(Table.java:252)
	at org.apache.cassandra.db.Table.open(Table.java:97)
	at org.apache.cassandra.db.Table.open(Table.java:75)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:168)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
"
CASSANDRA-4532,NPE when trying to select a slice from a composite table,"I posted this question on StackOverflow, because i need a solution. 

Created a table with :

{noformat}
create table compositetest(m_id ascii,i_id int,l_id ascii,body ascii, PRIMARY KEY(m_id,i_id,l_id));
{noformat}

wanted to slice the results returned, so did something like below, not sure if its the right way. The first one returns data perfectly as expected, second one to get the next 3 columns closes the transport of my cqlsh

{noformat}
cqlsh:testkeyspace1> select * from compositetest where i_id<=3 limit 3;
 m_id | i_id | l_id | body
------+------+------+------
   m1 |    1 |   l1 |   b1
   m1 |    2 |   l2 |   b2
   m2 |    1 |   l1 |   b1

cqlsh:testkeyspace1> Was trying to write something for slice range.

TSocket read 0 bytes
{noformat}

Is there a way to achieve what I am doing here, it would be good if some meaning ful error is sent back, instead of cqlsh closing the transport.

On the server side I see the following error.

{noformat}
ERROR [Thrift:3] 2012-08-12 15:15:24,414 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.cql3.statements.SelectStatement$Restriction.setBound(SelectStatement.java:1277)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1151)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1001)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:215)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}

With ThriftClient I get :

{noformat}
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_execute_cql_query(Cassandra.java:1402)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_cql_query(Cassandra.java:1388)
{noformat}"
CASSANDRA-4496,NPE on creating secondary index,"The following code has been working fine up to and including 1.0.x

public static String createIndexedColumnFamily(String cf){
    
    Cluster cluster = HectorConfig.cluster;
    ComparatorType ctName = ComparatorType.getByClassName(JNameComparator.class.getName());
    
    try{
      cluster.dropColumnFamily(HectorConfig.dfltKeyspaceName, cf );
    } catch (Exception e){}
  
    List<ColumnDefinition> cdL = new ArrayList<ColumnDefinition>();
    BasicColumnDefinition cd;
    
    cd = new BasicColumnDefinition();
    cd.setName(ss.toByteBuffer(""id""));
    cd.setIndexName(""id"");
    cd.setIndexType(ColumnIndexType.KEYS);
    cd.setValidationClass(JValueComparator.class.getName());
    cdL.add(cd);

    ThriftCfDef cfd= new ThriftCfDef(HectorConfig.dfltKeyspaceName, cf, ctName, cdL); 
    cfd.setKeyValidationClass(ComparatorType.UTF8TYPE.getClassName());
    cfd.setDefaultValidationClass(JValueComparator.class.getName());
    cluster.addColumnFamily(cfd); 
    
    return ""created: "" + cf;
  }
}

I'm inclined to exclude the presence of the custom comparator since:
(1) there is no issue using it if the cf doesn't have a secondary index
(2) the stack trace (see below) doesn't include the comparator 


The above code throws the following error in Cassandra 1.1.2 and 1.1.3


david@vlap1:~/opt/cassandra$ sudo bin/cassandra -f
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms128M -Xmx128M -Xmn32M -XX:+HeapDumpOnOutOfMemoryError -Xss160k

 INFO 23:15:31,333 Logging initialized
 INFO 23:15:31,337 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0
 INFO 23:15:31,338 Heap size: 130875392/130875392
 INFO 23:15:31,338 Classpath: /etc/cassandra:/usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/hector-core-1.0-3.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.9.2.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jellyfish.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.7.0.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/metrics-core-2.0.3.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.4.1.jar:/usr/share/cassandra/lib/snaptree-0.1.jar:/usr/share/cassandra/apache-cassandra-1.1.2.jar:/usr/share/cassandra/apache-cassandra-thrift-1.1.2.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/cassandra/stress.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 23:15:31,340 JNA not found. Native methods will be disabled.
 INFO 23:15:31,351 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 23:15:31,566 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 23:15:31,875 Global memtable threshold is enabled at 41MB
 INFO 23:15:32,380 Initializing key cache with capacity of 6 MBs.
 INFO 23:15:32,394 Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO 23:15:32,395 Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO 23:15:32,400 Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO 23:15:32,661 Couldn't detect any schema definitions in local storage.
 INFO 23:15:32,665 Found table data in data directories. Consider using the CLI to define your schema.
 INFO 23:15:32,714 No commitlog files found; skipping replay
 INFO 23:15:32,740 Cassandra version: 1.1.2
 INFO 23:15:32,754 Thrift API version: 19.32.0
 INFO 23:15:32,763 CQL supported versions: 2.0.0,3.0.0-beta1 (default: 2.0.0)
 INFO 23:15:32,803 Loading persisted ring state
 INFO 23:15:32,807 Starting up server gossip
 INFO 23:15:32,821 Enqueuing flush of Memtable-LocationInfo@1473831778(163/203 serialized/live bytes, 3 ops)
 INFO 23:15:32,822 Writing Memtable-LocationInfo@1473831778(163/203 serialized/live bytes, 3 ops)
 INFO 23:15:33,072 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-hd-1-Data.db (235 bytes) for commitlog position ReplayPosition(segmentId=126105713813993, position=587)
 INFO 23:15:33,094 Starting Messaging Service on port 7000
 INFO 23:15:33,102 This node will not auto bootstrap because it is configured to be a seed node.
 WARN 23:15:33,112 Generated random token 73932790073580646673058527716402832414. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
 INFO 23:15:33,160 Enqueuing flush of Memtable-LocationInfo@248801290(77/96 serialized/live bytes, 2 ops)
 INFO 23:15:33,161 Writing Memtable-LocationInfo@248801290(77/96 serialized/live bytes, 2 ops)
 INFO 23:15:33,364 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-hd-2-Data.db (163 bytes) for commitlog position ReplayPosition(segmentId=126105713813993, position=768)
 INFO 23:15:33,366 Node vlap1/127.0.1.1 state jump to normal
 INFO 23:15:33,367 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 23:15:33,368 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 23:15:33,408 Binding thrift service to vlap1/127.0.1.1:9160
 INFO 23:15:33,429 Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO 23:15:33,432 Using synchronous/threadpool thrift server on vlap1/127.0.1.1 : 9160
 INFO 23:15:33,433 Listening for thrift clients...






 INFO 23:16:16,665 Enqueuing flush of Memtable-schema_keyspaces@1373026871(199/248 serialized/live bytes, 4 ops)
 INFO 23:16:16,665 Writing Memtable-schema_keyspaces@1373026871(199/248 serialized/live bytes, 4 ops)
 INFO 23:16:16,944 Completed flushing /var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-hd-1-Data.db (246 bytes) for commitlog position ReplayPosition(segmentId=126105713813993, position=1023)
 INFO 23:16:33,592 Enqueuing flush of Memtable-schema_columnfamilies@341129413(1202/1502 serialized/live bytes, 20 ops)
 INFO 23:16:33,592 Writing Memtable-schema_columnfamilies@341129413(1202/1502 serialized/live bytes, 20 ops)
 INFO 23:16:33,900 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-1-Data.db (1257 bytes) for commitlog position ReplayPosition(segmentId=126105713813993, position=2577)
 INFO 23:16:33,900 Enqueuing flush of Memtable-schema_columns@43094749(283/353 serialized/live bytes, 5 ops)
 INFO 23:16:33,901 Writing Memtable-schema_columns@43094749(283/353 serialized/live bytes, 5 ops)
 INFO 23:16:34,101 Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-1-Data.db (330 bytes) for commitlog position ReplayPosition(segmentId=126105713813993, position=2577)
ERROR 23:16:34,132 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:139)
	at org.apache.cassandra.thrift.CassandraServer.system_add_column_family(CassandraServer.java:926)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3410)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3398)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
	... 11 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:359)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
ERROR 23:16:34,140 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:359)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)






Upon a restart of the server another NPE is thrown:




david@vlap1:~/opt/cassandra$ sudo bin/cassandra -f
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms128M -Xmx128M -Xmn32M -XX:+HeapDumpOnOutOfMemoryError -Xss160k
 INFO 23:17:51,158 Logging initialized
 INFO 23:17:51,163 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0
 INFO 23:17:51,163 Heap size: 130875392/130875392
 INFO 23:17:51,163 Classpath: /etc/cassandra:/usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/hector-core-1.0-3.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.9.2.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jellyfish.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.7.0.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/metrics-core-2.0.3.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.4.1.jar:/usr/share/cassandra/lib/snaptree-0.1.jar:/usr/share/cassandra/apache-cassandra-1.1.2.jar:/usr/share/cassandra/apache-cassandra-thrift-1.1.2.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/cassandra/stress.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 23:17:51,165 JNA not found. Native methods will be disabled.
 INFO 23:17:51,187 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 23:17:51,416 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 23:17:51,713 Global memtable threshold is enabled at 41MB
 INFO 23:17:52,131 Initializing key cache with capacity of 6 MBs.
 INFO 23:17:52,142 Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO 23:17:52,143 Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO 23:17:52,148 Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO 23:17:52,239 Opening /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-1 (1257 bytes)
 INFO 23:17:52,272 Opening /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-1 (330 bytes)
 INFO 23:17:52,287 Opening /var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-hd-1 (246 bytes)
 INFO 23:17:52,291 Opening /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-hd-2 (163 bytes)
 INFO 23:17:52,297 Opening /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-hd-1 (235 bytes)
ERROR 23:17:52,500 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: null
david@vlap1:~/opt/cassandra$ 

"
CASSANDRA-4489,LCS with Composite Columns NPE,"Creating the CF in cqlsh -3
cqlsh> CREATE KEYSPACE tt WITH strategy_class=SimpleStrategy AND strategy_options:replication_factor=1;
cqlsh> USE tt;
cqlsh:tt> CREATE TABLE breakable (
      ...   dt timestamp,
      ...   id timeuuid,
      ...   metadata text,
      ...   PRIMARY KEY (dt, id)
      ... );
cqlsh:tt> 

Then changing to LCS using the CLI
[default@unknown] use tt;
Authenticated to keyspace: tt
[default@tt] update column family breakable with compaction_strategy=LeveledCompactionStrategy;
org.apache.thrift.transport.TTransportException

And then trying to view the table schema
cqlsh:tt> describe table breakable;

'NoneType' object has no attribute 'startswith'
cqlsh:tt> 

Restarting cassandra causes an NPE

ERROR 17:10:53,487 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: null
"
CASSANDRA-4482,In-memory merkle trees for repair,"this sounds cool, we should reimplement it in the open source cassandra;

http://www.acunu.com/2/post/2012/07/incremental-repair.html

"
CASSANDRA-4474,Respect five-minute flush moratorium after initial CL replay,"As noted in CASSANDRA-1967, the post-replay flush can kick off compactions before the five minute grace period introduced in CASSANDRA-3181 to avoid i/o contention while server is warming up."
CASSANDRA-4446,nodetool drain sometimes doesn't mark commitlog fully flushed,"I recently wiped a customer's QA cluster. I drained each node and verified that they were drained. When I restarted the nodes, I saw the commitlog replay create a memtable and then flush it. I have attached a sanitized log snippet from a representative node at the time. 

It appears to show the following :
1) Drain begins
2) Drain triggers flush
3) Flush triggers compaction
4) StorageService logs DRAINED message
5) compaction thread excepts
6) on restart, same CF creates a memtable
7) and then flushes it [1]

The columnfamily involved in the replay in 7) is the CF for which the compaction thread excepted in 5). This seems to suggest a timing issue whereby the exception in 5) prevents the flush in 3) from marking all the segments flushed, causing them to replay after restart.

In case it might be relevant, I did an online change of compaction strategy from Leveled to SizeTiered during the uptime period preceding this drain.

[1] Isn't commitlog replay not supposed to automatically trigger a flush in modern cassandra?"
CASSANDRA-4415,Add cursor API/auto paging to the native CQL protocol,"The goal here would be to use a query paging mechanism to the CQL native protocol. Typically the client/server with that would look something like this:
{noformat}
C sends query to S.
S sends N first rows matching the query + flag saying the response is not complete
C requests the next N rows
S sends N next rows + flag saying whether there is more
C requests the next N rows
...
S sends last rows + flag saying there is no more result
{noformat}

The clear goal is for user to not have to worry about limiting queries and doing manual paging."
CASSANDRA-4367,NPE in murmur-hash,"We were running nodetool repair -pr <ZMail> <MsgConvMapper> and received the following error during a compaction

 INFO [CompactionExecutor:595] 2012-06-13 08:46:59,927 CompactionTask.java (line 113) Compacting [SSTableReader(path='/home/sas/system/data/ZMail/MsgConvMapper-hc-183-Data.db'), SSTableReader(path='/home/sas/system/data/ZMail/MsgConvMapper-hc-185-Data.db'), SSTableReader(path='/home/sas/system/data/ZMail/MsgConvMapper-hc-171-Data.db'), SSTableReader(path='/home/sas/system/data/ZMail/MsgConvMapper-hc-182-Data.db')]

 INFO [CompactionExecutor:595] 2012-06-13 09:39:58,570 CompactionController.java (line 133) Compacting large row ZMail/MsgConvMapper:691000001157001 (81190450 bytes) incrementally

ERROR [CompactionExecutor:595] 2012-06-13 09:44:46,718 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:595,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.utils.MurmurHash.hash64(MurmurHash.java:102)
        at org.apache.cassandra.utils.BloomFilter.getHashBuckets(BloomFilter.java:103)
        at org.apache.cassandra.utils.BloomFilter.getHashBuckets(BloomFilter.java:92)
        at org.apache.cassandra.utils.BloomFilter.add(BloomFilter.java:114)
        at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:96)
        at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:51)
        at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:134)
        at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

The CF Def is:

Keyspace: ZMail:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [DC2:2, DC1:3]
  Column Families:
    ColumnFamily: MsgConvMapper (Super)
      Key Validation Class: org.apache.cassandra.db.marshal.LongType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.SerializingCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 1209600
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy

I do not see those any of those SSTables in disk, that participated in the above failed compaction."
CASSANDRA-4355,Better debian packaging permissions,"The debian package creates a cassandra user for the process to run as. It chowns /var/lib/cassandra and /var/log/cassandra, but it doesn't grant group level access to these files. It should do a 'chown cassandra:cassandra ...' so that users in the cassandra group can also access those files. Also we should chown /etc/cassandra and any other files/directories created."
CASSANDRA-4321,stackoverflow building interval tree & possible sstable corruptions,"After upgrading to 1.1.1 (from 1.1.0) I have started experiencing StackOverflowError's resulting in compaction backlog and failure to restart. 

The ring currently consists of 6 DC's and 22 nodes using LCS & compression.  This issue was first noted on 2 nodes in one DC and then appears to have spread to various other nodes in the other DC's.  

When the first occurrence of this was found I restarted the instance but it failed to start so I cleared its data and treated it as a replacement node for the token it was previously responsible for.  This node successfully streamed all the relevant data back but failed again a number of hours later with the same StackOverflowError and again was unable to restart. 

The initial stack overflow error on a running instance looks like this:

ERROR [CompactionExecutor:314] 2012-06-07 09:59:43,017 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:314,1,main]
java.lang.StackOverflowError
        at java.util.Arrays.mergeSort(Arrays.java:1157)
        at java.util.Arrays.sort(Arrays.java:1092)
        at java.util.Collections.sort(Collections.java:134)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow.  Compactions stop from this point onwards]


I restarted this failing instance with DEBUG logging enabled and it throws the following exception part way through startup:

ERROR 11:37:51,046 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.db.Table$2.apply(Table.java:574)
        at org.apache.cassandra.db.Table$2.apply(Table.java:571)
        at com.google.common.collect.Iterators$8.next(Iterators.java:751)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG 11:37:51,052 Initializing ksU.cfS


And then finally fails with the following:

DEBUG 11:49:03,752 Creating IntervalNode from [Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b))]
java.lang.reflect.InvocationTargetException
DEBUG 11:49:03,753 Configured datacenter replicas are dc1:2, dc2:2, dc3:2, dc4:2, dc5:0, dc6:2, dc7:0, dc8:0, dc9:2
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3

Running with assertions enabled allows me to start the instance but when doing so I get errors such as:

ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)

and:

ERROR 01:27:58,946 Exception in thread Thread[CompactionExecutor:9,1,main]
java.lang.AssertionError: Last written key DecoratedKey(81958437188197992567937826278457419048, 4fa1aebad23f81e4321d344d) >= current key DecoratedKey(64546479828744423263742604083767363606, 4fcafc0f19f6a8092d4d4f94) writing into /var/lib/XX/data/cassandra/ks1/cf1/ks1-cf1-tmp-hd-657317-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Just like the initial errors compactions appear to stop occurring after this point.  

Given the above this looks like sstables are getting corrupted.  By restarting nodes I am able to identify several hundred sstables exhibiting the same problem and this appears to be growing.

I have tried scrubbing those affected nodes but the problem continues to occur.  If this is due to sstable corruptions is there another way of validating sstables for correctness?  Given that it has spread to various servers in other DC's it looks like this is directly related to the 1.1.1 upgrade recently performed on the ring."
CASSANDRA-4311,clean up messagingservice protocol limitations,"Weaknesses of the existing protocol:

- information asymmetry: node A can know what version node B expects, but not vice versa (see CASSANDRA-4101)
- delayed information: node A will often not know what version node B expects, until after first contacting node B -- forcing it to throw that first message away and retry for the next one
- protocol cannot handle both cross-dc forwarding and broadcast_address != socket address (see bottom of CASSANDRA-4099)
- version is partly global, partly per-connection, and partly per-message, resulting in some interesting hacks (CASSANDRA-3166) and difficulty layering more sophisticated OutputStreams on the socket (CASSANDRA-3127, CASSANDRA-4139)"
CASSANDRA-4300,missing host ID results in NPE when delivering hints,"
In {{StorageService.handledStateNormal()}} the token-to-endpoint map is updated before the id-to-endpoint map, creating a small window where {{TokenMetadata.isMember()}} can return true before a host ID is available.

Trivial patch forthcoming.

{noformat}
[...]
 INFO [GossipStage:1] 2012-05-30 21:59:10,683 Gossiper.java (line 833) Node /10.2.131.32 has restarted, now UP
 INFO [GossipStage:1] 2012-05-30 21:59:10,684 Gossiper.java (line 799) InetAddress /10.2.131.32 is now UP
 INFO [HintedHandoff:1] 2012-05-30 21:59:10,697 HintedHandOffManager.java (line 304) Started hinted handoff for host: null with IP: /10.2.131.32
ERROR [HintedHandoff:1] 2012-05-30 21:59:10,698 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:112)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:305)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:265)
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:86)
	at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:439)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 Events.java (line 130) Node 10.2.131.32 now available: true
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 StorageService.java (line 1218) Node /10.2.131.32 state jump to normal
 INFO [GossipStage:1] 2012-05-30 21:59:10,701 Events.java (line 120) Node 10.2.131.32 is now in state NORMAL
[...]
{noformat}"
CASSANDRA-4258,Are we sorting the bloom filters in memory to increase the probability of getting proper result instead of just avoiding the false positive?,"I was just wondering if there is any logic for ""which bloom filter should be checked first"" to increase the probability of getting the result and not just minimizing the probability of false positive.

( *Note:* I have checked into the code and I am not talking about *""Getting BloomFilter with the lowest practical false positive probability""* OR *""Getting smallest BloomFilter that can provide the given false positive probability rate for the given number of elements.""* )

*Consider following Scenario:*

1) In our Cassandra Cluster we are inserting 130 millions of rows on daily basis for single column family and practically we cant keep this data compacted always.(As the loading time is much and compaction may take too much time that could affect the schedule for loading of data for next day )
2) We are inserting same rowkeys(values of all the 130 millions rows are same) everyday with different supercolumn.
{code}
For date 20120101 we have

super_CF= {row_1:{_super_column_20120101:{ col1 : val1, col2 : val2 }}
           row_2:{_super_column_20120101:{ col1 : val3, col2 : val4 }}
           row_3:{_super_column_20120101:{ col1 : val5, col2 : val6 }}
} 
and For date 20120102 it will be like

super_CF= {row_1:{_super_column_20120102:{ col1 : val7, col2 : val8 }}
           row_2:{_super_column_20120102:{ col1 : val9, col2 : val10 }}
           row_3:{_super_column_20120102:{ col1 : val11, col2 : val12 }}
} 

Note that set of rowkeys is same for all the days only supercolumn changes
{code}
3) So if we do not compact the data say for 30 days, each row key is present in 30 different sstables.
4) So in worst case, even with 0 probability of false positive, there could be 30 unnecessary disk accesses.
5) Because of this scenario we are experiencing extremely degraded read performance. 

*Proposed solution:*
1) We can have some sorting of bloom-filters based on logic like the bloom filter of the sstable which resulted into successfully serving the read request will have higher priority over other bloom filters.
I mean we will go for the bloom filter of the sstable which is most recently accessed and which successfully returned the requested columns.(MRU approach, As the probability of getting result from MRU sstable is greater).This way we can reduce the disk access.

2) The point is we should have some sort of logic for sorting of bloom filters to boost the read performance in case where sstables are not yet compacted."
CASSANDRA-4256,Include stress tool in debian packaging,"The stress tool isn't included in the debian packaging. We need to update that to grab the stress shell script as well as put the stress.jar file in lib.

Also the stress shell script needs to be updated to include looking in /usr/share/cassandra... when searching for the stress jar so it will run in packaged installations."
CASSANDRA-4227,StorageProxy throws NPEs for when there's no hostids for a target,"On trunk...

if there is no host id due to an old node, an info log is generated, but the code continues to use the null host id causing NPEs in decompose... Should this bypass this code, or perhaps can the plain ip address be used in this case? don't know.

as follows...



                    UUID hostId = StorageService.instance.getTokenMetadata().getHostId(target);
                    if ((hostId == null) && (Gossiper.instance.getVersion(target) < MessagingService.VERSION_12))
                        logger.info(""Unable to store hint for host with missing ID, {} (old node?)"", target.toString());
                    RowMutation hintedMutation = RowMutation.hintFor(mutation, ByteBuffer.wrap(UUIDGen.decompose(hostId)));
                    hintedMutation.apply();
"
CASSANDRA-4218,NPE thrown after switching to LeveledCompactionStrategy,"After running out of disk space on our six node Cassandra cluster we switched to a Levelled Compaction Strategy. The servers began to degrade performance accruing a large number of pending tasks - we noticed that there was a NullPointerException appearing in the system log:

ERROR [CompactionExecutor:22] 2012-05-03 20:34:38,931 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:22,1,main]
java.lang.NullPointerException
        at java.io.File.<init>(File.java:222)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:642)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:653)
        at org.apache.cassandra.db.ColumnFamilyStore.createCompactionWriter(ColumnFamilyStore.java:1892)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:153)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:57)
        at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:135)
        at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:115)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

"
CASSANDRA-4215,"clarify distinction between in-memory size() method and on-disk serializedSize() for CF, Column classes",
CASSANDRA-4175,"Reduce memory, disk space, and cpu usage with a column name/id map","We spend a lot of memory on column names, both transiently (during reads) and more permanently (in the row cache).  Compression mitigates this on disk but not on the heap.

The overhead is significant for typical small column values, e.g., ints.

Even though we intern once we get to the memtable, this affects writes too via very high allocation rates in the young generation, hence more GC activity.

Now that CQL3 provides us some guarantees that column names must be defined before they are inserted, we could create a map of (say) 32-bit int column id, to names, and use that internally right up until we return a resultset to the client."
CASSANDRA-4163,CQL3 ALTER TABLE command causes NPE,"To reproduce the problem:

./cqlsh --cql3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.30.0]
Use HELP for help.

cqlsh> CREATE KEYSPACE test34 WITH strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor='1';

cqlsh> USE test34;

cqlsh:test34> CREATE TABLE users (
          ... password varchar,
          ... gender varchar,
          ... session_token varchar,
          ... state varchar,
          ... birth_year bigint,
          ... pk varchar,
          ... PRIMARY KEY (pk)
          ... );

cqlsh:test34> ALTER TABLE users ADD coupon_code varchar;
TSocket read 0 bytes
"
CASSANDRA-4139,Add varint encoding to Messaging service,
CASSANDRA-4104,Cassandra appears to hang when JNA enabled and heapsize > free memory,"When JNA is enabled heapsize is larger than free memory, all that is printed out is the classpath, then the printouts stop.

If you hit enter again, you get the commandline, but no Cassandra process is running.

Tested on both OpenJDK and Oracle Java.

{noformat}
datastax@datastax-image:~/repos/cassandra$ free -m
             total       used       free     shared    buffers     cached
Mem:          2008        740       1267          0          3         54
-/+ buffers/cache:        682       1326
Swap:            0          0          0
datastax@datastax-image:~/repos/cassandra$ sudo bin/cassandra
datastax@datastax-image:~/repos/cassandra$  INFO 14:31:32,520 Logging initialized
 INFO 14:31:32,533 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_31
 INFO 14:31:32,534 Heap size: 1247805440/1247805440
 INFO 14:31:32,534 Classpath: bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/avro-1.4.0-fixes.jar:bin/../lib/avro-1.4.0-sources-fixes.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:bin/../lib/guava-r08.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/jna.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.6.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.6.1.jar:bin/../lib/slf4j-log4j12-1.6.1.jar:bin/../lib/snakeyaml-1.6.jar:bin/../lib/snappy-java-1.0.4.1.jar:bin/../lib/jamm-0.2.5.jar

datastax@datastax-image:~/repos/cassandra$ ps auwx | grep cass
datastax 18374  1.0  0.0  13448   904 pts/2    S+   14:32   0:00 grep --color=auto cass

{noformat}"
CASSANDRA-4101,Gossip should propagate MessagingService.version,"In CASSANDRA-4099 it's becoming apparent that it's time to fix our hacky versioning tricks we've used to remain backward-compatible.  As a first step, let's communicate the version via gossip so we can eventually reason based on that."
CASSANDRA-4078,StackOverflowError when upgrading to 1.0.8 from 0.8.10,"Hello

I am trying to upgrade our 1-node setup from 0.8.10 to 1.0.8 and seeing the following exception when starting up 1.0.8.  We have been running 0.8.10 without any issues.
 
Attached is the entire log file during startup of 1.0.8.  There are 2 exceptions:

1. StackOverflowError (line 2599)
2. InstanceAlreadyExistsException (line 3632)

I tried ""run scrub"" under 0.8.10 first, it did not help.  Also, I tried dropping the column family which caused the exception, it just got the same exceptions from another column family.

Thanks
"
CASSANDRA-4073,cqlsh: flush CAPTURE output after each command,"In cqlsh A user might want to enable capturing, run a command, and then go look at the file to see the output. This workflow could be useful, for instance, where the output is large. Internal buffering forces the user to turn capture off each time he or she wants to see the output."
CASSANDRA-4065,Bogus MemoryMeter liveRatio calculations,"I get strange cfs.liveRatios.

A couple of mem meter runs seem to calculate bogus results: 

{noformat}
Tue 09:14:48 dd@blnrzh045:~$ grep 'setting live ratio to maximum of 64 instead of' /var/log/cassandra/system.log
 WARN [MemoryMeter:1] 2012-03-20 08:08:07,253 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:09,160 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:13,274 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:22,032 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:12:41,057 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 67.11787351054079
 WARN [MemoryMeter:1] 2012-03-20 08:13:50,877 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 112.58547951925435
 WARN [MemoryMeter:1] 2012-03-20 08:15:29,021 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 193.36945063589877
 WARN [MemoryMeter:1] 2012-03-20 08:17:50,716 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 348.45008340969434
{noformat}

Because meter runs never decrease liveRatio in Memtable (Which seems strange to me. If past calcs should be included for any reason wouldn't averaging make more sense?):

{noformat}
cfs.liveRatio = Math.max(cfs.liveRatio, newRatio);
{noformat}

Memtables are flushed every couple of secs:

{noformat}
ColumnFamilyStore.java (line 712) Enqueuing flush of Memtable-BlobStore@935814661(1874540/149963200 serialized/live bytes, 202 ops)
{noformat}

Even though a saner liveRatio has been calculated after the bogus runs:

{noformat}
INFO [MemoryMeter:1] 2012-03-20 08:19:55,934 Memtable.java (line 198) CFS(Keyspace='SmeetBlob', ColumnFamily='BlobStore') 
   liveRatio is 64.0 (just-counted was 2.97165811895841).  calculation took 124ms for 58 columns
{noformat}"
CASSANDRA-4056,[patch] guard against npe due to null sstable,SSTableIdentityIterator ctor can be called from sibling ctor with a null sstable. So catch block's markSuspect should be npe guarded.
CASSANDRA-4030,cql: support gc_grace seconds when creating a columnfamily,Add support for specifying gc_grace_seconds to CQL when creating column families.
CASSANDRA-3997,Make SerializingCache Memory Pluggable,"Serializing cache uses native malloc and free by making FM pluggable, users will have a choice of gcc malloc, TCMalloc or JEMalloc as needed. 
Initial tests shows less fragmentation in JEMalloc but the only issue with it is that (both TCMalloc and JEMalloc) are kind of single threaded (at-least they crash in my test otherwise)."
CASSANDRA-3946,BulkRecordWriter shouldn't stream any empty data/index files that might be created at end of flush,"If by chance, we flush sstables during BulkRecordWriter (we have seen it happen), I want to make sure we don't try to stream them."
CASSANDRA-3932,schema IAE and read path NPE after cluster re-deploy,"On the same cluster (but later) as the one where we observed CASSANDRA-3931 we were running some performance/latency testing. ycsb reads, plus a separate little python client. All was fine.

I then did a fast re-deploy for changed GC settings, which would have let to a complete cluster restart almost simultaneously (triggering races?). When I re-ran my Python client, I suddenly got an error saying Keyspace1 did not exist. On re-run I started getting timeouts. Looking at the endpoints of the key that I was getting a timeout for, the first error ever seen is:

{code}
java.lang.IllegalArgumentException: Unknown ColumnFamily Standard1 in keyspace Keyspace1
        at org.apache.cassandra.config.Schema.getComparator(Schema.java:234)
        at org.apache.cassandra.db.ColumnFamily.getComparatorFor(ColumnFamily.java:312)
        at org.apache.cassandra.db.ReadCommand.getComparator(ReadCommand.java:94)
        at org.apache.cassandra.db.SliceByNamesReadCommand.<init>(SliceByNamesReadCommand.java:44)
        at org.apache.cassandra.db.SliceByNamesReadCommandSerializer.deserialize(SliceByNamesReadCommand.java:113)
        at org.apache.cassandra.db.SliceByNamesReadCommandSerializer.deserialize(SliceByNamesReadCommand.java:81)
        at org.apache.cassandra.db.ReadCommandSerializer.deserialize(ReadCommand.java:134)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:53)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

And later in the read path NPE:s like these:

{code}
java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:321)
        at org.apache.cassandra.db.Table.<init>(Table.java:277)
        at org.apache.cassandra.db.Table.open(Table.java:120)
        at org.apache.cassandra.db.Table.open(Table.java:103)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
"
CASSANDRA-3906,BulkRecordWriter throws NPE for counter columns,"Using BulkRecordWriter, fails with counters due to an NPE (we used column instead of counter_column). I also noticed this broke for super columns too."
CASSANDRA-3904,do not generate NPE on aborted stream-out sessions,"https://issues.apache.org/jira/browse/CASSANDRA-3569?focusedCommentId=13207189&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13207189

Attaching patch to make this a friendlier log entry."
CASSANDRA-3903,Intermittent unexpected errors: possibly race condition around CQL parser?,"When running multiple simultaneous instances of the test_cql.py piece of the python-cql test suite, I can reliably reproduce intermittent and unpredictable errors in the tests.

The failures often occur at the point of keyspace creation during test setup, with a CQL statement of the form:

{code}
        CREATE KEYSPACE 'asnvzpot' WITH strategy_class = SimpleStrategy
            AND strategy_options:replication_factor = 1
    
{code}

An InvalidRequestException is returned to the cql driver, which re-raises it as a cql.ProgrammingError. The message:

{code}
ProgrammingError: Bad Request: line 2:24 no viable alternative at input 'asnvzpot'
{code}

In a few cases, Cassandra threw an ArrayIndexOutOfBoundsException and this traceback, closing the thrift connection:

{code}
ERROR [Thrift:244] 2012-02-10 15:51:46,815 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1520)
        at org.apache.cassandra.thrift.ThriftValidation.validateCfDef(ThriftValidation.java:634)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:744)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:898)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1245)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3458)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3446)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

Sometimes I see an ArrayOutOfBoundsError with no traceback:

{code}
ERROR [Thrift:858] 2012-02-13 12:04:01,537 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException
{code}

Sometimes I get this:

{code}
ERROR [MigrationStage:1] 2012-02-13 12:04:46,077 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.IllegalArgumentException: value already present: 1558
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
        at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:111)
        at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:96)
        at com.google.common.collect.HashBiMap.put(HashBiMap.java:84)
        at org.apache.cassandra.config.Schema.load(Schema.java:392)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:284)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:209)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyImpl(AddColumnFamily.java:49)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:66)
        at org.apache.cassandra.cql.QueryProcessor$1.call(QueryProcessor.java:334)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

Again, around 99% of the instances of this {{CREATE KEYSPACE}} statement work fine, so it's a little hard to git bisect out, but I guess I'll see what I can do."
CASSANDRA-3898,do not leak internal mutable TokenMetadata state,"It would be great if TokenMetadata never ever leaked internal mutable state, avoiding once and for all further concurrency issues here. Persistent collections (CASSANDRA-3856) should be a good candidate here to accomplish this without additional complexity, as long as read performance is reasonably close to their non-persistent counterparts.
"
CASSANDRA-3887,NPE  on start-up due to missing stage,"On 1.1 (with our patches, but fairly sure they aren't involved):

{code}
 INFO [main] 2012-02-10 17:57:26,220 StorageService.java (line 768) JOINING: waiting for ring and schema information

ERROR [Thread-6] 2012-02-10 17:57:26,333 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-6,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:564)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:160)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:96)
ERROR [Thread-8] 2012-02-10 17:57:26,334 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-8,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:564)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:160)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:96)
{code}

That NPE is after an assertion (not triggered due to lack of -ea). Race on start-up - getting messages before stages set up? (not investigating further right now)

"
CASSANDRA-3882,avoid distributed deadlock in migration stage,"This is follow-up work for the remainders of CASSANDRA-3832 which was only a partial fix. The deadlock in the migration stage needs to be fixed, as it can cause bootstrap (at least) to take potentially a very very long time to complete, and might also cause a lack of schema propagation until otherwise ""poked""."
CASSANDRA-3798,get_count paging often asks for a page uselessly,"Current get_count paging stopping condition is:
{noformat}
if ((requestedCount == 0) || ((columns.size() == 1) && (lastName.equals(predicate.slice_range.start))))
{
    break;
}
{noformat}
On a ""count how many columns this row has"" query (arguably the main reason why you'd use get_count), requestedCount will never be 0, and so we'll stop whenever a page has only returned the last column of the preceding page. While this isn't wrong, we could stop as soon as a page returns less element than requested and avoid querying that last 1 column page."
CASSANDRA-3771,Allow paging through non-ordered partitioner results in CQL3,"CQL < 3 silently turns a ""key >= X"" into ""token(key) >= token(X)"".  This is not what users will expect, since many of the rows returned will not in fact satisfy the requested key inequality.  We should add syntax that makes the difference between keys and tokens explicit, possibly with a token() ""function"" as imagined here."
CASSANDRA-3755,NPE on invalid CQL DELETE command,"The CQL command {{delete from k where key='bar';}} causes Cassandra to hit a NullPointerException when the ""k"" column family does not exist, and it subsequently closes the Thrift connection instead of reporting an IRE or whatever. This is probably wrong."
CASSANDRA-3743,Lower memory consumption used by index sampling,"currently j.o.a.c.io.sstable.indexsummary is implemented as ArrayList of KeyPosition (RowPosition key, long offset)i propose to change it to:

RowPosition keys[]
long offsets[]

and use standard binary search on it. This will lower number of java objects used per entry from 2 (KeyPosition + RowPosition) to 1 (RowPosition).

For building these arrays convenient ArrayList class can be used and then call to .toArray() on it.

This is very important because index sampling uses a lot of memory on nodes with billions rows"
CASSANDRA-3702,CQL count() needs paging support,"Doing

{noformat}
SELECT count(*) from <cf>;
{noformat}

will max out at 10,000 because that is the default limit for cql queries. "
CASSANDRA-3677,NPE during HH delivery when gossip turned off on target,"probably not important bug

ERROR [OptionalTasks:1] 2011-12-27 21:44:25,342 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
        at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:371)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:356)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
"
CASSANDRA-3662,Report memory used by row index samples,"For better memory tuning on nodes with huge number of rows in CF, it will be good to know how much memory is used by entries collected during index sampling.

This will allow user to fine tune _index_interval_

Proposed output as addition to nodetool cfstats

                Column Family: sipdb
                SSTable count: 13
                Space used (live): 27378331692
                Space used (total): 27378331692
                Number of Keys (estimate): 216613000
                *Sampled index entries: 433226*
                *Memory used by sampled index: 51987120*
"
CASSANDRA-3659,Flush non-cfs backed secondary indexes along with CF,Non CFS backed secondary indexes currently don't get flushed alongside CF.  Only CFS backed ones do (i.e. KEYS)
CASSANDRA-3655,NPE when running upgradesstables,"Running a test upgrade from 0.7(version f sstables) to 1.0.
upgradesstables runs for about 40 minutes and then NPE's when trying to retrieve a key.

No files have been succesfully upgraded. Likely related is that scrub (without having run upgrade) consumes all RAM and OOMs.

Possible theory is that a lot of paths call IPartitioner's decorateKey, and, at least in the randompartitioner's implementation, if any of those callers pass a null ByteBuffer, they key will be null in the stack trace below.


java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:970)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1540)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:65)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:92)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:137)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
"
CASSANDRA-3644,parsing of chunk_length_kb silently overflows,"Not likely to trigger for ""real"" values; I noticed because some other bug caused the chunk length setting to be corrupted somehow and take on some huge value having nothing to do with what I asked for in my schema update (not yet identified why; separate issue)."
CASSANDRA-3616,Temp SSTable and file descriptor leak,"Discussion about this started in CASSANDRA-3532.  It's on it's own ticket now.

Anyhow:
The nodes in my cluster are using a lot of file descriptors, holding open tmp files. A few are using 50K+, nearing their limit (on Solaris, of 64K).

Here's a small snippet of lsof:
java 828 appdeployer *162u VREG 181,65540 0 333884 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776518-Data.db
java 828 appdeployer *163u VREG 181,65540 0 333502 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Data.db
java 828 appdeployer *165u VREG 181,65540 0 333929 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776527-Index.db
java 828 appdeployer *166u VREG 181,65540 0 333859 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Data.db
java 828 appdeployer *167u VREG 181,65540 0 333663 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776480-Data.db
java 828 appdeployer *168u VREG 181,65540 0 333812 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db

I spot checked a few and found they still exist on the filesystem too:
rw-rr- 1 appdeployer appdeployer 0 Dec 12 07:16 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db


After more investigation, it seems to happen during a CompactionTask.
I waited until I saw some -tmp- files hanging around in the data dir:

-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Data.db
-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Index.db

and then found this in the logs:
 INFO [CompactionExecutor:18839] 2011-12-12 21:47:07,173 CompactionTask.java (line 113) Compacting [SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760408-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760413-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760409-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-788314-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760407-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760412-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760410-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760411-Data.db')]

INFO [CompactionExecutor:18839] 2011-12-12 21:47:10,461 CompactionTask.java (line 218) Compacted to [/data1/cassandra/data/MA_DDR/messages_meta-hb-788896-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788897-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788898-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788899-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788900-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788901-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788902-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788903-Data.db,].  83,899,295 to 83,891,657 (~99% of original) bytes for 75,662 keys at 24.332518MB/s.  Time: 3,288ms.

Note that the timestamp of the 2nd log line matches the last modified time of the files, and has IDs leading up to, *but not including 788904*.

I thought this might be relavent information, but I haven't found the specific cause yet."
CASSANDRA-3615,CommitLog BufferOverflowException,"Reported on mailing list http://mail-archives.apache.org/mod_mbox/cassandra-dev/201112.mbox/%3CCAJHHpg2Rw_BWFJ9DycRGSYkmwMwrJDK3%3Dzw3HwRoutWHbUcULw%40mail.gmail.com%3E

ERROR 14:07:31,215 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.nio.BufferOverflowException
at java.nio.Buffer.nextPutIndex(Buffer.java:501)
at java.nio.DirectByteBuffer.putInt(DirectByteBuffer.java:654)
at
org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:259)
at
org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:568)
at
org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662)
 INFO 14:07:31,504 flushing high-traffic column family CFS(Keyspace='***',
ColumnFamily='***') (estimated 103394287 bytes)

It happened during a fairly standard load process using M/R."
CASSANDRA-3606,Resource Leaks in code,"at the following locations:
At SSLFactory:[line 87]
At SSTableExportTest:[line 83]
At CommitLogTest:[line 191]
At KeyGenerator:[line 108]
At Ec2Snitch.java:[line 74]
At SSTableExportTest.java:[line 83]
At CassandraServer.java:[line 1164]
At CommitLogTest.java:[line 191]
At ByteBufferUtilTest.java:[line 194]
At LazilyCompactedRowTest.java:[line 108]"
CASSANDRA-3567,remove unmaintained redhat rpm packaging,"The red hat package hasn't been changed since it was updated for 0.7.0 over a year ago, and nobody noticed until Paul did for CASSANDRA-3458."
CASSANDRA-3564,flush before shutdown so restart is faster,"Cassandra handles flush in its shutdown hook for durable_writes=false CFs (otherwise we're *guaranteed* to lose data) but leaves it up to the operator otherwise.  I'd rather leave it that way to offer these semantics:

- cassandra stop = shutdown nicely [explicit flush, then kill -int]
- kill -INT = shutdown faster but don't lose any updates [current behavior]
- kill -KILL = lose most recent writes unless durable_writes=true and batch commits are on [also current behavior]

But if it's not reasonable to use nodetool from the init script then I guess we can just make the shutdown hook flush everything."
CASSANDRA-3563,Packaging should increase vm.max_map_count to accommodate leveled compaction,"As the title says, leveled can create a lot of files and you can run into an IOError trying to mmap all of them."
CASSANDRA-3548,NPE in AntiEntropyService$RepairSession.completed(),"This may be related to CASSANDRA-3519 (cluster it was observed on is still 1.0.1), however i think there is still a race condition.

Observed on a 2 DC cluster, during a repair that spanned the DC's.  

{noformat}
INFO [AntiEntropyStage:1] 2011-11-28 06:22:56,225 StreamingRepairTask.java (line 136) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] Forwarding streaming repair of 8602 
ranges to /10.6.130.70 (to be streamed with /10.37.114.10)
...
 INFO [AntiEntropyStage:66] 2011-11-29 11:20:57,109 StreamingRepairTask.java (line 253) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] task succeeded
ERROR [AntiEntropyStage:66] 2011-11-29 11:20:57,109 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropyStage:66,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.completed(AntiEntropyService.java:712)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer$1.run(AntiEntropyService.java:912)
        at org.apache.cassandra.streaming.StreamingRepairTask$2.run(StreamingRepairTask.java:186)
        at org.apache.cassandra.streaming.StreamingRepairTask$StreamingRepairResponse.doVerb(StreamingRepairTask.java:255)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
{noformat}

One of the nodes involved in the repair session failed, e.g. (Not sure if this is from the same repair session as the streaming task above, but it illustrates the issue)

{noformat}
ERROR [AntiEntropySessions:1] 2011-11-28 19:39:52,507 AntiEntropyService.java (line 688) [repair #2bf19860-197f-11e1-0000-5ff37d368cb6] session completed with the following error
java.io.IOException: Endpoint /10.29.60.10 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
ERROR [GossipTasks:1] 2011-11-28 19:39:52,507 StreamOutSession.java (line 232) StreamOutSession /10.29.60.10 failed because {} died or was restarted/removed
ERROR [GossipTasks:1] 2011-11-28 19:39:52,571 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:782)
        at java.util.ArrayList$Itr.next(ArrayList.java:754)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

{noformat}

When a node is marked as failed AntiEntropyService.RepairSession.forceShutdown() clears the activejobs map. But the jobs to other nodes will continue, and will eventually call completed(). 

RepairSession.terminated should stop completed() from checking the map, but there is a race between the map been cleared and if there is an error in finally block it wont be set. 
"
CASSANDRA-3547,Race between cf flush and  its secondary indexes flush,"When a CF with indexes is flushed, it's indexes are flushed too. In particular their memtable is switched, but without making the old memtable frozen. This can conflict with a concurrent flush of the index itself, as reported on the user list by Michael Vaknine:
{noformat}
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 java.lang.AssertionError
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:671)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:745)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceBlockingFlush(ColumnFamilyStore.java:750)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.keys.KeysIndex.forceBlockingFlush(KeysIndex.java:119)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.flushIndexesBlocking(SecondaryIndexManager.java:258)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.maybeBuildSecondaryIndexes(SecondaryIndexManager.java:123)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:151)
{noformat}"
CASSANDRA-3544,NPE on startup when there are permissions issues with directories,"If the directories used by cassandra for data, commitlog, and saved caches aren't readable due to permissions, you get an NPE on startup.  In particular, if none of them are readable, you'll see something like this:

{noformat}
ERROR 14:50:11,945 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: null
{noformat}

This traceback happens when the saved_caches directory isn't readable, but you can get different ones if only the data or commitlog directories aren't readable.

We should check the permissions of these directories before trying to list their contents."
CASSANDRA-3543,Commit Log Allocator deadlock after first start with empty commitlog directory,"While testing CASSANDRA-3541 at some point stress completely timed out.  I proceeded to shut the cluster down and 2/3 JVMs hang infinitely.  After a while, one of them logged:

{noformat}
WARN 19:07:50,133 Some hints were not written before shutdown.  This is not supposed to happen.  You should (a) run repair, and (b) file a bug report
{noformat}"
CASSANDRA-3534,Remove memory emergency pressure valve,"Seems like when these thresholds are reached we are trying to reduce the keycache and Memtable sizes, but in the trunk memtable is moved off-heap hence reducing that will not actually help.
Reducing the keycache might help but we might want to provide a way to bring the cache level back to the configured value when the pressure goes away.

Conversation from IRC: we might want to remove these setting all together after global keycache."
CASSANDRA-3508,requiring --debug to see stack traces for failures in cassandra-cli is a terrible idea (aka silent failure is never a valid option),"this manifests itself in cassandra-cli by returning null to the user.  In order to see what the problem was (and in many cases, just to know there was a problem at all) requires running cassandra-cli with ""--debug"""
CASSANDRA-3495,capture BloomFilter memory size in Cassandra (JMX),"Maybe this could be done in https://issues.apache.org/jira/browse/CASSANDRA-3347 also, but not sure what's the scope in that jira. It'd be great if the BF memory size can be captured in the JMX monitoring. 

Though not sure how you would capture the ""heap size"" (easily that is) of the object. There is a BF.serializedSize() , can this be exposed to the BloomFilterTracker and DataTracker... and use this instead? Anyway, will let the implementor to decide/design "
CASSANDRA-3482,Flush Assertion Error - CF size changed during serialization,"I have seen the following assert in the logs - there are no other suspicious or unexpected log messages.

INFO [FlushWriter:9] 2011-11-10 13:08:58,882 Memtable.java (line 237) Writing Memtable-UserData@1388955390(25676955/430716097 serialized/live bytes, 478913 ops)
ERROR [FlushWriter:9] 2011-11-10 13:08:59,513 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:9,5,main]
java.lang.AssertionError: CF size changed during serialization: was 4 initially but 3 written
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:94)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:177)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:264)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:47)
        at org.apache.cassandra.db.Memtable$4.runMayThrow(Memtable.java:289)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Once the error occurs, further MemtablePostFlusher tasks are blocked:

nodetool tpstats:
  Pool Name                    Active   Pending      Completed   Blocked  All time blocked
  MemtablePostFlusher               1        18             16         0                 0

It *seems* that all further flushed for the particular CF (in this case UserData) will also result in the same assertion error. Restarting the node fixes the problem."
CASSANDRA-3458,Add cqlsh to deb and rpm packaging,"Once (if?) CASSANDRA-3188 is committed, cqlsh will be distributed with the cassandra tarballs, but not in the debs or rpms. (Actually, it looks like the cqlsh script will get put in the rpm by accident, but not its associated libraries).

We might even want to break cqlsh out into a separate package from the same source, so that it can be installed on machines only intended to be used as cassandra clients, not servers.

Maybe that doesn't make sense without including nodetool and cassandra-cli too, and then we'd need yet another package to hold the jars that are common between cassandra and cassandra-client... maybe it's not worth it for now.

Either way, make sure installing cassandra debs and rpms ends up with a working cqlsh."
CASSANDRA-3446,Problem SliceByNamesReadCommand on super column family after flush operation,"I'm having a problem with doing a multiget_slice on a super column family
after its first flush. Updates to the column values work properly, but
trying to retrieve the updated values using a multiget_slice operation fail
to get the updated values. Instead they return the values from before the
flush. The problem is not apparent with standard column families.

I've seen this problem in Cassandra v1.0.0 and v1.0.1. The problem
is not present in Cassandra v0.7.6.

Steps to reproduce:

   1. Create one or more super column entries
   2. Verify the sub column values can be updated and that you can retrieve
   the new values
   3. Use nodetool to flush the column family or restart cassandra
   4. Update the sub column values
   5. Verify they have been updated using cassandra-cli
   6. Verify you *DO NOT* get the updated values when doing a
   multiget_slice; instead you get the old values from before the flush

You can get the most recent value by doing a flush followed by a major
compaction. However, future updates are not retrieved properly either.

With debug turned on, it looks like the multiget_slice query uses the
following command/consistency level:
SliceByNamesReadCommand(table='test_cassandra', key=666f6f,
columnParent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', columns=[foo,])/QUORUM.

Cassandra-cli uses the following command/consistency level for a get_slice:
SliceFromReadCommand(table='test_cassandra', key='666f6f',
column_parent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', start='', finish='', reversed=false,
count=1000000)/QUORUM

Notice the test program gets 'bar2' for the column values and cassandra-cli
gets 'bar3' for the column values:

tcpdump from test program using hector-core:1.0-1

16:46:07.424562 IP iam.47158 > iam.9160: Flags [P.], seq 55:138, ack 30,
win 257, options [nop,nop,TS val 27474096 ecr 27474095], length 83
E....#@.@.PK.........6#.....].8......{.....
..8...8.........multiget_slice................foo..........test................foo.........
16:46:07.424575 IP iam.9160 > iam.47158: Flags [.], ack 138, win 256,
options [nop,nop,TS val 27474096 ecr 27474096], length 0
E..4..@.@.<.........#..6].8..........(.....
..8...8.
16:46:07.428771 IP iam.9160 > iam.47158: Flags [P.], seq 30:173, ack 138,
win 256, options [nop,nop,TS val 27474097 ecr 27474096], length 143
@.@.<&........#..6].8................
............foo...............foo...............foo1.......bar2
........6h........foo2.......bar2
........I.....


tcpdump of cassandra-cli:

16:30:55.945123 IP iam.47134 > iam.9160: Flags [P.], seq 370:479, ack 5310,
win 387, options [nop,nop,TS val 27246226 ecr 27241207], length 109
E.....@.@.9q..........#..n.X\
.............
................get_range_slices..............test.........................................................d.........
16:30:55.945152 IP iam.9160 > iam.47134: Flags [.], ack 479, win 256,
options [nop,nop,TS val 27246226 ecr 27246226], length 0
E..4..@.@."".........#...\
...n.......(.....
........
16:30:55.949245 IP iam.9160 > iam.47134: Flags [P.], seq 5310:5461, ack
479, win 256, options [nop,nop,TS val 27246227 ecr 27246226], length 151
E.....@.@.""V........#...\
...n.............
....................get_range_slices...................foo..................foo...............foo1.......bar3
........&.........foo2.......bar3
........: ....."
CASSANDRA-3429,Fix truncate/compaction race without locking,See CASSANDRA-3399 for original problem description.
CASSANDRA-3385,NPE in hinted handoff,"I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more


this could possibly be related to #3291
"
CASSANDRA-3355,Race trying to register CFS MBean,"I heard reports of this multiple times, here's an example:

{noformat}
 INFO 16:08:23,672 reading saved cache /var/lib/cassandra/saved_caches/spider-InterfaceDailyCompressed-KeyCache
 INFO 16:08:23,995 Opening /var/lib/cassandra/data/spider/InterfaceDailyCompressed-g-63
 INFO 16:08:24,253 Opening /var/lib/cassandra/data/spider/InterfaceDailyCompressed-g-75
 INFO 16:08:24,422 Opening /var/lib/cassandra/data/spider/InterfaceDailyCompressed-g-113
 INFO 16:08:24,443 Opening /var/lib/cassandra/data/spider/InterfaceDailyCompressed-g-36
 INFO 16:08:24,756 Opening /var/lib/cassandra/data/spider/InterfaceDailyCompressed-g-102
 INFO 16:08:24,789 Opening /var/lib/cassandra/data/spider/InterfaceDailyCompressed-g-58
ERROR 16:08:25,105 Exception encountered during startup.
java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=spider,columnfamily=InterfaceDailyCompressed
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:303)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:465)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:435)
        at org.apache.cassandra.db.Table.initCf(Table.java:369)
        at org.apache.cassandra.db.Table.<init>(Table.java:306)
        at org.apache.cassandra.db.Table.open(Table.java:111)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:187)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:341)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:97)
Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=spider,columnfamily=InterfaceDailyCompressed
        at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:299)
        ... 8 more
Exception encountered during startup.
java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=spider,columnfamily=InterfaceDailyCompressed
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:303)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:465)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:435)
        at org.apache.cassandra.db.Table.initCf(Table.java:369)
        at org.apache.cassandra.db.Table.<init>(Table.java:306)
        at org.apache.cassandra.db.Table.open(Table.java:111)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:187)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:341)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:97)
Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=spider,columnfamily=InterfaceDailyCompressed
        at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:299)
        ... 8 more
{noformat}

This wouldn't be too big of a deal, except it's actually preventing startup which kind of sucks if you're expecting cassandra to start on the first try from an init script."
CASSANDRA-3349,NPE on malformed CQL,"It's not clear why, but the CQL grammar specification in Cql.g allows for an empty WHERE clause on DELETE, i.e.:

{noformat}
DELETE FROM someCF WHERE;
{noformat}

When this is used, with or without a column list, it causes an NPE on the node processing the CQL. Traceback on a recent 1.0.0 build:

{noformat}
ERROR [pool-2-thread-1] 2011-10-11 15:45:25,655 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
        at org.apache.cassandra.cql.CqlParser.deleteStatement(CqlParser.java:1994)
        at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:292)
        at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:984)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:500)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1268)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat}

The CQL client gets an error with the message, ""Internal application error"".

It might be better to allow leaving off the ""WHERE"" as well as the condition, to match SQL semantics, although fixing that probably won't solve this problem."
CASSANDRA-3348,NPE when running nodetool info on secondary interface,"When nodetool is run against eth0:

cassandra@ip-10-1-1-136:~$ nodetool -h 10.1.1.136 -p 9090 info
Token            : 113427455640312821154458202477256070484
Gossip active    : true
Load             : 3.54 MB
Generation No    : 1318365429
Uptime (seconds) : 1052
Heap Memory (MB) : 20.54 / 848.00
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:93)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.EndpointSnitchInfo.getDatacenter(EndpointSnitchInfo.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


When nodetool is run against tun0:

cassandra@ip-10-1-1-136:~$ nodetool -h 172.1.1.136 -p 9090 info
Token            : 113427455640312821154458202477256070484
Gossip active    : true
Load             : 3.54 MB
Generation No    : 1318365429
Uptime (seconds) : 1250
Heap Memory (MB) : 28.25 / 848.00
Data Center      : ap-southeast
Rack             : 1b
Exceptions       : 0

When I run 'nodetool ring' against either IP address, no NPE.  "
CASSANDRA-3332,"Build warning at org/apache/cassandra/io/util/Memory.java, is this expected ?","Build warning at org/apache/cassandra/io/util/Memory.java, is this expected ?

   [javac] /home/satish/workspace/cassandra/src/java/org/apache/cassandra/io/util/Memory.java:22: warning: sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
    [javac] import sun.misc.Unsafe;
    [javac]                ^
    [javac] /home/satish/workspace/cassandra/src/java/org/apache/cassandra/io/util/Memory.java:28: warning: sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
    [javac]     private static final Unsafe unsafe;
    [javac]                          ^
    [javac] /home/satish/workspace/cassandra/src/java/org/apache/cassandra/io/util/Memory.java:34: warning: sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
    [javac]             Field field = sun.misc.Unsafe.class.getDeclaredField(""theUnsafe"");
    [javac]                                   ^
    [javac] /home/satish/workspace/cassandra/src/java/org/apache/cassandra/io/util/Memory.java:36: warning: sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
    [javac]             unsafe = (sun.misc.Unsafe) field.get(null);
    [javac]                               ^


satish@ubuntu:~/workspace/cassandra$ java -version
java version ""1.6.0_27""
Java(TM) SE Runtime Environment (build 1.6.0_27-b07)
Java HotSpot(TM) 64-Bit Server VM (build 20.2-b06, mixed mode)

satish@ubuntu:~/workspace/cassandra$ javac -version
javac 1.6.0_27

satish@ubuntu:~/workspace/cassandra$ svn info
Path: .
URL: http://svn.apache.org/repos/asf/cassandra/trunk
Repository Root: http://svn.apache.org/repos/asf
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 1180097
Node Kind: directory
Schedule: normal
Last Changed Author: jbellis
Last Changed Rev: 1179902
Last Changed Date: 2011-10-06 17:00:21 -0700 (Thu, 06 Oct 2011)

"
CASSANDRA-3309,Nodetool Doesnt close the open JMX connection causing it to leak Threads,"When nodetool is used intensively we will see 1000's of ""JMX server connection timeout""

Fix is to close the connections when no longer needed."
CASSANDRA-3293,Metered Flusher log message confusing," INFO [NonPeriodicTasks:1] 2011-10-01 17:34:32,652 MeteredFlusher.java (line 62) flushing high-traffic column family ColumnFamilyStore(+table='rapidshare',+ columnFamily='resultcache')

instead of table it should be *keyspace=*"
CASSANDRA-3264,Add wide row paging for ColumnFamilyInputFormat and ColumnFamilyOutputFormat,"Hadoop input/output formats currently can OOM on wide rows.

We can add a new option to the ConfigHelper like columnPagingSize with a default of Integer.MAX_VALUE.
The input format would page the row internally rather than pull it over at once.
The output format could also use this to avoid sending huge rows over at once."
CASSANDRA-3257,Enabling SSL on a fairly light cluster leaks Open files.,"To reproduce:

Enable SSL encryption and let the server be idle for a day or so you will see the below....

[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
16333
Sun Sep 25 17:23:29 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7501 java.lang:type=Memory gc
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
64
Sun Sep 25 17:23:53 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ 

After running GC manually the issue goes away."
CASSANDRA-3254,MessagingService sends blank bytes,MessagingService is calling DataOutputBuffer.getData() instead of .asByteArray() in two places. This results in always sending the full buffer size of at least 128 bytes even if the message is smaller. For messages like gossip and single col mutations its around 40 to 80 bytes wasted per message.
CASSANDRA-3253,inherent deadlock situation in commitLog flush?,"after my system ran for a while, it consitently goes into frozen state where all the mutations stage threads are waiting
on the switchlock,

the reason is that the switchlock is held by commit log, as shown by the following thread dump:



""COMMIT-LOG-WRITER"" prio=10 tid=0x00000000010df000 nid=0x32d3 waiting on condition [0x00007f2d81557000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f3579eec060> (a java.util.concurrent.FutureTask$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:248)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.getContext(CommitLog.java:386)
        at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:650)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.commitlog.CommitLog.createNewSegment(CommitLog.java:573)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:81)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:596)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)


we can clearly see that the COMMIT-LOG-WRITER thread is running the regular appender , but the appender itself calls getContext(), which again submits a new Callable to be executed, and waits on the Callable. but the new Callable is never going to be executed since the executor has only *one* thread.


I believe this is a deterministic bug.



"
CASSANDRA-3244,JDBC CassandraConnection may lead to memory leak when used in a pool,"I may be wrong here but I noticed that the implementations of CassandraConnection#createStatement() and CassandraConnection#prepareStatement() keep(cache) the created Statement/PrepareStatement internally in a List.

They list is freed up only during CassandraConnection.close() which makes me think that, if the connection object is used in a pool implementation, it will lead to a memory leak as it will hold every single statement that is used to interact with the DB until the connection gets closed. 

"
CASSANDRA-3210,memtables do not need to be flushed on the Table.apply() path anymore after 2449,"2449 removes auto-flush from Table.apply(), but the data structure is still there, no harm, but better remove it:

in
https://github.com/apache/cassandra/blob/c7cdc317c9a14e29699f9842424388aee77d0e1a/src/java/org/apache/cassandra/db/Table.java

line 399 and 470"
CASSANDRA-3203,Odd flush behavior,"Given the same workload against 0.8, trunk is creating more than twice the amount of sstables.  Even though a uniform stress workload is being generated, flush size degrades quickly:

{noformat}
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:22,878 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2058235391(7741
035/110172631 serialized/live bytes, 151785 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:24,888 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1520390052(3887
220/72403158 serialized/live bytes, 76220 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:26,890 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1868496516(4097
085/76255481 serialized/live bytes, 80335 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:28,893 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@498232521(43513
20/80922269 serialized/live bytes, 85320 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:29,895 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1592308290(2310
810/44514839 serialized/live bytes, 45310 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:30,897 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@775439677(22684
80/64984390 serialized/live bytes, 44480 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:31,899 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@928217914(26741
85/76231422 serialized/live bytes, 52435 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:32,901 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@158103119(27511
95/77317732 serialized/live bytes, 53945 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:33,903 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2035169258(3132
420/88934701 serialized/live bytes, 61420 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:34,905 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1097314626(2979
675/83651699 serialized/live bytes, 58425 ops)
{noformat}

The serialized to live size ratio appears completely out of whack."
CASSANDRA-3198,debian packaging installation problem when installing for the first time,"when installing cassandra through the debian packaging for the first time, there is permission problem when starting Cassandra.

Normally, the postinst script change owner of /var/log/cassandra and /var/lib/cassandra from root to cassandra user.

there is a problem with the test which verify if threre is a need to change the owner of these directory or not.

On a new install, the $2 parameter is not set and the the test is false and the owner is not changed.

(simply, i think replace ""&&"" with ""||"" might work)
"
CASSANDRA-3192,NPE in RowRepairResolver,"On a 3 node brisk cluster (running against C* 1.0 branch), I was running the java stress tool and the terasort concurrently in two sessions.  Eventually both jobs failed with TimedOutException.
  
From this point forward most additional activity will fail with a TimedOutException. 
* Java Stress Tool - 5 rows / 10 columns - Operation [0] retried 10 times - error inserting key 0 ((TimedOutException))
* Hive - show tables: FAILED: Error in metadata: com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStoreException: There was a problem with the Cassandra Hive MetaStore: Could not connect to Cassandra. Reason: Error connecting to node localhost

However, the Cassandra CLI appears to be happy
* Cassandra CLI: you can successfully insert and read using consistencylevel as ONE or ALL

The seed node has the following error repeatedly occurring in the logs.  The other two nodes have no errors.

{code}
ERROR [ReadRepairStage:15] 2011-09-13 00:44:25,971 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadRepairStage:15,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:82)
	at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}"
CASSANDRA-3186,nodetool should not NPE when rack/dc info is not yet available,"As the title says.  What happens is the persisted ring is loaded, but if those nodes are down and you're using a snitch like ec2 that gets rack/dc info from gossip, nodetool NPEs instead of showing that the node is down."
CASSANDRA-3168,Arena allocation causes excessive flushing on small heaps,"adding allocator.size() to Memtable.getLiveSize has two problems:

1) it double-counts allocated parts of regions
2) it makes the size of an empty memtable the size of a single region

(2) is a particular problem because flushing a nearly-empty memtable will not actually free up much memory -- we just trade one almost-empty region, for another.  In testing, I even saw this happening to the low-traffic system tables like LocationInfo."
CASSANDRA-3155,Secondary index should report it's memory consumption,Non-CFS backed secondary indexes will consume RAM which should be reported back to Cassandra to be factored into it's flush by RAM amount.
CASSANDRA-3097,Missing encryption_options in results in NPE when repair:ing,"if encryption_options is not in cassandra.yaml, an NPE is thrown when trying to do nodetool repair.


ERROR [AntiEntropyStage:1] 2011-08-29 09:11:05,602 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[AntiEntropyStage:1,5,main]
java.lang.RuntimeException: java.io.IOException: Streaming repair failed.
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:796)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Streaming repair failed.
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.performStreamingRepair(AntiEntropyService.java:820)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:792)
	... 3 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.net.MessagingService.stream(MessagingService.java:420)
	at org.apache.cassandra.streaming.StreamOutSession.begin(StreamOutSession.java:176)
	at org.apache.cassandra.streaming.StreamOut.transferSSTables(StreamOut.java:166)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.performStreamingRepair(AntiEntropyService.java:814)
	... 4 more"
CASSANDRA-3090,NPE while get_range_slices,"We have 4 node Cassandra (version 0.8.1) cluster. 2 CF inside. While first CF is working properly (read/store), get_range_slices query on second CF return Internal error.


{code} 
ERROR [pool-2-thread-51] 2011-08-25 15:02:04,360 Cassandra.java (line 3210) Internal error processing get_range_slices
java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:298)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:406)
        at org.apache.cassandra.service.RowRepairResolver.maybeScheduleRepairs(RowRepairResolver.java:103)
        at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:120)
        at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:85)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:715)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:617)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:3202)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{code} "
CASSANDRA-3085,Race condition in sstable reference counting,"DataTracker gives us an atomic View of memtable/sstables, but acquiring references is not atomic.  So it is possible to acquire references to an SSTableReader object that is no longer valid, as in this example:

View V contains sstables {A, B}.  We attempt a read in thread T using this View.
Meanwhile, A and B are compacted to {C}, yielding View W.  No references exist to A or B so they are cleaned up.
Back in thread T we acquire references to A and B.  This does not cause an error, but it will when we attempt to read from them next."
CASSANDRA-3059,sstable2json on an index sstable failed with NPE,"$ ./bin/sstable2json /var/lib/cassandra-trunk/data/Keyspace1/Standard1.Idx1-h-1-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:74)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:69)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:64)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:147)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:71)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:177)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:142)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:134)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:304)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:335)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:348)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:406)


cfm is null for Index CF?"
CASSANDRA-3057,secondary index on a column that has a value of size > 64k will fail on flush,"exception seen on flush when an indexed column contain size > 64k:

granted that having a value > 64k possibly mean something that shouldn't be indexed as it most likely would have a high cardinality, but i think there would still be some valid use case for it.

test case:
simply run the stress test with 
-n 1 -u 0 -c 2  -y Standard  -o INSERT  -S 65536 -x KEYS

then call a flush

exception:
 INFO [FlushWriter:8] 2011-08-18 21:49:33,214 Memtable.java (line 218) Writing Memtable-Standard1.Idx1@1652462853(16/20 serialized/live bytes, 1 ops)
Standard1@980087547(196659/245823 serialized/live bytes, 3 ops)
ERROR [FlushWriter:8] 2011-08-18 21:49:33,230 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:8,5,RMI Runtime]
java.lang.AssertionError: 65536
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithShortLength(ByteBufferUtil.java:330)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:245)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

"
CASSANDRA-3052,CQL: ResultSet.next() gives NPE when run after an INSERT or CREATE statement,"This test script used to work until I upgraded the jdbc driver to 1.0.4.

*CQL 1.0.4*: apache-cassandra-cql-1.0.4-SNAPSHOT.jar build at revision 1158979

*Repro Script*: 
* drop in test directory, change package declaration and run:  ant test -Dtest.name=resultSetNPE
* The script gives you a NullPointerException when you uncomment out the following lines after a CREATE or INSERT statement.
{code}
colCount = res.getMetaData().getColumnCount();

res.next();
{code}
* Please note that there is no need to comment out those lines if a SELECT statement was run prior.


{code}
package com.datastax.bugs;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

import org.junit.Test;

public class resultSetNPE {
    
    @Test
    public void createKS() throws Exception {   
        Connection initConn = null;
        Connection connection = null;

        ResultSet res;
        Statement stmt;
        int colCount = 0;
        
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        
        // Check create keyspace
        initConn = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/default"");     
        stmt = initConn.createStatement();

        try {
          System.out.println(""Running DROP KS Statement"");  
          res = stmt.executeQuery(""DROP KEYSPACE ks1"");  
          // res.next();
          
        } catch (SQLException e) {
            if (e.getMessage().startsWith(""Keyspace does not exist"")) 
            {
                // Do nothing - this just means you tried to drop something that was not there.
                // res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
            } 
        }   
          
        System.out.println(""Running CREATE KS Statement"");
        res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
        // res.next();

        initConn.close();    
    }  
 
    @Test
    public void createCF() throws Exception 
    {   

        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();

        System.out.print(""Running CREATE CF Statement"");
        ResultSet res = stmt.executeQuery(""CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint)"");    
        
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        connection.close();               
    }  
    
    @Test
    public void simpleSelect() throws Exception 
    {   
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();
        
        System.out.print(""Running INSERT Statement"");
        ResultSet res = stmt.executeQuery(""INSERT INTO users (KEY, password) VALUES ('user1', 'ch@nge')"");  
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        System.out.print(""Running SELECT Statement"");
        res = stmt.executeQuery(""SELECT KEY, gender, state FROM users"");  
        colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        res.getRow();
        res.next();
            
        connection.close(); 
    }  
}
{code}
"
CASSANDRA-3046,"[patch] No need to use .equals on enums, just opens up chance of NPE","pretty trivial patch that change enum1.equals(enum2) into enum1 == enum2, as .equals isn't needed, and just opens up the possibility of NPEs where == handles them correctly."
CASSANDRA-3023,NPE in describe_ring,"Not sure how much of the following is relevant besides the stack trace, but here I go:

I have a 2 DC, 2 node per DC cluster. DC1 had it's seed replaced but I hadn't restarted. I upgraded to 0.8.4 in the following fashion:

-edited seeds
-stopped both DC1 nodes
-upgraded jars
-started both nodes at the same time

The non-seed node came up first and showed the following error. Then when the seed node came up, the error went away on the non-seed node but started occurring on the seed node:

ERROR [pool-2-thread-15] 2011-08-12 22:32:27,438 Cassandra.java (line 3668) Internal error processing describe_ring
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.getRangeToRpcaddressMap(StorageService.java:623)
	at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:731)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:3664)
	at org.apache.cassandra.thrift.Brisk$Processor.process(Brisk.java:464)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
"
CASSANDRA-3021,Null pointer dereference of m in org.apache.cassandra.db.commitlog.CommitLogSegment.dirtyString(),
CASSANDRA-3007,NullPointerException in MessagingService.java:420,"I'm getting large quantity of exceptions during streaming. It is always in MessagingService.java:420. The streaming appears to be blocked.

 INFO 10:11:14,734 Streaming to /10.235.77.27
ERROR 10:11:14,734 Fatal exception in thread Thread[StreamStage:2,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.MessagingService.stream(MessagingService.java:420)
        at org.apache.cassandra.streaming.StreamOutSession.begin(StreamOutSession.java:176)
        at org.apache.cassandra.streaming.StreamOut.transferRangesForRequest(StreamOut.java:148)
        at org.apache.cassandra.streaming.StreamRequestVerbHandler.doVerb(StreamRequestVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
"
CASSANDRA-2996,Fix NPE in getRangeToRpcaddressMap,"DatabaseDescriptor.getRpcAddress() can be null, which getRangeToRpcaddressMap doesn't take into account"
CASSANDRA-2994,OutOfBounds in CompressedSequentialWriter.flushData,"Near the beginning of a wide row test with CASSANDRA-47 compression enabled on a counter column family, I see the following exception:

{code:java} WARN [CompactionExecutor:5] 2011-08-04 21:50:14,558 FileUtils.java (line 95) Failed closing org.apache.cassandra.io.compress.CompressedSequentialWriter@28f01347
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.syncInternal(SequentialWriter.java:150)
	at org.apache.cassandra.io.util.SequentialWriter.close(SequentialWriter.java:283)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.close(CompressedSequentialWriter.java:159)
	at org.apache.cassandra.io.util.FileUtils.closeQuietly(FileUtils.java:91)
	at org.apache.cassandra.io.sstable.SSTableWriter.cleanupIfNecessary(SSTableWriter.java:201)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:176)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:5] 2011-08-04 21:50:14,561 AbstractCassandraDaemon.java (line 146) Fatal exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.reBuffer(SequentialWriter.java:226)
	at org.apache.cassandra.io.util.SequentialWriter.writeAtMost(SequentialWriter.java:117)
	at org.apache.cassandra.io.util.SequentialWriter.write(SequentialWriter.java:101)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:105)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:150)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:153)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}"
CASSANDRA-2977,add paging to multiget_count,"CASSANDRA-2894 added paging to the single-row get_count.  But, this does not help multiget_count since they are different code paths.

Options:
- do nothing; multiget_count is a fairly obscure call, and there is no sane way to support it in CQL so will become even more obscure
- translate multiget_count into a series of get_count calls; high-latency, if done naively
- translate get_count into a single multiget_count, and perform the paging in StorageProxy.  HIGHLY painful to manage paging multiple rows independently, in the general case."
CASSANDRA-2971,Append (not add new) InetAddress info logging when starting MessagingService,"Currently we have 

{code: title=MessagingService.getServerSocket(InetAddress localEp) }
logger_.info(""Starting Messaging Service on port {}"", DatabaseDescriptor.getStoragePort());
{code}

We should probably just print the whole binded address. The address is an InetSocketAddress:

{code}
InetSocketAddress address = new InetSocketAddress(localEp, DatabaseDescriptor.getStoragePort());
try
{
    ss.bind(address);
}
{code}

{code}
logger_.info(""Starting Messaging Service on {}"",address);
{code}

sample output with the new log:
{noformat}
 INFO [main] 2011-07-29 18:54:54,018 MessagingService.java (line 226) Starting Messaging Service on faranth/192.168.1.141:7000
{noformat}"
CASSANDRA-2958,Flush memtables on shutdown when durable writes are disabled,"Memtables need to be flushed on shutdown when durable_writes is set to false, otherwise data loss occurs as the data is not available to be replayed from the commit log. "
CASSANDRA-2951,FreeableMemory can be accessed after it is invalid,"SerializingCache.get looks like this:

{code}
    public V get(Object key)
    {
        FreeableMemory mem = map.get(key);
        if (mem == null)
            return null;
        return deserialize(mem);
    }
{code}

If a cache object is evicted or replaced after the get happens, but before deserialize completes, we will trigger an assertion failure (if asserts are enabled) or segfault (if they are not)."
CASSANDRA-2918,"After repair, missing rows from query if you don't flush other replicas","*Cluster Config*
{code}
cathy1  -  50.57.114.45 - Token: 0
cathy2  -  50.57.107.176 - Token: 56713727820156410577229101238628035242
cathy3  -  50.57.114.39 - Token: 113427455640312821154458202477256070484
{code}

*+2) Kill cathy3:  50.57.114.39+*
{code}
root@cathy2:~/cass-0.8/bin# ./nodetool -h localhost ring

Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               113427455640312821154458202477256070484     
50.57.114.45    datacenter1 rack1       Up     Normal  59.84 KB        33.33%  0                                           
50.57.107.176   datacenter1 rack1       Up     Normal  59.85 KB        33.33%  56713727820156410577229101238628035242      
50.57.114.39    datacenter1 rack1       Down   Normal  59.85 KB        33.33%  113427455640312821154458202477256070484     
{code}


*+3) Run java stress tool+*
{code}
./bin/stress -o insert -n 1000 -c 10 -l 3 -e QUORUM -d 50.57.114.45,50.57.107.176
{code}



*+4) Start Cassandra on cathy3+*


*+5) Run repair on cathy3+*
{code}
nodetool -h cathy3 repair Keyspace1 Standard1
{code}


*+6) Kill cathy1 and cathy2+*
{code}
root@cathy3:~/cass-0.8/bin# ./nodetool -h cathy3 ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               113427455640312821154458202477256070484     
50.57.114.45    datacenter1 rack1       Down   Normal  105.46 KB       33.33%  0                                           
50.57.107.176   datacenter1 rack1       Down   Normal  106 KB          33.33%  56713727820156410577229101238628035242      
50.57.114.39    datacenter1 rack1       Up     Normal  331.33 KB       33.33%  113427455640312821154458202477256070484 
{code}


*+7) Log into cassandra-cli on cathy3 - expect 1000 rows returned+*
{code}

[default@Keyspace1] consistencylevel as ONE;  
Consistency level is set to 'ONE'.

[default@Keyspace1] list Standard1 limit 2000;
.....

323 Rows Returned.
{code}

"
CASSANDRA-2894,add paging to get_count,"It is non-intuitive that get_count materializes the entire slice-to-count on the coordinator node (to perform read repair and > CL.ONE consistency).  Even experienced users have been known to cause memory problems by requesting large counts.

The user cannot page the count himself, because you need a start and stop column to do that, and get_count only returns an integer.

So the best fix is for us to do the paging under the hood, in CassandraServer.  Add a limit to the slicepredicate they specify, and page through it.

We could add a global setting for count_slice_size, and document that counts of more columns than that will have higher latency (because they make multiple calls through StorageProxy for the pages)."
CASSANDRA-2868,Native Memory Leak,"We have memory issues with long running servers. These have been confirmed by several users in the user list. That's why I report.

The memory consumption of the cassandra java process increases steadily until it's killed by the os because of oom (with no swap)

Our server is started with -Xmx3000M and running for around 23 days.

pmap -x shows

Total SST: 1961616 (mem mapped data and index files)
Anon  RSS: 6499640
Total RSS: 8478376

This shows that > 3G are 'overallocated'.

We will use BRAF on one of our less important nodes to check wether it is related to mmap and report back."
CASSANDRA-2866,Add higher nofile (ulimit -n) property to the install configuration for debian and rpm packaging,"Currently in the packaging we set the memlock to unlimited.  We should also up the nofile value (ulimit -n) so that it's more than 1024, likely unlimited.  Otherwise, there can be odd indirect bugs.  For example, I've seen compaction fail because of the ""too many open files"" error."
CASSANDRA-2863,NPE when writing SSTable generated via repair,"A NPE is generated during repair when closing an sstable generated via SSTable build. It doesn't happen always. The node had been scrubbed and compacted before calling repair.

 INFO [CompactionExecutor:2] 2011-07-06 11:11:32,640 SSTableReader.java (line 158) Opening /d2/cassandra/data/sbs/walf-g-730
ERROR [CompactionExecutor:2] 2011-07-06 11:11:34,327 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:2,1,main] 
java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1103)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1094)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
"
CASSANDRA-2829,memtable with no post-flush activity can leave commitlog permanently dirty,"Only dirty Memtables are flushed, and so only dirty memtables are used to discard obsolete commit log segments. This can result it log segments not been deleted even though the data has been flushed.  

Was using a 3 node 0.7.6-2 AWS cluster (DataStax AMI's) with pre 0.7 data loaded and a running application working against the cluster. Did a rolling restart and then kicked off a repair, one node filled up the commit log volume with 7GB+ of log data, there was about 20 hours of log files. 

{noformat}
$ sudo ls -lah commitlog/
total 6.9G
drwx------ 2 cassandra cassandra  12K 2011-06-24 20:38 .
drwxr-xr-x 3 cassandra cassandra 4.0K 2011-06-25 01:47 ..
-rw------- 1 cassandra cassandra 129M 2011-06-24 01:08 CommitLog-1308876643288.log
-rw------- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308876643288.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 01:36 CommitLog-1308877711517.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308877711517.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 02:20 CommitLog-1308879395824.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308879395824.log.header
...
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 20:38 CommitLog-1308946745380.log
-rw-r--r-- 1 cassandra cassandra   36 2011-06-24 20:47 CommitLog-1308946745380.log.header
-rw-r--r-- 1 cassandra cassandra 112M 2011-06-24 20:54 CommitLog-1308947888397.log
-rw-r--r-- 1 cassandra cassandra   44 2011-06-24 20:47 CommitLog-1308947888397.log.header
{noformat}

The user KS has 2 CF's with 60 minute flush times. System KS had the default settings which is 24 hours. Will create another ticket see if these can be reduced or if it's something users should do, in this case it would not have mattered. 

I grabbed the log headers and used the tool in CASSANDRA-2828 and most of the segments had the system CF's marked as dirty.

{noformat}
$ bin/logtool dirty /tmp/logs/commitlog/

Not connected to a server, Keyspace and Column Family names are not available.

/tmp/logs/commitlog/CommitLog-1308876643288.log.header
Keyspace Unknown:
	Cf id 0: 444
/tmp/logs/commitlog/CommitLog-1308877711517.log.header
Keyspace Unknown:
	Cf id 1: 68848763
...
/tmp/logs/commitlog/CommitLog-1308944451460.log.header
Keyspace Unknown:
	Cf id 1: 61074
/tmp/logs/commitlog/CommitLog-1308945597471.log.header
Keyspace Unknown:
	Cf id 1000: 43175492
	Cf id 1: 108483
/tmp/logs/commitlog/CommitLog-1308946745380.log.header
Keyspace Unknown:
	Cf id 1000: 239223
	Cf id 1: 172211

/tmp/logs/commitlog/CommitLog-1308947888397.log.header
Keyspace Unknown:
	Cf id 1001: 57595560
	Cf id 1: 816960
	Cf id 1000: 0
{noformat}

CF 0 is the Status / LocationInfo CF and 1 is the HintedHandof CF. I dont have it now, but IIRC CFStats showed the LocationInfo CF with dirty ops. 

I was able to repo a case where flushing the CF's did not mark the log segments as obsolete (attached unit-test patch). Steps are:

1. Write to cf1 and flush.
2. Current log segment is marked as dirty at the CL position when the flush started, CommitLog.discardCompletedSegmentsInternal()
3. Do not write to cf1 again.
4. Roll the log, my test does this manually. 
5. Write to CF2 and flush.
6. Only CF2 is flushed because it is the only dirty CF. cfs.maybeSwitchMemtable() is not called for cf1 and so log segment 1 is still marked as dirty from cf1.

Step 5 is not essential, just matched what I thought was happening. I thought SystemTable.updateToken() was called which does not flush, and this was the last thing that happened.  

The expired memtable thread created by Table uses the same cfs.forceFlush() which is a no-op if the cf or it's secondary indexes are clean. 
    
I think the same problem would exist in 0.8. "
CASSANDRA-2823,NPE during range slices with rowrepairs,"Doing some heavy testing of relatively fast feeding (5000+ mutations/sec) + repair on all node + range slices.
Then occasionally killing a node here and there and restarting it.

Triggers the following NPE
 ERROR [pool-2-thread-3] 2011-06-24 20:56:27,289 Cassandra.java (line 3210) Internal error processing get_range_slices
java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.maybeScheduleRepairs(RowRepairResolver.java:109)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:112)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:83)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:161)
	at org.apache.cassandra.utils.MergeIterator.computeNext(MergeIterator.java:88)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:120)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:43)

Looking at the code in getReduced:

{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
{noformat}
seems like resolved becomes null when this happens and versions.size is larger than 1.

RowRepairResolver.resolveSuperset() does actually return null if it cannot resolve anything, so there is definately a case here which can occur and is not handled.

It may also be an interesting question if it is guaranteed that                
versions.add(current.left.cf);
can never return null?

Jonathan suggested on IRC that maybe 
{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
                if (resolved == null)
                      return new Row(key, resolved);
{noformat}

could be a fix.
"
CASSANDRA-2817,Expose number of threads blocked on submitting a memtable for flush,"Writes can be blocked by a thread trying to submit a memtable while the flush queue is full. While this is the expected behavior (the goal being to prevent OOMing), it is worth exposing when that happens so that people can monitor it and modify settings accordingly if that happens too often."
CASSANDRA-2811,Repair doesn't stagger flushes,"When you do a nodetool repair (with no options), the following things occured:
* For each keyspace, a call to SS.forceTableRepair is issued
* In each of those calls: for each token range the node is responsible for, a repair session is created and started
* Each of these session will request one merkle tree by column family (to each node for which it makes sense, which includes the node the repair is started on)

All those merkle tree requests are done basically at the same time. And now that compaction is multi-threaded, this means that usually more than one validation compaction will be started at the same time. The problem is that a validation compaction starts by a flush. Given that by default the flush_queue_size is 4 and the number of compaction thread is the number of processors and given that on any recent machine the number of core will be >= 4, this means that this will easily end up blocking write for some period of time.

It turns out to also have a more subtle problem for repair itself. If two validation compaction for the same column family (but different range) are started in a very short time interval, the first validation will block on the flush, but the second one may not block at all if the memtable is clean when it request it's own flush. In which case that second validation will be executed on data older than it should.

I think the simpler fix is to make sure we only ever do one validation compaction at a time. It's probably a better use of resources anyway. "
CASSANDRA-2789,NPE on nodetool -h localhost -p 7199 info,"We use to running ""nodetool -h localhost"" as it is easy to create alias on the images (which are ok to break)... but this defnetly breaks once broadcast patch is committed (CASSANDRA-2491)... JMX will not be able to listern using the BA (NAT)....

Stack Trace:
[ajami_mr_cassandratest@ajami_mr_cassandra--useast1a-i-e985c587 ~]$ /apps/nfcassandra_server/bin/nodetool -h localhost -p 7501 info
85070591730234615865843651857942052863
Gossip active    : true
Load             : 13.41 KB
Generation No    : 1308310669
Uptime (seconds) : 17420
Heap Memory (MB) : 272.52 / 12083.25
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:93)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.EndpointSnitchInfo.getDatacenter(EndpointSnitchInfo.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)"
CASSANDRA-2760,NPE in sstable2json,"./sstable2json /var/lib/cassandra/data/test/snapshots/1307649033076/User-g-4-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.<init>(ColumnFamily.java:82)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:70)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:142)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:90)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:74)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:313)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:344)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:357)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:415)
"
CASSANDRA-2747,memtable flush during index build causes AssertionError,"Noticed when loading a lot of rows and then creating secondary indexes using update CF via the CLI. 

{code:java}
ERROR 18:56:25,008 Fatal exception in thread Thread[FlushWriter:3,5,main]
java.lang.AssertionError
        at org.apache.cassandra.io.sstable.SSTable.<init>(SSTable.java:91)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:71)
        at org.apache.cassandra.db.ColumnFamilyStore.createFlushWriter(ColumnFamilyStore.java:2124)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:246)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

Table.IndexBuilder.build() calls cfs.maybeSwitchMemtable() with writeCommitLog false. So a null ReplayPosition is eventually passed to Memtable.writeSortedContents(). 

SSTableRead.open() checks Descriptor.hasReplayPosition() and it looks like any 0.8 stats file should have a ReplayPosition. 

Looks like cfs.maybeSwitchMemtable() should use ReplayPosition.NONE rather than null. Patch looks easy, will also try to write a test."
CASSANDRA-2733,"nodetool ring with EC2Snitch, NPE checking for the zone and dc","Existing EC2Snitch... compare is done via == instead of equals() while comparing the IP's... 
(endpoint == FBUtilities.getLocalAddress())
It is ok to compare the Object Address as most of the code uses FBU.getLocalAddress() and it returns the same object everywhere... but it breaks nodetool ring."
CASSANDRA-2718,NPE in SSTableWriter when no ReplayPosition availible,"The following NPE occurs when durable_writes is set to false

{noformat}
ERROR 09:20:30,378 Fatal exception in thread Thread[FlushWriter:11,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.serialize(ReplayPosition.java:127)
	at org.apache.cassandra.io.sstable.SSTableWriter.writeMetadata(SSTableWriter.java:209)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:187)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:173)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}"
CASSANDRA-2713,Null strategy_options on a KsDef leads to an NPE.,"For add/update keyspace, a KsDef with null strategy_options will cause an NPE."
CASSANDRA-2708,memory leak in CompactionManager's estimatedCompactions,"CompactionManager's estimatedCompactions map seems to hold all or most ColumnFamilyStores in the system as keys.  Keys are never removed from estimatedCompactions.

I have a project that embeds Cassandra as a storage backend.  Some of my integration tests create and drop a single keyspace and pair of column families a hundred or 150 times in one JVM.  These tests always OOM'd.  Loading some near-death heapdumps in mat suggested CompactionManager's estimatedCompactions held over 80% of total heap via its ColumnFamilyStore keys.  estimatedCompactions had the only inbound reference to these CFSs, and the CFSs themselves had invalid = true.

As a workaround, I changed estimatedCompactions to a WeakReference-keyed map (using Guava MapMaker).  My integration tests no longer OOM.

I'm generally unfamiliar with Cassandra's guts.  I don't know whether weak referencing the keys of estimatedCompactions is correct (or ideal).  But, that did seem to confirm my guess that retained references to dead CFSs in estimatedCompactions were swamping my heap after lots of Keyspace+ColumnFamily drops."
CASSANDRA-2687,generate-eclipse-files ant target throws StackOverflowError in eclipse,
CASSANDRA-2685,NPE in Table.createReplicationStrategy during sends from HintedHandOffManager,"After about 800k inserts in a column family with RF=1, I get this exception:

{code}
ERROR [HintedHandoff:2] 2011-05-20 18:38:25,089 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[HintedHandoff:2,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:266)
	at org.apache.cassandra.db.Table.<init>(Table.java:212)
	at org.apache.cassandra.db.Table.open(Table.java:106)
	at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:409)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}"
CASSANDRA-2654,Work around native heap leak in sun.nio.ch.Util affecting IncomingTcpConnection,"NIO's leaky, per-thread caching of direct buffers in combination with IncomingTcpConnection's eager buffering of messages leads to leakage of large amounts of native heap. Details in [1]. More on the root cause in [2]. Even though it doesn't fix the leak, attached patch has been found to alleviate the problem by keeping the size of each direct buffer modest.

"
CASSANDRA-2626,stack overflow while compacting,"This is a trunk build from May 3.

After adding  CASSANDRA-2401, I have gotten the following on several nodes.
I am not 100% sure right now if it is related to 2401 but it may seem likely.

Unfortunately, as often is the case with stack overflows, I don't see the start of the stack

ERROR [CompactionExecutor:17] 2011-05-09 07:56:32,479 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:17,1,main]
java.lang.StackOverflowError
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
"
CASSANDRA-2618,DynamicSnitch race in adding latencies,"ERROR 15:33:48,614 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR 15:33:48,615 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more

What is happening that AdaptiveLatencyTracker.add is trying to add a latency, but the deque is full, so it makes a second effort to remove an entry from the deque and then try to add again.  However, when it tries to remove, the deque has already been emptied by DES.reset call clear() on all the ALTs.  This bug has existed for a long time, but it's very rare and difficult to trigger."
CASSANDRA-2552,ReadResponseResolver Race,"When receiving a response, ReadResponseResolver uses a 3 step process to decide whether to trigger the condition that enough responses have arrived:
# Add new response
# Check response set size
# Check that data is present

I think that these steps must have been reordered by the compiler in some cases, because I was able to reproduce a case for a QUORUM read where the condition is not properly triggered:
{noformat}
INFO [RequestResponseStage:15] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=false in 2 messages
INFO [RequestResponseStage:8] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=true in 1 messages
INFO [pool-1-thread-54] 2011-04-25 00:27:03,516 StorageProxy.java (line 623) Read timeout: java.util.concurrent.TimeoutException: ReadResponseResolver@1087367065(/10.34.131.109=false,/10.34.132.122=true,)
{noformat}
The last line shows that both results were present, and that one of them was holding data."
CASSANDRA-2538,CQL: NPE running SELECT with an IN clause,"*Test Case to Run*
{noformat}
cqlsh> select * from users where key in ('user2', 'user3');
Internal application error
{noformat}


*Test Setup*
{noformat}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar);

INSERT INTO users (KEY, password) VALUES ('user1', 'ch@ngem3a');
{noformat}


*Log Files*
{noformat}
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [pool-2-thread-5] 2011-04-21 23:37:12,026 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
	at org.apache.cassandra.cql.WhereClause.and(WhereClause.java:59)
	at org.apache.cassandra.cql.WhereClause.<init>(WhereClause.java:44)
	at org.apache.cassandra.cql.CqlParser.whereClause(CqlParser.java:816)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:502)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:191)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:834)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:463)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1134)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}"
CASSANDRA-2528,NPE from PrecompactedRow,"received a NPE from trunk (0.8) on PrecompactedRow:

ERROR [CompactionExecutor:2] 2011-04-21 17:21:31,610 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:86)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


size of data in /var/lib/cassandra is 11G on this, but there is also report that 1.7G also see the same.

data was previously populated from 0.7.4 cassandra

added debug logging, not sure how much this help (this is logged before the exception.)

 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,588 CompactionManager.java (line 534) Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-7-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-6-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-9-Data.db')]
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-10-Data.db   : 256
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-7-Data.db   : 512
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-6-Data.db   : 768
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-8-Data.db   : 1024
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-9-Data.db   : 1280
 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,609 CompactionIterator.java (line 185) Major@1181554512(cfs, inode.path, 523/10895) now compacting at 16777 bytes/ms.
"
CASSANDRA-2463,Flush and Compaction Unnecessarily Allocate 256MB Contiguous Buffers,"Currently, Cassandra 0.7.x allocates a 256MB contiguous byte array at the beginning of a memtable flush or compaction (presently hard-coded as Config.in_memory_compaction_limit_in_mb). When several memtable flushes are triggered at once (as by `nodetool flush` or `nodetool snapshot`), the tenured generation will typically experience extreme pressure as it attempts to locate [n] contiguous 256mb chunks of heap to allocate. This will often trigger a promotion failure, resulting in a stop-the-world GC until the allocation can be made. (Note that in the case of the ""release valve"" being triggered, the problem is even further exacerbated; the release valve will ironically trigger two contiguous 256MB allocations when attempting to flush the two largest memtables).

This patch sets the buffer to be used by BufferedRandomAccessFile to Math.min(bytesToWrite, BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE) rather than a hard-coded 256MB. The typical resulting buffer size is 64kb.

I've taken some time to measure the impact of this change on the base 0.7.4 release and with this patch applied. This test involved launching Cassandra, performing four million writes across three column families from three clients, and monitoring heap usage and garbage collections. Cassandra was launched with 2GB of heap and the default JVM options shipped with the project. This configuration has 7 column families with a total of 15GB of data.

Here's the base 0.7.4 release:
http://cl.ly/413g2K06121z252e2t10

Note that on launch, we see a flush + compaction triggered almost immediately, resulting in at least 7x very quick 256MB allocations maxing out the heap, resulting in a promotion failure and a full GC. As flushes proceeed, we see that most of these have a corresponding CMS, consistent with the pattern of a large allocation and immediate collection. We see a second promotion failure and full GC at the 75% mark as the allocations cannot be satisfied without a collection, along with several CMSs in between. In the failure cases, the allocation requests occur so quickly that a standard CMS phase cannot completed before a ParNew attempts to promote the surviving byte array into the tenured generation. The heap usage and GC profile of this graph is very unhealthy.

Here's the 0.7.4 release with this patch applied:
http://cl.ly/050I1g26401B1X0w3s1f

This graph is very different. At launch, rather than a immediate spike to full allocation and a promotion failure, we see a slow allocation slope reaching only 1/8th of total heap size. As writes begin, we see several flushes and compactions, but none result in immediate, large allocations. The ParNew collector keeps up with collections far more ably, resulting in only one healthy CMS collection with no promotion failure. Unlike the unhealthy rapid allocation and massive collection pattern we see in the first graph, this graph depicts a healthy sawtooth pattern of ParNews and an occasional effective CMS with no danger of heap fragmentation resulting in a promotion failure.

The bottom line is that there's no need to allocate a hard-coded 256MB write buffer for flushing memtables and compactions to disk. Doing so results in unhealthy rapid allocation patterns and increases the probability of triggering promotion failures and full stop-the-world GCs which can cause nodes to become unresponsive and shunned from the ring during flushes and compactions."
CASSANDRA-2454,Possible deadlock for counter mutations,"{{StorageProxy.applyCounterMutation}} is executed on the mutation stage, but it also submits tasks to the mutation stage, and then blocks for them. If there are more than a few concurrent mutations, this can lead to deadlock."
CASSANDRA-2408,Faulty memory causes adjacent nodes to have invalid data and fail compactation (java.io.IOException: Keys must be written in ascending order.),"Hi,

We had to replace a node with faulty ram. Besides the cassandra cluster getting unresponsive, we also observerd a ""keys must be written in ascending order"" exception on all adjacent quorum nodes, causing their compactation to fail.
We had another node with faulty ram a week earlier, and it was causing the same errors on it's neighbours.

A faulty node shouldn't affect the key ordering of adjacent nodes?

 INFO [CompactionExecutor:1] 2011-03-31 09:57:29,529 CompactionManager.java (line 396) Compacting [SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1793-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1798-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1803-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1808-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1832-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1841-Data.db'),SSTableReader(path='/cassandra/data/table_lists/table_lists-f-1852-Data.db')]
 INFO [CompactionExecutor:1] 2011-03-31 09:58:26,437 SSTableWriter.java (line 108) Last written key : DecoratedKey(ba578bed5e75a06d1156dabccc68abd5_www.abc.com_hostinlinks_9223370735413988125_870cc25d-7e4f-437b-aa83-a311dfbf7f88, 62613537386265643565373561303664313135366461626363633638616264355f7777772e6b6f7475736f7a6c756b2e636f6d5f686f7374696e6c696e6b735f393232333337303733353431333938383132355f38373063633235642d376534662d343337622d616138332d613331316466626637663838)
 INFO [CompactionExecutor:1] 2011-03-31 09:58:26,515 SSTableWriter.java (line 109) Current key : DecoratedKey(ba578bed5e75a06d1156dabccc68abd5_www.abc.com_hostinlinks_9223370735413988136_17070303-ab24-4207-83bd-1604e2c159e4, 62613537386265643565373561303664313135366461626363633638616264355f7777772e6b677475736f7a6c756b2e636f6d5f686f7374696e6c696e6b735f393232333337303733353431333938383133365f31373037303330332d616232342d343230372d383362642d313630346532633135396534)
 INFO [CompactionExecutor:1] 2011-03-31 09:58:26,515 SSTableWriter.java (line 110) Writing into file /cassandra/data/table_lists/table_lists-tmp-f-1861-Data.db
ERROR [CompactionExecutor:1] 2011-03-31 09:58:26,516 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOException: Keys must be written in ascending order.
      at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
      at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
      at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:452)
      at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
      at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
      at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
      at java.util.concurrent.FutureTask.run(FutureTask.java:138)
      at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
      at java.lang.Thread.run(Thread.java:662)
"
CASSANDRA-2404,if out of disk space reclaim compacted SSTables during memtable flush,"During compaction if there is not enough disk space we invoke GC to reclaim unused space.

During memtable and binary memtable flush we just error out if there is not enough disk space to flush the table. 

Can we make cfs.createFlushWriter() use the same logic as Table.getDataFileLocation() to reclaim space if needed?"
CASSANDRA-2381,orphaned data files may be created during migration race,"We try to prevent creating orphans by locking Table.flusherLock in maybeSwitchMemtable and the Migration process, but since the actual writing is done asynchronously in Memtable.writeSortedContents there is a race window, where we acquire lock in maybeSwitch, we're not dropped so we queue the flush and release the lock, Migration does the drop, then Memtable writes itself out."
CASSANDRA-2377,NPE During Repair In StreamReplyVerbHandler,"ERROR [MiscStage:4] 2011-03-24 02:45:05,172 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutorjava.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
ERROR [MiscStage:4] 2011-03-24 02:45:05,172 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[MiscStage:4,5,main]java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)"
CASSANDRA-2353,JMX call StorageService.Operations.getNaturalEndpoints returns an NPE,The JMX operation StorageService.Operations.getNaturalEndpoints in cassandra.db always returns an NPE.
CASSANDRA-2350,Races between schema changes and StorageService operations,"I only tested this on 0.7.0, but it judging by the 0.7.3 code (latest I've looked at) the same thing should happen.

The case in particular that I ran into is this: I force a compaction for all CFs in a keyspace, and while the compaction is happening I add another CF to the keyspace. I get the following exception because the underlying set of CFs has changed while being iterated over.

{noformat}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
        at java.util.HashMap$ValueIterator.next(Unknown Source)
        at java.util.Collections$UnmodifiableCollection$1.next(Unknown Source)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source) 
{noformat}

The problem is a little more fundamental than that, though, as I believe any schema change of CFs in the keyspace during one of these operations (e.g. flush, compaction, etc) has the potential to cause a race. I'm not sure what would happen if the set of CFs to compact was acquired and one of them was dropped before it had been compacted."
CASSANDRA-2330,Queue indexes for flush before the parent,"Secondary indexes flush when the parent does.  This has an unfortunate side effect: a single CF flushing with a single secondary index fills the flush queue and blocks further writes until the first one completes.  A simple but naive optimization here would be to queue the indexes before the parent since they are generally going to be smaller, and thus flush faster, reducing the amount of time writes are blocked."
CASSANDRA-2327,Table.flusherLock is static final.. remove static.,"I see read and write latency spike when the system tables are flushing(according to opscentral).... 

Only reason which i can come-up with is probably because of the Table.flusherLock is static final... i think it should not be static because the flush is per keyspace and this lock will lock all the read operations because one table is going through a flush."
CASSANDRA-2317,Column family deletion time is not always reseted after gc_grace,"Follow up of CASSANDRA-2305.
Reproducible (thanks to Jeffrey Wang) by: 

Create a CF with gc_grace_seconds = 0 and no row cache.
Insert row X, col A with timestamp 0.
Insert row X, col B with timestamp 2.
Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
Wait 1 second.
Force flush and compaction.
Insert row X, col A with timestamp 0.
Read row X, col A (see nothing)."
CASSANDRA-2313,CommutativeRowIndexer always read full row in memory,"CommutativeRowIndexer use CFSerializer.deserializeColumns() that read the full row in memory. We should use PreCompactedRow/LazilyCompactedRow instead to avoid this on huge row.

As an added benefit, using PreCompactedRow will avoid a current seek back to write the row size."
CASSANDRA-2305,Tombstoned rows not purged from cache after gcgraceseconds,"From email to list:

I was wondering if this is the expected behavior of deletes (0.7.0). Let's say I have a 1-node cluster with a single CF which has gc_grace_seconds = 0. The following sequence of operations happens (in the given order):

insert row X with timestamp T
delete row X with timestamp T+1
force flush + compaction
insert row X with timestamp T

My understanding is that the tombstone created by the delete (and row X) will disappear with the flush + compaction which means the last insertion should show up. My experimentation, however, suggests otherwise (the last insertion does not show up).

I believe I have traced this to the fact that the markedForDeleteAt field on the ColumnFamily does not get reset after a compaction (after gc_grace_seconds has passed); is this desirable? I think it introduces an inconsistency in how tombstoned columns work versus tombstoned CFs. Thanks."
CASSANDRA-2297,UnsupportedOperationException: Overflow in bytesPastMark(..),"I hit the following exception on a row that was more than 60GB.  
The row has column families of super column type.

This problem is discussed by the following thread.  
http://www.mail-archive.com/dev@cassandra.apache.org/msg01881.html

{code}
ERROR [HintedHandoff:1] 2011-02-26 18:49:35,708 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.bytesPastMark(BufferedRandomAccessFile.java:477)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:120)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:1)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1290)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$1(HintedHandOffManager.java:262)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}"
CASSANDRA-2295,memory pressure flusher should include secondary index CFs,
CASSANDRA-2294,secondary index CFs should use parent CF flush thresholds,
CASSANDRA-2289,Replicate on write NPE for empty row,Replicate on write will throw a NPE for the first write to a row.
CASSANDRA-2276,Pig memory issues with default LIMIT and large rows.,"Rows with a lot of columns, especially super-colums with a lot of values can cause OutOfMemory errors in Cassandra when queried with Pig."
CASSANDRA-2273,Possible Memory leak,"I have a few problematic nodes in my cluster that will crash OutOfMemory very often. This is Cassandra 0.7.3 downloaded from Hudson.

Heap size is 6GB, server memory is 8GB.
Memtable are flushed at 64MB, I have 5 CFs.
FlushLargestMemtablesAt is set at 0.8 but doesn't help with this issue.

I will attach a screenshot showing my issue. There is no compaction going on when the heap usage start increasing like crazy.

It could be a configuration issue but it kinda looks like a bug to me.
"
CASSANDRA-2270,nodetool info NPE when node isn't fully booted,"Running ""nodetool -h 127.0.0.1 info"" when the node is not yet ready throw a NPE.

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.gms.Gossiper.getCurrentGenerationNumber(Gossiper.java:313)
        at org.apache.cassandra.service.StorageService.getCurrentGenerationNumber(StorageService.java:1239)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
"
CASSANDRA-2228,Race conditions when reinitialisating nodes (OOM + Nullpointer),"I had a corrupt system table which wouldn't compact anymore and I deleted the files and restarted cassandra and let it take the same token/ip address.

I experienced the same errors when I'm adding a newly installed node under the same token/ip address before calling repair.

1)
After a few seconds/minutes, I get a OOM error:


 INFO [FlushWriter:1] 2011-02-23 16:40:28,958 Memtable.java (line 164) Completed flushing /cassandra/data/system/Schema-f-15-Data.db (8037 bytes)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 Migration.java (line 133) Applying migration 3e30e76b-1e3f-11e0-8369-5a9c1faed4ae Add keyspace: table_userentriesrep factor:3rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@58925d9[cfId=1024,tableName=table_userentries,cfName=table_userentries,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}], org.apache.cassandra.config.CFMetaData@11ab7246[cfId=1025,tableName=table_userentries,cfName=table_userentries_meta,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}]}
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Migrations at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-23 16:40:28,966 Memtable.java (line 157) Writing Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Schema at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,967 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Schema@139610466(8370 bytes, 15 operations)
 INFO [ScheduledTasks:1] 2011-02-23 16:40:28,972 StatusLogger.java (line 89) table_sourcedetection.table_sourcedetection                 0,0                 0/0            0/200000
ERROR [FlushWriter:1] 2011-02-23 16:41:01,240 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:75)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:158)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:51)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:176)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)





2) If I restart then, I'm getting an Nullpointer exception. The OOM error will only appear once.

ERROR [main] 2011-02-23 16:42:32,782 AbstractCassandraDaemon.java (line 333) Exception encountered during startup.
java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:925)
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:105)
        at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:161)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:185)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)


Killing and restarting the node multiple times will eventually ""fix"" these errors.


Steps to reproduce. Remove complete data directory and restart node with same token/ip.

"
CASSANDRA-2203,Class to measure real memory allocated for an object,We wanted to share a class we are using internally to measure actual object sizes for Hotspot VM. We've been using this on RowCache and Memtables.
CASSANDRA-2189,json2sstable fails due to OutOfMemory,"I have a json file created with sstable2json for a column family of super column type. Its size is about 1.9GB. (It's a dump of all keys because I cannot find out how to specify keys to dump in sstable2json.)
When I tried to create sstable from the json file, it failed with OutOfMemoryError as follows.

 WARN 00:31:58,595 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space
        at java.lang.String.intern(Native Method)
        at org.codehaus.jackson.util.InternCache.intern(InternCache.java:40)
        at org.codehaus.jackson.sym.BytesToNameCanonicalizer.addName(BytesToNameCanonicalizer.java:471)
        at org.codehaus.jackson.impl.Utf8StreamParser.addName(Utf8StreamParser.java:893)
        at org.codehaus.jackson.impl.Utf8StreamParser.findName(Utf8StreamParser.java:773)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseLongFieldName(Utf8StreamParser.java:379)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseMediumFieldName(Utf8StreamParser.java:347)
        at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:304)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:140)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)

So, what I had to is that split the json file with ""split"" command and modify them to be correct json file. Create sstable for each small json files.

Could you change json2sstable to avoid OutOfMemory?"
CASSANDRA-2183,memtable_flush_after_mins setting not working,"We have observed the behavior that memtable_flush_after_mins setting not working occasionally.   After some testing and code digging, we finally figured out what going on.
The memtable_flush_after_mins won't work on certain condition with current implementation in Cassandra.

In org.apache.cassandra.db.Table,  the scheduled flush task is setup by the following code during construction.

------------------------------------------------------------------------------------------------------------------
int minCheckMs = Integer.MAX_VALUE;
       
for (ColumnFamilyStore cfs : columnFamilyStores.values())  
{
    minCheckMs = Math.min(minCheckMs, cfs.getMemtableFlushAfterMins() * 60 * 1000);
}

Runnable runnable = new Runnable()
{
   public void run()
   {
       for (ColumnFamilyStore cfs : columnFamilyStores.values())
       {
           cfs.forceFlushIfExpired();
       }
   }
};
flushTask = StorageService.scheduledTasks.scheduleWithFixedDelay(runnable, minCheckMs, minCheckMs, TimeUnit.MILLISECONDS);
------------------------------------------------------------------------------------------------------------------------------

Now for our application, we will create a keyspacewithout without any columnfamily first.  And only add needed columnfamily later depends on request.

However, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with Integer.MAX_VALUE ms
since there is no columnfamily yet.

Later when you add columnfamily to this empty keyspace, the initCf() method in Table.java doesn't check whether the scheduled flush check task interval need
to be updated or not.   To fix this, we'd need to restart the Cassandra after columnfamily added into the keyspace. 

I would suggest that add additional logic in initCf() method to recreate a scheduled flush check task if needed.
"
CASSANDRA-2178,Memtable Flush writers doesn't actually flush in parallel,"The flushWriter JMXEnabledThreadPoolExecutor sets the core pool min to 1, and sets the LBQ to DatabaseDescriptor.getFlushWriters(). Increasing memtable_flush_writers should allow us to flush more in parallel. The pool will not grow until LBQ fills up to DatabaseDescriptor.getFlushWriters(). "
CASSANDRA-2175,make key cache preheating use less memory,"CASSANDRA-1878 pre-heats the key cache post-compaction so latency doesn't suffer while warming the cache back up.  This can double the memory used temporarily; for a large key cache, this can have a substantial impact.

For now a boolean on/off is probably the best we can do.  With http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=21 though, we could say ""preheat the hottest X keys."""
CASSANDRA-2171,Record and expose flush rate per CF,"In order to automatically throttle compaction to some multiple of the flush rate, we need to record the flush rate across the system. Since this might be useful information on a per CF basis, this ticket will deal with recording the flush rate in the CFStore object, and exposing it via JMX."
CASSANDRA-2169,user created with debian packaging is unable to increase memlock,"To reproduce:
- Install a fresh copy of ubuntu 10.04.
- Install sun's java6 jdk.
- Install libjna-java 3.2.7 into /usr/share/java.
- Install cassandra 0.7.0 from the apache debian packages.
- Start cassandra using /etc/init.d/cassandra
In the output.log there will be the following error:
{quote}
Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
{quote}
This shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'.

I tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf.  I was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but I could never get the init.d script to work and get beyond that error.

Based on all the trial and error, I think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'."
CASSANDRA-2158,memtable_throughput_in_mb can not support sizes over 2.2 gigs because of an integer overflow.,"If memtable_throughput_in_mb is set past 2.2 gigs, no errors are thrown.  However, as soon as data starts being written it is almost immediately being flushed.  Several hundred SSTables are created in minutes.  I am almost positive that the problem is that when memtable_throughput_in_mb is being converted into bytes the result is stored in an integer, which is overflowing.

From memtable.java:

    private final int THRESHOLD;
    private final int THRESHOLD_COUNT;

...
this.THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;
this.THRESHOLD_COUNT = (int) (cfs.getMemtableOperationsInMillions() * 1024 * 1024);


NOTE:
I also think currentThroughput also needs to be changed from an int to a long.  I'm not sure if it is as simple as this or if this also is used in other places."
CASSANDRA-2144,Don't slurp rows into memory during export,"SSTableExport uses SSTableIdentityIterator.getColumnFamilyWithColumns, which reads a full row into memory."
CASSANDRA-2142,"Add ""reduce memory usage because I tuned things poorly"" feature","Users frequently create too many columnfamilies, set the memtable thresholds too high (or adjust throughput while ignoring operations), and/or set caching thresholds too high.  Then their server OOMs and they tell their friends Cassandra sucks."
CASSANDRA-2105,Fix the read race condition in CFStore for counters ,"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
"
CASSANDRA-2087,Keep in-memory list of uncompactable sstables,"Rather than retrying compactions that we know will fail we should:
{quote}stop trying to compact that file and log what file the error occurred for. The list of corrupt sstables does not even have to be persistent, just an in memory list which gets wiped out on a restart.{quote}"
CASSANDRA-2083,Hinted Handoff and schema race,"If a node is down while a keyspace/cf is created and then data is inserted into the CF causing other nodes to hint, when the down node recovers it will lose some hints until the schema propagates:

{noformat}
ERROR 19:59:28,264 Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1000
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:117)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:377)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 19:59:28,356 Applying migration 28e2e7a4-2d74-11e0-9b6b-cdc89135952c
{noformat}"
CASSANDRA-2072,Race condition during decommission,"Occasionally when decommissioning a node, there is a race condition that occurs where another node will never remove the token and thus propagate it again with a state of down.  With CASSANDRA-1900 we can solve this, but it shouldn't occur in the first place.

Given nodes A, B, and C, if you decommission B it will stream to A and C.  When complete, B will decommission and receive this stacktrace:

ERROR 00:02:40,282 Fatal exception in thread Thread[Thread-5,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91

At this point A will show it is removing B's token, but C will not and instead its failure detector will report that B is dead, and nodetool ring on C shows B in a leaving/down state.  In another gossip round, C will propagate this state back to A."
CASSANDRA-2058,Load spikes due to MessagingService-generated garbage collection,"(Filing as a placeholder bug as I gather information.)

At ~10p 24 Jan, I upgraded our 20-node cluster from 0.6.8->0.6.10, turned on the DES, and moved some CFs from one KS into another (drain whole cluster, take it down, move files, change schema, put it back up). Since then, I've had four storms whereby a node's load will shoot to 700+ (400% CPU on a 4-cpu machine) and become totally unresponsive. After a moment or two like that, its neighbour dies too, and the failure cascades around the ring. Unfortunately because of the high load I'm not able to get into the machine to pull a thread dump to see wtf it's doing as it happens.

I've also had an issue where a single node spikes up to high load, but recovers. This may or may not be the same issue from which the nodes don't recover as above, but both are new behaviour"
CASSANDRA-2057,overflow in NodeCmd,We aggregate the long read/write counts across CFs into an int.
CASSANDRA-2044,CLI should loop on describe_schema until agreement or fatel exit with stacktrace/message if no agreement after X seconds,"see CASSANDRA-2026 for brief background.

It's easy to enter statements into the CLI before the schema has settled, often causing problems where it is no longer possible to get the nodes in agreement about the schema without removing the system directory.

The alleviate the most common problems with this, the CLI should issue the modification statement and loop on describe_schema until all nodes agree or until X seconds has passed.  If the timeout has been exceeded, the CLI should exit with an error and inform the user that the schema has not settled and further migrations are ill-advised until it does.

number_of_nodes/2+1 seconds seems like a decent wait time for schema migrations to start with.

Bonus points for making the value configurable."
CASSANDRA-2041,add paging of large rows to sstable2json,
CASSANDRA-2037,Unsafe Multimap Access in MessagingService,"MessagingSerice is a system singleton with a static Multimap field targets. Multimaps are not thread safe but no attempt is made to synchronize access to that field. Multimap ultimately uses the standard java HashMap which is susceptible to a race condition where threads will get stuck during a get operation yielding multiple threads similar to the following stack:

""pool-1-thread-6451"" prio=10 tid=0x00007fa5242c9000 nid=0x10f4 runnable [0x00007fa52fde4000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.get(HashMap.java:303)
	at com.google.common.collect.AbstractMultimap.getOrCreateCollection(AbstractMultimap.java:205)
	at com.google.common.collect.AbstractMultimap.put(AbstractMultimap.java:194)
	at com.google.common.collect.AbstractListMultimap.put(AbstractListMultimap.java:72)
	at com.google.common.collect.ArrayListMultimap.put(ArrayListMultimap.java:60)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:303)
	at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:353)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:229)
	at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:98)
	at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:289)
	at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2655)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)"
CASSANDRA-1991,"CFS.maybeSwitchMemtable() calls CommitLog.instance.getContext(), which may block, under flusher lock write lock","While investigate CASSANDRA-1955 I realized I was seeing very poor latencies for reasons that had nothing to do with flush_writers, even when using periodic commit log mode (and flush writers set ridiculously high, 500).

It turns out writes blocked were slow because Table.apply() was spending lots of time (I can easily trigger seconds on moderate work-load) trying to acquire a flusher lock read lock (""flush lock millis"" log printout in the logging patch I'll attach).

That in turns is caused by CFS.maybeSwitchMemtable() which acquires the flusher lock write lock.

Bisecting further revealed that the offending line of code that blocked was:

                    final CommitLogSegment.CommitLogContext ctx = writeCommitLog ? CommitLog.instance.getContext() : null;

Indeed, CommitLog.getContext() simply returns currentSegment().getContext(), but does so by submitting a callable on the service executor. So independently of flush writers, this can block all (global, for all cf:s) writes very easily, and does.

I'll attach a file that is an independent Python script that triggers it on my macos laptop (with an intel SSD, which is why I was particularly surprised) (it assumes CPython, out-of-the-box-or-almost Cassandra on localhost that isn't in a cluster, and it will drop/recreate a keyspace called '1955').

I'm also attaching, just FYI, the patch with log entries that I used while tracking it down.

Finally, I'll attach a patch with a suggested solution of keeping track of the latest commit log with an AtomicReference (as an alternative to synchronizing all access to segments). With that patch applied, latencies are not affected by my trigger case like they were before. There are some sub-optimal > 100 ms cases on my test machine, but for other reasons. I'm no longer able to trigger the extremes.

"
CASSANDRA-1982,"When creating snapshots, flush isn't required to finish before the snapshots are created","Right now ColumnFamilyStore.snapshot() works by doing a flush() first, then do symlinks on the sstables.

But flush() is actually submitted through Executor, so by the time the code reaches the symlink part, the flush() may have not happened at all."
CASSANDRA-1967,commit log replay shouldn't end with a flush,"(Apologies in advance if there is some very compelling reason to flush after replay, of which I am not currently aware. ;D)

Currently, when a node restarts, the following sequence occurs :

a) commitlog is replayed
b) any memtables resulting from a) are flushed 
c) a new commitlog is opened, new memtables are switched in
... (other stuff happens)
d) node starts taking traffic

This has side effects, perhaps most seriously the potential of triggering compaction. As a node is likely to struggle performance-wise after restarting, triggering compaction at that time seems like something we might wish to avoid.

I propose that the sequence be :

a) commitlog is replayed
b) a new commitlog is opened, new memtables are switched in 
... (other stuff happens)
c) node starts taking traffic

Looking through the relevant code, the only code that appears to depend on this flush is at src/java/org/apache/cassandra/db/commitlog/CommitLog.java:112 :
""
        // all old segments are recovered and deleted before CommitLog is instantiated.
        // All we need to do is create a new one.
        segments.add(new CommitLogSegment());
""

Presumably this code would have to be refactored to be aware of the currently open commitlog."
CASSANDRA-1955,memtable flushing can block writes due to queue size limitations even though overall write throughput is below capacity,"It seems that someone ran into this (see cassandra-user thread ""Question re: the use of multiple ColumnFamilies"").

If my interpretation is correct, the queue size is set to the concurrency in the case of the flushSorter, and set to memtable_flush_writers in the case of flushWriter in ColumnFamilyStore.

While the choice of concurrency for the two executors makes perfect sense, the queue sizing does not. As a user, I would expect, and did expect, that for a given memtable independently tuned (w.r.t. flushing thresholds etc), writes to the CF would not block until there is at least one other memtable *for that CF* waiting to be flushed.

With the current behavior, if I am not misinterpreting, whether or not writes will inappropriately block is very much dependent on not just the overall write throughput, but also the incidental timing of memtable flushes across multiple column families.

The simplest way to mitigate (but not fix) this is probably to set the queue size to be equal to the number of column families if that is higher than the number of CPU cores. But that is only a mitigation because nothing prevents e.g. a large number of memtable flushes for a small column family under temporary write load, can still block a large (possibly more important) memtable flush for another CF. Such a shared-but-larger queue would also not prevent heap usage spikes resulting from some a single cf with very large memtable thresholds being rapidly written to, with a queue sized for lots of cf:s that are in practice not used. In other words, this mitigation technique would effectively negate the backpressure mechanism in some cases and likely lead to more people having OOM issues when saturating a CF with writes.

A more involved change is to make each CF have it's own queue through which flushes go prior to being submitted to flushSorter, which would guarantee that at least one memtable can always be in pending flush state for a given CF. The global queue could effectively have size 1 hard-coded since the queue is no longer really used as if it were a queue.

The flushWriter is unaffected since it is a separate concern that is supposed to be I/O bound. The current behavior would not be perfect if there is a huge discrepancy between memtable flush thresholds of different memtables, but it does not seem high priority to make a change here in practice.

So, I propose either:

(a) changing the flushSorter queue size to be max(num cores, num cfs)
(b) creating a per-cf queue

I'll volunteer to work on it as a nice bite sized change, assuming there is agreement on what needs to be done. Given the concerns with (a), I think (b) is the right solution unless it turns out to cause major complexity. Worth noting is that these are not performance sensitive given the low frequency of memtable flushes, so an extra queue:ing step should not be an issue.
"
CASSANDRA-1930,db.Table flusherLock write lock fairness policy is sub-optimal,"h4. scenario:
1) high write throughput cluster
2) external services adding material cpu contention

h4. symptoms:
The row mutation stage falls *very* behind.

h4. cause:
ReentrantReadWriteLock's fair policy causes write lock acquisition / release to require a material amount of CPU time.

h4. summary:
When there are other services contending for the CPU, the RRW lock's fair policy causes write lock acquisition / release to take enough time to eventually put threads waiting for read lock acquisition very behind.  We repro'd this scenario by reducing the scope of the write lock to: 1 boolean check, 1 boolean assignment, and 1 db.Memtable instantiation (itself, just: 2 variable assignments) w/ the same performance.  Modifying the fairness policy to be the default policy allowed the row mutation stage to keep up."
CASSANDRA-1925,"""nodetool ring"" to be graceful when getting ring ownerships fails","When using a IPartitioner that doesn't support describeOwnership((List<Token>), for example see ByteOrderedPartitioner, ""nodetool ring"" completely fails.

""nodetool ring"" can instead report this failure via stderr and still print the rest of the output it can out.

This patch provides the following (stdout) output when using a ByteOrderedParitioner

$ ./nodetool ring -h localhost 
Address         Status State   Load            Owns    Token                                       
                                                       Token(bytes[fca66e9d8e6b1ca93340302470d6ed8c])
152.90.242.93   Up     Normal  5.68 GB         ?       Token(bytes[4985c50192edae33cc5b80fa66bae81f])
152.90.242.91   Up     Normal  5.95 GB         ?       Token(bytes[a8a20dff0e2b11e087a718a90540c600])
152.90.242.92   Up     Normal  8.09 GB         ?       Token(bytes[fca66e9d8e6b1ca93340302470d6ed8c])"
CASSANDRA-1919,Add shutdownhook to flush commitlog,this replaces the periodic_with_flush approach from CASSANDRA-1780 / CASSANDRA-1917
CASSANDRA-1912,Cassandra should flush a keyspace after it was dropped,"After dropping a keyspace one must not forget to flush it via
  nodetool -h host flush <dropped keyspace>

If you forget to do this and restart cassandra, the following traceback happens and cassandra doesn't start up again:

 INFO 20:22:56,420 Heap size: 4117889024/4137549824
 INFO 20:22:56,424 JNA not found. Native methods will be disabled.
 INFO 20:22:56,432 Loading settings from file:/home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/conf/cassandra.yaml
 INFO 20:22:56,547 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 20:22:56,612 Creating new commitlog segment /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/commitlog/CommitLog-1293564176612.log
 INFO 20:22:56,666 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/system-IndexInfo-KeyCache
 INFO 20:22:56,673 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/IndexInfo-e-1
 INFO 20:22:56,698 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/system-Schema-KeyCache
 INFO 20:22:56,700 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/Schema-e-41
 INFO 20:22:56,703 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/Schema-e-43
 INFO 20:22:56,705 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/Schema-e-42
 INFO 20:22:56,727 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/system-Migrations-KeyCache
 INFO 20:22:56,728 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/Migrations-e-41
 INFO 20:22:56,730 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/Migrations-e-42
 INFO 20:22:56,734 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/system-LocationInfo-KeyCache
 INFO 20:22:56,735 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/LocationInfo-e-6
 INFO 20:22:56,739 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/LocationInfo-e-7
 INFO 20:22:56,741 Opening /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/data/system/LocationInfo-e-5
 INFO 20:22:56,746 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/system-HintsColumnFamily-KeyCache
 INFO 20:22:56,776 Loading schema version 460e882a-1291-11e0-92ff-e700f669bcfc
 WARN 20:22:56,943 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 20:22:56,954 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/Keyspace1-Indexed1-KeyCache
 INFO 20:22:56,959 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/Keyspace1-Super1-KeyCache
 INFO 20:22:56,960 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/Keyspace1-Standard2-KeyCache
 INFO 20:22:56,961 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/Keyspace1-Super2-KeyCache
 INFO 20:22:56,962 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/Keyspace1-Standard1-KeyCache
 INFO 20:22:56,963 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/Keyspace1-Super3-KeyCache
 INFO 20:22:56,964 reading saved cache /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/saved_caches/Keyspace1-StandardByUUID1-KeyCache
 INFO 20:22:56,971 Replaying /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/commitlog/CommitLog-1293524239077.log, /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/commitlog/CommitLog-1293563866498.log
 INFO 20:22:56,974 Finished reading /home/dk/develop/cassandra/apache-cassandra-0.7.0-rc3/STORAGE/commitlog/CommitLog-1293524239077.log
ERROR 20:22:56,975 Exception encountered during startup.
java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:302)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:193)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:142)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Exception encountered during startup.
java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:302)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:193)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:142)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)



On IRC the only hint was to move the commitlog away, start cassandra, create the keyspace, stop cassandra, move the commitlog back and start it again.
I'm not sure, how stable this would be in a loaded environment, so I propose that cassandra does the flush itself after dropping a keyspace.
"
CASSANDRA-1911,write path should call MessagingService.removeRegisteredCallback,"it would reduce memory overhead to pre-emptively clear the callback when done the way the read path does.

(other IAsyncCallbacks could do this too, but only read/write have enough volume to matter.)"
CASSANDRA-1883,NPE in get_slice quorum read,"Getting this NPE as of the 2010-12-17 0.7 trunk.  Some data may be corrupt somewhere on a node.  It could be a null key somewhere.

ERROR [pool-1-thread-28] 2010-12-18 12:53:20,411 Cassandra.java (line 2707) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.service.DigestMismatchException.<init>(DigestMismatchException.java:30)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:92)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:91)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:362)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:229)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:225)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:301)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:263)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2699)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)"
CASSANDRA-1867,sstable2json runs out of memory when trying to export huge rows,"Currently, sstable2json can run out of memory if it encounters a huge row. The problem is that it creates an in-memory String for each row. Proposed solution is to pass the output PrintStream to the serializeRow() and serializeColumns() methods and write to the stream incrementally."
CASSANDRA-1837,Deleted columns are resurrected after a flush,"Easily reproduced with the cli:

{noformat}
[default@unknown] create keyspace testks;
2785d67c-02df-11e0-ac09-e700f669bcfc
[default@unknown] use testks;
Authenticated to keyspace: testks
[default@testks] create column family testcf;
2fbad20d-02df-11e0-ac09-e700f669bcfc
[default@testks] set testcf['test']['foo'] = 'foo';
Value inserted.
[default@testks] set testcf['test']['bar'] = 'bar';
Value inserted.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
[default@testks] del testcf['test'];
row removed.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test

1 Row Returned.
{noformat}

Now flush testks and look again:

{noformat}

[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
{noformat}"
CASSANDRA-1796,[patch] avoid npes when parsing potentially null strings->Integers,"code potentially performs Integer.parseInt(null) which throws NPEs, patch guards against this, and returns 0 instead."
CASSANDRA-1780,periodic + flush commitlog mode,"periodic-sync commitlog mode only flushes before it syncs, which means its best case durability is very similar to its worst case.  if we had a mode that flushed but did not sync then it would only lose data for actual power failures."
CASSANDRA-1764,NPE on system_update_cf when adding an index to a column without existing metadata,"When trying to create a secondary index using system_update_column_family(), if you try to add an index on a column that does not already have an existing entry in the CfDef's column_metadata, a NullPointerException is thrown.

Looks like the logic in o.a.c.config.CFMetaData.apply() is faulty.  Specifically, creating a toUpdate Set (similar to the toAdd and toDelete) sets and using that for the loop ~ line 663 would fix this."
CASSANDRA-1749,Streaming should hold a reference to the source SSTR to prevent GC races,"An SSTable waiting to be streamed will be GC'd and deleted from disk if there are no references being held to its SSTableReader. While streaming an SSTable, we should hold an SSTR reference."
CASSANDRA-1748,Flush before repair,"We don't currently flush before beginning a validation compaction, meaning that depending on the state of the memtables, we might end up with content on disk that is as different as a single memtable can make it (potentially, very different)."
CASSANDRA-1726,Update debian packaging to use alternatives,We should update the debian packaging to install configuration using alternatives. Additionally we can probably get rid of the custom cassandra.in.sh for debian packaging.
CASSANDRA-1715,More schema migration race conditions,"Related to CASSANDRA-1631.

This is still a bug with schema updates to an existing CF, since reloadCf is doing a unload/init cycle. So flushing + compaction is an issue there as well. Here is a stacktrace from during an index creation where it stubbed its toe on an incomplete sstable from an in-progress compaction (path names anonymized):
{code}
INFO [CompactionExecutor:1] 2010-11-02 16:31:00,553 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-8-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-9-Data.db')]
...
ERROR [MigrationStage:1] 2010-11-02 16:31:10,939 ColumnFamilyStore.java (line 244) Corrupt sstable Standard1-tmp-e-10-<>=[Data.db, Index.db]; skipped
java.io.EOFException
        at org.apache.cassandra.utils.FBUtilities.skipShortByteArray(FBUtilities.java:308)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:231)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:286)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:202)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:443)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:431)
        at org.apache.cassandra.db.Table.initCf(Table.java:335)
        at org.apache.cassandra.db.Table.reloadCf(Table.java:343)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:89)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:158)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
...
 INFO [CompactionExecutor:1] 2010-11-02 16:31:31,970 CompactionManager.java (line 303) Compacted to Standard1-tmp-e-10-Data.db.  213,657,983 to 213,657,983 (~100% of original) bytes for 626,563 keys.  Time: 31,416ms.
{code}

There is also a race between schema modification and streaming."
CASSANDRA-1673,Add a way to force remove tombstones before GCGraceSeconds,"In some circumstances it might be useful to be able to force delete tombstones before GCGraceSeconds has elapsed.

Example, If you know your cluster is consistent and you want to free up space."
CASSANDRA-1665,JMX threads leak in NodeProbe,There is a JMX threads leak in NodeProbe.  It creates and uses a JMXConnector but never calls its close() method.  I am working on a patch which add a close() method to NodeProbe  that calls JMXConnector.close().
CASSANDRA-1657,support in-memory column families,"Some workloads are such that you absolutely depend on column families being in-memory for performance, yet you most definitely want all the things that Cassandra offers in terms of replication, consistency, durability etc.

In order to semi-deterministically ensure acceptable performance for such data, Cassandra could support in-memory column families. Such an in-memory column family would imply that mlock() be used on sstables for this column family. On start-up and on compaction completion, they could be mmap():ed with MAP_POPULATE (Linux specific) or else just mmap():ed + mlock():ed in such a way as to otherwise guarantee it is in-memory (such as userland traversal of the entire file).
"
CASSANDRA-1640,NPE in StorageService when cluster is first being created,"Saw this exception on the 0.7.0-beta2 version of cassandra right after bringing up a cluster and trying to get the number of live nodes.

java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.stringify(StorageService.java:1151)
        at org.apache.cassandra.service.StorageService.getLiveNodes(StorageService.java:1138)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)


I fixed this by adding some null checks to the stringify methods 

    private Set<String> stringify(Collection<InetAddress> endpoints)
    {
        Set<String> stringEndpoints = new HashSet<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

    private List<String> stringify(List<InetAddress> endpoints)
    {
        List<String> stringEndpoints = new ArrayList<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

After adding those checks, then I got more reasonable/realistic errors from a different part of the code since the service wasn't up yet as the cluster was still initializing:

Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.service:type=StorageService
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:255)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:233)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:878)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:263)
"
CASSANDRA-1639,NPE in StorageService when cluster is first being created,"Saw this exception on the 0.7.0-beta2 version of cassandra right after bringing up a cluster and trying to get the number of live nodes.

java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.stringify(StorageService.java:1151)
        at org.apache.cassandra.service.StorageService.getLiveNodes(StorageService.java:1138)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)


I fixed this by adding some null checks to the stringify methods 

    private Set<String> stringify(Collection<InetAddress> endpoints)
    {
        Set<String> stringEndpoints = new HashSet<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

    private List<String> stringify(List<InetAddress> endpoints)
    {
        List<String> stringEndpoints = new ArrayList<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

After adding those checks, then I got more reasonable/realistic errors from a different part of the code since the service wasn't up yet as the cluster was still initializing:

Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.service:type=StorageService
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:255)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:233)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:878)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:263)
"
CASSANDRA-1631,dropping column families and keyspaces races with compaction and flushing,
CASSANDRA-1630,system_rename_* methods need to be removed until we can solve compaction and flush races.,
CASSANDRA-1549,cassandra-cli craps its pants and dies when 'gc_grace_seconds' is used in cf creation,"{noformat}
trunk$ bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] use Keyspace1
Authenticated to keyspace: Keyspace1
[default@Keyspace1] create column family cfname with gc_grace_seconds=86400
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:817)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:105)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:230)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:302)
{noformat}

it's just a missing ""break;"" statement in CliClient.java."
CASSANDRA-1533,expose MessagingService (OutboundTcpConnection) queue lengths in JMX,let's expose queue length in a jmx object as getPendingTasks.  might also be nice to have a getCompletedTasks like the other executors.
CASSANDRA-1493,AssertionError in MessagingService.receive for READ_REPAIR verb,"Read repair messages are causing an assertion error in MessagingService. Looks like the enum introduced in CASSANDRA-1465 is missing a verb?

Added two lines of debug output, so lines are a bit off:
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 373) Verb: READ_REPAIR
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 374) MessageType: null
ERROR [pool-1-thread-1] 2010-09-10 15:39:23,555 Cassandra.java (line 1744) Internal error processing get
java.lang.AssertionError
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:376)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:285)
        at org.apache.cassandra.service.ReadResponseResolver.maybeScheduleRepairs(ReadResponseResolver.java:163)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:116)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:89)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:430)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:266)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:113)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:317)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1734)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1634)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
"
CASSANDRA-1477,drop/recreate column family race condition,"using 0.7 latest from trunk as of few minutes ago.  1 client, 1 node

i have the scenario where i want to drop a column family and recreate it 
- unit testing for instance, is a good reason you may want to do this 
(always start fresh).

the problem i observe is that if i do the following:

1 - drop the column family
2 - recreate it
3 - read data from a key that existed before dropping, but doesn't exist now

if those steps happen fast enough, i will get the old row - definitely 
no good.

if they happen slow enough, get_slice throws:

""org.apache.thrift.TApplicationException: Internal error processing 
get_slice""

.. and on the server i see:

2010-09-07 13:53:48,086 ERROR 
[org.apache.cassandra.thrift.Cassandra$Processor] (pool-1-thread-4:) - 
Internal error processing get_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: 
java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:275)
     at 
org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:218)
     at 
org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:114)
     at 
org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:220)
     at 
org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:299)
     at 
org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:260)
     at 
org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2795)
     at 
org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2651)
     at 
org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
     at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: 
java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:271)
     ... 11 more
Caused by: java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
     at 
org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:509)
     at 
org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
     at 
org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:65)
     at 
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:76)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:961)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:856)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:826)
     at org.apache.cassandra.db.Table.getRow(Table.java:321)
     at 
org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
     at 
org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:737)
     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
     ... 3 more
Caused by: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.io.RandomAccessFile.open(Native Method)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
     at 
org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
     ... 15 more

"
CASSANDRA-1463,"Failed bootstrap can cause NPE in batch_mutate on every node, taking down the entire cluster","In adding a node to the cluster, the bootstrap failed (still investigating the cause). An hour later, the entire cluster failed, preventing any writes from being accepted. This exception started being printed to the logs:

{quote}
 INFO [Timer-0] 2010-09-03 12:23:33,282 Gossiper.java (line 402) FatClient /10.251.243.191 has been silent for 3600000ms, removing from gossip
ERROR [Timer-0] 2010-09-03 12:23:33,318 Gossiper.java (line 99) Gossip error
java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1048)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:383)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)
ERROR [pool-1-thread-69153] 2010-09-03 12:23:33,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [pool-1-thread-69154] 2010-09-03 12:23:33,869 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{quote}

After a large number of iterations of that (at least thousands), the printed exception was shortened (this shortening is what made me mistakenly file #1462) to

{quote}
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,883 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,894 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:22,985 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:23,084 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
{quote}

Rolling a restart over the cluster fixed it, but every node had to be restarted before it started accepting writes again."
CASSANDRA-1462,Unexpected exceptions' stacktraces can be lost,"o.a.c.thrift.Cassandra.login.process has a catch that looks like:

        } catch (Throwable th) {
          LOGGER.error(""Internal error processing login"", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, ""Internal error processing login"");
          oprot.writeMessageBegin(new TMessage(""login"", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }

Unfortunately this loses vital debugging information as it loses the stack trace associated with the exception. In particular it made tracking down the source of this very difficult:

ERROR [pool-1-thread-9723] 2010-09-03 12:34:16,978 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-9556] 2010-09-03 12:34:17,031 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-9556] 2010-09-03 12:34:17,129 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-9794] 2010-09-03 12:34:17,190 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-9794] 2010-09-03 12:34:17,245 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-9794] 2010-09-03 12:34:17,285 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-9794] 2010-09-03 12:34:17,392 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException

I'd like to file a bug for that one too, but I have no idea what caused it :)

For reference, my log4j.properties:

log4j.rootLogger=INFO,stdout,R

# stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%5p %d{HH:mm:ss,SSS} %m%n

# rolling log file
log4j.appender.R=org.apache.log4j.RollingFileAppender
log4j.appender.file.maxFileSize=20MB
log4j.appender.file.maxBackupIndex=50
log4j.appender.R.layout=org.apache.log4j.PatternLayout
log4j.appender.R.layout.ConversionPattern=%5p [%t] %d{ISO8601} %F (line %L) %m%n
# Edit the next line to point to your logs directory
log4j.appender.R.File=/cassandra/log/system.log

# Application logging options
#log4j.logger.com.facebook=DEBUG
#log4j.logger.com.facebook.infrastructure.gms=DEBUG
#log4j.logger.com.facebook.infrastructure.db=DEBUG"
CASSANDRA-1450,"Memtable flush causes bad ""reversed"" get_slice","If columns are inserted into a row before and after a memtable flush, a get_slice() after the flush with reversed=True will return incorrect results.  See attached patch to reproduce."
CASSANDRA-1416,SStableSliceIterator leaks FDs,
CASSANDRA-1382,Race condition leads to FileNotFoundException on startup,"On startup LocationInfo file is deleted then attempted to be read from.

Steps to reproduce: Kill then quickly restart

Switching to ParallelGC to avoid CMS/CompressedOops incompatibility
INFO 17:05:08,680 DiskAccessMode isstandard, indexAccessMode is mmap
 INFO 17:05:08,786 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,797 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,807 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,833 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,834 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,839 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,862 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,864 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,876 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,885 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,892 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,893 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,897 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,901 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,906 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,909 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,918 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,922 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,928 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:08,933 Deleted /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db
 INFO 17:05:08,936 Deleted /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db
 INFO 17:05:08,936 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,937 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,947 Deleted /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db
 INFO 17:05:08,947 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,948 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,948 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,950 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,951 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,970 Deleted /var/lib/cassandra/data/system/LocationInfo-e-13-Data.db
 INFO 17:05:08,971 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-14-<>
ERROR 17:05:08,971 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-14-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,972 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,973 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,973 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,974 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,974 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,996 Loading schema version acc5646a-a59d-11df-83fb-e700f669bcfc
 WARN 17:05:09,158 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 17:05:09,164 Replaying /var/lib/cassandra/commitlog/CommitLog-1281571453475.log, /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571453475.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,173 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,183 Enqueuing flush of Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,183 Writing Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,184 switching in a fresh Memtable for Statistics at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,184 Enqueuing flush of Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,265 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-18-Data.db
 INFO 17:05:09,273 Writing Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,352 Completed flushing /var/lib/cassandra/data/system/Statistics-e-1-Data.db
 INFO 17:05:09,353 Recovery complete "
CASSANDRA-1377,NPE aborts streaming operations for keyspaces with hyphens ('-') in their names,"When streaming starts for operations such as repair or bootstrap, it will fail due to an NPE if they rows are in a keyspace that has a hyphen in its name.  One workaround for this issue would be to not use keyspace names containing hyphens.  It would be even nicer if streaming worked for keyspace names with hyphens, since keyspaces named like that seem to be fine in all other ways.

To reproduce:
 1. With a multi-node ring, load up a keyspace with a hyphen in its name
 2. Add some data to that keyspace
 3. nodetool repair

Expected results:
Repair operations complete normally

Actual results:
Repair operations don't complete normally.  The stacktrace below is correlated with the repair request.  

 INFO [AE-SERVICE-STAGE:1] 2010-06-30 14:11:29,744 AntiEntropyService.java (line 619) Performing streaming repair of 1 ranges to /10.255.0.20 for (my-keyspace,AColumnFamily)
ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-06-30 14:11:30,034 DebuggableThreadPoolExecutor.java (line 101) Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.getNewNames(StreamInitiateVerbHandler.java:154)
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.doVerb(StreamInitiateVerbHandler.java:76)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)"
CASSANDRA-1330,AssertionError: discard at CommitLogContext(file=...) is not after last flush at  ...,"Looks related to CASSANDRA-936?

ERROR [MEMTABLE-POST-FLUSHER:1] 2010-07-28 11:39:36,909 CassandraDaemon.java (line 83) Uncaught exception in thread Thread[MEMTABLE-POST-FLUSHER:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:373)
        at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:365)
        ... 8 more
Caused by: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:394)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:359)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
"
CASSANDRA-1301,Table needs to be aware of indexed column CFSes so they can be flushed correctly,
CASSANDRA-1298,avoid replaying fully-flushed commitlog segments,
CASSANDRA-1276,GCGraceSeconds per ColumnFamily,"From: Jonathan Ellis [jbellis@gmail.com]
Received: 7/12/10 9:15 PM
To: user@cassandra.apache.org [user@cassandra.apache.org]
Subject: Re: GCGraceSeconds per ColumnFamily/Keyspace

Probably.  Can you open a ticket?

On Mon, Jul 12, 2010 at 10:41 PM, Todd Burruss <bburruss@real.com> wrote:
> Is it possible to get this feature in 0.7?
>
>
>
> -----Original Message-----
> From: Jonathan Ellis [jbellis@gmail.com]
> Received: 7/12/10 5:06 PM
> To: user@cassandra.apache.org [user@cassandra.apache.org]
> Subject: Re: GCGraceSeconds per ColumnFamily/Keyspace
>
> GCGS per CF sounds totally reasonable to me.
>
> On Mon, Jul 12, 2010 at 6:33 PM, Todd Burruss <bburruss@real.com> wrote:
>> I have two CFs in my keyspace.  one i care about allowing a good amount of
>> time for tombstones to propagate (GCGraceSeconds large) ... but the other i
>> couldn't care and in fact i want them gone ASAP so i don't iterate over
>> them.  has any thought been given to making this setting per Keyspace or per
>> ColumnFamily?
>>
>> my scenario is that i add columns to rows in one CF, UserData, with
>> logging data or activity, but we only want to keep, say 5000 columns per
>> user.  So i also store the user's ID in another CF, PruneCollection, and
>> periodically iterate over it using the IDs found in PruneCollection to
>> ""prune"" the columns in UserData - and then immediately delete the ID from
>> PruneCollection.  if the code is adding, say 50 IDs per second to
>> PruneCollection then the number of deleted keys starts to build up, forcing
>> my iterator to skip over large amounts of deleted keys.  With a small
>> GCGraceSeconds these keys are removed nicely, but i can't do that because it
>> affects the tombstones in UserData as well, which need to be propagated.
>>
>> thoughts?"
CASSANDRA-1230,Memory use grows extremely fast with super column families,"I have a script that inserts about 1kB of key/values into 10k super columns each into 1k rows. Or at least I tried to. I noticed that Cassandra's memory usage went up so fast that I was only able to insert into a few dozen rows before my machine run out of memory. When I use regular column families Cassandra's memory usage seems pretty flat, so this seems an issue specifically with super columns.

test program is attached and copied below

{code}
#!/usr/bin/env python
# Program to demonstrate a use case where Cassandra memory usage grows
# without bounds using super column family:
#  -  1 row  140 MB RES 1400 MB VIRT
#  -  5 rows 532        1600
#  - 10      580        1632
#  - 20      801        1775
#  - 40      958        2047
#  ...
#
# Stopping Cassandra and restarting makes it jump immediately to the same
# virtual memory usage. Resident memory size seems to be about
# half of the state prior to stopping.
# 
# _JAVA_OPTIONS: -Xms64m -Xmx1G
# Cassandra 0.6.2 with default storage-conf.xml on single node
# Ubuntu 10.04 64bit
# sun-java6
# pycassa 0.3.0

import uuid

import pycassa

def insert10k(cf, rowkey):
    for i in xrange(10000):
        cf.insert(rowkey, {
                str(i): {
                    ""abcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""bbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""cbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""dbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""ebcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""fbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""gbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""hbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""ibcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""jbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""kbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""lbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""mbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""nbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""obcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""pbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""qbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""rbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""sbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""tbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""ubcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""vbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""wbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""xbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""ybcdefghijklmnopqrstuvwxyz"":'1234567890',
                    ""zbcdefghijklmnopqrstuvwxyz"":'1234567890',
                    },
                })    

def super_column():
    client = pycassa.connect()
    cf = pycassa.ColumnFamily(client, 'Keyspace1', 'Super1', super=True)

    i = 0
    while i < 1000:
        insert10k(cf, uuid.uuid4().hex)
        print i, 'inserted 10k'
        i += 1

if __name__ == '__main__':
    super_column()
{code}
"
CASSANDRA-1224,Cassandra NPE on insert after one node goes down.,"Hi all,

I posted this in a different thread and was instructed to create a new bug.  As far as I can tell it is not too major of an issue as it may have been cause by us prematurely taking down a node.

I just had this happen in Cassandra 0.6.1. We're only running two nodes as of now and our second one was barely accepting any requests and only being replicated to for the most part. The load went up to 9 consistently so we investigated and noticed its ""Load"" on nodetool was 2x as large as our other instance. I went and cleared out the data and commitlogs, set autobootstrap to true and put it back in.

This is where our case gets funky...we noticed the other instance's load going up a lot and saw that the one I just readded was not doing much. After awhile of contemplating, I took down the second one again. Minutes later I found an open case about the anticompaction happening before full bootstrapping occurs. I found the data/stream dir on the working instance and saw that it was complete...but I had already taken down the second one! So I deleted the stream dir to save space and figured I'd start the process again tomorrow.

A few hours later I am getting these Internal errors on writes:

ERROR [pool-1-thread-287117] 2010-06-23 19:16:51,754 Cassandra.java (line 1492) Internal error processing insert
java.lang.NullPointerException

That was the entire trace.   We tried to kill -3 Cassandra...waited hours and it never killed.  Did a kill -6 but got no usable dump.   Perhaps it is possible for someone to recreate this situation?

I also noticed that the virtual memory Cassandra was taking up tacked on the extra 10+GB for the stream file.  It never released this either which is bad.

Thanks,

Jeff"
CASSANDRA-1201,Red Hat files needed for packaging,"The following files are needed for packaging an RPM.  This is in line with the debian based things put in contrib.  A spec file, an init script.  I would prefer that this go in a redhat folder.  And then fedora and RHEL can use it."
CASSANDRA-1188,multiget_slice calls do not close files resulting in file descriptor leak,"Insert  1000 rows into a super column family. Read them back in a loop using multiget_slice. Note leaked file descriptors with lsof:
lsof -p `ps ax | grep [C]assandraDaemon | awk '{print $1}'` | awk '{print $9}' | sort | uniq -c | sort -n | tail -n 5

Looks like SSTableNamesIterator is never closing the files it creates via the sstable ...?

This is similar to CASSANDRA-1178 except for use of multiget_slice instead of get_slice

"
CASSANDRA-1177,OutOfMemory on heavy inserts,"We have cluster of 6 Cassandra 0.6.2 nodes running under SunOS (see environment).

On initial import (using the thrift API) we see some weird behavior of half the cluster. While cas04-06 look fine as you can see from the attached munin graphs, the other 3 nodes kept on GCing (see log file) until they became unreachable and went OOM. (This is also why the stats are so spotty - munin could no longer reach the boxes) We have seen the same behavior on 0.6.2 and 0.6.1. This started after around 100 million inserts.

Looking at the hprof (which is of course to big to attach) we see lots of ConcurrentSkipListMap$Node's and quite some Column objects. Please see the stats attached.

This looks similar to https://issues.apache.org/jira/browse/CASSANDRA-1014 but we are not sure it really is the same."
CASSANDRA-1175,Client/Server deadlock in aggressive bulk upload (maybe a thrift bug?) (2010-06-07_12-31-16),"I was testing to see how long it takes to upload some 222M lines into Cassandra. Using a single machine (4-core, 8G of mem, opensolaris, 1.6.0_10 64bit and 32bit) to run the server and client.

The client creates a single keyspace with a single column family. The inserted data is 8-byte key, with 26 NVs (3-5 ascii bytes per key, 8 bytes per value) for each line. Using RackUnawareStrategy and replication factor 1. The server install is pretty much out-of-the-box, with -d64 -server -Xmx6G for the server end. (3G for the 32bit VM). The client writes the changes in batches of 1000 lines with batch_mutate, and outputs a logging line every 50k lines.

The import hangs at random points - sometimes after 6900K mark (I think I saw even >10M yesterday, but I lost the window and the backbuffer with it), sometimes only after 1750K. kill -QUIT gives for the server:
---
""pool-1-thread-1"" prio=3 tid=0x0000000000b68800 nid=0x5c runnable [0xfffffd7e676f5000..0xfffffd7e676f5920]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        - locked <0xfffffd7e7ac80fb8> (a java.io.BufferedInputStream)
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
        at org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:363)
        at org.apache.cassandra.thrift.Cassandra$batch_mutate_args.read(Cassandra.java:12840)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1743)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1317)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
---
and for the client:
---
""main"" prio=3 tid=0x08070000 nid=0x2 runnable [0xfe38e000..0xfe38ed38]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        - locked <0xbad8a840> (a java.io.BufferedInputStream)
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:126)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:314)
        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:262)
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:192)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:745)
        at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:729)
        at mycode.MyClass.main(MyClass.java:169)
---

It looks like both ends are trying to read simultaneously from each other, which kind of looks like a thrift bug; but I don't have clear idea what happens in org.apache.cassandra.thrift.Cassandra.

I tried using with thrift r948492, but it didn't help (I didn't recompile the interface classes, I only switched the runtime jar).
"
CASSANDRA-1174,"Debian packaging should auto-detect the JVM, not require OpenJDK","The current init.d script for Debian-packaged Cassandra has the OpenJDK's JAVA_HOME hard-coded in, making it impossible to use sun-java6 without modifying the file. Ideally it should use the same sort of auto-detection logic used by other Debian-packaged Java projects to figure out which JVM it should use.

(I have a patch for this that I'll upload shortly.)"
CASSANDRA-1173,Debian packaging refers to now nonexistent DISCLAIMER.txt,Debian packaging refers to now nonexistent DISCLAIMER.txt. Trivial patch attached.
CASSANDRA-1160,race with insufficiently constructed Gossiper,"Gossiper.start needs to be integrated into the constructor.  Currently you can have threads using the gossiper instance before start finishes (or even starts?), resulting in tracebacks like this:

ERROR [GMFD:1] 2010-06-02 10:45:49,878 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
ERROR [GMFD:2] 2010-06-02 10:45:49,880 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:2,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)"
CASSANDRA-1100,better defaults for flush sorter + writer executor queue sizes,"since currently-in-process-by-executor tasks are no longer in the queue, we don't need to pad the queue size with room for them.

(see CASSANDRA-1099 for background.)
"
CASSANDRA-1099,make CFS.flushWriter_ size configurable,"For small heaps (including our out-of-the-box 1G) we want to allow reducing this from the automatic (1 + 2 * DatabaseDescriptor.getAllDataFileLocations().length); having 3 memtables in the queue, plus another being written, is over 200MB which is a significant amount of data for the GC to chew through (see CASSANDRA-1014).

For large heaps, we might also want to allow *increasing* this value, to accommodate load spikes better.

Unfortunately making this configurable at runtime does not appear to be an option due to the lack of a setCapacity method on any of the BlockingQueue classes."
CASSANDRA-1081,Thrift sockets leak in 0.6 hadoop interface,"Thrift connections appear not to be closed properly in 0.6 in ColumnFamilyRecordReader, which causes a file descriptor leak on the server and may eventually cause jobs to fail.

This appear to be fixed in 0.7 https://issues.apache.org/jira/browse/CASSANDRA-1017 so it may be worth backporting the patch or add a quick fix to close the Tsockets."
CASSANDRA-1080,NPE when no keyspaces section is found in yaml file,Everything's in the summary
CASSANDRA-1079,Cache capacity settings done via nodetool get reset on memtable flushes,"In an experiment we set cache capacities via nodetool. The config file had the KeyCache for this CF at 1000000, we set the RowCache to 10000000 via nodetool.

The next time we flushed a memtable for that CF, the cache capacity settings got reverted to what is in the conf file. We repeated the experiment with the same results."
CASSANDRA-1040,read failure during flush,"Joost Ouwerkerk writes:
	
On a single-node cassandra cluster with basic config (-Xmx:1G)
loop {
  * insert 5,000 records in a single columnfamily with UUID keys and
random string values (between 1 and 1000 chars) in 5 different columns
spanning two different supercolumns
  * delete all the data by iterating over the rows with
get_range_slices(ONE) and calling remove(QUORUM) on each row id
returned (path containing only columnfamily)
  * count number of non-tombstone rows by iterating over the rows
with get_range_slices(ONE) and testing data.  Break if not zero.
}

while this is running, call ""bin/nodetool -h localhost -p 8081 flush KeySpace"" in the background every minute or so.  When the data hits some critical size, the loop will break."
CASSANDRA-1028,NPE in AntiEntropyService.getNeighbors,"Sometimes, but not always, I see this during a test run:

    [junit] Testsuite: org.apache.cassandra.service.AntiEntropyServiceTest
    [junit] Tests run: 10, Failures: 0, Errors: 0, Time elapsed: 3.189 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 10:19:09,743 Error in executor futuretask
    [junit] java.util.concurrent.ExecutionException: java.lang.NullPointerException
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:637)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:151)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:176)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:86)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	... 2 more
    [junit] ------------- ---------------- ---------------

Ideally it would be nice if this could cause an actual test failure when it happens.  Not sure how feasible that is."
CASSANDRA-1015,Internal Messaging should be backwards compatible,"Currently, incompatible changes in the node-to-node communication prevent rolling restarts of clusters.

In order to fix this we should:

1) use a framework that makes doing compatible changes easy
2) have a policy of only making compatible changes between versions n and n+1*


* Running multiple versions should only be supported for small periods of time. Running clusters of mixed version is not needed here."
CASSANDRA-1014,"GC storming, possible memory leak","There appears to be a GC issue due to memory pressure in the 0.6 branch.  You can see this by starting the server and performing many inserts.  Quickly the jvm will consume most of its heap, and pauses for stop-the-world GC will begin.  With verbose GC turned on, this can be observed as follows:

[GC [ParNew (promotion failed): 79703K->79703K(84544K), 0.0622980 secs][CMS[CMS-concurrent-mark: 3.678/5.031 secs] [Times: user=10.35 sys=4.22, real=5.03 secs]
 (concurrent mode failure): 944529K->492222K(963392K), 2.8264480 secs] 990745K->492222K(1047936K), 2.8890500 secs] [Times: user=2.90 sys=0.04, real=2.90 secs]

After enough inserts (around 75-100 million) the server will GC storm and then OOM.

jbellis and I narrowed this down to patch 0001 in CASSANDRA-724.  Switching LBQ with ABQ made no difference, however using batch mode instead of periodic for the commitlog does prevent the issue from occurring.  The attached screenshot shows the heap usage in jconsole first when the issue is exhibiting, a restart, and then the same amount of inserts when it does not."
CASSANDRA-1010,race condition with Gossiper with receiving messages when starting up?,"I occasionally get this exception when starting a node:

ERROR 17:53:52,941 Fatal exception in thread Thread[GMFD:4,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:55)
	at org.apache.cassandra.net.Header.<init>(Header.java:73)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:295)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:888)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

I believe I've tracked down this to there being a small window of time between starting the Gossiper.GossipSynVerbHandler thread and when Gossiper.start is called with it's endpoint. This is because the thread is started in the StorageService constructor, which seems to be getting initialized before thrift.CassandraDaemon.setup is being called."
CASSANDRA-1007,Make memtable flush thresholds per-CF instead of global,"This is particularly useful in the scenario where you have a few CFs with a high volume of overwrite operations; increasing the memtable size/op count means that you can do the overwrite in memory before it ever hits disk.  Once on disk compaction is much more work for the system.

But, you don't want to give _all_ your CFs that high of a threshold because the memory is better used elsewhere, and because it makes commitlog replay unnecessarily painful."
CASSANDRA-1004,"Debian packaging should allow a specific user name and optionally create it as ""cassandra"" on install",
CASSANDRA-1001,Improve messaging and reduce barrier to entry post CASSANDRA-44,"As seen on the mailinglist and from own experience the CASSANDRA-44 changes make it slightly confusing for a first time user to get his first Cassandra instance up and running. We should reduce the risk of turning away potential users.

* Improve our messaging (README, error msg, NEWS etc). 
* Make it much easier to load/create a schema after a first startup. Starting jconsole and digging around for some obscure loading method is confusing and time consuming, we should provide a simple tool to do so."
CASSANDRA-995,"restarting node crashes with NPE when, while replaying the commitlog, the cfMetaData is requested","Removing the commitlog directory completely fixes this.   I can reliably reproduce it by 1) starting and configuring a schema with one keyspace, one super CF with LongType supercolumns; 2) inserting data; 3) shutting down and restarting the node.

Here's my schema expressed in cassidy.pl, should be obvious what the parameters are:
./cassidy.pl -server X -port Y -keyspace system 'kdefine test org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port Y -keyspace test 'fdefine Status Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

The problem seems to be related to CASSANDRA-44 as it happens when the CF metadata is requested but I don't know what's causing it.

10/04/16 15:25:11 INFO commitlog.CommitLog: Replaying /home/cassandra/commitlog/CommitLog-1271449410100.log, /home/cassandra/commitlog/CommitLog-1271449378151.log, /home/cassandra/commitlog/CommitLog-1271449415800.log
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.<init>(Table.java:261)
        at org.apache.cassandra.db.Table.open(Table.java:102)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:233)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:104)
        at org.apache.cassandra.thrift.CassandraDaemon.init(CassandraDaemon.java:151)
        ... 5 more
"
CASSANDRA-987,We need a way to recover from a crash during migration if the crash happens before 'definitions' are flushed.,"Because what happens is this:  the schema exists only in the commit log and so will not be loaded during initialization.  The schema sstables will then be written during CL recovery, but the changes never applied.  This could all be resolved with another restart, but I'd like to find a better way.  This shouldn't be too hard."
CASSANDRA-983,flush and snapshot keyspace/CF before dropping it,
CASSANDRA-946,Add a configuration and implementation to populate the data into memory,"Proactively load data into the memory when the node is started, there will be a configuration to enable this function and will be per Columnfamily. The requirement is to speed up the reads for data which can reside 100% of in the memory.... In addition to enabling the RowCache to 100% we can do this so as upgrades or any other means of restart will not clear the cache in the server."
CASSANDRA-934,NPE in sstable2json,"When sstable2json is not passed any excluded keys via -x, an NPE is raised."
CASSANDRA-893,Hinted Handoff Paging for Columns,"I noticed this in HHManager:

TODO handle rows that have incrementally grown too large for a single message.)

In CASSANDRA-883 I had noticed a lot of CPU burning cycles. I was unable to figure out until tonight why this was. We had a very very large row (9.2M+ columns). If this row ever got hinted, it was trying to load entire row into memory. 

It might be a good idea to actually add paging support (send target multiple row mutations) so that we don't have read the _entire_ row into memory."
CASSANDRA-878,CalloutLocation and StagingFileDirectory are no longer needed.,I couldn't find anywhere these were used in the code.  They should be removed from the default storage-conf.xml.
CASSANDRA-856,Binary Memtable interface writes the same data after each flush.,Subsequent flushes to an a Binary Memtable that has not been modified since the last flush has ghost data written out to the data directory. I've verified that the md5sum of the all the subsequent files are identical.
CASSANDRA-847,Make the reading half of compactions memory-efficient,"This issue is the next on the road to finally fixing CASSANDRA-16. To make compactions memory efficient, we have to be able to perform the compaction process on the smallest possible chunks that might intersect and contend one-another, meaning that we need a better abstraction for reading from SSTables."
CASSANDRA-828,possible NPE in StorageService,"the code
 {{{

     if (endPointThatLeft.equals(FBUtilities.getLocalAddress()))
            {
                logger_.info(""Received removeToken gossip about myself. Is this node a replacement for a removed one?"");
                return;
            }
            if (logger_.isDebugEnabled())
                logger_.debug(""Token "" + token + "" removed manually (endpoint was "" + ((endPointThatLeft == null) ? ""unknown"" : endPointThatLeft) + "")"");
            if (endPointThatLeft != null)
            {
                removeEndPointLocally(endPointThatLeft);
            }
}}}

appears wrong: if it is possible for the leaving endpoint to be unknown then the first ""if"" has a possible null dereference, which can be eliminated by swapping the arguments or reordering the code.

As a side note, I believe FBUtilities.getLocalAddress should probably be synchronized (or localInetAddress made volatile) per the usual ""the java MM does not guarantee any change will ever be visible""  mantra which may or may not be considered relevant :)"
CASSANDRA-789,"Add configurable range sizes, paging to hadoop range queries","For very large (billions) numbers of keys, the current hardcoded 4096 keys per InputSplit could cause the split generator to OOM, since all splits are held in memory at once.  So we want to make 2 changes:

 1) make the number of keys configurable*
 2) make record reader page instead of assuming it can read all rows into memory at once

Note: going back to specifying number of splits instead of number of keys is bad for two reasons.  First, it does not work with the standard hadoop mapred.min.split.size configuration option.  Second, it means we have no way of measuring progress in the record reader, since we have no idea how many keys are in the split.  If we specify number of keys, then even if we page we know (to within a small margin of error) how many keys to expect, even if we page.

See CASSANDRA-775, CASSANDRA-342 for background."
CASSANDRA-787,NPE in DatacenterShardStatergy,There is a long pending fix to contribute back... Plz find the patch.
CASSANDRA-778,Gossiper thread deadlock,"Found this while attempting to bootstrap a node with more than a trivial amount of data:

Found one Java-level deadlock:
=============================
""GMFD:1"":
  waiting to lock monitor 0x0000000100861d60 (object 0x00000001066a7ed8, a org.apache.cassandra.service.StorageService),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x0000000100860710 (object 0x0000000106c7c968, a org.apache.cassandra.gms.Gossiper),
  which is held by ""GMFD:1""

Java stack information for the threads listed above:
===================================================
""GMFD:1"":
	at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:226)
	- waiting to lock <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:634)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:502)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:445)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:812)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:607)
	at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:582)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:649)
	- locked <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1061)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
""main"":
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:861)
	- waiting to lock <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.service.StorageService.startBootstrap(StorageService.java:347)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:318)
	- locked <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:99)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:174)

Found 1 deadlock.

main acquires SS lock and doesn't release it before attempting to acquire the Gossiper lock.  Meanwhile, the gossip stage acquires the Gossiper lock and then attempts to acquire the SS lock.

Solution is to have finer-grained locking on the resource in SS (map of replication strategies), or to move the collection to a different class (DD maybe?).  This was introduced in CASSANDRA-620."
CASSANDRA-774,NPE during compaction,"After an update on trunk, I restarted and began receiving this exception during compaction:

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.io.IOException: Unable to create compaction marker
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
        at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:566)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Unable to create compaction marker
        at org.apache.cassandra.io.SSTableReader.markCompacted(SSTableReader.java:458)
        at org.apache.cassandra.io.SSTableTracker.replace(SSTableTracker.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:621)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:307)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:101)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:82)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more

I restarted a few more times and now only receive this exception:

java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
        at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:566)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.SSTableReader.markCompacted(SSTableReader.java:460)
        at org.apache.cassandra.io.SSTableTracker.replace(SSTableTracker.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:621)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:307)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:101)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:82)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
    
"
CASSANDRA-713,Stacktrace when node taken offline,"I took a node offline last week and then attempted to re-bootstrap its token range with a new cassandra install on the same IP. I made gossip forget about the node by restarting all other instances, then brought up the new node. It said was bootstrapping, but it never finished bootstrapping after several days. The node never showed up in the ring, but when I take it offline, I get the following exception continually from all other nodes in the cluster:

ERROR [pool-1-thread-8] 2010-01-18 21:01:32,405 Cassandra.java (line 1096) Internal error processing batch_insert
java.lang.NullPointerException
        at org.apache.cassandra.dht.BigIntegerToken.compareTo(BigIntegerToken.java:38)
        at org.apache.cassandra.dht.BigIntegerToken.compareTo(BigIntegerToken.java:23)
        at java.util.Collections.indexedBinarySearch(Collections.java:215)
        at java.util.Collections.binarySearch(Collections.java:201)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:130)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
        at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1183)
        at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
        at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
        at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
        at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

In addition, I get frequent UnavailableExceptions on the other nodes.

I cannot remove the token range for this node because it never officially joined the ring."
CASSANDRA-694,Failure to flush commit log,"The following exception occurs consistently on at least node (note did not occur on other same-configured nodes) during startup:

INFO - Replaying /var/lib/cassandra/commitlog/CommitLog-1262855754427.log, /var/lib/cassandra/commitlog/CommitLog-1262832689989.log, /var/lib/cassandra/commitlog/CommitLog-1262885833186.log, /var/lib/cassandra/commitlog/CommitLog-1262900845019.log, /var/lib/cassandra/commitlog/CommitLog-1262913267844.log, /var/lib/cassandra/commitlog/CommitLog-1262927898170.log, /var/lib/cassandra/commitlog/CommitLog-1262961421039.log, /var/lib/cassandra/commitlog/CommitLog-1262977175175.log, /var/lib/cassandra/commitlog/CommitLog-1262989588783.log, /var/lib/cassandra/commitlog/CommitLog-1263000573676.log, /var/lib/cassandra/commitlog/CommitLog-1263013691393.log, /var/lib/cassandra/commitlog/CommitLog-1263044706108.log, /var/lib/cassandra/commitlog/CommitLog-1263060004191.log, /var/lib/cassandra/commitlog/CommitLog-1263071446342.log, /var/lib/cassandra/commitlog/CommitLog-1263082950154.log, /var/lib/cassandra/commitlog/CommitLog-1263095400814.log, /var/lib/cassandra/commitlog/CommitLog-1263118331046.log, /var/lib/cassandra/commitlog/CommitLog-1263143402963.log, /var/lib/cassandra/commitlog/CommitLog-1263155294308.log, /var/lib/cassandra/commitlog/CommitLog-1263166154352.log, /var/lib/cassandra/commitlog/CommitLog-1263178359247.log, /var/lib/cassandra/commitlog/CommitLog-1263202112017.log, /var/lib/cassandra/commitlog/CommitLog-1263230932274.log, /var/lib/cassandra/commitlog/CommitLog-1263250726505.log, /var/lib/cassandra/commitlog/CommitLog-1263264159438.log, /var/lib/cassandra/commitlog/CommitLog-1263289964249.log, /var/lib/cassandra/commitlog/CommitLog-1263317974387.log, /var/lib/cassandra/commitlog/CommitLog-1263331989090.log, /var/lib/cassandra/commitlog/CommitLog-1263344147667.log, /var/lib/cassandra/commitlog/CommitLog-1263359751527.log, /var/lib/cassandra/commitlog/CommitLog-1263395707008.log, /var/lib/cassandra/commitlog/CommitLog-1263397833524.log, /var/lib/cassandra/commitlog/CommitLog-1263398736183.log, /var/lib/cassandra/commitlog/CommitLog-1263399753707.log, /var/lib/cassandra/commitlog/CommitLog-1263401667504.log, /var/lib/cassandra/commitlog/CommitLog-1263404640782.log, /var/lib/cassandra/commitlog/CommitLog-1263405827234.log, /var/lib/cassandra/commitlog/CommitLog-1263406901115.log
INFO - LocationInfo has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(LocationInfo)@25934689
INFO - HintsColumnFamily has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(HintsColumnFamily)@4766820
INFO - AdXRequestStatistics has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(AdXRequestStatistics)@21521158
INFO - TokenGoogleIDCF has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(TokenGoogleIDCF)@22889075
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.db.Table.flush(Table.java:464)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:397)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:65)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:90)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:135)
        ... 5 more

And the same exception occurs intermittently on other node (running) nodes during 'nodeprobe flush':

root@domU-12-31-38-00-26-31:~# nodeprobe -host localhost -port 8080 flush Logger
Exception in thread ""main"" java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.service.StorageService.forceTableFlush(StorageService.java:984)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)"
CASSANDRA-693,MessagingService really doesn't need an instance method,"MessagingService.instance() is only used to get the perf gain from not allocating a new object everytime we send a message or get a verb handler. There is no instance data munged during it's existence and the way even the one instance is used currently is not thread-safe. It's basically a singleton with a weirdo lock around it's creation.

It seems kind of silly to have all the mental overhead of Yet Another Not-Constructor Constructor. We could do many things instead. One idea is to just make MessagingService.instance a public property of it and be done with the whole thing."
CASSANDRA-680,hinted handoff reads all hints for a single keyspace into memory,Need to add paging to HHOM.deliverAllHints
CASSANDRA-659,clean up MessagingService,
CASSANDRA-631,possible NPE in StorageProxy?,"insert() in StorageProxy contains a logging statement that refers to a possibly un-initialized variable
{{{
logger.debug(""insert writing key "" + rm.key() + "" to "" + unhintedMessage.getMessageId() + ""@"" + hintedTarget + "" for "" + target);
}}}

this could happen if getHintedEndpointMap(rm.key(), naturalEndpoints) returns only elements for which target.equals(hintedTarget) returns false, which seems possible to me. 

Looking at the code I get the feeling the reference should probably be to 'hintedMessage', instead of ""unhintedMessage"", if not so an 
assert statement could be appropriate"
CASSANDRA-585,hinted handoff null pointer exception,"During the course of running the cluster I have now run into this error

2009-11-26_02:28:08.99076 ERROR - Error in executor futuretask
2009-11-26_02:28:08.99076 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2009-11-26_02:28:08.99076       at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-26_02:28:08.99076       at java.lang.Thread.run(Thread.java:636)
2009-11-26_02:28:08.99076 Caused by: java.lang.RuntimeException: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager$3.run(HintedHandOffManager.java:281)
2009-11-26_02:28:08.99076       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-26_02:28:08.99076       ... 2 more
2009-11-26_02:28:08.99076 Caused by: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.RowMutation.add(RowMutation.java:119)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:115)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:219)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:75)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager$3.run(HintedHandOffManager.java:277)
2009-11-26_02:28:08.99076       ... 6 more

"
CASSANDRA-578,get_range_slice NPE,"If I call get_range_slice with arguments in the SliceRange structure, then it seems to NPE.  I think it only does it when there is nothing in the range specified in the column slice start and end.


ERROR - Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:8,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
"
CASSANDRA-552,file descriptor leak in getKeyRange,"paste from mailing list:

Cassandra reported the following:

WARN [GMFD:1] 2009-11-12 16:07:24,961 MessagingService.java (line 393)
Exception was generated at : 11/12/2009 16:07:24 on thread GMFD:1
Too many open files
java.net.SocketException: Too many open files
       at sun.nio.ch.Net.socket0(Native Method)
       at sun.nio.ch.Net.socket(Unknown Source)
       at sun.nio.ch.DatagramChannelImpl.<init>(Unknown Source)
       at sun.nio.ch.SelectorProviderImpl.openDatagramChannel(Unknown
Source)
       at java.nio.channels.DatagramChannel.open(Unknown Source)
       at
org.apache.cassandra.net.UdpConnection.init(UdpConnection.java:49)
       at
org.apache.cassandra.net.MessagingService.sendUdpOneWay(MessagingService.java:388)
       at
org.apache.cassandra.gms.GossipDigestSynVerbHandler.doVerb(Gossiper.java:889)
       at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown
Source)
       at java.lang.Thread.run(Unknown Source)

Lsof reports that the java process has 65486 files open (I have ulimit
-n 65535 set in cassandra.in.sh).  Many of the lsof entries include a
trailing '(deleted)' comment after the file path.

This appears to be similar to CASSANDRA-283.  Anyone have a work around
for this?  Would a forced GC take care of the ones marked deleted?

Here is my sample code to count the number of keys:

public class CClient
{
   public static void main(String[] args)
   throws TException, InvalidRequestException, UnavailableException,
UnsupportedEncodingException, NotFoundException
   {
       TTransport tr = new TSocket(""localhost"", 9160);
       TProtocol proto = new TBinaryProtocol(tr);
       Cassandra.Client client = new Cassandra.Client(proto);
       tr.open();

       int     count = 0;
       int     block = 1000;
       String  key   = "" "";

       while (true)
       {
           List<String> list = client.get_key_range(""Keyspace1"",
               ""Standard1"", key, ""~"", block, ConsistencyLevel.ONE);
           int size = list.size();
           if (size == 0)
               break;
           count += size;
           key = list.get(size - 1) + '~';
           System.out.println(""Count: "" + Integer.toString(count));
      }
       tr.close();
   }
}"
CASSANDRA-539,thread flushes from recoverymanager,"this will improve recovery speed

the intent appears to have been to avoid opening up the server to extra load while we're still cleaning up after recovery, but now that we return Future from flush we can have our cake and eat it too"
CASSANDRA-532,Flush creates empty SSTables if nothing exists in that CF,"When calling flush() through nodeprobe, we see SSTables being created that are empty for CFs:

 INFO [COMPACTION-POOL:1] 2009-11-05 22:58:09,515 ColumnFamilyStore.java (line 850) Compacting [org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-9-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-10-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-7-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-11-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-8-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-5-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-6-Data.db')]
ERROR [COMPACTION-POOL:1] 2009-11-05 22:58:09,516 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.SSTableReader.getApproximateKeyCount(SSTableReader.java:102)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompactionInternal(ColumnFamilyStore.java:673)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompaction(ColumnFamilyStore.java:645)
        at org.apache.cassandra.db.CompactionManager$OnDemandCompactor.run(CompactionManager.java:123)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
"
CASSANDRA-522,NPE heisenbug when starting entire cluster (for first time) with autobootstrap=true,
CASSANDRA-505,turn nodeprobe flush_binary into nodeprobe flush,"want it to look like

nodeprobe flush (cf)*

i.e., zero or more CF names as arguments

then it will flush normal and binary memtables for the given CFs; if no CFs are given it should do all of them

this is important for the 0.4 -> 0.5 upgrade since the commitlog format has changed (so we want to let people just blow away the commitlog, after flushing)"
CASSANDRA-478,NPE during read repair,"From Teodor Sigaev:

ERROR [RESPONSE-STAGE:1] 2009-10-08 17:05:25,864 DebuggableThreadPoolExecutor.java (line 110) Error in ThreadPoolExecutor
java.lang.NullPointerException
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:68)
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
ERROR [RESPONSE-STAGE:1] 2009-10-08 17:05:25,916 CassandraDaemon.java (line 71) Fatal exception in thread Thread[RESPONSE-STAGE:1,5,main]
java.lang.NullPointerException
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:68)
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)"
CASSANDRA-471,Submit Flush is Failing with a RejectedExecutionException,"This is probably very specific to my BMT loading job; however, I have started running into this problem lately.  

$ bin/nodeprobe -host 127.0.0.1 flush_binary MyApp
Exception in thread ""main"" java.util.concurrent.RejectedExecutionException
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1760)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:926)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlushBinary(ColumnFamilyStore.java:427)
        at org.apache.cassandra.service.StorageService.forceTableFlushBinary(StorageService.java:802)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
"
CASSANDRA-467,nodeprobe flush_binary using Order Preserving Partitioner,"DecoratedKey.toString is trying to be called on null.

Exception:

        at org.apache.cassandra.db.DecoratedKey.toString(DecoratedKey.java:91)
        at org.apache.cassandra.db.BinaryMemtable.writeSortedContents(BinaryMemtable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore$3$1.run(ColumnFamilyStore.java:950)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)"
CASSANDRA-463,"add MemtableFlushAfterMinutes, a global replacement for FlushPeriodInMinutes",
CASSANDRA-458,Null pointer exception in doIndexing(ColumnIndexer.java:142),"INFO - Saved Token not found. Using 17570558338530880605478324248305304996
INFO - Cassandra starting up...
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@15830327
INFO - Sorting Memtable(Standard1)@15830327
INFO - Writing Memtable(Standard1)@15830327
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-1-Data.db
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@22655307
INFO - Sorting Memtable(Standard1)@22655307
INFO - Writing Memtable(Standard1)@22655307
ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutur
eExceptions(DebuggableThreadPoolExecutor.java:95)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExe
cute(DebuggableThreadPoolExecutor.java:82)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:887)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.ColumnIndexer.doIndexing(ColumnIndexer.java:1
07)
        at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:62
)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(C
olumnFamilySerializer.java:78)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:22
2)
        at org.apache.cassandra.db.ColumnFamilyStore$2$1.run(ColumnFamilyStore.j
ava:934)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:885)
        ... 2 more
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@14600171
INFO - Sorting Memtable(Standard1)@14600171
INFO - Writing Memtable(Standard1)@14600171
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-3-Data.db 

How to reproduce: Run perl script pointed below, three at once.  In short, script just inserts a row and immediately removes it.
#!/usr/local/bin/perl
use lib qw(/usr/local/cassandra/interface/gen-perl/Cassandra /usr/local/cassandra/interface/gen-perl);
use strict;

use Cassandra;

use Thrift::Socket;
use Thrift::BinaryProtocol;
use Thrift::FramedTransport;
use Thrift::BufferedTransport;

use Data::Dumper;
use Time::HiRes qw( gettimeofday tv_interval );
use Getopt::Std;
my %opt;
getopts('iu:t:rn:', \%opt);

my $socket = Thrift::Socket->new('localhost', 9160);
   $socket->setSendTimeout(1000);
   $socket->setRecvTimeout(5000);
my $transport =  Thrift::BufferedTransport->new($socket, 1024, 1024);
my $protocol = Thrift::BinaryProtocol->new($transport);
my $client = Cassandra::CassandraClient->new($protocol);

$transport->open();


eval {
    my $id=0;
    for(;;) {
        $id++;
        my $PID = sprintf(""%040lld"", int(1000000 * rand()));
        $client->batch_insert(
            'Keyspace1',
            $PID,
            {
                'Standard1' => _makeColumnList ({
                    map {
                        $_=>'0'x(int(1 + 100 * rand()))
                    } (0..int(1+10*rand()))
                })
            },
            Cassandra::ConsistencyLevel::ONE
        );
 
        $client->remove(
            'Keyspace1',
            $PID,
            Cassandra::ColumnPath->new({
                column_family=>'Standard1',
            }),
            time(),
            Cassandra::ConsistencyLevel::ONE
        );
        print ""$id\n"" if ($id%100 == 0);
    }
};
 
die Dumper($@) if ($@);
 
$transport->close();
sub _makeColumnList($$) {
    my ($row) = @_;
 
    my @cfmap;
 
    foreach my $k (keys %$row) {
        push @cfmap, Cassandra::ColumnOrSuperColumn->new({
            column=>Cassandra::Column->new({
                name=>$k,
                value=>$row->{$k},
                timestamp=>time(),
            })
        });
    }
    die if $#cfmap < 0;
    return \@cfmap;
}

"
CASSANDRA-445,"commitlog may consider writes flushed, that are not yet","Jun Rao explains:

Suppose there are 3 updates u1, u2, and u3. They are written to commit log in that order. If u1 and u3 are applied to memtable first and at that point, a flush is triggered. After the flush completes, it will move the commit log restarting position based on the log for u3. However, u2 hasn't been persisted on disk yet. This means that if the node dies now, the recovery logic won't replay u2 from the log."
CASSANDRA-443,Add the ability to flush column families via nodeprobe,"It would be useful to be able to flush column families prior to compacting or snapshotting.  Compacting and snapshotting are both available via nodeprobe, so this would be useful there as well.  I would imagine being able to do something like

% nodeprobe flush
% nodeprobe compact
% nodeprobe snapshot

To flush, compact, and snapshot all column families and

% nodeprobe flush myColumnFamily
% nodeprobe compact myColumnFamily
% nodeprobe snapshot myColumnFamily

To flush, compact, and snapshot a single column family."
CASSANDRA-433,Remove item flush limit in BinaryMemtable,"The BinaryMemtable flushes in memory data to disk when the size of the data reaches a certain limit. There is also a hard coded limit that initiates the flush when more then 50000 items have been inserted. That causes issues if a lot of small items are inserted, we should remove or make the limit configurable."
CASSANDRA-405,Race condition with ConcurrentLinkedHashMap,We are seeing a race condition with ConcurrentLinkedHashMap using appendToTail. We could remove the ConcurrentLinkedHashMap for now until that's resolved.
CASSANDRA-401,"Less crappy failure mode when swamped with inserts than ""run out of memory and gc-storm to death""","Suggestion was made that http://java.sun.com/j2se/1.5.0/docs/api/java/lang/management/MemoryPoolMXBean.html#setCollectionUsageThreshold(long) is relevant.  Correlation eludes me, but I Am Not A Java Expert. :)"
CASSANDRA-392,Deadlock with SelectorManager.doProcess and TcpConnection.write,"We ran into a deadlock last night:
Name: MESSAGE-SERIALIZER-POOL:2
State: BLOCKED on sun.nio.ch.SelectionKeyImpl@2e257f1b owned by: TCP Selector Manager
Total blocked: 1  Total waited: 1

Stack trace: 
org.apache.cassandra.net.SelectionKeyHandler.turnOnInterestOps(SelectionKeyHandler.java:73)
org.apache.cassandra.net.TcpConnection.write(TcpConnection.java:186)
   - locked org.apache.cassandra.net.TcpConnection@5ab9f791
org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:67)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
java.lang.Thread.run(Thread.java:619)



Name: TCP Selector Manager
State: BLOCKED on org.apache.cassandra.net.TcpConnection@5ab9f791 owned by: MESSAGE-SERIALIZER-POOL:2
Total blocked: 2  Total waited: 0

Stack trace: 
org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:360)
org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:131)
   - locked sun.nio.ch.SelectionKeyImpl@2e257f1b
org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:98)


The SelectionManager.doProcess acquires a monitor on the SelectionKey and then calls methods such as TcpConnection.connect(SelectionKey key) which obtains a monitor for the TcpConnection object itself.  Another task eg: MessageSerializationTask can come along and call write(Message message) which obtains a monitor for the TCPConnection first and then on calls to turnOnInterestOps tries to obtain the monitor for the SelectionKey which causes the deadlock.


"
CASSANDRA-367,CommitLog does not flush on writes,"When you write data to CommitLog, we are not calling a flush() in class LogRecordAdder.

Is this acceptable? We added flush() and its working now. This bug was introduced when things were consolidated in r799942"
CASSANDRA-358,SystemTable.initMetadata throws an NPE when called twice,"While this is not the expected use case of it, SystemTable.initMetadata throws an NPE when called twice in the same process.  The error points to tokenColumn being null even through cf is not at line 111."
CASSANDRA-346,Improve the speed of RandomPartitioner comparator which will help flushing of BMT and compaction,During our BMT import we found that the flushing process couldn't keep up with the incoming data.  One of the bottlenecks was due to the sorting of decorated keys.
CASSANDRA-320,Memtable sometimes not enqueued for flush,"There appears to be a (rare) race condition with flushing memtables.   Occasionally, when Cassandra is in the process of flushing a table, another switch will occur.  The latter table will be added to memtablesPendingFlush in ColumnFamilyStore, but it will not be enqueued for flush and thus hang around forever.  Adding an else clause and a print statement to Memtable.java's enqueueFlush method reveals that the memtable is not being flushed because isFrozen_ is already true."
CASSANDRA-313,File descriptor leak in CommitLog,"There is a file descriptor leak in CommitLog.java.  On systems with a default ulimit of 1024, this causes Cassandra to eventually crash due to too many open files.  A descriptor appears to be leaked at each memtable rotation."
CASSANDRA-283,Cassandra leaks FDs,"Cassandra leaks file descriptors like crazy. I started getting these errors after a few hours of uptime:

java.lang.RuntimeException: java.io.FileNotFoundException: /var/cassandra/data/Digg-Items-2-Data.db (Too many open files)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:84)
	at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:181)
	at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:859)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.FileNotFoundException: /var/cassandra/data/Digg-Items-2-Data.db (Too many open files)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:141)
	at org.apache.cassandra.io.SequenceFile$BufferReader.init(SequenceFile.java:811)
	at org.apache.cassandra.io.SequenceFile$Reader.<init>(SequenceFile.java:743)
	at org.apache.cassandra.io.SequenceFile$BufferReader.<init>(SequenceFile.java:805)
	at org.apache.cassandra.io.SequenceFile$ColumnGroupReader.<init>(SequenceFile.java:248)
	at org.apache.cassandra.io.SSTableReader.getColumnGroupReader(SSTableReader.java:346)
	at org.apache.cassandra.db.SSTableColumnIterator.<init>(ColumnIterator.java:61)
	at org.apache.cassandra.db.ColumnFamilyStore.getSliceFrom(ColumnFamilyStore.java:1589)
	at org.apache.cassandra.db.Table.getRow(Table.java:596)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:60)
	at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:600)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:303)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:80)

I have an open file limit of 1024. Examining the lsof output for Cassandra shows 975 FDs for the same file: /var/cassandra/data/Digg-Items-2-Data.db

Clearly, these FDs are leaking somewhere."
CASSANDRA-230,Race in ChecksumManager.instance(),There is a minor race condition in ChecksumManager.instance(). Patch attached.
CASSANDRA-204,Replayed log data is not flushed before logs are wiped,"The memtable created by replaying commit logs on startup is supposed to be flushed as a SSTable before the commitlog is removed, but this is not happening.  So you can lose data by doing the following:

1. insert data
2. restart cassandra (using kill, to force replay)
3. restart cassandra again
"
CASSANDRA-201,get_slice_from forces iterating all columns and leaks file handlers with exception ,"There are 2 bugs in the get_slice_from code in CFS.java. 
1. The following 2 lines forces all columns to be iterated in each iterator, which is inefficient.
            List<IColumn> L = new ArrayList();
            CollectionUtils.addAll(L, collated);
2. If any exception occurs, the opened file handlers are not closed.
"
CASSANDRA-160,race condition in compaction makes it possible to return null when data in fact exists,
CASSANDRA-157,make cassandra not allow itself to run out of memory during sustained inserts,"Tv on IRC pointed out to me that the issue that I've been encountering
is probably point 2. in this roadmap:

    http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00160.html

I was unable to find any existing issue for this topic, so I'm creating a new one.

Since this issue would block our use of Cassandra I'm happy to look into it,
but if this is a known issue perhaps there's already a plan for addressing it
that could be clarified?"
CASSANDRA-149,regression in CF.digest causes NPE when CF contains no columns,
CASSANDRA-141,forceFlush skips flush when there are pending operations,
CASSANDRA-134,"Support flush based on timer interval, in addtion to size","Today, the CFs are flushed purely based on the size of the data accumulated in Memtable. If a table has multiple CFs and some CFs are updated at a much slower pace than others, this can prevent a larger number of log files from being deleted. This is because the CF bit in the log header is only turned off when a CF is flushed. A log can't be deleted until all CF bits in the header are cleared. One solution is to add a background flusher that periodically force-flushes every CF."
CASSANDRA-129,`show config file` in cli causes server to throw NPE ,"Booting up the cli and running the command ""show config file"" in the lastest from trunk (r771019) on a fresh and empty cassandra instance causes a NullPointerError to be thrown. By looking at the code (but being a not-so-hot java developer), it looks like the problem is simply DatabaseDescriptor.getConfigFileName() returning a null because configFileName_ never gets set anywhere in the code.

The error in question:

ERROR - Error occurred during processing of message.
java.lang.NullPointerException
	at java.io.FileInputStream.<init>(FileInputStream.java:133)
	at java.io.FileInputStream.<init>(FileInputStream.java:96)
	at org.apache.cassandra.service.CassandraServer.getStringProperty(CassandraServer.java:485)
	at org.apache.cassandra.service.Cassandra$Processor$getStringProperty.process(Cassandra.java:1294)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:860)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:713)

"
CASSANDRA-98,Reads (get_column) miss data or return stale values if a memtable is being flushed,"Reads can return missing values (null/exception) or find stale copies of a column if the read happens during an SSTable flush.

The get_column can go in, and not find the data in the current memtable. When it looks in the ""historical"" memtable, if that CF has already been flushed, then  it gets cleared from the historical memtable. As a result, the read looks for the column in older SSTables and finds a stale value (if it exists) or returns with null.

It can be tricky to reproduce this problem, but the reason is pretty easy to see.

While subsequent reads might return the correct value (from disk), this behavior makes it very difficult for apps that expect to ""read your writes"", at least in the absence of failures."
CASSANDRA-97,race condition prevents startup under Xen vm,Reported by Soo Hwan Park on the mailing list.
CASSANDRA-76,Don't rely on flushkey_ special value to force flush,We can force flush programatically w/o needing this workaround.
CASSANDRA-60,Enable line numbers in test stack traces,"Right now the stack traces in tests don't include the line number, it makes debugging more difficult."
CASSANDRA-51, Memory footprint for memtable,"The implementation of EfficientBidiMap(EBM) today stores the column in two place, a map and a sorted set. Both data structures store exactly the same values.

I assume we're storing this twice so that the map can give us O(1) reads while the sortedset is important for efficient flush. Is this tradeoff important ? Do we want to store the data twice to get O(1) reads over O(log(n)) reads from sortedset? Is the sortedset implementation broken? Perhaps we should consider a configuration option that turns off the map -- write performance will be slightly improved, read performance will be somewhat worse, and the memory footprint will probably be about half. Certainly sounds like a good alternative tradeoff.
"
CASSANDRA-16,Memory efficient compactions ,"The basic idea is to allow rows to get large enough that they don't have to fit in memory entirely, but can easily fit on a disk. The compaction algorithm today de-serializes the entire row in memory before writing out the compacted SSTable (see ColumnFamilyStore.doCompaction() and associated methods).

The requirement is to have a compaction method with a lower memory requirement so we can support rows larger than available main memory. To re-use the old FB example, if we stored a user's inbox in a row, we'd want the inbox to grow bigger than memory so long as it fit on disk."
